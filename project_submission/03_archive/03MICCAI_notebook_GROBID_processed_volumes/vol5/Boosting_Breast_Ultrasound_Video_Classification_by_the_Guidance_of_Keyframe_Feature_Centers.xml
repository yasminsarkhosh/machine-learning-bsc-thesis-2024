<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers</title>
				<funder ref="#_NatvQeD">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
				<funder ref="#_myuDr4b">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anlan</forename><surname>Sun</surname></persName>
							<email>anlan.sun@yizhun-ai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yizhun Medical AI Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
							<email>zhangzh@stu.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Lei</surname></persName>
							<email>leimeng@stu.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Dai</surname></persName>
							<email>yuting.dai@yizhun-ai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yizhun Medical AI Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wangdongcis@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@pku.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Machine Learning Research</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="441" to="451"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C2602CEA13D24645200C02F295BAEF0E</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Breast ultrasound classification</term>
					<term>Ultrasound video</term>
					<term>Coherence loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Breast ultrasound videos contain richer information than ultrasound images, therefore it is more meaningful to develop video models for this diagnosis task. However, the collection of ultrasound video datasets is much harder. In this paper, we explore the feasibility of enhancing the performance of ultrasound video classification using the static image dataset. To this end, we propose KGA-Net and coherence loss. The KGA-Net adopts both video clips and static images to train the network. The coherence loss uses the feature centers generated by the static images to guide the frame attention in the video model. Our KGA-Net boosts the performance on the public BUSV dataset by a large margin. The visualization results of frame attention prove the explainability of our method. We release the code and model weights in https://github.com/PlayerSAL/KGA-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer is a life-threatening disease that has surpassed lung cancer as leading cancer in some countries and regions <ref type="bibr" target="#b19">[20]</ref>. Breast ultrasound is the primary screening method for diagnosing breast cancer, and accurately distinguishing between malignant and benign breast lesions is crucial. This task is also an essential component of computer-aided diagnosis. Since each frame in an ultrasound video can only capture a specific view of a lesion, it is essential to aggregate information from the entire video to perform accurate automatic lesion diagnosis. Therefore, in this study, we focus on the classification of breast ultrasound videos for detecting malignant and benign breast lesions.  <ref type="bibr" target="#b14">[15]</ref> and static images from BUSI <ref type="bibr" target="#b0">[1]</ref>. We use a 2D ResNet trained on ultrasound images to get the features.</p><p>While ultrasound videos offer more information, prior studies have primarily focused on static image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>. Obtaining ultrasound video data with pathology gold standard results poses a major challenge. Sonographers typically record keyframe images during general ultrasound examinations, not entire videos. Prospective collection requires additional efforts to track corresponding pathological results. Consequently, while there are many breast ultrasound image datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>, breast ultrasound video datasets remain scarce, with only one relatively small dataset <ref type="bibr" target="#b14">[15]</ref> containing 188 videos available currently.</p><p>Given the difficulties in collecting ultrasound video data, we investigate the feasibility of enhancing the performance of ultrasound video classification using a static image dataset. To achieve this, we first analyze the relationship between ultrasound videos and images. The images in the ultrasound dataset are keyframes of a lesion that exhibit the clearest appearance and most typical symptoms, making them more discriminative for diagnosis. Although ultrasound videos provide more information, the abundance of frames may introduce redundancy or vagueness that could disrupt classification. From the aspect of feature distribution, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the feature points of static images are more concentrated, while the feature of video frames sometimes are away from the class centers. Frames far from the centers are harder to classify. Therefore, it is a promising approach to guide the video model to pay more attention to important frames close to the class center with the assistance of static keyframe images. Meanwhile, our approach aligns with the diagnosis of ultrasound physicians, automatically evaluates the importance of frames, and diagnoses based on the information of key frames. Additionally, our method provides interpretability through key frames.</p><p>In this paper, we propose a novel Keyframe Guided Attention Network (KGA-Net) to boost ultrasound video classification. Our approach leverages both image (keyframes) and video datasets to train the network. To classify videos, we use frame attention to predict feature weights for all frames and aggregate them to make the final classification. The feature weights determine the contribution of each frame for the final diagnosis. During training, we construct category feature centers for malignant and benign examples respectively using center loss <ref type="bibr" target="#b25">[26]</ref> on static image inputs and use the centers to guide the training of video frame attention. Specifically, we propose coherence loss, which promotes the frames close to the centers to have high attention weights and decreases the weights for frames far from the centers. Due to the feature centers being generated by the larger scale image dataset, it provides more accurate and discriminative feature centers which can guide the video frame attention to focus on important frames, and finally leads to better video classification.</p><p>Our experimental results on the public BUSV dataset <ref type="bibr" target="#b14">[15]</ref> show that our KGA-Net significantly outperforms other video classification models by using an external ultrasound image dataset. Additionally, we visualized attention values guided by the coherence loss. The frames with clear diagnostic characteristics are given higher attention values. This phenomenon makes our method more explainable and provides a new perspective for selecting keyframes from video.</p><p>In conclusion, our contributions are as follows:</p><p>1. We analyze the relationship between ultrasound video data and image data, and propose the coherence loss to use image feature centers to guide the training of frame attention. 2. We propose KGA-Net, which adopts a static image dataset to boost the performance of ultrasound video classification. KGA-Net significantly outperforms other video baselines on the BUSV dataset. 3. The qualitative analysis of the frame attention verifies the explainability of our method and provides a new perspective for selecting keyframes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Breast Ultrasound Classification. Breast ultrasound (BUS) plays an important supporting role in the diagnosis of breast-related diseases. Recent research demonstrated the potential of deep learning for breast lesion classification tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> design ensemble methods to integrate the features of multiple models to obtain higher accuracy. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref> utilize multi-task learning to improve the model performance. However, all of them are based on image datasets, such as BUSI <ref type="bibr" target="#b0">[1]</ref>, while few works focus on the video modality. <ref type="bibr" target="#b13">[14]</ref> design a pre-training model based on contrastive learning for ultrasound video classification. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> develop a keyframe extraction model for ultrasound videos and utilized the extracted keyframes to perform various classification tasks. However, these methods rely on keyframe supervision, which limits their applicability. Fortunately, the recent publicly available dataset BUSV <ref type="bibr" target="#b14">[15]</ref> has made the research on the task of BUS video-based classification possible. In this paper, we build our model based on this dataset.</p><p>Video Recognition Based on Neural Networks. Traditional methods are based on Two-stream networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. Since I3D <ref type="bibr" target="#b2">[3]</ref> was proposed, 3D CNNs have dominated video understanding for a long time. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> decompose 3D convolution in different ways to reduce computation complexity without losing performance. <ref type="bibr" target="#b7">[8]</ref> designed two branches to focus on temporal information and spatial features, respectively. However, 3D CNNs have a limited receptive field, and thus struggle to capture long-range dependency. Vision Transformers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> have become popular for their ability to aggregate spatial-temporal information.</p><p>To address computational complexity, MViT <ref type="bibr" target="#b6">[7]</ref> employed a hierarchical structure and Video Swin <ref type="bibr" target="#b16">[17]</ref> introduced 3D shifted window attention. Our proposed KGA-Net is a simple framework that leverages the frame attention module to aggregate multi-frame features efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our KGA-Net takes the video inputs and static image inputs simultaneously to train the network. The coherence loss is proposed to guide the frame attention by using the feature centers generated by the images. We will then elaborate on each component in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video and Image Classification Network</head><p>The video classification network is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> (a). The model is composed of a 2D CNN backbone, a frame attention module, and a classification head. For an input video clip V composed of N frames, it is first processed by the backbone network and the feature vectors of the frames {F i } N i=1 are obtained. Then, the frame attention module predicts the attention weight for each frame using a FC and sigmoid layer, and then the features are aggregated by the weights to form an integrated feature vector. Formally,</p><formula xml:id="formula_0">w i = Sigmoid(FC(F i ))<label>(1)</label></formula><p>where w i denotes the weight for the i th frame and FC is the fully-connected layer. Then, the features are aggregated by</p><formula xml:id="formula_1">F V = N i=1 w i • F i .</formula><p>Finally, the classification head is applied to the final result of lesion classification. To train the model, the cross-entropy loss (CE Loss) is applied to the classification prediction of the video. The image classification network is used to assist in training the video model. We use the same 2D CNN as the backbone network in the video classification network. The model weights are shared for the two backbones for better generalization. To promote the formation of feature centers, we apply the center loss <ref type="bibr" target="#b25">[26]</ref> to the image model besides the cross-entropy loss. In addition, the frame-level cross-entropy loss is also applied to the video frames to facilitate training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Coherence Loss</head><p>In this section, we introduce the coherence loss to guide the frame attention with the assistance of the category feature centers. We use the same method as center loss <ref type="bibr" target="#b25">[26]</ref> to obtain the feature centers for the malignancy and benign lesions, which are denoted as C mal and C benign , respectively.</p><p>The distances of frame features and the feature centers can measure the quality of the frames. The frame features close to the centers are more discriminative for the classification task. Therefore, we use these distances to guide the generation of frame attention. Specifically, we push the frames close to the centers to have higher attention weights and decrease the weights far from the centers. To do this, for each video frame with feature F i , we first calculate the feature distance from its corresponding class center. Formally,</p><formula xml:id="formula_2">d i = F i -C Y 2 , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where Y ∈ {mal, benign} is the label of the video V and d i is the computed distance of frame i. Afterward, we apply coherence loss to the attention weights w = [w 1 , w 2 , ..., w N ] to make them have a similar distribution with the feature distances d = [d 1 , d 2 , ..., d N ] . To supervise the distribution, the coherence loss is defined as the L2 loss of the gram matrix of these two vectors</p><formula xml:id="formula_4">L Coh = Gram w -Gram d 2 , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">Gram w = (1-w)•(1-w) 1-w 2 2</formula><p>is the gram matrix of normalized attention weights, and</p><formula xml:id="formula_7">Gram d = d•d d 2 2</formula><p>is the gram matrix of normalized feature distances. Note that lower distances correspond to stronger attention, hence we use the opposite of w to get Gram w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Total Training Loss</head><p>To summarize, the total training loss of our KGA-Net</p><formula xml:id="formula_8">L total = L V CE + L I CE + L Center + λ • L Coh . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>L V CE and L I CE denote the cross-entropy for video classification and image and frame classification. L Center means the center loss. λ is the weight for coherence loss. Empirically, we set λ = 1 in our experiments.</p><p>During inference, to perform classification on video data, the video classification network can be utilized individually for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Datasets. We use the public BUSV dataset <ref type="bibr" target="#b14">[15]</ref> for video classification and the BUSI dataset <ref type="bibr" target="#b0">[1]</ref> as the image dataset. BUSV consists of 113 malignant videos and 75 benign videos. BUSI contains 445 images of benign lesions and 210 images of malignant lesions. For the BUSV dataset, we use the official data split in <ref type="bibr" target="#b14">[15]</ref>. All images of the BUSI dataset are adopted to train our KGA-Net. Model Details. ResNet-50 <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref> is used as backbone. We use SGD optimizer with an initial learning rate of 0.005, which is reduced by 10× at the 4,000th and 6,000th iteration. The total learning iteration number is 8,000. The learning rate warmup is used in the first 1,000 iterations. For each batch, the video clips and static images are both sampled and sent to the network. We use a total batchsize of 16 and the sample probability of video clips and images is 1:1. We implement the model based on Pytorch and train it with NVIDIA Titan RTX GPU cards.</p><p>During inference, we use the video classification network individually. In order to satisfy the fixed video length requirement of MViT <ref type="bibr" target="#b6">[7]</ref>, we sample up to 128 frames of each video to form a video clip and predict its classification result using all the models in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Video Models</head><p>In this section, we compare our KGA-Net with other competitive video classification models. However, comparing with ultrasound-video-based work is challenging due to limited code accessibility and lack of keyframe detection model in existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Therefore, we compare our method with strong video baselines on natural images. We include CNN-based models, I3D <ref type="bibr" target="#b2">[3]</ref>, Slow-Fast <ref type="bibr" target="#b7">[8]</ref>, R(2+1)D <ref type="bibr" target="#b21">[22]</ref>, and CSN <ref type="bibr" target="#b20">[21]</ref>, along with the popular transformer-based model MViT <ref type="bibr" target="#b6">[7]</ref>. For fairness comparison, we train these models using both video and image data, treating images as static videos. Evaluation metrics are reported on the BUSV test set for performance assessment.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, by leveraging the guidance of the image dataset, our KGA-Net significantly surpasses all other models on all of the metrics. The video classification model of our KGA-Net is composed of a standard 2D ResNet-50 and a light feature attention module, while the baseline models are with net structures carefully designed for video analysis. Therefore, the success of our KGA-Net lies in the correct usage of the image guidance. The feature centers formed by the image dataset with larger data size and clear appearance effectively improve the accuracy of frame attention hence boosting the video classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we ablate the contribution of each key design in our KGA-Net. We observe their importance by removing these key components from the whole network. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. The results of KGA-Net are shown in the last row in Table <ref type="table" target="#tab_1">2</ref>, while the components are ablated in the first three rows. We use the same training schedule for all of the experiments.</p><p>Image guidance is the main purpose of our method. To portray the effect of using the image dataset, we train the KGA-Net using BUSV dataset alone in the first row of Table <ref type="table" target="#tab_1">2</ref>. Without the image dataset, we generate the feature centers from the video frames. As a result, the performance significantly drops due to the decrease in dataset scale. It also shows that the feature centers generated by the image dataset are more discriminative than that of the video dataset. It is not only because the lesion number of BUSI is larger than BUSV, but also because the images in BUSI are all the keyframes that contain typical characteristics of lesions.</p><p>Frame attention and coherence loss are two essential modules of our KGA-Net. We train a KGA-Net without the coherence loss in the third row of Table <ref type="table" target="#tab_1">2</ref>.</p><p>In the second row, we further replace the feature attention module with feature averaging of video frames. It can be seen that both of these two modules contribute to the overall performance according to AUC and ACC. It is worth noting that these two models without coherence loss obtain very low sensitivity and high specificity, which means the model predictions are imbalanced and intend to make benign predictions. It is because that clear malignant appearances usually only exist in limited frames in a malignant video. Without our coherence loss or frame attention, it is difficult for the model to focus on typical frames that possess malignant features. This phenomenon certifies the effectiveness of our KGA-Net to prevent false negatives in diagnosis.  In Fig. <ref type="figure" target="#fig_2">3</ref>, we illustrate video frames with their corresponding frame attention weights predicted by KGA-Net. Overall speaking, the frames with high attention weights do have clear image appearances for diagnosis. For example, the first three frames in Fig. <ref type="figure" target="#fig_2">3(b</ref>) clearly demonstrate the edge micro-lobulation and irregular shapes, which lead to malignant judgment. Furthermore, we plot the relationships between the predicted attention values and the feature distances to the centers. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(e), these two variables are linearly related, which indicates that KGA-Net the attention weights are effectively guided by the feature distances.</p><p>The qualitative analysis proves the interpretability of our method, which will benefit clinical usage. Moreover, the attention weights reveal the importance of each frame for lesion diagnosis. Therefore, it can provide a new perspective for the keyframe extraction task of ultrasound videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose KGA-Net, a novel video classification model for breast ultrasound diagnosis. Our KGA-Net takes as input both the video data and image data to train the network. We propose the coherence loss to guide the training of the video model by the guidance of feature centers of the images. Our method significantly exceeds the performance of other competitive video baselines. The visualization of the attention weights validates the effectiveness and interpretability of our KGA-Net.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Feature distribution of video frames from BUSV<ref type="bibr" target="#b14">[15]</ref> and static images from BUSI<ref type="bibr" target="#b0">[1]</ref>. We use a 2D ResNet trained on ultrasound images to get the features.</figDesc><graphic coords="2,43,29,54,29,337,42,128,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed keyframe-guided attention network.</figDesc><graphic coords="4,41,79,54,50,340,21,168,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual Analysis. (a-d) Visualization of video frames and corresponding frame attention weights. (e) Relationship between attention weight and feature distance.</figDesc><graphic coords="8,54,54,232,40,322,87,143,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other video models. Classification thresholds are determined by Youden index.</figDesc><table><row><cell>Model</cell><cell cols="4">AUC(%) ACC(%) Sensitivity(%) Specificity(%)</cell></row><row><cell>I3D [3]</cell><cell>88.31</cell><cell>81.58</cell><cell>84.00</cell><cell>76.92</cell></row><row><cell>SlowFast [8]</cell><cell>82.54</cell><cell>79.49</cell><cell>76.92</cell><cell>84.62</cell></row><row><cell>R(2+1)D [22]</cell><cell>86.46</cell><cell>81.58</cell><cell>84.00</cell><cell>76.92</cell></row><row><cell>CSN [21]</cell><cell>83.38</cell><cell>81.58</cell><cell>84.00</cell><cell>76.92</cell></row><row><cell>MViT [7]</cell><cell>90.53</cell><cell>82.05</cell><cell>80.77</cell><cell>84.62</cell></row><row><cell cols="2">KGA-Net (Our) 94.67</cell><cell>89.74</cell><cell>88.46</cell><cell>92.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies. Model components are removed in the first three lines to analyze their contributions in KGA-Net. Classification thresholds are determined by Youden index.</figDesc><table><row><cell>Model</cell><cell cols="4">AUC(%) ACC(%) Sensitivity(%) Specificity(%)</cell></row><row><cell>w/o image guidance</cell><cell>85.21</cell><cell>76.92</cell><cell>73.08</cell><cell>84.62</cell></row><row><cell cols="2">w/o coherence loss &amp; attention 88.17</cell><cell>74.36</cell><cell>61.54</cell><cell>100.0</cell></row><row><cell>w/o coherence loss</cell><cell>92.90</cell><cell>87.18</cell><cell>80.77</cell><cell>100.0</cell></row><row><cell>KGA-Net</cell><cell>94.67</cell><cell>89.74</cell><cell>88.46</cell><cell>92.31</cell></row><row><cell>4.4 Visual Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2022ZD0114900</rs>) and <rs type="funder">National Science Foundation of China</rs> (<rs type="grantNumber">NSFC62276005</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_myuDr4b">
					<idno type="grant-number">2022ZD0114900</idno>
				</org>
				<org type="funding" xml:id="_NatvQeD">
					<idno type="grant-number">NSFC62276005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breast mass classification with transfer learning based on scaling of deep representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Byra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">102828</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks based classification of breast ultrasonography images by hybrid method with respect to benign, malignant, and normal using mRMR</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eroğlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Çinar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">104407</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision transformers for classification of breast ultrasound images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gheflati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="480" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extracting keyframes of breast ultrasound video using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102490</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Masked video modeling with correlationaware contrastive learning for breast cancer diagnosis in ultrasound</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16876-5_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16876-5_11" />
	</analytic>
	<monogr>
		<title level="m">Resource-Efficient Medical Image Analysis: First MICCAI Workshop</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
	<note>REMIA</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computeraided diagnosis of breast ultrasound images using ensemble learning from convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="page">105361</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully-automated deep learning pipeline for segmentation and classification of breast ultrasound images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Piano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101816</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Colorectal cancer statistics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="193" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>CA</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Information bottleneck-based interpretable multitask network for breast cancer classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102687</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46484-8_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46484-8_2" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-frame guided network for thyroid nodule recognition using ultrasound videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_23" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46478-7_31" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SHA-MTL: soft and hard attention multi-task learning for automated breast cancer ultrasound image segmentation and classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1719" to="1725" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BUSIS: a benchmark for breast ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">729</biblScope>
			<date type="published" when="2022">2022</date>
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
