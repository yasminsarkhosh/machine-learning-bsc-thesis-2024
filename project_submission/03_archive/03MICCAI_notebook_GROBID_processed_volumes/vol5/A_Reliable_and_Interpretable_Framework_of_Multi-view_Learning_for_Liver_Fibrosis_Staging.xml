<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging</title>
				<funder ref="#_eu36hSe">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_kysakRp">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheyao</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanye</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuping</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Nuffield Department of Population Health</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nannan</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shanghai Public Health Clinical Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shanghai Public Health Clinical Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiahai</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="178" to="188"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DECAA26FA7E4B373C7599970D2826AD1</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Liver fibrosis</term>
					<term>Multi-view learning</term>
					<term>Uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Staging of liver fibrosis is important in the diagnosis and treatment planning of patients suffering from liver diseases. Current deep learning-based methods using abdominal magnetic resonance imaging (MRI) usually take a sub-region of the liver as an input, which nevertheless could miss critical information. To explore richer representations, we formulate this task as a multi-view learning problem and employ multiple sub-regions of the liver. Previously, features or predictions are usually combined in an implicit manner, and uncertainty-aware methods have been proposed. However, these methods could be challenged to capture cross-view representations, which can be important in the accurate prediction of staging. Therefore, we propose a reliable multi-view learning method with interpretable combination rules, which can model global representations to improve the accuracy of predictions. Specifically, the proposed method estimates uncertainties based on subjective logic to improve reliability, and an explicit combination rule is applied based on Dempster-Shafer's evidence theory with good power of interpretability. Moreover, a data-efficient transformer is introduced to capture representations in the global view. Results evaluated on enhanced MRI data show that our method delivers superior performance over existing multi-view learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Viral or metabolic chronic liver diseases that cause liver fibrosis impose great challenges on global health. Accurate staging for the severity of liver fibrosis is essential in the diagnosis of various liver diseases. Current deep learning-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> mainly use abdominal MRI and computed tomography (CT) data for liver fibrosis staging. Usually, a square sub-region of the liver instead of the whole image is cropped as input features, since the shape of the liver is irregular and unrelated anatomies in the abdominal image could disturb the training of deep learning models. To automatically extract the region of interest (ROI), a recent work <ref type="bibr" target="#b7">[8]</ref> proposes to use slide windows to crop multiple image patches around the centroid of the liver for data augmentation. However, it only uses one patch as input at each time, which only captures a sub-view of the liver. To exploit informative features across the whole liver, we formulate this task as a multi-view learning problem and consider each patch as a view.</p><p>The aim of multi-view learning is to exploit complementary information from multiple features <ref type="bibr" target="#b22">[23]</ref>. The central problem is how to integrate features from multiple views properly. In addition to the naive method that concatenates features at the input level <ref type="bibr" target="#b4">[5]</ref>, feature-level fusion strategies seek a common representation between different views through canonical correlation analysis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> or maximizing the mutual information between different views using contrastive learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. In terms of decision-level fusion, the widely used methods are decision averaging <ref type="bibr" target="#b17">[18]</ref>, decision voting <ref type="bibr" target="#b13">[14]</ref>, and attention-based decision fusion <ref type="bibr" target="#b8">[9]</ref>. However, in the methods above, the weighting of multi-view features is either equal or learned implicitly through model training, which undermines the interpretability of the decision-making process. Besides, they are not capable of quantifying uncertainties, which could be non-trustworthy in healthcare applications.</p><p>To enhance the interpretability and reliability of multi-view learning methods, recent works have proposed uncertainty-aware decision-level fusion strategies. Typically, they first estimate uncertainties through Bayesian methods such  as Monte-Carlo dropout <ref type="bibr" target="#b19">[20]</ref>, variational inference <ref type="bibr" target="#b18">[19]</ref>, ensemble methods <ref type="bibr" target="#b3">[4]</ref>, and evidential learning <ref type="bibr" target="#b16">[17]</ref>. Then, the predictions from each view are aggregated through explicit uncertainty-aware combination rules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>, as logic rules are commonly acknowledged to be interpretable in a complex model <ref type="bibr" target="#b25">[26]</ref>. However, the predictions before the combination are made based on each independent view. Cross-view features are not captured to support the final prediction. In our task, global features could also be informative in the staging of liver fibrosis.</p><p>In this work, we propose an uncertainty-aware multi-view learning method with an interpretable fusion strategy of liver fibrosis staging, which captures both global features across views and local features in each independent view. The road map for this work is shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. The uncertainty of each view is estimated through the evidential network and subjective logic to improve reliability. Based on the uncertainties, we apply an explicit combination rule according to Dempster-Shafer's evidence theory to obtain the final prediction, which improves explainability. Moreover, we incorporate an additional global view to model the cross-view representation through the data-efficient transformer.</p><p>Our contribution has three folds. First, we are the first to formulate liver fibrosis staging as a multi-view learning problem and propose an uncertaintyaware framework with an interpretable fusion strategy based on Dempster-Shafer Evidence Theory. Second, we propose to incorporate global representation in the multi-view learning framework through the data-efficient transformer network. Third, we evaluate the proposed framework on enhanced liver MRI data. The results show that our method outperforms existing multi-view learning methods and yields lower calibration errors than other uncertainty estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The aim of our method is to derive a distribution of class probabilities with uncertainty based on multiple views of a liver image. The pipeline for view extraction is shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. A square region of interest (ROI) is cropped based on the segmentation of the foreground. Then nine sub-views of the liver are extracted in the ROI through overlapped sliding windows. The multi-view learning framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Our framework mainly consists of three parts, i.e., evidential network, subjective logic, and combination rule. The evidential networks encode local views and the whole ROI as global view to evidence vectors e. For local views, the networks are implemented with the convolutional structure. While for the global view, a data-efficient vision transformer with shifted patch tokenization (SPT) and locality self-attention (LSA) strategy is applied. Subjective logic serves as a principle that transforms the vector e into the parameter α of the Dirichlet distribution of classification predictions, and the opinion D with uncertainty u. Then, Dempster's combination rule is applied to form the final opinion with overall uncertainty, which can be transformed into the final prediction. The details of subjective logic, Dempster's combination rule, the data-efficient transformer, and the training paradigm are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subjective Logic for Uncertainty Estimation</head><p>Subjective logic, as a generalization of the Bayesian theory, is a principled method of probabilistic reasoning under uncertainty <ref type="bibr" target="#b9">[10]</ref>. It serves as the guideline of the estimation of both uncertainty and distribution of predicted probabilities in our framework. Given an image</p><formula xml:id="formula_0">x k from view k, k ∈ {1, 2, • • • , K}, the evidence vector e k = [e k</formula><p>1 , e k 2 , ..., e k C ] with non-negative elements for C classes is estimated through the evidential network, which is implemented using a classification network with softplus activation for the output.</p><p>According to subjective logic, the Dirichlet distribution of class probabilities Dir(p k |α k ) is determined by the evidence. For simplicity, we follow <ref type="bibr" target="#b16">[17]</ref> and derive the parameter of the distribution by α k = e k + 1. Then the Dirichlet distribution is mapped to an opinion</p><formula xml:id="formula_1">D k = {{b k c } C c=1 , u k }, subject to u k + C c=1 b k c = 1,<label>(1)</label></formula><p>where and predicted probabilities pk can be derived in an end-to-end manner.</p><formula xml:id="formula_2">b k c = α k c -1 S k is the belief mass for class c, S k = C c=1 α k c is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combination Rule</head><p>Based on opinions derived from each view, Dempster's combination rule <ref type="bibr" target="#b10">[11]</ref> is applied to obtain the overall opinion with uncertainty, which could be converted to the distribution of the final prediction. Specifically, given opinions</p><formula xml:id="formula_3">D 1 = {{b 1 c } C c=1 , u 1 } and D 2 = {{b 2 c } C c=1 , u 2 }, the combined opinion D = {{b c } C c=1 , u} = D 1 ⊕ D 2 is derived by the following rule, b c = 1 N (b 1 c b 2 c + b 1 c u 2 + b 2 c u 1 ), u = 1 N u 1 u 2 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where N = 1i =j b 1 i b 2 j is the normalization factor. According to Eq. ( <ref type="formula" target="#formula_3">2</ref>), the combination rule indicates that the combined belief b c depends more on the opinion which is confident (with small u). In terms of uncertainty, the combined u is small when at least one opinion is confident.</p><p>For opinions from K local views and one global view, the combined opinion could be derived by applying the above rule for K times, i.e., D = D 1 </p><formula xml:id="formula_5">⊕ • • • ⊕ D K ⊕ D Global .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Global Representation Modeling</head><p>To capture the global representation, we apply a data-efficient transformer as the evidential network for the global view. We follow <ref type="bibr" target="#b12">[13]</ref> and improve the performance of the transformer on small datasets by increasing locality inductive bias, i.e., the assumption about relations between adjacent pixels. The standard vision transformer (ViT) <ref type="bibr" target="#b2">[3]</ref> without such assumptions typically require more training data than convolutional networks <ref type="bibr" target="#b14">[15]</ref>. Therefore, we adopt the SPT and LSA strategy to improve the locality inductive bias.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, SPT is different from the standard tokenization in that the input image is shifted in four diagonal directions by half the patch size, and the shifted images are concatenated with the original images in the channel dimension to further utilize spatial relations between neighboring pixels. Then, the concatenated images are partitioned into patches and linearly projected as visual tokens in the same way as ViT.</p><p>LSA modifies self-attention in ViT by sharpening the distribution of the attention map to pay more attention to important visual tokens. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, diagonal masking and temperature scaling are performed before applying softmax to the attention map. Given the input feature X, The LSA module is formalized as,</p><formula xml:id="formula_6">L(X) = softmax(M(qk T )/τ )v,<label>(3)</label></formula><p>where q, k, v are the query, key, and value vectors obtained by linear projections of X. M is the diagonal masking operator that sets the diagonal elements of qk T to a small number (e.g.,-∞). τ ∈ R is the learnable scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Paradigm</head><p>Theoretically, the proposed framework could be trained in an end-to-end manner.</p><p>For each view k, we use the integrated cross-entropy loss as in <ref type="bibr" target="#b16">[17]</ref>,</p><formula xml:id="formula_7">L k ice = E p k ∼Dir(p k |α k ) [L CE (p k , y k )] = C c=1 y k c (ψ(S k ) -ψ(α k c )), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ψ is the digamma function and y k is the one-hot label. We also apply a regularization term to increase the uncertainty of misclassified samples,</p><formula xml:id="formula_9">L k = L k ice + λKL[Dir(p k | αk )||Dir(p k |1)], (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where λ is the balance factor which gradually increases during training and αk = y k + (1 -y k ) α k . The overall loss is the summation of losses from all views and the loss for the combined opinion,</p><formula xml:id="formula_11">L Overall = L Combined + L Global + K k=1 L k , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where L Combined and L Global are losses of the combined and global opinions, implemented in the same way as L k . In practice, we pre-train the evidential networks before training with Eq. ( <ref type="formula" target="#formula_11">6</ref>). For local views, we use the model weights pre-trained on ImageNet, and the transformer is pre-trained on the global view images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The proposed method was evaluated on Gd-EOB-DTPA-enhanced <ref type="bibr" target="#b24">[25]</ref> hepatobiliary phase MRI data, including 342 patients acquired from two scanners, i.e., Siemens 1.5T and Siemens 3.0T. The gold standard was obtained through the pathological analysis of the liver biopsy or liver resection within 3 months before and after MRI scans. Please refer to supplementary materials for more data acquisition details. Among all patients, 88 individuals were identified with fibrosis stage S1, 41 with S2, 40 with S3, and 174 with the most advanced stage S4. Following <ref type="bibr" target="#b24">[25]</ref>, the slices with the largest liver area in images were selected. The data were then preprocessed with z-score normalization, resampled to a resolution of 1.5 × 1.5 mm 2 , and cropped to 256 × 256 pixel. For multi-view extraction, the size of the ROI, window, and stride were 160, 96, 32, respectively. For all experiments, a four-fold cross-validation strategy was employed, and results of two tasks with clinical significance <ref type="bibr" target="#b24">[25]</ref> were evaluated, i.e., staging cirrhosis (S4 vs S1-3) and identifying substantial fibrosis (S1 vs S2-4). To keep a balanced number of samples for each class, we over-sampled the S1 data and under-sampled S4 data in the experiments of staging substantial fibrosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Augmentations such as random rescale, flip, and cutout <ref type="bibr" target="#b1">[2]</ref> were applied during training. We chose ResNet34 as the evidential network for local views. For configurations of the transformer, please refer to supplementary materials. The framework was trained using Adam optimizer with an initial learning rate of 1e -4 for 500 epochs, which was decreased by using the polynomial scheduler. The balance factor λ was set to increase linearly from 0 to 1 during training.</p><p>The transformer network was pre-trained for 200 epochs using the same setting.</p><p>The framework was implemented using Pytorch and was run on one Nvidia RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparison with Multi-view Learning Methods. To assess the effectiveness of the proposed multi-view learning framework for liver fibrosis staging, we compared it with five multi-view learning methods, including Concat <ref type="bibr" target="#b4">[5]</ref>, DCCAE <ref type="bibr" target="#b21">[22]</ref>, CMC <ref type="bibr" target="#b20">[21]</ref>, PredSum <ref type="bibr" target="#b17">[18]</ref>, and Attention <ref type="bibr" target="#b8">[9]</ref>. Concat is a commonly used method that concatenates multi-view images at the input level. DCCAE and CMC are feature-level strategies. PredSum and Attention are based on decision-level fusion. Additionally, SingleView <ref type="bibr" target="#b7">[8]</ref> was adopted as the baseline method for liver fibrosis staging, which uses a single patch as input.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, our method outperformed the SingleView method by 10.3% and 12% in AUC on the two tasks, respectively, indicating that the proposed method could exploit more informative features than the method using single view. Our method also set the new state of the art, when compared with other multi-view learning methods. This could be due to the fact that our method was able to capture both the global and local features, and the uncertainty-aware fusion strategy could be more robust than the methods with implicit fusion strategies.</p><p>Comparison with Uncertainty-Aware Methods. To demonstrate reliability, we compared the proposed method with other methods. Specifically, these methods estimate uncertainty using Monte-Carlo dropout (Dropout) <ref type="bibr" target="#b19">[20]</ref>, variational inference (VI) <ref type="bibr" target="#b18">[19]</ref>, ensemble <ref type="bibr" target="#b3">[4]</ref>, and softmax entropy <ref type="bibr" target="#b15">[16]</ref>, respectively. Following <ref type="bibr" target="#b5">[6]</ref>, we evaluated the expected calibration error (ECE), which measures the gap between model confidence and expected accuracy.  Table <ref type="table" target="#tab_1">2</ref> shows that our method achieved better results in ACC and AUC for both tasks than the other uncertainty-ware multi-view learning methods. It indicates that the uncertainty in our framework could paint a clearer picture of the reliability of each view, and thus the final prediction was more accurate based on the proposed scheme of rule-based combination. Our method also achieved the lowest ECE, indicating that the correspondence between the model confidence and overall results was more accurate.</p><p>Interpretability. The proposed framework could explain which view of the input image contains more decisive information for liver fibrosis staging through uncertainties. To evaluate the quality of explanations, we compared the estimated uncertainties with annotations from experienced physicians. Views that contain more signs of fibrosis are supposed to have lower uncertainties. According to Fig. <ref type="figure" target="#fig_3">3</ref>, the predicted uncertainties are consistent with annotations in local views of the S4 sample (a). In the S1 sample (b), the uncertainty of global view is low. It is reasonable since there are no visible signs of fibrosis in this stage. The model needs to capture the entire view to discriminate the S1 sample. Ablation Study. We performed this ablation study to investigate the roles of local views and global view, as well as to validate the effectiveness of the data-efficient transformer. Table <ref type="table" target="#tab_2">3</ref> shows that using the global view solely achieved the worst performance in the staging of cirrhosis. This means that it could be difficult to extract useful features without complementary information from local views. This is consistent with Fig. <ref type="figure" target="#fig_3">3(a)</ref>, where the uncertainty derived from the global view is high, even if there are many signs of fibrosis. While in Fig. <ref type="figure" target="#fig_3">3</ref>(b), the uncertainty of the global view is low, which indicates that it is easier to make decisions from the global view when there is no visible sign of fibrosis. Therefore, we concluded that the global view was more valuable in identifying substantial fibrosis. Compared with the method that only used local views, our method gained more improvement in the substantial fibrosis identification task, which further confirms the aforementioned conclusion. Our method also performed better than the method that applied a convolution neural network (CNN) for the global view. This demonstrates that the proposed data-efficient transformer was more suitable for the modeling of global representation than CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we have proposed a reliable and interpretable multi-view learning framework for liver fibrosis staging. Specifically, uncertainty is estimated through subjective logic to improve reliability, and an explicit fusion strategy is applied which promotes interpretability. Furthermore, we use a data-efficient transformer to model the global representation, which improves the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The road map of this work. (b) The pipeline to extract sub-views of the liver. First, the foreground is extracted using intensity-based segmentation. Based on the segmentation, a square region of interest (ROI) centered at the centroid of the liver is cropped. Then overlapped sliding windows are used in the ROI to obtain nine sub-views of the liver.</figDesc><graphic coords="2,91,47,434,93,269,32,99,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The left side shows the main framework. Multi-view images are first encoded as evidence vectors by evidential networks. For each view, an opinion with uncertainty u is derived from evidence, under the guidance of subjective logic. Finally, the opinions are combined based on an explicit rule to derive the overall opinion, which can be converted to the distribution of classification probabilities. The right side illustrates the SPT and LSA modules in the data-efficient transformer that serves as the evidential network for the global view.</figDesc><graphic coords="3,44,79,53,96,334,51,134,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the Dirichlet strength, and u k = C S k indicates the uncertainty. The predicted probabilities pk ∈ R C of all classes are the expectation of Dirichlet distribution, i.e., pk = E Dir(p k |α k ) [p k ]. Therefore, the uncertainty u k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Typical samples of stage 4 (a) and stage 1 (b). Visible signs of liver fibrosis are highlighted by circles. Yellow circles indicate the nodular surface contour and green circles denote numerous regenerative nodules. Uncertainties (U) of local and global views estimated by our model were demonstrated. Notably, local views of lower uncertainty contain more signs of fibrosis. Please refer to supplementary materials for more high-resolute images (Color figure online)</figDesc><graphic coords="8,57,96,188,57,336,16,92,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with multi-view learning methods. Results are evaluated in accuracy (ACC) and area under the receiver operating characteristic curve (AUC) for both tasks.</figDesc><table><row><cell>Method</cell><cell cols="2">Cirrhosis(S4 vs S1-3)</cell><cell cols="2">Substantial Fibrosis(S1 vs S2-4)</cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell cols="5">SingleView [8] 77.1 ± 3.17 78.7 ± 4.17 78.2 ± 7.18 75.0 ± 11.5</cell></row><row><cell>Concat [5]</cell><cell cols="4">80.0 ± 2.49 81.8 ± 3.17 80.5 ± 2.52 83.3 ± 3.65</cell></row><row><cell cols="5">DCCAE [22] 80.6 ± 3.17 82.7 ± 4.03 83.1 ± 5.30 84.5 ± 4.77</cell></row><row><cell>CMC [21]</cell><cell cols="4">80.6 ± 1.95 83.5 ± 3.67 83.4 ± 3.22 85.3 ± 4.06</cell></row><row><cell cols="5">PredSum [18] 78.8 ± 4.16 78.2 ± 4.94 81.1 ± 2.65 84.9 ± 3.21</cell></row><row><cell cols="5">Attention [9] 76.2 ± 0.98 78.9 ± 3.72 81.4 ± 4.27 84.4 ± 5.34</cell></row><row><cell>Ours</cell><cell cols="4">84.4 ± 1.74 89.0 ± 0.03 85.5 ± 1.91 88.4 ± 1.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with uncertainty-aware methods. The expected calibration error (ECE) is evaluated in addition to ACC and AUC. Methods with lower ECE are more reliable.</figDesc><table><row><cell>Method</cell><cell cols="2">Cirrhosis(S4 vs S1-3)</cell><cell></cell><cell cols="3">Substantial Fibrosis(S1 vs S2-4)</cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ECE</cell><cell>ACC</cell><cell>AUC</cell><cell>ECE</cell></row><row><cell>Softmax</cell><cell cols="3">77.1 ± 3.17 78.7 ± 4.17 0.256 ± 0.040</cell><cell cols="3">78.2 ± 7.18 83.3 ± 3.65 0.237 ± 0.065</cell></row><row><cell cols="4">Dropout [20] 77.1 ± 4.89 79.8 ± 4.50 0.183 ± 0.063</cell><cell cols="3">80.2 ± 5.00 83.8 ± 6.12 0.171 ± 0.067</cell></row><row><cell>VI [19]</cell><cell cols="3">77.6 ± 2.20 79.5 ± 4.50 0.229 ± 0.020</cell><cell cols="3">81.1 ± 2.08 82.2 ± 6.12 0.191 ± 0.023</cell></row><row><cell cols="4">Ensemble [4] 78.1 ± 1.91 80.8 ± 3.13 0.181 ± 0.040</cell><cell>79.3 ± 5.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for the roles of local and global views, and effectiveness of the data-efficient transformer.</figDesc><table><row><cell>Method</cell><cell cols="2">Cirrhosis(S4 vs S1-3)</cell><cell></cell><cell cols="3">Substantial Fibrosis(S1 vs S2-4)</cell></row><row><cell></cell><cell>ACC</cell><cell>AUC</cell><cell>ECE</cell><cell>ACC</cell><cell>AUC</cell><cell>ECE</cell></row><row><cell cols="4">Global View solely 76.8 ± 2.81 79.4 ± 4.76 0.192 ± 0.071</cell><cell cols="3">82.4 ± 3.45 84.9 ± 5.42 0.192 ± 0.071</cell></row><row><cell cols="7">Local Views solely 84.1 ± 6.47 88.0 ± 8.39 0.148 ± 0.086 82.0 ± 6.07 86.9 ± 6.68 0.180 ± 0.060</cell></row><row><cell cols="4">Both views by CNN 82.9 ± 3.17 87.8 ± 3.09 0.171 ± 0.029</cell><cell cols="3">82.0 ± 3.54 87.1 ± 3.47 0.174 ± 0.039</cell></row><row><cell>Ours</cell><cell cols="3">84.4 ± 1.74 89.0 ± 0.03 0.154 ± 0.028</cell><cell cols="3">85.5 ± 1.91 88.4 ± 1.84 0.156 ± 0.019</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p><rs type="person">Z. Gao</rs> and <rs type="person">Y. Liu-These</rs> two authors contribute equally. X. Zhuang-This work was funded by the <rs type="funder">National Natural Science Foundation of China</rs> (grant No. <rs type="grantNumber">61971142</rs> and <rs type="grantNumber">62111530195</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eu36hSe">
					<idno type="grant-number">61971142</idno>
				</org>
				<org type="funding" xml:id="_kysakRp">
					<idno type="grant-number">62111530195</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 18.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Masksembles for uncertainty estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Durasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13539" to="13548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trusted multi-view classification with dynamic evidential fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2551" to="2566" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully automated prediction of liver fibrosis using deep learning analysis of gadoxetic acid-enhanced MRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hectors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3805" to="3814" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jøsang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-42337-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-42337-1" />
		<title level="m">Subjective Logic</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpretation and fusion of hyper opinions in subjective logic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jøsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hankin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1225" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep probabilistic canonical correlation analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8055" to="8063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13492</idno>
		<title level="m">Vision transformer for small-size datasets</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Late fusion incomplete multi-view clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2410" to="2423" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards learning convolutions from scratch</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8078" to="8088" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04972</idno>
		<title level="m">Understanding softmax confidence and uncertainty</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uncertainty-aware audiovisual activity recognition using deep Bayesian variational inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Subedar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6301" to="6310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uno: uncertainty-aware noisy-or multimodal fusion for unanticipated input degradation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5716" to="5723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58621-8_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58621-845" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12356</biblScope>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep multi-view learning methods: a review</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">448</biblScope>
			<biblScope unit="page" from="106" to="129" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning for staging liver fibrosis on CT: a pilot study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kunimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="4578" to="4585" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Liver fibrosis: deep convolutional neural network for staging by using gadoxetic acid-enhanced hepatobiliary phase mr images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kunimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="155" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on neural network interpretability</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tiňo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Top. Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
