<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
							<email>jianpeng.zhang0@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianghua</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The First Affiliated Hospital of College of Medicine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxing</forename><surname>Tang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minfeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Provincial People&apos;s Hospital</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaiyi</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Provincial People&apos;s Hospital</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Le Lu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9A2D05A74F6BEC211371707AF7A16D1</idno>
					<idno type="DOI">10.1007/978-3-031-43904-920.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lung cancer is a leading cause of death worldwide and early screening is critical for improving survival outcomes. In clinical practice, the contextual structure of nodules and the accumulated experience of radiologists are the two core elements related to the accuracy of identification of benign and malignant nodules. Contextual information provides comprehensive information about nodules such as location, shape, and peripheral vessels, and experienced radiologists can search for clues from previous cases as a reference to enrich the basis of decision-making. In this paper, we propose a radiologist-inspired method to simulate the diagnostic process of radiologists, which is composed of context parsing and prototype recalling modules. The context parsing module first segments the context structure of nodules and then aggregates contextual information for a more comprehensive understanding of the nodule. The prototype recalling module utilizes prototype-based learning to condense previously learned cases as prototypes for comparative analysis, which is updated online in a momentum way during training. Building on the two modules, our method leverages both the intrinsic characteristics of the nodules and the external knowledge accumulated from other nodules to achieve a sound diagnosis. To meet the needs of both lowdose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology-or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lung cancer screening has a significant impact on the rate of mortality associated with lung cancer. Studies have proven that regular lung cancer screening with low-dose computed tomography (LDCT) can lessen the rate of lung cancer mortality by up to 20% <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. As most (e.g., 95% <ref type="bibr" target="#b12">[13]</ref>) of the detected nodules are benign, it is critical to accurately assess their malignancy on CT to achieve a timely diagnosis of malignant nodules and avoid unnecessary procedures such as biopsy for benign ones. Particularly, the evaluation of nodule (i.e., 8-30mm) malignancy is recommended in the guidelines <ref type="bibr" target="#b12">[13]</ref>.</p><p>Fig. <ref type="figure">1</ref>. In PARE, a nodule is diagnosed from two levels: first parsing the contextual information contained in the nodule itself, and then recalling the previously learned nodules to look for related clues.</p><p>One of the major challenges of lung nodule malignancy prediction is the quality of datasets <ref type="bibr" target="#b5">[6]</ref>. It is characterized by a lack of standard-oftruth of labels for malignancy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, and due to this limitation, many studies use radiologists' subjective judgment on CT as labels, such as LIDC-IDRI <ref type="bibr" target="#b2">[3]</ref>. Recent works have focused on collecting pathologically labeled data to develop reliable malignancy prediction models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. For example, Shao et al. <ref type="bibr" target="#b15">[16]</ref> collated a pathological gold standard dataset of 990 CT scans. Another issue is most of the studies focus on LDCT for malignancy prediction <ref type="bibr" target="#b9">[10]</ref>. However, the majority of lung nodules are incidentally detected by routine imaging other than LDCT <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, such as noncontrast chest CT (NCCT, the most frequently performed CT exam, nearly 40% <ref type="bibr" target="#b17">[18]</ref>).</p><p>Technically, current studies on lung nodule malignancy prediction mainly focus on deep learning-based techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Liao et al. <ref type="bibr" target="#b9">[10]</ref> trained a 3D region proposal network to detect suspicious nodules and then selected the top five to predict the probability of lung cancer for the whole CT scan, instead of each nodule. To achieve the nodule-level prediction, Xie et al. <ref type="bibr" target="#b23">[24]</ref> introduced a knowledge-based collaborative model that hierarchically ensembles multi-view predictions at the decision level for each nodule. Liu et al. <ref type="bibr" target="#b11">[12]</ref> extracted both nodules' and contextual features and fused them for malignancy prediction. Shi et al. <ref type="bibr" target="#b16">[17]</ref> effectively improved the malignancy prediction accuracy by using a transfer learning and semi-supervised strategy. Despite their advantages in representation learning, these methods do not take into account expert diagnostic knowledge and experience, which may lead to a bad consequence of poor generalization. We believe a robust algorithm should be closely related to the diagnosis experience of professionals, working like a radiologist rather than a black box.</p><p>In this paper, we suggest mimicking radiologists' diagnostic procedures from intra-context parsing and inter-nodule recalling (see illustrations in Fig. <ref type="figure">1</ref>), </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>abbreviated as PARE.</head><p>At the intra-level, the contextual information of the nodules provides clues about their shape, size, and surroundings, and the integration of this information can facilitate a more reliable diagnosis of whether they are benign or malignant. Motivated by this, we first segment the context structure, i.e., nodule and its surroundings, and then aggregate the context information to the nodule representation via the attention-based dependency modeling, allowing for a more comprehensive understanding of the nodule itself. At the inter-level, we hypothesize that the diagnosis process does not have to rely solely on the current nodule itself, but can also find clues from past learned cases. This is similar to how radiologists rely on their accumulated experience in clinical practice. Thus, the model is expected to have the ability to store and recall knowledge, i.e., the knowledge learned can be recorded in time and then recalled as a reference for comparative analysis. To achieve this, we condense the learned nodule knowledge in the form of prototypes, and recall them to explore potential inter-level clues as an additional discriminant criterion for the new case. To fulfill both LDCT and NCCT screening needs, we curate a large-scale lung nodule dataset with pathology-or follow-up-confirmed benign/malignant labels. For the LDCT, we annotate more than 12,852 nodules from 8,271 patients from the NLST dataset <ref type="bibr" target="#b13">[14]</ref>. For the NCCT, we annotate over 4,029 nodules from over 2,565 patients from our collaborating hospital. Experimental results on several datasets demonstrate that our method achieves outstanding performance on both LDCT and NCCT screening scenarios.</p><p>Our contributions are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose context parsing to extract and aggregate rich contextual information for each nodule. (2) We condense the diagnostic knowledge from the learned nodules into the prototypes and use them as a reference to assist in diagnosing new nodules. (3) We curate the largest-scale lung nodule dataset with high-quality benign/malignant labels to fulfill both LDCT and NCCT screening needs. (4) Our method achieves advanced malignancy prediction performance in both screening scenarios (0.931 AUC), and exhibits strong generalization in external validation, setting a new state of the art on LUNGx (0.801 AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">2</ref> illustrates the overall architecture of PARE, which consists of three stages: context segmentation, intra context parsing, and inter prototype recalling. We now delve into different stages in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Segmentation</head><p>The nodule context information has an important effect on the benign and malignant diagnosis. For example, a nodule associated with vessel feeding is more likely to be malignant than a solitary one <ref type="bibr" target="#b21">[22]</ref>. Therefore, we use a U-like network (UNet) to parse the semantic mask m for the input image patch x, thus allowing subsequent context modeling of both the nodule and its surrounding structures. Specifically, each voxel of m belongs to {0 : background, 1 : lung, 2 : nodule, 3 : vessel, 4 : trachea}. This segmentation process allows PARE to gather comprehensive context information that is crucial for an accurate diagnosis. For the diagnosis purpose, we extract the global feature from the bottleneck of UNet as the nodule embedding q, which will be used in later diagnostic stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intra Context Parse</head><p>In this stage, we attempt to enhance the discriminative representations of nodules by aggregating contextual information produced by the segmentation model. Specifically, the context mask is tokenized into a set of sequences via the overlapped patch embedding. The input image is also split into patches and then embedded into the context tokens to keep the original image information. Besides, positional encoding is added in a learnable manner to retain location information. Similar to the class token in ViT <ref type="bibr" target="#b6">[7]</ref>, we prepend the nodule embedding token to the context sequences, denoted by [q; t 1 , ..., t g ] ∈ R (g+1)×D . Here g is the number of context tokens, and D represents the embedding dimension. Then we perform the self-attention modeling on these tokens simultaneously, called Self Context Attention (SCA), to aggregate context information into the nodule embedding. The nodule embedding token at the output of the last SCA block serves as the updated nodule representation. We believe that explicitly modeling the dependency between nodule embedding and its contextual structure can lead to the evolution of more discriminative representations, thereby improving discrimination between benign and malignant nodules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inter Prototype Recall</head><p>Definition of the Prototype: To retain previously acquired knowledge, a more efficient approach is needed instead of storing all learned nodules in memory, which leads to a waste of storage and computing resources. To simplify this process, we suggest condensing these pertinent nodules into a form of prototypes. As for a group of nodules, we cluster them into N groups {C 1 , ..., C N } by minimizing the objective function N i=1 p∈Ci d(p, P i ) where d is the Euclidean distance function and p represents the nodule embedding, and refer the center of each cluster, P i = 1 |Ci| p∈Ci p, as its prototype. Considering the differences between benign and malignant nodules, we deliberately divide the prototypes into benign and malignant groups, denoted by P B ∈ R N/2×D and P M ∈ R N/2×D . Cross Prototype Attention: In addition to parsing intra context, we also encourage the model to capture inter-level dependencies between nodules and external prototypes. This enables PARE to explore relevant identification basis beyond individual nodules. To accomplish this, we develop a Cross-Prototype Attention (CPA) module that utilizes nodule embedding as the query and the prototypes as the key and value. It allows the nodule embedding to selectively attend to the most relevant parts of prototype sequences. The state of query at the output of the last CPA module servers as the final nodule representation to predict its malignancy label, either "benign" (y = 0) or "malignant" (y = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Updating Prototype Online:</head><p>The prototypes are updated in an online manner, thereby allowing them to adjust quickly to changes in the nodule representations. As for the nodule embedding q of the data (x, y), its nearest prototype is singled out and then updated by the following momentum rules,</p><formula xml:id="formula_0">P B arg min j d(q,P B j ) = λ • P B arg min j d(q,P B j ) + (1 -λ) • q if y = 0 P M arg min j d(q,P M j ) = λ • P M arg min j d(q,P M j ) + (1 -λ) • q otherwise (1)</formula><p>where λ is the momentum factor, set to 0.95 by default. The momentum updating can help accelerate the convergence and improve the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Process of PARE</head><p>The Algorithm 1 outlines the training process of our PARE model which is based on two objectives: segmentation and classification. The Dice and crossentropy loss are combined for segmentation, while cross-entropy loss is used for classification. Additionally, deep classification supervision is utilized to enhance the representation of nodule embedding in shallow layers like the output of the UNet and SCA modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>Data Collection and Curation: NLST is the first large-scale LDCT dataset for low-dose CT lung cancer screening purpose <ref type="bibr" target="#b13">[14]</ref>. There are 8,271 patients enrolled in this study. An experienced radiologist chose the last CT scan of each for l = 1, ..., L do 12:</p><formula xml:id="formula_1">z l ← CPA(LN(z l-1 )) + z l-1</formula><p>Cross prototype attention 13:</p><formula xml:id="formula_2">z l ← MLP(LN(z l )) + z l 14:</formula><p>end for <ref type="bibr" target="#b14">15</ref>:</p><formula xml:id="formula_3">p 1 ← MLP(z 0 L ) Classification head</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Update prototype according to Eq. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>J ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) Update loss</p><p>18: end for patient, and localized and labeled the nodules in the scan as benign or malignant based on the rough candidate nodule location and whether the patient develops lung cancer provided by NLST metadata. The nodules with a diameter smaller than 4mm were excluded. The in-house cohort was retrospectively collected from 2,565 patients at our collaborating hospital between 2019 and 2022. Unlike NLST, this dataset is noncontrast chest CT, which is used for routine clinical care. Segmentation annotation: We provide the segmentation mask for our in-house data, but not for the NLST data considering its high cost of pixel-level labeling. The nodule mask of each in-house data was manually annotated with the assistance of CT labeler <ref type="bibr" target="#b19">[20]</ref> by our radiologists, while other contextual masks such as lung, vessel, and trachea were generated using the TotalSegmentator <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train-Val-Test:</head><p>The training set contains 9,910 (9,413 benign and 497 malignant) nodules from 6,366 patients at NLST, and 2,592 (843 benign and 1,749 malignant) nodules from 2,113 patients at the in-house cohort. The validation set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients at NLST. The NLST test set has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. The in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452 patients. We additionally evaluate our method on the LUNGx <ref type="bibr" target="#b1">[2]</ref> challenge dataset, which is usually used for external validation in previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. LUNGx contains 83 (42 benign and 41 malignant) nodules, part of which (13 scans) were contrast-enhanced. Segmentation: We also evaluate the segmentation performance of our method on the public nodule segmentation dataset LIDC-IDRI <ref type="bibr" target="#b2">[3]</ref>, which has 2,630 nodules with nodule segmentation mask. Evaluation metrics: The area under the receiver operating characteristic curve (AUC) is used to evaluate the malignancy prediction performance.</p><p>Implementation: All experiments in this work were implemented based on the nnUnet framework <ref type="bibr" target="#b7">[8]</ref>, with the input size of 32 × 48 × 48, batch size of 64, and total training iterations of 10K. In the context patch embedding, each patch token is generated from a window of 8 × 8 × 8. The hyper-parameters of PARE are empirically set based on the ablation experiments on the validation set. For example, the Transformer layer is set to 4 in both SCA and CPA modules, and the number of prototypes is fixed to 40 by default. More details can be found in the ablation. Due to the lack of manual annotation of nodule masks for the NLST dataset, we can only optimize the segmentation task using our in-house dataset, which has manual nodule masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Results</head><p>Ablation Study: In Table <ref type="table" target="#tab_0">1</ref>, we investigate the impact of different configurations on the performance of PARE on the validation set, including Transformer layers, number of prototypes, embedding dimension, and deep supervision. We observe that a higher AUC score can be obtained by increasing the number of Transformer layers, increasing the number of prototypes, doubling the channel dimension of token embeddings, or using deep classification supervision. Based on the highest AUC score of 0.931, we empirically set L=4, N=40, D=256, and DS=True in the following experiments. In Table <ref type="table" target="#tab_1">2</ref>, we investigate the ablation study of different methods/modules on the validation set and observe the following results: (1) The pure segmentation method performs better than the pure classification method, primarily because it enables greater supervision at the pixel level, (2) the joint segmentation and classification is superior to any single method, indicating the complementary effect of both tasks, (3) both context parsing and prototype comparing contribute to improved performance on the strong baseline, demonstrating the effectiveness of both modules, and (4) segmenting more contextual structures such as vessels, lungs, and trachea provide a slight improvement, compared to solely segmenting nodules.  External Evaluation on LUNGx: We used LUNGx <ref type="bibr" target="#b1">[2]</ref> as an external test to evaluate the generalization of PARE. It is worth noting that these compared methods have never been trained on LUNGx. Table <ref type="table" target="#tab_4">4</ref> shows that our PARE model achieves the highest AUC of 0.801, which is 2% higher than the best method DAR <ref type="bibr" target="#b10">[11]</ref>. We also conducted a reader study to compare PARE with two experienced radiologists, who have 8 and 13 years of lung nodule diagnosis experience respectively. Results in Fig. <ref type="figure">3</ref> reveal that our method achieves performance comparable to that of radiologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization on LDCT and NCCT:</head><p>Our model is trained on a mix of LDCT and NCCT datasets, which can perform robustly across low-dose and regular-dose applications. We compare the generalization performance of the models obtained under three training data configurations (LDCT, NCCT, and a combination of them). We find that the models trained on either LDCT or NCCT dataset alone cannot generalize well to other modalities, with at least a 6% AUC drop. However, our mixed training approach performs best on both LDCT and NCCT with almost no performance degradation. Method AUC NLNL <ref type="bibr" target="#b8">[9]</ref> 0.683 D2CNN <ref type="bibr" target="#b25">[26]</ref> 0.746 KBC <ref type="bibr" target="#b23">[24]</ref> 0.768 DAR <ref type="bibr" target="#b10">[11]</ref> 0.781 PARE (Ours) 0.801 Fig. <ref type="figure">3</ref>. Reader study compared with AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose the PARE model to mimic the radiologists' diagnostic procedures for accurate lung nodule malignancy prediction. Concretely, we achieve this purpose by parsing the contextual information from the nodule itself and recalling the previous diagnostic knowledge to explore related benign or malignancy clues. Besides, we curate a large-scale pathological-confirmed dataset with up to 13,000 nodules to fulfill the needs of both LDCT and NCCT screening scenarios. With the support of a high-quality dataset, our PARE achieves outstanding malignancy prediction performance in both scenarios and demonstrates a strong generalization ability on the external validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview architecture of our proposed PARE model.</figDesc><graphic coords="3,58,47,59,39,335,08,95,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 . 1 : 7 :</head><label>117</label><figDesc>Training process of PARE model. for iteration = 1, 2, ... do 2: {x, m, y} ← Sample(D)z l ← SCA(LN(z l-1 )) + z l-1self context attention8:z l ← MLP(LN(z l )) + z l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation</figDesc><table><row><cell></cell><cell>comparison of hyper-</cell></row><row><cell cols="2">parameters (Transformer layers (L), num-</cell></row><row><cell cols="2">ber of prototypes (N), embedding dimen-</cell></row><row><cell cols="2">sion (D), and deep supervision (DS))</cell></row><row><cell cols="2">L N D DS AUC</cell></row><row><cell>1 20 128</cell><cell>0.912</cell></row><row><cell>2 20 128</cell><cell>0.918</cell></row><row><cell>4 20 128</cell><cell>0.924</cell></row><row><cell>4 10 128</cell><cell>0.920</cell></row><row><cell>4 40 128</cell><cell>0.924</cell></row><row><cell>4 40 256</cell><cell>0.931</cell></row><row><cell cols="2">4 40 256 × 0.926</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Effectiveness</figDesc><table><row><cell></cell><cell>of different mod-</cell></row><row><cell cols="2">ules. MT: multi-task learning. Context:</cell></row><row><cell cols="2">intra context parsing. Prototype: inter</cell></row><row><cell cols="2">prototype recalling. *: only nodule mask</cell></row><row><cell cols="2">was used in the segmentation task.</cell></row><row><cell>Method</cell><cell>AUC</cell></row><row><cell>Pure classification</cell><cell>0.907</cell></row><row><cell>Pure segmentation</cell><cell>0.915</cell></row><row><cell>MT</cell><cell>0.916</cell></row><row><cell>MT+Context*</cell><cell>0.921</cell></row><row><cell>MT+Context</cell><cell>0.924</cell></row><row><cell cols="2">MT+Context+Prototype 0.931</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different methods on both NLST and in-house test sets. † : pure classification; ‡ : pure segmentation; : multi-task learning; *: ensemble of deep supervision heads. Note that we add the segmentation task in CA-Net.</figDesc><table><row><cell>Method</cell><cell cols="2">NLST test set</cell><cell></cell><cell></cell><cell cols="2">In-house test set</cell><cell></cell></row><row><cell></cell><cell cols="4">&lt;10 mm 10∼20 mm &gt;20 mm All</cell><cell cols="4">&lt;10 mm 10∼20 mm &gt;20 mm All</cell></row><row><cell>CNN  †</cell><cell>0.742</cell><cell>0.706</cell><cell>0.780</cell><cell cols="2">0.894 0.851</cell><cell>0.797</cell><cell>0.744</cell><cell>0.901</cell></row><row><cell>ASPP [5]  †</cell><cell>0.798</cell><cell>0.716</cell><cell>0.801</cell><cell cols="2">0.902 0.854</cell><cell>0.788</cell><cell>0.743</cell><cell>0.901</cell></row><row><cell>MiT [25]  †</cell><cell>0.821</cell><cell>0.755</cell><cell>0.810</cell><cell cols="2">0.908 0.858</cell><cell>0.784</cell><cell>0.751</cell><cell>0.904</cell></row><row><cell cols="2">nnUnet [8]  ‡ 0.815</cell><cell>0.736</cell><cell>0.815</cell><cell cols="2">0.910 0.863</cell><cell>0.804</cell><cell>0.750</cell><cell>0.911</cell></row><row><cell cols="2">CA-Net [12] 0.833</cell><cell>0.759</cell><cell>0.807</cell><cell cols="2">0.916 0.878</cell><cell>0.786</cell><cell>0.779</cell><cell>0.918</cell></row><row><cell>PARE</cell><cell>0.882</cell><cell>0.770</cell><cell>0.826</cell><cell cols="2">0.928 0.892</cell><cell>0.817</cell><cell>0.783</cell><cell>0.927</cell></row><row><cell>PARE *</cell><cell>0.890</cell><cell>0.781</cell><cell>0.827</cell><cell cols="2">0.931 0.899</cell><cell>0.821</cell><cell>0.780</cell><cell>0.931</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Comparison to Other Methods on Both Screening Scenarios:</head><label></label><figDesc>Table3presents a comparison of PARE with other advanced methods, including pure classification-based, pure segmentation-based, and multi-task-based methods. Stratification assessments were made in both test sets based on the nodule size distribution. The results indicate that the segmentation-based method outperforms pure classification methods, mainly due to its superior ability to segment contextual structures. Additionally, the multi-task-based CA-Net outperforms any single-task method. Our PARE method surpasses all other methods on both NLST and In-house test sets. Moreover, by utilizing the ensemble of multiple deep supervision heads, the overall AUC is further improved to 0.931 on both datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison to other competitive methods on LUNGx<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="954" to="961" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LUNGx challenge for computerized lung nodule classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44506</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bidaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Artificial intelligence in cancer imaging: clinical challenges and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="157" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CIRDataset: a large-scale dataset for clinicallyinterpretable lung nodule radiomics and malignancy prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadeem</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-92" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NLNL: negative learning for noisy labels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluate the malignancy of pulmonary nodules using the 3D deep leaky noisy-or network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3484" to="3495" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from ambiguous labels for lung nodule malignancy prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CA-Net: leveraging contextual features for lung cancer prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_3</idno>
		<idno>978- 3-030-87240-3 3</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating the patient with a pulmonary nodule: a review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Mazzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="273" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reduced lung-cancer mortality with low-dose computed tomographic screening</title>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="395" to="409" />
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>National Lung Screening Trial Research Team</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lung cancer diagnosed through screening, lung nodule, and neither program: a prospective observational study of the detecting early lung cancer (DELUGE) in the mississippi delta cohort</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">U</forename><surname>Osarogiagbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2094</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIDP: a lung image dataset with pathological information for lung cancer screening</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_74</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-874" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised deep transfer learning for benign-malignant diagnosis of pulmonary nodules in chest CT images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="781" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent CT, cumulative radiation exposure, and associated radiation-induced cancer risks from CT of adults</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepLN: a multi-task AI tool to predict the imaging characteristics, malignancy and pathological subtypes in CT-detected pulmonary nodules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">683792</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A cascaded approach for ultraly high performance lesion detection and false positive removal in liver CT scans</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16036</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wasserthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Breit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cyriac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segeroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.05868</idno>
		<title level="m">Totalsegmentator: robust segmentation of 104 anatomical structures in CT images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stratified learning of local anatomical context for lung nodules in CT images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2791" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable multimodel ensemble for benign-malignant lung nodule classification on chest CT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-775" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge-based collaborative deep learning for benign-malignant lung nodule classification on chest CT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="991" to="1004" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UniMiSS: universal medical self-supervised learning via breaking dimensionality barrier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-833" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022. ECCV 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep discriminative CNN with temporal ensembling for ambiguously-labeled image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12669" to="12676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trustworthy learning with (un)sure annotation for lung nodule diagnosis with CT</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102627</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
