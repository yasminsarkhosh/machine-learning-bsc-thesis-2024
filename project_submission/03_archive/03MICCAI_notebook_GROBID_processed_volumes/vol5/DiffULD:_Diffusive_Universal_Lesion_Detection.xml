<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffULD: Diffusive Universal Lesion Detection</title>
				<funder ref="#_R9Yk5dQ">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_CmeVe59">
					<orgName type="full">Open Fund Project of Guangdong Academy of Medical Sciences, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peiang</forename><surname>Zhao</surname></persName>
							<email>pazhao@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Medical Imaging</orgName>
								<orgName type="department" key="dep2">Analytic Computing &amp; Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou</settlement>
									<region>Jiangsu</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Medical Imaging</orgName>
								<orgName type="department" key="dep2">Analytic Computing &amp; Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou</settlement>
									<region>Jiangsu</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiyang</forename><surname>Jin</surname></persName>
							<email>ryjin@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Medical Imaging</orgName>
								<orgName type="department" key="dep2">Analytic Computing &amp; Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou</settlement>
									<region>Jiangsu</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Medical Imaging</orgName>
								<orgName type="department" key="dep2">Analytic Computing &amp; Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou</settlement>
									<region>Jiangsu</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DiffULD: Diffusive Universal Lesion Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="94" to="105"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0520C0EF850411D7F93DA34FF9FBF67E</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Universal Lesion Detection â€¢ Diffusion Model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Universal Lesion Detection (ULD) in computed tomography (CT) plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by anchor-based detection designs, but they have inherent drawbacks due to the use of anchors: (i) Insufficient training target and (ii) Difficulties in anchor design. Diffusion probability models (DPM) have demonstrated outstanding capabilities in many vision tasks. Many DPM-based approaches achieve great success in natural image object detection without using anchors. But they are still ineffective for ULD due to the insufficient training targets. In this paper, we propose a novel ULD method, DiffULD, which utilizes DPM for lesion detection. To tackle the negative effect triggered by insufficient targets, we introduce a novel Center-aligned bounding box (BBox) padding strategy that provides additional high-quality training targets yet avoids significant performance deterioration. DiffULD is inherently advanced in locating lesions with diverse sizes and shapes since it can predict with arbitrary boxes. Experiments on the benchmark dataset DeepLesion <ref type="bibr" target="#b32">[32]</ref> show the superiority of DiffULD when compared to state-of-the-art ULD approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Universal Lesion Detection (ULD) in computed tomography (CT) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44]</ref> plays an important role in computer-aided diagnosis (CAD) <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>. The design of detection-only instead of identifying the lesion types in ULD <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41]</ref> prominently decreases the difficulty of this task for a specific organ (e.g., lung, liver), but it is still challenging for lesions vary in shapes and sizes among whole human body.</p><p>Previous arts in ULD are mainly motivated by the anchor-based detection framework, e.g., Faster-RCNN <ref type="bibr" target="#b21">[21]</ref>. These studies focus on adapting the detection backbone to universally locate lesions in CT scans. For instance, Li et al. <ref type="bibr" target="#b15">[16]</ref> propose the so-called MVP-Net, a multi-view FPN with a position-aware attention mechanism to assist ULD training. Yang et al. <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref> propose a series of 3D feature fusion operators to incorporate context information from several adjacent CT slices for better performance. Li et al. <ref type="bibr" target="#b14">[15]</ref> introduce a plug-and-play transformer block to form hybrid backbones which can better model long-distance feature dependency. While achieving success, these anchor-based methods have inherent drawbacks: (i) Insufficient training target. In stage-1, anchor-based methods identify the positive (lesion) anchors and label them as the region of interest (RoI) based on the IoU between anchors and ground-truth (GT) bounding boxes (BBoxes). An anchor is considered positive if its IoU with any GT BBox is greater than the IoU threshold and negative otherwise <ref type="bibr" target="#b21">[21]</ref>. The positive anchors are sufficient in natural images as they usually have many targets per image <ref type="bibr" target="#b12">[13]</ref>. However, the number of lesions per CT scan is limited, most CT slices only contain one or two lesions (i.e., detection targets in ULD) per CT slice <ref type="bibr" target="#b13">[14]</ref>. Still applying the IoU-based anchor matching mechanism with such limited targets can lead to severe data imbalance and further hinders network convergence. Simply lowering the positive IoU threshold in the anchor-selecting mechanism can alleviate the shortage of positive anchors to some degree, but it leads to a higher false positive (FP) rate by labeling more low-IoU anchors as positive. (ii) Difficulties in anchor design. In anchor-based methods, the size, ratio and number of anchors are pre-defined hyper-parameters that significantly influence the detection performance <ref type="bibr" target="#b26">[26]</ref>. Thus a proper design of anchor hyper-parameters is of great importance. However, tuning anchor hyper-parameters is a challenging task in ULD because of the variety of lesions (target) with diverse diameters (from 0.21 to 342.5 mm). Even with a careful design, the fixed rectangle anchor boxes can be a kind of obstruction in capturing heterogeneous lesions, which have irregular shapes and vary in size.</p><p>To get rid of the drawbacks caused by the anchor mechanism, researchers resort to anchor-free detection, e.g., FCOS <ref type="bibr" target="#b31">[31]</ref> and DETR <ref type="bibr" target="#b4">[5]</ref>. But these methods experience difficulties in achieving state-of-the-art results in ULD, as they lack the initialization of position prior provided by anchors.</p><p>Recently, the diffusion probabilistic model (DPM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">23]</ref> has demonstrated its outstanding capabilities in various vision tasks. Chen et al. follow the key idea of DPM and propose a noise-to-box pipeline, DiffusionDet <ref type="bibr" target="#b6">[7]</ref>, for natural image object detection. They achieved success on natural images with sufficient training targets, but still experience difficulties in dealing with tasks with insufficient training targets like ULD. This is because the DPM's denoising is a dense distribution-to-distribution forecasting procedure that heavily relied on a large number of high-quality training targets to learn targets' distribution accurately.</p><p>To address this issue, we hereby introduce a novel center-aligned BBox padding strategy in DPM detection to form a diffusion-based detector for Universal Lesion Detection, termed DiffULD.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, DiffULD also formulates lesion detection as a denoising diffusion process from noisy boxes to prediction boxes similar to <ref type="bibr" target="#b6">[7]</ref>, but we further introduce the center-aligned BBox padding before DiffULD's forward diffusion process to generate perturbated GT. Specifically, we add random perturbations to both scales and center coordinates of the original GT BBox, resulting in perturbated boxes whose centers remain aligned with the corresponding original GT BBox. Next, original GT boxes paired with these perturbated boxes are called perturbated GT boxes for simplicity. Finally, we feed the perturbated GT boxes to the model as the training objective during training. Compared with other training target padding methods (e.g., padding with random boxes), our strategy can provide additional targets of higher quality, i.e., center aligned with the original GT BBox. This approach effectively expands the insufficient training targets on CT scans, enhancing DPM's detection performance and avoiding deterioration triggered by adding random targets.</p><p>The following DPM training procedure contains two diffusion processes. i) In the forward training process, DiffULD corrupts the perturbated GT with Gaussian noise gradually to generate noisy boxes step by step. Then the model is trained to remove the noise and reconstruct the original perturbated GT boxes. ii) In the reverse inference process, the trained DiffULD can refine a set of randomly generated boxes iteratively to obtain the final detect predictions.</p><p>Our method gets rid of the drawbacks of pre-defined anchors and the deterioration of training DPM with insufficient GT targets. Besides, DiffULD is inherently advanced in locating targets with diverse sizes since it can predict with arbitrary boxes, which is an advantageous feature for detecting lesions of irregular shapes and various sizes. To validate the effectiveness of our method, we conduct experiments against seven state-of-the-art ULD methods on the public dataset DeepLesion <ref type="bibr" target="#b32">[32]</ref>. The results demonstrate that our method achieves competitive performance compared to state-of-the-art ULD approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we first formulate our overall detection process for DiffULD and then specify the training manner, inference process and backbone design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Diffusion-Based Detector for Lesion Detection</head><p>Universal Lesion Detection can be formulated as locating lesions in input CT scan I ct with a set of boxes predictions z 0 . For a particular box z , it can be denoted as z = [x 1 , y 1 , x 2 , y 2 ], where x 1 , y 1 and x 2 , y 2 are the coordinates of the top-left and bottom-right corners, respectively.</p><p>We design our model based on a diffusion model mentioned in <ref type="bibr" target="#b6">[7]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our method consists of two stages, a forward diffusion (or training) process and a reverse refinement (or inference) process. In the forward process, We denote GT BBoxes as z 0 and generate corrupted training samples z 1 , z 2 , ..., z T for latter DiffULD training by adding Gaussian noise iteratively, which can be defined as:</p><formula xml:id="formula_0">(z t | z 0 ) = N z t | âˆš á¾±t z 0 , (1 -á¾±t ) I (1)</formula><p>where á¾±t represents the noise variance schedule and t âˆˆ {0, 1, ..., T }. Subsequently, a neural network f Î¸ (z t , t, I ct ) conditioned on the corresponding CT scan I ct is trained to predict z 0 from a noisy box z T by reversing the noising process step by step. During inference, for an input CT scan I ct with a set of random boxes, the model is able to refine the random boxes to get a lesion detection prediction box z 0 , iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>In this section, we specify the training process with our novelty introduced 'Center-aligned BBox padding'.</p><p>Center-Aligned BBox Padding. As shown in Fig. We consider the generation in two parts: box scaling and center sampling. (i) Box scaling: We set a hyper-parameter Î» scale âˆˆ (0, 1) for scaling. For z i , ] of perturbated boxes from a 2D Gaussian distribution N whose probability density function can be denoted as:</p><formula xml:id="formula_1">f(x, y) = exp - x -x i c 2 + y -y i c 2 2Ïƒ 2 , (x, y) âˆˆ z i (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where Ïƒ is a size-adaptive parameter, which can be calculated according to the z i 's width and height:</p><formula xml:id="formula_3">Ïƒ = 1 6 (w i + h i ).<label>(3)</label></formula><p>Besides, for each input CT scan I ct , we collect all GT BBoxes in z and add random perturbations to them and generate multiple perturbated boxes for each of them. Thus the number of perturbated boxes in an image varies with its number of GT BBoxes. For better training, we fix the number of perturbated boxes as N for all training images. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>., the perturbated boxes cluster together and their centers are still aligned with the corresponding original GT BBox. Subsequently, perturbated GT boxes z 0 are sent for corruption as the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box Corruption.</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we corrupt the parameters of z 0 with Gaussian noises. The noise scale is controlled by á¾±t (in Eq. 1), which adopts the decreasing cosine scheduler in the different time step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function.</head><p>As the model generates the same number of (N ) predictions for the input image, termed as a prediction set, the loss function should be set-wised <ref type="bibr" target="#b4">[5]</ref>. Specifically, each GT is matched with the prediction by the least matching cost, and the overall training loss <ref type="bibr" target="#b4">[5]</ref> can be represented as:</p><formula xml:id="formula_4">L = Î» L1box â€¢ L L1box + Î» giou â€¢ L giou (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where L L1box and L giou are the pairwise box loss. We adopt Î» L1box = 2.0 and Î» giou = 5.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>At the inference stage, with a set of random boxes sampled from Gaussian distribution, the model does refinement step by step to obtain the final predictions z 0 . For better performance, two key components are used:</p><p>Box Filtering. In each refinement step, the model receives a set of box proposals from the last step. As the prediction starts from arbitrary boxes and the lack of GT (lesion), most of these proposals are very far from lesions. Keeping refining them in the following steps will hinder network training. Toward efficient detection, we send the proposals to the detection head and remove the boxes whose confidential scores are lower than a particular threshold Î» conf . The remaining high-quality proposals are sent for followed DDIM sampling.</p><p>Box Update with DDIM Sampling. DDIM <ref type="bibr" target="#b27">[27]</ref> is utilized to further refine the received box proposals by denoising. Next, these refined boxes are sent to the next step and start a new round of refinement. After multiple steps, final predictions are obtained. However, we observe that if we just filter out boxes with low scores during iterative refinement, the model runs out of usable box proposals rapidly, which also leads to a deterioration in performance. Therefore, after the box updating, we add new boxes sampled from a Gaussian Distribution to the set of remaining boxes with. The number of box proposals per image is padded to the fixed number N before they are sent to the next refinement step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Backbone Design</head><p>Our overall backbone design is identical to <ref type="bibr" target="#b15">[16]</ref>. The input CT scan is rendered with different window widths and window levels. Then, multi-window features are extracted with a ConvNeXt-T <ref type="bibr" target="#b18">[18]</ref> shared network and sent to 3D context feature fusion module. Subsequently, the fused feature is sent to the detector.</p><p>Multi-window Input. Most prior arts in ULD use a single and fixed window (e.g., a wide window of <ref type="bibr">[1024,</ref><ref type="bibr">4096]</ref>) to render the input CT scan, which suppresses organ-specific information and makes it hard for the network to focus on the various lesions. Therefore, taking cues from <ref type="bibr" target="#b15">[16]</ref>, we introduce 3 organ-specific HU windows to highlight multiple organs of interest. Their window widths and window levels are: W 1 = [1200, -600] for chest organs, W 2 = [400, 50] for soft tissues and W 3 = [200, 30] for abdomenal organs.</p><p>3D Context Feature Fusion. We modify the original A3D <ref type="bibr" target="#b37">[37]</ref> DenseNet backbone for context fusion. We remove the first Conv3D Block and use the truncated network as our 3D context fusion module, which fuses the multiwindow features from the last module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings</head><p>Our experiments are conducted on the standard ULD dataset DeepLesion <ref type="bibr" target="#b32">[32]</ref>. The dataset contains 32,735 lesions on 32,120 axial slices from 10,594 CT studies of 4,427 unique patients. We use the official data split of DeepLesion which consists of 70%, 15%, 15% for training, validation, and test, respectively. Besides, we also evaluate the performance of 3 methods based on a revised test set from <ref type="bibr" target="#b3">[4]</ref>.</p><p>Training Details. DiffULD is trained on CT scans of size 512 Ã— 512 with a batch size of 4 on 4 NVIDIA RTX Titan GPUs with 24GB memory. For hyperparameters, the threshold N for box padding is set to 300. Î» scale for box scaling is set to 0.4. Î» conf for box filtering is set to 0.5. We use the Adam optimizer with an initial learning rate of 2e -4 and the weight decay as 1e -4. The default training schedule is 120K iterations, with the learning rate divided by 10 at 60K and 100K iterations. Data augmentation strategies contain random horizontal flipping, rotation, and random brightness adjustment.</p><p>Evaluation Metrics. The lesion detection is classified as true positive (TP) when the IoU between the predicted and the GT BBox is larger than 0.5. Average sensitivities computed at 0.5, 1, 2, and 4 false-positives (FP) per image are reported as the evaluation metrics on the test set for a fair comparison (Table <ref type="table" target="#tab_1">2</ref>).</p><p>Table <ref type="table">1</ref>. Sensitivity (%) at various FPPI on the standard test set of DeepLesion. DKA-ULD <ref type="bibr" target="#b25">[25]</ref> and SATr <ref type="bibr" target="#b14">[15]</ref> are up-to-date SOTA ULD methods under the settings of 3 slices and 7 slices, respectively. The numbers in brackets indicate the performance gains of our method comparing to the previous SOTA methods under the same settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Slices @0.5 @1 @2 @4 Avg.[0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lesion Detection Performance</head><p>We evaluate the effectiveness of DiffULD against anchor-based ULD approaches such as 3DCE <ref type="bibr" target="#b33">[33]</ref>, MVP-Net <ref type="bibr" target="#b15">[16]</ref>, A3D <ref type="bibr" target="#b37">[37]</ref> and SATr <ref type="bibr" target="#b14">[15]</ref> on DeepLesion. Several anchor-free natural image detection methods such as FCOS <ref type="bibr" target="#b31">[31]</ref> and DN-DETR <ref type="bibr" target="#b11">[12]</ref> are also introduced for comparison. We report the performance of DiffusionDet <ref type="bibr" target="#b6">[7]</ref> trained with our proposed backbone in 2.4 as well. In addition,</p><p>we conduct an extensive experiment to explore DiffULD's potential on an revised test set of completely annotated DeepLesion volumes, introduced by Lesion-Harvester <ref type="bibr" target="#b3">[4]</ref>. Table <ref type="table">1</ref> demonstrates that our proposed DiffULD achieves favorable performances when compared to recent state-of-the-art anchor-based ULD approaches such as SATr on both 3 slices and 7 slices. It outperforms prior well-established methods such as A3D and MULAN by a non-trivial margin. This validates that with our padding strategy, the concise DPM can be utilized in general medical object detection tasks such as ULD and attain impressive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We provide an ablation study about our proposed approach: center-aligned BBox padding. As shown in Table <ref type="table" target="#tab_2">3</ref>., we compared it with various other padding strategies, including: (i) duplicating original GT boxes; (ii) padding random boxes sampled from a uniform distribution; (iii) padding random boxes sampled from a Gaussian distribution; (iv) padding with center-aligned strategy.</p><p>Our baseline method is training the diffusion model <ref type="bibr" target="#b6">[7]</ref> directly with no box padding, using our proposed backbone in 2.4. The performance is increased by 0.30% by simply duplicating the original GT boxes. Padding random boxes following uniform and Gaussian distributions brings 0.51% and 0.91% improvement respectively. Our center-aligned padding strategy accounts for 1.13% improvement from the baseline. We attribute this performance boost to center-aligned padding's ability to provide high-quality additional training targets. It effectively expands the insufficient training targets on CT scans and enhances DPM's detection performance while avoiding deterioration triggered by adding random targets. This property is favorable for utilizing DPMs on a limited amount of GT like ULD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel ULD method termed DiffULD by introducing the diffusion probability model (DPM) to Universal Lesion Detection. We present a novel center-aligned BBox padding strategy to tackle the performance deterioration caused by directly utilizing DPM on CT scans with sparse lesion BBoxes. Compared with other training target padding methods (e.g., padding with random boxes), our strategy can provide additional training targets of higher quality and boost detection performance while avoiding significant deterioration. DiffULD is inherently advanced in locating lesions with diverse sizes and shapes since it can predict with arbitrary boxes, making it a promising method for ULD. Experiments on both standard and revised DeepLesion datasets show that our proposed method can achieve competitive performance compared to state-of-the-art ULD approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed center-aligned BBox padding strategy.</figDesc><graphic coords="2,64,47,219,77,324,04,124,03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of DiffULD. The backbone extracts feature representation from an input CT scan. Then, the model takes the feature map and a set of noisy boxes as input and makes predictions.</figDesc><graphic coords="5,44,79,54,38,334,51,197,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1, we utilize Center-aligned BBox padding to generate perturbated boxes. Then, the perturbated boxes are paired with original GT BBoxes, forming perturbated GT boxes which are used to generate corrupted training samples z 1 , z 2 , ..., z T for the latter DiffULD training by adding Gaussian noise iteratively.For clarity, we denote [x i c , y i c ] as center coordinates of original GT BBox, where z i , [w i , h i ] are the width and height of z i .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Sensitivity (%) at various FPPI on the revised test set introduced by Lesion-Harvester<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>5,1,2,4]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of padding strategies at various FPs per image (FPPI).</figDesc><table><row><cell>Baseline Duplicate Gaussian Uniform Center-Aligned FPPI = 0.5</cell><cell>FPPI = 1</cell></row><row><cell>76.71</cell><cell>83.49</cell></row><row><cell>77.01</cell><cell>83.61</cell></row><row><cell>77.68</cell><cell>83.98</cell></row><row><cell>77.22</cell><cell>83.75</cell></row><row><cell cols="2">77.84 (1.13â†‘) 84.57 (1.08â†‘)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. Supported by <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62271465</rs> and <rs type="funder">Open Fund Project of Guangdong Academy of Medical Sciences, China</rs> (No. <rs type="grantNumber">YKY-KF202206</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_R9Yk5dQ">
					<idno type="grant-number">62271465</idno>
				</org>
				<org type="funding" xml:id="_CmeVe59">
					<idno type="grant-number">YKY-KF202206</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">nnDetection: a selfconfiguring method for medical object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>JÃ¤ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-351" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DiffuseMorph: unsupervised deformable image registration using diffusion model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boah</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19821-220" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>CissÃ©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="347" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep volumetric universal lesion detection using light-weight pseudo 3D convolution and surface point regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_1</idno>
		<idno>978-3-030-59719-1 1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lesion-harvester: iteratively mining unlabeled lesions and hardnegative examples at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EllipseNet: anchor-free ellipse detection for automatic cardiac biometrics in fetal echocardiography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-221" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09788</idno>
		<title level="m">DiffusionDet: diffusion model for object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalist framework for panoptic segmentation of images and videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06366</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CenterNet++ for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08394</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DiffPose: multi-hypothesis human pose estimation using diffusion models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Holmquist</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">DN-DETR: accelerate DETR training by introducing query DeNoising</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>IEEE CVPR</publisher>
			<biblScope unit="page" from="13619" to="13627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bounding maps for universal lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-141" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional training with bounding map for universal lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-314" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SATr: slice attention with transformer for universal lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-816" />
		<editor>Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S.</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="163" to="174" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>MICCAI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MVP-net: multi-view FPN with position-aware attention for deep universal lesion detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-72" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated malaria cells detection from blood smears under severe class imbalance via importance-aware balanced group softmax</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-344" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A ConvNet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="11976" to="11986" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OXnet: deep omni-supervised thoracic disease detection from chest X-Rays</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-350" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="537" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A segmentation-assisted model for universal lesion detection with partial labels</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_12</idno>
		<idno>978-3-030-87240-3 12</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retina-match: ipsilateral mammography lesion matching in a single shot detection pipeline</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TUN-Det: a novel network for thyroid ultrasound nodule detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudnejad</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-262" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="656" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An efficient anchor-free universal lesion detection in CT-scans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sheoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DKMA-ULD: domain knowledge augmented multi-head attention based robust universal lesion detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sheoran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06886</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ULDor: a universal lesion detector for CT scans with pseudo masks and hard negative example mining</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="833" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly-supervised universal lesion segmentation with regional level set loss</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-348" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="515" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving deep lesion detection using 3D contextual and spatial attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-721" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="page" from="9627" to="9636" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36501" to="036501" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D context enhanced region-based convolutional neural network for end-to-end lesion detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_58</idno>
		<idno>978-3-030-00928-1 58</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-LÃ³pez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MULAN: multitask universal lesion analysis network for joint lesion detection, tagging, and segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_22</idno>
		<idno>978-3-030-32226-7 22</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from multiple datasets with heterogeneous and partial labels for universal lesion detection in CT</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2759" to="2770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">AlignShift: bridging the gap of imaging thickness in 3D anisotropic volumes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-155" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric 3D context fusion for universal lesion detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_55</idno>
		<idno>978-3-030-87240-3 55</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="571" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinventing 2D convolutions for 3D images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3009" to="3018" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep attentive panoptic model for prostate cancer detection using biparametric MRI scans</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-158" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting 3D context modeling with supervised pre-training for universal lesion detection in CT slices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_53</idno>
		<idno>978-3-030-59719-1 53</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Positive-unlabeled learning for cell detection in histopathology images with incomplete annotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-349" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Handbook of Medical Image Computing and Computer Assisted Intervention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="820" to="838" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving RetinaNet for CT lesion detection with dense masks from weak RECIST labels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zlocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_45</idno>
		<idno>978-3-030-32226-7 45</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
