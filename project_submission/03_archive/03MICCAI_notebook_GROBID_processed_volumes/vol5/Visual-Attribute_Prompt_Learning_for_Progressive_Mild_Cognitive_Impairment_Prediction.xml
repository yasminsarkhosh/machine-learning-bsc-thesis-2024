<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction</title>
				<funder ref="#_Ke87Yvc">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_qZnCKqM">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial</orgName>
				</funder>
				<funder ref="#_Fhws7wp">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_2uZP53k">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luoyao</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifan</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Laboratory of Big Data Computing</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen. L. Kang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="547" to="557"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DCFF6DD7A21D0123FDF61E3E7FF08F1A</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Alzheimer&apos;s disease</term>
					<term>Prompt learning</term>
					<term>Magnetic resonance imaging</term>
					<term>Multi-modal classification</term>
					<term>Transformer</term>
					<term>Attention modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning (DL) has been used in the automatic diagnosis of Mild Cognitive Impairment (MCI) and Alzheimer's Disease (AD) with brain imaging data. However, previous methods have not fully exploited the relation between brain image and clinical information that is widely adopted by experts in practice. To exploit the heterogeneous features from imaging and tabular data simultaneously, we propose the Visual-Attribute Prompt Learning-based Transformer (VAP-Former), a transformer-based network that efficiently extracts and fuses the multi-modal features with prompt fine-tuning. Furthermore, we propose a Prompt fine-Tuning (PT) scheme to transfer the knowledge from AD prediction task for progressive MCI (pMCI) diagnosis. In details, we first pre-train the VAP-Former without prompts on the AD diagnosis task and then fine-tune the model on the pMCI detection task with PT, which only needs to optimize a small amount of parameters while keeping the backbone frozen. Next, we propose a novel global prompt token for the visual prompts to provide global guidance to the multi-modal representations. Extensive experiments not only show the superiority of our method compared with the state-of-the-art methods in pMCI prediction but also demonstrate that the global prompt can make the prompt learning process more effective and stable. Interestingly, the proposed prompt learning model even outperforms the fully fine-tuning baseline on transferring the knowledge from AD to pMCI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alzheimer's disease (AD) is one of the most common neurological diseases in elderly people, accounting for 50-70% of dementia cases <ref type="bibr" target="#b31">[31]</ref>. The progression of AD triggers memory deterioration, impairment of cognition, irreversible loss of neurons, and further genetically complex disorders as well. Mild Cognitive Impairment (MCI), the prodromal stage of AD, has been shown as the optimal stage to be treated to prevent the MCI-to-AD conversion <ref type="bibr" target="#b29">[29]</ref>. Progressive MCI (pMCI) group denotes those MCI patients who progress to AD within 3 years, while stable MCI (sMCI) patients remained stable over the same time period. pMCI is an important group to study longitudinal changes associated with the development of AD <ref type="bibr" target="#b25">[26]</ref>. By predicting the pMCI progress of the patients, we can intervene and delay the progress of AD. Thus, it is valuable to distinguish patients with pMCI from those with sMCI in the early stage with a computeraided diagnosis system.</p><p>In recent years, deep learning (DL) based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> have been widely used to identify pMCI or AD based on brain MRI data. Several works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">29]</ref> take both brain MRI and clinical tabular data into account, using convolutional neural networks (CNNs) and multi-layer perceptions as the feature encoder. Due to the limited data in pMCI diagnosis, some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">29]</ref> resort to transfer learning to fine-tune the model on the pMCI-related task, by pre-training weights on the AD detection task. Previous CNN-based approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">29]</ref> may fail on the lack of modeling long-range relationship, while the models based on two-stage fine-tuning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">29]</ref> are inefficient and may cause networks to forget learned knowledge <ref type="bibr" target="#b32">[32]</ref>.</p><p>Inspired by the advance in transformers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">28]</ref> and prompt learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>, we tailor-design an effective multi-modal transformer-based framework based on prompt learning for pMCI detection. The proposed framework is composed of a transformer-based visual encoder, a transformer-based attribute encoder, a multi-modal fusion module, and a prompt learning scheme. Clinical attributes and brain MR images are sent to the attribute encoder and visual encoder with the prompt tokens, respectively. Then the high-level visual and tabular features are aggregated and sent to fully-connected layers for final classification. For the proposed prompt tuning scheme, we first pre-train the neural network on the AD classification task. After that, we fine-tune the neural network by introducing and updating only a few trainable parameters. Importantly, we observe that the number of image patches and input tokens have a gap between 2D natural images and 3D MR images, due to the different dimensions. To complement local interactions between prompt and other tokens, we develop a global prompt token to strengthen the global guidance and make the visual feature extraction process more efficient and stable.</p><p>Our contributions have three folds: <ref type="bibr" target="#b0">(1)</ref> We propose a visual-attribute prompt learning framework based on transformers (VAP-Former) for pMCI detection.</p><p>(2) We design a global prompt to adapt to high-dimension MRI data and build a prompt learning framework for transferring knowledge from AD diagnosis to pMCI detection. <ref type="bibr" target="#b2">(3)</ref> Experiments not only show that our VAP-Former obtains state-of-the-art results on pMCI prediction task by exceeding the full fine-tuning methods, but also verify the global prompt can make the training more efficient and stable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We aim to predict if an MCI patient will remain stable or develop Alzheimer's Disease, and formulate the problem as a binary classification task based on the brain MR image and the corresponding tabular attribute information of an MCI patient from baseline examination. As Fig. <ref type="figure" target="#fig_0">1</ref> shows, we adopt the model weights learned from AD identification to initialize the prediction model of MCI conversion. In the prompt fine-tuning stage, we keep all encoders frozen, only optimizing the prompts concatenated with feature tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Architecture</head><p>We propose a transformer-based framework for pMCI prediction (VAP-Former) based on visual and attribute data, which is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. VAP-Former is mainly composed of three parts: a visual encoder for processing MRI data, an attribute encoder for processing attribute data, and a transformer-based fusion block for combining the multi-modal feature. Considering that capturing the long-range relationship of MRI is important, we employ the encoder of 3D UNETR++ <ref type="bibr" target="#b28">[28]</ref> as our visual encoder. For the attribute encoder, since the clinical variables have no order or position, we embed the tabular data with the transformer blocks <ref type="bibr" target="#b30">[30]</ref>. Followed by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we prepend a class token for dualmodality before the transformer-based fusion block. A class token is a learnable vector concatenated with dual-modal feature vectors. The class token of dualmodal is further processed by fully-connected layers for the final classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Transfer with Multi-modal Prompt Learning</head><p>To effectively transfer the knowledge learned from AD prediction task to the pMCI prediction task, we propose a multi-modal prompt fine-tuning strategy that adds a small number of learnable parameters (i.e., prompt tokens) to the input of the transformer layer and keeps the backbone frozen. The overall pipeline is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where the upper part indicates adding the prompt tokens to the attribute transformer, while the lower part denotes sending the prompt tokens to the visual transformer.</p><p>Tabular Context Prompt Learning. In the attribute encoder, we insert prompts into every transformer layer's input <ref type="bibr" target="#b14">[15]</ref>. For the (i)-th Layer L i of SA transformer block, we denote collection of p prompts as</p><formula xml:id="formula_0">P = {p k ∈ R C k ∈ N, 1 ≤ k ≤ p},</formula><p>where k is the number of the tabular prompt. The prompt fine-tuning Tabtransformer can be formulated as:</p><formula xml:id="formula_1">[ , X i ] = L i ([P i-1 , X i-1 ]), [ , X i+1 ] = L i+1 ([P i , X i ]),<label>(1)</label></formula><p>where X i ∈ R M ×C denotes the attribute embedding at L i 's output space and P i denotes the attribute prompt at L i+1 's input space concatenated with X i .</p><p>Visual Context Prompt Learning with Paired Attention. For the visual prompt learning part, we concatenate a small number of prompts with visual embedding to take part in the spatial-wise and channel-wise attention block <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> denoted as the prompt fine-tuning efficient paired attention block in Fig. <ref type="figure" target="#fig_1">2</ref>. Within the prompt fine-tuning efficient paired attention block, we insert shared prompts into the spatial-wise attention module (SWA) and the channel-wise attention module (CWA), respectively. With a shared keys-queries scheme, queries, keys, and values are noted as Q p shared , K p shared , V p spatial , and</p><formula xml:id="formula_2">V p channel . Let [•,</formula><p>•] be the concatenation operation, the SWA and CWA can be formulated as:</p><formula xml:id="formula_3">[P spatial , I S ] = SW A(Q p shared , K p shared , V p spatial ), [P channel , I C ] = CW A(Q p shared , K p shared , V p channel ),<label>(2)</label></formula><p>where I S ∈ R N ×C and P spatial ∈ R P 2 ×C are spatial-wise visual embedding and prompts, and I C ∈ R N ×C and P channel ∈ R P 2 ×C are channel-wise visual embedding and prompts, and P is the number of visual prompt. After that, the initial feature map I is added to the attention feature map using a skip connection. Let + be the element-wise addition, this process is formulated as I = I + (I S + I C ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Global Prompt for Better Visual Prompt Learning</head><p>Compared to natural images with relatively low dimensionality (e.g., shaped 224 × 224 × 3) and the salient region usually locates in a small part of the image, brain MRIs for diagnosis of Alzheimer's are usually high dimensional (e.g., shaped 144 × 144 × 144) and the cues to diagnosis disease (e.g., cortical atrophy and beta protein deposition) can occupy a large area of the image. Therefore, vanilla prompt learning <ref type="bibr" target="#b32">[32]</ref> methods that are designed for natural images may not be directly and effectively applied to MRI's Recognition of Alzheimer's disease. We consider that the above differences lead to the following two problems: (1) vanilla prompt token often focuses on local visual information features; <ref type="bibr" target="#b1">(2)</ref> the interaction between vanilla prompt token and visual feature is insufficient. Therefore, we consider that a sophisticated prompt learning module should be able to address the above-mentioned issues with the following feature:</p><p>(1) the prompt token can influence the global feature; (2) the prompt token can effectively interact with the visual input features. A simple approach to achieve the second goal is to increase the number of prompt tokens so that they can better interact with other input features. However, the experiment proves that this is not feasible. We think it is because too many randomly initialized prompt tokens (i.e., unknown information) will make the model hard to train.</p><p>Thus, we tailor-design a global prompt token g to achieve the above two goals. Specifically, we apply a linear transformation T to the input prompt tokens P to obtain the global prompt token g (i.e., a vector), which further multiplies the global feature map. Since the vector is directly multiplied with the global feature, we can better find the global feature responses in each layer of the visual network, which enables the model to better focus on some important global features for pMCI diagnosis, such as cortical atrophy. Since this linear transformation, T operation is learnable in the prompt training stage, the original prompt token can better interact with other features through the global token. To embed the global prompt token into our framework, we rewrite Eq. 3 as:</p><formula xml:id="formula_4">I = I + (I S + I C ) × T ([P spatial , P channel ]),<label>(3)</label></formula><p>where T denotes the linear transformation layer with P × C elements as input and 1×C element as output. Experiments not only demonstrate the effectiveness of the above method but also make the training stage more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>The datasets are from Alzheimer's Disease Neuroimaging Initiative (ADNI) <ref type="bibr" target="#b13">[14]</ref>, including ADNI-1 and ADNI-2. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>, we adopt ADNI- We preprocess the MR image as <ref type="bibr" target="#b21">[22]</ref>. All MRI scans are pre-processed via 4 steps: (1) motion correction, (2) intensity normalization, (3) skull stripping, and (4) spatial normalization to a template. We use the Computational Anatomy Toolbox (CAT12)<ref type="foot" target="#foot_0">1</ref> via Statistical Parametric Mapping software (SPM12)<ref type="foot" target="#foot_1">2</ref> to perform the above procedures. Then all images are re-sampled as the size of 113×137×113 and the resolution of 1×1×1 mm 3 . For tabular clinical data, we select 7 attributes including age, gender, education, ApoE4, P-tau181, T-tau, and a summary measure (FDG) derived from 18F-fluorodeoxyglucose PET imaging. For tabular clinical data, we apply one-hot encoding to the categorical variables and min-max normalization to the numerical variables. We adopt the model weights learned from AD identification to initialize the prediction model of MCI conversion. Our model is trained using 1 NVIDIA V100 GPU of 32GB via AdamW optimizer <ref type="bibr" target="#b19">[20]</ref> with 30 epochs for AD identification and 20 epochs for pMCI detection. The batch size is set to 4. We adopt the ReduceLROnPlateau <ref type="bibr" target="#b0">[1]</ref> learning rate decay strategy with an initial learning rate of 1e-5. The loss function is binary crossentropy loss. The number of visual and tabular prompts is 10 and 5, respectively. We take F1-score <ref type="bibr" target="#b22">[23]</ref>, class balanced accuracy (BACC) <ref type="bibr" target="#b2">[3]</ref>, and the area under the receiver operating characteristic curve (AUC) <ref type="bibr" target="#b22">[23]</ref> as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with the State-of-the-Art Methods</head><p>To validate the proposed VAP-Former and prompt fine-tuning (PT) strategy, we compare our model with three unimodal baselines: 1) UNETR++, which denotes the encoder of UNETR++ <ref type="bibr" target="#b28">[28]</ref> only using MRI data as input, 2) Tabformer <ref type="bibr" target="#b20">[21]</ref>, which only uses tabular data and is similar to the attribute encoder work in our model. 3) 4-Way Classifier <ref type="bibr" target="#b26">[27]</ref> which used 3D DenseNet as the backbone to construct Alzheimer's disease diagnosis model only using MRI data as input. To further evaluate the efficiency of our model and the proposed PT strategy, we integrate the DAFT <ref type="bibr" target="#b23">[24]</ref>, HAMT <ref type="bibr" target="#b4">[5]</ref>, and DFAF <ref type="bibr" target="#b9">[10]</ref> into the same attribute and visual encoder for a fair comparison. And we fine-tune the proposed model with two strategies including full fine-tuning (FT) and prompt tuning (PT).</p><p>In Table <ref type="table" target="#tab_1">1</ref>, the proposed model with PT strategy achieves 79.22% BACC, 63.13% F1, and 86.31% AUC. VAP-Former and VA-Former outperform all unimodal baselines in all metrics, indicating that our model can effectively exploit the relation between MRI data and tabular data to improve the prediction of pMCI. By using the PT strategy, the VAP-Former outperforms the second-best model, DAFT, by 1.16% BACC, 0.21% F1, and 0.98% AUC, demonstrating that the proposed PT strategy can efficiently adapt learned knowledge to pMCI detection and even achieves better classification results. Besides that, VAP-Former achieves the best results by tuning the minimum number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study and Investigation of Hyper-parameters</head><p>In this section, we study the internal settings and mechanism of the proposed PT strategy. First, we investigate how the prompts affect the VAP-Former performance. So we remove the visual prompts from the VAP-Former (denoted as TabPrompt in Table <ref type="table">2</ref>) and tabular prompts (denoted as VisPrompt), respectively. Compared with VA-Former, VisPrompt outperforms it by 0.07% AUC and TabPrompt degrades the performance. However, VAP-Former, which combines tabular prompts and visual prompts, significantly outperforms VA-Former by 1.54% AUC, indicating that introducing both types of prompts into the model simultaneously results in a more robust pMCI classification model. We further validate the importance of the global prompt module in the VAP-Former by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To detect pMCI with visual and attribute data, we propose a simple but effective transformer-based model, VA-Former, to learn multi-modal representations. Besides, we propose a global prompt-based tuning strategy, which is integrated with the VA-Former to obtain our overall framework VAP-Former. The proposed framework can efficiently transfer the learned knowledge from AD classification to the pMCI prediction task. The experimental results not only show that the VAP-Former performs better than uni-modal models, but also suggest that the VAP-Former with the proposed prompt tuning strategy even surpasses the full fine-tuning while dramatically reducing the tuned parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed Visual-Attribute Prompt learning transformer framework. The left part shows the architecture of VA-Former without using prompts. The right part shows the proposed framework with the prompt tuning strategy (VAP-Former) that only updates the learnable prompts.</figDesc><graphic coords="3,55,98,96,98,340,30,148,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed global prompt-based learning strategy. The strategy replaces the attribute encoder with the Prompt Fine-tuning Tab-transformer (upper part) and replaces the Efficient Paired Attention (EPA) block with the prompt fine-tuning EPA block (lower part).</figDesc><graphic coords="4,39,39,44,48,348,91,178,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1/ADNI-2 as the train/test set. The subjects that exist in both datasets are excluded from ADNI-2. There are 1340 baseline T1-weighted structure MRI scans of 4 categories including Alzheimer's disease (AD), normal control (NC), stable mild cognitive impairment (sMCI), and progressive mild cognitive impairment (pMCI). 707 subjects (158 AD, 193 NC, 130 pMCI, 226 sMCI) are from ADNI-1, while 633 subjects (137 AD, 159 NC, 78 pMCI, 259 sMCI) are from ADNI-2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with existing pMCI classification methods. 'FT' represents full fine-tuning without prompts and 'PT' represents prompt fine-tuning. '# params' denotes the number of parameters being tuned in the fine-tuning stage.</figDesc><table><row><cell>Method</cell><cell>Modal</cell><cell cols="3">Fine-tuning #params (M) BACC</cell><cell>F1</cell><cell>AUC</cell></row><row><cell></cell><cell>Vis Tab</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNETR++ [28]</cell><cell></cell><cell>FT</cell><cell>64.59</cell><cell cols="2">65.94±1.19 47.42±1.20 70.46±1.67</cell></row><row><cell>Tabformer [21]</cell><cell></cell><cell>FT</cell><cell>27.13</cell><cell cols="2">78.05±0.52 60.29±0.27 84.26±0.17</cell></row><row><cell>4-Way Classifier [27]</cell><cell></cell><cell>FT</cell><cell>12.27</cell><cell cols="2">71.86±2.07 53.49±3.44 75.81±1.01</cell></row><row><cell>DFAF [10]</cell><cell></cell><cell>FT</cell><cell>69.32</cell><cell cols="2">77.14±0.35 59.66±0.85 84.41±0.49</cell></row><row><cell>HAMT [5]</cell><cell></cell><cell>FT</cell><cell>71.83</cell><cell cols="2">75.07±0.91 55.58±1.11 84.23±0.31</cell></row><row><cell>DAFT [24]</cell><cell></cell><cell>FT</cell><cell>67.89</cell><cell cols="2">78.06±0.55 62.92±0.71 85.33±0.41</cell></row><row><cell>VA-Former</cell><cell></cell><cell>FT</cell><cell>70.19</cell><cell cols="2">78.29±0.52 62.93±0.29 84.77±0.35</cell></row><row><cell>VAP-Former</cell><cell></cell><cell>PT</cell><cell>0.59</cell><cell cols="2">79.22±0.58 63.13±0.11 86.31±0.25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://neuro-jena.github.io/cat/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.fil.ion.ucl.ac.uk/spm/software/spm12/.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), and the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62102267</rs>), and the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2023A1515011464</rs>), and the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>), and the <rs type="funder">Guangdong Provincial</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Fhws7wp">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_Ke87Yvc">
					<idno type="grant-number">62102267</idno>
				</org>
				<org type="funding" xml:id="_2uZP53k">
					<idno type="grant-number">2023A1515011464</idno>
				</org>
				<org type="funding" xml:id="_qZnCKqM">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 53.  removing it from the model (denoted as Vis-TabPrompt). As shown in Table <ref type="table">2</ref>, removing the global prompt results in degraded performance by 1.08% AUC.</p><p>To investigate the impact of the number of prompts on performance, we evaluate VAP-Former with varying numbers of prompts. Given that the number of visual tokens exceeds that of tabular tokens. We fix the number of tabular prompts at 5 (left plot in Fig. <ref type="figure">3</ref>) and fix the number of visual prompts at 10 (right plot), respectively while changing the other one. We hypothesize that the interaction between prompts and feature tokens is insufficient, so we gradually increase the number of tokens. As shown in Fig. <ref type="figure">3</ref>, we observe that the model's performance ceases to increase after a certain number of prompts, confirming our assumption in Sect. 3.2 that too many randomly initialized prompt tokens make the model difficult to train. Conversely, when the number of prompts is small, the prompts can not learn enough information for the task and the interaction between prompt and feature tokens is not sufficient. Furthermore, as depicted in Fig. <ref type="figure">3</ref>, the light blue area is smaller than the orange area, suggesting that the global prompt module makes VAP-Former more robust by helping the model to focus on important global features for pMCI detection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduling techniques for liver segmentation: Reducelronplateau vs OneCycleLR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Kababji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bensaali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Dakua</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08277-1_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08277-117" />
	</analytic>
	<monogr>
		<title level="m">ISPR 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bennour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Ensari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Kessentini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Eom</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1589</biblScope>
			<biblScope unit="page" from="204" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single subject prediction of brain disorders in neuroimaging: promises and pitfalls</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Arbabshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Plis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Calhoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="137" to="165" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The balanced accuracy and its posterior distribution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Brodersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3121" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">History aware multimodal transformer for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Guhur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5834" to="5847" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning to detect Alzheimer&apos;s disease from neuroimaging: a systematic literature review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ebrahimighahnavieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page">105242</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal multitask deep learning model for Alzheimer&apos;s disease progression detection based on time series data</title>
		<author>
			<persName><forename type="first">S</forename><surname>El-Sappagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abuhmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="197" to="215" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vqamix: conditional triplet mixup for medical visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3332" to="3343" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local context encoder: robust biomedical image segmentation against adversarial attacks</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8417" to="8424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentive symmetric autoencoder for brain MRI segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-920" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19827-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19827-441" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">View-disentangled transformer for brain lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depthwise nonlocal module for fast salient object detection using a single thread</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6188" to="6199" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical fully convolutional network for joint atrophy localization and alzheimer&apos;s disease diagnosis using structural MRI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="880" to="893" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rk6qdGgCZ" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tabular transformers for modeling multivariate time series</title>
		<author>
			<persName><forename type="first">I</forename><surname>Padhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3565" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative image synthesis and disease diagnosis for classification of neurodegenerative disorders with incomplete multimodal neuroimages</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatially-constrained fisher representation for brain disease identification with incomplete multi-modal neuroimages</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2965" to="2975" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining 3D image and tabular data via the dynamic affine feature map transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-366" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Baseline MRI predictors of conversion from MCI to probable AD in the ADNI cohort</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Risacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Firpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Alzheimer Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="361" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D DenseNet ensemble in 4-way classification of alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Modasshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shamim Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BI 2020. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mahmud</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vassanelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Zhong</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12241</biblScope>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59277-6_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59277-68" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unetr++: delving into efficient and accurate 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04497</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duggento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Toschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Defeating Alzheimer&apos;s disease and other dementias: a priority for European science and society</title>
		<author>
			<persName><forename type="first">B</forename><surname>Winblad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Neurol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="455" to="532" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
