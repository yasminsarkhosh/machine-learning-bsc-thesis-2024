<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation</title>
				<funder ref="#_qkVAVsA">
					<orgName type="full">Natural Science Foundation of Hubei Province</orgName>
				</funder>
				<funder ref="#_epWUE8w">
					<orgName type="full">Renmin Hospital of Wuhan University</orgName>
				</funder>
				<funder ref="#_eDxYPPz">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_SbBmzZr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qikui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gynecology</orgName>
								<orgName type="institution">Renmin Hospital of Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanxiang</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gynecology</orgName>
								<orgName type="institution">Renmin Hospital of Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shuo</forename><surname>Li</surname></persName>
							<email>slishuo@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="338" to="347"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F9CF80F5F9AE0D1D3F61929B9771D9F6</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing tumor augmentation methods cannot deal with both domain and content information at the same time, causing a content distortion or domain gap (distortion problem) in the generated tumor. To address this challenge, we propose a Domain-aware and Contentconsistent Cross-cycle Framework, named DCAug, for tumor augmentation to eliminate the distortion problem and improve the diversity and quality of synthetic tumors. Specifically, DCAug consists of one novel Cross-cycle Framework and two novel contrastive learning strategies: 1) Domain-aware Contrastive Learning (DaCL) and 2) Cross-domain Consistency Learning (CdCL), which disentangles the image information into two solely independent parts: 1) Domain-invariant content information; 2) Individual-specific domain information. During new sample generation, DCAug maintains the consistency of domain-invariant content information while adaptively adjusting individual-specific domain information through the advancement of DaCL and CdCL. We analyze and evaluate DCAug on two challenging tumor segmentation tasks. Experimental results (10.48% improvement in KiTS, 5.25% improvement in ATLAS) demonstrate that DCAug outperforms current state-of-the-art tumor augmentation methods and significantly improves the quality of the synthetic tumors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Existing tumor augmentation methods, including "Copy-Paste" strategy based methods <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b18">19</ref>] and style-transfer based methods <ref type="bibr" target="#b4">[5]</ref>, only considered content or style information when synthesizing new samples, which leads to a distortion gap in content or domain space between the true image and synthetic image, and further causes a distortion problem <ref type="bibr" target="#b13">[14]</ref> as shown in Fig. <ref type="figure">1</ref> <ref type="bibr" target="#b0">(1)</ref>. The distortion problem damages the effectiveness of DCNNs in feature representation learning as proven in many studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref>. Therefore, a domain and content simultaneously aware data augmentation method is urgently needed to eliminate and avoid the distortion challenges during tumor generation. It remains, however, a very challenging task because the content and domain space lack of clear border, and the domain information always influences the distribution of content. This is also the main reason that style transfer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> still suffers from spurious artifacts such as disharmonious colors and repetitive patterns, and a large gap is still left between real artwork and synthetic style <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, it's necessary to reduce the influence of the domain on content and keep the content consistent during image generation.</p><p>To overcome the above challenges, a Domain-aware and Content-consistent tumor Augmentation method, named DCAug, is developed (Fig. <ref type="figure">1</ref> Experimental results on two public tumor segmentation datasets show that DCAug improves the tumor segmentation accuracy compared with state-of-theart tumor augmentation methods. In summary, our contributions are as follows:</p><p>-A content-aware and domain-aware tumor augmentation method is proposed, which eliminates the distortion in content and domain space between the true tumor image and synthetic tumor image. -Our novel DaCL and CdCL disentangle the image information into two completely independent parts: 1) domain-invariant content information; 2) individual-specific domain information. It has the advantage of alleviating the challenge of distortion in synthetic tumor images. -Experimental results on two public tumor segmentation datasets demonstrate that DCAug improves the diversity and quality of synthetic tumor images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Formulation: Given two images and the corresponding tumor labels {X A , Y A }, {X B , Y B }, tumor composition process can be formulated as:</p><formula xml:id="formula_0">X b A = X B • Y B + X A • (1 -Y B ), Y b A = Y B + Y A • (1 -Y B )<label>(1)</label></formula><formula xml:id="formula_1">X a B = X A • Y A + X B • (1 -Y A ), Y a B = Y A + Y B • (1 -Y A )<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">• is element-wise multiplication, X b A represents the tumor in image X B is copied to image X A , X a B represents the tumor in image X A is copied to image X B , Y b A and Y A b is the corresponding new tumor labels of X b A , X a B</formula><p>, respectively. There are two challenges need to be solved: 1) X b→A A , X a→B B , by adjusting the domain information of the copied tumor, making the copied tumor have the same domain space as the target image to avoid domain distortion; 2)</p><formula xml:id="formula_3">X b→A A → ← X b A , X a→B B → ← X a</formula><p>B , maintaining the domain-invariant content information consistency during tumor copy to avoid content distortion.</p><p>To achieve the above goals, a novel Cross-cycle Framework (Fig. <ref type="figure" target="#fig_2">2</ref>) is designed, which consists of two generators and can disentangle the tumor information into two solely independent parts: 1) Domain-invariant content information, 2) Individual-specific domain information, through two new learning strategies: 1) Domain-aware contrastive learning (DaCL); 2) Cross-domain consistency learning (CdCL). When generating new sample, the domain-invariant content information is preserved by CdCL, while the individual-specific domain information is adjusted by DaCL based on the domain space of target tumor image. The details are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain-Aware Contrastive Learning for Domain Adaptation</head><p>Our domain-aware contrastive learning (DaCL) strategy can adaptively adjust the domain space of the transferred tumor and makes the domain space consistent for domain adaptation. Specifically, the input of DCAug is two combined images X b A , X a B that consist of source images and tumor regions copied from another image. The synthetic tumors X b→A A generated by the generator, the  A as the anchor, the positive and the negative sample, respectively. To find the domain space of these samples for contrast, a fixed pre-trained style representation extractor f is used to obtain domain representations for different images. Thus, DaCL between the anchor, the positive, and the negative sample can be formulated as:</p><formula xml:id="formula_4">L contrastive (X b→A A , X A , X b A ) = n i=1 w i D(f (X b→A A ), f(X A )) D(f (X b→A A ), f(X b A ))<label>(3)</label></formula><p>where D(x, y) is the L 2 distance between x and y, w i is weighting factor. Additionally, to further disentangle the individual-specific domain information, a reversed process is designed. By utilizing the synthetic tumors X a→B B , X b→A A , the reversed images X a→B A ,X b→A B can be construed as:</p><formula xml:id="formula_5">X a→B A = X a→B B • Y A + X A • (1 -Y A ), X b→A B = X b→A A • Y B + X B • (1 -Y B ) (4)</formula><p>The whole reversed process receives the reversed images X a→B A , X b→A B as inputs and tries to restore the original domain information of the synthetic tumor XA , XB .</p><formula xml:id="formula_6">L b→A content = L i=1 ( μ(φ i (X b→A A • Y B )) -μ(φ i (X B • Y B )) 2 + σ(φ i (X b→A A • Y B )) -σ(φ i (X B • Y B )) 2 ) (9)</formula><p>where φ denotes the ith layer of the VGG-19 network, μ and σ represent the mean and standard deviation of feature maps extracted by φ, respectively. In summary, the total loss for the cross-cycle framework is</p><formula xml:id="formula_7">L total = α(L A pixel + L B pixel ) + β(L b→A contrastive + L a→B contrastive ) + γ(L b→A content + L a→B content )<label>(10</label></formula><p>) where α, β, and γ represent the weight coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>ATLAS Dataset <ref type="bibr" target="#b10">[11]</ref>: The ATLAS dataset consists of 229 T1-weighted MR images from 220 subjects with chronic stroke lesions. These images were acquired from different cohorts and different scanners. The chronic stroke lesions are annotated by a group of 11 experts. The dimension of the pre-processed images is 197 × 233 × 189 with an isotropic 1mm 3 resolution. Identical with the study in <ref type="bibr" target="#b16">[17]</ref>, We selected 50 images as the test set and the rest of the cases as the training set. KiTS19 Dataset <ref type="bibr" target="#b3">[4]</ref>: The KiTS19 consists of 210 3D abdominal CT images with kidney tumor subtypes and segmentation of kidney and kidney tumors. These CT images are from more than 50 institutions and scanned with different CT scanners and acquisition protocols. In our experiment, we randomly split the published 210 images into a training set with 168 images and a testing set with 42 images. Training Details: The generator in DCAug is built on the RAIN <ref type="bibr" target="#b11">[12]</ref> backbone, all of the weights in generators are shared. Our DCAug is implemented using PyTorch <ref type="bibr" target="#b12">[13]</ref> and trained end-to-end with Adam <ref type="bibr" target="#b8">[9]</ref> optimization method. In the training phase, the learning rate is initially set to 0.0001 and decreased by a weight decay of 1.0 × 10 -6 after each epoch. The experiments were carried out on one NVIDIA RTX A4000 GPU with 16 GB memory. The weight valule of α, β, and γ is 1.0,1.0,1.0, separately. Baseline: nnUNet <ref type="bibr" target="#b5">[6]</ref> is selected as the baseline model. The default hyperparameters and default traditional data augmentation (TDA) including rotation, scaling, mirroring, elastic deformation, intensity perturbation are used when model training. The maximum number of training epochs was set to 500 for the two datasets. Parts of tumors generated are shown in Fig. <ref type="figure" target="#fig_4">3</ref>. And the dice coefficients of the segmentation results on the same test set are computed to evaluate the effectiveness of methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Art Methods</head><p>Experimental results in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_5">4</ref> show that compared with other stateof-the-art methods, including Mixup <ref type="bibr" target="#b15">[16]</ref>, CutMix <ref type="bibr" target="#b14">[15]</ref>, CarveMix <ref type="bibr" target="#b16">[17]</ref>, SelfMix <ref type="bibr" target="#b18">[19]</ref>, StyleMix <ref type="bibr" target="#b4">[5]</ref>, nnUnet combined with DCAug achieves the highest improvement on the two datasets, which convincingly demonstrates the innovations and contribution of DCAug in generating higher quality tumor. And it is worth noting that CutMix ("Copy-Paste"method that only considers content information) even degrades the segmentation performance, which indicates that both content and domain information has a significant influence on the tumor segmentation.</p><p>The representative segmentation scans are shown in Fig. <ref type="figure" target="#fig_5">4</ref>. Our DCAug produced better segmentation results than the competing methods, which further proves the effectiveness of DCAug in tumor generation. What's more, the potential of DCAug in an extremely low-data regime is also demonstrated. We randomly select 25% and 50% of data from the training set same as training data. DCAug also assists the baseline model to achieve higher  Dice coefficients, which convincingly demonstrates the effectiveness of DCAug in generating new tumor samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Significant in Improving Existing Tumor Augmentation Methods</head><p>The necessity of considering both content and domain information in the tumor generation is also demonstrated, three representative methods, Mixup ("Copy-Paste"), CutMix ("Copy-Paste"), and StyleMix (style-transfer), are selected. The DCAug optimizes generated samples from above methods from content and domain aspects to further improve the quality of generated samples. And the nnUet are trained by optimized samples. From the segmentation performances (Table <ref type="table" target="#tab_1">2</ref>), we can notice that DCAug can further boost the quality of generated samples produced by existing methods. Specifically, the DCAug assists the Mixup, CutMix, and StyleMix to obtain a 3.15%, 8.53%, and 0.60% improvement in segmentation performance, respectively, which demonstrates that 1) it is necessary to consider both content and domain information during samples generation; 2) avoiding the content and domain distortion challenge can further improve the quality of generated samples; 3) DCAug can alleviate the challenge of distortion problem present in existing tumor augmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, our domain-aware and content-consistent tumor augmentation method eliminated the content distortion and domain gap between the true tumor and synthetic tumor by simultaneously focusing the content information and domain information. Specifically, DCAug can maintain the domain-invariant content information consistency and adaptive adjust individual-specific domain information by a new cross-cycle framework and two novel contrastive learning strategies when generating synthetic tumor. Experimental results on two tumor segmentation tasks show that our DCAug can significantly improve the quality of the synthetic tumors, eliminate the gaps, and has practical value in medical imaging applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 . 1 )</head><label>11</label><figDesc>Fig. 1. 1) The t-SNE visualization of the feature distribution of synthesized tumor images from one image by various methods demonstrates that the distortion problem exists. 2) Our DCAug can solve the above challenges through two novel contrastive learning strategies and one newly designed cross-cycle framework.</figDesc><graphic coords="2,66,36,78,74,323,92,76,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(2)). DCAug consists of two novel contrastive learning strategies, 1) Domain-aware Contrastive Learning (DaCL) and 2) Cross-domain Consistency Learning (CdCL) and one newly designed Cross-cycle Framework, focus on both domain and content information during sample generation, which reduces the content and domain distortion challenge present in existing tumor augmentation methods. Specifically, the core idea of DaCL is to associate the transferred tumor image with target domain examples while disassociating them from the source domain examples that are regarded as "negatives". The CdCL learning strategy is designed to preserve the domain-invariant content information in the synthetic tumor images for avoiding content distortion. When generating synthetic tumor images, CdCL and DaCL disentangle the tumor information into two solely independent parts: 1) Domain-invariant content information; 2) Individual-specific domain information. The domain-invariant content information is preserved for avoiding tumor content distortion through CdCL, and the individual-specific domain information is adaptively transferred by DaCL for eliminating the domain gap between true tumor image and synthetic tumor image. The above goal is achieved via our novel designed Cross-cycle Framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our cross-cycle framework disentangles the tumor information into two solely independent parts by two newly learning strategies for avoiding content distortion and eliminating the domain gap between true tumor and synthetic tumor.</figDesc><graphic coords="4,59,58,53,90,227,68,254,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>source image X A , and the combined image X b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Part of augmented samples produced by various data augmentation methods.</figDesc><graphic coords="7,44,79,60,50,334,60,173,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative comparison with same samples segmented by nnUNet trained by various data augmentation methods.</figDesc><graphic coords="8,60,33,62,84,333,16,192,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Means and standard deviations of the Dice coefficients (%) of the segmentation results on ATLAS/KiTS19 dataset.</figDesc><table><row><cell>KiTS19 25% 65.41 ± 31.93 62.82 ± 27.84 61.59 ± 30.36 64.20 ± 35.08 65.91 ± 29.54</cell><cell>65.97 ± 29.54 72.29 ± 29.08</cell></row><row><cell>50% 68.25 ± 24.41 68.29 ± 22.38 62.23 ± 31.07 75.31 ± 23.38 73.95 ± 27.81</cell><cell>75.04 ± 27.05 77.33 ± 27.48</cell></row><row><cell>100% 72.63 ± 24.40 73.94 ± 22.68 73.77 ± 29.68 79.99 ± 22.98 79.74 ± 20.43</cell><cell>79.04 ± 18.90 83.11 ± 14.15</cell></row></table><note><p>Dataset Num Means and Standard deviations of the Dice coefficients (%) TDA Mixup CutMix CarveMix SelfMix StyleMix DCAug ATLAS 25% 49.87 ± 32.19 49.18 ± 32.72 41.19 ± 33.98 55.16 ± 32.16 57.89 ± 31.05 52.84 ± 34.36 56.43 ± 32.33 50% 56.72 ± 30.74 58.40 ± 29.35 54.25 ± 30.24 58.34 ± 31.32 58.81 ± 31.75 58.04 ± 30.39 59.75 ± 31.41 100% 59.39 ± 32.45 59.33 ± 33.06 56.11 ± 32.44 62.32 ± 31.10 63.5 ± 31.06 64.00 ± 28.89 64.64 ± 29.91</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Means and standard deviations of the Dice coefficients (%) of the segmentation results on the test set for the ATLAS.</figDesc><table><row><cell cols="3">Method Dice coefficients (%) Method</cell><cell>Dice coefficients (%)</cell></row><row><cell>Mixup</cell><cell>59.33 ± 33.06</cell><cell>Mixup → DCAug</cell><cell>62.48 ± 31.10 (3.15↑)</cell></row><row><cell cols="2">CutMix 56.11 ± 32.44</cell><cell cols="2">CutMix → DCAug 64.64 ± 29.91 (8.53↑)</cell></row><row><cell cols="2">StyleMix 64.00 ± 28.89</cell><cell cols="2">StyleMix → DCAug 64.60 ± 29.93 (0.60↑)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by cross-innovation talent project in <rs type="funder">Renmin Hospital of Wuhan University</rs> (grant number <rs type="grantNumber">JCRCZN-2022-016</rs>); <rs type="funder">Natural Science Foundation of Hubei Province</rs> (grant number <rs type="grantNumber">2022CFB252</rs>); Undergraduate education quality construction comprehensive reform project (grant number <rs type="grantNumber">2022ZG282</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (grant number <rs type="grantNumber">81860276</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_epWUE8w">
					<idno type="grant-number">JCRCZN-2022-016</idno>
				</org>
				<org type="funding" xml:id="_qkVAVsA">
					<idno type="grant-number">2022CFB252</idno>
				</org>
				<org type="funding" xml:id="_eDxYPPz">
					<idno type="grant-number">2022ZG282</idno>
				</org>
				<org type="funding" xml:id="_SbBmzZr">
					<idno type="grant-number">81860276</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the content in {X A , X a→B A , XA }, {X B , X b→A B , XB } is same, by comparing the information inside {X A , X a→B A , XA }, {X B , X b→A B , XB }, the difference represents the change in the domain space. To further disentangle the individualspecific domain information, the DaCL is proposed:</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Domain Consistency Learning for Content Preservation</head><p>Cross-domain consistency learning (CdCL) strategy can preserve the domaininvariant content information of tumor in the synthesized images X b→A A , X a→B B for avoiding content distortion. Specifically, given the original images</p><p>produced by generator, and the reconstructed images XA , XB generated by the reversed process. The tumor can be first extracted from those images</p><p>Although the domain space is various, the tumor content inside</p><p>To evaluate the tumor content inside cross-domain images, the content consistency losses, including L A pixel (X A , XA ), L B pixel (X B , XB ), L a→B content , L b→A content , are computed between those images for supervising the content change. The details of content consistency loss are described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>In summary, three types of losses are used to supervise the cross-cycle framework. Specifically, given the original images X A , X B and the combined images X b A , X B a , the synthesized images X a→B B , X b→A A are produced by the generator, and the reconstructed images XA , XB are generated by the reversed process.</p><p>The pixel-wise loss (L pixel ) computes the difference between original images and reconstructed images at the pixel level.</p><p>To disentangle the individual-specific domain information, the higher feature representations extracted from pre-trained networks combined with CL are used:</p><p>And two content loss L b→A content , L a→B content are employed to maintain tumor content information during the domain adaptation:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compound domain generalization via meta-knowledge encoding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7119" to="7129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artistic style transfer with internal-external learning and contrastive learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26561" to="26573" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The kits19 challenge data: 300 kidney tumor cases with clinical context, CT semantic segmentations, and surgical outcomes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00445</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stylemix: separating content and style for enhanced data augmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14862" to="14870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NNU-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory-guided unsupervised image-to-image translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6558" to="6567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A content transformation block for image style transfer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kotovenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10032" to="10041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A large, open source dataset of stroke anatomical brain images and manual lesion segmentations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Region-aware adaptive instance normalization for image harmonization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9361" to="9370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CyCMIS: cycle-consistent cross-domain medical image segmentation via diverse image augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102328</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CutMix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CarveMix: a simple data augmentation method for brain lesion segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-219" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
	<note>Carvemix: A simple data augmentation method for brain lesion segmentation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boundary-weighted domain adaptive neural network for prostate MR image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="753" to="763" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SelfMix: a self-adaptive data augmentation method for lesion segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="683" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-865" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
