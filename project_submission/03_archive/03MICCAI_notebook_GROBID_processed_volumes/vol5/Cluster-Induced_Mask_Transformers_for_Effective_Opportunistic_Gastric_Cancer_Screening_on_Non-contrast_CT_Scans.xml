<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</title>
				<funder>
					<orgName type="full">Alibaba Group through Alibaba Research Intern Program</orgName>
				</funder>
				<funder ref="#_NBd9PCK">
					<orgName type="full">Clinical Medicine Plus X-Young Scholars Project of Peking University</orgName>
				</funder>
				<funder ref="#_G5WreUe #_e8DXhhf">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingze</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingda</forename><surname>Xia</surname></persName>
							<email>yingda.xia@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Guangdong Province People&apos;s Hospital</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junli</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">The First Affiliated Hospital of Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyan</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hexin</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Peking University Changsha Institute for Computing and Digital Economy</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zaiyi</forename><surname>Liu</surname></persName>
							<email>zyliu@163.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Guangdong Province People&apos;s Hospital</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="146" to="156"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0B12CE615965AA55BC45CDB47DBB4B37</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gastric cancer</term>
					<term>Large-scale cancer screening</term>
					<term>Mask Transformers</term>
					<term>Non-contrast CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gastric cancer is the third leading cause of cancer-related mortality worldwide, but no guideline-recommended screening test exists. Existing methods can be invasive, expensive, and lack sensitivity to identify early-stage gastric cancer. In this study, we explore the feasibility of using a deep learning approach on non-contrast CT scans for gastric cancer detection. We propose a novel cluster-induced Mask Transformer that jointly segments the tumor and classifies abnormality in a multi-task manner. Our model incorporates learnable clusters that encode the texture and shape prototypes of gastric cancer, utilizing self-and cross-attention to interact with convolutional features. In our experiments, the proposed method achieves a sensitivity of 85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test set consisting of 100 patients with cancer and 148 normal. In comparison, two radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We also obtain a specificity of 97.7% on an external test set with 903 normal cases. Our approach performs comparably to established state-of-the-art gastric cancer screening tools like blood testing and endoscopy, while also being more sensitive in detecting early-stage cancer. This demonstrates the potential of our approach as a novel, noninvasive, low-cost, and accurate method for opportunistic gastric cancer screening.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gastric cancer (GC) is the third leading cause of cancer-related deaths worldwide <ref type="bibr" target="#b18">[19]</ref>. The five-year survival rate for GC is approximately 33% <ref type="bibr" target="#b15">[16]</ref>, which is mainly attributed to patients being diagnosed with advanced-stage disease harboring unresectable tumors. This is often due to the latent and nonspecific signs and symptoms of early-stage GC. However, patients with early-stage disease have a substantially higher five-year survival rate of around 72% <ref type="bibr" target="#b15">[16]</ref>. Therefore, early detection of resectable/curable gastric cancers, preferably before the onset of symptoms, presents a promising strategy to reduce associated mortality. Unfortunately, current guidelines do not recommend any screening tests for GC <ref type="bibr" target="#b21">[22]</ref>. While several screening tools have been developed, such as Barium-meal gastric photofluorography <ref type="bibr" target="#b4">[5]</ref>, upper endoscopy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>, and serum pepsinogen levels <ref type="bibr" target="#b14">[15]</ref>, they are challenging to apply to the general population due to their invasiveness, moderate sensitivity/specificity, high cost, or side effects. Therefore, there is an urgent need for novel screening methods that are noninvasive, highly accurate, low-cost, and ready to distribute.</p><p>Non-contrast CT is a commonly used imaging protocol for various clinical purposes. It is a non-invasive, relatively low-cost, and safe procedure that exposes patients to less radiation dose and does not require the use of contrast injection that may cause serious side effects (compared to multi-phase contrastenhanced CT). With recent advances in AI, opportunistic screening of diseases using non-contrast CT during routine clinical care performed for other clinical indications, such as lung and colorectal cancer screening, presents an attractive approach to early detect treatable and preventable diseases <ref type="bibr" target="#b16">[17]</ref>. However, whether early detection of gastric cancer using non-contrast CT scans is possible remains unknown. This is because early-stage gastric tumors may only invade the mucosal and muscularis layers, which are difficult to identify without the help of stomach preparation and contrast injection. Additionally, the poor contrast between the tumor and normal stomach wall/tissues on non-contrast CT scans and various shape alterations of gastric cancer, further exacerbates this challenge.</p><p>In this paper, we propose a novel approach for detecting gastric cancer on non-contrast CT scans. Unlike the conventional "segmentation for classification" methods that directly employ segmentation networks, we developed a clusterinduced Mask Transformer that performs segmentation and global classification simultaneously. Given the high variability in shape and texture of gastric cancer, we encode these features into learnable clusters and utilize cluster analysis during inference. By incorporating self-attention layers for global context modeling, our model can leverage both local and global cues for accurate detection. In our experiments, the proposed approach outperforms nnUNet <ref type="bibr" target="#b7">[8]</ref> by 0.032 in AUC, 5.0% in sensitivity, and 4.1% in specificity. These results demonstrate the potential of our approach for opportunistic screening of gastric cancer in asymptomatic patients using non-contrast CT scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automated Cancer Detection. Researchers have explored automated tumor detection techniques on endoscopic <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, pathological images <ref type="bibr" target="#b19">[20]</ref>, and the prediction of cancer prognosis <ref type="bibr" target="#b11">[12]</ref>. Recent developments in deep learning have significantly improved the segmentation of gastric tumors <ref type="bibr" target="#b10">[11]</ref>, which is critical for their detection. However, our framework is specifically designed for noncontrast CT scans, which is beneficial for asymptomatic patients. While previous studies have successfully detected pancreatic <ref type="bibr" target="#b24">[25]</ref> and esophageal <ref type="bibr" target="#b25">[26]</ref> cancers on non-contrast CT, identifying gastric cancer presents a unique challenge due to its subtle texture changes, various shape alterations, and complex background, e.g., irregular gastric wall; liquid and contents in the stomach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask Transformers.</head><p>Recent studies have used Transformers for natural and medical image segmentation <ref type="bibr" target="#b20">[21]</ref>. Mask Transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref> further enhance CNN-based backbones by incorporating stand-alone Transformer blocks, treating object queries in DETR <ref type="bibr" target="#b0">[1]</ref> as memory-encoded queries for segmentation. CMT-Deeplab <ref type="bibr" target="#b26">[27]</ref> and KMaX-Deeplab <ref type="bibr" target="#b27">[28]</ref> have recently proposed interpreting the queries as clustering centers and adding regulatory constraints for learning the cluster representations of the queries. Mask Transformers are locally sensitive to image textures for precise segmentation and globally aware of organtumor morphology for recognition. Their cluster representations demonstrate a remarkable balance of intra-cluster similarity and inter-class discrepancy. Therefore, Mask Transformers are an ideal choice for an end-to-end joint segmentation and classification system for detecting gastric cancer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Problem Formulation. Given a non-contrast CT scan, cancer screening is a binary classification with two classes as L = {0, 1}, where 0 stands for"normal" and 1 for"GC" (gastric cancer). The entire dataset is denoted by</p><formula xml:id="formula_0">S = {(X i , Y i , P i )|i = 1, 2, • • • , N}</formula><p>, where X i is the i-th non-contrast CT volume, with Y i being the voxel-wise label map of the same size as X i and K channels. Here, K = 3 represents the background, stomach, and GC tumor. P i ∈ L is the class label of the image, confirmed by pathology, radiology, or clinical records. In the testing phase, only X i is given, and our goal is to predict a class label for X i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.</head><p>To address difficulties with tumor annotation on non-contrast CTs, the radiologists start by annotating a voxel-wise tumor mask on the contrast-enhanced CT, referring to clinical and endoscopy reports as needed. DEEDs <ref type="bibr" target="#b5">[6]</ref> registration is then performed to align the contrast-enhanced CT with the non-contrast CT and the resulting deformation field is applied to the annotated mask. Any misaligned ones are revised manually. In this manner (Fig. <ref type="figure" target="#fig_0">1d</ref>), a relatively coarse yet highly reliable tumor mask can be obtained for the non-contrast CT image. Cluster-Induced Classification with Mask Transformers. Segmentation for classification is widely used in tumor detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>. We first train a UNet <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> to segment the stomach and tumor regions using the masks from the previous step. This UNet considers local information and can only extract stomach ROIs well during testing. However, local textures are inadequate for accurate gastric tumor detection on non-contrast CTs, so we need a network of both local sensitivity to textures and global awareness of the organ-tumor morphology. Mask transformer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref> is a well-suited approach to boost the CNN backbone with stand-alone transformer blocks. Recent studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> suggest interpreting object queries as cluster centers, which naturally exhibit intra-cluster similarity and inter-class discrepancy. Inspired by this, we further develop a deep classification model on top of learnable cluster representations.</p><p>Specifically, given image X ∈ R H×W ×D , annotation Y ∈ R K×HW D , and patient class P ∈ L, our model consists of three components: 1) a CNN backbone to extract its pixel-wise features F ∈ R C×HW D (Fig. <ref type="figure" target="#fig_0">1a</ref>), 2) a transformer module (Fig. <ref type="figure" target="#fig_0">1b</ref>), and 3) a multi-task cluster inference module (Fig. <ref type="figure" target="#fig_0">1c</ref>). The transformer module gradually updates a set of randomly initialized object queries C ∈ R N ×C , i.e., to meaningful mask embedding vectors through cross-attention between object queries and multi-scale pixel features,</p><formula xml:id="formula_1">C ← C + arg max N (Q c (K p ) T )V p , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where c and p stand for query and pixel features, Q c , K p , V p represent linearly projected query, key, and value. We adopt cluster-wise argmax from KMax-DeepLab <ref type="bibr" target="#b27">[28]</ref> to substitute spatial-wise softmax in the original settings.</p><p>We further interpret the object queries as cluster centers from a cluster analysis perspective. All the pixels in the convolutional feature map are assigned to different clusters based on these centers. The assignment of clusters (a.k.a. mask prediction) M ∈ R N ×HW D is computed as the cluster-wise softmax function over the matrix product between the cluster centers C and pixel-wise feature matrix F, i.e.,</p><formula xml:id="formula_3">M = Softmax N (R) = Softmax N (CF).<label>(2)</label></formula><p>The final segmentation logits Z ∈ R K×HW D are obtained by aggregating the pixels within each cluster according to cluster-wise classification, which treats pixels within a cluster as a whole. The aggregation of pixels is achieved by Z = C K M, where the cluster-wise classification C K is represented by an MLP that projects the cluster centers C to K channels (the number of segmentation classes).</p><p>The learned cluster centers possess high-level semantics with both intercluster discrepancy and intra-cluster similarity for effective classification. Rather than directly classifying the final feature map, we first generate the clusterpath feature vector by taking the channel-wise average of cluster centers C =</p><formula xml:id="formula_4">1 N i=1 C i ∈ R C .</formula><p>Additionally, to enhance the consistency between the segmentation and classification outputs, we apply global max pooling to cluster assignments R to obtain the pixel-path feature vector R ∈ R N . This establishes a direct connection between classification features and segmentation predictions. Finally, we concatenate these two feature vectors to obtain the final feature and project it onto the classification prediction P ∈ R 2 via a two-layer MLP.</p><p>The overall training objective is formulated as,</p><formula xml:id="formula_5">L = L seg (Z, Y) + L cls ( P, P),<label>(3)</label></formula><p>where the segmentation loss L seg (•, •) is a combination of Dice and cross entropy losses, and the classification loss L cls (•, •) is cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset and Ground Truth. Our study analyzed a dataset of CT scans collected from Guangdong Province People's Hospital between years 2018 and 2020, with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. We used the latest patients in the second half of 2020 as a hold-out test set, resulting in a training set of 687 gastric cancer and 1,204 normal cases, and a test set of 100 gastric cancer and 148 normal cases. We randomly selected 20% of the training data as an internal validation set. To further evaluate specificity in a larger population, we collected an external test set of 903 normal cases from Shengjing Hospital. Cancer cases were confirmed through endoscopy (and pathology) reports, while normal cases were confirmed by radiology reports and a two-year follow-up. All patients underwent multi-phase CTs with a median spacing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. Tumors were annotated on the venous phase by an experienced radiologist specializing in gastric imaging using CTLabeler <ref type="bibr" target="#b22">[23]</ref>, while the stomach was automatically annotated using a self-learning model <ref type="bibr" target="#b30">[31]</ref>.</p><p>Implementation Details. We resampled each CT volume to the median spacing while normalizing it to have zero mean and unit variance. During training, we cropped the 3D bounding box of the stomach and added a small margin of <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4)</ref>. We used nnUNet <ref type="bibr" target="#b7">[8]</ref> as the backbone, with four transformer decoders, each taking pixel features with output strides of 32, 16, 8, and 4. We set the number of object queries N to 8, with each having a dimension of 128, and included an eight-head self-attention layer in each block. The patch size used during training and inference is (192, 224, 40) voxel. We followed <ref type="bibr" target="#b7">[8]</ref> to augment data. We trained the model with RAdam using a learning rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs, with a frozen backbone of the pretrained nnUNet <ref type="bibr" target="#b7">[8]</ref> for the first 50 epochs. To enhance performance, we added deep supervision by aligning the cross-attention map with the final segmentation map, as per KMax-Deeplab <ref type="bibr" target="#b26">[27]</ref>. The hidden layer dimension in the two-layer MLP is 128. We also trained a standard UNet <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> to localize the stomach region in the entire image in the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics and Reader Study.</head><p>For the binary classification, model performance is evaluated using area under ROC curve (AUC), sensitivity (Sens.), and specificity (Spec.). And successful localization of the tumors is considered when the overlap between the segmentation mask generated by the model and the ground truth is greater than 0.01, measured by the Dice score. A reader study was conducted with two experienced radiologists, one from Guangdong Province People's Hospital with 20 years of experience and the other from The First Affiliated Hospital of Zhejiang University with 9 years of experience in gastric imaging. The readers were given 248 non-contrast CT scans from the test set and asked to provide a binary decision for each scan, indicating whether the scan showed gastric cancer. No patient information or records were provided to the readers. Readers were informed that the dataset might contain more tumor cases than the standard prevalence observed in screening, but the proportion of case types was not disclosed. Readers used ITK-SNAP <ref type="bibr" target="#b29">[30]</ref> to interpret the CT scans without any time constraints.  <ref type="table" target="#tab_0">1</ref> presents a comparative analysis of our proposed method with three baselines. The first two approaches belong to "Segmentation for classification" (S4C) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>, using nnUNet <ref type="bibr" target="#b7">[8]</ref> and TransUNet <ref type="bibr" target="#b1">[2]</ref>. A case is classified as positive if the segmented tumor volume exceeds a threshold that maximizes the sum of sensitivity and specificity on the validation set. The third baseline (denoted as "nnUNet-Joint") integrates a CNN classification head into UNet <ref type="bibr" target="#b7">[8]</ref> and trained end-to-end. We obtain the 95% confidence interval of AUC, sensitivity, and specificity values from 1000 bootstrap replicas of the test dataset for statistical analysis. For statistical significance, we conduct a DeLong test between two AUCs (ours vs. compared method) and a permutation test between two sensitivities or specificities (ours vs. compared method and radiologists).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Our method Outperforms Baselines. Our method outperforms three baselines (Table <ref type="table" target="#tab_0">1</ref>) in all metrics, particularly in AUC and sensitivity. The advantage of our approach is that it captures the local and global information simultaneously in virtue of the unique architecture of mask transformer. It also extracts high-level semantics from cluster representations, making it suitable for classification and facilitating a holistic decision-making process. Moreover, our method reaches a considerable specificity of 97.7% on the external test set, which is crucial in opportunistic screening for less false positives and unnecessary human workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI Models Surpass Experienced Radiologists on Non-contrast CT Scans.</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2a</ref>, our AI model's ROC curve is superior to that of two experienced radiologists. The model achieves a sensitivity of 85.0% in detecting gastric cancer, which significantly exceeds the mean performance of doctors (73.5%) and also surpasses the best performing doctor (R2: 75.0%), while maintaining a high specificity. A visual example is presented in Fig. <ref type="figure" target="#fig_1">2b</ref>. This early-stage cancer (T1) is miss-detected by both radiologists, whereas classified and localized precisely by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgroup Analysis.</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we report the performance of patient-level detection and tumor-level localization stratified by tumor (T) stage. We compare our model's performance with that of both radiologists. The results show that our model performs better in detecting early stage tumors (T1, T2) and provides more precise tumor localization. Specifically, our model detects 60.0% (6/10) T1 cancers, and 77.8% (7/9) T2 cancers, surpassing the best performing expert (50% T1, 55.6% T2). Meanwhile, our model maintains a reliable detection rate and credible localization accuracy for T3 and T4 tumors (2 of 34 T3 tumors missed).</p><p>Comparison with Established Screening Tools. Our method surpasses or performs on par with established screening tools <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> in terms of sensitivity for gastric cancer detection at a similar specificity level with a relatively large testing patient size (n = 1151 by integrating the internal and external test sets), as shown in Table <ref type="table" target="#tab_2">3</ref>. This finding sheds light on the opportunity to employ automated AI systems to screen gastric cancer using non-contrast CT scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel Cluster-induced Mask Transformer for gastric cancer detection on non-contrast CT scans. Our approach outperforms strong baselines and experienced radiologists. Compared to other screening methods, such as blood tests, endoscopy, upper-gastrointestinal series, and ME-NBI, our approach is non-invasive, cost-effective, safe, and more accurate for detecting early-stage tumors. The robust performance of our approach demonstrates its potential for opportunistic screening of gastric cancer in the general population.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Method overview. (a) The non-contrast CT image is first forwarded into a U-Net [8, 18] to extract a feature map. (b) Learnable object queries interact with the multi-level U-Net features through a Transformer Decoder and produce learned cluster centers. (c) All the pixels are assigned to cluster centers by matrix multiplication. The cluster assignment (a.k.a. mask prediction) is further used to generate the final segmentation output and the classification probability. (d) The entire network is supervised by transferred masks from radiologists' annotation on contrast-enhanced CT and endoscopy or pathology-confirmed ground truth.</figDesc><graphic coords="4,58,98,54,26,334,48,131,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) ROC curve for our model versus two experts on the hold-out test set of n = 248 patients for binary classification. (b) A visual example in the test set. This early-stage GC case is miss-detected by both radiologists and nnUNet [8] but our model succeeds to locate the tumor.</figDesc><graphic coords="6,252,96,54,11,136,00,168,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on binary classification: gastric cancer vs. normal. The 95% confidence interval of each metric is listed. †: p &lt; 0.05 for DeLong test (ours vs. nnUNet-S4C). *: p &lt; 0.05 for permutation test (ours vs. nnUNet-S4C and radiologist experts). Sens.: Sensitivity. Spec.: Specificity.</figDesc><table><row><cell></cell><cell cols="2">Internal Hold-out (n = 248)</cell><cell></cell><cell>External (n = 903)</cell></row><row><cell>Method</cell><cell>AUC</cell><cell>Sens.(%)</cell><cell>Spec.(%)</cell><cell>Spec.(%)</cell></row><row><cell cols="2">Mean of radiologists -</cell><cell>73.5</cell><cell>84.1</cell><cell>-</cell></row><row><cell>nnUNet-S4C [8]</cell><cell>0.907</cell><cell>80.0</cell><cell>88.5</cell><cell>96.6</cell></row><row><cell></cell><cell cols="4">(0.862, 0.942) (72.0, 87.5) (83.3, 93.5) (95.2, 97.8)</cell></row><row><cell cols="2">TransUNet-S4C [2] 0.916</cell><cell>82.0</cell><cell>90.5</cell><cell>96.0</cell></row><row><cell></cell><cell cols="4">(0.876, 0.952) (74.7, 89.5) (86.1, 94.8) (94.8, 97.2)</cell></row><row><cell>nnUNet-Joint [8]</cell><cell>0.924</cell><cell>81.0</cell><cell>90.5</cell><cell>97.6</cell></row><row><cell></cell><cell cols="4">(0.885, 0.959) (73.0, 87.9) (85.1, 95.0) (96.5, 98.6)</cell></row><row><cell>Ours</cell><cell>0.939  †</cell><cell>85.0  *</cell><cell>92.6  *</cell><cell>97.7</cell></row><row><cell></cell><cell cols="4">(0.910, 0.964) (78.1, 91.1) (88.0, 96.5) (96.7, 98.7)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Patient-level detection and tumor-level localization results (%) over gastric cancer across different T-stages. Tumor-level localization evaluates how segmented masks overlap with the ground-truth cancer (Dice &gt; 0.01 for correct detection). Miss-T: Missing of T stage information.</figDesc><table><row><cell>Method</cell><cell>Criteria T1</cell><cell>T2</cell><cell>T3</cell><cell>T4</cell><cell>Miss-T</cell></row><row><cell cols="6">nnUNet-Joint [8] Patient 30.0(3/10) 66.7(6/9) 94.1(32/34) 100.0(9/9) 86.1(31/36)</cell></row><row><cell></cell><cell cols="5">Tumor 20.0(2/10) 55.6(5/9) 94.1(32/34) 100.0(9/9) 80.6(29/36)</cell></row><row><cell>Ours</cell><cell cols="5">Patient 60.0(6/10) 77.8(7/9) 94.1(32/34) 100.0(9/9) 86.1(31/36)</cell></row><row><cell></cell><cell cols="5">Tumor 30.0(3/10) 66.7(6/9) 94.1(32/34) 100.0(9/9) 80.6(30/36)</cell></row><row><cell>Radiologist 1</cell><cell cols="5">Patient 50.0(5/10) 55.6(5/9) 76.5(26/34) 88.9(8/9) 77.8(28/36)</cell></row><row><cell>Radiologist 2</cell><cell cols="5">Patient 30.0(3/10) 55.6(5/9) 85.3(29/34) 100.0(9/9) 80.6(29/36)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with a state-of-the-art blood test on gastric cancer detection<ref type="bibr" target="#b9">[10]</ref>, UGIS and endoscopy screening performance in large population<ref type="bibr" target="#b3">[4]</ref>, and early stage gastric cancer detection rate of senior radiologists on narrow-band imaging with magnifying endoscopy (ME-NBI)<ref type="bibr" target="#b6">[7]</ref>. ( * : We leave out two tumors in situ within the test set in accordance with the setting in<ref type="bibr" target="#b9">[10]</ref>. †: We also merely consider early-stage gastric cancer cases, including Tumor in situ, T1, and T2 stages, among whom we successfully detect 17 of 19 cases.)</figDesc><table><row><cell>Method</cell><cell cols="3">Spec.(%) Sens.(%) Our sensitivity(%) at the same specificity</cell></row><row><cell>Blood Test [10]</cell><cell>99.5</cell><cell>66.7</cell><cell>69.4  *</cell></row><row><cell cols="2">Upper-gastrointestinal series [4] 96.1</cell><cell>36.7</cell><cell>85.0</cell></row><row><cell>Endoscopy screening [4]</cell><cell>96.0</cell><cell>69.0</cell><cell>85.0</cell></row><row><cell>ME-NBI (early-stage) [7]</cell><cell>74.2</cell><cell>76.7</cell><cell>89.5</cell></row></table><note><p><p>†</p>Compared Baselines. Table</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">Alibaba Group through Alibaba Research Intern Program</rs>. <rs type="person">Bin Dong</rs> and <rs type="person">Li Zhang</rs> was partly supported by <rs type="funder">NSFC</rs> <rs type="grantNumber">12090022</rs> and <rs type="grantNumber">11831002</rs>, and <rs type="funder">Clinical Medicine Plus X-Young Scholars Project of Peking University</rs> <rs type="grantNumber">PKU2023LCXQ041</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G5WreUe">
					<idno type="grant-number">12090022</idno>
				</org>
				<org type="funding" xml:id="_e8DXhhf">
					<idno type="grant-number">11831002</idno>
				</org>
				<org type="funding" xml:id="_NBd9PCK">
					<idno type="grant-number">PKU2023LCXQ041</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TransuNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17864" to="17875" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of different gastric cancer screening methods in Korea: a population-based study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">50041</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Japanese guidelines for gastric cancer screening</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hamashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jpn. J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="267" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MRF-based deformable registration and ventilation estimation of lung CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1239" to="1248" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying early gastric cancer under magnifying narrow-band images with deep learning: a multicenter study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1341" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NNU-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effectiveness of the Korean national cancer screening program in reducing gastric cancer mortality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1319" to="1328" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clinical validation of a targeted methylation-based multi-cancer early detection test using an independent validation set</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Oncol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1177" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d IFPN: improved feature pyramid network for automatic segmentation of gastric tumor</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">618496</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CT-based delta radiomics in predicting the prognosis of stage iv gastric cancer to immune checkpoint inhibitors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1059874</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network for the diagnosis of early gastric cancer based on magnifying narrow band imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastric Cancer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="126" to="132" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time artificial intelligence for detection of upper gastrointestinal cancer by endoscopy: a multicentre, case-control, diagnostic study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1645" to="1654" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gastric cancer screening using the serum pepsinogen test method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastric Cancer</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://seer.cancer.gov/statfacts/html/stomach.html" />
		<title level="m">Cancer stat facts: Stomach cancer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>National Cancer Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Value-added opportunistic CT screening: state of the art</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Pickhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="254" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gastric cancer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Grabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Van Grieken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lordick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">396</biblScope>
			<biblScope unit="page" from="635" to="648" />
			<date type="published" when="2020">10251. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clinically applicable histopathological diagnosis system for gastric cancer detection using deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4294</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised pre-training of Swin transformers for 3d medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://www.uspreventiveservicestaskforce.org/uspstf/" />
		<title level="m">topic search results?topic status=P</title>
		<imprint>
			<publisher>USPSTF: U.S. Preventive Services Task Force</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A cascaded approach for ultraly high performance lesion detection and false positive removal in liver CT scans</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16036</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Max-deeplab: end-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effective pancreatic cancer screening on non-contrast CT scans via anatomy-aware transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_25</idno>
		<idno>978-3-030-87240-3 25</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="259" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective opportunistic esophageal cancer screening using noncontrast CT imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CMT-DeepLab: clustering mask transformers for panoptic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2560" to="2570" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">k-means mask transformer</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19818-2_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19818-217" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision. ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13689</biblScope>
			<biblScope unit="page" from="288" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Devil is in the queries: advancing mask transformers for realworld medical image segmentation and out-of-distribution localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23879" to="23889" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">User-guided 3d active contour segmentation of anatomical structures: significantly improved efficiency and reliability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Selflearning to detect and segment cysts in lung CT images without manual annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1100" to="1103" />
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale coarse-to-fine segmentation for screening pancreatic ductal adenocarcinoma</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-71" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
