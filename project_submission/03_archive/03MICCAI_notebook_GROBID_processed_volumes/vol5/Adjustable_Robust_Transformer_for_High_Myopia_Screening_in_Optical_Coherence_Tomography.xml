<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography</title>
				<funder ref="#_kkRJcJB">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_5genhJB #_7Ec5nVd">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zetian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zexuan</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Na</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Nanjing Medical University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Songtao</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Nanjing Medical University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<email>chen2qiang@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>0 -3 -6 -9 -12</addrLine>
									<postCode>22 24 26 28 30</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="504" to="514"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9B71C58E9B6A28D619588AC702DC3357</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High myopia screening</term>
					<term>Optical coherence tomography</term>
					<term>Adjustable model</term>
					<term>Label noise learning High Deviation Low Deviation Regression Sample Experience</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Myopia is a manifestation of visual impairment caused by an excessively elongated eyeball. Image data is critical material for studying high myopia and pathological myopia. Measurements of spherical equivalent and axial length are the gold standards for identifying high myopia, but the available image data for matching them is scarce. In addition, the criteria for defining high myopia vary from study to study, and therefore the inclusion of samples in automated screening efforts requires an appropriate assessment of interpretability. In this work, we propose a model called adjustable robust transformer (ARTran) for high myopia screening of optical coherence tomography (OCT) data. Based on vision transformer, we propose anisotropic patch embedding (APE) to capture more discriminative features of high myopia. To make the model effective under variable screening conditions, we propose an adjustable class embedding (ACE) to replace the fixed class token, which changes the output to adapt to different conditions. Considering the confusion of the data at high myopia and low myopia threshold, we introduce the label noise learning strategy and propose a shifted subspace transition matrix (SST) to enhance the robustness of the model. Besides, combining the two structures proposed above, the model can provide evidence for uncertainty evaluation. The experimental results demonstrate the effectiveness and reliability of the proposed method. Code is available at: https://github.com/maxiao0234/ARTran.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Myopia, resulting in blurred distance vision, is one of the most common eye diseases, with a rising prevalence around the world, particularly among schoolchildren and young adults <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. Common wisdom attributes the causes of myopia to excessive elongation of the eyeball, the development of which can continue throughout childhood, and especially in patients with high myopia, throughout adulthood <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. In the coming decades, the prognosis for patients with high myopia will continue to deteriorate, with some developing pathological myopia, leading to irreversible vision damage involving posterior scleral staphyloma, macular retinoschisis, macular hole, retinal detachment, choroidal neovascularization, dome-shaped macula, etc. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>As a crucial tool in the study of high myopia, fundus images demonstrate the retinal changes affected by myopia. Myopic maculopathy in color fundus photographs (CFP) can be important evidence in the evaluation of high myopia and myopic fundus diseases <ref type="bibr" target="#b20">[21]</ref>. However, some myopic macular lesions such as myopic traction maculopathy and domeshaped macula are usually not observed in CFP. Optical coherence tomography (OCT), characterized by non-invasive and high-resolution three-dimensional retinal structures, has more advantages in the examination of high myopia <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. Several studies have used convolutional neural networks to automatically diagnose high myopia and pathological myopia <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. Choi et al. employed two ResNet-50 <ref type="bibr" target="#b9">[10]</ref> networks to inference vertical and horizontal OCT images simultaneously. Li et al. introduced focal loss into Inception-Resnet-V2 <ref type="bibr" target="#b23">[24]</ref> to enhance its identification ability. However, the different classes of images in these tasks have significant differences, and the performance of the model, when used for more complex screening scenarios, is not discussed. Hence, this work aims to design a highly generalizable screening model for high myopia on OCT images.</p><p>There exist challenges to developing an automatic model that meets certain clinical conditions. For the inclusion criteria for high myopia, different studies will expect different outputs under different thresholds. High myopia is defined by a spherical equivalent (SE) ≤ -6.0 dioptres (D) or an axial length (AL) ≥ 26.0mm in most cases, but some researchers set the threshold of SE to -5.0D <ref type="bibr" target="#b3">[4]</ref> or -8.0D <ref type="bibr" target="#b19">[20]</ref>. Moreover, some scenarios where screening risk needs to be controlled may modify the thresholds appropriately. To remedy this issue, the screening scheme should ideally output compliant results for different thresholds. For the accuracy of supervision, although a worse SE has a higher chance of causing structural changes in the retina (and vice versa), the two are not the single cause and effect, i.e., there are natural label noises when constructing datasets. One direct piece of evidence is that both measures of SE and AL can be used as inclusion criteria for high myopia, but there is disagreement in some samples. We illustrate the distribution of samples with both SE and AL in our dataset in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Clinical experience considers that AL and SE can be roughly estimated from each other using a linear relationship, which is indicated by the red line in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Most of the samples are located in the area (purple) that satisfies both conditions, but the remaining samples can only satisfy one. In more detail, Fig. <ref type="figure" target="#fig_0">1(b)</ref> shows three samples with a low deviation of the experience trend (the top row) and three samples with a high deviation (the bottom row), where the worse SE does not always correspond to longer AL or more retinal structural changes. To mitigate degenerate representation caused by noisy data, the screening scheme should avoid over-fitting of extremely confusing samples. Besides, rich interpretable decisions support enhancing confidence in screening results. To this end, the screening scheme should evaluate the uncertainty of the results.</p><p>The contributions of our work are summarized as: <ref type="bibr" target="#b0">(1)</ref> we propose a novel adjustable robust transformer (ARTran) for high myopia screening to adapt variable inclusion criteria. We design an anisotropic patch embedding (APE) to encode more myopia-related information in OCT images, and an adjustable class embedding (ACE) to obtain adjustable inferences. <ref type="bibr" target="#b1">(2)</ref> We propose shifted subspace transition matrix (SST) to mitigate the negative impacts of label noise, which maps the class-posteriors to the corresponding distribution range according to the variable inclusion criteria. (3) We implement our ARTran on a high myopia dataset and verify the effectiveness of screening, and jointly use the proposed modules to generate multi-perspective uncertainty evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we propose a novel framework for high myopia screening in OCT called adjustable robust transformer (ARTran). This model can obtain the corresponding decisions based on the given threshold of inclusion criteria for high myopia and is trained end-to-end for all thresholds at once. During the testing phase, the screening results can be predicted interactively for a given condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modified Vision Transformer for Screening</head><p>Transformers have shown promising performance in visual tasks attributed to long-range dependencies. Inspired by this, we use ViT <ref type="bibr" target="#b7">[8]</ref> as the backbone of our  framework and make improvements for the task of screening high myopia OCT images.</p><p>Patients with high myopia are usually accompanied by directional structural changes, such as increased retinal curvature and choroidal thinning. On the other hand, due to the direction of light incidence perpendicular to the macula, OCT images have unbalanced information in the horizontal and vertical directions. Therefore, we propose a novel non-square strategy called anisotropic patch embedding (APE) to replace vanilla patch embedding for perceiving finer structural information, where an example is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. First, we reduced the height size of the patches. This strategy increases the number of patches per column, i.e., the number of patches within the retinal height range, which enhances the information perception of overall and individual layer thickness. In order not to introduce extra operations, we reduce the number of patches per row. We also use an overlapping sampling strategy for sliding windows in the horizontal direction, where a wider patch captures more information about the peripheral structure of the layer in which it is located. Specifically, based on the 16×16 size of ViT, our APE changed the height to 8 pixels and the width to 56 pixels with an overlap of 28 pixels, which keeps the number of embedded patches.</p><p>Considering that different researchers may have different requirements for inclusion criteria or risk control, we propose adjustable class embedding (ACE) to adapt to variable conditions. Take the -6.0D as the benchmark of inclusion criteria, we define the relationship of the biased label, SE, and the adjustment coefficient Δ:</p><formula xml:id="formula_0">Y (SE = s, Δ = δ) := 0 , -0.25D • δ + s &gt; -6.0D 1 , -0.25D • δ + s ≤ -6.0D , -1 ≤ δ ≤ 1<label>(1)</label></formula><p>where 1 indicates the positive label and 0 indicates the negative label. Our ACE introduces two parameterized vectors v 1 and v 2 , and constructs a linear combination with the given δ to obtain v AP E (δ) to replace the fixed class token:</p><formula xml:id="formula_1">v AP E (δ) = 1 + δ 2 • v 1 + 1 -δ 2 • v 2<label>(2)</label></formula><p>where v AP E (0) is the equal combination and others are biased combinations.</p><p>Several studies have demonstrated impressive performance in multi-label tasks using transformers with multiple class tokens <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. Inspired by this, we set v AP E (0) as the benchmark class embedding and v AP E (δ) as the biased class embedding. In the training stage, the inclusion threshold is varied around the benchmark, affecting the supervision consequently. The ACE adaptively changes the input state according to the scale of the adjustment coefficient to obtain the corresponding output. The scheme for constructing loss functions using two outputs is described in Sect. 2.2. In the testing stage, the ACE interactively makes the model output different results depending on the given conditions. Furthermore, we did not add the position encoding to ACE, so both tokens are position-independently involved to multi-head self-attention and are only distinguished by adjustment coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shifted Subspace Transition Matrix</head><p>To enhance the robustness of the model, we follow the common assumptions of some impressive label noise learning methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. Conventional wisdom is that the clean class-posterior P (Y |X = x) can be inferred by utilizing the noisy class-posterior P ( Ỹ |X = x) and the transition matrix T (x):</p><formula xml:id="formula_2">P ( Ỹ |X = x) = T (x)P (Y |X = x)<label>(3)</label></formula><p>where the transition matrix guarantees statistical consistency. Li et al. <ref type="bibr" target="#b13">[14]</ref> have proved that identifying the transition matrix can be treated as the problem of recovering simplex for any x, i.e., T (x) = T . Based on this theory, in this work, we further propose a class-dependent and adjustment-dependent transition matrix T (x, δ) = T (δ) called shifted subspace transition matrix (SST) to adapt to the variable class-posteriors distribution space. Simplistically, this work only discusses the binary classification approach applicable to screening. For each specific inclusion threshold δ ∈ [-1, 1], the range of the noisy class-posterior is determined jointly by the threshold and the SST:</p><formula xml:id="formula_3">P ( Ỹ = 0|x, δ) P ( Ỹ = 1|x, δ) = T 1,1 (δ) 1 -T 2,2 (δ) 1 -T 1,1 (δ) T 2,2 (δ) P (Y = 0|x, δ) P (Y = 1|x, δ)<label>(4)</label></formula><p>where T i,i (δ) &gt; 0.5 is the i th diagonal element of SST, and the sum of each column of the matrix is 1. Thus any class-posterior of x is inside the simplex form columns of T (δ) <ref type="bibr" target="#b1">[2]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the benchmark simplex is formed by T (0), where the orange arrows indicate the two sides of the simplex. When adjusting the inclusion criteria for high myopia, we expect the adjusted classposterior to prefer same the adjustment direction compared to the benchmark. One solution is to offset both the upper and lower bounds of the class-posterior according to the scales of the adjustment coefficient:</p><formula xml:id="formula_4">T i,i (δ) = 1 + S(θ 0 ) • S(θ i ) 2 + 1 -S(θ 0 ) 4 • 1 -δ ,i = 1 1 + δ ,i = 2<label>(5)</label></formula><p>where the S(•) is the Sigmoid function, θ i is the parameter used only for column, and θ 0 is the parameter shared by both columns. Adjusting the δ is equivalent to shifting the geometric space of the simplex, where the spaces with a closer adjustment coefficient share a more common area. This ensures that the distribution of the noise class-posterior has a strong consistency with the adjustment coefficient. Furthermore, the range distribution of any T (δ) is one subspace of an extended transition matrix T Σ , whose edges is defined as the edges of T (-1) and T (1):</p><formula xml:id="formula_5">T Σ i,i = 1 + S(θ 0 ) • S(θ i ) -S(θ 0 ) 2<label>(6)</label></formula><p>To train the proposed ARTran, we organize a group of loss functions to jointly optimize the proposed modules. The benchmark noise posteriors and the adjusted noise posteriors are optimized with classification loss with benchmark and adjusted labels respectively. Following the instance-independent scheme, we optimize the SST by minimizing the volume of the extended SST <ref type="bibr" target="#b13">[14]</ref>. The total loss function L we give is as follows:</p><formula xml:id="formula_6">L = L cls (P ( Ỹ |x, 0), Y (s, 0)) + L cls (P ( Ỹ |x, δ), Y (s, δ)) + L vol (T Σ ) (7)</formula><p>where L cls (•) indicates the cross entropy function, and L vol (•) indicates the volume minimization function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We conduct experiments on an OCT dataset including 509 volumes of 290 subjects from the same OCT system (RTVue-XR, Optovue, CA) with high diversity in SE range and retinal shape. Each OCT volume has a size of 400 (frames) × 400 (width) × 640 (height) corresponding to a 6mm × 6mm × 2mm volume centered at the retinal macular region. The exclusion criteria were as follows: eyes with the opacity of refractive media that interfered with the retinal image quality, and eyes that have undergone myopia correction surgery. Our dataset contains 234 low (or non) myopia volumes, and 275 high myopia volumes, where labels are determined according to a threshold spherical equivalent -6.0D. We divide the dataset evenly into 5 folds for cross-validation according to the principle of subject independence for all experiments. For data selection, we select the center 100 frames of each volume for training and testing, so a total of 50,900 images were added to the experiment. And the final decision outcome of one model for each volume is determined by the classification results of the majority of frames. For data augmentation, due to the special appearance and location characteristics of high myopia in OCT, we only adopt random horizontal flipping and random vertical translation with a range of [0, 0.1]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Experiments and Ablations</head><p>To evaluate the proposed ARTran in predicting under a benchmark inclusion criteria (-6.0D), we compare it with two convolution-based baselines: ResNet-50 <ref type="bibr" target="#b9">[10]</ref> and EfficientNet-B5 <ref type="bibr" target="#b24">[25]</ref>; three transformer-based baselines: ViT-Small <ref type="bibr" target="#b7">[8]</ref>, Swin-Tiny <ref type="bibr" target="#b17">[18]</ref> and Swin-V2-Tiny <ref type="bibr" target="#b16">[17]</ref>; and two state-of-the-art high myopia screening methods: Choi's method and Li's method <ref type="bibr" target="#b15">[16]</ref>. Table <ref type="table" target="#tab_1">1</ref> presents the classification accuracy, precision, and recall by our ARTran and comparison methods. The proposed ARTran already outperforms baseline methods remarkably with a 2.9% to 5.1% higher accuracy and a lower variance. This is because we design modules to capture the features of high myopia, which brings effectiveness and robustness. Although consisting of fewer parameters, our ARTran obtains higher accuracy than two state-of-the-art screening methods. Moreover, clinical practice requirements generally advocate the model that predicts smaller false negative samples, i.e., a higher recall. It is observed that the recall of our approach is the best performance, which means that our model has minimal risk of missing highly myopic samples.</p><p>We further perform ablations in order to better understand the effectiveness of the proposed modules. Table <ref type="table" target="#tab_2">2</ref> presents quantitative comparisons between different combinations of our APE, ACE, and SST. As can be seen, ablation of APE leads to a rapid decrease in recall, which means that this patch embedding approach captures better features about the propensity for high myopia. The ablation of ACE represents the ablation of the proposed adjustment scheme, which leads to lower accuracy. The ACE drives the network to learn more discriminative features for images in the adjustment range during the training process. The ablation of SST leads to a rapid decrease in precision. The possible reason is that the label noise may be more from negative samples, leading to increased false positive samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed ARTran to screen high myopia using OCT images. Experimental results demonstrated that our approach outperforms baseline classification methods and other screening methods. The ablation results also demonstrated that our modules helps the network to capture the features associated with high myopia and to mitigate the noise of labels. We organized the evaluation of the adjustable and interpretable ability. Experimental results showed that our method exhibits robustness under variable inclusion criteria of high myopia. We evaluated uncertainty and found that confusing samples had higher uncertainty scores, which could increase the interpretability of the screening task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of spherical equivalent (SE) and axial length (AL) for some samples in the dataset. (a) Scatterplot on two measured values. The red line indicates the rough estimation relationship from clinical experience, the blue line indicates the linear representation of regression. (b) OCT examples for low deviation of the experience trend (the top row) and with high deviation (the bottom row). (Color figure online)</figDesc><graphic coords="2,96,48,54,62,259,60,128,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of our proposed adjustable robust transformer (ARTran). The proposed APE and ACE encode the image information and adjustment information as the input to transformer. The proposed SST learns the label noise and establishes the connection between the class-posteriors. The range of class-posteriors are also shown.</figDesc><graphic coords="4,53,55,54,47,346,00,164,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the benchmark criteria, and on the other hand proves that the proposed model is effective for the adjustment problem.To evaluate the interpretability of our ARTran, we propose novel uncertainty scores based on the proposed adjustable scheme: (1) The closer the posteriors closer to 0.5 indicates larger uncertainty. (2) More frames in a volume with disagreement indicate larger uncertainty. (3) Based on a set of adjustment coefficients, the greater the variance, the greater the difficulty or uncertainty. Some examples are shown in Fig. 3. The two correctly classified (TP&amp;TN) examples are less difficult and therefore smaller uncertainty score. Large uncertainty scores are more likely to occur around inclusion criteria. Each of the two error examples (FP&amp;FN) contains at least one uncertainty score higher than the other examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Benchmark Clean Posterior Benchmark Benchmark Noise Posterior Adjusted Noise Posterior Adjusted Clean Posterior Adjusted Volume Minimization Cross Entropy Cross Entropy Equal Combination Biased Combination parameterized vector 1 parameterized vector 2 Benchmark SST</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Benchmark</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Norm</cell><cell></cell></row><row><cell></cell><cell>Adjustable Class</cell><cell></cell><cell></cell><cell></cell><cell>Extended SST</cell></row><row><cell>Position Encoding</cell><cell>Embedding (ACE)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Softmax</cell><cell></cell></row><row><cell cols="2">28 28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8 8</cell><cell>8 8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>56 56</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shifted SST</cell></row><row><cell></cell><cell></cell><cell>Q</cell><cell>K</cell><cell>V</cell></row><row><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell></row><row><cell>Input Image (224 × 224)</cell><cell>Anisotropic Patch Embedding (APE)</cell><cell cols="3">Vision Transformer Encoder</cell><cell>Loss Functions</cell><cell>Class-posterior Range</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of classification methods with benchmark inclusion criteria.</figDesc><table><row><cell>Model</cell><cell cols="4">Parameters(M ) Accuracy(%) Precision(%) Recall(%)</cell></row><row><cell>ResNet-50 [10]</cell><cell>23.5</cell><cell>82.97±9.0</cell><cell>85.4±27.4</cell><cell>83.3±4.0</cell></row><row><cell cols="2">EfficientNet-B5 [25] 28.3</cell><cell>81.1±7.0</cell><cell>83.1±14.2</cell><cell>80.4±5.6</cell></row><row><cell>ViT-Small [8]</cell><cell>21.7</cell><cell>82.7±4.1</cell><cell>85.0±4.6</cell><cell>82.5±4.7</cell></row><row><cell>Swin-Tiny [18]</cell><cell>27.5</cell><cell>83.3±13.3</cell><cell>83.9±19.4</cell><cell>85.5±26.1</cell></row><row><cell cols="2">Swin-V2-Tiny [17] 27.5</cell><cell>83.3±7.4</cell><cell>83.2±23.2</cell><cell>86.2±34.7</cell></row><row><cell>Choi's Method [6]</cell><cell>47.0</cell><cell>83.5±5.7</cell><cell>84.3±12.3</cell><cell>86.2±10.4</cell></row><row><cell>Li's Method [16]</cell><cell>54.3</cell><cell>84.8±10.7</cell><cell>85.6±14.5</cell><cell>86.5±30.2</cell></row><row><cell>ARTran(Ours)</cell><cell>21.9</cell><cell>86.2±1.4</cell><cell>87.5±2.4</cell><cell>86.9 ±2.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The ablation study of our anisotropic patch embedding (APE), adjustable class embedding (ACE), and shifted subspace transition matrix (SST).</figDesc><table><row><cell cols="6">APE ACE SST Accuracy(%) Precision(%) Recall(%)</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>86.2</cell><cell>87.5</cell><cell>86.9</cell></row><row><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>84.1</cell><cell>86.6</cell><cell>84.7</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>84.7</cell><cell>86.5</cell><cell>86.2</cell></row><row><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>83.9</cell><cell>84.8</cell><cell>85.6</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>83.3</cell><cell>86.2</cell><cell>84.4</cell></row><row><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>82.7</cell><cell>85.0</cell><cell>82.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">National Science Foundation of China</rs> under Grants No. <rs type="grantNumber">62172223</rs> and <rs type="grantNumber">62072241</rs>, the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> No. <rs type="grantNumber">30921013105</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5genhJB">
					<idno type="grant-number">62172223</idno>
				</org>
				<org type="funding" xml:id="_7Ec5nVd">
					<idno type="grant-number">62072241</idno>
				</org>
				<org type="funding" xml:id="_kkRJcJB">
					<idno type="grant-number">30921013105</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adjustable Evaluation and Uncertainty Evaluation</head><p>To evaluate the effectiveness of the adjustment module, we change the adjustment coefficient several times during the testing phase to obtain screening results at different thresholds. Figure <ref type="figure">3</ref>(a) depicts the PR curve when adjusting the adjustment coefficient. The performance of the two endpoints (Δ = -1 and Δ = 1) is marked. Even with a high recall rate, the precision is not low. Figure <ref type="figure">3</ref>(b) shows the performance of the biased labels for different adjustment coefficients. As can be seen, the accuracy improves when offsetting the inclusion criteria, which on the one hand may be due to the difficulty of classification near</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Myopia. Nature Rev. Dis. Primers</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prevalence and causes of visual impairment and blindness among 9980 scandinavian adults: the Copenhagen city eye study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vinding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>La Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Myopia control: why each diopter matters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bullimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optom. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="463" to="465" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The risks and benefits of myopia control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bullimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ritchey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leveziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Flitcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1561" to="1579" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning models for screening of high myopia using optical coherence tomography</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21663</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prevalence and characteristics of myopic degeneration in an adult Chinese American population: the chinese American eye study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Oct-based diagnostic criteria for different stages of myopic maculopathy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1018" to="1032" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1036" to="1042" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prevalence and causes of low vision and blindness in a Japanese adult population: the tajimi study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iwase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1354" to="1362" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16478" to="16488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Provably end-to-end label-noise learning without anchor points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6403" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Advances in oct imaging in myopia and pathologic myopia</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1418</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning system to screen vision-threatening conditions in high myopia using optical coherence tomography images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="633" to="639" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference On Computer Vision</title>
		<meeting>the IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ohno-Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Saw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Myopia. The Lancet</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="issue">9827</biblScope>
			<biblScope unit="page" from="1739" to="1748" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantitative evaluations of posterior staphylomas in highly myopic eyes by ultra-widefield optical coherence tomography</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nakao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi-Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ohno-Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="20" to="20" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">International photographic classification and grading system for myopic maculopathy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ohno-Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="883" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Myopia-a 21st century public health issue</title>
		<author>
			<persName><forename type="first">S</forename><surname>Resnikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Myopic maculopathy: current status and proposal for a new classification and grading system (ATN)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Medrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Flores-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>García-Layana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ruiz-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Retin. Eye Res</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="80" to="115" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EfficientnNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-class token transformer for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4310" to="4319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual t: reducing estimation error for transition matrix in label-noise learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7260" to="7271" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
