<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement</title>
				<funder>
					<orgName type="full">CIFAR</orgName>
				</funder>
				<funder ref="#_5R7qtvP #_gGWw8qh">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada (NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Canadian Institute for Advanced Research (CIFAR) AI Catalyst Grants</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Peter Munk Cardiac Centre</orgName>
								<orgName type="institution">University Health Network</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine and Pathobiology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<email>bowang@vectorinstitute.ai</email>
							<affiliation key="aff0">
								<orgName type="department">Peter Munk Cardiac Centre</orgName>
								<orgName type="institution">University Health Network</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine and Pathobiology</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">AI Hub</orgName>
								<orgName type="institution" key="instit2">University Health Network</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="3" to="13"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">845AE3A2FB873B375ECEE58149142D62</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based medical image enhancement methods (e.g., denoising and super-resolution) mainly rely on paired data and correspondingly the well-trained models can only handle one type of task. In this paper, we address the limitation with a diffusion modelbased framework that mitigates the requirement of paired data and can simultaneously handle multiple enhancement tasks by one pre-trained diffusion model without fine-tuning. Experiments on low-dose CT and heart MR datasets demonstrate that the proposed method is versatile and robust for image denoising and super-resolution. We believe our work constitutes a practical and versatile solution to scalable and generalizable image enhancement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computed Tomography (CT) and Magnetic Resonance (MR) are two widely used imaging techniques in clinical practice. CT imaging uses X-rays to produce detailed, cross-sectional images of the body, which is particularly useful for imaging bones and detecting certain types of cancers with fast imaging speed. However, CT imaging has relatively high radiation doses that can pose a risk of radiation exposure to patients. Low-dose CT techniques have been developed to address this concern by using lower doses of radiation, but the image quality is degraded with increased noise, which may compromise diagnostic accuracy <ref type="bibr" target="#b8">[9]</ref>.</p><p>MR imaging, on the other hand, uses a strong magnetic field and radio waves to create detailed images of the body's internal structures, which can produce high-contrast images for soft tissues and does not involve ionizing radiation. This makes MR imaging safer for patients, particularly for those who require frequent or repeated scans. However, MR imaging typically has a lower resolution than CT <ref type="bibr" target="#b17">[18]</ref>, which limits its ability to visualize small structures or abnormalities.</p><p>Motivated by the aforementioned, there is a pressing need to improve the quality of low-dose CT images and low-resolution MR images to ensure that they provide the necessary diagnostic information. Numerous algorithms have been developed for CT and MR image enhancement, with deep learning-based methods emerging as a prominent trend <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>, such as using the conditional generative adversarial network for CT image denoising <ref type="bibr" target="#b31">[32]</ref> and convolutional neural network for MR image Super-Resolution (SR) <ref type="bibr" target="#b3">[4]</ref>.</p><p>These algorithms are capable of improving image quality, but they have two significant limitations. First, paired images are required for training, e.g., low-dose and full-dose CT images; low-resolution and high-resolution MR images). However, acquiring such paired data is challenging in real clinical scenarios. Although it is possible to simulate low-quality images from high-quality images, the models derived from such data may have limited generalization ability when applied to real data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. Second, customized models are required for each task. For example, for MR super-resolution tasks with different degradation levels (i.e., 4x and 8x downsampling), one may need to train a customized model for each degradation level and the trained model cannot generalize to other degradation levels. Addressing these limitations is crucial for widespread adoption in clinical practice.</p><p>Recently, pre-trained diffusion models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref> have shown great promise in the context of unsupervised natural image reconstruction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. However, their applicability to medical images has not been fully explored due to the absence of publicly available pre-trained diffusion models tailored for the medical imaging community. The training of diffusion models requires a significant amount of computational resources and training images. For example, openai's improved diffusion models <ref type="bibr" target="#b20">[21]</ref> took 1600-16000 A100 hours to be trained on the ImageNet dataset with one million images, which is prohibitively expensive. Several studies have used diffusion models for low-dose CT denoising <ref type="bibr" target="#b29">[30]</ref> and MR image reconstruction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, but they still rely on paired images.</p><p>In this paper, we aim at addressing the limitations of existing image enhancement methods and the scarcity of pre-trained diffusion models for medical images. Specifically, we provide two well-trained diffusion models on full-dose CT images and high-resolution heart MR images, suitable for a range of applications including image generation, denoising, and super-resolution. Motivated by the existing plug-and-play image restoration methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> and denoising diffusion restoration and null-space models (DDNM) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, we further introduce a paradigm for plug-and-play CT and MR image denoising and super-resolution as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Notably, it eliminates the need for paired data, enabling greater scalability and wider applicability than existing paired-image dependent methods. Moreover, it eliminates the need to train a customized model for each task. Our method does not need additional training on specific tasks and can directly use the single pre-trained diffusion model on multiple medical image enhancement tasks. The pre-trained diffusion models and PyTorch code of the present method are publicly available at https://github.com/bowang-lab/ DPM-MedImgEnhance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This section begins with a brief overview of diffusion models for image generation and the mathematical model and algorithm for general image enhancement. We then introduce a plug-and-play framework that harnesses the strengths of both approaches to enable unsupervised medical image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Denoising Diffusion Probabilistic Models (DDPM) for Unconditional Image Generation</head><p>Image generation models aim to capture the intrinsic data distribution from a set of training images and generate new images from the model itself. We use DDPM <ref type="bibr" target="#b10">[11]</ref> for unconditional medical image generation, which contains a diffusion (or forward) process and a sampling (or reverse) process. The diffusion process gradually adds random Gaussian noise to the input image x 0 , following a Markov Chain with transition kernel q(x</p><formula xml:id="formula_0">t |x t-1 ) = N (x t ; √ 1 -β t x t-1 , β t I),</formula><p>where t ∈ {1, • • • , T } represents the current timestep, x t and x t-1 are adjacent image status, and</p><formula xml:id="formula_1">β t ∈ {β 1 , • • • , β T }</formula><p>is a predefined noise schedule. Furthermore, we can directly obtain x t based on x 0 at any timestep t by:</p><formula xml:id="formula_2">q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1 -ᾱt )I), e.g., x t = √ ᾱt x 0 + (1 -ᾱt ) ,<label>(1)</label></formula><p>where α t := 1-β t , ᾱt := Π t s=1 α s , and ∼ N (0, I). This property enables simple model training where the input is the noisy image x t and the timestep t and the output is the predicted noise θ (θ denotes model parameters). Intuitively, a denoising network is trained with the mean square loss E t,x ||θ (x t , t)|| 2 . The sampling process aims to generate a clean image from Gaussian noise x T ∼ N (0, I), and each reverse step is defined by:</p><formula xml:id="formula_3">x t-1 = 1 √ α t x t - 1 -α t √ 1 -ᾱt θ (x t , t) + β t z; z ∼ N (0, I).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Enhancement with Denoising Algorithm</head><p>In general, image enhancement tasks can be formulated by:</p><formula xml:id="formula_4">y = Hx + n, (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where y is the degraded image, H is a degradation matrix, x is the unknown original image, and n is the independent random noise. This model can represent various image restoration tasks. For instance, in the image denoising task, H is the identity matrix, and in the image super-resolution task, H is the downsampling operator. The main objective is to recover x by solving the minimization problem:</p><p>x * = arg min</p><formula xml:id="formula_6">x y -Hx 2 + R(x), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where the first data-fidelity term keeps the data consistency and the second dataregularization term R(x) imposes prior knowledge constraints on the solution. This problem can be solved by the Iterative Denoising and Backward Projections (IDBP) algorithm <ref type="bibr" target="#b25">[26]</ref>, which optimizes the revised equivalent problem:</p><formula xml:id="formula_8">x * , ŷ * = arg min x,ŷ ŷ -x H T H + R(x) s.t. ŷ = H † y, (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where</p><formula xml:id="formula_10">H † := H T (HH T ) -1</formula><p>is the pseudo inverse of the degradation matrix H and f H T H := f T H T Hf. Specifically, x * and ŷ * can be alternatively estimated by solving min x ŷx 2 2 + R(x) and min ŷ ŷx 2 2 s.t. H ŷ = y. Estimating x * is essentially a denoising problem that can be solved by a denoising operator and ŷ * has a closed-form solution:</p><formula xml:id="formula_11">x * = Denoiser(ŷ); ŷ * = H † y + (I -H † H)x. (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>Intuitively, IDBP iteratively estimates the original image from the current degraded image and makes a projection by constraining it with prior knowledge.</p><p>Although IDBP offers a flexible way to solve image enhancement problems, it still requires paired images to train the denoising operator <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Trained Diffusion Models for Plug-and-play Medical Image Enhancement</head><p>We introduce a plug-and-play framework by leveraging the benefits of the diffusion model and IDBP algorithm. Here we highlight two benefits: (1) it removes the need for paired images; and (2) it can simply apply the single pre-trained diffusion model across multiple medical image enhancement tasks. First, we reformulate the DDPM sampling process <ref type="bibr" target="#b10">[11]</ref> x t-1 ∼ p θ (x t-1 |x t ) = N (x t-1 ; μ θ (x t , t), β t I) into:</p><formula xml:id="formula_13">x 0|t = 1 √ ᾱt x t - √ 1 -ᾱt θ (x t , t) ,<label>(7)</label></formula><p>and</p><formula xml:id="formula_14">x t-1 = √ ᾱt-1 β t 1 -ᾱt x 0|t + √ α t (1 -ᾱt-1 ) 1 -α t x t + β t z. (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>Intuitively, each sampling iteration has two steps. The first step estimates the denoised image x 0|t based on the current noisy image x t and the trained denoising network θ (x t , t). The second step generates a rectified image x t-1 by taking a weighted sum of x 0|t and x t and adding a Gaussian noise perturbation.</p><p>As mentioned in Eq. ( <ref type="formula" target="#formula_4">3</ref>), our goal is to restore an unknown original image x 0 from a degraded image y. Thus, the degraded image y needs to be involved in the sampling process. Inspired by the iteration loop in IDBP, we project the estimated x 0|t on the hyperplane y = Hx:</p><formula xml:id="formula_16">x0|t = H † y + (I -H † H)x 0|t . (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>It can be easily proved that H x0|t = HH † y + Hx 0|t -HH † Hx 0|t = y. By replacing x 0|t with x0|t in Eq. ( <ref type="formula" target="#formula_14">8</ref>), we have:</p><formula xml:id="formula_18">x t-1 = √ ᾱt-1 β t 1 -ᾱt x0|t + √ α t (1 -ᾱt-1 ) 1 -α t x t + β t z. (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>Algorithm 1 shows the complete steps for image enhancement, which inherit the denoising operator from DDPM and the projection operator from IDBP. The former employs the strong denoising capability in the diffusion model and the latter can make sure that the generated results match the input image. Notably, the final algorithm is equivalent to DDNM <ref type="bibr" target="#b27">[28]</ref>, but it is derived from different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Pre-trained DDPM for plug-and-play medical image enhancement</head><p>Require: Pre-trained DDPM θ , low-quality image y, degradation operator H 1: Initialize xT ∼ N (0, I). 2: for t = T to 1 do 3: </p><formula xml:id="formula_20">x 0|t = 1 √ ᾱt xt - √ 1 -ᾱt θ (xt, t) //</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. We conducted experiments on two common image enhancement tasks: denoising and SR. To mimic the real-world setting, the diffusion models were trained on a diverse dataset, including images from different centers and scanners. The testing set (e.g., MR images) is from a new medical center that has not appeared in the training set. Experiments show that our model can generalize to these unseen images. Specifically, the denoising task is based on the AAPM Low Dose CT Grand Challenge abdominal dataset <ref type="bibr" target="#b18">[19]</ref>, which can be also used for SR <ref type="bibr" target="#b32">[33]</ref>. The heart MR SR task is based on three datasets: ACDC <ref type="bibr" target="#b0">[1]</ref>, M&amp;Ms1-2 <ref type="bibr" target="#b2">[3]</ref>, and CMRxMotion <ref type="bibr" target="#b26">[27]</ref>. Notably, the presented framework eliminates the requirement of paired data. For the CT image enhancement task, we trained a diffusion model <ref type="bibr" target="#b20">[21]</ref> based on the full-dose dataset that contains 5351 images, and the hold-out quarter-dose images were used for testing. For the MR enhancement task, we used the whole ACDC <ref type="bibr" target="#b0">[1]</ref> and M&amp;Ms1-2 <ref type="bibr" target="#b2">[3]</ref> for training the diffusion model and the CMRxMotion <ref type="bibr" target="#b26">[27]</ref> dataset for testing. The testing images were downsampled by operator H with factors of 4× and 8× to produce low-resolution images, and the original images served as the ground truth.</p><p>Evaluation Metrics. The image quality was quantitatively evaluated by the Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM) <ref type="bibr" target="#b28">[29]</ref>, and Visual Information Fidelity (VIF) <ref type="bibr" target="#b23">[24]</ref>, which are widely used measures in medical image enhancement tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. Implementation Details. We followed the standard configuration in <ref type="bibr" target="#b20">[21]</ref> to train the diffusion model from scratch. Specifically, the diffusion step used a linear noise schedule β ∈ [1e -4, 0.02] and the number of diffusion timesteps was T = 1000. The input image size was normalized to 256 × 256 and the 2D U-Net <ref type="bibr" target="#b22">[23]</ref> was optimized by Adam <ref type="bibr" target="#b12">[13]</ref> with a batch size of 16 and a learning rate of 10 -4 , and an Exponential Moving Average (EMA) over model parameters with a rate of 0.9999. All the models were trained on A100 GPU and the total training time was 16 d. The implementation was based on DDNM <ref type="bibr" target="#b27">[28]</ref>. For an efficient sampling, we used DDIM <ref type="bibr" target="#b24">[25]</ref> with 100 diffusion steps. We followed the degradation operator settings in DDNM. Specifically, we used the identity matrix I as the degradation operator for the denoising task and scaled the projection difference H † (Hx 0|t -y) with coefficient Σ to balance the information from measurement y and denoising output x 0|t . The downsampling operator implemented with torch.nn.AdaptiveAvgPool2d for the super-resolution task. The pseudo-inverse operator H † is I for the denoising task and upsampling operator for the SR task.</p><p>Comparison with Other Methods. The pseudo-inverse operator H † was used as the baseline method, namely, x * = H † y. We also compared the present method with one commonly used image enhancement method DIP <ref type="bibr" target="#b9">[10]</ref> and two recent diffusion model-based methods: IVLR <ref type="bibr" target="#b5">[6]</ref>, which adopted low-frequency information from measurement y to guide the generation process towards a narrow data manifold, and DPS <ref type="bibr" target="#b6">[7]</ref>, which addressed the intractability of posterior sampling through Laplacian approximation. Notably, DPS used 1000 sampling steps while we only used 100 sampling steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Low-Dose CT Image Enhancement. The presented method outperformed all other methods on the denoising task in all metrics, as shown in Table <ref type="table" target="#tab_0">1</ref>, with average PSNR, SSIM, and VIF of 28.3, 0.803, and 0.510, respectively. Supplementary Fig. <ref type="figure" target="#fig_0">1</ref> (a) visually compares the denoising results, showing that the presented method effectively removes the noise and preserves the anatomical details, while other methods either fail to suppress the noise or result in loss of tissue information. We also used the same pre-trained diffusion model for simultaneously denoising and SR by setting H as the downsampling operator. Our method still achieves better performance across all metrics as shown in Table <ref type="table" target="#tab_1">2</ref>. By visually comparing the enhancement results in Fig. <ref type="figure" target="#fig_0">1 (b</ref>) and (c), our results can reconstruct more anatomical details even in the challenging noisy 8× SR task. In contrast, DIP tends to smooth tissues and ILVR and DPS fail to recover the tiny structures such as liver vessels in the zoom-in regions. MR Image Enhancement. To demonstrate the generality of the presented method, we also applied it for the heart MR image 4× and 8× SR tasks, and the quantitative results are presented in Table <ref type="table" target="#tab_2">3</ref>. Our results still outperformed IVLR and DPS in all metrics. DIP obtains slightly better scores in PSNR for the 4× SR task and PSNR and SSIM for the 8× SR tasks, but visualized image quality is significantly worse than our results as shown in Supplementary Fig. <ref type="figure">2</ref>, e.g., many anatomical structures are smoothed. This is because perceptual and distortion qualities are in opposition to each other as theoretically proven in <ref type="bibr" target="#b1">[2]</ref>. DIP mainly prioritizes the distortion measures for the noise-free SR tasks while our results achieve a better trade-off between the perceptual and distortion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In summary, we have provided two well-trained diffusion models for abdomen CT and heart MR, and introduced a plug-and-play framework for image enhancement. Our experiments have demonstrated that a single pre-trained diffusion model could address different degradation levels without customized models. However, there are still some limitations to be solved. The degradation operator and its pseudo-inverse should be explicitly given, which limits its application in tasks such as heart MR motion deblurring. Although the present method is in general applicable for 3D images, training the 3D diffusion model still remains prohibitively expensive. Moreover, the sampling process currently requires multiple network inferences, but it could be solved with recent advances in one-step generation models <ref type="bibr" target="#b14">[15]</ref> and faster algorithms <ref type="bibr" target="#b15">[16]</ref>. Despite these limitations, the versatile and scalable nature of the presented paradigm has great potential to revolutionize medical image enhancement tasks. Future work could focus on developing more efficient algorithms for 3D diffusion models and expanding this paradigm to more clinical applications such as low-dose PET denoising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of (a) the common paired-image dependent paradigm and (b) the plug-and-play paradigm for medical image enhancement. The former needs to build customized models for different tasks based on paired low/high-quality images, while the latter can share one pre-trained diffusion model for all tasks and only high-quality images are required as training data. The pre-trained model can handle unseen images as demonstrated in experiments.</figDesc><graphic coords="3,55,98,53,75,340,18,161,95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Denoise xt with pre-trained DDPM 4: x0|t = x 0|t -ΣtH † (Hx 0|ty) // Project x 0|t on the hyperplane y = Hx xt + βtz, z ∼ N (0, I) // Sampling 6: end for 7: return Enhanced image x0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance (mean±standard deviation) on CT denoising task. The arrows indicate directions of better performance.</figDesc><table><row><cell cols="2">Methods PSNR ↑</cell><cell>SSIM ↑</cell><cell>VIF ↑</cell></row><row><cell cols="2">Baseline 24.9 ± 2.4</cell><cell>0.778 ± 0.07</cell><cell>0.451 ± 0.07</cell></row><row><cell cols="2">DIP [10] 25.9 ± 2.4</cell><cell>0.783 ± 0.06</cell><cell>0.444 ± 0.07</cell></row><row><cell cols="2">IVLR [6] 25.8 ± 2.3</cell><cell>0.695 ± 0.11</cell><cell>0.432 ± 0.09</cell></row><row><cell cols="2">DPS [7] 26.5 ± 2.3</cell><cell>0.791 ± 0.08</cell><cell>0.475 ± 0.09</cell></row><row><cell>Ours</cell><cell cols="3">28.3 ± 2.8 0.803 ± 0.11 0.510 ± 0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results of joint denoising and super-resolution for CT images.</figDesc><table><row><cell cols="2">Methods Noisy 4× SR</cell><cell></cell><cell>Noisy 8× SR</cell></row><row><cell></cell><cell>PSNR ↑ SSIM ↑</cell><cell>VIF ↑</cell><cell>PSNR ↑ SSIM ↑</cell><cell>VIF ↑</cell></row><row><cell cols="5">Baseline 20.4±0.6 0.583±0.06 0.191±0.01 18.4±0.6 0.484±0.06 0.087±0.01</cell></row><row><cell cols="5">DIP [10] 21.8±0.7 0.642±0.06 0.243±0.02 19.6±0.7 0.560±0.07 0.146±0.02</cell></row><row><cell cols="5">IVLR [6] 25.1±2.5 0.715±0.10 0.417±0.09 24.6±2.8 0.702±0.12 0.395±0.11</cell></row><row><cell cols="5">DPS [7] 25.1±2.8 0.172±0.13 0.416±0.12 24.7±3.1 0.705±0.14 0.398±0.12</cell></row><row><cell>Ours</cell><cell cols="4">26.2±2.6 0.743±0.10 0.446±0.10 25.9±3.1 0.731±0.13 0.431±0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results of heart MR image super-resolution tasks.</figDesc><table><row><cell cols="2">Methods 4× SR</cell><cell></cell><cell>8× SR</cell></row><row><cell></cell><cell>PSNR ↑ SSIM ↑</cell><cell>VIF ↑</cell><cell>PSNR ↑ SSIM ↑</cell><cell>VIF ↑</cell></row><row><cell cols="2">Baseline 23.5±0.6 0.734±0.03</cell><cell cols="3">0.288±0.01 19.9±0.7 0.646±0.04 0.123±0.01</cell></row><row><cell cols="2">DIP [10] 27.0±0.8 0.784±0.02</cell><cell cols="3">0.392±0.02 22.6±0.8 0.712±0.03 0.224±0.02</cell></row><row><cell cols="2">IVLR [6] 25.4±0.7 0.731±0.03</cell><cell cols="3">0.355±0.02 21.2±0.8 0.622±0.04 0.180±0.02</cell></row><row><cell cols="2">DPS [7] 25.6±0.8 0.741±0.03</cell><cell cols="3">0.335±0.03 21.2±0.8 0.635±0.04 0.177±0.02</cell></row><row><cell>Ours</cell><cell cols="4">26.9±0.8 0.805±0.025 0.416±0.03 22.4±0.8 0.700±0.03 0.226±0.02</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC</rs>, <rs type="grantNumber">RGPIN-2020-06189</rs> and <rs type="grantNumber">DGECR-2020-00294</rs>), <rs type="funder">Canadian Institute for Advanced Research (CIFAR) AI Catalyst Grants</rs>, and <rs type="funder">CIFAR</rs> AI Chair programs. We thank the IDDPM [21], guided-diffusion [8], and DDNM [28] team, as their implementation served as an important basis for our work. We want to especially mention <rs type="person">Jiwen Yu</rs>, who provided invaluable guidance and support. We also thank the organizers of <rs type="institution">AAPM Low Dose CT Grand Challenge</rs> [20], ACDC [1], M&amp;Ms1-2 [3], and CMRxMothion [27] for making the datasets publicly available.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5R7qtvP">
					<idno type="grant-number">RGPIN-2020-06189</idno>
				</org>
				<org type="funding" xml:id="_gGWw8qh">
					<idno type="grant-number">DGECR-2020-00294</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-centre, multi-vendor and multi-disease cardiac segmentation: the m&amp;ms challenge</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3543" to="3554" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution musculoskeletal MRI using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2139" to="2154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-dose CT denoising with convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ILVR: conditioning method for denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14347" to="14356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion posterior sampling for general noisy inverse problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8780" to="8794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review on CT image noise and its denoising</title>
		<author>
			<persName><forename type="first">M</forename><surname>Diwakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="73" to="88" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1867" to="1888" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denoising diffusion restoration models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artificial intelligence for MR image reconstruction: an overview for clinicians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flow straight and fast: learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DPM-solver: a fast ode solver for diffusion probabilistic model sampling in around 10 steps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparison of objective image quality metrics to expert radiologists&apos; scoring of diagnostic quality of MR images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning in radiology: an overview of the concepts and a survey of the state of the art with focus on MRI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Bashir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="954" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-dose CT for the detection and classification of metastatic liver lesions: Results of the 2016 low dose CT grand challenge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="339" to="e352" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-dose CT image and projection dataset</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Moen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="902" to="911" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards performant and reliable undersampled MR reconstruction via diffusion model sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="623" to="633" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image restoration by iterative denoising and backward projections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1220" to="1234" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The extreme cardiac MRI analysis challenge under respiratory motion (cmrxmotion)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06385</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot image restoration using denoising diffusion null-space model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Low-dose CT using denoising diffusion probabilistic model for 20x times speedup</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15136</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measurement-conditioned denoising diffusion probabilistic model for under-sampled medical image reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="655" to="664" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sharpness-aware low-dose CT denoising using conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="655" to="669" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ct super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-circle)</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Plug-and-play image restoration with deep denoiser prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6360" to="6376" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
