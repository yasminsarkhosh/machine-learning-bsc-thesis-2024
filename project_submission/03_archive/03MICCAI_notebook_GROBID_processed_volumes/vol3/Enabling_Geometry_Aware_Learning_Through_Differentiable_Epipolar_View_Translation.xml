<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Maximilian</forename><surname>Rohleder</surname></persName>
							<email>maxi.rohleder@fau.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthineers AG</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlotte</forename><surname>Pradel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Wagner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mareike</forename><surname>Thies</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Maul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthineers AG</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Denzinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthineers AG</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Friedrich-Alexander-University</orgName>
								<address>
									<settlement>Erlangen-Nürnberg, Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bjoern</forename><surname>Kreher</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthineers AG</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="57" to="65"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">97D51588DA0DD8948B3EFCA677D78EF3</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cone-Beam Computed Tomography</term>
					<term>Epipolar Geometry</term>
					<term>Operator Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Epipolar geometry is exploited in several applications in the field of Cone-Beam Computed Tomography (CBCT) imaging. By leveraging consistency conditions between multiple views of the same scene, motion artifacts can be minimized, the effects of beam hardening can be reduced, and segmentation masks can be refined. In this work, we explore the idea of enabling deep learning models to access the known geometrical relations between views. This implicit 3D information can potentially enhance various projection domain algorithms such as segmentation, detection, or inpainting. We introduce a differentiable feature translation operator, which uses available projection matrices to calculate and integrate over the epipolar line in a second view. As an example application, we evaluate the effects of the operator on the task of projection domain metal segmentation. By re-sampling a stack of projections into orthogonal view pairs, we segment each projection image jointly with a second view acquired roughly 90 • apart. The comparison with an equivalent single-view segmentation model reveals an improved segmentation performance of 0.95 over 0.91 measured by the dice coefficient. By providing an implementation of this operator as an open-access differentiable layer, we seek to enable future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cone-Beam Computed Tomography (CBCT) scans are widely used for guidance and verification in the operating room. The clinical value of this imaging modality is however limited by artifacts originating from patient and device motion or metal objects in the X-Ray beam. To compensate these effects, knowledge about the relative acquisition geometry between views can be exploited. This so-called epipolar geometry is widely used in computer vision and can be applied to CBCT imaging due to the similar system geometry <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11]</ref>.</p><p>By formulating and enforcing consistency conditions based on this geometrical relationship, motion can be compensated <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, beam-hardening effects can be reduced <ref type="bibr" target="#b11">[11]</ref>, and multi-view segmentations can be refined <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. Motion and beam hardening effects can be corrected by optimizing for consistency through either updating the projection matrices or image values while the respective other is assumed fixed.</p><p>In segmentation refinement, the principal idea is to incorporate the known acquisition geometry to unify the binary predictions on corresponding detector pixels. This inter-view consistency can be iteratively optimized to reduce falsepositives in angiography data <ref type="bibr" target="#b7">[8]</ref>. Alternatively, an entire stack of segmented projection images can be backprojected, the reconstructed volume thresholded and re-projected to obtain 3D consistent masks <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this work, we explore the idea of incorporating epipolar geometry into the learning-based segmentation process itself instead of a separate post-processing step. A differentiable image transform operator is embedded into the model architecture, which translates intermediate features across views allowing the model to adjust its predictions to this conditional information. By making this information accessible to neural networks and enabling dual-view joint processing, we expect benefits for projection domain processing tasks such as inpainting, segmentation or regression. As a proof-of-concept, we embed the operator into a segmentation model and evaluate its influence in a simulation study. To summarize, we make the following contributions:</p><p>-We analytically derive formulations for forward-and backward pass of the view translation operator -We provide an open-source implementation thereof which is compatible with real-world projection matrices and PyTorch framework -As an example of its application, we evaluate the operator in a simulation study to investigate its effect on projection domain segmentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In the following sections we introduce the geometrical relationships between epipolar views, define a view translation operator, and analytically derive gradients needed for supervised learning.</p><p>Epipolar Geometry and the Fundamental Matrix. A projection matrix P ∈ R 3×4 encodes the extrinsic device pose and intrinsic viewing parameters of the cone-beam imaging system. These projection matrices are typically available for images acquired with CBCT-capable C-Arm systems. Mathematically, this non-linear projective transform maps a point in volume coordinates to detector coordinates in homogeneous form <ref type="bibr" target="#b0">[1]</ref>. When two projection images of the same scene are available, the two detector coordinate systems can be linked through epipolar geometry as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The Fundamental matrix F ∈ R 3×3 directly encodes the inherent geometric relation between two detector coordinate systems. More specifically, a point u in one projection image is mapped onto a line l through l = F u , where l, u are vectors in the 2D projective homogeneous coordinate space P 2+ (notation from <ref type="bibr" target="#b0">[1]</ref>). Given projection matrices P, P ∈ R 3×4 , the fundamental matrix can be derived as</p><formula xml:id="formula_0">F = [P c ] × P P + ,<label>(1)</label></formula><p>where • + denotes the pseudo-inverse, c ∈ P 3+ is the camera center in homogeneous world coordinates, and [•] × constructs the tensor-representation of a cross product. Note, that the camera center can be derived as the kernel of the projection c = ker(P ). Additional details on epipolar geometry can be found in literature <ref type="bibr" target="#b2">[3]</ref>.</p><p>The Epipolar View Translation Operator (EVT). The goal of the proposed operator is to provide a neural network with spatially registered feature information from a second view of known geometry. Consider the dual view setup as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Epipolar geometry dictates that a 3D landmark detected at a detector position u in projective view P is located somewhere along the epipolar line l in the respective other view P . Naturally this only holds true as long as the landmark is within the volume of interest (VOI) depicted in both images.</p><p>To capture this geometric relationship and make spatially corresponding information available to the model, an epipolar map Ψ is computed from the input image p. As shown in Eq. 2, each point u in the output map Ψ , is computed as the integral along its epipolar line l = F u . </p><formula xml:id="formula_1">Ψ (u ) = L p(u) dl, L := {u ∈ P 2+ : u F u = 0}<label>(2)</label></formula><p>Gradient Derivation. To embed an operator into a model architecture, the gradient with respect to its inputs and all trainable parameters needs to be computed. As the proposed operator contains no trainable parameters, only the gradient with respect to the input is derived.</p><p>The forward function for one output pixel Y u = Ψ (u ) can be described through a 2D integral over the image coordinates u</p><formula xml:id="formula_2">Y u = u δ(u , u, F )X u , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where X u denotes the value in the input image p(u, v) at position u. Here, the indicator function δ(•) signals if the coordinate u lies on the epipolar line defined by F and u :</p><formula xml:id="formula_4">δ(u , u, F ) = 1, u F u = 0 0, else<label>(4)</label></formula><p>After calculation of a loss L, it is backpropagated through the network graph. At the operator, the loss arrives w.r.t. to the predicted consistency map ∂L ∂Y . From this image-shaped loss, the gradient w.r.t. the input image needs to be derived. By marginalisation over the loss image ∂L ∂Y , the contribution of one intensity value X u in the input image can be written as</p><formula xml:id="formula_5">∂L ∂X u = ∂L ∂Y ∂Y ∂X u = u ∂L ∂Y u ∂Y u ∂X u . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Deriving Eq. 3 w.r.t. X u eliminates the integral and only leaves the indicator function as an implicit form of the epipolar line. With this inserted in Eq. 5, the loss for one pixel in the input image can be expressed as</p><formula xml:id="formula_7">∂L ∂X u = u δ(u , u, F ) ∂L ∂Y u . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Note, that forward (Eq. 3) and backward formulation (Eq. 6) are similar and can thus be realised by the same operator. Due to the symmetry shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the backward function can be efficiently formulated similar to Eq. 3 by integration along the line l defined by the reversed Fundamental matrix F as</p><formula xml:id="formula_9">∂L ∂p (u) = L ∂L ∂Ψ (u ) dl L := {u ∈ P 2+ : u F u = 0} . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Implementation. The formulations above used to derive the gradient assume a continuous input and output distribution. To implement this operation on discrete images, the integral is replaced with a summation of constant step size of one pixel and bi-linear value interpolation. As the forward and backward functions compute the epipolar line given the current pixel position, slight mismatches in interpolation coefficients might occur. However, well-tested trainable reconstruction operators use similar approximate gradients and are proven to converge regardless <ref type="bibr" target="#b5">[6]</ref>. The differentiable operator is implemented as a PyTorch function using the torch.utils.cpp_extension. To enable parallel computation, the view transformation is implemented as a CUDA kernel. The source code will be made public upon publication.<ref type="foot" target="#foot_0">1</ref> Fig. <ref type="figure">2</ref>. Dual View Segmentation Model with embedded EVT operator. The operator is embedded at three different scale-levels in the decoder section of a 2D U-Net <ref type="bibr" target="#b4">[5]</ref>. By exchanging spatially translated feature maps after each up-block features from the second view are considered during mask decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>As a proof-of-concept application, we assess the operator's influence on the task of projection domain metal segmentation. To reconstruct a CBCT scan, 400 projection images are acquired in a circular trajectory over an angular range of 200 • . To experimentally validate the effects of the proposed image operator, the images are re-sampled into orthogonal view pairs and jointly segmented using a model with an embedded EVT operator.</p><p>Dual View U-Net with EVT Operator. To jointly segment two projection images, the Siamese architecture shown in Fig. <ref type="figure">2</ref> is used. It comprises two U-Net backbones with shared weights which process the two given views. The EVT operator is embedded as a skip connection between the two mirrored models to spatially align feature maps with the respective other view. Each view is fed through the model individually up to the point where epipolar information is added. There, the forward pass is synchronized and the translated feature maps from the respective other view are concatenated. We place the operator at three positions in the model -right after each upsampling block except the last. In the decoder block, feature maps are arguably more sparse. Intuitively, this increases the value of the proposed operator as fewer objects are in the same epipolar plane and thus correspondence is more directly inferable.</p><p>Compared Models. To investigate the effects of the newly introduced operator, the architecture described above is compared to variants of the U-Net architecture. As a logical baseline, the plain U-Net architecture from <ref type="bibr" target="#b4">[5]</ref> is trained to segment each projection image individually. Additionally, the same architecture is provided with two projection images concatenated along the channel axis. This approach verifies that any changes in segmentation performance are attributable to the feature translation operator and not simply due to providing a second view.</p><p>Data. For the purpose of this study, we use a simulated projection image dataset. Analogous to DeepDRR <ref type="bibr" target="#b8">[9]</ref>, we use an analytical polychromatic forward model to generate X-Ray images from given CBCT volume data. In total, 29 volumes with approximate spatial dimensions 16 cm 3 from 4 anatomical regions are used (18 × spine, 4 × elbow, 5 × knee, and 2 × wrist). To simulate realistic metal shapes, objects are selected from a library of surgical tools made available by Nuvasive (San Diego, USA) and Königsee Implantate (Allendorf, Germany). From this primary data, six spine scans are selected for testing, and three are selected for validation. Metal implants are manually positioned relative to the anatomy using 3D modelling tools. The metal objects are assembled such that they resemble frequently conducted procedures including pedicle screw placement and k-wire insertions. In total, there are 12 unique scenes fitted to the scans in the test set, and 5 in the validation set.</p><p>The training set consists of randomly selected and assembled metal objects. Each of the remaining 20 volumes is equipped with n ∈ {4, 6, 8, 10} randomly positioned (non-overlapping) metal objects creating 80 unique scenes.</p><p>During simulation, the objects are randomly assigned either iron or titanium as a material which influences the choice of attenuation coefficients. For each scene, 100 projection images are generated whose central ray angles on the circular trajectory are approximately 2 • apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Training</head><p>The three models are trained using the Adam optimizer with the dice coefficient as a loss function and a learning rate of 10 -5 . The best model is selected on the validation loss. As a data augmentation strategy, realistic noise of random strength is added to simulate varying dose levels or patient thickness <ref type="bibr" target="#b10">[10]</ref>. We empirically choose the range of noise through the parameter photon count #p ∈ [10 2 , 10 4 ]. During validation and testing, the noise is set to a medium noise level #p = 10 3 . Furthermore, each projection image is normalized to its own mean and standard deviation. The models are trained for 200 epochs on an NVIDIA A100 graphics card. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Quantitative. The per-view averaged segmentation test set statistics are shown in Table <ref type="table" target="#tab_0">1</ref>. The model predicting a single view yields a dice score of 0.916±0.119, thus outperforming the model which is fed two projection images at an average dice similarity of 0.889 ± 0.122. The model equipped with our operator, which also is presented with two views, but translates feature maps internally, yields the highest dice score of 0.950 ± 0.067.</p><p>Qualitative. To illustrate the reported quantitative results, the segmentation prediction is compared on two selected projection images in Fig. <ref type="figure" target="#fig_1">3</ref>. Test 1 shows a lateral view of a spine with 6 pedicle screws and tulips inserted into the 3 central </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Building upon previous work on epipolar geometry and the resulting consistency conditions in X-Ray imaging, we propose a novel approach to incorporate this information into a model. Rather than explicitly formulating the conditions and optimizing for them, we propose a feature translation operator that allows the model to capture these geometric relationships implicitly.</p><p>As a proof-of-concept study, we evaluate the operator on the task of projection domain segmentation. The operator's introduction enhances segmentation performance compared to the two baseline methods, as shown by both qualitative and quantitative results. Primarily we found the information from a second view made available by the operator to improve the segmentation in two ways:</p><p>(1) Reduction of false positive segmentations (2) Increased sensitivity in strongly attenuated areas. Especially for the segmentation of spinal implants, the model performance on lateral images was improved by epipolar information from an orthogonal anterior-posterior projection. Lateral images are usually harder to segment because of the drastic attenuation gradient as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. It is noteworthy that the U-Net architecture utilizing two images as input exhibits inferior performance compared to the single view model. The simple strategy of incorporating supplementary views into the network fails to demonstrate any discernible synergistic effect, likely due to the use of the same model complexity for essentially conducting two segmentation tasks simultaneously.</p><p>The simulation study's promising results encourage exploring the epipolar feature transform as a differentiable operator. Future work involves evaluating the presented segmentation application on measured data, analyzing the optimal integration of the operator into a network architecture, and investigating susceptibility to slight geometry calibration inaccuracies. As the U-Net's generalization on real data has been demonstrated, we expect no issues with our method.</p><p>In conclusion, this work introduces an open-source<ref type="foot" target="#foot_1">2</ref> differentiable operator to translate feature maps along known projection geometry. In addition to analytic derivation of gradients, we demonstrate that these geometry informed epipolar feature maps can be integrated into a model architecture to jointly segment two projection images of the same scene.</p><p>Data Use Declaration. The work follows appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects, or cadavers of both kind. All data acquisitions were done in consultation with the Institutional Review Board of the University Hospital of Erlangen, Germany.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Epipolar Geometry as defined in<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> and the proposed View Translation Operator. The intrinsic relation between two images under projection P and P is compactly captured in the Fundamental matrix F . In the forward pass, a point u in the epipolar map Ψ is calculated by integrating along the epipolar line l = F u . During gradient backpropagation, the contribution of a point u in the input image is determined by integrating along its epipolar line in the gradient image ∂L ∂Ψ .</figDesc><graphic coords="3,125,34,374,48,201,76,135,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation results from the three compared models on three selected projection images of the test set. Correctly segmented pixels are colored green and black, white indicates false negatives and red indicates the false positives. (Color figure online)</figDesc><graphic coords="7,64,14,211,70,332,05,165,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of the three compared model variations over the test set. All values reported as mean ± standard deviation. It is shown here as an approximately representative illustration of the improvement in precision due to the operator. A bone edge is falsely segmented as metal in both the single view and dual view mode. The model equipped with the operator neglects this false segmentation. Test 2 illustrates a case of improved recall, where one screw (top right) is occluded by strongly attenuating anatomy. Our model is able to recover the screw at least partially, whereas it is missed by the two other models.</figDesc><table><row><cell></cell><cell>Dice</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Single View</cell><cell>0.916 ± 0.119</cell><cell>0.976 ± 0.013</cell><cell>0.882 ± 0.168</cell></row><row><cell>Dual View</cell><cell>0.889 ± 0.122</cell><cell>0.970 ± 0.013</cell><cell>0.842 ± 0.177</cell></row><row><cell cols="4">Dual View + EVT 0.950 ± 0.067 0.991 ± 0.010 0.919 ± 0.106</cell></row><row><cell>vertebrae.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/maxrohleder/FUME.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/maxrohleder/FUME.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Epipolar consistency in transmission imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aichert</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2015.2426417</idno>
		<ptr target="https://doi.org/10.1109/TMI.2015.2426417" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2205" to="2219" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning-based patch-wise metal segmentation with consistency check</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Kreher</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-658-33198-6_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-658-33198-6_4" />
	</analytic>
	<monogr>
		<title level="m">Bildverarbeitung für die Medizin 2021. I</title>
		<meeting><address><addrLine>Wiesbaden</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511811685</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511811685" />
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Symmetry prior for epipolar consistency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Preuhs</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-02027-8</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-02027-8" />
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1541" to="1551" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Technical Note: PYRONN: Python reconstruction operators in neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Syben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ploner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maier</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.13753</idno>
		<ptr target="https://doi.org/10.1002/mp.13753" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5110" to="5115" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving segmentation quality in rotational angiography using epipolar consistency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Achenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc MICCAI CVII-STENT</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Balocco</surname></persName>
		</editor>
		<meeting>MICCAI CVII-STENT<address><addrLine>Athens</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consistency-based respiratory motion estimation in rotational angiography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Achenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="113" to="e124" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepDRR -a catalyst for machine learning in fluoroscopyguided procedures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_12" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-dose preview for patient-specific, task-specific technique selection in cone-beam CT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.4884039</idno>
		<ptr target="https://doi.org/10.1118/1.4884039" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">71915</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Calibration-free beam hardening reduction in x-ray CBCT using the epipolar consistency condition and physical constraints</title>
		<author>
			<persName><forename type="first">T</forename><surname>Würfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maaß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dennerlein</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.13625</idno>
		<ptr target="https://doi.org/10.1002/mp.13625" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="810" to="e822" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
