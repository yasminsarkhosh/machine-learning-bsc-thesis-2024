<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Bayesian VIB-DeepSSM</title>
				<funder ref="#_dX9gJqT #_MxyPqwQ #_rHeXwYC #_2jqmxVY">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jadie</forename><surname>Adams</surname></persName>
							<email>jadie.adams@utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Scientific Computing and Imaging Institute</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Kahlert School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shireen</forename><forename type="middle">Y</forename><surname>Elhabian</surname></persName>
							<email>shireen@sci.utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Scientific Computing and Imaging Institute</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Kahlert School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<region>UT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Bayesian VIB-DeepSSM</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="346" to="356"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">603F7B371BB404DB312ADA176C680606</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Statistical Shape Modeling</term>
					<term>Bayesian Deep Learning</term>
					<term>Variational Information Bottleneck</term>
					<term>Epistemic Uncertainty Quantification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Statistical shape modeling (SSM) enables population-based quantitative analysis of anatomical shapes, informing clinical diagnosis. Deep learning approaches predict correspondence-based SSM directly from unsegmented 3D images but require calibrated uncertainty quantification, motivating Bayesian formulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is an effective, principled framework for predicting probabilistic shapes of anatomy from images with aleatoric uncertainty quantification. However, VIB is only half-Bayesian and lacks epistemic uncertainty inference. We derive a fully Bayesian VIB formulation and demonstrate the efficacy of two scalable implementation approaches: concrete dropout and batch ensemble. Additionally, we introduce a novel combination of the two that further enhances uncertainty calibration via multimodal marginalization. Experiments on synthetic shapes and left atrium data demonstrate that the fully Bayesian VIB network predicts SSM from images with improved uncertainty reasoning without sacrificing accuracy. * (Source code is publicly available: https:// github.com/jadie1/BVIB-DeepSSM)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>pipelines require expert-driven, intensive steps such as segmentation, shape registration, and tuning correspondence optimization parameters or defining an atlas/template for pairwise surface matching. Deep learning approaches have mitigated this overhead by providing end-to-end solutions, predicting PDMs from unsegmented 3D images with little preprocessing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">25]</ref>. Such solutions cannot be safely deployed in sensitive, clinical decision-making scenarios without uncertainty reasoning <ref type="bibr" target="#b10">[11]</ref>, which provides necessary insight into the degree of model confidence and serves as a metric of prediction reliability. There are two primary forms of uncertainty aleatoric (or data-dependent) and epistemic (or model-dependent) <ref type="bibr" target="#b15">[16]</ref>. The overall prediction uncertainty is the sum of the two. It is essential to distinguish between these forms, as epistemic is reducible and can be decreased given more training data or by refining the model <ref type="bibr" target="#b9">[10]</ref>. Bayesian deep learning frameworks automatically provide epistemic uncertainty and can be defined to predict distributions, providing aleatoric uncertainty quantification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>DeepSSM <ref type="bibr" target="#b4">[5]</ref> is a state-of-the-art framework providing SSM estimates that perform statistically similarly to traditional SSM methods in downstream tasks <ref type="bibr" target="#b5">[6]</ref>. Uncertain-DeepSSM <ref type="bibr" target="#b0">[1]</ref> adapted the DeepSSM network to be Bayesian, providing both forms of uncertainties. DeepSSM, Uncertain-DeepSSM, and other formulations <ref type="bibr" target="#b25">[25]</ref> rely on a shape prior in the form of a supervised latent encoding pre-computed using principal component analysis (PCA). PCA supervision imposes a linear relationship between the latent and the output space, restricts the learning task, and does not scale in the case of large sets of high-dimensional shape data. VIB-DeepSSM <ref type="bibr" target="#b1">[2]</ref> relaxes these assumptions to provide improved accuracy and aleatoric uncertainty estimates over the existing state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">25]</ref>. This probabilistic formulation utilizes a variational information bottleneck (VIB) <ref type="bibr" target="#b2">[3]</ref> architecture to learn the latent encoding in the context of the task, resulting in a more scalable, flexible model VIB-DeepSSM is selfregularized via a latent prior, increasing generalizability and helping alleviate the need for the computationally expensive DeepSSM data augmentation process. However, this approach does not quantify epistemic uncertainty because VIB is only half-Bayesian <ref type="bibr" target="#b2">[3]</ref>. In this paper, we propose to significantly extend the VIB-DeepSSM framework to be fully Bayesian, predicting probabilistic anatomy shapes directly from images with both forms of uncertainty quantification.</p><p>The contributions of this work include the following: (1) We mathematically derive fully Bayesian VIB from the variational inference perspective. <ref type="bibr" target="#b1">(2)</ref> We demonstrate two scalable approaches for Bayesian VIB-DeepSSM with epistemic uncertainty quantification (concrete dropout and batch ensemble) and compare them to naive ensembling. (3) We introduce and theoretically justify a novel combination of concrete dropout and ensembling for improved uncertainty calibration. <ref type="bibr" target="#b3">(4)</ref> We illustrate that the fully Bayesian formulations improve uncertainty reasoning (especially the proposed method) on synthetic and real data without sacrificing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We denote a set of paired training data as D = {X , Y}. X = {x n } N n=1 is a set of N unsegmented images, where x n ∈ R H×W ×D . Y = {y n } N n=1 is the set of PDMs comprised of M 3D correspondence points, where y n ∈ R 3M . VIB utilizes a stochastic latent encoding Z = {z n } N n=1 , where z n ∈ R L and L 3M . InBayesian modeling, model parameters Θ are obtained by maximizing the likelihood p(y|x, Θ). The predictive distribution is found by marginalizing over Θ, which requires solving for the posterior p(Θ|D). In most cases, p(Θ|D) is not analytically tractable; thus, an approximate posterior q(Θ) is found via variational inference (VI). Bayesian networks maximize the VI evidence lower bound (ELBO) by minimizing:</p><formula xml:id="formula_0">L VI = E Θ∼q(Θ) -log p(y|x, Θ) + β KL [q(Θ) p(Θ)] (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where p(Θ) is the prior on network weights, and β is a weighting parameter. The deep Variational Information Bottleneck (VIB) <ref type="bibr" target="#b2">[3]</ref> model learns to predict y from x using a low dimensional stochastic encoding z. The VIB architecture comprises of a stochastic encoder parameterized by φ, q(z|x, φ), and a decoder parameterized by θ, p(y|z, θ) (Fig. <ref type="figure" target="#fig_0">1</ref>). VIB utilizes VI to derive a theoretical lower bound on the information bottleneck objective:</p><formula xml:id="formula_2">L V IB = E ẑ∼q(z|x,φ) [-log p(y|ẑ, θ)] + β KL [q(z|x, φ) p(z)]<label>(2)</label></formula><p>The entropy of the p(y|z) distribution (computed via sampling) captures aleatoric uncertainty. The VIB objective has also been derived using an alternative motivation: Bayesian inference via optimizing a PAC style upper bound on the true negative log-likelihood risk <ref type="bibr" target="#b3">[4]</ref>. Through this PAC-Bayes lens, it has been proven that VIB is half Bayesian, as the Bayesian strategy is applied to minimize an upper bound with respect to the conditional expectation of y, but Maximum Likelihood Estimation (MLE) is used to approximate the expectation over inputs. The VIB objective can be made a fully valid bound on the true risk by applying an additional PAC-Bound with respect to the parameters, resulting in a fully Bayesian VIB that captures epistemic uncertainty in addition to aleatoric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Variational Information Bottleneck</head><p>In fully-Bayesian VIB (BVIB), rather than fitting the model parameters Θ = {φ, θ} via MLE, we use VI to approximate the posterior p(Θ|D). There are now two intractable posteriors p(z|x, φ) and p(Θ|x, y). The first is approximated via q(z|x, φ) as in Eq. 2 and the second is approximated by q(Θ) as in Eq. 1.</p><p>Minimizing these two KL divergences via a joint ELBO gives the objective (see Appendix A) for derivation details):</p><formula xml:id="formula_3">L BVIB = E Θ E ẑ -log p(y|ẑ, θ) + KL q(z|x, φ) p(z) + KL [q(Θ) p(Θ)] (3)</formula><p>where Θ ∼ q(Θ) and ẑ ∼ q(z|x, φ). This objective is equivalent to the BVIB objective acquired via applying a PAC-Bound with respect to the conditional expectation of targets and then another with respect to parameters <ref type="bibr" target="#b3">[4]</ref>. This is expected, as it has been proven that the VI formulation using ELBO and the PAC-Bayes formulation with negative log-likelihood as the risk metric are algorithmically identical <ref type="bibr" target="#b22">[23]</ref>. Additionally, this matches the objective derived for the Bayesian VAE when y = x <ref type="bibr" target="#b8">[9]</ref>. Implementing BVIB requires defining a prior distribution for the latent representation p(z) and the network weights p(Θ). Following VIB, we define, p(z) = N (z|0, I). Different methods exist for defining p(Θ), and multiple approaches are explored in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed BVIB-DeepSSM Model Variants</head><p>In adapting VIB-DeepSSM to be fully Bayesian, we propose utilizing two approaches that have demonstratively captured epistemic uncertainty without significantly increasing computational and memory costs: concrete dropout <ref type="bibr" target="#b12">[13]</ref> and batch ensemble <ref type="bibr" target="#b26">[26]</ref>. Additionally, we propose a novel integration for a more flexible, multimodal posterior approximation. Concrete Dropout (CD) utilizes Monte Carlo dropout sampling as a scalable solution for approximate VI <ref type="bibr" target="#b12">[13]</ref>. Epistemic uncertainty is captured by the spread of predictions with sampled dropout masks in inference. CD automatically optimizes layer-wise dropout probabilities along with the network weights.</p><p>Naive Ensemble (NE) models combine outputs from several networks for improved performance. Networks trained with different initialization converge to different local minima, resulting in test prediction disagreement <ref type="bibr" target="#b11">[12]</ref>. The spread in predictions effectively captures epistemic uncertainty <ref type="bibr" target="#b19">[20]</ref>. NE models are computationally expensive, as cost increases linearly with number of members.</p><p>Batch Ensemble (BE) <ref type="bibr" target="#b26">[26]</ref> compromises between a single network and NE, balancing the trade-off between accuracy and running time and memory. In BE, each weight matrix is defined to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. BE provides an ensemble from one network, where the only extra computation cost is the Hadamard product, and the only added memory overhead is sets of 1D vectors.</p><p>Novel Integration of Dropout and Ensembling: Deep ensembles have historically been considered a non-Bayesian competitor for uncertainty estimation. However, recent work argues that ensembles approximate the predictive distribution more closely than canonical approximate inference procedures (i.e., VI) and are an effective mechanism for approximate Bayesian marginalization <ref type="bibr" target="#b28">[28]</ref>. Combining traditional Bayesian methods with ensembling improves the fidelity of approximate inference via multimodal marginalization, resulting in a more robust, accurate model <ref type="bibr" target="#b27">[27]</ref>. In concrete dropout, the approximate variational distribution is parameterized via a concrete distribution. While this parameterization enables efficient Bayesian inference, it greatly limits the expressivity of the approximate posterior. To help remedy this, we propose integrating concrete dropout and ensembling (BE-CD and NE-CD) to acquire a multimodal approximate posterior on weights for increased flexibility and expressiveness. While ensembling has previously been combined with MC dropout for regularization <ref type="bibr" target="#b26">[26]</ref>, this combination has not been proposed with the motivation of multimodal marginalization for improved uncertainty calibration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BVIB-DeepSSM Implementation</head><p>We compare the proposed BVIB approaches with the original VIB-DeepSSM formulation <ref type="bibr" target="#b1">[2]</ref>. All models have the overall structure shown in Fig. <ref type="figure" target="#fig_0">1</ref>, comprised of a 3D convolutional encoder(f e ) and fully connected decoder (f d ). CD models have concrete dropout following every layer, BE weights have four members (the maximum GPU memory would allow), and four models were used to create NE models for a fair comparison. Following <ref type="bibr" target="#b1">[2]</ref>, burn-in is used to convert the loss from deterministic (L2) to probabilistic (Eqs 10, 3, 13) <ref type="bibr" target="#b1">[2]</ref>. This counteracts the typical reduction in accuracy that occurs when a negative log-likelihood based loss is used with a gradient-based optimizer <ref type="bibr" target="#b20">[21]</ref>. An additional dropout burn-in phase is used for CD models to increase the speed of convergence <ref type="bibr" target="#b1">[2]</ref>. All models were trained until the validation accuracy had not decreased in 50 epochs. A table of model hyperparameters and tested ranges is proved in Appendix C. The training was done on Tesla V100 GPU with Xavier initialization <ref type="bibr" target="#b14">[15]</ref>, Adam optimization <ref type="bibr" target="#b16">[17]</ref>. The prediction uncertainty is a sum of the epistemic (variance resulting from marginalizing over Θ) and aleatoric (variance resulting from marginalizing over z) uncertainty (see Appendix B for calculation details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We expect well-calibrated prediction uncertainty to correlate with the error, aleatoric uncertainty to correlate with the input image outlier degree (given that it is data-dependent), and epistemic uncertainty to correlate with the shape outlier degree (i.e., to detect out-of-distribution data). The outlier degree value for each mesh and image is quantified by running PCA (preserving 95% of variability) and then considering the Mahalanobis distance of the PCA scores to the mean (within-subspace distance) and the reconstruction error (off-subspace distance). The sum of these values provides a measure of similarity to the whole set in standard deviation units <ref type="bibr" target="#b18">[19]</ref>. Experiments are designed to evaluate this expected correlation as well as accuracy, which is calculated as the root mean square error (RMSE) between the true and predicted points. Additionally, we quantify the surface-to-surface distance between a mesh reconstructed from the predicted PDM (predicted mesh) and the ground truth segmented mesh. The reported results are an average of four runs for each model, excluding the NE models, which ensemble the four runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Supershapes Experiments</head><p>Supershapes (SS) are synthetic 3D shapes parameterized by variables that determine the curvature and number of lobes <ref type="bibr" target="#b13">[14]</ref>. We generated 1200 supershapes with lobes randomly selected between 3 and 7 and curvature parameters randomly sampled from a χ 2 distribution with 4 • of freedom. Corresponding 3D images were generated with foreground and background intensity values modeled as Gaussian distributions with different means and equal variance. Images were blurred with a Gaussian filter (size randomly selected between 1 and 8) to mimic diffuse shape boundaries. Figure <ref type="figure" target="#fig_1">2A</ref> displays example shape meshes and images with corresponding outlier degrees, demonstrating the wide variation. We randomly split the mesh/image pairs to create a training set of size 1000, a validation set of size 100, and a testing set of size 100. ShapeWorks <ref type="bibr" target="#b7">[8]</ref> was used to optimize PDMs of 128 points on the training set. Target PDMs were then optimized for validation and test sets, keeping the training PDMs fixed so that the test set statistics were not captured by the training PDMs.</p><p>Figure <ref type="figure" target="#fig_1">2B</ref> demonstrates that all BVIB models performed similarly or better than the baseline VIB in terms of RMSE and surface-to-surface distance, with the BE models performing best. Interestingly, the BE models were more accurate than the NE. This effect could result from the random sign initialization of BE fast weights, which increases members diversity. Adding CD hurt the accuracy slightly, likely because the learning task is made more difficult when layer-wise dropout probabilities are added as variational parameters. However, CD is the cheapest way to add epistemic uncertainty and improve prediction uncertainty calibration. Figure <ref type="figure" target="#fig_1">2C</ref> demonstrates prediction uncertainty is well-calibrated for all models (with an error correlation greater than 0.7) and NE-CD-BVIB achieves the best correlation. The aleatoric and epistemic uncertainty correlation was similar across models, with the ensemble-based models performing best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Left Atrium Experiments</head><p>The left atrium (LA) dataset comprises 1041 anonymized LGE MRIs from unique patients. The images were manually segmented at the University of Utah Division of Cardiovascular Medicine with spatial resolution 0.65 × 0.65 × 2.5 mm 3 , and the endocardium wall was used to cut off pulmonary veins.</p><p>The images were cropped around the region of interest, then downsampled by a factor of 0.8 for memory purposes. This dataset contains significant shape variations, including overall size, LA appendage size, and pulmonary veins' number and length. The input images vary widely in intensity and quality, and LA boundaries are blurred and have low contrast with the surrounding structures. Shapes and image pairs with the largest outlier degrees were held out as outlier test sets, resulting in a shape outlier test set of 40 and image outlier test set of 78. We randomly split the remaining samples (90%, 10%, 10%) to get a training set of 739, a validation set of 92, and an inlier test set of 92. The target PDMs were optimized with ShapeWorks <ref type="bibr" target="#b7">[8]</ref> to have 1024 particles.</p><p>The accuracy and uncertainty calibration analysis in Figs. <ref type="figure" target="#fig_2">3B</ref> and<ref type="figure" target="#fig_2">3C</ref> show similar results to the supershapes experiment. In both experiments, the proposed combination of dropout and ensembling provided the best-calibrated prediction uncertainty, highlighting the benefit of multimodal Bayesian marginalization. Additionally, the proposed combination gave more accurate predictions on the LA outlier test sets, suggesting improved robustness. BE-CD-BVIB provided the best prediction uncertainty for the LA and the second best (just behind NE-CD-BVIB) for the SS. BE-CD-BVIB is a favorable approach as it does not require training multiple models as NE does and requires relatively low memory addition to the base VIB model. Further qualitative LA results are provided in Appendix F in the form of heat maps of the error and uncertainty on test meshes. Here we can see how the uncertainty correlates locally with the error. As expected, both are highest in the LA appendage and pulmonary veins region, where LA's and the segmentation process vary the most. It is worth noting a standard normal prior was used for p(z) in all models. Defining a more flexible prior, or potentially learning the prior, could provide better results and will be considered in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The traditional computational pipeline for generating Statistical Shape Models (SSM) is expensive and labor-intensive, which limits its widespread use in clinical research. Deep learning approaches have the potential to overcome these barriers by predicting SSM from unsegmented 3D images in seconds, but such a solution cannot be deployed in a clinical setting without calibrated estimates of epistemic and aleatoric uncertainty. The VIB-DeepSSM model provided a principled approach to quantify aleatoric uncertainty but lacked epistemic uncertainty. To address this limitation, we proposed a fully Bayesian VIB model that can predict anatomical SSM with both forms of uncertainty. We demonstrated the efficacy of two practical and scalable approaches, concrete dropout and batch ensemble, and compared them to the baseline VIB and naive ensembling. Finally, we proposed a novel combination of dropout and ensembling and showed that the proposed approach provides improved uncertainty calibration and model robustness on synthetic supershape and real left atrium datasets. While combining Bayesian methods with ensembling increases memory costs, it enables multimodal marginalization improving accuracy. These contributions are an important step towards replacing the traditional SSM pipeline with a deep network and increasing the feasibility of fast, accessible SSM in clinical research and practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Common VIB-DeepSSM Architecture for all proposed variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Supershapes (A) Left: Five examples of SS mesh and image pairs with corresponding outlier degrees. Right: Examples of training points, where color denotes point correspondence. (B) Distribution of errors over the test set, lower is better. (C) Uncertainty correlation, where a higher Pearson r coefficient suggests better calibration. Best values are marked in red, and the second best in blue.</figDesc><graphic coords="6,109,47,277,67,233,59,256,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left Atrium (A) The distribution of shape and image outlier degrees with thresholds is displayed with examples. (B) Box plots show the distribution of errors over the test sets. (C) Scatterplots show uncertainty correlation with error across test sets and box plots show the distribution of uncertainty for each test set. The best values are marked in red, and the second best in blue.</figDesc><graphic coords="8,97,47,54,23,257,41,314,47" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Institutes of Health</rs> under grant numbers <rs type="grantNumber">NIBIB-U24EB029011</rs>, <rs type="grantNumber">NIAMS-R01AR076120</rs>, <rs type="grantNumber">NHLBI-R01HL135568</rs>, and <rs type="grantNumber">NIBIB-R01EB016701</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The authors would like to thank the <rs type="institution">University of Utah Division of Cardiovascular Medicine</rs> for providing left atrium MRI scans and segmentations from the Atrial Fibrillation projects.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dX9gJqT">
					<idno type="grant-number">NIBIB-U24EB029011</idno>
				</org>
				<org type="funding" xml:id="_MxyPqwQ">
					<idno type="grant-number">NIAMS-R01AR076120</idno>
				</org>
				<org type="funding" xml:id="_rHeXwYC">
					<idno type="grant-number">NHLBI-R01HL135568</idno>
				</org>
				<org type="funding" xml:id="_2jqmxVY">
					<idno type="grant-number">NIBIB-R01EB016701</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_34.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertain-DeepSSM: from images to probabilistic shape models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elhabian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-61056-2_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-61056-2_5" />
	</analytic>
	<monogr>
		<title level="m">ShapeMI 2020</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Reuter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Paniagua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12474</biblScope>
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From images to probabilistic anatomical shapes: a deep variational bottleneck approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elhabian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_46" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="474" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vib is half bayes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepSSM: a deep learning framework for statistical shape modeling from raw images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Elhabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-04747-4_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-04747-4_23" />
	</analytic>
	<monogr>
		<title level="m">ShapeMI 2018</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Reuter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Paniagua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lüthi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11167</biblScope>
			<biblScope unit="page" from="244" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for end-to-end atrial fibrillation recurrence estimation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhalodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Cardiology</title>
		<meeting><address><addrLine>Maastricht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-23">2018. September 23-26, 2018 (2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shapeworks: Particle-based shape correspondence and visualization software</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elhabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Shape and Deformation Analysis</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="257" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian variational autoencoders for unsupervised out-of-distribution detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daxberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05651</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aleatory or epistemic? does it matter?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Der Kiureghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ditlevsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Saf</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Assessing the credibility of computational modeling and simulation in medical device submissions. U.S. Department of Health and Human Services, Food and Drug Administration</title>
		<author>
			<persName><surname>Fda</surname></persName>
		</author>
		<ptr target="https://www.fda.gov/media/154985/download" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Center for Devices and Radiological Health</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<title level="m">Deep ensembles: a loss landscape perspective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A generic geometric transformation that unifies a wide range of natural and abstract shapes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gielis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Bot</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty quantification and deep ensembles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rahaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">20063-20075 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Framework for the statistical shape analysis of brain structures using SPHARM-PDM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insight J</title>
		<imprint>
			<biblScope unit="page" from="242" to="250" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unifying variational inference and PAC-bayes for supervised learning that scales</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10367</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilistic 3D surface reconstruction from sparse MRI information</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tóthová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="813" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_79</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_79" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in CNN-based surface prediction using shape priors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tóthová</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-04747-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-04747-4_28" />
	</analytic>
	<monogr>
		<title level="m">ShapeMI 2018</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Reuter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Paniagua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lüthi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11167</biblScope>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian deep learning and a probabilistic perspective of generalization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4697" to="4708" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10995</idno>
		<title level="m">The case for bayesian deep learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
