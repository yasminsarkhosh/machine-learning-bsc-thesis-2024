<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation</title>
				<funder ref="#_SRcHUhh">
					<orgName type="full">Universidad Nacional del Litoral</orgName>
				</funder>
				<funder ref="#_C4fmXTE">
					<orgName type="full">ANPCyT</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Canada</orgName>
				</funder>
				<funder ref="#_N3AnFN2">
					<orgName type="full">Google Award for Inclusion Research</orgName>
					<orgName type="abbreviated">AIR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Agostina</forename><forename type="middle">J</forename><surname>Larrazabal</surname></persName>
							<email>alarrazabal@sinc.unl.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computational Intelligence</orgName>
								<orgName type="institution">Research Institute for Signals</orgName>
								<address>
									<addrLine>sinc(i)</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">FICH-UNL</orgName>
								<orgName type="institution" key="instit2">CONICET</orgName>
								<address>
									<settlement>Santa Fe</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<settlement>Tryolabs, Montevideo</settlement>
									<country key="UY">Uruguay</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">César</forename><surname>Martínez</surname></persName>
							<email>cmartinez@sinc.unl.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computational Intelligence</orgName>
								<orgName type="institution">Research Institute for Signals</orgName>
								<address>
									<addrLine>sinc(i)</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">FICH-UNL</orgName>
								<orgName type="institution" key="instit2">CONICET</orgName>
								<address>
									<settlement>Santa Fe</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Dolz</surname></persName>
							<email>jose.dolz@etsmtl.ca</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">LIVIA</orgName>
								<orgName type="institution" key="instit2">ETS Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enzo</forename><surname>Ferrante</surname></persName>
							<email>eferrante@sinc.unl.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Systems and Computational Intelligence</orgName>
								<orgName type="institution">Research Institute for Signals</orgName>
								<address>
									<addrLine>sinc(i)</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">FICH-UNL</orgName>
								<orgName type="institution" key="instit2">CONICET</orgName>
								<address>
									<settlement>Santa Fe</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="273" to="283"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">042E7E044ABFAC27FDA79A182DB298FF</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image segmentation</term>
					<term>uncertainty</term>
					<term>calibration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep neural networks achieved remarkable progress in medical image segmentation tasks. However, it has recently been observed that they tend to produce overconfident estimates, even in situations of high uncertainty, leading to poorly calibrated and unreliable models. In this work we introduce Maximum Entropy on Erroneous Predictions (MEEP), a training strategy for segmentation networks which selectively penalizes overconfident predictions, focusing only on misclassified pixels. Our method is agnostic to the neural architecture, does not increase model complexity and can be coupled with multiple segmentation loss functions. We benchmark the proposed strategy in two challenging segmentation tasks: white matter hyperintensity lesions in magnetic resonance images (MRI) of the brain, and atrial segmentation in cardiac MRI. The experimental results demonstrate that coupling MEEP with standard segmentation losses leads to improvements not only in terms of model calibration, but also in segmentation quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning have become the de facto solution for medical image segmentation. Nevertheless, despite their ability to learn highly discriminative features, these models have shown to be poorly calibrated, often resulting in over-confident predictions, even when they are wrong <ref type="bibr" target="#b0">[1]</ref>. When a model is miscalibrated, there is little correlation between the confidence of its predictions and how accurate such predictions actually are <ref type="bibr" target="#b1">[2]</ref>. This results in a major problem, which can have catastrophic consequences in medical diagnosis systems where decisions may depend on predicted probabilities. As shown in <ref type="bibr" target="#b2">[3]</ref>, the uncertainty estimates inferred from segmentation models can provide insights into the confidence of any particular segmentation mask, and highlight areas of likely errors for the practitioner. In order to improve the accuracy and reliability of these models, it is crucial to develop both accurate and well-calibrated systems. Despite the growing popularity of calibration for image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, the impact of miscalibrated networks on image segmentation, especially in the realm of biomedical images, has only recently begun to be explored <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution.</head><p>In this work, we propose a novel method based on entropy maximization to enhance the quality of pixel-level segmentation posteriors. Our hypothesis is that penalizing low entropy on the probability estimates for erroneous pixel predictions during training should help to avoid overconfident estimates in situations of high uncertainty. The underlying idea is that, if a pixel is difficult to classify, it is better assigning uniformly distributed (i.e. high entropy) probabilities to all classes, rather than being overconfident on the wrong class. To this end, we design two simple regularization terms which push the estimated posteriors for misclassified pixels towards a uniform distribution by penalizing low entropy predictions. We benchmark the proposed method in two challenging medical image segmentation tasks. Last, we further show that assessing segmentation models only from a discriminative perspective does not provide a complete overview of the model performance, and argue that including calibration metrics should be preferred. This will allow to not only evaluate the segmentation power of a given model, but also its reliability, of pivotal importance in healthcare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work.</head><p>Obtaining well-calibrated probability estimates of supervised machine learning approaches has attracted the attention of the research community even before the deep learning era, including approaches like histogram <ref type="bibr" target="#b6">[7]</ref> or Bayesian binning <ref type="bibr" target="#b7">[8]</ref>. Nevertheless, with the increase of popularity of deep neural networks, several works to directly address the calibration of these models have recently emerged. For instance, Bayesian neural networks learn a posterior distribution over parameters that quantifies parameter uncertainty -a type of epistemic uncertainty-, providing a natural approach to quantify model uncertainty. Among others, well-known Bayesian methods include variational inference <ref type="bibr" target="#b8">[9]</ref>, dropout-based variational inference <ref type="bibr" target="#b9">[10]</ref> or stochastic expectation propagation <ref type="bibr" target="#b10">[11]</ref>. A popular non-Bayesian method is ensemble learning, a simple strategy that improves both the robustness and calibration performance of predictive models <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. However, even though this technique tends to improve the networks calibration, it does not directly promote uncertainty awareness. Furthermore, ensembling typically requires retraining several models from scratch, incurring into computationally expensive steps for large datasets and complex models. Guo et al. <ref type="bibr" target="#b0">[1]</ref> empirically evaluated several post training ad-hoc calibration strategies, finding that a simple temperature scaling of logits yielded the best results. A drawback of this simple strategy, though, is that calibration performance largely degrades under data distribution shift <ref type="bibr" target="#b15">[16]</ref>.</p><p>Another alternative is to address the calibration problem during training, for example by clamping over-confident predictions. In <ref type="bibr" target="#b16">[17]</ref>, authors proposed to regularize the neural network output by penalizing low entropy output distributions, which was achieved by integrating an entropy regularized term into the main learning objective. We want to emphasize that, even though the main motivation in <ref type="bibr" target="#b16">[17]</ref> was to achieve better generalization by avoiding overfitting, recent observations <ref type="bibr" target="#b17">[18]</ref> highlight that these techniques have a favorable effect on model calibration. In a similar line of work, <ref type="bibr" target="#b4">[5]</ref> empirically justified the excellent performance of focal loss to learn well-calibrated models. More concretely, authors observed that focal loss <ref type="bibr" target="#b18">[19]</ref> minimizes a Kullback-Leibler (KL) divergence between the predicted softmax distribution and the target distribution, while increasing the entropy of the predicted distribution.</p><p>An in-depth analysis of the calibration quality obtained by training segmentation networks with the two most commonly used loss functions, Dice coefficient and cross entropy, was conducted in <ref type="bibr" target="#b5">[6]</ref>. In line with <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, authors showed that loss functions directly impact calibration quality and segmentation performance, noting that models trained with soft Dice loss tend to be poorly calibrated and overconfident. Authors also highlight the need to explore new loss functions to improve both segmentation and calibration quality. Label smoothing (LS) has also been proposed to improve calibration in segmentation models. Islam et al. <ref type="bibr" target="#b21">[22]</ref> propose a label smoothing strategy for image segmentation by designing a weight matrix with a Gaussian kernel which is applied across the one-hot encoded expert labels to obtain soft class probabilities. They stress that the resulting label probabilities for each class are similar to one-hot within homogeneous areas and thus preserve high confidence in non-ambiguous regions, whereas uncertainty is captured near object boundaries. Our proposed method achieves the same effect but generalized to different sources of uncertainty by selectively maximizing the entropy only for difficult to classify pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Maximum Entropy on Erroneous Predictions</head><p>Let us have a training dataset D = {(x, y) n } 1≤n≤|D| , where x n ∈ R Ωn denotes an input image and y n ∈ {0, 1} Ωn×K its corresponding pixel-wise one-hot label. Ω n denotes the spatial image domain and K the number of segmentation classes. We aim at training a model, parameterized by θ, which approximates the underlying conditional distribution p(y|x, θ), where θ is chosen to optimize a given loss function. The output of our model, at a given pixel i, is given as ŷi , whose associated class probability is p(y|x, θ). Thus, p(ŷ i,k = k|x i , θ) will indicate the probability that a given pixel (or voxel) i is assigned to the class k ∈ K. For simplicity, we will denote this probability as pi,k .</p><p>Since confident predictions correspond to low entropy output distributions, a network is overconfident when it places all the predicted probability on a single class for each training example, which is often a symptom of overfitting <ref type="bibr" target="#b22">[23]</ref>. Therefore, maximizing the entropy of the output probability distribution encourages high uncertainty (or low confidence) in the network predictions. In contrast to prior work <ref type="bibr" target="#b16">[17]</ref>, which penalizes low entropy in the entire output distributions, we propose to selectively penalize overconfidence exclusively for those pixels which are misclassified, i.e. the more challenging ones. To motivate our strategy, we plot the distribution of the magnitude of softmax probabilities in Fig. <ref type="figure" target="#fig_0">1</ref>.b. It can be observed that for models trained with standard L dice loss <ref type="bibr" target="#b19">[20]</ref>, most of the predictions lie in the first or last bin of the histogram We hypothesize that encouraging the network to assign high entropy values solely to erroneous predictions (i.e. uniformly distributed probabilities) will help to penalize overconfidence in complex scenarios. To this end, for every training iteration we define the set of misclassified pixels as ŷw = {y i |ŷ i = y i }. We can then compute the entropy for this set as:</p><formula xml:id="formula_0">H(ŷ w ) = - 1 |ŷ w | k,i∈ŷw pi,k log pi,k ,<label>(1)</label></formula><p>where | • | is used to denote the set cardinality. As we aim at maximizing the entropy of the output probabilities ŷw (Eq. ( <ref type="formula" target="#formula_0">1</ref>)), this equals to minimizing the negative entropy, i.e., min θ -H(ŷ w ). From now, we will use L H (ŷ w ) = H(ŷ w ) to refer to the additional loss term computing the entropy for the misclassified pixels following Eq. 1. Note that given a uniform distribution q, maximizing the entropy of y w boils down to minimizing the Kullback-Leibler (KL) divergence between y w and q. In what follows, we define another term based on this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proxy for Entropy Maximization:</head><p>In addition to explicitly maximizing the entropy of predictions (or to minimizing the negative entropy) as proposed in Eq. 1, we resort to an alternative regularizer, which is a variant of the KL divergence <ref type="bibr" target="#b23">[24]</ref>. The idea is to encourage the output probabilities in y w (the misclassified pixels) to be close to the uniform distribution (i.e. all elements in the probability simplex vector q are equal to<ref type="foot" target="#foot_0">1</ref> K ), resulting in max-uncertainty. This term is:</p><formula xml:id="formula_1">D KL (q||ŷ w ) K = H(q, ŷw ),<label>(2)</label></formula><p>with q being the uniform distribution and the symbol K = representing equality up to an additive or multiplicative constant associated with the number of classes. We refer the reader to the Appendix I in <ref type="bibr" target="#b23">[24]</ref> for the Proof of this KL divergence variant, as well as its gradients. It is important to note that despite both terms, (1) and ( <ref type="formula" target="#formula_1">2</ref>), push ŷw towards a uniform distribution, their gradient dynamics are different, and thus the effect on the weight updates differs. Here we perform an experimental analysis to assess which term leads to better performance. We will use L KL (ŷ w ) = D KL (q||ŷ w ) to refer to the additional loss based on Eq. 2.</p><p>Global Learning Objective: Our final loss function takes the following form: L = L Seg (y, ŷ) -λL me (ŷ w ), where ŷ is the entire set of pixel predictions, L Seg the segmentation loss 1 , L me is one of the proposed maximum entropy regularization terms and λ balances the importance of each objective. Note that L me can take the form of the standard entropy definition, i.e. L me (ŷ w ) = L H (ŷ w ) (eq. ( <ref type="formula" target="#formula_0">1</ref>)) or the proxy for entropy maximization using the KL divergence, i.e. L me (ŷ w ) = L KL (ŷ w ) (eq. ( <ref type="formula" target="#formula_1">2</ref>)). While the first term will account for producing good quality segmentations the second term will penalize overconfident predictions only for challenging pixels, increasing the awareness of the model about the more uncertain image regions, maintaining high confidence in regions that are actually identified correctly.</p><p>Baseline Models: We trained baseline networks using a simple loss composed of a single segmentation objective L Seg , without adding any regularization term. We used the two most popular segmentation losses: cross-entropy (L CE ) and the negative soft Dice coefficient (L dice ) as defined by <ref type="bibr" target="#b19">[20]</ref>. Furthermore, we also compare our method to state-of-the-art calibration approaches. First, due to its similarity with our work, we include the confidence penalty loss proposed in <ref type="bibr" target="#b16">[17]</ref>, which discourages all the neural network predictions from being overconfident by penalizing low-entropy distributions. This is achieved by adding a low-entropy penalty term over all the pixels (in contrast with our method that only penalizes the misclassified pixels), which can be defined as:</p><formula xml:id="formula_2">L H (ŷ) = -1 |ŷ| k,i∈ŷ pi,k log pi,k .</formula><p>We train two baseline models using the aforementioned regularizer L H (ŷ), considering cross-entropy (L CE ) and Dice losses (L dice ). We also assess the performance of focal-loss <ref type="bibr" target="#b18">[19]</ref>, since recent findings <ref type="bibr" target="#b4">[5]</ref> demonstrated the benefits of using this objective to train well-calibrated networks.</p><p>Post-hoc Calibration Baselines. We also included two well known calibration methods typically employed for classification <ref type="bibr" target="#b0">[1]</ref>: isotonic regression (IR) <ref type="bibr" target="#b24">[25]</ref> and Platt scaling (PS) <ref type="bibr" target="#b25">[26]</ref>. Differently from our methods which only use the training split, IR and PS are trained using validation data <ref type="bibr" target="#b0">[1]</ref>, keeping the original network parameters fixed. This is an advantage of our approaches since they do not require to keep a hold-out set for calibration. We apply IR and PS to the predictions of the vanilla baseline models trained with L dice and L CE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset and Network Details. We benchmark the proposed method in the context of Left Atrial (LA) cavity and White Matter Hyperintensities (WMH) segmentation in MR images. For LA, we used the Atrial Segmentation Challenge dataset <ref type="bibr" target="#b26">[27]</ref>, which provides 100 3D gadolinium-enhanced MR imaging scans (GE-MRIs) and LA segmentation masks for training and validation. These scans have an isotropic resolution of 0.625 × 0.625 × 0.625 mm 3 . We used the splits and pre-processed data from <ref type="bibr" target="#b27">[28]</ref> (80 scans for training and 20 for evaluation -5% of training images were used for validation). The WMH dataset <ref type="bibr" target="#b28">[29]</ref> consists of 60 MR images binary WMH masks. Each subject includes a co-registered 3D T1-weighted and a 2D multi-slice FLAIR of 1 × 1 × 3 mm. We split the dataset into independent training (42), validation (3) and test (15) sets. We benchmark our proposed method with two state-of-the-art DNN architectures (UNet <ref type="bibr" target="#b29">[30]</ref> and ResUNet <ref type="bibr" target="#b30">[31]</ref>) implemented using Tensorflow 2.3<ref type="foot" target="#foot_1">2</ref> (results for ResUNet are included in the Supp. Mat.). During training, for the WMH dataset we extract patches of size 64 × 64 × 64, and we train the networks until convergence by randomly sampling patches so that the central pixel corresponds to foreground label with 0.9 probability to account for label imbalance. For LA dataset all the scans were cropped to size 144 × 144 × 80 and centered at the heart region. We used Adam optimizer with a batch size of 64 for WMH and 2 for LA. The learning rate was set to 0.0001, and reduced by a factor of 0.85 every 10 epochs. Hyper-parameters were chosen using the validation split, and results reported on the hold-out test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details and Evaluation Metrics.</head><p>As baselines, we used networks trained with L CE and L dice only. We also included the aforementioned posthoc calibration methods (namely IR and PS) as post-processing step for these vanilla models. We also implemented the confidence penalty-based method <ref type="bibr" target="#b16">[17]</ref> previously discussed by adding the entropy penalizer L H (ŷ) and using the hyperparameter β = 0.2 suggested by the authors. We also include the focal-loss (L F L ) with γ = 2, following the authors' findings <ref type="bibr" target="#b18">[19]</ref> and compare with the proposed regularizers which penalize low entropy in wrongly classified pixels: L H (ŷ w ) (Eq. 1) and L KL (ŷ w ) (Eq. 2). We performed grid search with different λ, and we found empirically that 0.3 works best for WMH models trained with L CE and 1.0 for L dice . For the LA dataset, we chose 0.1 for L CE and 0.5 for L dice . For each setting we trained 3 models and report the average results.</p><p>To assess segmentation performance we resort to Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), whereas we use standard calibration metrics: Brier score <ref type="bibr" target="#b31">[32]</ref>, Stratified Brier score <ref type="bibr" target="#b32">[33]</ref> (adapted to image segmentation following <ref type="bibr" target="#b14">[15]</ref>) and Expected Calibration Error <ref type="bibr" target="#b7">[8]</ref>. We also employ reliability diagrams, depicting the observed frequency as a function of the class probability. Note that in a perfectly calibrated model, the frequency on each bin matches the confidence, and hence all the bars lie on the diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Our main goal is to improve the estimated uncertainty of the predictions, while retaining the segmentation power of original losses. Thus, we first assess whether integrating our regularizers leads to a performance degradation. Table <ref type="table" target="#tab_0">1</ref> reports the results across the different datasets with the UNet model (results for ResUNet are included in the Supp. Mat.). First, we can observe that adding the proposed regularizers does not result in a remarkable loss of segmentation performance. Indeed, in some cases, e.g., L dice + L KL ( Ŷw ) in WMH, the proposed model outperforms the baseline by more than 3% in terms of HD. Furthermore, this behaviour holds when the standard CE loss is used in conjunction with the proposed terms, suggesting that the overall segmentation performance is not negatively impacted by adding our regularizers into the main learning objective. Last, it is noteworthy to mention that even though L FL sometimes outperforms the baselines, it typically falls behind our two losses. In terms of qualitative performance, Fig. <ref type="figure" target="#fig_0">1</ref>.a depicts exemplar cases of the improvement in probability maps obtained for each loss function in LA segmentation.</p><p>Regarding calibration performance, recent empirical evidence <ref type="bibr" target="#b5">[6]</ref> shows that, despite leading to strong predictive models, CE and specially Dice losses result in highly-confident predictions. The results obtained for calibration metrics (Brier and ECE in Table <ref type="table" target="#tab_0">1</ref> and reliability plots in Fig. <ref type="figure" target="#fig_1">2</ref>) are in line with these obser- vations. These results evidence that regardless of the dataset, networks trained with any of these losses as a single objective, lead to worse calibrated models compared to the proposed penalizers. Explicitly penalizing low-entropy predictions over all the pixels, as in <ref type="bibr" target="#b16">[17]</ref>, typically improves calibration. Nevertheless, despite the gains observed with <ref type="bibr" target="#b16">[17]</ref>, empirical results demonstrate that penalizing low-entropy values only over misclassified pixels brings the largest improvements, regardless of the main segmentation loss used. In particular, the proposed MEEP regularizers outperform the baselines in all the three calibration metrics and in both datasets, with improvements ranging from 1% to 13%, except for Brier+ in WMH. However, in this case, even though IR achieves a better Brier + , it results in worse ECE.</p><p>When evaluating the proposed MEEP regularizers (L KL ( Ŷw ) and L H ( Ŷw )) combined with the segmentation losses based on DSC and CE, we observe that DSC with L KL ( Ŷw ) consistently achieves better performance in most of the cases. However, for CE, both regularizers alternate best results, which depend on the dataset used. We hypothesize that this might be due to the different gradient dynamics shown by the two regularizers<ref type="foot" target="#foot_2">3</ref> . Regarding the focal loss, even though it improves model calibration when compared with the vanilla models, we observe that the proposed regularizers achieve better calibration metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we presented a simple yet effective approach to improve the uncertainty estimates inferred from segmentation models when trained with popular segmentation losses. In contrast to prior literature, our regularizers penalize highconfident predictions only on misclassified pixels, increasing network uncertainty in complex scenarios. In addition to directly maximizing the entropy on the set of erroneous pixels, we present a proxy for this term, formulated with a KL regularizer modeling high uncertainty over those pixels. Comprehensive results on two popular datasets, losses and architectures demonstrate the potential of our approach. Nevertheless, we have also identified several limitations. For example, we have not assessed the effect of the proposed regularizers under severe domain shift (e.g. when testing on images of different organs). In this case it is not clear whether the model will output highly uncertain posteriors, or result again on overconfident but wrong predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Results obtained on the left atrium (LA) segmentation task (ground truth in green).From left to right: 1st column corresponds to UNet trained with standard segmentation losses, i.e., Dice (L dice ) and cross entropy (LCE). 2nd column: we include a regularization term LH ( Ŷ ), which penalizes low entropy in all the predictions,<ref type="bibr" target="#b16">[17]</ref>. 3rd and 4th columns: results obtained with variations of the proposed MEEP method which penalizes low entropy predictions only on misclassified pixels. We can clearly observe how the proposed MEEP models push the predicted probabilities towards 0.5 in highly uncertain areas. 5th column: results with focal loss. (b) Distribution of the magnitude of softmax probabilities on the Left Atrium dataset obtained when training with different loss functions. These plots motivate the proposed selective confidence penalty (bottom row ) over prior work (top right)<ref type="bibr" target="#b16">[17]</ref>, as the resulting probability distributions produced by our method are smoother, leading to better calibrated networks. (Color figure online)</figDesc><graphic coords="6,41,79,54,59,340,21,128,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The first row shows the reliability plot calculated on the entire volume of test images for each of the models while the bottom row shows histogram of probabilities produced by each method.</figDesc><graphic coords="8,42,30,53,87,166,30,113,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean accuracy and standard deviation for both WMH and LS segmentation tasks with UNet as backbone. Our models are gray-shadowed and best results are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Segmentation performance</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Calibration performance</cell><cell></cell></row><row><cell cols="2">Training loss</cell><cell cols="2">Dice coefficient</cell><cell></cell><cell>HD</cell><cell cols="2">Brier (10 -4 )</cell><cell cols="2">B r i e r +</cell><cell cols="2">ECE (10 -3 )</cell></row><row><cell></cell><cell></cell><cell>WMH</cell><cell>LA</cell><cell>WMH</cell><cell>LA</cell><cell>WMH</cell><cell>LA</cell><cell>WMH</cell><cell>LA</cell><cell>WMH</cell><cell>LA</cell></row><row><cell></cell><cell>-</cell><cell cols="3">0.770 (0.100) 0.886 (0.060) 24.041 (10.845)</cell><cell>28.282 (11.316)</cell><cell>6.717 (4.184)</cell><cell>29.182(15.068)</cell><cell>0.257 (0.125)</cell><cell>0.107 (0.090)</cell><cell>0.667 (0.414)</cell><cell>28.861 (15.009)</cell></row><row><cell></cell><cell>+ PS</cell><cell>0.763 (0.103)</cell><cell>0.884 (0.065)</cell><cell cols="3">24.151 (10.937) 26.565 (10.683) 6.187 (3.974)</cell><cell>24.953 (14.250)</cell><cell>0.271 (0.126)</cell><cell>0.114 (0.087)</cell><cell>1.563 (0.235)</cell><cell>16.346 (13.143)</cell></row><row><cell>Ldice</cell><cell>+ IR</cell><cell cols="2">0.770 (0.098) 0.883 (0.065)</cell><cell>24.176 (10.725)</cell><cell>26.699 (11.031)</cell><cell>5.541 (3.391)</cell><cell cols="3">24.617 (13.936) 0.212 (0.107) 0.111 (0.083)</cell><cell>1.539 (0.181)</cell><cell>16.303 (13.670)</cell></row><row><cell></cell><cell cols="2">+LH ( Ŷ ) [17] 0.769 (0.099)</cell><cell>0.885 (0.050)</cell><cell>21.608 (8.830)</cell><cell>29.811 (11.168)</cell><cell>6.751 (4.194)</cell><cell>29.019(12.709)</cell><cell>0.249 (0.125)</cell><cell>0.109 (0.077)</cell><cell>0.670 (0.415)</cell><cell>28.458 (12.514)</cell></row><row><cell></cell><cell>+LH ( Ŷw)</cell><cell>0.758 (0.108)</cell><cell>0.873 (0.069)</cell><cell>21.243 (8.755)</cell><cell>29.374 (10.965)</cell><cell>5.874 (3.875)</cell><cell>24.709(13.774)</cell><cell>0.244 (0.124)</cell><cell>0.103 (0.086)</cell><cell>0.510 (0.350)</cell><cell>18.796 (15.005)</cell></row><row><cell></cell><cell cols="11">+LKL( Ŷw) 0.770 (0.098) 0.881 (0.064) 20.804 (8.122) 28.415 (12.860) 5.564 (3.586) 23.182(12.464) 0.231 (0.114) 0.095 (0.077) 0.471 (0.318) 15.587 (13.391)</cell></row><row><cell></cell><cell>-</cell><cell>0.755 (0.111)</cell><cell>0.878 (0.070)</cell><cell cols="3">21.236 (7.735) 27.163 (11.967) 6.462 (4.141)</cell><cell>24.447 (14.876)</cell><cell>0.280 (0.140)</cell><cell>0.108 (0.092)</cell><cell>0.620 (0.400)</cell><cell>18.383 (16.700)</cell></row><row><cell>LCE</cell><cell>+ PS + IR</cell><cell>0.763 (0.105) 0.764 (0.105)</cell><cell>0.878 (0.069) 0.878 (0.070)</cell><cell>21.008 (7.637) 21.202 (7.855)</cell><cell>27.203 (11.963) 27.223 (11.944)</cell><cell>5.459 (3.367) 5.430 (3.326)</cell><cell cols="3">23.458 (13.462) 23.544 (13.803) 0.210 (0.112) 0.102 (0.084) 0.214 (0.115) 0.100 (0.081)</cell><cell>1.631 (0.188) 1.622 (0.198)</cell><cell>16.576 (15.427) 16.421 (15.500)</cell></row><row><cell></cell><cell cols="2">+LH ( Ŷ ) [17] 0.760 (0.109)</cell><cell>0.881 (0.070)</cell><cell>23.124 (9.523)</cell><cell>29.464 (14.389)</cell><cell>6.369 (4.018)</cell><cell>23.539 (11.903)</cell><cell>0.242 (0.125)</cell><cell>0.096 (0.070)</cell><cell>4.100 (0.582)</cell><cell>15.590 (14.002)</cell></row><row><cell></cell><cell>+LH ( Ŷw)</cell><cell cols="4">0.770 (0.095) 0.883 (0.058) 19.544 (7.254) 28.560(13.352)</cell><cell cols="5">5.417 (3.547) 22.506 (11.903) 0.217 (0.104) 0.093 (0.071) 0.436 (0.301)</cell><cell>15.242 (13.730)</cell></row><row><cell></cell><cell cols="3">+LKL( Ŷw) 0.777 (0.093) 0.876 (0.070)</cell><cell>22.298 (9.566)</cell><cell cols="3">28.736 (11.972) 5.331 (3.478) 24.085 (13.330)</cell><cell>0.213 (0.099)</cell><cell cols="2">0.105 (0.090) 0.422 (0.289)</cell><cell>17.348 (14.786)</cell></row><row><cell></cell><cell>LFL</cell><cell>0.753 (0.113)</cell><cell>0.881 (0.064)</cell><cell>21.931 (8.167)</cell><cell>28.599 (11.968)</cell><cell>5.760 (3.732)</cell><cell>23.928 (11.626)</cell><cell>0.243 (0.130)</cell><cell>0.095 (0.066)</cell><cell>0.438 (0.310)</cell><cell>25.998 (12.740)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>LSeg can take the form of any segmentation loss (e.g., CE or Dice).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Code: https://github.com/agosl/Maximum-Entropy-on-Erroneous-Predictions/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We refer toFig 3 and  Appendix I in<ref type="bibr" target="#b23">[24]</ref> for a detailed explanation regarding the different energies for binary classification and their derivatives.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The authors gratefully acknowledge NVIDIA Corporation with the donation of the GPUs used for this research, the support of <rs type="funder">Universidad Nacional del Litoral</rs> with the <rs type="programName">CAID program</rs> and <rs type="funder">ANPCyT</rs> (<rs type="grantNumber">PRH-2019-00009</rs>). EF is supported by the <rs type="funder">Google Award for Inclusion Research (AIR) Program</rs>. AL was partiallly supported by the <rs type="programName">Emerging Leaders in the Americas Program (ELAP) program</rs>. We also thank <rs type="person">Calcul Quebec</rs> and <rs type="funder">Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SRcHUhh">
					<orgName type="program" subtype="full">CAID program</orgName>
				</org>
				<org type="funding" xml:id="_C4fmXTE">
					<idno type="grant-number">PRH-2019-00009</idno>
				</org>
				<org type="funding" xml:id="_N3AnFN2">
					<orgName type="program" subtype="full">Emerging Leaders in the Americas Program (ELAP) program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 27.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving calibration and out-of-distribution detection in medical image segmentation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06569</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is segmentation uncertainty useful?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Czolbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arnavaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_55</idno>
		<idno>978-3-030-78191-0 55</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="715" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The devil is in the margin: marginbased label smoothing for network calibration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Calibrating deep neural networks using focal loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Confidence calibration and predictive uncertainty estimation for deep medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tempany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kapur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3868" to="3878" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Obtaining calibrated probability estimates from decision trees and Naive Bayesian classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="609" to="616" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using Bayesian binning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of Bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diverse ensembles improve calibration</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orthogonal ensemble networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Larrazabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_56</idno>
		<idno>978-3-030-87199-4 56</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? Evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations -Workshop Track</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards increased trustworthiness of deep learning segmentation methods on cardiac MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">10949</biblScope>
			<biblScope unit="page">1094919</biblScope>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatially varying label smoothing: capturing uncertainty from expert annotations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-052" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="677" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep interpretable classification and weakly-supervised segmentation of histology images via max-min uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging (TMI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transforming classifier scores into accurate multiclass probability estimates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Large Margin Classif</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A global benchmark of algorithms for segmenting late gadoliniumenhanced cardiac magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-867" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Standardized assessment of automatic segmentation of white matter hyperintensities and results of the WMH segmentation challenge</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kuijf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2556" to="2568" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mon. Weather Rev</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving class probability estimates for imbalanced data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Dahabreh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
