<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation</title>
				<funder ref="#_NTMWtd3">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieru</forename><surname>Mei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor, Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz, Santa Cruz</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengtian</forename><surname>Sang</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz, Santa Cruz</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz, Santa Cruz</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="486" to="496"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">FDED40B7E24AD6005608D734B5715053</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in large-scale Vision Transformers have made significant strides in improving pre-trained models for medical image segmentation. However, these methods face a notable challenge in acquiring a substantial amount of pre-training data, particularly within the medical field. To address this limitation, we present Masked Multiview with Swin Transformers (SwinMM), a novel multi-view pipeline for enabling accurate and data-efficient self-supervised medical image analysis. Our strategy harnesses the potential of multi-view information by incorporating two principal components. In the pre-training phase, we deploy a masked multi-view encoder devised to concurrently train masked multi-view observations through a range of diverse proxy tasks. These tasks span image reconstruction, rotation, contrastive learning, and a novel task that employs a mutual learning paradigm. This new task capitalizes on the consistency between predictions from various perspectives, enabling the extraction of hidden multi-view information from 3D medical data. In the fine-tuning stage, a cross-view decoder is developed to aggregate the multi-view information through a cross-attention block. Compared with the previous state-of-the-art self-supervised learning method Swin UNETR, SwinMM demonstrates a notable advantage on several medical image segmentation tasks. It allows for a smooth integration of multi-view information, significantly boosting both the accuracy and data-efficiency of the model. Code and models are available at https://github.com/UCSC-VLAA/SwinMM/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is a critical task in computer-assisted diagnosis, treatment planning, and intervention. While large-scale transformers have demonstrated impressive performance in various computer vision tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, such as natural image recognition, detection, and segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, they face significant challenges when applied to medical image analysis. The primary challenge is the scarcity of labeled medical images due to the difficulty in collecting and labeling them, which requires specialized medical knowledge and is timeconsuming <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. The second challenge is the ability to identify sparse and obscure patterns in medical images, including blurred and dim images with small segmentation targets. Hence, it is imperative to develop a precise and dataefficient pipeline for medical image analysis networks to enhance their accuracy and reliability in computer-assisted medical diagnoses.</p><p>Self-supervised learning, a technique for constructing feature embedding spaces by designing pretext tasks, has emerged as a promising solution for addressing the issue of label deficiency. One representative methodology for self-supervised learning is the masked autoencoder (MAE) <ref type="bibr" target="#b10">[11]</ref>. MAEs learn to reconstruct input data after randomly masking certain input features. This approach has been successfully deployed in various applications, including image denoising, text completion, anomaly detection, and feature learning. In the field of medical image analysis, MAE pre-training has also been found to be effective <ref type="bibr" target="#b31">[32]</ref>. Nevertheless, these studies have a limitation in that they require a large set of unlabeled data and do not prioritize improving output reliability, which may undermine their practicality in the real world.</p><p>In this paper, we propose Masked Multi-view with Swin (SwinMM), the first comprehensive multi-view pipeline for self-supervised medical image segmentation. We draw inspiration from previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> and aim to enhance output reliability and data utilization by incorporating multi-view learning into the self-supervised learning pipeline. During the pre-training stage, the proposed approach randomly masks 3D medical images and creates various observations from different views. A masked multi-view encoder processes these observations simultaneously to accomplish four proxy tasks: image reconstruction, rotation, contrastive learning, and a novel proxy task that utilizes a mutual learning paradigm to maximize consistency between predictions from different views. This approach effectively leverages hidden multi-view information from 3D medical data and allows the encoder to learn enriched high-level representations of the original images, which benefits the downstream segmentation task. In the fine-tuning stage, different views from the same image are encoded into a series of representations, which will interact with each other in a specially designed cross-view attention block. A multi-view consistency loss is imposed to generate aligned output predictions from various perspectives, which enhances the reliability and precision of the final output. The complementary nature of the different views used in SwinMM results in higher precision, requiring less training data and annotations, which holds significant potential for advancing the state-of-the-art in this field. In summary, the contributions of our study are as follows:</p><p>-We present SwinMM, a unique and data-efficient pipeline for 3D medical image analysis, providing the first comprehensive multi-view, self-supervised approach in this field. -Our design includes a masked multi-view encoder and a novel mutual learningbased proxy task, facilitating effective self-supervised pretraining. -We incorporate a cross-view decoder for optimizing the utilization of multiview information via a cross-attention block. -SwinMM delivers superior performance with an average Dice score of 86.18% on the WORD dataset, outperforming other leading segmentation methods in both data efficiency and segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> provides an overview of SwinMM, comprising a masked multi-view encoder and a cross-view decoder. SwinMM creates multiple views by randomly masking an input image, subsequently feeding these masked views into the encoder for self-supervised pre-training. In the fine-tuning stage, we architect a cross-view attention module within the decoder. This design facilitates the effective utilization of multi-view information, enabling the generation of more precise segmentation predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-training</head><p>Masked Multi-view Encoder. Following <ref type="bibr" target="#b10">[11]</ref>, we divided the 3D images into sub-volumes of the same size and randomly masked a portion of them, as demonstrated in Fig. <ref type="figure">2</ref>. These masked 3D patches, from different perspectives, were then utilized for self-supervised pretraining by the masked multi-view encoder.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the encoder is comprised of a patch partition layer, a patch embedding layer, and four Swin Transformer layers <ref type="bibr" target="#b16">[17]</ref>. Notably, unlike typical transformer encoders, our masked multi-view encoder can process multiple inputs from diverse images with varying views, making it more robust for a broad range of applications.</p><p>Pre-training Strategy. To incorporate multiple perspectives of a 3D volume, we generated views from different observation angles, including axial, coronal, and sagittal. Furthermore, we applied rotation operations aligned with each perspective, consisting of angles of 0 • , 90 • , 180 • , and 270 • along the corresponding direction. To facilitate self-supervised pre-training, we devised four proxy tasks. The reconstruction and rotation tasks measure the model's performance on each input individually, while the contrastive and mutual learning tasks enable the model to integrate information across multiple views.</p><p>-The reconstruction task compares the difference between unmasked input X and the reconstructed image y rec . Following <ref type="bibr" target="#b10">[11]</ref>, we adopt Mean-Square-Error (MSE) to compute the reconstruction loss: -The rotation task aims to detect the rotation angle of the masked input along the axis of the selected perspective, with possible rotation angles of 0 • , 90 • , 180 • , and 270 • . The model's performance is evaluated using cross-entropy loss, as shown in Eq. 2, where y rot and y r represent the predicted probabilities of the rotation angle and the ground truth, respectively.</p><formula xml:id="formula_0">L rec = (X -y rec ) 2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">L rot = - R r=1 y r log(y rot ).<label>(2)</label></formula><p>-The contrastive learning task aims to assess the effectiveness of a model in representing input data by comparing high-level features of multiple views. Our working assumption is that although the representations of the same sample may vary at the local level when viewed from different perspectives, they should be consistent at the global level. To compute the contrastive loss, we use cosine similarity sim(•), where y con i and y con j represent the contrastive pair, t is a temperature constant, and 1 is the indicator function.</p><formula xml:id="formula_3">L con = -log exp(sim(y con i , y con j )/t) 2N k 1 k =i exp(sim(y con i , y con k )/t) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>-The mutual learning task assesses the consistency of reconstruction results from different views to enable the model to learn aligned information from multi-view inputs. Reconstruction results are transformed into a uniform perspective and used to compute a mutual loss L mul , which, like the reconstruction task, employs the MSE loss. Here, y rec i and y rec j represent the predicted reconstruction from views i and j, respectively.</p><formula xml:id="formula_5">L mul = (y rec i -y rec j ) 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>The total pre-training loss is as shown in Eq. 5. The weight coefficients α 1 , α 2 , α 3 and α 4 are set equal in our experiment (α 1 = α 2 = α 3 = α 4 = 1).</p><formula xml:id="formula_7">L pre = α 1 L rec + α 2 L rot + α 3 L con + α 4 L mul .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Tuning</head><p>Cross-View Decoder. The structure of the cross-view decoder, comprising Conv-Blocks for skip connection, Up-Blocks for up-sampling, and a Crossview Attention block for views interaction, is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The Conv-Blocks operate on different layers, reshaping the latent representations from various levels of the masked multi-view encoder by performing the convolution, enabling them to conform to the feature size in corresponding decoder layers ( H <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>. At the bottom of the U-shaped structure, the cross-view attention module integrates the information from two views. The representations at this level are assumed to contain similar semantics. The details of the cross-view attention mechanism are presented in Fig. <ref type="figure" target="#fig_0">1</ref> and Eq. 6. In the equation, f i and f j denote the representations of different views, while Q i , K i , and V i refer to the query, key, and value matrices of f i , respectively.</p><formula xml:id="formula_8">2 i , W 2 i , D 2 i , i = 0, 1, 2, 3,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Attention(f</head><formula xml:id="formula_9">i , f j ) = [Softmax QiK j √ d V j , Softmax Qj K i √ d V i ].<label>(6)</label></formula><p>Multi-view Consistency Loss. We assume consistent segmentation results should be achieved across different views of the same volume. To quantify the consistency of the multi-view results, we introduce a consistency loss L mc , calculated using KL divergence in the fine-tuning stage, as in previous work on mutual learning <ref type="bibr" target="#b28">[29]</ref>. The advantage of KL divergence is that it does not require class labels and has been shown to be more robust during the fine-tuning stage.</p><p>We evaluate the effectiveness of different mutual loss functions in an ablation study (see supplementary). The KL divergence calculation is shown in Eq. 7:</p><formula xml:id="formula_10">L MC = D KL (V i V j ) = N m=1 V i (x m ) • log Vi(xm) Vj (xm) ,<label>(7)</label></formula><p>where V i (x m ) and V j (x m ) denote the different view prediction of m-th voxel.</p><p>N represents the number of voxels of case x. V i (x) and V j (x) denote different view prediction of case x. We measure segmentation performance using L DiceCE , which combines Dice Loss and Cross Entropy Loss according to <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_11">L DiceCE = 1 - N m=1 ( 2|pm∩ym| N (|pm|+|ym|) + ym log(pm) N ),<label>(8)</label></formula><p>where p m and y i respectively represent the predicted and ground truth labels for the m-th voxel, while N is the total number of voxels. We used L fin during the fine-tuning stage, as specified in Eq. 9, and added weight coefficients β DiceCE and β mc for different loss functions, both set to a default value of 1. </p><formula xml:id="formula_12">L fin = β DiceCE L DiceCE + β MC L MC . (<label>9</label></formula><formula xml:id="formula_13">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semi-supervised Learning with SwinMM</head><p>As mentioned earlier, the multi-view nature of SwinMM can substantially enhance the reliability and accuracy of its final output while minimizing the need for large, high-quality labeled medical datasets, making it a promising candidate for semi-supervised learning. In this study, we propose a simple variant of SwinMM to handle semi-supervision. As depicted in Fig. <ref type="figure">3</ref>, we leverage the diverse predictions from different views for unlabeled data and generate aggregated pseudo-labels for the training process. Compared to single-view models, SwinMM's multi-view scheme can alleviate prediction uncertainty by incorporating more comprehensive information from different views, while ensemble operations can mitigate individual bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets and Evaluation. Our pre-training dataset includes 5833 volumes from 8 public datasets: AbdomenCT-1K <ref type="bibr" target="#b18">[19]</ref>, BTCV <ref type="bibr" target="#b12">[13]</ref>, MSD <ref type="bibr" target="#b0">[1]</ref>, TCIA-Covid19 <ref type="bibr" target="#b8">[9]</ref>, WORD <ref type="bibr" target="#b17">[18]</ref>, TCIA-Colon <ref type="bibr" target="#b13">[14]</ref>, LiDC <ref type="bibr" target="#b1">[2]</ref>, and HNSCC <ref type="bibr" target="#b7">[8]</ref>. We choose two popular datasets, WORD (The Whole abdominal ORgan Dataset) and ACDC <ref type="bibr" target="#b2">[3]</ref> (Automated Cardiac Diagnosis Challenge), to test the downstream segmentation performance. The accuracy of our segmentation results is evaluated using two commonly used metrics: the Dice coefficient and Hausdorff Distance (HD).</p><p>Implementation Details. Our SwinMM is trained on 8 A100 Nvidia GPUs with 80G gpu memory. In the pre-training process, we use a masking ratio of 50%, a batch size of 2 on each GPU, and an initial learning rate of 5e-4 and weight decay of 1e-1. In the finetuning process, we apply a learning rate of 3e-4 and a layer-wise learning rate decay of 0.75. We set 100K steps for pre-training and 2500 epochs for fine-tuning. We use the AdamW optimizer and the cosine learning rate scheduler in all experiments with a warm-up of 50 iterations to train our model. We follow the official data-splitting methods on both WORD and ACDC, and report the results on the test dataset. For inference on these datasets, we applied a double slicing window inference, where the window size is 64 × 64 × 64 and the overlapping between windows is 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Comparing with SOTA Baselines. We compare the segmentation performance of SwinMM with several popular and prominent networks, comprising fully supervised networks, i.e., U-Net <ref type="bibr" target="#b21">[22]</ref>, Swin UNet <ref type="bibr" target="#b16">[17]</ref>, VT-UNet <ref type="bibr" target="#b20">[21]</ref>, UNETR <ref type="bibr" target="#b9">[10]</ref>, DeepLab V3+ <ref type="bibr" target="#b5">[6]</ref>, ESPNet <ref type="bibr" target="#b19">[20]</ref>, DMFNet <ref type="bibr" target="#b3">[4]</ref>, and LCOVNet <ref type="bibr" target="#b29">[30]</ref>, as well as self-supervised method Swin UNETR <ref type="bibr" target="#b23">[24]</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, our proposed SwinMM exhibits remarkable efficiency in medical segmentation by surpassing all other methods and achieves higher average Dice (86.18% on WORD and 90.80% on ACDC) and lower HD (9.35 on WORD and 6.37 on ACDC).</p><p>Single View vs. Multiple Views. To evaluate the effectiveness of our proposed multi-view self-supervised pretraining pipeline, we compared it with the state-of-the-art self-supervised learning method SwinUNETR <ref type="bibr" target="#b23">[24]</ref> on WORD <ref type="bibr" target="#b17">[18]</ref> dataset. Specifically, two SwinUNETR-based methods are compared: using fixed single views (Axial, Sagittal, and Coronal) and using ensembled predictions from multiple views (denoted as SwinUNETR-Fuse). Our results, presented in Table <ref type="table" target="#tab_2">3</ref>, show that our SwinMM surpasses all other methods including SwinUNETR-Fuse, highlighting the advantages of our unique multi-view designs. Moreover, by incorporating multi-view ensemble operations, SwinMM can effectively diminish the outliers in hard labels and produce more precise outputs, especially when dealing with harder cases such as smaller organs. The supplementary material provides qualitative comparisons of 2D/3D segmentation outcomes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>To fairly evaluate the benefits of our proposed multi-view design, we separately investigate its impact in the pre-training stage, the fine-tuning stage, as well as both stages. Additionally, we analyze the role of each pre-training loss functions.</p><p>Pre-training Loss Functions. The multi-view pre-training is implemented by proxy tasks. The role of each task can be revealed by taking off other loss functions. For cheaper computations, we only pre-train our model on 2639 volumes from 5 datasets (AbdomenCT-1K, BTCV, MSD, TCIA-Covid19, and WORD) in these experiments, and we applied a 50% overlapping window ratio, during testing time. As shown in Table <ref type="table" target="#tab_3">4</ref>, our proposed mutual loss brings a noticeable improvement in Dice (around 1%) over the original SwinUNETR setting. When combining all the proxy tasks, our SwinMM achieves the best performance.</p><p>Data Efficiency. The data efficiency is evaluated under various semi-supervised settings. Initially, a base model is trained from scratch with a proportion of supervised data from the WORD dataset for 100 epochs. Then, the base model finishes the remaining training procedure with unsupervised data. The proportion of supervised data (denoted as label ratio) varies from 10% to 100%. Table <ref type="table" target="#tab_4">5</ref> shows SwinMM consistently achieves higher Dice (%) than SwinUNETR, and its superiority is more remarkable when training with fewer supervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduces SwinMM, a self-supervised multi-view pipeline for medical image analysis. SwinMM integrates a masked multi-view encoder in the pre-training phase and a cross-view decoder in the fine-tuning phase, enabling seamless integration of multi-view information, thus boosting model accuracy and data efficiency. Notably, it introduces a new proxy task employing a mutual learning paradigm, extracting hidden multi-view information from 3D medical data. The approach achieves competitive segmentation performance and higher data-efficiency than existing methods and underscores the potential and efficacy of multi-view learning within the domain of self-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of our proposed SwinMM. The Conv-Blocks convolve the latent representations obtained from different levels of the masked multi-view encoder, adapting their feature size to match that of the corresponding decoder layer. The Up-Blocks perform deconvolution to upsample the feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. SwinMM's pre-training stage.Fig.3. The Fully-supervised/Semisupervised pipeline with SwinMM.</figDesc><graphic coords="6,53,55,54,62,166,00,143,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of ACDC dataset. Note: RV -right ventricle, Myomyocardium, LV -left ventricle.</figDesc><table><row><cell>Methods</cell><cell cols="2">DICE (%) ↑</cell><cell></cell><cell>HD ↓</cell></row><row><cell></cell><cell>RV</cell><cell>Myo LV</cell><cell cols="2">Average RV</cell><cell>Myo LV</cell><cell>Average</cell></row><row><cell>U-Net [22]</cell><cell cols="3">54.17 43.92 60.23 52.77</cell><cell cols="2">24.15 35.32 60.16 39.88</cell></row><row><cell>Swin UNet [17]</cell><cell cols="3">78.50 77.92 86.31 80.91</cell><cell cols="2">11.42 5.61 7.42 8.12</cell></row><row><cell>VT-UNet [21]</cell><cell cols="3">80.44 80.71 89.53 83.56</cell><cell cols="2">11.09 5.24 6.32 7.55</cell></row><row><cell>UNETR [10]</cell><cell cols="3">84.52 84.36 92.57 87.15</cell><cell cols="2">12.14 5.19 4.55 7.29</cell></row><row><cell>Swin UNETR [24]</cell><cell cols="3">87.49 88.25 92.72 89.49</cell><cell cols="2">12.45 5.78 4.03 7.42</cell></row><row><cell>SwinMM</cell><cell cols="3">90.21 88.92 93.28 90.80</cell><cell cols="2">8.85 3.10 7.16 6.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of WORD dataset. Note: Liv -liver, Spl -spleen, Kid L -left kidney, Kid R -right kidney, Sto -stomach, Gal -gallbladder, Eso -esophagus, Pan -pancreas, Duo -duodenum, Col -colon, Int -intestine, Adr -adrenal, Recrectum, Bla -bladder, Fem L -left femur, Fem R -right femur.</figDesc><table><row><cell>Methods</cell><cell>Liv</cell><cell>Spl</cell><cell cols="2">Kid L Kid R Sto</cell><cell>Gal Eso Pan Duo Col</cell><cell>Int</cell><cell>Adr Rec Bla</cell><cell>Fem L Fem R DICE (%)↑ HD↓</cell></row><row><cell>UNETR [10]</cell><cell cols="8">94.67 92.85 91.49 91.72 85.56 65.08 67.71 74.79 57.56 74.62 80.4 60.76 74.06 85.42 89.47 90.17 79.77</cell><cell>17.34</cell></row><row><cell>CoTr [27]</cell><cell cols="8">95.58 94.9 93.26 93.63 89.99 76.4 74.37 81.02 63.58 84.14 86.39 69.06 80.0 89.27 91.03 91.87 84.66</cell><cell>12.83</cell></row><row><cell cols="9">DeepLab V3+ [6] 96.21 94.68 92.01 91.84 91.16 80.05 74.88 82.39 62.81 82.72 85.96 66.82 81.85 90.86 92.01 92.29 84.91</cell><cell>9.67</cell></row><row><cell cols="9">Swin UNETR [24] 96.08 95.32 94.20 94.00 90.32 74.86 76.57 82.60 65.37 84.56 87.37 66.84 79.66 92.05 86.40 83.31 84.34</cell><cell>14.24</cell></row><row><cell>ESPNet [20]</cell><cell cols="8">95.64 93.9 92.24 94.39 87.37 67.19 67.91 75.78 62.03 78.77 72.8 60.55 74.32 78.58 88.24 89.04 79.92</cell><cell>15.02</cell></row><row><cell>DMFNet [4]</cell><cell cols="3">95.96 94.64 94.7</cell><cell cols="5">94.96 89.88 79.84 74.1 81.66 66.66 83.51 86.95 66.73 79.26 88.18 91.99 92.55 85.1</cell><cell>7.52</cell></row><row><cell>LCOVNet [30]</cell><cell cols="8">95.89 95.4 95.17 95.78 90.86 78.87 74.55 82.59 68.23 84.22 87.19 69.82 79.99 88.18 92.48 93.23 85.82</cell><cell>9.11</cell></row><row><cell>SwinMM</cell><cell cols="8">96.30 95.46 93.83 94.47 91.43 80.08 76.59 83.60 67.38 86.42 88.58 69.12 80.48 90.56 92.16 92.40 86.18</cell><cell>9.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results of WORD dataset. Abbreviations follows Table 2. 96.09 95.32 93.53 94.72 90.68 73.31 74.10 83.07 66.98 84.21 86.37 68.07 78.89 91.18 91.67 91.28 84.97 40.88 Swin UNETR Cor 96.12 95.49 93.91 94.80 90.25 71.78 75.27 82.83 66.26 84.07 86.98 66.23 79.38 90.93 88.09 86.74 84.32 14.02 Swin UNETR Fuse 96.25 95.71 94.20 94.85 91.05 74.80 77.04 83.73 67.36 85.15 87.69 67.84 80.29 92.31 90.44 89.36 85.50 13.87</figDesc><table><row><cell>Methods</cell><cell>Liv</cell><cell>Spl</cell><cell>Kid L Kid R Sto</cell><cell>Gal Eso Pan Duo Col</cell><cell>Int</cell><cell>Adr Rec Bla</cell><cell cols="2">Fem L Fem R DICE (%)↑ HD↓</cell></row><row><cell cols="8">Swin UNETR Axi 96.08 95.32 94.20 94.00 90.32 74.86 76.57 82.60 65.37 84.56 87.37 66.84 79.66 92.05 86.40 83.31 84.34</cell><cell>14.24</cell></row><row><cell>Swin UNETR Sag SwinMM</cell><cell cols="7">96.30 95.46 93.83 94.47 91.43 80.08 76.59 83.60 67.38 86.42 88.58 69.12 80.48 90.56 92.16 92.40 86.18</cell><cell>9.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The ablation study of proxy tasks during pre-training.</figDesc><table><row><cell>Methods</cell><cell cols="6">Rec Mut Rot Con DICE (%) ↑ HD ↓</cell></row><row><cell cols="2">SwinMM (w/o pretraining) -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.78</cell><cell>11.77</cell></row><row><cell>SwinMM</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.93</cell><cell>11.61</cell></row><row><cell>SwinMM</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>84.91</cell><cell>11.69</cell></row><row><cell>SwinMM</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>85.65</cell><cell>10.25</cell></row><row><cell>SwinMM</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>85.19</cell><cell>10.98</cell></row><row><cell>SwinMM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85.74</cell><cell>9.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The ablation study of label ratios.</figDesc><table><row><cell cols="3">label ratio Swin UNETR SwinMM</cell></row><row><cell>10%</cell><cell>56.14</cell><cell>67.83</cell></row><row><cell>30%</cell><cell>70.65</cell><cell>78.91</cell></row><row><cell>50%</cell><cell>77.28</cell><cell>82.03</cell></row><row><cell>70%</cell><cell>81.07</cell><cell>83.25</cell></row><row><cell>90%</cell><cell>82.46</cell><cell>84.32</cell></row><row><cell>100%</cell><cell>83.13</cell><cell>84.78</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is partially supported by the <rs type="programName">Google Cloud Research Credits program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NTMWtd3">
					<orgName type="program" subtype="full">Google Cloud Research Credits program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_47.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cervenansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D dilated multi-fiber network for real-time brain tumor segmentation in MRI</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_21" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="184" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_49" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imaging and clinical data archive for head and neck squamous cell carcinoma patients treated with radiotherapy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180173</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Harmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll'ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A distance transformation deep forest framework with hybridfeature fusion for CXR image classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-atlas segmentation of biomedical images: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="219" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accuracy of CT colonography for detection of large adenomas and cancers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Toledano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obstet. Gynecol. Surv</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="35" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ViT-NeT: interpretable vision transformers with neural tree decoder</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LViT: language meets vision transformer in medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WORD: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102642</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AbdomenCT-1K: is abdominal organ segmentation a solved problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ESPNet: efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01249-6_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01249-6_34" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11214</biblScope>
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3D tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_16" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of Swin transformers for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A learning based deformable template matching method for automatic rib centerline extraction and labeling in CT images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="980" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semisupervised medical image segmentation and domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoTr: efficiently bridging CNN and transformer for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MVCNet: multiview contrastive network for unsupervised representation learning for 3-D CT lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LCOV-NET: a lightweight neural network for COVID-19 pneumonia lesion segmentation from 3D CT images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MMGL: multi-scale multi-view global-local contrastive learning for semi-supervised cardiac image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="401" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05573</idno>
		<title level="m">Self pre-training with masked autoencoders for medical image analysis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
