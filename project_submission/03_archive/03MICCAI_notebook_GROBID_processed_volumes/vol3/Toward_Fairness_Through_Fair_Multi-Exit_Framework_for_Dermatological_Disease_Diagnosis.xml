<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ching-Hao</forename><surname>Chiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao-Wei</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Jen</forename><surname>Chen</surname></persName>
							<email>yujenchen@gapp.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsung-Yi</forename><surname>Ho</surname></persName>
							<email>tyho@cse.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="97" to="107"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">56C3763D764DDA6764FAC9677D1167B7</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dermatological Disease Diagnosis</term>
					<term>AI Fairness C</term>
					<term>-H</term>
					<term>Chiu and H</term>
					<term>-W</term>
					<term>Chung-Equal contributions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fairness has become increasingly pivotal in medical image recognition. However, without mitigating bias, deploying unfair medical AI systems could harm the interests of underprivileged populations. In this paper, we observe that while features extracted from the deeper layers of neural networks generally offer higher accuracy, fairness conditions deteriorate as we extract features from deeper layers. This phenomenon motivates us to extend the concept of multi-exit frameworks. Unlike existing works mainly focusing on accuracy, our multi-exit framework is fairness-oriented; the internal classifiers are trained to be more accurate and fairer, with high extensibility to apply to most existing fairnessaware frameworks. During inference, any instance with high confidence from an internal classifier is allowed to exit early. Experimental results show that the proposed framework can improve the fairness condition over the state-of-the-art in two dermatological disease datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, machine learning-based medical diagnosis systems have been introduced by many institutions. Although these systems achieve high accuracy in predicting medical conditions, bias has been found in dermatological disease datasets as shown in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">19]</ref>. This bias can arise when there is an imbalance in the number of images representing different skin tones, which can lead to inaccurate predictions and misdiagnosis due to biases towards certain skin tones.</p><p>The discriminatory nature of these models can have a negative impact on society by limiting access to healthcare resources for different sensitive groups, such as those based on race or gender.</p><p>Several methods are proposed to alleviate the bias in machine learning models, including pre-processing, in-processing, and post-processing strategies. Preprocessing strategies adjust training data before training <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref> or assign different weights to different data samples to suppress the sensitive information during training <ref type="bibr" target="#b9">[10]</ref>. In-processing modifies the model architecture, training strategy, and loss function to achieve fairness, such as adversarial training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">22]</ref> or regularization-based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">16]</ref> methods. Recently, pruning <ref type="bibr" target="#b21">[21]</ref> techniques have also been used to achieve fairness in dermatological disease diagnosis. However, these methods may decrease accuracy for both groups and do not guarantee explicit protection for the unprivileged group when enforcing fairness constraints. Postprocessing techniques enhance fairness by adjusting the model's output distribution. This calibration is done by taking the model's prediction, and the sensitive attribute as inputs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">23]</ref>. However, pre-processing and post-processing methods have limitations that are not applicable to dermatological disease diagnostic tasks since they need extra sensitive information during the training time.</p><p>In this paper, we observe that although features from a deep layer of a neural network are discriminative for target groups (i.e., different dermatological diseases), they cause fairness conditions to deteriorate, and we will demonstrate this observation by analyzing the entanglement degree regarding sensitive information with the soft nearest neighbor loss <ref type="bibr" target="#b4">[5]</ref> of image features in Sect. 2. This finding is similar to "overthinking" phenomenon in neural networks <ref type="bibr" target="#b10">[11]</ref>, where accuracy decreases as the features come from deeper in a neural network and motivate us to use a multi-exit network <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">18]</ref> to address the fairness issue.</p><p>Through extensive experiments, we demonstrate that our proposed multi-exit convolutional neural network (ME-CNN) can achieve fairness without using sensitive attributes (unawareness) in the training process, which is suitable for dermatological disease diagnosis because the sensitive attributes information exists privacy issues and is not always available. We compare our approach to the current state-of-the-art method proposed in <ref type="bibr" target="#b21">[21]</ref> and find that the ME-CNN can achieve similar levels of fairness. To further improve fairness, we designed a new framework for fair multi-exit. With the fairness constraint and the early exit strategy at the inference stage, a sufficient discriminative basis can be obtained based on low-level features when classifying easier samples. This contributes to selecting a more optimal prediction regarding the trade-off between accuracy and fairness for each test instance.</p><p>The main contributions of the proposed method are as follows:</p><p>-Our quantitative analysis shows that the features from a deep layer of a neural network are highly discriminative yet cause fairness to deteriorate. -We propose a fairness through unawareness framework and use multi-exit training to improve fairness in dermatological disease classification. -We demonstrate the extensibility of our framework, which can be applied to various state-of-the-art models and achieve further improvement. Through extensive experiments, we show that our approach can improve fairness while keeping competitive accuracy on both the dermatological disease dataset, ISIC 2019 <ref type="bibr" target="#b1">[2]</ref>, and Fitzpatrick-17k datasets <ref type="bibr" target="#b5">[6]</ref>. -For reproducibility, we have released our code at https://github.com/chiuhaohao/Fair-Multi-Exit-Framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In this section, we will discuss the motivation behind our work. The soft nearest neighbor loss (SNNL), as introduced in <ref type="bibr" target="#b4">[5]</ref>, measures the entanglement degree between features for different classes in the embedded space, and can serve as a proxy for analyzing the degree of fairness in a model. Precisely, we measure the features of SNNL concerning the different sensitive attributes. When the measured SNNL is high, the entangled features are indistinguishable among sensitive attributes. On the other hand, when the SNNL is low, the feature becomes more distinguishable between the sensitive attributes, leading to a biased performance in downstream tasks since the features consist of sensitive information.</p><p>To evaluate the SNNL, we analyzed the performance of ResNet18 <ref type="bibr" target="#b7">[8]</ref> and VGG-11 <ref type="bibr" target="#b17">[17]</ref> on the ISIC 2019 and Fitzpatrick-17k datasets. We compute the SNNL of sensitive attributes at different inference positions and observed that the SNNL in both datasets decreased by an average of 1.1% and 0.5%, respectively, for each inference position from shallow to deep in ResNet18. For VGG-11, the SNNL in both datasets decreases by an average of 1.4% and 1.2%, respectively. This phenomenon indicates that the features become more distinguishable to sensitive attributes. The details are provided in the supplementary materials.</p><p>A practical approach to avoiding using features that are distinguishable to sensitive attributes for prediction is to choose the result at a shallow layer for the final prediction. To the best of our knowledge, we are the first work that leverages the multi-exit network to improve fairness. In Sect. 5, we demonstrate that our framework can be applied to different network architectures and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>In the classification task, define input features x ∈ X = R d , target class, y ∈ Y = {1, 2, ..., N }, and sensitive attributes a ∈ A = {1, 2, ..., M }. In this paper, we focus on the sensitive attributes in binary case, that is a ∈ A = {0, 1}. The goal is to learn a classifier f : X → Y that predicts the target class y to achieve high accuracy while being unbiased to the sensitive attributes a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Exit (ME) Training Framework</head><p>Our approach is based on the observation that deep neural networks can exhibit bias against certain sensitive groups, despite achieving high accuracy in deeper layers. To address this issue, we propose a framework leveraging an early exit policy, which allows us to select a result at a shallower layer with high confidence while maintaining accuracy and mitigating fairness problems.</p><p>We illustrate our multi-exit framework in Fig. <ref type="figure" target="#fig_0">1</ref>. Our proposed loss function consists of the cross-entropy loss, l t , and a fairness regularization loss, l s , such as the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b8">[9]</ref> or the Hilbert-Schmidt Independence Criterion (HSIC) <ref type="bibr" target="#b16">[16]</ref>, which are replicated for each internal classifier (CLF n ) and original final classifier (CLF f ). The final loss is obtained through a weighted sum of each CLF 's loss, i.e., loss = (</p><formula xml:id="formula_0">n k=1 α k (l CLF k t + λl CLF k s )) + α f (l CLF f t +λl CLF f s</formula><p>), where α is determined by the depth of each CLF , similar to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">13]</ref>. Moreover, the hyperparameter λ controls the trade-off between accuracy and fairness. Our approach ensures that the model optimizes for accuracy and fairness, leveraging both shallow and deep layer features in the loss function. Even without any fairness regularization (λ = 0), our experiments demonstrate a notable improvement in fairness (see Sect. 5.2).</p><p>Our framework can also be extended to other pruning-based fairness methods, such as FairPrune <ref type="bibr" target="#b21">[21]</ref>. We first optimize the multi-exit model using the original multi-exit loss function and then prune it using the corresponding pruning strategy. Our approach has been shown to be effective (see Sect. 5.1).</p><p>During inference, we calculate the softmax score of each internal classifier's prediction, taking the maximum probability value as the confidence score. We use a confidence threshold θ to maintain fairness and accuracy. High-confidence instances exit early, and we select the earliest internal classifier with confidence above θ for an optimal prediction of accuracy and fairness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In this work, we evaluate our method on two dermatological disease datasets, including ISIC 2019 challenge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19]</ref> and the Fitzpatrick-17k dataset <ref type="bibr" target="#b5">[6]</ref>. ISIC 2019 challenge contains 25,331 images in 9 diagnostic categories for target labels, and we take gender as our sensitive attribute. The Fitzpatrick-17k dataset contains 16,577 images in 114 skin conditions of target labels and defines skin tone as the sensitive attribute. Next, we apply the data augmentation, including random flipping, rotation, scaling and autoaugment <ref type="bibr" target="#b2">[3]</ref>. After that, we follow the same data split described in <ref type="bibr" target="#b21">[21]</ref> to split the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details and Evaluation Protocol</head><p>We employ ResNet18 and VGG-11 as the backbone architectures for our models. The baseline CNN and the multi-exit models are trained for 200 epochs using an SGD optimizer with a batch size of 256 and a learning rate of 1e-2. Each backbone consisted of four internal classifiers (CLF s) and a final classifier (CLF f ). For ResNet18, the internal features are extracted from the end of each residual block, and for VGG-11 the features are extracted from the last four max pooling layers. The loss weight hyperparameter α is selected based on the approach of <ref type="bibr" target="#b10">[11]</ref> and set to [0.3, 0.45, 0.6, 0.75, 0.9] for the multi-exit models and the λ is set to 0.01. The confidence threshold θ of the test set is set to 0.999, based on the best result after performing a grid search on the validation set.</p><p>To evaluate the fairness performance of our framework, we adopted the multiclass equalized opportunity (Eopp0 and Eopp1) and equalized odds (Eodd) metrics proposed in <ref type="bibr" target="#b6">[7]</ref>. Specifically, we followed the approach of <ref type="bibr" target="#b21">[21]</ref> for calculating these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with State-of-the-Art</head><p>In this section, we compare our framework with several baselines, including CNN (ResNet18 and VGG-11), AdvConf <ref type="bibr" target="#b0">[1]</ref>, AdvRev <ref type="bibr" target="#b22">[22]</ref>, DomainIndep <ref type="bibr" target="#b20">[20]</ref>, HSIC <ref type="bibr" target="#b16">[16]</ref>, and MFD <ref type="bibr" target="#b8">[9]</ref>. We also compare our framework to the current state-of-theart method FairPrune <ref type="bibr" target="#b21">[21]</ref>. For each dataset, we report accuracy and fairness results, including precision, recall, and F1-score metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISIC 2019 Dataset.</head><p>In Table <ref type="table" target="#tab_0">1</ref>, ME-FairPrune refers to the FairPrune applied in our framework. It shows that our ME framework has improved all fairness scores and accuracy in all average scores when applied to FairPrune. Additionally, the difference in each accuracy metric is smaller than that of the original FairPrune. This is because the ME framework ensures that the early exited instances have a high level of confidence in their correctness, and the classification through shallower, fairer features further improves fairness. Fitzpatrick-17k Dataset. To evaluate the extensibility of our framework on different model structures, we use VGG-11 as the backbone of the Fitzpatrick-17k dataset. Table <ref type="table" target="#tab_1">2</ref> demonstrates that the ME framework outperforms all other methods with the best Eopp1 and Eodd scores, showing a 7.5% and 7.9% improvement over FairPrune, respectively. Similar to the ISIC 2019 dataset, our results show better mean accuracy and more minor accuracy differences in all criteria than FairPrune. Furthermore, the F1-score and Precision average value are superior to other methods, which show 8.1% and 4.1% improvement over FairPrune, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Exit Training on Different Method</head><p>In this section, we evaluate the performance of our ME framework when applied to different methods. Table <ref type="table" target="#tab_2">3</ref> presents the results of our experiments on ResNet18, MFD, and HSIC. Our ME framework improved the Eopp1 and Eodd scores of the original ResNet18 model, which did not apply fairness regularization loss, l s , in total loss. Furthermore, our framework achieved comparable performance in terms of fairness to FairPrune, as shown in Table <ref type="table" target="#tab_0">1</ref>. This demonstrates its potential to achieve fairness without using sensitive attributes during training.</p><p>We also applied our ME framework to MFD and HSIC, which initially exhibited better fairness performance than other baselines. With our framework, these models showed better fairness while maintaining similar levels of accuracy. These findings suggest that our ME framework can improve the fairness of existing models, making them more equitable without compromising accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Effect of Different Confidence Thresholds. To investigate the impact of varying confidence thresholds θ on accuracy and fairness, we apply the ME-FairPrune method to a pre-trained model from the ISIC 2019 dataset and test different thresholds. Our results, shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>, indicate that increasing the threshold improves accuracy and fairness. Thus, we recommend setting the threshold to 0.999 for optimal performance. Effect of Early Exits. In Fig. <ref type="figure" target="#fig_1">2</ref>(b), we compare ME-FairPrune using an early exit policy with exiting from each specific exit. The results show that across all criteria, no specific CLF outperforms the early exit policy in terms of Eodd, while our early exit policy achieves an accuracy level comparable to the original classifier output CLF f . These findings underscore the importance of the proposed early exit strategy for achieving optimal performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We address the issue of deteriorating fairness in deeper layers of deep neural networks by proposing a multi-exit training framework. Our framework can be applied to various bias mitigation methods and uses a confidence-based exit strategy to simultaneously achieve high accuracy and fairness. Our results demonstrate that our framework achieves the best trade-off between accuracy and fairness compared to the state-of-the-art on two dermatological disease datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed multi-exit training framework. lt and ls are the loss function related to target and sensitive attributes, respectively. The classifiers CLF1 through CLFn refer to internal classifiers, while CLF f refers to the original final classifier.</figDesc><graphic coords="4,59,31,367,07,305,86,152,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ablation study on (a) the early exit confidence threshold θ and (b) the early exit strategy for gender fairness on ISIC 2019 dataset. The CLF1 through CLF4 refer to internal classifiers, while CLF f refers to the final classifier.</figDesc><graphic coords="10,54,30,54,32,315,64,119,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of accuracy and fairness of different methods on ISIC 2019 dataset, using gender as the sensitive attribute. The female is the privileged group with higher accuracy by vanilla training. All results are our implementation.</figDesc><table><row><cell>Method</cell><cell cols="2">Gender Accuracy</cell><cell></cell><cell>Fairness</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Precision Recall F1-score Eopp0 ↓ Eopp1 ↓ Eodd ↓</cell></row><row><cell>ResNet18</cell><cell cols="2">Female 0.793</cell><cell>0.721 0.746</cell><cell>0.006</cell><cell>0.044</cell><cell>0.022</cell></row><row><cell></cell><cell>Male</cell><cell>0.731</cell><cell>0.725 0.723</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.762</cell><cell>0.723 0.735</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.063</cell><cell>0.004 0.023</cell><cell></cell><cell></cell></row><row><cell>AdvConf</cell><cell cols="2">Female 0.755</cell><cell>0.738 0.741</cell><cell>0.008</cell><cell>0.070</cell><cell>0.037</cell></row><row><cell></cell><cell>Male</cell><cell>0.710</cell><cell>0.757 0.731</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.733</cell><cell>0.747 0.736</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.045</cell><cell>0.020 0.010</cell><cell></cell><cell></cell></row><row><cell>AdvRev</cell><cell cols="2">Female 0.778</cell><cell>0.683 0.716</cell><cell>0.007</cell><cell>0.059</cell><cell>0.033</cell></row><row><cell></cell><cell>Male</cell><cell>0.773</cell><cell>0.706 0.729</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.775</cell><cell>0.694 0.723</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.006</cell><cell>0.023 0.014</cell><cell></cell><cell></cell></row><row><cell cols="3">DomainIndep Female 0.729</cell><cell>0.747 0.734</cell><cell>0.010</cell><cell>0.086</cell><cell>0.042</cell></row><row><cell></cell><cell>Male</cell><cell>0.725</cell><cell>0.694 0.702</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.727</cell><cell>0.721 0.718</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.004</cell><cell>0.053 0.031</cell><cell></cell><cell></cell></row><row><cell>HSIC</cell><cell cols="2">Female 0.744</cell><cell>0.660 0.696</cell><cell>0.008</cell><cell>0.042</cell><cell>0.020</cell></row><row><cell></cell><cell>Male</cell><cell>0.718</cell><cell>0.697 0.705</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.731</cell><cell>0.679 0.700</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.026</cell><cell>0.037 0.009</cell><cell></cell><cell></cell></row><row><cell>MFD</cell><cell cols="2">Female 0.770</cell><cell>0.697 0.726</cell><cell>0.005</cell><cell>0.051</cell><cell>0.024</cell></row><row><cell></cell><cell>Male</cell><cell>0.772</cell><cell>0.726 0.744</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.771</cell><cell>0.712 0.735</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.002</cell><cell>0.029 0.018</cell><cell></cell><cell></cell></row><row><cell>FairPrune</cell><cell cols="2">Female 0.776</cell><cell>0.711 0.734</cell><cell>0.007</cell><cell>0.026</cell><cell>0.014</cell></row><row><cell></cell><cell>Male</cell><cell>0.721</cell><cell>0.725 0.720</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.748</cell><cell>0.718 0.727</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.055</cell><cell>0.014 0.014</cell><cell></cell><cell></cell></row><row><cell cols="3">ME-FairPrune Female 0.770</cell><cell>0.723 0.742</cell><cell>0.006</cell><cell>0.020</cell><cell>0.010</cell></row><row><cell></cell><cell>Male</cell><cell>0.739</cell><cell>0.728 0.730</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.755</cell><cell>0.725 0.736</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.032</cell><cell>0.005 0.012</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of accuracy and fairness of different methods on Fitzpatrick-17k dataset, using skin tone as the sensitive attribute. The dark skin is the privileged group with higher accuracy by vanilla training. The results of VGG-11, AdvConf, AdvRev, DomainIndep and FairPrune are the experimental results reported in<ref type="bibr" target="#b21">[21]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Skin Tone Accuracy</cell><cell></cell><cell>Fairness</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Precision Recall F1-score Eopp0 ↓ Eopp1 ↓ Eodd ↓</cell></row><row><cell>VGG-11</cell><cell>Dark</cell><cell>0.563</cell><cell>0.581 0.546</cell><cell>0.0013</cell><cell>0.361</cell><cell>0.182</cell></row><row><cell></cell><cell>Light</cell><cell>0.482</cell><cell>0.495 0.473</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.523</cell><cell>0.538 0.510</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.081</cell><cell>0.086 0.073</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdvConf</cell><cell>Dark</cell><cell>0.506</cell><cell>0.562 0.506</cell><cell>0.0011</cell><cell>0.339</cell><cell>0.169</cell></row><row><cell></cell><cell>Light</cell><cell>0.427</cell><cell>0.464 0.426</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.467</cell><cell>0.513 0.466</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.079</cell><cell>0.098 0.080</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdvRev</cell><cell>Dark</cell><cell>0.514</cell><cell>0.545 0.503</cell><cell>0.0011</cell><cell>0.334</cell><cell>0.166</cell></row><row><cell></cell><cell>Light</cell><cell>0.489</cell><cell>0.469 0.457</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.502</cell><cell>0.507 0.480</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.025</cell><cell>0.076 0.046</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DomainIndep Dark</cell><cell>0.559</cell><cell>0.540 0.530</cell><cell>0.0012</cell><cell>0.323</cell><cell>0.161</cell></row><row><cell></cell><cell>Light</cell><cell>0.541</cell><cell>0.529 0.512</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.550</cell><cell>0.534 0.521</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.018</cell><cell>0.010 0.018</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HSIC</cell><cell>Dark</cell><cell>0.548</cell><cell>0.522 0.513</cell><cell>0.0013</cell><cell>0.331</cell><cell>0.166</cell></row><row><cell></cell><cell>Light</cell><cell>0.513</cell><cell>0.506 0.486</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg.↑</cell><cell>0.530</cell><cell>0.515 0.500</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.040</cell><cell>0.018 0.029</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MFD</cell><cell>Dark</cell><cell>0.514</cell><cell>0.545 0.503</cell><cell>0.0011</cell><cell>0.334</cell><cell>0.166</cell></row><row><cell></cell><cell>Light</cell><cell>0.489</cell><cell>0.469 0.457</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.502</cell><cell>0.507 0.480</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.025</cell><cell>0.076 0.046</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FairPrune</cell><cell>Dark</cell><cell>0.567</cell><cell>0.519 0.507</cell><cell cols="2">0.0008 0.330</cell><cell>0.165</cell></row><row><cell></cell><cell>Light</cell><cell>0.496</cell><cell>0.477 0.459</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.531</cell><cell>0.498 0.483</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.071</cell><cell>0.042 0.048</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ME-FairPrune Dark</cell><cell>0.564</cell><cell>0.529 0.523</cell><cell>0.0012</cell><cell>0.305</cell><cell>0.152</cell></row><row><cell></cell><cell>Light</cell><cell>0.542</cell><cell>0.535 0.522</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Avg. ↑</cell><cell>0.553</cell><cell>0.532 0.522</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Diff. ↓</cell><cell>0.022</cell><cell>0.006 0.001</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Accuracy and fairness of classification results across different baselines with and without the ME training framework on the ISIC 2019 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Gender Accuracy</cell><cell></cell><cell>Fairness</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Precision Recall F1-score Eopp0 ↓ Eopp1 ↓ Eodd ↓</cell></row><row><cell>ResNet18</cell><cell cols="2">Female 0.793</cell><cell>0.721 0.746</cell><cell>0.006</cell><cell>0.044</cell><cell>0.022</cell></row><row><cell></cell><cell>Male</cell><cell>0.731</cell><cell>0.725 0.723</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.762</cell><cell>0.723 0.735</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.063</cell><cell>0.004 0.023</cell><cell></cell><cell></cell></row><row><cell cols="3">ME-ResNet18 Female 0.748</cell><cell>0.724 0.733</cell><cell>0.006</cell><cell>0.031</cell><cell>0.016</cell></row><row><cell></cell><cell>Male</cell><cell>0.723</cell><cell>0.736 0.726</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.735</cell><cell>0.730 0.730</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.025</cell><cell>0.012 0.007</cell><cell></cell><cell></cell></row><row><cell>HSIC</cell><cell cols="2">Female 0.744</cell><cell>0.660 0.696</cell><cell>0.008</cell><cell>0.042</cell><cell>0.020</cell></row><row><cell></cell><cell>Male</cell><cell>0.718</cell><cell>0.697 0.705</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.731</cell><cell>0.679 0.700</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.026</cell><cell>0.037 0.009</cell><cell></cell><cell></cell></row><row><cell>ME-HSIC</cell><cell cols="2">Female 0.733</cell><cell>0.707 0.716</cell><cell>0.007</cell><cell>0.034</cell><cell>0.018</cell></row><row><cell></cell><cell>Male</cell><cell>0.713</cell><cell>0.707 0.707</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.723</cell><cell>0.707 0.712</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.020</cell><cell>0.000 0.009</cell><cell></cell><cell></cell></row><row><cell>MFD</cell><cell cols="2">Female 0.770</cell><cell>0.697 0.726</cell><cell>0.005</cell><cell>0.051</cell><cell>0.024</cell></row><row><cell></cell><cell>Male</cell><cell>0.772</cell><cell>0.726 0.744</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.771</cell><cell>0.712 0.735</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.002</cell><cell>0.029 0.018</cell><cell></cell><cell></cell></row><row><cell>ME-MFD</cell><cell cols="2">Female 0.733</cell><cell>0.698 0.711</cell><cell>0.005</cell><cell>0.024</cell><cell>0.012</cell></row><row><cell></cell><cell>Male</cell><cell>0.772</cell><cell>0.713 0.739</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Avg. ↑ 0.752</cell><cell>0.706 0.725</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Diff. ↓ 0.039</cell><cell>0.015 0.028</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Turning a blind eye: explicit removal of biases and variation from deep neural network embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nellåker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11009-3_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11009-334" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11129</biblScope>
			<biblScope unit="page" from="556" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">BCN20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">AutoAugment: learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fairness in deep learning: a computational perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="25" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2012" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating deep neural networks trained on clinical images in dermatology with the Fitzpatrick 17k dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1820" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fair feature distillation for visual recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12115" to="12124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shallow-deep networks: understanding and mitigating network overthinking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dumitras</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3301" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairness of classifiers across skin tones in dermatology</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kinyanjui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part VI</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="320" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-231" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gender bias in neural natural language processing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Amancharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-62077-6_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-62077-614" />
	</analytic>
	<monogr>
		<title level="m">Logic, Language, and Security</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Nigam</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12300</biblScope>
			<biblScope unit="page" from="189" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias remediation in driver drowsiness detection systems using generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ngxande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Tapamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="55592" to="55601" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering fair representations in the data domain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8227" to="8236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BranchyNet: fast inference via early exiting from deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2464" to="2469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards fairness in visual recognition: effective strategies for bias mitigation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8919" to="8928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FairPrune: achieving fairness through pruning for dermatological disease diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-670" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="743" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09457</idno>
		<title level="m">Men also like shopping: reducing gender bias amplification using corpus-level constraints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
