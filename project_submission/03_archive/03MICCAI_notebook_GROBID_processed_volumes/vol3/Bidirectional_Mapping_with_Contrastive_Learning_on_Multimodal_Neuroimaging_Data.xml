<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data</title>
				<funder ref="#_NRnzBqC #_tkNEn27">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_jDtqd3Q #_eZdVcuQ #_chW3Mx4 #_FJxSdTX #_7yjUubN">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoteng</forename><surname>Tang</surname></persName>
							<email>tanghaoteng@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas Rio Grande Valley</orgName>
								<address>
									<postCode>78539</postCode>
									<settlement>Edinburg</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyuan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johnny</forename><forename type="middle">Yuehan</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Thomas Jefferson High School for Science and Technology</orgName>
								<address>
									<postCode>22312</postCode>
									<settlement>Alexandria</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yalin</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Leow</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60612</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90032</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Zhan</surname></persName>
							<email>zhan.liang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="138" to="148"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9DF9439078017468094A3C43DBF7DBBE</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bidirectional reconstruction</term>
					<term>BOLD signals</term>
					<term>Structural networks</term>
					<term>Prediction</term>
					<term>Biomarkers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The modeling of the interaction between brain structure and function using deep learning techniques has yielded remarkable success in identifying potential biomarkers for different clinical phenotypes and brain diseases. However, most existing studies focus on one-way mapping, either projecting brain function to brain structure or inversely. This type of unidirectional mapping approach is limited by the fact that it treats the mapping as a one-way task and neglects the intrinsic unity between these two modalities. Moreover, when dealing with the same biological brain, mapping from structure to function and from function to structure yields dissimilar outcomes, highlighting the likelihood of bias in one-way mapping. To address this issue, we propose a novel bidirectional mapping model, named Bidirectional Mapping with Contrastive Learning (BMCL), to reduce the bias between these two unidirectional mappings via ROI-level contrastive learning. We evaluate our framework on clinical phenotype and neurodegenerative disease predictions using two publicly available datasets (HCP and OASIS). Our results demonstrate the superiority of BMCL compared to several state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advancements in applying machine learning techniques to MRI-based brain imaging studies have shown substantial progress in predicting neurodegenerative diseases (e.g., Alzheimer's Disease or AD) and clinical phenotypes (e.g., behavior measures), and in uncovering novel biomarkers that are closely related to them <ref type="bibr" target="#b3">[4]</ref>. Different MRI techniques can be used to depict different aspects of the brain organization or dynamics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. In general, diffusion MRI can derive brain structural networks that depict the connectivity of white matter tracks among brain regions, which gains system-level insights into the brain structural changes related to brain diseases and those phenotypes <ref type="bibr" target="#b28">[29]</ref>. However, the structural networks may not inform us about whether this tract or the regions it connects are "activated" or "not activated" in a specific state. As a complementary counterpart, the functional MRI provides measures of BOLD (blood-oxygenlevel-dependent) signals to present activities of brain regions over time <ref type="bibr" target="#b2">[3]</ref>, but no clue on whether those regions are physically connected or not. Therefore, different brain imaging data provide distinct but complementary information, and separately analyzing the data of each modality will always be suboptimal. In this context, multimodal approaches are being explored to improve prediction accuracy by integrating multiple information sources <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. For example, it has been shown that combining different modalities of data (e.g., image and text) can enhance performance in image classification and clustering tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. In the healthcare field, multimodal machine learning has shown its potential in disease detection and diagnosis <ref type="bibr" target="#b12">[13]</ref>. In brain imaging studies, many studies aim to explore multimodal MRI data representations by modeling the communications between functional MRI and its structural counterpart. Most of these studies primarily focus on establishing a unidirectional mapping between these two imaging modalities (i.e., mapping from structural MRI data to the functional counterpart <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>, or the inverse <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>). However, for the same biological brain, these two mappings generate distinct results, which highlights the likelihood of bias in the unidirectional mapping approach.</p><p>To address this, we propose a novel bidirectional mapping framework, where the mapping from structural MRI data (i.e., diffusion MRI-derived brain structural network) to the functional counterpart (i.e., BOLD signals) and the inverse mapping are implemented simultaneously. Unlike previous studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> that employ unidirectional mappings, our approach leverages bidirectional mapping, minimizing the discrepancies in the latent space of each one-way mapping through contrastive learning at the brain region-of-interest level (ROI level). This method subsequently unveils the inherent unity across both imaging modalities. Moreover, our framework is interpretable, where we employ integrated gradients <ref type="bibr" target="#b19">[20]</ref> to generate brain saliency maps for interpreting the outcomes of our model. Specifically, the identified top key brain ROIs in the brain saliency maps are closely related to the predicted diseases and clinical phenotypes. Extensive experiments have been conducted to demonstrate the effectiveness and superiority of our proposed method on two publicly available datasets (i.e., the Human Connectome Project (HCP), and Open Access Series of Imaging Studies (OASIS)). In summary, the contributions of this paper can be outlined as follows:</p><p>-We propose a novel bidirectional framework to yield multimodal brain MRI representations by modeling the interactions between brain structure and the functional counterpart. -We use contrastive learning to extract the intrinsic unity of both modalities. -The experimental results on two publicly available datasets demonstrate the superiority of our proposed method in predicting neurodegenerative diseases and clinical phenotypes. Furthermore, the interpretability analysis highlights that our method provides biologically meaningful insights. Afterward, ROI-level contrastive learning is applied to these extracted representations, facilitating their alignment in a common space. These derived representations are then utilized for downstream prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The proposed bidirectional mapping framework (Fig. <ref type="figure" target="#fig_0">1</ref>) comprises two encoderdecoder structures. One constructs BOLD signals from structural networks, while the other performs the inverse mapping. A ROI-level's contrastive learning is utilized between the encoder and decoder to minimize the distinction of the latent spaces within two reconstruction mappings. Finally, a multilayer perceptron (MLP) is utilized for task predictions. It's worth mentioning that instead of using the functional connectivity matrix, we directly utilize BOLD signals for bidirectional mapping. We believe this approach is reasonable as it allows us to capture the dynamic nature of the brain through the BOLD time sequence. Using the functional connectivity matrix may potentially disrupt this dynamic information due to the calculations of correlations. Furthermore, our experiments indicate that our encoder can directly model the temporal relations between different brain regions from the BOLD signals, eliminating the need to construct functional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries.</head><p>A structural brain network is an attributed and weighted graph G = (A, H) with N nodes, where H ∈ R N ×d is the node feature matrix, and A ∈ R N ×N is the adjacency matrix where a i,j ∈ R represents the edge weight between node i and node j. Meanwhile, we utilize X B ∈ R N ×T to represent the BOLD signal matrix derived from functional MRI data of each subject, where each brain ROI has a time series BOLD signal with T points.</p><p>Reconstruction. For the reconstruction task, we deploy an encoder-decoder architecture and utilize the L 1 loss function. Particularly, we use a multi-layer feed-forward neural network as the encoder and decoder. Our method differs from previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref>, where the encoder and decoder do not necessitate a GNN-based framework, allowing us to directly utilize the adjacency matrix A of structural networks as the inputs. Previous studies randomly initialize the node features (i.e., H) for the GNN input, since it is difficult to find informative brain node features that provide valuable information from the HCP and OASIS datasets. Hence, we propose a reconstruction framework that detours using the node feature matrix. Our framework is bidirectional, where we simultaneously conduct structural network and BOLD signal reconstruction. Here, we have latent representations Z B = Encoder B (X B ) and Z S = Encoder S (A) for BOLD signals and structural networks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROI-Level's Contrastive Representation Learning.</head><p>With latent representation Z B ∈ R N ×dB generated from BOLD signal and Z S ∈ R N ×dS from structural networks, we then conduct ROI-level's contrastive learning to associate the static structural and dynamic functional patterns of multimodal brain measurements. The contrastive learning loss aims to minimize the distinctions between latent representations from two modalities. To this end, we first utilize linear layers to project Z B and Z S to the common space, where we obtain</p><formula xml:id="formula_0">Z B = W Z B + b, Z B ∈ R N ×d and similarly, Z S ∈ R N ×d . We use (z B i , z S i ) i=1•••N</formula><p>to denote representations from the same ROI, where z B i and z S i are elements of Z B and Z S , respectively. For the same brain ROI, the static structural representation and the dynamic functional counterpart are expected to share a maximum similarity. Conversely, for the pairs that do not match, represented as (z B i , z S j ) i =j , these are drawn from different ROIs and should share a minimum similarity.</p><p>To formally build up the ROI-level's contrastive loss, it is intuitive to construct positive samples and negative ones based on the match of ROIs. Specifically, we construct (z B i , z S i ) i=1•••N as positive sample pair, and (z B i , z S j ) i =j as negative sample pair. And our contrastive loss can be formulated as follow:</p><formula xml:id="formula_1">L C1 = -E log i=1•••N Similarity(z B i , z S i ) N j=1 Similarity(z B i , z S j ) L C2 = -E log i=1•••N Similarity(z S i , z B i ) N j=1 Similarity(z S i , z B j ) L contrast = L C1 + L C2 (1)</formula><p>where Similarity(•) is substantiated as cosine similarity.</p><p>Loss Functions. The loss functions within our proposed framework are summarized here. Besides the reconstruction loss (L rec ) and the ROI-level's contrastive loss (L contrast ), we utilize cross-entropy loss (L supervised = L cross-entropy ) for classification tasks, and L 1 loss (L supervised = L mean-absolute-error ) for regression tasks, respectively. In summary, the loss function can be described as:</p><formula xml:id="formula_2">L = η 1 L contrast + η 2 L rec + η 3 L supervised , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where η 1 , η 2 and η 3 are loss weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Description and Preprocessing</head><p>Two publicly available datasets were used to evaluate our framework. The first includes data from 1206 young healthy subjects (mean age 28.19 ± 7.15, 657 women) from the Human Connectome Project <ref type="bibr" target="#b24">[25]</ref> (HCP). The second includes 1326 subjects (mean age = 70.42 ± 8.95, 738 women) from the Open Access Series of Imaging Studies (OASIS) dataset <ref type="bibr" target="#b11">[12]</ref>. Details of each dataset may be found on their official websites. CONN <ref type="bibr" target="#b25">[26]</ref> and FSL <ref type="bibr" target="#b9">[10]</ref> were used to reconstruct the functional and structural networks, respectively. For the HCP data, both networks have a dimension of 82 × 82 based on 82 ROIs defined using FreeSurfer (V6.0) <ref type="bibr" target="#b6">[7]</ref>. For the OASIS data, both networks have a dimension of 132 × 132 based on the Harvard-Oxford Atlas and AAL Atlas. We deliberately chose different network resolutions for HCP and OASIS, to evaluate whether the performance of our new framework is affected by the network dimension or atlas. The source code is available at: https://github.com/FlynnYe/BMCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup and Evaluation Metrics</head><p>We randomly split each dataset into 5 disjoint sets for 5-fold cross-validations, and all the results are reported in mean (s.t.d.) across 5 folds. To evaluate the performance of each model, we utilize accuracy, precision score, and F 1 score for classification tasks, and mean absolute error (MAE) for regression tasks. The learning rate is set as 1 × 10 -4 and 1 × 10 -3 for classification and regression tasks, respectively. The loss weights (i.e., η 1 ,η 2 , and η 3 ) are set equally as 1/3. To demonstrate the superiority of our method in cross-modal learning, bidirectional mapping, and ROI-level's contrastive learning, we select four baselines including 2 single-modal graph learning methods (i.e., DIFFPOOL <ref type="bibr" target="#b29">[30]</ref> and SAGPOOL <ref type="bibr" target="#b13">[14]</ref>), as well as 2 multimodal methods (i.e., VGAE <ref type="bibr" target="#b10">[11]</ref> and DSBGM <ref type="bibr" target="#b21">[22]</ref>) for all tasks. We use both functional brain networks, in which edge weights are defined as the Pearson Correlation between BOLD signals, and brain structural networks as input for baseline methods. The functional brain networks are signed graphs including positive and negative edge weights, however, the DIFFPOOL, SAGPOOL, and VGAE can only take unsigned graphs (i.e., graphs only include positive edges) as input. Therefore, we convert the functional brain networks to unsigned graphs by using the absolute values of the edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BOLD Signal and Structural Network Reconstruction</head><p>We train the model in a task-free manner where no task-specific supervised loss is involved. The MAE values between the edge weights in the ground-truth and reconstructed structural networks are 0.0413 ± 0.0009 and 0.0309 ± 0.0015 under 5-fold cross-validation on the HCP and OASIS, respectively. The MAE values between ground-truth and reconstructed BOLD signals are 0.0049 ± 0.0001 and 0.0734 ± 0.0016 on the HCP and OASIS, respectively. The reconstruction results on HCP are visualized in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Disease and Sex Classification</head><p>We conduct Alzheimer's disease (AD) classification on the OASIS dataset, and sex classification on the HCP dataset. As shown in Table <ref type="table" target="#tab_0">1</ref>, our proposed BMCL can achieve the best results in accuracy, precision, and F 1 score for both tasks among all methods. For example, in the AD classification, our model outperforms the baselines with at least 4.2%, 5.8% and 4.0% increases in accuracy, precision and F 1 scores, respectively. In general, multimodal methods can outperform single-model methods. The superiority of our bidirectional BMCL model, compared to the unidirectional methods, attributes to the fact that our BMCL reduces the distinction between the latent spaces generated by two unidirectional mappings through ROI-level's contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ASR and MMSE Regression</head><p>Mini-Mental State Exam (MMSE) is a quantitative measure of cognitive status in adults, and Adult Self-Report scale (ASR) <ref type="bibr" target="#b0">[1]</ref> is to measure the adult's behavior. As shown in Table <ref type="table" target="#tab_1">2</ref>, our proposed BMCL model outperforms all baselines in terms of MAE values. The regression results also demonstrate the superiority of bidirectional mapping and the importance of ROI-level's contrastive learning, which is consistent with the results in the classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>To demonstrate the significance of bidirectional mapping, we remove a part of our proposed BMCL model to yield two unidirectional mappings (i.e., either mapping from structural network to BOLD signal, or mapping inversely). As shown in the bottom three rows in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, the prediction results are declined when we remove each directional mapping, which clearly demonstrates the importance of bidirectional mapping.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Interpretability</head><p>The 10 key brain regions (Fig. <ref type="figure" target="#fig_2">3</ref>) associated with AD (from OASIS) and with each sex (from HCP) are identified using the brain saliency map. The salient regions for AD are concentrated in cerebelum (i.e., cerebelum 3 right and left, cerebelum 8 left, cerebelum crus2 right) and middle Temporal gyrus (i.e., the posterior division left and right, as well as the temporooccipital right of middle temporal gyrus), which have been verified as core AD biomarkers in literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. Similarly, 10 key regions (Fig. <ref type="figure" target="#fig_2">3</ref>) are identified for regression tasks (i.e., 3 ASR from HCP and MMSE from OASIS). Interestingly, several brain regions (including left and right accumbens areas, cortex left hemisphere cuneus and insula, as well as cortex right hemisphere posteriorcingulate and parahippocampal) are consistently identified across 3 ASR scales (i.e., aggression, rule-break, and intrusive). This finding is supported by <ref type="bibr" target="#b20">[21]</ref>, which suggests that similar ASR exhibits common or similar biomarkers. Also, these regions have been reported as important biomarkers for aggressive-related behaviors in literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We propose a new multimodal data mining framework, named BMCL, to learn the representation from two modality data through bidirectional mapping between them. The elaborated ROI-level contrastive learning in BMCL can reduce the distinction and eliminate biases between two one-way mappings.</p><p>Our results on two publicly available datasets show that BMCL outperforms all baselines, which demonstrates the superiority of bidirectional mapping with ROI-level contrastive learning. Beyond these, our model can identify key brain regions highly related to different clinical phenotypes and brain diseases, which demonstrates that our framework is interpretable and the results are biologically meaningful. The contrastive learning method, while emphasizing the alignment of features from different modalities, may inadvertently neglect the unique characteristics inherent to each modality. Moving forward, we intend to refine our method by aiming for a balance between the alignment of modalities and the preservation of modality-specific information. Additionally, the pre-selection of important features or the consideration of subnetworks holds promising for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of Bidirectional Mapping with Contrastive Learning (BMCL).The brain structural network and BOLD signals are initially processed by two separate encoders for representation learning. Afterward, ROI-level contrastive learning is applied to these extracted representations, facilitating their alignment in a common space. These derived representations are then utilized for downstream prediction tasks.</figDesc><graphic coords="3,77,31,179,30,269,74,163,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Bidirectional reconstruction results on the HCP and OASIS dataset.</figDesc><graphic coords="5,51,00,295,97,200,65,211,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Saliency maps to identify top 10 regions associated with (a) intrusiveness, (b) aggression, (c) rule-break, (d) sex, (e) AD and (f) MMSE, respectively.</figDesc><graphic coords="8,71,46,219,56,309,40,170,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results for sex classification on HCP and AD classification on OASIS. The best results are highlighted in bold font. Methods marked with † are unimodal methods.</figDesc><table><row><cell>Method</cell><cell>HCP (gender)</cell><cell></cell><cell></cell><cell cols="2">OASIS (disease)</cell></row><row><cell></cell><cell>Acc</cell><cell>Pre</cell><cell>F1</cell><cell>Acc</cell><cell>Pre</cell><cell>F1</cell></row><row><cell cols="7">DIFFPOOL  † w/ F 67.77 (3.56) 65.25 (2.65) 68.82 (1.72) 68.97 (1.34) 66.03 (3.36) 69.24 (1.83)</cell></row><row><cell cols="7">SAGPOOL  † w/ F 70.95 (2.88) 69.83 (1.85) 71.44 (1.29) 65.65 (2.01) 63.33 (1.95) 67.27 (2.09)</cell></row><row><cell cols="4">DIFFPOOL  † w/ S 58.71 (4.62) 30.96 (4.73) 40.6 (5.17)</cell><cell cols="3">86.04 (2.65) 64.92 (4.16) 74.01 (3.64)</cell></row><row><cell cols="7">SAGPOOL  † w/ S 61.06 (4.58) 32.79 (3.54) 42.64 (3.78) 88.48 (2.51) 68.71 (3.92) 77.33 (3.44)</cell></row><row><cell>VGAE</cell><cell>73.59(2.42)</cell><cell cols="5">74.43 (1.84) 76.25 (1.49) 64.68 (2.49) 62.57 (2.19) 65.85 (1.91)</cell></row><row><cell>DSBGM</cell><cell cols="3">82.19 (2.01) 85.35 (1.99) 84.71(2.37)</cell><cell cols="3">78.92 (1.38) 79.81 (1.41) 80.22(2.25)</cell></row><row><cell>BMCL w/o F</cell><cell cols="6">93.68 (2.88) 91.71 (2.19) 92.31 (2.31) 90.09 (2.65) 71.26 (4.16) 83.61 (3.64)</cell></row><row><cell>BMCL w/o S</cell><cell cols="6">69.54 (1.77) 68.61 (1.71) 56.82 (2.60) 89.66 (2.93) 73.35 (3.15) 79.52 (3.29)</cell></row><row><cell>BMCL</cell><cell cols="6">94.83 (1.35) 93.47 (3.65) 93.21 (1.98) 92.23 (0.62) 84.47(2.14) 83.38(0.76)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The experimental results for ASR regression on HCP and MMSE regression on OASIS. The best results are highlighted in bold font. Methods marked with † are unimodal methods.</figDesc><table><row><cell>Method</cell><cell cols="4">HCP (aggression) HCP (rule-break) HCP (intrusive) OASIS (MMSE)</cell></row><row><cell cols="2">DIFFPOOL  † w/ F 2.39 (0.021)</cell><cell>2.26 (0.0092)</cell><cell>2.47 (0.15)</cell><cell>1.77 (0.56)</cell></row><row><cell cols="2">SAGPOOL  † w/ F 3.07 (0.062)</cell><cell>2.88 (0.0022)</cell><cell>3.47 (0.029)</cell><cell>1.73 (0.79)</cell></row><row><cell cols="2">DIFFPOOL  † w/ S 1.78 (0.268)</cell><cell>1.12 (0.473)</cell><cell>0.61 (0.3335)</cell><cell>2.13 (15.5941)</cell></row><row><cell cols="2">SAGPOOL  † w/ S 1.82 (0.2674)</cell><cell>1.13 (0.3672)</cell><cell>0.63 (0.2608)</cell><cell>0.53 (0.2125)</cell></row><row><cell>VGAE</cell><cell>1.74(0.019)</cell><cell>1.37(0.051)</cell><cell>0.67 (0.022)</cell><cell>1.27 (0.25)</cell></row><row><cell>DSBGM</cell><cell>1.71 (0.11)</cell><cell>1.21 (0.24)</cell><cell>0.65 (0.026)</cell><cell>0.87 (0.18)</cell></row><row><cell>BMCL w/o F</cell><cell>1.98 (0.2688)</cell><cell>1.12 (0.3508)</cell><cell>0.62 (0.3145)</cell><cell>0.49 (0.1908)</cell></row><row><cell>BMCL w/o S</cell><cell>2.03 (0.2045)</cell><cell>1.11 (0.3704)</cell><cell>0.63 (0.3839)</cell><cell>0.50 (0.2008)</cell></row><row><cell>BMCL</cell><cell>1.68 (0.2374)</cell><cell>1.05 (0.5046)</cell><cell>0.58 (0.3377)</cell><cell>0.45 (0.1726)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This study was partially supported by <rs type="funder">NIH</rs> (<rs type="grantNumber">R01AG071243</rs>, <rs type="grantNumber">R01MH125928</rs>, <rs type="grantNumber">R21AG065942</rs>, <rs type="grantNumber">R01EY032125</rs>, and <rs type="grantNumber">U01AG068057</rs>) and <rs type="funder">NSF</rs> (<rs type="grantNumber">IIS 2045848</rs> and <rs type="grantNumber">IIS 1837956</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jDtqd3Q">
					<idno type="grant-number">R01AG071243</idno>
				</org>
				<org type="funding" xml:id="_eZdVcuQ">
					<idno type="grant-number">R01MH125928</idno>
				</org>
				<org type="funding" xml:id="_chW3Mx4">
					<idno type="grant-number">R21AG065942</idno>
				</org>
				<org type="funding" xml:id="_FJxSdTX">
					<idno type="grant-number">R01EY032125</idno>
				</org>
				<org type="funding" xml:id="_7yjUubN">
					<idno type="grant-number">U01AG068057</idno>
				</org>
				<org type="funding" xml:id="_NRnzBqC">
					<idno type="grant-number">IIS 2045848</idno>
				</org>
				<org type="funding" xml:id="_tkNEn27">
					<idno type="grant-number">IIS 1837956</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manual for the ASEBA brief problem monitor (BPM)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Achenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcconaughy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rescorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASEBA</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2011">2011</date>
			<pubPlace>Burlington, VT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identification of novel GDNF isoforms and cis-antisense GDNFOS gene and their regulation in human middle temporal gyrus of Alzheimer disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Airavaara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pletnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biol. Chem</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="45093" to="45102" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Functional brain network organisation of children between 2 and 5 years derived from reconstructed activity of cortical sources of high-density EEG recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bathelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Clayden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="595" to="604" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal fusion of brain imaging data: a key to finding the missing link (s) in complex mental illness</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiatry Cogn. Neurosci. Neuroimaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The rewarding effect of aggression is reduced by nucleus accumbens dopamine receptor antagonism in mice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Couppis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychopharmacology</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BrainGB: a benchmark for brain network analysis with graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freesurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fundamentals of Brain Network Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fornito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zalesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to predict without looking ahead: world models without forward prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="790" />
			<date type="published" when="2012">2012</date>
			<publisher>FSL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Oasis-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer disease</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lamontagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2012" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting Alzheimer&apos;s disease progression using multi-modal deep learning approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1952</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BrainGNN: interpretable brain graph neural network for fMRI analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102233</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatial-temporal convolutional attention for mapping functional brain networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.02315</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The role of asymmetrical frontal cortical activity in aggression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Shackman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harmon-Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="92" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transcriptome changes in the Alzheimer&apos;s disease middle temporal gyrus: importance of RNA metabolism and mitochondria-associated membrane genes</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Piras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Alzheimers Dis</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="691" to="713" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The influence of construction methodology on structural brain network measures: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meesters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nicolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ossenblok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="170" to="182" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical brain embedding using explainable graph learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Signed graph representation learning for functional-to-structural brain network mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102674</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive brain network learning via hierarchical signed graph pooling model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mapping functional brain networks from the structural connectome: relating the series expansion and eigenmode approaches</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tewarie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page">116805</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Wu-Minn human connectome project: an overview</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="62" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whitfield-Gabrieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nieto-Castanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Connect</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show, attend and tell: neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling spatio-temporal patterns of holistic functional brain networks via multi-head guided attention graph neural networks (multi-head GaGNNs)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102518</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mapping structural connectivity using diffusion MRI: challenges and opportunities</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Connelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1666" to="1682" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting brain structural network using functional connectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102463</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep representation learning for multimodal brain networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-360" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="613" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep supervised cross-modal retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10394" to="10403" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
