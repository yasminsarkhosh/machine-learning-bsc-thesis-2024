<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACC-UNet: A Completely Convolutional UNet Model for the 2020s</title>
				<funder ref="#_n8eUkC9">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_r9bbq3N">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
				<funder ref="#_vH2y8Cx #_zEqFTDQ #_yERFmV8 #_B8vpBhV #_hpNR2cR #_S6vPxWW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nabil</forename><surname>Ibtehaz</surname></persName>
							<idno type="ORCID">0000-0003-3625-5972</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
							<email>dkihara@purdue.edu</email>
							<idno type="ORCID">0000-0003-4091-6614</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACC-UNet: A Completely Convolutional UNet Model for the 2020s</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="692" to="702"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">431C3A96844689C0B2EE561D376AF350</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>UNet</term>
					<term>image segmentation</term>
					<term>fully convolutional network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This decade is marked by the introduction of Vision Transformer, a radical paradigm shift in broad computer vision. A similar trend is followed in medical imaging, UNet, one of the most influential architectures, has been redesigned with transformers. Recently, the efficacy of convolutional models in vision is being reinvestigated by seminal works such as ConvNext, which elevates a ResNet to Swin Transformer level. Deriving inspiration from this, we aim to improve a purely convolutional UNet model so that it can be on par with the transformer-based models, e.g., Swin-Unet or UCTransNet. We examined several advantages of the transformer-based UNet models, primarily long-range dependencies and cross-level skip connections. We attempted to emulate them through convolution operations and thus propose, ACC-UNet, a completely convolutional UNet model that brings the best of both worlds, the inherent inductive biases of convnets with the design decisions of transformers. ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and consistently outperformed convnets, transformers, and their hybrids. Notably, ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by 2.64 ± 2.54% and 0.45 ± 1.61% in terms of dice score, respectively, while using a fraction of their parameters (59.26% and 24.24%). Our codes are available at https://github.com/ kiharalab/ACC-UNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation, an essential component of computer-aided medical image analysis, identifies and highlights regions of interest in various diagnosis tasks. However, this often becomes complicated due to various factors involving image modality and acquisition along with pathological and biological variations <ref type="bibr" target="#b18">[18]</ref>. The application of deep learning in this domain has thus certainly benefited in this regard. Most notably, ever since its introduction, the UNet model <ref type="bibr" target="#b19">[19]</ref> has demonstrated astounding efficacy in medical image segmentation. As a result, UNet and its derivatives have become the de-facto standard <ref type="bibr" target="#b26">[25]</ref>.</p><p>The original UNet model comprises a symmetric encoder-decoder architecture (Fig. <ref type="figure" target="#fig_0">1a</ref>) and employs skip-connections, which provide the decoder spatial information probably lost during the pooling operations in the encoder. Although this information propagation through simple concatenation improves the performance, there exists a likely semantic gap between the encoder-decoder feature maps. This led to the development of a second class of UNets (Fig. <ref type="figure" target="#fig_0">1b</ref>). U-Net++ <ref type="bibr" target="#b27">[26]</ref> leveraged dense connections and MultiResUNet <ref type="bibr" target="#b11">[11]</ref> added additional convolutional blocks along the skip connection as a potential remedy.</p><p>Till this point in the history of UNet, all the innovations were performed using CNNs. However, the decade of 2020 brought radical changes in the computer vision landscape. The long-standing dominance of CNNs in vision was disrupted by vision transformers <ref type="bibr" target="#b6">[7]</ref>. Swin Transformers <ref type="bibr" target="#b15">[15]</ref> further adapted transformers for general vision applications. Thus, UNet models started adopting transformers <ref type="bibr" target="#b4">[5]</ref>. Swin-Unet <ref type="bibr" target="#b8">[9]</ref> replaced the convolutional blocks with Swin Transformer blocks and thus initiated a new class of models (Fig. <ref type="figure" target="#fig_0">1c</ref>). Nevertheless, CNNs still having various merits in image segmentation, led to the development of fusing those two <ref type="bibr" target="#b1">[2]</ref>. This hybrid class of UNet models (Fig. <ref type="figure" target="#fig_0">1d</ref>) employs convolutional blocks in the encoder-decoder and uses transformer layers along the skip connections. UCTransNet <ref type="bibr" target="#b22">[22]</ref> and MCTrans <ref type="bibr" target="#b24">[24]</ref> are two representative models of this class. Finally, there have also been attempts to develop all-transformer UNet architectures (Fig. <ref type="figure" target="#fig_0">1e</ref>), for instance, SMESwin Unet <ref type="bibr" target="#b28">[27]</ref> uses transformer both in encoder-decoder blocks and the skip-connection.</p><p>Very recently, studies have begun rediscovering the potential of CNNs in light of the advancements brought by transformers. The pioneering work in this regard is 'A ConvNet for the 20202020ss' <ref type="bibr" target="#b16">[16]</ref>, which explores the various ideas introduced by transformers and their applicability in convolutional networks. By gradually incorporating ideas from training protocol and micro-macro design choices, this work enabled ResNet models to outperform Swin Transformer models.</p><p>In this paper, we ask the same question but in the context of UNet models. We investigate if a UNet model solely based on convolution can compete with the transformer-based UNets. In doing so, we derive motivations from the transformer architecture and develop a purely convolutional UNet model. We propose a patch-based context aggregation contrary to window-based self-attention. In addition, we innovate the skip connections by fusing the feature maps from multiple levels of encoders. Extensive experiments on 5 benchmark datasets suggest that our proposed modifications have the potential to improve UNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Firstly, we analyze the transformer-based UNet models from a high-level. Deriving motivation and insight from this, we design two convolutional blocks to simulate the operations performed in transformers. Finally, we integrate them in a vanilla UNet backbone and develop our proposed ACC-UNet architecture. Leveraging the Long-Range Dependency of Self-attention. Transformers can compute features from a much larger view of context through the use of (windowed) self-attention. In addition, they improve expressivity by adopting inverted bottlenecks, i.e., increasing the neurons in the MLP layer. Furthermore, they contain shortcut connections, which facilitate the learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>Adaptive Multi-level Feature Combination Through Channel Attention. Transformer-based UNets fuse the feature maps from multiple encoder levels adaptively using channel attention. This generates enriched features due to the combination of various regions of interest from different levels compared to simple skip-connection which is limited by the information at the current level <ref type="bibr" target="#b22">[22]</ref>.</p><p>Based on these observations, we modify the convolutional blocks and skipconnections in a vanilla UNet model to induce the capabilities of long-range dependency and multi-level feature combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Aggregation of Neighborhood Context (HANC)</head><p>We first explore the possibility of inducing long-range dependency along with improving expressivity in convolutional blocks. We only use pointwise and depthwise convolutions to reduce the computational complexity <ref type="bibr" target="#b7">[8]</ref>.</p><p>In order to increase the expressive capability, we propose to include inverted bottlenecks in convolutional blocks <ref type="bibr" target="#b16">[16]</ref>, which can be achieve by increasing the number of channels from c in to c inv = c in * inv_fctr using pointwise convolution. Since these additional channels will increase the model complexity, we use 3 × 3 depthwise convolution to compensate. An input feature map x in ∈ R cin,n,m is thus transformed to x 1 ∈ R cinv,n,m as (Fig. <ref type="figure" target="#fig_1">2b</ref>)</p><formula xml:id="formula_0">x 1 = DConv 3×3 (P Conv cin→cinv (x in ))<label>(1)</label></formula><p>Next, we wish to emulate self-attention in our convolution block, which at its core is comparing a pixel with the other pixels in its neighborhood <ref type="bibr" target="#b15">[15]</ref>. This comparison can be simplified by comparing a pixel value with the mean and maximum of its neighborhood. Therefore, we can provide an approximate notion of neighborhood comparison by appending the mean and max of the neighboring pixel features. Consecutive pointwise convolution can thus consider these and capture a contrasting view. Since hierarchical analysis is beneficial for images <ref type="bibr" target="#b23">[23]</ref>, instead of computing this aggregation in a single large window, we compute this in multiple levels hierarchically, for example,</p><formula xml:id="formula_1">2 × 2, 2 2 × 2 2 , • • • , 2 k-1 × 2 k-1</formula><p>patches. For k = 1, it would be the ordinary convolution operation, but as we increase the value of k, more contextual information will be provided, bypassing the need for larger convolutional kernels. Thus, our proposed hierarchical neighborhood context aggregation enriches feature map x 1 ∈ R cinv,n,m with contextual information as x 2 ∈ R cinv * (2k-1),n,m (Fig. <ref type="figure" target="#fig_1">2b</ref>), where || corresponds to concatenation along the channel dimension</p><formula xml:id="formula_2">x 2 = (x 1 ||mean 2×2 (x 1 )||mean 2 2 ×2 2 (x 1 )|| • • • ||mean 2 k-1 ×2 k-1 (x 1 ) ||max 2×2 (x 1 )||max 2 2 ×2 2 (x 1 )|| • • • ||max 2 k-1 ×2 k-1 (x 1 ))<label>(2)</label></formula><p>Next, similar to the transformer, we include a shortcut connection in the convolution block for better gradient propagation. Hence, we perform another pointwise convolution to reduce the number of channels to c in and add with the input feature map. Thus, x 2 ∈ R cinv * (2k-1),n,m becomes x 3 ∈ R cin,n,m (Fig. <ref type="figure" target="#fig_1">2b</ref>)</p><formula xml:id="formula_3">x 3 = P Conv cinv * (2k-1)→cin (x 2 ) + x in (3)</formula><p>Finally, we change the number of filters to c out , as the output, using pointwise convolution (Fig. <ref type="figure" target="#fig_1">2b</ref>)</p><formula xml:id="formula_4">x out = P Conv cin→cout (x 3 )<label>(4)</label></formula><p>Thus, we propose a novel Hierarchical Aggregation of Neighborhood Context (HANC) block using convolution but bringing the benefits of transformers. The operation of this block is illustrated in Fig. <ref type="figure" target="#fig_1">2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi Level Feature Compilation (MLFC)</head><p>Next, we investigate the feasibility of multi-level feature combination, which is the other advantage of using transformer-based UNets.</p><p>Transformer-based skip connections have demonstrated effective feature fusion of all the encoder levels and appropriate filtering from the compiled feature maps by the individual decoders <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">27]</ref>. This is performed through concatenating the projected tokens from different levels <ref type="bibr" target="#b22">[22]</ref>. Following this approach, we resize the convolutional feature maps obtained from the different encoder levels to make them equisized and concatenate them. This provides us with an overview of the feature maps across the different semantic levels. We apply pointwise convolution operation to summarize this representation and merge with the corresponding encoder feature map. This fusion of the overall and individual information is passed through another convolution, which we hypothesize enriches the current level feature with information from other level features.</p><p>For the features, x 1 , x 2 , x 3 , x 4 from 4 different levels, the feature maps can be enriched with multilevel information as (Fig. <ref type="figure" target="#fig_1">2d</ref>)</p><formula xml:id="formula_5">x comb,i = P Conv ctot→ci (resize i (x1)||resize i (x2)||resize i (x3)||resize i (x4)) (5) x i = P Conv 2ci→ci (x comb,i ||x i ), i= 1, 2, 3, 4<label>(6)</label></formula><p>Here, resize i (x j ) is an operation that resizes x j to the size of x i and c tot = c 1 + c 2 + c 3 + c 4 . This operation is done individually for all the different levels.</p><p>We thus propose another novel block named Multi Level Feature Compilation (MLFC), which aggregates information from multiple encoder levels and enriches the individual encoder feature maps. This block is illustrated in Fig. <ref type="figure" target="#fig_1">2d</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ACC-UNet</head><p>Therefore, we propose fully convolutional ACC-UNet (Fig. <ref type="figure" target="#fig_1">2a</ref>). We started with a vanilla UNet model and reduced the number of filters by half. Then, we replaced the convolutional blocks from the encoder and decoder with our proposed HANC blocks. We considered inv_fctr = 3, other than the last decoder block at level 3 (inv_fctr = 34) to mimic the expansion at stage 3 of Swin Transformer. k = 3, which considers up to 4 × 4 patches, was selected for all but the bottleneck level (k = 1) and the one next to it (k = 2). Next, we modified the skip connections by using residual blocks (Fig. <ref type="figure" target="#fig_1">2c</ref>) to reduce semantic gap <ref type="bibr" target="#b11">[11]</ref> and stacked 3 MLFC blocks. All the convolutional layers were batch-normalized <ref type="bibr" target="#b12">[12]</ref>, activated by Leaky-RELU <ref type="bibr" target="#b17">[17]</ref> and recalibrated by squeeze and excitation <ref type="bibr" target="#b10">[10]</ref>.</p><p>To summarize, in a UNet model, we replaced the classical convolutional blocks with our proposed HANC blocks that perform an approximate version of self-attention and modified the skip connection with MLFC blocks which consider the feature maps from different encoder levels. The proposed model has 16.77 M parameters, roughly a 2M increase than the vanilla UNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>In order to evaluate ACC-UNet, we conducted experiments on 5 public datasets across different tasks and modalities. We used ISIC-2018 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">21]</ref> (dermoscopy, 2594 images), BUSI <ref type="bibr" target="#b2">[3]</ref>(breast ultrasound, used 437 benign and 210 malignant images similar to <ref type="bibr" target="#b13">[13]</ref>), CVC-ClinicDB <ref type="bibr" target="#b3">[4]</ref> (colonoscopy, 612 images), COVID <ref type="bibr" target="#b0">[1]</ref> (pneumonia lesion segmentation, 100 images), and GlaS <ref type="bibr" target="#b20">[20]</ref> (gland segmentation, 85 training, and 80 test images). All the images and masks were resized to 224 × 224. For the GlaS dataset, we considered the original test split as the test data, for the other datasets we randomly selected 20% of images as test data. The remaining 60% and 20% images were used for training and validation and the experiments were repeated 3 times with different random shuffling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implemented ACC-UNet model in PyTorch and used a workstation equipped with AMD EPYC 7443P 24-Core CPU and NVIDIA RTX A6000 (48G) GPU for our experiments. We designed our training protocol identical to previous works <ref type="bibr" target="#b22">[22]</ref>, except for using a batch size of 12 throughout our experiments <ref type="bibr" target="#b28">[27]</ref>. The models were trained for 1000 epochs <ref type="bibr" target="#b28">[27]</ref> and we employed an early stopping patience of 100 epochs. We minimized the combined cross-entropy and dice loss <ref type="bibr" target="#b22">[22]</ref> using the Adam <ref type="bibr" target="#b14">[14]</ref> optimizer with an initial learning rate of 10 -3 , which was adjusted through cosine annealing learning rate scheduler <ref type="bibr" target="#b13">[13]</ref> <ref type="foot" target="#foot_0">1</ref> . We performed online data augmentations in the form of random flipping and rotating <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons with State-of-the-Art Methods</head><p>We evaluated ACC-UNet against UNet, MultiResUNet, Swin-Unet, UCTransnet, SMESwin-Unet, i.e., one representative model from the 5 classes of UNet, respectively (Fig. <ref type="figure" target="#fig_0">1</ref>). Table <ref type="table" target="#tab_0">1</ref> presents the dice score obtained on the test sets. The results show an interesting pattern. Apparently, for the comparatively larger datasets (ISIC-18) transformer-based Swin-Unet was the 2nd best method, as transformers require more data for proper training <ref type="bibr" target="#b1">[2]</ref>. On the other end of the spectrum, lightweight convolutional model (MultiResUNet) achieved the 2nd best score for small datasets (GlaS). For the remaining datasets, hybrid model (UCTransnet) seemed to perform as the 2 nd best method. SMESwin-Unet fell behind in all the cases, despite having such a large number of parameters, which in turn probably makes it difficult to be trained on small-scale datasets.</p><p>However, our model combining the design principles of transformers with the inductive bias of CNNs seemed to perform best in all the different categories with much lower parameters. Compared to much larger state-of-the-art models, for the 5 datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice score, respectively. Thus, our model is not only accurate, but it is also efficient in using the moderately small parameters it possesses. In terms of FLOPs, our model is comparable with convolutional UNets, the transformer-based UNets have smaller FLOPs due to the massive downsampling at patch partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparative Qualitative Results on the Five Datasets</head><p>In addition to, achieving higher dice scores, apparently, ACC-UNet generated better qualitative results. Figure <ref type="figure" target="#fig_2">3</ref> presents a qualitative comparison of ACC-UNet with the other models. Each row of the figure comprises one example from each of the datasets and the segmentation predicted by ACC-UNet and the ground truth mask are presented in the rightmost two columns. For the 1 st example from the ISIC-18 dataset, our model did not oversegment but rather followed the lesion boundary. In the 2 nd example from CVC-ClinicDB, our model managed to distinguish the finger from the polyp almost perfectly. Next in the 3 rd example from BUSI, our prediction filtered out the apparent nodule region on the left, which was predicted as a false positive tumor by all the other models. Similarly, in the 4 th sample from the COVID dataset, we were capable to model the gaps in the consolidation of the left lung visually better, which in turn resulted in 2.9% higher dice score than the 2 nd best method. Again, in the final example from the GlaS dataset, we not only successfully predicted the gland at the bottom right corner but also identified the glands at the top left individually, which were mostly missed or merged by the other models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We performed an ablation study on the CVC-ClinicDB dataset to analyze the contributions of the different design choices in our roadmap (Fig. <ref type="figure" target="#fig_3">4</ref>). We started with a UNet model with the number of filters halved as our base model, which results in a dice score of 87.77% with 7.8M parameters. Using depthwise convolutional along with increasing the bottleneck by 4 raised the dice score to 88.26% while slightly reducing the parameters to 7.5M . Next, HANC block was added with k = 3 throughout, which increased the number of parameters by 340% for an increase of 1.1% dice score. Shortcut connections increased the performance by 2.16%. We also slowly reduced both k and inv_fctr which reduced the number of parameters without any fall in performance. Finally, we added the MLFC blocks (4 stacks) and gradually optimized k and inv_fctr along with dropping one MLFC stage, which led to the development of ACC-UNet. Some other inter- esting ablations were ACC-UNet without MLFC (dice 91.9%) or HANC (dice 90.96%, with 25% more filters to keep the number of parameters comparable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Acknowledging the benefits of various design paradigms in transformers, we investigate the suitability of similar ideas in convolutional UNets. The resultant ACC-UNet possesses the inductive bias of CNNs infused with long-range and multi-level feature accumulation of transformers. Our experiments reveals this amalgamation indeed has the potential to improve UNet models. One limitation of our model is the slowdown from concat operations (please see supplementary materials), which can be solved by replacing them. In addition, there are more innovations brought by transformers <ref type="bibr" target="#b16">[16]</ref>, e.g., layer normalization, GELU activation, AdamW optimizer, these will be explored further in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Developments and innovations in the UNet architecture.</figDesc><graphic coords="2,61,47,53,78,329,80,83,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (A) Architecture of the proposed ACC-UNet. (B) A generalized view of HAN C k,inv_fctr block. (C) A generic residual block used in skip connection. (D) An example view of the 3rd level MLF C block</figDesc><graphic coords="3,61,80,210,71,300,28,340,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparative qualitative results, with dice score provided inside the parenthesis.</figDesc><graphic coords="8,61,47,54,02,329,92,218,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ablation study on the CVC-ClinicDB dataset.</figDesc><graphic coords="9,77,79,53,99,268,84,88,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art models. The first and second best scores are styled as bold and italic, respectively. The subscripts denote the standard deviation.</figDesc><table><row><cell>Model</cell><cell cols="3">params FLOPs ISIC-18</cell><cell>ClinicDB BUSI</cell><cell>COVID</cell><cell>GlaS</cell></row><row><cell>UNet</cell><cell>14M</cell><cell>37G</cell><cell cols="3">87.970.11 90.660.92 72.270.86 71.211.4</cell><cell>87.991.32</cell></row><row><cell cols="2">MultiResUNet 7.3 M</cell><cell>1.1G</cell><cell cols="3">88.550.24 88.201.67 72.430.91 71.333.59 88 .341.05</cell></row><row><cell>Swin-Unet</cell><cell cols="2">27.2 M 6.2G</cell><cell cols="3">89 .240.14 90.690.50 76.060.43 68.561.07 86.450.28</cell></row><row><cell>UCTransnet</cell><cell cols="5">66.4 M 38.8G 89.080.44 92 .570.39 76 .560.2 73 .093.63 87.170.85</cell></row><row><cell cols="3">SMESwin-Unet 169.8 M 6.4G</cell><cell cols="3">88.570.13 89.620.08 73.942.06 58.40.03</cell><cell>83.720.18</cell></row><row><cell>ACC-UNet</cell><cell cols="2">16.8 M 38G</cell><cell>89.370.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>34 92.670.57 77.190.87 73.990.53 88.610.61</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Swin-UNet-based models were trained with SGD<ref type="bibr" target="#b8">[9]</ref> for poor performance of Adam.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partly supported by the <rs type="funder">National Institutes of Health</rs> (<rs type="grantNumber">R01GM133840</rs> and <rs type="grantNumber">3R01GM133840-02S1</rs>) and the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">CMMI1825941</rs>, <rs type="grantNumber">MCB1925643</rs>, <rs type="grantNumber">IIS2211598</rs>, <rs type="grantNumber">DMS2151678</rs>, <rs type="grantNumber">DBI2146026</rs>, and <rs type="grantNumber">DBI2003635</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_r9bbq3N">
					<idno type="grant-number">R01GM133840</idno>
				</org>
				<org type="funding" xml:id="_n8eUkC9">
					<idno type="grant-number">3R01GM133840-02S1</idno>
				</org>
				<org type="funding" xml:id="_vH2y8Cx">
					<idno type="grant-number">CMMI1825941</idno>
				</org>
				<org type="funding" xml:id="_zEqFTDQ">
					<idno type="grant-number">MCB1925643</idno>
				</org>
				<org type="funding" xml:id="_yERFmV8">
					<idno type="grant-number">IIS2211598</idno>
				</org>
				<org type="funding" xml:id="_B8vpBhV">
					<idno type="grant-number">DMS2151678</idno>
				</org>
				<org type="funding" xml:id="_hpNR2cR">
					<idno type="grant-number">DBI2146026</idno>
				</org>
				<org type="funding" xml:id="_S6vPxWW">
					<idno type="grant-number">DBI2003635</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_66.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://medicalsegmentation.com/covid19/" />
		<title level="m">Covid-19 ct segmentation dataset</title>
		<imprint>
			<date type="published" when="2022-08-20">20 Aug 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ConTrans: improving transformer with convolutional attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ailiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jinxing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guangming</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imaging collaboration (ISIC</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Swin-Unet: unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongsheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiaopeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MultiResUNet: rethinking the U-Net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UNeXt: MLP-based rapid medical image segmentation network</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vis ion transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A ConvNet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11966" to="11976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interaction in the segmentation of medical images: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Olabarriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="142" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: the glas challenge contest</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UCTransNet: rethinking the skip connections in U-net from a channel-wise perspective with transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2441" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Focal modulation networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-compound transformer for accurate biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijn</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="326" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_31" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="820" to="838" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SMESwin Unet: merging CNN and transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ziheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_50" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
