<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI</title>
				<funder>
					<orgName type="full">Stanford Center for Artificial Intelligence in Medicine and Imaging</orgName>
					<orgName type="abbreviated">AIMI</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Health and Life Sciences</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jameson</forename><surname>Merkow</surname></persName>
							<email>jameson.merkow@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arjun</forename><surname>Soin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">Paul</forename><surname>Cohen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Smitha</forename><surname>Saligrama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Bridge</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Quantitative Translational Imaging in Medicine Laboratory</orgName>
								<orgName type="department" key="dep2">Athinoula A. Martinos Center for Biomedical Imaging</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Quantitative Translational Imaging in Medicine Laboratory</orgName>
								<orgName type="department" key="dep2">Athinoula A. Martinos Center for Biomedical Imaging</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Kaiser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Borg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Tarapov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Health and Life Sciences (HLS)</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="326" to="336"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">83DF460C5FC0301E3B4171E216662530</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical imaging</term>
					<term>Model drift</term>
					<term>AI monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical AI applications, particularly medical imaging, are increasingly being adopted in healthcare systems worldwide. However, a crucial question remains: what happens after the AI model is put into production? We present our novel multi-modal model drift framework capable of tracking drift without contemporaneous ground truth using only readily available inputs, namely DICOM metadata, image appearance representation from a variational autoencoder (VAE), and model output probabilities. CheXStray was developed and tested using CheXpert, PadChest and Pediatric Pneumonia Chest X-ray datasets and we demonstrate that our framework generates a strong proxy for ground truth performance. In this work, we offer new insights into the challenges and solutions for observing deployed medical imaging AI and make three key contributions to real-time medical imaging AI monitoring: (1) proofof-concept for medical imaging drift detection including use of VAE and domain specific statistical methods (2) a multi-modal methodology for measuring and unifying drift metrics (3) new insights into the challenges and solutions for observing deployed medical imaging AI. Our framework is released as open-source tools so that others may easily run their own workflows and build upon our work. Code available at: https://github. com/microsoft/MedImaging-ModelDriftMonitoring</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen a significant increase in the use of artificial intelligence (AI) in medical imaging, as evidenced by the rising number of academic publications and the accelerated approval of commercial AI applications for clinical use <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">12,</ref><ref type="bibr">14,</ref><ref type="bibr">19,</ref><ref type="bibr">21]</ref>. Despite the growing availability of market-ready AI products and clinical enthusiasm to adopt these solutions <ref type="bibr">[20]</ref>, the translation of AI into real-world clinical practice remains limited. The reasons for this gap are multifaceted and include technical challenges, restricted IT resources, and a deficiency of clear data-driven clinical utility analyses. Efforts are underway to address these many barriers through existing and emerging solutions <ref type="bibr">[4,</ref><ref type="bibr">6,</ref><ref type="bibr">13,</ref><ref type="bibr">22</ref>]. However, a major concern remains: What happens to the AI after it is put into production?</p><p>Monitoring the performance of AI models in production systems is crucial to ensure safety and effectiveness in healthcare, particularly for medical imaging applications. The unrealistic expectation that input data and model performance will remain static indefinitely runs counter to decades of machine learning operations research, as outlined by extensive experience in AI model deployment for other verticals <ref type="bibr">[11,</ref><ref type="bibr">17]</ref>. Traditional drift detection methods require real-time feedback and lack the ability to guard against performance drift crucial for safe AI deployment in healthcare and, as such, the absence of solutions for monitoring AI model performance in medical imaging is a significant barrier to widespread adoption of AI in healthcare <ref type="bibr">[7]</ref>.</p><p>In healthcare, the availability of real-time ground truth data is often limited, presenting a significant challenge to accurate and timely performance monitoring. This limitation renders many existing monitoring strategies inadequate, as they require access to contemporaneous ground truth labels. Moreover, existing solutions do not tackle the distinct challenges posed by monitoring medical imaging data, including both pixel and non-pixel data, as they are primarily designed for structured tabular data. Our challenge is then to develop a systematic approach to real-time monitoring of medical imaging AI models without contemporaneous ground truth labels. This gap in the current landscape of monitoring strategies is what our method aims to fill.</p><p>In this manuscript, we present a solution that relies on only statistics of input data, deep-learning based pixel data representations, and output predictions. Our innovative approach goes beyond traditional methods and addresses this gap by not necessitating the use of up-to-date ground truth labels. Our framework is coupled with a novel multi-modal integration methodology for realtime monitoring of medical imaging AI systems for conditions which will likely have an adverse effect on performance. Through the solution proposed in this paper, we make a meaningful contribution to the medical imaging AI monitoring landscape, offering an approach specifically tailored to navigate the inherent constraints and challenges in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data and Deep Learning Model</head><p>We test our medical imaging AI drift workflow using the CheXpert [9] and Pad-Chest <ref type="bibr" target="#b1">[2]</ref> datasets. CheXpert comprises 224, 316 images of 65, 240 patients who Unlike other datasets, PadChest keeps the chronology of scans making it valuable for drift experiments. We categorized PadChest into three sets using examination dates: training, validation, and test. Our experimentation period spans the first year of the test set. To align with the labels in CheXpert, we consolidate relevant labels from the PadChest dataset into a set of ten unified labels. See Table <ref type="table" target="#tab_0">1</ref> for details (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Our approach utilizes a Densely Connected Convolutional Neural Network [8], pretrained on frontal-only CheXpert data, then we fine-tuned only the final classifier layers of the model using PadChest frontal training data. To assess the performance of our classifier over a simulated production timeframe, we employ AUROC as an evaluation metric. This approach offers a definitive indication of any potential model drift, but it necessitates real-time, domain expert-labeled ground truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Stream Drift and Concordance</head><p>Our approach differs from others in that we monitor for similarity or concordance of the datastream with respect to a reference dataset rather than highlighting differences. When the concordance metric decreases the degree to which the data has drifted has increased. Our method summarizes each exam in the datasteam into an embedding consisting of DICOM metadata, pixel features and model output which is sampled into temporal detection windows in order to compare distributions of individual features to a reference set using statistical tests. Our framework, though extensible, uses two statistical tests: 1) the Kolmogorov-Smirnov (K-S) test and 2) the chi-square (χ 2 ) goodness of fit test. The K-S test is a non-parametric test which measures distribution shift in a real-valued sample without assuming any specific distribution <ref type="bibr">[5]</ref>. The chi-square goodnessof-fit test compares observed frequencies in categorical data to expected values and calculates the likelihood they are obtained from the reference distribution <ref type="bibr">[16]</ref>. Both these tests provide a p-value and statistical similarity (distance) value. We found that statistical "distance" provided a smoother and more consistent metric which we use exclusively in our experiments, ignoring p-values.</p><p>Multi-Modal Embedding. To calculate statistics, each image must be embedded into a compressed representation suitable for our statistical tests. Our embedding is comprised of three categories: 1) DICOM metadata, 2) image appearance, and 2) model output (Fig. <ref type="figure" target="#fig_1">2</ref>). AI performance in medical imaging can be affected by shifts in imaging data, due to factors like hardware changes or disease presentation variations. To address this, we employ a Variational Autoencoder (VAE)-an auto-encoder variant that models underlying parametric probability distributions of input data for fine-grained, explainable analysis [3,18,23]. Our approach uses a VAE to encode images and apply statistical tests for drift detection, representing, to our knowledge, the first VAE use case for medical imaging drift analysis. The VAE is trained on PadChest's frontal and lateral images.</p><p>The aim of live medical data stream monitoring is to ensure consistency, detect changes impacting model performance, and identify shifts in class distribution or visual representations. We utilize soft predictions (model raw score/activation) to monitor model output and detect subtle distribution changes that hard predictions may overlook, enhancing early detection capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Measurement and Unification</head><p>Our framework constructs detection windows using a sliding window approach, where the temporal parameters dictate the duration and step size of each window. Specifically, the duration defines the length of time that each window covers, while the step size determines the amount of time between the start of one window and the start of the next. We use ψi (ω t ) = mt i to denote individual metrics calculated at time t from a ω t , and m[a,b] i to represent the collection of individual metric values from time a to b.</p><p>To mitigate sample size sensitivity, we employ a bootstrap method. This involves repeatedly calculating metrics on fixed-size samples drawn with replacement from the detection window. We then average these repeated measures to yield a final, more robust metric value. Formally:</p><formula xml:id="formula_0">Θ ψ (ω, N, K) = ψi (ω) = 1 N N 0 ψ i θ K (ω)<label>(1)</label></formula><p>where θ K collects K samples from ω with replacement and ψ i is the metric function calculated on the sample. There remains three main challenges to metric unification: 1) fluctuation normalization, 2) scale standardization, and 3) metric weighting. Fluctuation normalization and scale standardization are necessary to ensure that the metrics are compatible and can be meaningfully compared and aggregated. Without these steps, comparing or combining metrics could lead to misleading results due to the variations in the scale and distribution of different metrics. We address the first two challenges by utilizing a standardization function, Γ , which normalizes each individual metric into a numerical space with consistent upper and lower bounds across all metrics. This function serves to align the metric values so that they fall within a standard range, thereby eliminating the influence of extreme values or discrepancies in the original scales of the metrics. In our experiments, we apply a simple normalization function using scale (η) and offset factors (ζ), specifically: Γ (m) = m-ζ η . Metric weighting is used to reflect the relative importance or reliability of each metric in the final unified metric. The weights are determined through a separate process which takes into account factors such as the sensitivity and specificity of each metric. We then calculate our unified multi-modal concordance metric, MMC , on a detection window ω by aggregating individual metric values across L metrics using predefined weights, α i , for each metric, as follows:</p><formula xml:id="formula_1">MMC (ω) = L i=1 α i • Γ i ψi (ω) = L i=1 α i η i ψi (ω) -ζ i (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ψi (ω) represents the ith metric calculated on detection window ω, Γ i represents the standardization function, and α i represents the weight used for the ith metric value. Each metric value is derived by a function that measures a specific property or characteristic of the detection window. For instance, one metric could measure the average intensity of the window, while another could measure the variability of intensities. By calculating MMC on a time-indexed detection window set Ω [a,b] , we obtain a robust multi-modal concordance measure that can monitor drift over the given time period from a to b, denoted as MMC [a,b] . This unified metric is advantageous as it provides a single, comprehensive measurement that takes into account multiple aspects of the data, making it easier to track and understand changes over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentation</head><p>Our framework is evaluated through three simulations, inspired by clinical scenarios, each involving a datastream modification to induce noticeable drift. All experiments share settings of a 30-day detection window, one-day stride, and parameters K = 2500 and N = 20 in Eq. 1. Windows with less than 150 exams are skipped. We use the reference set for generating Ω r and calculating η and ζ. Weights α i are calculated by augmenting Ω r with poor-performing samples.</p><p>Scenario 1: Performance Degradation. We investigate if performance changes are detectable by inducing degradation through hard data mining. We compile a pool of difficult exams for the AI to classify by selecting exams with low model scores but positive ground truth for their label as well as high scoring negatives. Exams are chosen based on per-label quantiles of scores, with Q = 0.25 indicating the lowest 25% positives and highest 25% negatives are included. These difficult exams replace all other exams in each detection window at a given point in the datastream. Scenario 2: Metadata Filter Failure. In this scenario, we simulate a workflow failure, resulting in processing out-of-spec data, specifically lateral images, in contrast to model training on frontal images only. The datastream is modified at two points to include and then limit to lateral images.</p><p>Scenario 3: No Metadata Available. The final experiment involves a nometadata scenario using the Pediatric Pneumonia Chest X-ray dataset [10], comprising of 5, 856 pediatric Chest X-rays. This simulates a drift scenario with a compliance boundary, relying solely on the input image. The stream is altered at two points to first include and then limit to out-of-spec data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The performance and concordance metrics of each experiment are visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. Each sub-figure the top panel depicts performance as well as calculated MMC as calculated in each detection window. Each sub-figure has vertical lines which represent points indicating where the datastream was modified.</p><p>We start by discussing results in Fig. <ref type="figure" target="#fig_2">3a</ref> from the first experiment. The top panel shows micro-averaged AUROC, the middle panel shows MMC w , and the bottom panel shows MMC 0 . As Q decreases, performance drops and the concordance metric drops as well. Both the weighted and unweighted versions of our metric are shown, with the weighted version providing clearer separation between performance profiles showing that our weighting methodology emphasizes relevant metrics for consistent performance proxy.</p><p>Next, Fig. <ref type="figure" target="#fig_2">3b</ref> shows results of our second experiment. The top panel shows performance, and the bottom panel shows our metric MMC w . Two trials are depicted, a baseline (original data stream, blue) and a second trial (red) where drift is induced. Two vertical lines denote points in time where data stream is modified: at point A, lateral images are introduced, and at point B, indistribution (frontal) data is removed, leaving only laterals. The figure shows a correlation between performance and MMC w ; at point A, performance drops from above 0.9 to approx. 0.85 and MMC w drops to approx. -4. At point B, performance drops to around 0.75 and MMC w drops to and hovers around -8. This demonstrates the robustness of our method to changes in data composition detectable by metadata tags and visual appearance.</p><p>Results of our final scenario, appear in Fig. <ref type="figure" target="#fig_2">3c</ref>. In this experiment, we measure Pneumonia AUROC, as the pediatric data includes only pneumonia labels. We observe a drop in performance and concordance at both points where we modify the data stream, show that our approach remains robust without metadata and can still detect drift. We also notice a larger drop in concordance compared to performance, indicating that concordance may be more sensitive to data stream changes, which could be desirable for detecting this type of drift when the AI model is not cleared for use on pediatric patients.</p><p>We demonstrate model monitoring for a medical imaging with CheXStray can achieve real-time drift metrics in the absence of contemporaneous ground truth in a chest X-ray model use case to inform potential change in model performance. This work will inform further development of automated medical imaging AI monitoring tools to ensure ongoing safety and quality in production to enable safe and effective AI adoption in medical practice. The important contributions include the use of VAE in reconstructing medical images for the purpose of detecting input data changes in the absence of ground truth labels, data-driven unsupervised drift detection statistical metrics that correlate with supervised drift detection approaches and ground truth performance, and open source code and datasets to optimize validation and reproducibility for the broader community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of our Multi-modal concordance algorithm. From each exam in datastream, we extract DICOM metadata, model predicted probabilities and a latent representation produced by a VAE then compare distributions of extracted data to a reference. We standardize and weigh these measures combining them into a value representative of the concordance to the reference.</figDesc><graphic coords="3,41,79,53,99,340,21,191,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multi-Model Embedding. CheXstray utilizes data from three sources to calculate drift: 1) image appearance features (VAE), 2) model output probabilities, 3) DICOM metadata.</figDesc><graphic coords="5,56,31,171,62,311,08,107,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results for all three scenarios.</figDesc><graphic coords="8,80,01,54,35,292,51,498,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>PadChest Condensed Labels Descriptions and Distribution</figDesc><table><row><cell></cell><cell>Training</cell><cell cols="2">Validation Test</cell></row><row><cell>Start Date</cell><cell cols="3">2007-05-03 2013-01-01 2014-01-01</cell></row><row><cell>End Date</cell><cell cols="3">2012-12-28 2013-12-31 2014-12-31</cell></row><row><cell>Total Images</cell><cell>63,699</cell><cell>15,267</cell><cell>11,509</cell></row><row><cell cols="3">Pathology Counts and Descriptions</cell><cell></cell></row><row><cell>Atelectasis</cell><cell>4,516</cell><cell>1,121</cell><cell>954</cell></row><row><cell cols="4">laminar atelectasis, fibrotic band, atelectasis, lobar atelectasis, segmental atelectasis,</cell></row><row><cell cols="2">atelectasis basal, total atelectasis</cell><cell></cell><cell></cell></row><row><cell>Cardiomegaly</cell><cell>5,611</cell><cell>1,357</cell><cell>935</cell></row><row><cell cols="2">cardiomegaly, pericardial effusion</cell><cell></cell><cell></cell></row><row><cell>Consolidation</cell><cell>1,161</cell><cell>225</cell><cell>106</cell></row><row><cell>consolidation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Edema</cell><cell>26</cell><cell>9</cell><cell>17</cell></row><row><cell>kerley lines</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lesion</cell><cell>1,950</cell><cell>390</cell><cell>294</cell></row><row><cell cols="4">nodule, pulmonary mass, lung metastasis, multiple nodules, mass</cell></row><row><cell>No Finding</cell><cell>21,112</cell><cell>4,634</cell><cell>3,525</cell></row><row><cell>normal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Opacity</cell><cell>11,604</cell><cell>2,577</cell><cell>2,018</cell></row><row><cell cols="4">infiltrates, alveolar pattern, pneumonia, interstitial pattern, increased density,</cell></row><row><cell cols="4">consolidation, bronchovascular markings, pulmonary edema, pulmonary fibrosis,</cell></row><row><cell cols="4">tuberculosis sequelae, cavitation, reticular interstitial pattern, ground glass pattern,</cell></row><row><cell cols="4">atypical pneumonia, post radiotherapy changes, reticulonodular interstitial pattern,</cell></row><row><cell cols="2">tuberculosis, miliary opacities</cell><cell></cell><cell></cell></row><row><cell cols="2">Pleural Abnormalities 6,875</cell><cell>1,708</cell><cell>1,272</cell></row><row><cell cols="4">costophrenic angle blunting, pleural effusion, pleural thickening, calcified pleural</cell></row><row><cell cols="4">thickening, calcified pleural plaques, loculated pleural effusion, loculated fissural</cell></row><row><cell cols="4">effusion, asbestosis signs, hydropneumothorax, pleural plaques</cell></row><row><cell>Pleural Effusion</cell><cell>4,365</cell><cell>1,026</cell><cell>710</cell></row><row><cell>pleural effusion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pneumonia</cell><cell>3,287</cell><cell>584</cell><cell>379</cell></row><row><cell>pneumonia</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was was supported in part by the <rs type="funder">Stanford Center for Artificial Intelligence in Medicine and Imaging (AIMI)</rs> and <rs type="funder">Microsoft Health and Life Sciences</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benjamens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhunnoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meskó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PadChest: a large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De La Iglesia-Vayá</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101797</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101797" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
