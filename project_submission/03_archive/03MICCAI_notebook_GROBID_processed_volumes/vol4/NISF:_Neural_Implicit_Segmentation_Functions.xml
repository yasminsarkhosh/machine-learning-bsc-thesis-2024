<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NISF: Neural Implicit Segmentation Functions</title>
				<funder ref="#_MkE4G4E">
					<orgName type="full">Munich Center for Machine Learning and European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_3gMgcw9">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nil</forename><surname>Stolt-Ansó</surname></persName>
							<idno type="ORCID">0009-0001-4457-0967</idno>
							<affiliation key="aff0">
								<orgName type="department">Munich Center for Machine Learning</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcginnis</surname></persName>
							<idno type="ORCID">0009-0000-2224-7600</idno>
							<affiliation key="aff1">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiazhen</forename><surname>Pan</surname></persName>
							<idno type="ORCID">0000-0002-6305-8117</idno>
							<affiliation key="aff2">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution" key="instit1">Klinikum Rechts der Isar</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kerstin</forename><surname>Hammernik</surname></persName>
							<idno type="ORCID">0000-0002-2734-1409</idno>
							<affiliation key="aff1">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<idno type="ORCID">0000-0002-5683-5889</idno>
							<affiliation key="aff0">
								<orgName type="department">Munich Center for Machine Learning</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution" key="instit1">Klinikum Rechts der Isar</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NISF: Neural Implicit Segmentation Functions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="734" to="744"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E129F7BBCC6DADB0A31533A8105B657D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation of anatomical shapes from medical images has taken an important role in the automation of clinical measurements. While typical deep-learning segmentation approaches are performed on discrete voxels, the underlying objects being analysed exist in a realvalued continuous space. Approaches that rely on convolutional neural networks (CNNs) are limited to grid-like inputs and not easily applicable to sparse or partial measurements. We propose a novel family of image segmentation models that tackle many of CNNs' shortcomings: Neural Implicit Segmentation Functions (NISF). Our framework takes inspiration from the field of neural implicit functions where a network learns a mapping from a real-valued coordinate-space to a shape representation. NISFs have the ability to segment anatomical shapes in high-dimensional continuous spaces. Training is not limited to voxelized grids, and covers applications with sparse and partial data. Interpolation between observations is learnt naturally in the training procedure and requires no postprocessing. Furthermore, NISFs allow the leveraging of learnt shape priors to make predictions for regions outside of the original image plane. We go on to show the framework achieves dice scores of 0.87 ± 0.045 on a (3D+t) short-axis cardiac segmentation task using the UK Biobank dataset. We also provide a qualitative analysis on our frameworks ability to perform segmentation and image interpolation on unseen regions of an image volume at arbitrary resolutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image segmentation is a core task in domains where the area, volume or surface of an object is of interest. The principle of segmentation involves assigning a class to every presented point in the input space. Typically, the input is presented in the form of images: aligned pixel (or voxel) grids, with the intention to obtain a class label for each. In this context, the application of deep learning to the medical imaging domain has shown great promise in recent years. With the advent of the U-Net <ref type="bibr" target="#b20">[20]</ref>, Convolutional Neural Networks (CNN) have been successfully applied to a multitude of imaging domains and achieved (or even surpassed) human performance <ref type="bibr" target="#b10">[11]</ref>. The convolution operation make CNNs an obvious choice for dealing with inputs in the form of 2D pixel-or 3D voxel-grids.</p><p>Despite their efficacy, CNNs suffer from a range of limitations that lead to incompatibilities for some imaging domains. CNNs are restricted to data in the form of grids, and cannot easily handle sparse or partial inputs. Moreover, due to the CNN's segmentation output also being confined to a grid, obtaining smooth object surfaces requires post-processing heuristics. Predicting a high resolution segmentations also has implications on the memory and compute requirements in high-dimensional domains. Finally, the learning of long-distance spatial correlations requires deep stacks of layers, which may pose too taxing in low resource domains.</p><p>We introduce a novel approach to image segmentation that circumvents these shortcomings: Neural Implicit Segmentation Functions (NISF). Inspired by ongoing research in the field of neural implicit functions (NIF), a neural network is taught to learn a mapping from a coordinate space to any arbitrary real-valued space, such as segmentation, distance function, or image intensity. While CNNs employ the image's pixel or voxel intensities as an input, NISF's input is a real-valued vector c ∈ R N for a single N-dimensional coordinate, alongside a subject-specific latent representation vector h ∈ R d . Given c and h, the network is taught to predict image intensity and segmentation value pairs. The space H over all possible latent vectors h serves as a learnable prior over all possible subject representations.</p><p>In this paper, we describe an auto-decoder process by which a previously unseen subject's pairs of coordinate-image intensity values (c, i) may be used to approximate that subject's latent representation h. Given a latent code, the intensity and segmentation predictions from any arbitrary coordinates in the volume may be sampled. We evaluate the proposed framework's segmentation scores and investigate its generalization properties on the UK-Biobank cardiac magnetic resonance imaging (MRI) short-axis dataset. We make the source code publicly available<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cardiac MRI. Cardiac magnetic resonance imaging (MRI) is often the preferred imaging modality for the assessment of function and structure of the cardiovascular system. This is in equal parts due to its non-invasive nature, and due to its high spatial and temporal resolution capabilities. The short-axis (SAX) view is a (3+t)-dimensional volume made up of stacked cross-sectional (2D+t) acquisitions which lay orthogonal to the ventricle's long axis (see Fig. <ref type="figure" target="#fig_0">1</ref>). Spatial resolution is highest in-plane (typically &lt;3 mm 2 ), with a much lower inter-slice resolution (10 mm), and a temporal resolution of ≤45 ms <ref type="bibr" target="#b15">[15]</ref>. On the other hand, long-axis (LAX) views are (2D+t) acquisitions orthogonal to the SAX plane and provide high resolution along the ventricle's long axis.</p><p>Image Segmentation. The capabilities of the CNN has caused it to become the predominant choice for image segmentation tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">20]</ref>. However, a pitfall of these models is their poor generalization to certain input transformations. One such transformation is scaling. This drawback limits the use of CNNs on domains with large variations in pixel spacings. Past works have attempted to mitigate this issue by accounting for dataset characteristics <ref type="bibr" target="#b10">[11]</ref>, building resilience through augmentations <ref type="bibr" target="#b29">[29]</ref>, or using multi-scale feature extractors <ref type="bibr" target="#b4">[5]</ref>.</p><p>Additionally, segmentation performed by fully convolutional model is restricted to predicting in pixel (or voxel) grids. This requires post-processing heuristics to extract smooth object surfaces. Works such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">19]</ref> try to mitigate this issue through point-wise decoders that operate on interpolated convolutional features. Alternatives to binarized segmentation have been recently proposed such as soft segmentations <ref type="bibr" target="#b6">[7]</ref> and distance field predictions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">24]</ref>. Smoothness can also be improved by predicting at higher resolutions. This is however limited by the exponential increase of memory that comes with high-dimensional data. Partitioning of the input can make memory requirements manageable <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, but doing so disallows the ability to learn long-distance spatial correlations.</p><p>Neural Implicit Functions. In recent years, NIFs have achieved notable milestones in the field of shape representations <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. NIFs have multiple advantages over classical voxelized approaches that makes them remarkably interesting for applications in the medical imaging domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">28]</ref>. First, NIFs can sample shapes at any points in space at arbitrary resolutions. This makes them particularly fit for working with sparse, partial, or non-uniform data. Implicit functions thus remove the need for traditional interpolation as high-resolution shapes are learnt implicitly by the network <ref type="bibr" target="#b0">[1]</ref>. This is specially relevant to the medical imaging community, where scans may have complex sampling strategies, have missing or unusable regions, or have highly anisotropic voxel sizes. These properties may further vary across scanners and acquisition protocols, making generalization across datasets a challenge. Additionally, the ability to process each point independently allows implicit functions to have flexible optimization strategies, making entire volumes be optimizable holistically.</p><p>Image Priors. The typical application of a NIF involves the training of a multilayer perceptron (MLP) on a single scene. Although generalization still occurs in generating novel views of the target scene, the introduction of prior knowledge and conditioning of the MLP is subject to ongoing research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. Approaches such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18]</ref> opt for auto-decoder architectures where the network is modulated by latent code at the input level. At inference time, the latent code of the target scene is optimized by backpropagation. Works such as <ref type="bibr" target="#b16">[16]</ref> choose to instead modulate the network at its activation functions. Other frameworks obtain the latent code in a single-shot fashion through the use of an encoder network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. This latent code is then used by a hyper-network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b23">23]</ref> or a meta-learning approach <ref type="bibr" target="#b22">[22]</ref> to generate the weights of a decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Shared Prior. In order to generalize to unseen subjects, we attempt to build a shared prior H over all subjects. This is done by conditioning the classifier with a latent vector h ∈ R d at the input level. Each individual subject j in a population X, can be thought of having a distinct h j that serves as a latent code of their unique features. Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18]</ref>, we initialize a matrix H ∈ R Xd , where each row is a latent vector h j corresponding to a single subject j in the dataset. The latent vector h j of a subject is fed to the MLP alongside a point's coordinate and can be optimized through back-propagation. This allows H to be optimized to capture useful inter-patient features.</p><p>Model Architecture. The architecture is composed of a segmentation function f θ and a reconstruction function f φ . At each continuous-valued coordinate c ∈ R N , function f θ models the shape's segmentation probability s c for all M classes, and function f φ models the image intensity i c . The functions are conditioned by a latent vector h at the input level as follows:</p><formula xml:id="formula_0">f θ : c ∈ R N × h ∈ R d → s c ∈ [0, 1] M , M i=1 s i c = 1 (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">f φ : c ∈ R N × h ∈ R d → i c ∈ [0, 1]<label>(2)</label></formula><p>In order to improve local agreement between the segmentation and reconstruction functions, we jointly model f θ and f φ by a unique multi-layer perceptron (MLP) with two output heads (Fig. <ref type="figure" target="#fig_1">2</ref>). We employ Gabor wavelet activation functions <ref type="bibr" target="#b21">[21]</ref> which are known to be more expressive than Fourier Features combined with ReLU <ref type="bibr" target="#b26">[26]</ref> or sinusoidal activation functions <ref type="bibr" target="#b23">[23]</ref>.</p><p>Prior Training. Following the setup described in <ref type="bibr" target="#b0">[1]</ref>, we randomly initialize the matrix H consisting of a trainable latent vector h j ∼ N 0, 10 -2 for each subject in the training set. On each training sample, the parameters of the MLP are jointly optimized with the subject's h j . We select a training batch by uniformly sampling a time frame t and using all points within that 3D volume. Each voxel in the sample is processed in parallel along the batch dimension. Coordinates are normalized to the range [0, 1] based on the voxel's relative position.</p><p>The difference in image reconstruction from the ground-truth voxel intensities is supervised using binary cross-entropy (BCE). This is motivated by our data's voxel intensity distribution being heavily skewed towards the extremes. The segmentation loss is a sum of a BCE loss component and a Dice loss component. We found that adding a weighting factor of α = 10 to the image reconstruction loss component yielded inference-time improvements on both image reconstruction and segmentation metrics. Additionally, L2 regularization is applied to the latent vector h j and the MLP's parameters. The full loss is summarized as follows:</p><formula xml:id="formula_3">L train (θ, φ, h j ) = L BCE f θ (c, h j ), s c + L Dice f θ (c, h j ), s c + α L BCE f φ (c, h j ), i c + L L2 (θ) + L L2 (φ) + L L2 (h j ) (3)</formula><p>Inference. Once the segmentation function f θ has learnt a mapping from the population prior H to the segmentation space S, inference becomes a task of finding a latent code h within H that correctly models the new subject's features. The ground-truth segmentation of a new subject is obviously not available at inference, and it is thus not possible to use f θ to optimize h. However, since both functions f φ (image reconstruction) and f θ (segmentation) have been jointly trained by consistently using the same latent vector h, we make the following assumption: A latent code h optimized for image reconstruction under f φ will also produce accurate segmentations under f θ . This assumption makes it possible to use the image reconstruction function f φ alone to find a latent code h for an unseen image in order to generalize segmentation predictions using f θ .</p><p>For this task, a new h ∼ N 0, 10 -4 is initialized. The weights of the MLP are frozen, such that the only tuneable parameters are those of h. Optimization is performed exclusively on the image reconstruction loss (dashed green line in Fig. <ref type="figure" target="#fig_1">2</ref>):</p><formula xml:id="formula_4">L inf er (h j ) = L BCE f φ (c, h j ), i c + L L2 (h j )<label>(4)</label></formula><p>Due to the loss being composed exclusively by the image reconstruction term, h is expected to eventually overfit to f φ . Special care should be taken to find a step-number hyperparameter that stops the optimization of h at the optimal segmentation performance. In our experiments, we chose this parameter based on the Dice score of the best validation run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Data Overview. The dataset consists of a random subset of 1150 subjects from the UK Biobank's short-axis cardiac MRI acquisitions <ref type="bibr" target="#b25">[25]</ref>. An overview of the UK Biobank cohort's baseline statistics can be found in their showcase website <ref type="bibr" target="#b27">[27]</ref>. The dataset split included 1000 subjects for the prior training, 50 for validation, and 100 for testing. The (3D+t) short-axis volumes are anisotropic in nature and have a wide range of shapes and pixel spacings along the spatial dimensions. No form of preprocessing was performed on the images except for an intensity normalization to the range [0, 1] as performed in similar literature <ref type="bibr" target="#b1">[2]</ref>. The high dimensionality of (3D+t) volumes makes manual annotation prohibitively time consuming. Due to this, we make use of synthetic segmentation as ground truth shapes created using a trained state of the art segmentation CNN provided by <ref type="bibr" target="#b1">[2]</ref>. The object of interest in each scan is composed of three distinct, mutually exclusive sub-regions: The left ventricle (LV) blood pool, LV myocardium, and right ventricle (RV) blood pool (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Implementation Details. The architecture consists of 8 residual layers, each with 128 hidden units. The subject latent codes had 128 learnable parameters. The model was implemented using Pytorch and trained on an NVIDIA A40 GPU for 1000 epochs, lasting approximately 9 days. Inference optimization lasted 3-7 minutes per subject depending on volume dimensions. Losses are minimized using the ADAM optimizer <ref type="bibr" target="#b12">[13]</ref> using a learning rate of 10 -4 during the prior training and 10 -4 during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>As the latent code is optimized during inference, segmentation metrics follow an overfitting pattern (see Fig. <ref type="figure" target="#fig_2">3</ref>). This is an expected consequence of the inference process optimizing solely on the image reconstruction loss. Early stopping should be employed to obtain the best performing latent code state.</p><p>The benefits of training a prior over the population is investigated by tracking inference-time Dice scores obtained from spaced-out validation runs. Training of the prior is shown to significantly improve performance of segmentation and image reconstruction at inference-time as seen in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Validation results showed the average optimal number of latent code optmimization steps at inference to be 672. Thus, the test set per-class Dice scores (Table <ref type="table" target="#tab_0">1</ref>) were obtained after 672 optimization steps on h for each test subject.     Further investigation is performed on the generalization capabilities of the subject prior by producing segmentations for held-out sections of the image volume. First, the subject's latent code is optimized using the inference process. Then, the model's output is sampled at the held-out region's coordinates.</p><p>Right ventricle segmentation in basal slices is notoriously challenging to manually annotate due to the delineation of the atrial and ventricular cavity combined with the sparsity of the resolution along the long axis <ref type="bibr" target="#b3">[4]</ref>. Nonetheless, as seen in Fig. <ref type="figure" target="#fig_4">5</ref>, our approach is capable of capturing smooth and plausible morphology of these regions despite not having access to the image information.</p><p>We go on to show NISF's ability to generate high-resolution segmentation for out-of-plane views. We optimize on a short-axis volume at inference and subsequently sample coordinates corresponding to long-axis views. Despite never presenting a ground-truth long-axis image, the model reconstructs an interpolated view and provides an accurate segmentation along its plane (Fig. <ref type="figure" target="#fig_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel family of image segmentation models that can model shapes at arbitrary resolutions. The approach is able to leverage priors to make predictions for regions not present in the original image data. Working directly on the coordinate space has the benefit of accepting high-dimensional sparse data, as well as not being affected by variations in image shapes and resolutions. We implement a simple version of this framework and evaluate it on a short-axis cardiac MRI segmentation task using the UK Biobank. Reported Dice scores on 100 unseen subjects average 0.87 ± 0.045. We also perform a qualitative analysis on the framework's ability to predict held-out sections of image volumes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Short axis volumes have low resolution along the ventricle's long axis. Given a short axis image volume, a NISF can produce arbitrary resolution segmentations along the long axis.</figDesc><graphic coords="3,44,79,53,69,334,54,83,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Training and inference setups. During the prior's training, the MLP and the input latent code h are jointly optimized on image reconstruction and segmentation losses (solid blue line). At inference, solely the latent code h is optimized exclusively on the image reconstruction (dashed green line). (Color figure online)</figDesc><graphic coords="6,55,98,53,69,340,15,86,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation Dice trend during a subject's inference. Early stopping is important to prevent overfitting on reconstruction task. a) Non-optimized latent code creates blurry images with 'generic' morphology. b) As the latent code is optimized, subject morphology begins to be accurately reconstructed. Segmentation performance reaches an optimum. c) Reconstruction continues to improve, but segmentation deteriorates.</figDesc><graphic coords="7,52,80,136,82,318,49,121,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Inference-time segmentation and image reconstruction at various stages of the prior's training process. a) Prior has not been trained. Inference can roughly reconstruct the image outline. Segmentation fails. b) Early on, reconstructed images are blurry. Segmentation is poor, but at the correct region. c) Eventually images are reconstructed with great detail and segmentations are accurate. d) Ground truth.</figDesc><graphic coords="7,57,30,337,73,309,49,131,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Interpolation predictions for a held-out basal slice. Top row: Predicted segmentation overlayed on predicted image. Bottom row: Ground truth segmentation overlayed on original image. Middle column is never shown to network during inference. Black slices don't exist in original image volume. The model appears to understand how the ventricles come into view as we descend down the slice dimension.</figDesc><graphic coords="8,58,98,273,95,334,51,81,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Segmentation of a held-out long-axis 4-chamber plane from SAX image data. a) Ground-truth long-axis 4-chamber view (not presented to network). b) Nearestneighbour interpolation of 4-chamber view from SAX volume. c) Predicted 4-chamber image plane. d) Predicted 4-chamber view segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Class Dice scores for the 100 subject test dataset.</figDesc><table><row><cell>Class</cell><cell cols="4">Classes average LV blood Pool LV myocardium RV blood Pool</cell></row><row><cell cols="2">Dice score 0.87 ± 0.045</cell><cell>0.90 ± 0.037</cell><cell>0.82 ± 0.075</cell><cell>0.88 ± 0.063</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code repository: https://github.com/NILOIDE/Implicit segmentation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is funded by the <rs type="funder">Munich Center for Machine Learning and European Research Council (ERC)</rs> project <rs type="grantNumber">Deep4MI (884622</rs>). This research has been conducted using the UK Biobank Resource under Application Number <rs type="grantNumber">87802</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MkE4G4E">
					<idno type="grant-number">Deep4MI (884622</idno>
				</org>
				<org type="funding" xml:id="_3gMgcw9">
					<idno type="grant-number">87802</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning shape reconstruction from sparse measurements with neural implicit functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amiranashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lüdke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated cardiovascular magnetic resonance image analysis with fully convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cardiovasc. Magn. Reson</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review on the strategies and techniques of image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Advanced Computing &amp; Communication Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully automatic segmentation of right and left ventricle on shortaxis cardiac MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Budai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">101786</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention to scale: scaleaware semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoderpredictor CNNs and shape synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SoftSeg: advantages of soft versus binary training for image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102038</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07947</idno>
		<title level="m">Efficient multiple instance convolutional neural networks for gigapixel resolution image classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="174" to="182" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08479</idno>
		<title level="m">Neural implicit k-Space for binning-free non-cartesian cardiac MR imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Implicit neural representations for medical imaging segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09">7-9 May 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypernetwork functional image representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klocek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wo Lczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Śmieja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2019: Workshop and Special Sessions: 28th International Conference on Artificial Neural Networks</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Tetko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Kurková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Karpov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Theis</surname></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-19">17-19 September 2019</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="496" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30493-5_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-30493-548" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Standardized cardiovascular magnetic resonance imaging (CMR) protocols: 2020 update</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barkhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bucciarelli-Ducci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Flamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cardiovasc. Magn. Reson</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modulated periodic activations for generalizable local functional representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14214" to="14223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DeepSDF: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58580-8_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58580-831" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">12348. 2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="523" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10-09">5-9 October 2015. 2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Saragadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lejeune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05187</idno>
		<title level="m">WIRE: wavelet implicit neural representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MetaSDF: metalearning signed distance functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10136" to="10147" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning 3D shape completion from laser scan data with weak supervision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1955" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UK Biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sudlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1001779</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="https://biobank.ndph.ox.ac.uk/showcase/" />
		<title level="m">UK Biobank: Data showcase</title>
		<imprint>
			<date type="published" when="2023-03-07">7 Mar 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicit neural representations for deformable image registration</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Zwienenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brune</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1349" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
