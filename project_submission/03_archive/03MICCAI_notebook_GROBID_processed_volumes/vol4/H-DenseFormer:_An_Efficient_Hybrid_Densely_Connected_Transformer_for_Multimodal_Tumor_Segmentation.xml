<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</title>
				<funder ref="#_e6hAfHJ">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jun</forename><surname>Shi</surname></persName>
							<email>shijun18@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Kan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shulan</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minfan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaohui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>An</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xudong</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Hubei Cancer Hospital</orgName>
								<orgName type="institution" key="instit2">Tongji Medical College</orgName>
								<orgName type="institution" key="instit3">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="692" to="702"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7D72EA16011001C375D2355EA1C023E0</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tumor segmentation</term>
					<term>Multimodal medical image</term>
					<term>Transformer</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning methods have been widely used for tumor segmentation of multimodal medical images with promising results. However, most existing methods are limited by insufficient representational ability, specific modality number and high computational complexity. In this paper, we propose a hybrid densely connected network for tumor segmentation, named H-DenseFormer, which combines the representational power of the Convolutional Neural Network (CNN) and the Transformer structures. Specifically, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module that can take an arbitrary number of modalities as input to extract the fusion features from different modalities. Then, the multimodal fusion features are delivered to different levels of the encoder to enhance multimodal learning representation. Besides, we design a lightweight Densely Connected Transformer (DCT) block to replace the standard Transformer block, thus significantly reducing computational complexity. We conduct extensive experiments on two public multimodal datasets, HECK-TOR21 and PI-CAI22. The experimental results show that our proposed method outperforms the existing state-of-the-art methods while having lower computational complexity. The source code is available at https:// github.com/shijun18/H-DenseFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate tumor segmentation from medical images is essential for quantitative assessment of cancer progression and preoperative treatment planning <ref type="bibr" target="#b3">[3]</ref>. Tumor tissues usually present different features in different imaging modalities. For example, Computed Tomography (CT) and Positron Emission Tomography (PET) are beneficial to represent morphological and metabolic information of tumors, respectively. In clinical practice, multimodal registered images, such as PET-CT images and Magnetic Resonance (MR) images with different sequences, are often utilized to delineate tumors to improve accuracy. However, manual delineation is time-consuming and error-prone, with a low inter-professional agreement <ref type="bibr" target="#b12">[12]</ref>. These have prompted the demand for intelligent applications that can automatically segment tumors from multimodal images to optimize clinical procedures.</p><p>Recently, multimodal tumor segmentation has attracted the interest of many researchers. With the emergence of multimodal datasets (e.g., BRATS <ref type="bibr" target="#b25">[25]</ref> and HECKTOR <ref type="bibr" target="#b0">[1]</ref>), various deep-learning-based multimodal image segmentation methods have been proposed <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b32">31]</ref>. Overall, large efforts have been made on effectively fusing image features of different modalities to improve segmentation accuracy. According to the way of feature fusion, the existing methods can be roughly divided into three categories <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b37">36]</ref>: input-level fusion, decisionlevel fusion, and layer-level fusion. As a typical approach, input-level fusion <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b35">34]</ref> refers to concatenating multimodal images in the channel dimension as network input during the data processing or augmentation stage. This approach is suitable for most existing end-to-end models <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b33">32]</ref>, such as U-Net <ref type="bibr" target="#b29">[28]</ref> and U-Net++ <ref type="bibr" target="#b38">[37]</ref>. However, the shallow fusion entangles the low-level features from different modalities, preventing the effective extraction of high-level semantics and resulting in limited performance gains. In contrast, <ref type="bibr" target="#b36">[35]</ref> and <ref type="bibr" target="#b21">[21]</ref> propose a solution based on decision-level fusion. The core idea is to train an independent segmentation network for each data modality and fuse the results in a specific way. These approaches can bring much extra computation at the same time, as the number of networks is positively correlated with the number of modalities. As a compromise alternative, layer-level fusion methods such as HyperDense-Net <ref type="bibr" target="#b10">[10]</ref> advocate the cross-fusion of the multimodal features in the middle layer of the network.</p><p>In addition to the progress on the fusion of multimodal features, improving the model representation ability is also an effective way to boost segmentation performance. In the past few years, Transformer structure <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">30]</ref>, centered on the multi-head attention mechanism, has been introduced to multimodal image segmentation tasks. Extensive studies <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16]</ref> have shown that the Transformer can effectively model global context to enhance semantic representations and facilitate pixel-level prediction. Wang et al. <ref type="bibr" target="#b32">[31]</ref> proposed TransBTS, a form of input-level fusion with a U-like structure, to segment brain tumors from multimodal MR images. TransBTS employs the Transformer as a bottleneck layer to wrap the features generated by the encoder, outperforming the traditional end-to-end models. Saeed et al. <ref type="bibr" target="#b30">[29]</ref> adopted a similar structure in which the Transformer serves as the encoder rather than a wrapper, also achieving promising performance. Other works like <ref type="bibr" target="#b9">[9]</ref> and <ref type="bibr" target="#b34">[33]</ref>, which combine the Transformer with the multimodal feature fusion approaches mentioned above, further demonstrate the potential of this idea for multimodal tumor segmentation.</p><p>Although remarkable performance has been accomplished with these efforts, there still exist several challenges to be resolved. Most existing methods are either limited to specific modality numbers due to the design of asymmetric connections or suffer from large computational complexity because of the huge amount of model parameters. Therefore, how to improve model ability while ensuring computational efficiency is the main focus of this paper.</p><p>To this end, we propose an efficient multimodal tumor segmentation solution named Hybrid Densely Connected Network (H-DenseFormer). First, our method leverages Transformer to enhance the global contextual information of different modalities. Second, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module, which can extract and fuse multimodal image features as a complement to naive input-level fusion structure. Specifically, MPE assigns an independent encoding path to each modality, then merges the semantic features of all paths and feeds them to the encoder of the segmentation network. This decouples the feature representations of different modalities while relaxing the input constraint on the specific number of modalities. Finally, we design a lightweight, Densely Connected Transformer (DCT) module to replace the standard Transformer to ensure performance and computational efficiency. Extensive experimental results on two publicly available datasets demonstrate the effectiveness of our proposed method. as the auxiliary extractor of multimodal fusion features, while the latter is used to generate predictions. Specifically, given a multimodal image input X 3D ∈ R C×H×W ×D or X 2D ∈ R C×H×W with a spatial resolution of H × W , the depth dimension of D (number of slices) and C channels (number of modalities), we first utilize MPE to extract and fuse multimodal image features. Then, the obtained features are progressively upsampled and delivered to the encoder of the segmentation network to enhance the semantic representation. Finally, the segmentation network generates multi-scale outputs, which are used to calculate deep supervision loss as the optimization target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture of H-DenseFormer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-path Parallel Embedding</head><p>Many methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15]</ref> have proved that decoupling the feature representation of different modalities facilitates the extraction of high-quality multimodal features. Inspired by this, we design a Multip-path Parallel Embedding (MPE) module to enhance the representational ability of the network. As shown in Fig. <ref type="figure">1</ref>, each modality has an independent encoding path consisting of a patch embedding module, stacked Densely Connected Transformer (DCT) modules, and a reshape operation. The independence of the different paths allows MPE to handle an arbitrary number of input modalities. Besides, the introduction of the Transformer provides the ability to model global contextual information. Given the input X 3D , after convolutional embedding and tokenization, the obtained feature of the i-th path is</p><formula xml:id="formula_0">F i ∈ R l× H p × W p × D p</formula><p>, where i ∈ [1, 2, ..., C], p = 16 and l = 128 denote the path size and embedding feature length respectively. First, we concatenate the features of all modalities and entangle them using a convolution operation. Then, interpolation upsampling is performed to obtain the multimodal fusion feature</p><formula xml:id="formula_1">F out ∈ R k× H 8 × W 8 × D 8 ,</formula><p>where k = 128 refers to the channel dimension. Finally, F out is progressively upsampled to multiple scales and delivered to different encoder stages to enhance the learned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Densely Connected Transformer</head><p>Standard Transformer structures <ref type="bibr" target="#b11">[11]</ref> typically consist of dense linear layers with a computational complexity proportional to the feature dimension. Therefore, integrating the Transformer could lead to a mass of additional computation and memory requirements. Shortening the feature length can effectively reduce computation, but it also weakens the representation capability meanwhile. To address this problem, we propose the Densely Connected Transformer (DCT) module inspired by DenseNet <ref type="bibr" target="#b17">[17]</ref> to balance computational cost and representation capability. Figure <ref type="figure">1</ref> details the DCT module, which consists of four Transformer layers and a feedforward layer. Each Transformer layer has a linear projection layer that reduces the input feature dimension to g = 32 to save computation. Different Transformer layers are connected densely to preserve representational power with lower feature dimensions. The feedforward layer at the end generates the fusion features of the different layers. Specifically, the output z j of the j-th (j ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b4">4]</ref>) Transformer layer can be calculated by:</p><formula xml:id="formula_2">zj-1 = p(cat([z 0 ; z 1 ; ...; z j-1 ])),<label>(1)</label></formula><formula xml:id="formula_3">zj = att(norm(z j-1 )) + zj-1 ,<label>(2)</label></formula><formula xml:id="formula_4">z j = f (norm(z j )),<label>(3)</label></formula><p>where z 0 represents the original input, cat(•) and p(•) denote the concatenation operator and the linear layer, respectively. The norm(•), att(•), f (•) are the regular layer normalization, multi-head self-attention mechanism, and feedforward layer. The output of DCT is z out = f (cat([z 0 ; z 1 ; ...; z 4 ])). Table <ref type="table" target="#tab_0">1</ref> shows that the stacked DCT has lower parameters and computational complexity than a standard Transformer structure with the same number of layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Segmentation Backbone Network</head><p>The H-DenseFormer adopts a U-shaped encoder-decoder structure as its backbone. As shown in Fig. <ref type="figure">1</ref>, the encoder extracts features and reduces their resolution progressively. To preserve more details, we set the maximum downsampling factor to 8. The multi-level multimodal features from MPE are fused in a bitwise addition way to enrich the semantic information. The decoder is used to restore the resolution of the features, consisting of deconvolutional and convolutional layers with skip connections to the encoder. In particular, we employ Deep Supervision (DS) loss to improve convergence, which means that the multiscale output of the decoder is involved in the final loss computation. Deep Supervision Loss. During training, the decoder has four outputs; for example, the i-th output of 2D H-DenseFormer is</p><formula xml:id="formula_5">O i ∈ R c× H 2 i × W 2 i</formula><p>, where i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of segmentation classes. To mitigate the pixel imbalance problem, we use a combined loss of Focal loss <ref type="bibr" target="#b23">[23]</ref> and Dice loss as the optimization target, defined as follows:</p><formula xml:id="formula_6">ζ F D = 1 - 2 N t=1 p t q t N t=1 p t + q t + 1 N N t=1 -(1 -p t ) γ log(p t ),<label>(4)</label></formula><p>where N refers to the total number of pixels, p t and q t denote the predicted probability and ground truth of the t-th pixel, respectively, and r = 2 is the modulation factor. Thus, DS loss can be calculated as follows:</p><formula xml:id="formula_7">ζ DS = α i • ζ F D (O i , G i ), α i = 2 -i . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where G i represents the ground truth after resizing and has the same size as O i . α is a weighting factor to control the proportion of loss corresponding to the output at different scales. This approach can improve the convergence speed and performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Metrics</head><p>To validate the effectiveness of our proposed method, we performed extensive experiments on HECKTOR21 <ref type="bibr" target="#b0">[1]</ref> and PI-CAI22<ref type="foot" target="#foot_0">1</ref> . HECKTOR21 is a dualmodality dataset for head and neck tumor segmentation, containing 224 PET-CT image pairs. Each PET-CT pair is registered and cropped to a fixed size of (144,144,144). PI-CAI22 provides multimodal MR images of 220 patients with prostate cancer, including T2-Weighted imaging (T2W), high b-value Diffusion-Weighted imaging (DWI), and Apparent Diffusion Coefficient (ADC) maps. After standard resampling and center cropping, all images have a size of <ref type="bibr" target="#b24">(24,</ref><ref type="bibr">384,</ref><ref type="bibr">384)</ref>. We randomly select 180 samples for each dataset as the training set and the rest as the independent test set (44 cases for HECKTOR21 and 40 cases for PI-CAI22). Specifically, the training set is further randomly divided into five folds for cross-validation. For quantitative analysis, we use the Dice Similarity Coefficient (DSC), the Jaccard Index (JI), and the 95% Hausdorff Distance (HD95) as evaluation metrics for segmentation performance. A better segmentation will have a smaller HD95 and larger values for DSC and JI. We also conduct holistic t-tests of the overall performance for our method and all baseline models with the two-tailed p &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We use Pytorch to implement our proposed method and the baselines. For a fair comparison, all models are trained from scratch using two NVIDIA A100 GPUs and all comparison methods are implemented with open-source codes, following their original configurations. In particular, we evaluate the 3D and 2D H-DenseFormer on HECKTOR21 and PI-CAI22, respectively. During the training phase, the Adam optimizer is employed to minimize the loss with an initial learning rate of 10 -3 and a weight decay of 10 -4 . We use the PolyLR strategy <ref type="bibr" target="#b19">[19]</ref> to control the learning rate change. We also use an early stopping strategy with a tolerance of 30 epochs to find the best model within 100 epochs. Online data augmentation, including random rotation and flipping, is performed to alleviate the overfitting problem. Table <ref type="table" target="#tab_1">2</ref> compares the performance and computational complexity of our proposed method with the existing state-of-the-art methods on the independent test sets. For HECKTOR21, 3D H-DenseFormer achieves a DSC of 73.9%, HD95 of 8.1mm, and JI of 62.5%, which is a significant improvement (p &lt; 0.01) over 3D U-Net <ref type="bibr" target="#b7">[7]</ref>, UNETR <ref type="bibr" target="#b16">[16]</ref>, and TransBTS <ref type="bibr" target="#b32">[31]</ref>. It is worth noting that the performance of hybrid models such as UNETR is not as good as expected, even worse than 3D U-Net, perhaps due to the small size of the dataset. Moreover, compared to the champion solution of HECKTOR20 proposed by Iantsen et al. <ref type="bibr" target="#b18">[18]</ref>, our method has higher accuracy and about 10 and 5 times lower amount of network parameters and computational cost, respectively. For PI-CAI22, the 2D variant of H-DenseFormer also outperforms existing methods (p &lt; 0.05), achieving a DSC of 49.9%, HD95 of 35.9 mm, and JI of 37.1%. Overall, H-DenseFormer reaches an effective balance of performance and computational cost compared to existing CNNs and hybrid structures. For qualitative analysis, we show a visual comparison of the different methods. It is evident from Fig. <ref type="figure" target="#fig_1">2</ref> that our approach can describe tumor contours more accurately while providing better segmentation accuracy for small-volume targets. These results further demonstrate the effectiveness of our proposed method in multimodal tumor segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Sensitivity and Ablation Study</head><p>Impact of DCT Depth. As illustrated in Table <ref type="table" target="#tab_2">3</ref>, the network performance varies with the change in DCT depth. H-DenseFormer achieves the best performance at the DCT depth of 6. An interesting finding is that although the depth  of the DCT has increased from 3 to 9, the performance does not improve or even worsen. We suspect that the reason is over-fitting due to over-parameterization. Therefore, choosing a proper DCT depth is crucial to improve accuracy. Impact of Different Modules. The above results demonstrate the superiority of our method, but it is unclear which module plays a more critical role in performance improvement. Therefore, we perform ablation experiments on MPE, DCT and DS loss. Specifically, w/o MPE refers to keeping one embedding path, w/o DCT signifies using a standard 12-layer Transformer, and w/o DS loss denotes removing the deep supervision mechanism. As shown in Table <ref type="table" target="#tab_3">4</ref>, the performance decreases with varying degrees when removing them separately, which means all the modules are critical for H-DenseFormer. We can observe that DCT has a greater impact on overall performance than the others, further demonstrating its effectiveness. In particular, the degradation after removing the MPE also con- firms that decoupling the feature expression of different modalities helps obtain higher-quality multimodal features and improve segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed an efficient hybrid model (H-DenseFormer) that combines Transformer and CNN for multimodal tumor segmentation. Concretely, a Multi-path Parallel Embedding module and a Densely Connected Transformer block were developed and integrated to balance accuracy and computational complexity. Extensive experimental results demonstrated the effectiveness and superiority of our proposed H-DenseFormer. In future work, we will extend our method to more tasks and explore more efficient multimodal feature fusion methods to further improve computational efficiency and segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Fig. 1 .</head><label>11</label><figDesc>Figure 1 illustrates the overall architecture of our method. H-DenseFormer comprises a Multi-path Parallel Embedding (MPE) module and a U-shaped segmentation backbone network in form of input-level fusion. The former serves</figDesc><graphic coords="3,61,80,410,00,301,03,167,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualizations of different models on HECKTOR21 (left) and PI-CAI22 (right).</figDesc><graphic coords="8,67,47,54,17,317,50,125,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the computational complexity between the standard 12-layer Transformer structure and the stacked 3 (=12/4) DCT modules.</figDesc><table><row><cell cols="2">Feature Dimension Resolution Transformer</cell><cell></cell><cell cols="2">Stacked DCT (×3)</cell></row><row><cell></cell><cell cols="4">GFLOPs ↓ Params ↓ GFLOPs ↓ Params ↓</cell></row><row><cell>256</cell><cell>(512,512) 6.837</cell><cell>6.382M</cell><cell>2.671</cell><cell>1.435M</cell></row><row><cell>512</cell><cell>(512,512) 26.256</cell><cell cols="2">25.347M 3.544</cell><cell>2.290M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with existing methods on independent test set. We show the mean ± std (standard deviation) scores of averaged over the 5 folds.</figDesc><table><row><cell>Methods (Year)</cell><cell cols="4">Params↓ GFLOPs↓ DSC(%) ↑ HD95(mm) ↓ JI(%) ↑</cell></row><row><cell cols="2">HECKTOR21, two modalities (CT and PET)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D U-Net (2016) [7]</cell><cell>12.95M 629.07</cell><cell>68.8 ± 1.4</cell><cell>14.9 ± 2.2</cell><cell>58.0 ± 1.4</cell></row><row><cell>UNETR (2022) [16]</cell><cell>95.76M 282.19</cell><cell>59.6 ± 2.5</cell><cell>23.7 ± 3.4</cell><cell>48.2 ± 2.6</cell></row><row><cell cols="2">Iantsen et al. (2021) [18] 38.66M 1119.75</cell><cell>72.4 ± 0.8</cell><cell>9.6 ± 1.0</cell><cell>60.5 ± 1.1</cell></row><row><cell>TransBTS (2021) [31]</cell><cell>30.62M 372.80</cell><cell>64.8 ± 1.0</cell><cell>20.9 ± 3.9</cell><cell>52.9 ± 1.2</cell></row><row><cell>3D H-DenseFormer</cell><cell>3.64M 242.96</cell><cell cols="2">73.9 ± 0.5 8.1 ± 0.6</cell><cell>62.5 ± 0.5</cell></row><row><cell cols="3">PI-CAI22, three modalities (T2W, DWI and ADC)</cell><cell></cell><cell></cell></row><row><cell>Deeplabv3+ (2018) [6]</cell><cell>12.33M 10.35</cell><cell>47.4 ± 1.9</cell><cell>48.4 ± 14.3</cell><cell>35.4 ± 1.7</cell></row><row><cell>U-Net++ (2019) [37]</cell><cell>15.97M 36.08</cell><cell cols="2">49.7 ± 3.9 38.5 ± 6.7</cell><cell>36.9 ± 3.3</cell></row><row><cell>ITUNet (2022) [22]</cell><cell>18.13M 32.67</cell><cell>42.1 ± 2.3</cell><cell>67.6 ± 10.3</cell><cell>31.3 ± 1.6</cell></row><row><cell>Transunet (2021) [4]</cell><cell>93.23M 72.62</cell><cell>44.8 ± 3.0</cell><cell>59.3 ± 14.8</cell><cell>33.2 ± 2.5</cell></row><row><cell>2D H-DenseFormer</cell><cell>4.25M 31.46</cell><cell cols="3">49.9 ± 1.2 35.9 ± 8.2 37.1 ± 1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Parameter sensitivity analysis on DCT depth.</figDesc><table><row><cell>3</cell><cell>3.25M</cell><cell>242.38</cell><cell>73.5 ± 1.4 8.4 ± 0.7</cell><cell>62.2 ± 1.6</cell></row><row><cell>6</cell><cell>3.64M</cell><cell>242.96</cell><cell>73.9 ± 0.5 8.1 ± 0.6</cell><cell>62.5 ± 0.5</cell></row><row><cell>9</cell><cell>4.03M</cell><cell>243.55</cell><cell>72.</cell><cell></cell></row></table><note><p>DCT Depth Params↓ GFLOPs↓ DSC (%) ↑ HD95 (mm) ↓ JI (%) ↑ 7 ± 1.2 8.7 ± 0.6 61.2 ± 1.3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of 3D H-DenseFormer, w/o denotes without.</figDesc><table><row><cell>Method</cell><cell cols="2">DSC (%) ↑ HD95 (mm) ↓ JI (%) ↑</cell></row><row><cell>3D H-DenseFormer w/o MPE</cell><cell>72.1 ± 0.8 10.8 ± 1.1</cell><cell>60.4 ± 0.8</cell></row><row><cell>3D H-DenseFormer w/o DCT</cell><cell>70.7 ± 1.8 11.9 ± 1.9</cell><cell>58.6 ± 2.1</cell></row></table><note><p>3D H-DenseFormer w/o DS loss 72.2 ± 0.9 10.2 ± 1.0 60.1 ± 1.2 3D H-DenseFormer 73.9 ± 0.5 8.1 ± 0.6 62.5 ± 0.5</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://pi-cai.grand-challenge.org/.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>contributed equally. This study was supported by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">YD2150002001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_e6hAfHJ">
					<idno type="grant-number">YD2150002001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the HECKTOR challenge at MICCAI 2020: automatic head and neck tumor segmentation in PET/CT</title>
		<author>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2020</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Oreiller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12603</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-67194-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-67194-5_1" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Swin-UNet: UNet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022 Workshops. ECCV 2022, Part III</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_50" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part III 22</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MRI tumor segmentation with densely connected 3D CNN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dsouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Abidin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wismüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page" from="357" to="364" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with Atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_49" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic semantic segmentation of brain gliomas from MRI images using a deep cascaded neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthc. Eng</title>
		<imprint>
			<biblScope unit="page">4940593</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining CNNs with transformer for multimodal 3D MRI brain tumor segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dobko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kolinko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Viniavskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yelisieiev</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-09002-8_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-09002-8_21" />
	</analytic>
	<monogr>
		<title level="m">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event</title>
		<title level="s">Revised Selected Papers</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09-27">27 September 2021. 2022</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HyperDense-Net: a hyper-densely connected CNN for multi-modal image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1116" to="1126" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A review on segmentation of positron emission tomography images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mollura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Bio. Med</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="76" to="96" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal spatial attention module for targeting multimodal PET-CT lung tumor segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3507" to="3516" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UTNet: a hybrid transformer architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_6" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part III</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning-based image segmentation on multimodal medical imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV 2022 Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017 Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation normalization for automated delineation of head and neck primary tumors in combined PET and CT images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iantsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Visvikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-67194-5_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-67194-5_4" />
	</analytic>
	<monogr>
		<title level="m">HECKTOR 2020</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Oreiller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12603</biblScope>
			<biblScope unit="page" from="37" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensembles of multiple models and architectures for robust brain tumour segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-75238-9_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-75238-9_38" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2017</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10670</biblScope>
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ITUnet: Integration of transformers and UNet for organs-at-risk segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMBC 2022</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2123" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017 Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2021 Proceedings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in MRI images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal brain tumor segmentation using 3D convolutional networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Rodríguez Colmeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Verrastro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grosges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BrainLes 2017</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10670</biblScope>
			<biblScope unit="page" from="226" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-75238-9_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-75238-9_20" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III 18</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TMSS: an end-to-end transformer-based multimodal network for segmentation and survival prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sobirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al Majzoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_31" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="319" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part I 24</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Weighted Res-UNet for high-quality retina vessel segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<editor>ITME</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022 Proceedings, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deep learning model integrating FCNNs and CRFs for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="98" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">3D fully convolutional networks for co-segmentation of tumors on PET-CT images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<editor>ISBI</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A review: deep learning for medical image segmentation using multi-modality fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Array</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100004</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">UNet++: redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
