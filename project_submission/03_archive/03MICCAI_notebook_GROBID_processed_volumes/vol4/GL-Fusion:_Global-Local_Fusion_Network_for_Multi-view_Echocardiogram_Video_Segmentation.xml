<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation</title>
				<funder ref="#_t3xeNN7">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_kdeV946 #_3nRfBkC">
					<orgName type="full">Foshan HKUST</orgName>
				</funder>
				<funder ref="#_7xej3Me">
					<orgName type="full">Beijing Institute of Collaborative Innovation</orgName>
					<orgName type="abbreviated">BICI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiewen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinpeng</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
							<email>xiao.wei.xu@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Guangdong Cardiovascular Institute</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital (Guangdong Academy of Medical Sciences)</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="78" to="88"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">69E80F84E9F4A2B812AB550849BAF34E</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-view fusion</term>
					<term>Echocardiogram videos</term>
					<term>Cardiac structure segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cardiac structure segmentation from echocardiogram videos plays a crucial role in diagnosing heart disease. The combination of multi-view echocardiogram data is essential to enhance the accuracy and robustness of automated methods. However, due to the visual disparity of the data, deriving cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. In this study, we propose a novel Gobal-Local fusion (GL-Fusion) network to jointly utilize multi-view information globally and locally that improve the accuracy of echocardiogram analysis. Specifically, a Multi-view Global-based Fusion Module (MGFM) is proposed to extract global context information and to explore the cyclic relationship of different heartbeat cycles in an echocardiogram video. Additionally, a Multi-view Local-based Fusion Module (MLFM) is designed to extract correlations of cardiac structures from different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 82.29% average dice score, which demonstrates a 7.83% improvement over the baseline method, and outperforms other existing state-of-theart methods. To our knowledge, this is the first exploration of a multiview method for echocardiogram video segmentation. Code available at: https://github.com/xmed-lab/GL-Fusion</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate segmentation of the cardiac structure from echocardiogram videos is integral to several analysis tasks <ref type="bibr" target="#b11">[11]</ref> and has a significant impact on clinical Z. Zheng and J. Yang-Two authors contributed equally to this work. Z. Zheng-Work completed during the internship at HKUST. practice <ref type="bibr" target="#b27">[26]</ref>. For example, segmentation of the left ventricle (LV) enables quantifiable functional analysis of the heart, facilitating the detection and diagnosis of heart diseases <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b22">21]</ref>. Compared with the single view segmentation, multi-view information is crucial to diagnose heart disease, e.g., the diagnosis of congenital heart disease requires the analysis of four views: parasternal longaxis view (PSLAX), parasternal short-axis view (PSSAX), subxiphoid long-axis view (SXLAX), and suprasternal long-axis view (SSLAX) <ref type="bibr" target="#b23">[22]</ref>. Consequently, to assist clinicians in diagnostic decision-making, there is a high demand for developing automated multi-view cardiac structure segmentation methods from echocardiogram videos in clinical practice. Existing echocardiogram segmentation approaches are primarily designed for single-view images or videos. For instance, Li et al. <ref type="bibr" target="#b27">[26]</ref> proposed a dynamic neural network capable of segmenting the LV from a long-axis fetal echocardiogram. In comparison, Leclerc et al. <ref type="bibr" target="#b12">[12]</ref> evaluated an encoder-decoder deep convolutional neural network that independently segments two and four-chamber images. However, these approaches have not addressed multi-view segmentation, where multi-view segmentation methods already exist in other medical domains, such as the CT-MRI <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b19">18]</ref>, multiview cardiac MRI <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16]</ref>, multi-view mammogram <ref type="bibr" target="#b1">[2]</ref>, and longitudinal multiple sclerosis <ref type="bibr" target="#b0">[1]</ref>. Applying the proposed methods to multi-view echocardiogram segmentation presents several limitations: (1) Some methods are built for specific datasets and cannot adapt to our task. For instance, UMCT <ref type="bibr" target="#b26">[25]</ref> designated supervised training in one view by generating pseudo segmentation labels from other views, but has limitations in our task due to the significant gaps between views. In contrast, InfoTrans <ref type="bibr" target="#b13">[13]</ref> is designed for transmitting information between views instead of fusion them. While VCN <ref type="bibr" target="#b5">[6]</ref> employs contrastive learning to predict volume but may not be suitable for our task since defining positive and negative pairs is challenging due to the significant gap between views and labels. (2) Methods such as JOIN <ref type="bibr" target="#b1">[2]</ref>, ROI-based fine-grained CNN <ref type="bibr" target="#b14">[14]</ref>,</p><p>MIMTP <ref type="bibr" target="#b0">[1]</ref>, MV U-Net <ref type="bibr" target="#b3">[4]</ref>, MV-CNN <ref type="bibr" target="#b24">[23]</ref>, and Type-I, II, III <ref type="bibr" target="#b8">[9]</ref> concatenate the features or predicted probability maps of different views and then apply a fully-connected layer. However, these naive fusion strategies have shown limited performance and may even lead to worse results; see results in Table <ref type="table" target="#tab_1">1</ref>. (3) Existing multi-view segmentation methods such as TransFusion <ref type="bibr" target="#b15">[15]</ref> and rDLA <ref type="bibr" target="#b16">[16]</ref> mainly apply multi-view fusion with only global features. However, using global features for multi-view fusion may result in tangling the foreground/background pixels <ref type="bibr" target="#b9">[10]</ref> or leads to high levels of background noise in echocardiograms.</p><p>To address this limitation, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we first collect a multi-view echocardiogram video dataset, including three views: parasternal left ventricle long axis (PLVLA view), left ventricular short axis (LVSA) view, and apical 4 chamber (A4C) view. Different views of echocardiograms contain annotations for different chambers, such as, the PLVLA view contains the left ventricle (LV) and right ventricle (RV), the LVSA view contains the LV and RV, and the A4C view contains the LV, left atrium (LA), right atrium (RA), and RV. Furthermore, we propose a novel global-local fusion (GL-Fusion) network for multiview echocardiogram video segmentation, where GL-Fusion includes a multi-view local-global fusion module designed to aggregate information from different views and improve the representation of each view. The GL-Fusion comprises two components. First, a multi-view global fusion module (MGFM) interacts with the global semantics between different views and thus enhances the representation of each view. Second, since the global semantics may contain a significant amount of noisy information, a multi-view local fusion module (MLFM) is introduced to encourage the model to focus on foreground information.</p><p>In addition to capturing multi-view information, we propose a novel dense cycle loss designed to utilize unlabelled video data for improved representation learning. Our motivation is based on the idea that standard multi-view data is obtained from the same patient and under the same stable conditions, without abnormal behaviours such as suffocating or exercising, ensuring consistent cardiac cycles. Previous work <ref type="bibr" target="#b6">[7]</ref> proposed an unsupervised method called cycle loss, which trains the model with unlabelled frames based on the heartbeat cycle's characteristics. Nevertheless, the proposed cycle loss only focuses on a pair in two different cycles but ignores possibly similar images that may appear simultaneously in a systolic or diastolic period, resulting in features from similar frames being considered distant. To address this issue, our dense cycle loss examines all possible pairings throughout the heartbeat cycle. In summary, our contributions are as follows:</p><p>-To the best of our knowledge, this is the first study to examine multi-view echocardiogram video segmentation. -Our proposed GL-Fusion uses a multi-view local-global fusion module to combine information from different views and improve the representation of each view. -We further design a dense cycle loss that utilizes unlabelled data to enforce feature similarity based on temporal cyclicality.  -Extensive experiments demonstrate our method improved performance over existing methods, achieving an average dice score of 0.81. We plan to make our code publicly available upon paper acceptance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Overall Framework</head><formula xml:id="formula_0">V = {X i } V i=1</formula><p>, where X i ∈ R C×H×W ×T is the i-th view video and V is the number of views, and, C, H, W and T indicate the channels, height, width, and length of input images. Each video consists of T frames, i.e., X i = {x i t } T t=1 , where T remain the same for different view and x i t ∈ R C×H×W indicate t-th frame of i-th view video.</p><p>Since only sparse frames are provided segmentation annotation for training in a video, thus we denote the annotation frame pair as {x i tn , y i tn } N n=1 , where t n is the index of the annotation and N is the number of labelled frames that N &lt;&lt; T .</p><p>During the training, We feed the videos V into the view-based encoder to extract the corresponding feature maps {F i } V i=1 of each view, where F i ∈ R D×h×w×T , and, D, h and w indicate the channel number, height and width of feature maps. Then the multi-view global-local fusion module aims to obtain the multi-view fused features {F i } V i=1 , which extract global and local semantics information from other views to enhance the representation of each view (See Sect. 2.2). Following is the view-based decoder that generates the predicted segmentation result y i from fused features, and maps the results to corresponding segmentation annotation, i.e., ŷi tn to the segmentation masks y i tn . For the annotated frames, we use the segmentation loss to supervise them, formulated as follows:</p><formula xml:id="formula_1">L seg = V i=1 N tn=1 L bce (ŷ i tn , y i tn ),<label>(1)</label></formula><p>where L bce is the Binary Cross Entropy. The sparse annotations are only a few frames in the whole video; thus can not obtain a robust model. To leverage a large number of unlabelled frames, we design the dense cycle loss L cyc to enforce temporal feature similarity of videos based on cyclicality; See Sect. 2.3.</p><p>The overall loss function of our model is as follows:</p><formula xml:id="formula_2">L = L seg + αL cyc , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where α is the hyper-parameter to control the weight between two losses. In the following, we will illustrate the multi-view global-local fusion module and the dense cycle loss in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-view Global-Local Fusion Module</head><p>In this section, we describe the multi-view global-local fusion module that aggregates the information from different views to enhance their feature representation. To this end, we first concatenate extracted feature {F i } V i=1 from different views in a view-wise manner to obtain F = {f t } T t=1 , where f t is the t-th feature vector in F, and f t ∈ R D×V ×w×h . Then, we describe the multi-view global and local fusion with F global and F local , respectively.</p><p>Multi-view Global Fusion. In order to enhance the representation of each view, we propose the global-based fusion module (MGFM) to interact with the global semantics between different views. To this end, we introduce a view-wise non-local block, which extracts the context information across views. Similarly to the previous research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">24]</ref> that applied attention to fuse the information, we here introduce the view-wise attention module to aggregate the cross-view information (see Fig. <ref type="figure" target="#fig_1">2</ref>). Then fused feature F global will be sent to both compute the dense cycle loss and cooperate with the local fused feature for segmentation prediction.</p><p>Multi-view Local Fusion. Since each view represents different morphological information of the heart and may contain the same cardiac structure as others, for example, the view PLVLA and LVSA both contain left ventricle(LA) and right ventricle(RV). Hence, extracting the local feature that represents the cardiac structure can contribute to feature fusion more efficiently. In this module, the extracted feature F local will first pass to both the view-based decoder and a center block, where the decoder and center block has the same components with different output. The decoder provides the pseudo label {ŷ i } V i=1 of different cardiac structures. A center block is introduced to acquire the weight {w i } V i=1 of {ŷ i } V i=1 and compute the local feature masks {M i } V i=1 as Eq. 3,</p><formula xml:id="formula_4">M i = σ(pooling(σ(ŷ i )) × σ(w i )),<label>(3)</label></formula><p>where weight w has the greatest volume in the central area of the segmented regions and attenuation with distance, σ denotes the sigmoid function and M ∈ R 1×H×W ×T . These masks highlight features with a stronger intensity that are closer to the object center, while discarding background information that is farther away from the center. This selection is based on the understanding that morphological information should remain consistent closer to the center. In the final, similar to the process of MLFM, the view-wise local feature will be conducted view-wise concatenation operation and multiplied with local feature mask {M i } V i=1 . Then sent to the view-wise attention module to acquire the local fused feature F local .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dense Cycle Loss</head><p>In echocardiogram videos, since only sparse annotation is available for the supervised training, involving the unlabelled data for our training and enhancing the performance is a challenge. The previous research <ref type="bibr" target="#b6">[7]</ref> proposes an unsupervised method named cycle loss, which jointly trains the model with the unlabelled frames according to the characteristic of the heartbeat cycle. However, the proposed cycle loss considers only one clip in an iteration, which has the possibility to match frames that are morphologically identical but not in the same state, such as the search region being end-diastole while the template region is endsystole.</p><p>Thus, we propose the dense cycle loss, which considers all the possible matching across all template and search regions in each view independently. For the multi-view fused feature F global of each video will be separated to template region P i and search region Q i with a ratio in 2:3 according to total frame length T . Then we densely sample all feature intervals {p i 1 , ..., p i n } from P i and {q i 1 , ..., q i m } from Q i , respectively, both sampling use the same chunk size s and in our experiment, n and m is 2 5 × T s and 3 5 × T s . Then we compute the similarity between candidate interval p i k and target intervals q i j of Q i . where W(•) is the computation of the similarity matrix. The similarity will be used as the weight to reconstruct the feature interval pi k . Then we back to template region P i and compute the similarity between pi k and all feature intervals{p i 1 , ..., p i n } in P i . Then we consider the index of p i k as one-hot label g of the most similar interval of pi k and compute view-wise cycle loss L cyc with label g as shown in the following equation:</p><formula xml:id="formula_5">α i j = W({p} i k , {q} i j ) × {q} i j ,<label>(4)</label></formula><formula xml:id="formula_6">L cyc = V i=1 j∈P i 1 j=g log(α i j )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Datasets. We collect a large multi-view echocardiogram video dataset named MvEVD from one medical institution, with a total of 254 sparsely annotated videos and 10 fully annotated videos with 800 × 600 resolution across three cardiac views (PLVLA, LVSA and A4C view). Each video includes 5 annotated frames. The average length of each video is larger than 100 frames that are able to cover more than one cardiac cycle.</p><p>Implementation Details. We use the model DeeplabV3 <ref type="bibr" target="#b4">[5]</ref>     Validation and Testing. We use all fully annotated videos and split them into validation and testing with a ratio of 2:8. In this stage, we resize each frame in 144 × 144 size and conduct center cropping to them with the size of 112 × 112. Selecting the best model based on validation performance and report results in the testing set with Dice score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison with the State-of-the-Art Methods</head><p>To evaluate the performance of our method, we do the comparison with two types of methods: single-view methods and fusion-based methods in Table <ref type="table" target="#tab_1">1</ref>. To be specific, single-view methods independently train segmentation networks for each view without using any strategy across views or simply conducting semi-supervised approaches <ref type="bibr" target="#b6">[7]</ref>. Fusion-based methods use feature-fusion modules to aggregate features and predict the segmentation masks. Our GL-Fusion method can reach 83.84%, 81.76% and 81.28% performance in Dice score across three different views, with 10.49%, 4.19% and 4.68% boosts when compared with the best single-view method <ref type="bibr" target="#b20">[19]</ref>, and 4.75%, 2.06%, 3.57% enhancement when compared to the best single-view with semi-supervised method CSS <ref type="bibr" target="#b6">[7]</ref>. Also, compared with the different global fusion methods, our global and local fusion methods conduct significant improvements compared with the early-fusion approach. The visualization in Fig. <ref type="figure" target="#fig_3">3</ref> compares the segmentation quality with our GL-fusion method and others across three different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>In this section, we analyze the contribution to the performance of the proposed modules Multi-view Global Fusion Module (MGFM) and Multi-view Global Fusion Module (MGFM) of our framework. All results are illustrated in Table <ref type="table" target="#tab_3">2</ref>. a-b, the baseline without adapting any fusion strategy presents the lowest average dice, while using only MGFM or MLFM module can boost the result to 80.20% and 78.41%, respectively. The combination of these two modules can reach 82.29% dice score with a 2.09% increase in Dice score. In contrast, using the fusion method and cycle loss will lead to worse performance, while our proposed dense cycle loss can boost the result from 80.36% to 82.29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel fusion framework called GL-Fusion, which jointly uses global and local information to enhance the segmentation performance of echocardiogram videos. Additionally, to ensure fair evaluation of the multi-view segmentation results, we introduce a multi-view echocardiogram video dataset called MvEVD, which provides full annotation for validating and testing performance. Our results demonstrate that the proposed GL-Fusion framework significantly outperforms other methods. In the future, we aim to further improve our method and make it more efficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of multi-view echocardiogram dataset MvEVD, including PLVLA, LVSA, and A4C from top to bottom row. The colours red, green, blue, and cyan denote the LV, RV, LA, and RA cardiac structures. Our train set is sparsely annotated (5 frames per video), while the validation set and test set are fully annotated for each video frame. (Color figure online)</figDesc><graphic coords="2,62,40,54,44,333,43,109,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overview framework of GL-fusion. The Multi-view Global-based Fusion Module (MGFM) is proposed for global context information extraction and introduces dense cycle loss to devise the enforcement of the similarity of dense features between two heartbeat cycles from an echocardiogram video. The proposed Multi-view Local-based Fusion Module (MLFM) focuses on mining the correlation of local features of chambers in a different view.</figDesc><graphic coords="4,55,98,54,59,340,30,206,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure2shows the overall pipeline of our proposed Multi-view Echocardiogram Global-Local Fusion Network (GL-Fusion), which consists of four main components: a view-based encoder, a multi-view global-local fusion module, a dense cycle loss module and a view-based decoder, where view-based indicate that parameters of the network of each independent view are non-shared. In our experiment, we use DeeplabV3<ref type="bibr" target="#b4">[5]</ref> as our view-based encoder and decoder.Formally, we denoted the sample echocardiogram videos as V = {X i } V i=1 , where X i ∈ R C×H×W ×T is the i-th view video and V is the number of views, and, C, H, W and T indicate the channels, height, width, and length of input images. Each video consists of T frames, i.e., X i = {x i t } T t=1 , where T remain the same for different view and x i t ∈ R C×H×W indicate t-th frame of i-th view video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation results from three views of echocardiogram videos, including PLVLA, LVSA, and A4C from top to bottom row. The red, green, blue, and cyan colours refer to LV, RV, LA, and RA cardiac structures, respectively. (Color figure online)</figDesc><graphic coords="8,55,98,180,20,340,30,127,33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>MGFM MLFM Global Fusion Module View * N</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multiview</cell><cell>View Based</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Video Flow</cell><cell>Fused Feature</cell><cell>Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Image Flow</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Element-wise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>View Based Concatenated</cell><cell></cell><cell></cell><cell>Multiplication</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Video Feature</cell><cell>Share</cell><cell></cell><cell>: Element-wise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Weight</cell><cell></cell><cell>Addition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>View Based Concatenated</cell><cell>Global Fusion Module</cell><cell></cell><cell>Global Fused</cell><cell>Dense Cycle Loss</cell><cell>BCE Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Image Feature</cell><cell></cell><cell></cell><cell>Video Feature</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Local</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pooling</cell><cell></cell><cell>Local</cell><cell>Fusion Module</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>View Based Decoder</cell><cell></cell><cell></cell><cell>Feature</cell></row><row><cell>Annotated</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask</cell></row><row><cell>Frames</cell><cell>Unlabeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Videos</cell><cell>View Based</cell><cell>Feature</cell><cell></cell><cell></cell><cell></cell><cell>View Based</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell>Maps</cell><cell>Center Block</cell><cell></cell><cell></cell><cell>Concatenation</cell><cell>Ground Truth</cell></row><row><cell>Loss Dense Cycle</cell><cell cols="2">Template Region</cell><cell>Search Region</cell><cell></cell><cell>Most Similar</cell><cell></cell><cell>Ground Truth</cell><cell>Global &amp; Local Fusion Module</cell><cell>Fused Feature Projection reshape</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dense Cycle</cell><cell>reshape</cell><cell>reshape</cell><cell>reshape</cell></row><row><cell cols="2">Heartbeat Cycle</cell><cell></cell><cell></cell><cell cols="2">Compute Dense</cell><cell>Loss</cell><cell>Reconstruct</cell><cell>Q</cell><cell>K</cell><cell>V</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Similarity Matrix</cell><cell></cell><cell>Template Feature</cell><cell>Multi-view Feature</cell></row></table><note><p>Sample All points from the video clip</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The comparison with other methods. all results are reported in Dice Score.</figDesc><table><row><cell>Method</cell><cell cols="2">PLVLA LVSA A4C Average Dice (%)</cell></row><row><cell>Single-view DeeplabV3 [5]</cell><cell>70.93</cell><cell>75.14 77.33 74.46</cell></row><row><cell>U-Net [19]</cell><cell>73.35</cell><cell>77.57 76.60 75.84</cell></row><row><cell>CSS [7]</cell><cell>79.09</cell><cell>79.70 77.71 78.83</cell></row><row><cell>Fusion-based Early-fusion</cell><cell>79.78</cell><cell>77.07 77.58 78.14</cell></row><row><cell>Mid-fusion</cell><cell>77.89</cell><cell>76.75 72.44 75.69</cell></row><row><cell>Late-fusion</cell><cell>71.62</cell><cell>75.31 74.68 73.87</cell></row><row><cell cols="2">TransFusion [15] 78.79</cell><cell>80.23 59.31 72.78</cell></row><row><cell>Ours</cell><cell cols="2">83.84 81.76 81.28 82.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>as our view-based encoder and decoder, and select Adam optimizer for the model training with initial learning rate as 3e -4 and weight decay of 1e -5 . When training, we use all sparsely annotated videos. All annotated frames are selected to supervise training while randomly selecting 40 consecutive frames from videos for semisupervised training. The training batch size of annotated images and unlabeled videos is 8 and 1, respectively. In the final, we use CosineAnnealing as a scheduler and set the total training epoch to 100. The framework is built with Pytorch with 4 NVIDIA RTX3090 GPUs for training. For the data augmentation in the training stage, we resize each frame in 144 × 144 size and then randomly crop them to 112 × 112.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Effectiveness of MGFM and MLFM. This table shows the performance of the Global and Local fusion modules</figDesc><table><row><cell></cell><cell cols="3">MGFM MLFM Avg. Dice(%)</cell></row><row><cell>Base</cell><cell>✗</cell><cell>✗</cell><cell>74.46</cell></row><row><cell>Base+MGFM</cell><cell></cell><cell>✗</cell><cell>80.20</cell></row><row><cell cols="2">Base+MLFM ✗</cell><cell></cell><cell>78.41</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>82.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Effectiveness of Cyc. and Dense Cyc.. This table shows the effectiveness of vanilla cycle loss<ref type="bibr" target="#b6">[7]</ref> (Noted by Cyc.) and our proposed dense cycle (Noted by Dense Cyc.).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Cyc. Dense Cyc. Avg. Dice(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fusion-only ✗</cell><cell>✗</cell><cell>80.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fusion+Cyc.</cell><cell>✗</cell><cell>79.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GL-Fusion</cell><cell></cell><cell>82.29</cell></row><row><cell>Input</cell><cell>Ground Truth</cell><cell>DeeplabV3</cell><cell>Unet</cell><cell>CSS</cell><cell cols="3">Early Fusion Mid Fusion Late Fusion</cell><cell>Ours</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by the <rs type="funder">Beijing Institute of Collaborative Innovation (BICI)</rs> under Grant <rs type="grantNumber">HCIC-004</rs>, in collaboration with HKUST; the <rs type="funder">Foshan HKUST</rs> Projects under Grants <rs type="grantNumber">FSUST21-HKUST10E</rs> and <rs type="grantNumber">FSUST21-HKUST11E</rs>; and the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> under Project <rs type="grantNumber">ITS/030/21</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7xej3Me">
					<idno type="grant-number">HCIC-004</idno>
				</org>
				<org type="funding" xml:id="_kdeV946">
					<idno type="grant-number">FSUST21-HKUST10E</idno>
				</org>
				<org type="funding" xml:id="_3nRfBkC">
					<idno type="grant-number">FSUST21-HKUST11E</idno>
				</org>
				<org type="funding" xml:id="_t3xeNN7">
					<idno type="grant-number">ITS/030/21</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Longitudinal multiple sclerosis lesion segmentation using multi-view convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Birenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46976-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46976-8_7" />
	</analytic>
	<monogr>
		<title level="m">LABELS/DLMIA -2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10008</biblScope>
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated analysis of unregistered multi-view mammograms with deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2751523</idno>
		<ptr target="https://doi.org/10.1109/TMI.2017.2751523" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2355" to="2365" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivativebased search methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2011.2169273</idno>
		<ptr target="https://doi.org/10.1109/TIP.2011.2169273" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="968" to="982" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning shape priors for robust cardiac MR segmentation from multi-view images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tarroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_58" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrastive learning for echocardiographic view integration</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Der Geest</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_33" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cyclical self-supervision for semi-supervised ejection fraction prediction from echocardiogram videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-set based cross-supervision for video grounding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11573" to="11582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning-based image segmentation on multimodal medical imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="169" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Every pixel matters: center-aware feature alignment for domain adaptive object detector</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="733" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-7_42" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully automatic pediatric echocardiography segmentation using deep convolutional networks based on biSeNet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6561" to="6564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning for segmentation using an open large-scale dataset in 2D echocardiography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leclerc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2198" to="2210" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Right ventricular segmentation from shortand long-axis MRIs via information transition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-93722-5_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-93722-5_28" />
	</analytic>
	<monogr>
		<title level="m">STACOM 2021</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Puyol Antón</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13131</biblScope>
			<biblScope unit="page" from="259" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view spatial aggregation framework for joint localization and segmentation of organs at risk in head and neck CT images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.2975853</idno>
		<ptr target="https://doi.org/10.1109/TMI.2020.2975853" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2794" to="2805" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TransFusion: multi-view divergent fusion for medical image segmentation with transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_47" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Refined deep layer aggregation for multi-disease, multi-view &amp; multi-center cardiac MR segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Axel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STACOM 2021</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Puyol Antón</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13131</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-93722-5_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-93722-5_34" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Medical image fusion based on multi-scaling (drt) and multi-resolution (dwt) technique</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Communication and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="654" to="0657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Duo-SegNet: adversarial dual-views for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_40" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Realtime automatic assessment of cardiac function in echocardiography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Storve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Grue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">O</forename><surname>Haugen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Torp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ultrason. Ferroelectr. Freq. Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="358" to="368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Benchmarking framework for myocardial tracking and deformation algorithms: an open access database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tobon-Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="632" to="648" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated interpretation of congenital heart disease from multiview echocardiograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101942</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-view deep convolutional neural networks for lung nodule segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2017.8037182</idno>
		<ptr target="https://doi.org/10.1109/EMBC.2017.8037182" />
	</analytic>
	<monogr>
		<title level="m">2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1752" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision And Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmentation of fetal left ventricle in echocardiographic sequences based on dynamic convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1886" to="1895" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
