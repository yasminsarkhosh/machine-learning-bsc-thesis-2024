<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation</title>
				<funder ref="#_cn4RdXU">
					<orgName type="full">Key Research and Development Program of Shaanxi Province, China</orgName>
				</funder>
				<funder ref="#_5h8AnHw">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_MSv2ayH">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_KzQfgbe">
					<orgName type="full">Key Technologies Research and Development Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongsheng</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hengfei</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="89" to="99"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3C78D5B847D44501271E08BACF8636DB</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain generalization</term>
					<term>Domain randomization</term>
					<term>Medical image segmentation</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although recent years have witnessed the great success of convolutional neural networks (CNNs) in medical image segmentation, the domain shift issue caused by the highly variable image quality of medical images hinders the deployment of CNNs in real-world clinical applications. Domain generalization (DG) methods aim to address this issue by training a robust model on the source domain, which has a strong generalization ability. Previously, many DG methods based on feature-space domain randomization have been proposed, which, however, suffer from the limited and unordered search space of feature styles. In this paper, we propose a multi-source DG method called Treasure in Distribution (TriD), which constructs an unprecedented search space to obtain the model with strong robustness by randomly sampling from a uniform distribution. To learn the domain-invariant representations explicitly, we further devise a style-mixing strategy in our TriD, which mixes the feature styles by randomly mixing the augmented and original statistics along the channel wise and can be extended to other DG methods. Extensive experiments on two medical segmentation tasks with different modalities demonstrate that our TriD achieves superior generalization performance on unseen target-domain data. Code is available at https://github.com/Chen-Ziyang/TriD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is an essential task in computer-aided diagnosis. On this task, convolutional neural networks (CNNs) have demonstrated their effectiveness in an extensive literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. However, these CNN models obtained on training (i.e., source domain) data can hardly generalize well on the unseen test (i.e., target domain) data. The poor generalization ability, which hinders CNNs to be used in real-world clinical applications, can be attributed to the fact that the quality of medical images varies greatly across healthcare centers with different scanners and imaging protocols, resulting in large distribution discrepancy (a.k.a., domain shift). To improve the generalization ability, domain generalization (DG) methods have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. These methods can be trained on the data from one or multiple source domains. Considering the diversity of training data, we focus on multi-source DG in this study.</p><p>Most studies on DG attempt to alleviate the distribution discrepancy by standardizing the features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> and/or adding extra structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> to the network. However, the former suffers from over-standardization and may hinder the network to preserve semantic contents, while the latter may introduce excess misjudgment risk when estimating the distance between source-and target-domain data. A recent mainstream in DG research is to simulate the distributions of unseen target-domain data via domain randomization, i.e., perturbing the styles of source-domain data. The perturbation function can be defined in the input space <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>. Thus, it is easy to evaluate the quality of perturbed images, but the definition generally requires domain knowledge and expertise <ref type="bibr" target="#b2">[3]</ref>. By contrast, the perturbation can be performed in the feature space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. However, this may cause difficulties in monitoring the perturbation degree of semantic contents in the feature space due to the lack of visualization. Recently, two critical attributes of the feature space are revealed by IBN-Net <ref type="bibr" target="#b13">[14]</ref> and AdaIN <ref type="bibr" target="#b7">[8]</ref>, respectively. First, most style-texture information resides in the low-level features extracted by shallow layers. Second, the content-preserving style transformation can be performed by changing the statistics (e.g., mean and standard deviation) of the low-level features. Inspired by them, MixStyle <ref type="bibr" target="#b24">[25]</ref> perturbs the feature styles using augmented statistics, which are generated by randomly mixing the statistics of the low-level features from two samples. Subsequently, more research efforts have been devoted to designing the search space that covers a larger area in the feature-style space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. Despite their improved performance, using the statistics of source-domain data for feature perturbation may limit the search space and can hardly explore in the feature-style space evenly (see Fig. <ref type="figure" target="#fig_0">1(a),</ref><ref type="figure">(b</ref>) and (c)). The points indicating the augmented statistics are scattered and do not completely cover the points from unseen target domain. Moreover, since all feature channels are perturbed, these methods lack a reference to the original feature, which prevents them from learning the domain-invariant representations explicitly.</p><p>To address these issues, in this paper, we propose a simple but effective multisource DG method called Treasure in Distribution (TriD), which consists of two major steps: statistics randomization (SR) and style mixing (SM). SR aims to tap the potential of distribution by randomly sampling the augmented statistics from a uniform distribution to perturb the original intermediate features, which can expand the search space to cover more cases evenly (see Fig. <ref type="figure" target="#fig_0">1(d)</ref>). It can be observed that the red points are distributed evenly and cover not only the unseen target domain, but also the source domains. This leads to the issue that the perturbed features may have unreal styles. We hypothetically extend the unreal styles to the feature space with the inspiration from <ref type="bibr" target="#b16">[17]</ref>, which demonstrated the effectiveness of unreal styles in the input space. SM is devised to mix the feature styles by randomly mixing the augmented and original statistics in the channel dimension, thus making it feasible to learn the domain-invariant representations explicitly. We have evaluated our proposed TriD on two medical segmentation tasks: (1) the prostate segmentation using magnetic resonance imaging (MRI) from six domains and (2) joint segmentation of optic disc (OD) and optic cup (OC) in fundus images from five domains. Extensive experiments demonstrate that our TriD achieves a superior generalization ability to the state-of-the-art DG methods on unseen target-domain data.</p><p>Our contributions are three-fold: (1) The proposed multi-source DG method called TriD can boost the robustness of model and alleviate the performance drop on the unseen target-domain data. (2) We focus on expanding the search space of feature styles and therefore devise the statistics-randomization strategy, which allows exploring in the feature-style space evenly. (3) Different from perturbing all feature channels, we introduce the original statistics to the augmented statistics to learn the domain-invariant representations explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Let f ∈ R B×C×H×W be the intermediate features in a mini-batch, where B, C, H, and W respectively denote the mini-batch size, channel, height, and width. MixStyle <ref type="bibr" target="#b24">[25]</ref> perturbs the features by randomly mixing different feature statistics, formulated as follows:</p><formula xml:id="formula_0">MixStyle(f i ) = γ m f i -μ(f i ) σ(f i ) + β m , (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">γ m = λ m σ(f i ) + (1 -λ m )σ(f j ), β m = λ m μ(f i ) + (1 -λ m )μ(f j ),<label>(2)</label></formula><p>where λ m ∈ R B is a weight coefficient sampled from a Beta distribution <ref type="bibr" target="#b24">[25]</ref>,</p><formula xml:id="formula_3">f i , f j(j =i) ∈ R C×H×W indicate</formula><p>the features from two different images in a minibatch, and μ( * ), σ( * ) ∈ R B×C are the mean and standard deviation computed across the spatial dimension within each channel of each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Treasure in Distribution (TriD)</head><p>The TriD is designed to perturb the intermediate feature styles by randomly changing the feature statistics (i.e., mean and standard deviation), as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The feature statistics is substituted by the mixed statistics, which is generated by mixing the augmented and original statistics in channel dimension.</p><p>It can be implemented as a plug-and-play module inserted into any CNN-based architecture. In this study, we use ResNet-34 <ref type="bibr" target="#b4">[5]</ref> as the backbone to construct the segmentation network in a U-shape architecture <ref type="bibr" target="#b5">[6]</ref>. The TriD is inserted behind the first and second residual blocks during training, and will be removed in the inference phase. We now delve into the details of our TriD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics Randomization (SR).</head><p>Inspired by effectiveness of unreal styles in the input space <ref type="bibr" target="#b16">[17]</ref>, we hypothesis that unreal styles can also be extended to the feature space. To cover more cases evenly, we randomly sample the augmented statistics σ r , μ r ∈ R B×C from a uniform distribution which contains most feature statistics: σ r ∼ U (0, 1), μ r ∼ U (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Mixing (SM).</head><p>To learn the domain-invariant representations explicitly, SM strategy is designed to randomly mix the augmented and original statistics along the channel wise. We first sample P ∈ R B×C from the Beta distribution: P ∼ Beta(α, α), and use P as the probability to generate the Bernoulli distribution from which to sample λ ∈ R B×C : λ ∼ Bern(P ), where α is set to 0.1 empirically <ref type="bibr" target="#b24">[25]</ref>. Then the mixed statistics is calculated as:</p><formula xml:id="formula_4">γ mix = λσ(r) + (1 -λ)σ(f ), β mix = λμ(r) + (1 -λ)μ(f ),<label>(3)</label></formula><p>where f denotes the intermediate features. Finally, the mixed feature statistics is applied to perturb the normalized f similar to Eq. ( <ref type="formula" target="#formula_0">1</ref>),</p><formula xml:id="formula_5">T riD(f ) = γ mix f -μ(f ) σ(f ) + β mix . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Different from MixStyle, we replace the batch-wise fusion with channel-wise mixing, which avoids the sampling preference and introduces original-feature reference, so as to learn the domain-invariant representations explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training and Inference</head><p>Let D s = {(x di , y di ) N d i=1 } K d=1 be a set including K source domains, where x di is the i-th image in the d-th source domain, and y di is the corresponding segmentation mask of x di . Our goal is to train a segmentation model that can generalize well to an unseen target domain</p><formula xml:id="formula_7">D t = (x i ) Nt i=1 . Training.</formula><p>During training, we empirically set a probability of 0.5 to activate TriD in the forward pass <ref type="bibr" target="#b24">[25]</ref>. The segmentation network is trained on the source domains D s by using the combination of Dice loss (L Dice ) and cross-entropy loss (L ce ) as the objective:</p><formula xml:id="formula_8">L seg = L Dice + L ce .</formula><p>Inference. During inference, all the TriD modules are removed, and the segmentation network is tested on the unseen target domain D t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>Two datasets are used for this study, whose details are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The first dataset contains 116 MRI cases from six domains for prostate segmentation <ref type="bibr" target="#b9">[10]</ref>. We preprocess these MRI cases same as a previous study <ref type="bibr" target="#b6">[7]</ref> and only preserve the slices with the prostate region for consistent and objective segmentation evaluation. These slices are resized to 384×384 with same voxel spacing. On this dataset, we employ the Dice Similarity Coefficient (DSC) and Average Surface Distance (ASD) to evaluate the prostate segmentation. Note that we regard prostate segmentation as a 2D segmentation task, but calculate metrics on 3D volumes. The second dataset is a collection of two large and three small public datasets used for joint segmentation of optic disc (OD) and optic cup (OC) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>,  <ref type="bibr" target="#b5">[6]</ref>. We employ DSC to evaluate the joint segmentation of OD and OC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We set the mini-batch size to 8 and adopt the SGD optimizer with a momentum of 0.99 for both tasks. The initial learning rate l 0 is set to 0.01 (prostate) and 0.001 (OD/OC) respectively and decays according to the polynomial rule l t = l 0 × (1t/T ) 0.9 , where l t is the learning rate of the t-th epoch and T is the number of total epochs that is set to 200 for prostate segmentation and 100 for joint segmentation of OD and OC. For both tasks, the leave-one-domain-out strategy was used to evaluate the performance of each DG method, i.e., training on K-1 source domains and evaluating on the left domain. We consistently apply the above implementation settings to our TriD and other competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparing to Other DG Methods. We used the same segmentation network and loss function to compare our TriD with seven DG methods, including (1) DCAC: dynamic structure <ref type="bibr" target="#b6">[7]</ref>, (2) SAN-SAW: based on normalization  and whitening <ref type="bibr" target="#b15">[16]</ref>, (3) RandConv: input-space domain randomization <ref type="bibr" target="#b20">[21]</ref>, (4-6) MixStyle, EFDM, DSU: feature-space domain randomization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> and <ref type="bibr" target="#b6">(7)</ref> MaxStyle: adversarial noise <ref type="bibr" target="#b2">[3]</ref>. Note that since SAN-SAW requires the data with at least two classes, we did not provide its results for prostate segmentation. Besides, we also compared our method with another two settings, including the 'Intra-Domain' and 'DeepAll'. Under the 'Intra-Domain' setting, training and test data are from the same domain, where three-fold cross-validation is used for prostate segmentation due to the lack of data split. Under the 'DeepAll' setting, the model is directly trained on the data aggregated from all source domains and tested on the unseen target domain. The results are shown in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>. It can be observed that the overall performance of our TriD is not only superior to the 'DeepAll' baseline but also better than other DG approaches. Furthermore, we found that the performance ranking of MixStyle, EFDM, DSU and our TriD is T riD &gt; M ixStyle ≈ EF DM &gt; DSU , which is consistent with the ranking of search scope in Fig. <ref type="figure" target="#fig_0">1</ref>. It reveals that the unreal feature styles are indeed effective, and a larger search space is beneficial to boost the robustness.   Extendibility of SM. Under the same segmentation task, we further evaluate the extendibility of SM by combining it with other DG methods (i.e., MixStyle and EFDM), and the results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. It shows that each approach plus SM achieves better performance in most scenarios and has superior average DSC, proving that our SM strategy can be extended to other DG methods.</p><p>Location of TriD. To discuss where to apply our TriD, we repeated the experiments in joint segmentation of OD and OC and listed the results in Table <ref type="table" target="#tab_5">5</ref>. These four residual blocks of ResNet-34 are denoted as 'res1-4', and we trained different variants via applying TriD to different blocks. The results reveal that (1) the best performance is achieved by applying TriD to 'res12' that extract the low-level features with the most style-texture information; (2) the performance degrades when applying TriD to the third and last blocks that tend to capture semantic content rather than style texture <ref type="bibr" target="#b13">[14]</ref>.</p><p>Uniform Distribution vs. Normal Distribution. It is particularly critical to choose the distribution from which to randomly sample the augmented statistics.</p><p>To verify the advantages of uniform distribution, we repeated the experiments in joint segmentation of OD and OC by replacing the uniform distribution with a normal distribution N (0.5, 1) and compared the effectiveness of them in Table <ref type="table" target="#tab_6">6</ref>.</p><p>It shows that the normal distribution indeed results in performance drop due to the limited search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed the TriD, a domain-randomization based multi-source domain generalization method, for medical image segmentation. To solve the limitations existing in preview methods, TriD perturbs the intermediate features with two steps: (1) SR: randomly sampling the augmented statistics from a uniform distribution to expand the search space of feature styles; (2) SM: mixing the feature styles for explicit domain-invariant representation learning. Through extensive experiments on two medical segmentation tasks with different modalities, the proposed TriD is demonstrated to achieve superior performance over the baselines and other state-of-the-art DG methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of the statistics (1st row: standard deviation, 2nd row: mean) computed from the features of the first residual block of ResNet-34 trained on prostate dataset, which has six different domains. We take five as source domains and the left one as unseen target domain and visualize the augmented statistics produced by MixStyle, EFDM, DSU and our SR using 2D t-SNE. (a) MixStyle: Using a linear combination of different statistics. (b) EFDM: Using exact histogram matching and the statistics-fusion operation of MixStyle. (c) DSU: Sampling from a normal distribution constructed based on the original statistics. (d) TriD (Ours): Sampling from a uniform distribution.</figDesc><graphic coords="2,73,80,306,29,276,43,103,39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our TriD for feature perturbation.</figDesc><graphic coords="5,86,49,440,57,279,46,91,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of two DG methods with/without SM in joint segmentation of OD and OC. The value of each domain indicate the mean DSC of OD and OC.</figDesc><graphic coords="8,61,41,58,01,320,59,59,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of two datasets used for this study. Number of cases with '/' denotes the data split (training/test cases).</figDesc><table><row><cell>Task</cell><cell>Modality</cell><cell cols="2">Number of Domains Cases in Each Domain</cell></row><row><cell cols="2">Prostate Segmentation MRI</cell><cell>6</cell><cell>30; 30; 19; 13; 12; 12</cell></row><row><cell cols="3">OD/OC Segmentation Color Fundus Image 5</cell><cell>156/39; 76/19; 320/80; 500/150; 50/51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance (OD, OC) of Intra-Domain, DeepAll, our TriD and seven DG methods in joint segmentation of OD and OC. The best results except for the ones of Intra-Domain are highlighted with bold.</figDesc><table><row><cell>Methods</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Domain 3</cell><cell>Domain 4</cell><cell>Domain 5</cell><cell>Average</cell></row><row><cell></cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell></row><row><cell>Intra-Domain</cell><cell cols="6">(95.53, 82.53) (94.92, 83.94) (96.08, 86.30) (92.37, 85.44) (96.07, 86.44) 89.96</cell></row><row><cell>DeepAll</cell><cell cols="6">(92.87, 77.73) (91.33, 77.41) (91.45, 79.27) (83.51, 73.84) (90.82, 78.54) 83.68</cell></row><row><cell cols="7">SAN-SAW (CVPR 2022) [16] (93.34, 76.31) (92.88, 82.65) (90.78, 81.18) (88.07, 77.61) (93.43, 83.97) 86.02</cell></row><row><cell>DCAC (TMI 2022) [7]</cell><cell cols="6">(94.34, 76.72) (93.70, 79.21) (91.05, 81.23) (88.12, 77.87) (95.71, 85.32) 86.33</cell></row><row><cell cols="7">RandConv (ICLR 2021) [21] (93.11, 76.21) (92.50, 81.33) (89.01, 81.33) (88.33, 76.56) (95.32, 85.16) 85.89</cell></row><row><cell>MixStyle (ICLR 2021) [25]</cell><cell cols="6">(94.40, 79.11) (92.02, 79.19) (91.64, 80.79) (86.41, 76.44) (93.09, 83.35) 85.64</cell></row><row><cell>EFDM (CVPR 2022) [22]</cell><cell cols="6">(93.84, 77.04) (91.00, 79.53) (91.53, 81.59) (86.31, 76.39) (93.69, 81.31) 85.22</cell></row><row><cell>DSU (ICLR 2022) [9]</cell><cell cols="6">(93.71, 77.48) (91.79, 81.65) (92.11, 79.78) (87.96, 76.76) (93.58, 84.91) 85.97</cell></row><row><cell cols="7">MaxStyle (MICCAI 2022) [3] (94.57, 77.59) (93.67, 82.66) (92.40, 79.34) (88.81, 76.93) (96.02, 84.39) 86.64</cell></row><row><cell>TriD (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>(94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62 which can evaluate our TriD under different data-amount scenarios. Each of these five datasets has a training/test split, and in total, we have 1,102 cases for training and 339 cases for test. Each image is center-cropped and resized to 512 × 512</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of Intra-Domain, DeepAll, our TriD and six DG methods in prostate segmentation. The best results except for the ones of Intra-Domain are highlighted with bold. DSC↑ ASD ↓ DSC↑ ASD ↓ DSC↑ ASD ↓ DSC↑ ASD ↓ DSC↑ ASD ↓ DSC↑ ASD ↓</figDesc><table><row><cell>Methods</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Domain 3</cell><cell>Domain 4</cell><cell>Domain 5</cell><cell>Domain 6</cell><cell>Average</cell></row><row><cell cols="2">DSC↑ ASD ↓ Intra-Domain 93.24 0.59</cell><cell>91.85 0.59</cell><cell>90.52 1.57</cell><cell>89.69 0.81</cell><cell>88.19 1.29</cell><cell>91.09 0.69</cell><cell>90.76 0.93</cell></row><row><cell>DeepAll</cell><cell>90.72 1.04</cell><cell>88.53 0.77</cell><cell>85.10 3.30</cell><cell>88.04 0.91</cell><cell>85.84 1.98</cell><cell>89.01 0.81</cell><cell>87.87 1.47</cell></row><row><cell>DCAC (TMI 2022) [7]</cell><cell>90.51 0.98</cell><cell>88.18 1.34</cell><cell>84.35 4.05</cell><cell>88.32 0.83</cell><cell>87.01 2.73</cell><cell>89.95 0.64</cell><cell>88.05 1.76</cell></row><row><cell cols="2">RandConv (ICLR 2021) [21] 90.21 1.01</cell><cell>88.59 0.76</cell><cell>84.18 3.39</cell><cell>88.40 0.73</cell><cell>86.80 2.58</cell><cell>89.17 0.76</cell><cell>87.89 1.54</cell></row><row><cell>MixStyle (ICLR 2021) [25]</cell><cell>91.60 0.70</cell><cell>90.10 0.69</cell><cell>85.62 3.09</cell><cell>88.45 0.87</cell><cell>87.21 1.45</cell><cell>90.02 0.61</cell><cell>88.83 1.23</cell></row><row><cell>EFDM (CVPR 2022) [22]</cell><cell>91.57 0.72</cell><cell>90.18 0.70</cell><cell>85.34 3.30</cell><cell>89.25 0.71</cell><cell>86.82 1.71</cell><cell>89.52 0.71</cell><cell>88.78 1.31</cell></row><row><cell>DSU (ICLR 2022) [9]</cell><cell>90.92 0.79</cell><cell>88.19 0.82</cell><cell>84.57 3.86</cell><cell>88.68 0.73</cell><cell>86.25 1.80</cell><cell>89.13 0.70</cell><cell>87.96 1.45</cell></row><row><cell cols="2">MaxStyle (MICCAI 2022) [3] 90.33 0.79</cell><cell>89.17 0.74</cell><cell>85.34 2.91</cell><cell>88.72 0.67</cell><cell>87.46 1.38</cell><cell>88.15 0.72</cell><cell>88.19 1.20</cell></row><row><cell>TriD (Ours)</cell><cell>91.63 0.66</cell><cell>90.71 0.64</cell><cell>86.91 2.77</cell><cell>89.42 0.68</cell><cell>88.67 1.33</cell><cell>90.11 0.63</cell><cell>89.57 1.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance (OD, OC) of DeepAll, our TriD and its two variants in joint segmentation of OD and OC. The best results are highlighted with bold.</figDesc><table><row><cell>Methods</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Domain 3</cell><cell>Domain 4</cell><cell>Domain 5</cell><cell>Average</cell></row><row><cell></cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell></row><row><cell>DeepAll</cell><cell cols="6">(92.87, 77.73) (91.33, 77.41) (91.45, 79.27) (83.51, 73.84) (90.82, 78.54) 83.68</cell></row><row><cell>DeepAll+SR</cell><cell cols="6">(94.30, 79.23) (91.70, 81.15) (91.30, 81.10) (87.87, 76.96) (95.08, 85.77) 86.45</cell></row><row><cell>DeepAll+SR+Mixup</cell><cell cols="6">(94.68, 78.62) (92.08, 80.76) (91.06, 80.81) (87.18, 75.26) (94.58, 84.15) 85.92</cell></row></table><note><p>TriD (DeepAll+SR+SM) (94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Contribution of Each Component. To evaluate the contribution of statistics randomization (SR) and style mixing (SM), we chose the model trained in joint segmentation of OD and OC as an example and conducted a series of ablation experiments, as shown in Table4. Note that the 'Mixup' denotes the fusion strategy proposed in MixStyle. It shows that (1) introducing SR to baseline can lead to huge performance gains; (2) adding the Mixup operation will degrade the</figDesc><table><row><cell>82.00 84.00 86.00 88.00 90.00</cell><cell>86.76 87.64</cell><cell>85.61 87.43</cell><cell cols="4">81.43 82.45 88.22 88.86 85.99 86.22</cell><cell>85.64 86.47</cell><cell>82.00 84.00 86.00 88.00</cell><cell>85.44 86.43</cell><cell>85.27 87.00</cell><cell cols="2">86.56 86.55</cell><cell>81.35 83.49</cell><cell>87.50</cell><cell>85.22 87.28 86.15</cell></row><row><cell>80.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Domain</cell><cell>Domain</cell><cell cols="2">Domain</cell><cell>Domain</cell><cell>Domain</cell><cell>Average</cell><cell></cell><cell>Domain</cell><cell>Domain</cell><cell cols="2">Domain</cell><cell>Domain</cell><cell>Domain</cell><cell>Average</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell>5</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell cols="2">MixStyle</cell><cell cols="3">MixStyle+SM</cell><cell></cell><cell></cell><cell></cell><cell cols="2">EFDM</cell><cell>EFDM+SM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance (OD, OC) of different locations of TriD in joint segmentation of OD and OC. The best results are highlighted with bold.</figDesc><table><row><cell cols="2">Methods Domain 1</cell><cell>Domain 2</cell><cell>Domain 3</cell><cell>Domain 4</cell><cell>Domain 5</cell><cell>Average</cell></row><row><cell></cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell></row><row><cell>res1</cell><cell cols="6">(94.52, 78.12) (93.40, 82.92) (92.14, 79.94) (87.36, 77.22) (94.84, 85.20) 86.57</cell></row><row><cell>res2</cell><cell cols="6">(93.65, 79.20) (93.07, 81.16) (92.12, 78.91) (87.29, 77.12) (93.47, 84.15) 86.01</cell></row><row><cell>res12</cell><cell cols="6">(94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62</cell></row><row><cell>res123</cell><cell cols="6">(92.52, 75.11) (90.64, 80.40) (91.24, 80.06) (87.82, 77.42) (95.14, 84.18) 85.45</cell></row><row><cell cols="7">res1234 (90.14, 75.27) (90.35, 79.45) (91.59, 80.76) (86.77, 77.04) (94.21, 84.17) 84.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance (OD, OC) of using normal distribution and uniform distribution in joint segmentation of OD and OC. The best results are highlighted with bold.</figDesc><table><row><cell>Methods</cell><cell>Domain 1</cell><cell>Domain 2</cell><cell>Domain 3</cell><cell>Domain 4</cell><cell>Domain 5</cell><cell>Average</cell></row><row><cell></cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell><cell>DSC↑</cell></row><row><cell cols="7">Normal Distribution (92.93, 78.22) (91.37, 81.06) (91.64, 80.60) (84.66, 72.20) (93.55, 83.39) 84.96</cell></row><row><cell cols="7">Uniform Distribution (94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62</cell></row></table><note><p>robustness of model due to the limited search space; (3) the best performance is achieved when SR and SM are jointly used (i.e., our TriD).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62171377</rs>, in part by the <rs type="funder">Key Technologies Research and Development Program</rs> under Grant <rs type="grantNumber">2022YFC2009903/2022YFC2009900</rs>, in part by the <rs type="funder">Key Research and Development Program of Shaanxi Province, China</rs>, under Grant <rs type="grantNumber">2022GY-084</rs>, and in part by the <rs type="funder">China Postdoctoral Science Foundation</rs> <rs type="grantNumber">2021M703340/BX2021333</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5h8AnHw">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funding" xml:id="_KzQfgbe">
					<idno type="grant-number">2022YFC2009903/2022YFC2009900</idno>
				</org>
				<org type="funding" xml:id="_cn4RdXU">
					<idno type="grant-number">2022GY-084</idno>
				</org>
				<org type="funding" xml:id="_MSv2ayH">
					<idno type="grant-number">2021M703340/BX2021333</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retinal fundus images for glaucoma analysis: the RIGA dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almazroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging Informatics for Healthcare, Research, and Applications</title>
		<imprint>
			<biblScope unit="volume">10579</biblScope>
			<biblScope unit="page" from="55" to="62" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation of natural and medical images: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asgari Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="137" to="178" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MaxStyle: adversarial style composition for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_15" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RobustNet: improving domain generalization in urban-scene segmentation via instance selective whitening</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11580" to="11590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain specific convolution and high frequency reconstruction based unsupervised domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_62" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain and content adaptive convolution based multi-source domain generalization for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="244" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty modeling for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-9_46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="475" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric and textural augmentation for domain gap reduction</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14340" to="14350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction in the segmentation of medical images: a survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Olabarriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="142" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">REFUGE challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two at once: enhancing learning and generalization capacities via IBN-Net</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01225-0_29" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Switchable whitening for deep representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic-aware domain generalized segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2594" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global and local texture randomization for synthetic-to-real semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6594" to="6608" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization embeddings for deep domain generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Segu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page">109115</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Drishti-GS: retinal image dataset for optic nerve head (ONH) segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">U S</forename><surname>Tabish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: a survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8052" to="8072" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust and generalizable visual representation learning via random convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exact feature distribution matching for arbitrary style transfer and domain generalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8035" to="8045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ORIGA-light: an online retinal fundus image database for glaucoma analysis and research</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Annual International Conference of the IEEE Engineering in Medicine and Biology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3065" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain generalization: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4396" to="4415" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain generalization with MixStyle</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
