<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Kang</surname></persName>
							<idno type="ORCID">0000-0002-5256-0363</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Subang Jaya</addrLine>
									<country>Malaysia Campus, Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chee-Ming</forename><surname>Ting</surname></persName>
							<email>ting.cheeming@monash.edu</email>
							<idno type="ORCID">0000-0002-6037-3728</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Subang Jaya</addrLine>
									<country>Malaysia Campus, Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fung</forename><forename type="middle">Fung</forename><surname>Ting</surname></persName>
							<idno type="ORCID">0000-0001-9855-6020</idno>
						</author>
						<author>
							<persName><forename type="first">Null-</forename><forename type="middle">W</forename><surname>Phan</surname></persName>
							<idno type="ORCID">0000-0001-7448-4595</idno>
						</author>
						<title level="a" type="main">RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="600" to="610"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A3AF8725BA8F5615A829D98A1A8D0B86</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image detection</term>
					<term>YOLO</term>
					<term>Reparameterized convolution</term>
					<term>Channel shuffle</term>
					<term>Computation efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With an excellent balance between speed and accuracy, cutting-edge YOLO frameworks have become one of the most efficient algorithms for object detection. However, the performance of using YOLO networks is scarcely investigated in brain tumor detection. We propose a novel YOLO architecture with Reparameterized Convolution based on channel Shuffle (RCS-YOLO). We present RCS and a One-Shot Aggregation of RCS (RCS-OSA), which link feature cascade and computation efficiency to extract richer information and reduce time consumption. Experimental results on the brain tumor dataset Br35H show that the proposed model surpasses YOLOv6, YOLOv7, and YOLOv8 in speed and accuracy. Notably, compared with YOLOv7, the precision of RCS-YOLO improves by 1%, and the inference speed by 60% at 114.8 images detected per second (FPS). Our proposed RCS-YOLO achieves state-of-the-art performance on the brain tumor detection task. The code is available at https://github.com/mkang315/RCS-YOLO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic detection of brain tumors from Magnetic Resonance Imaging (MRI) is complex, tedious, and time-consuming because there are a lot of missed, misinterpreted, and misleading tumor-like lesions in the images of the brain tumors <ref type="bibr" target="#b7">[8]</ref>. Most of the current work focuses on brain tumor classification and segmentation from MRI and detection tasks are less explored <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>. While existing studies showed that various Convolutional Neural Networks (CNNs) are efficient for brain tumor detection, the performance of using You Only Look Once (YOLO) networks is scarcely investigated <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>With the rapid development of CNNs, the accuracies of different visual tasks are constantly improved. However, the increasingly complex network architecture in CNN-based models, such as ResNet <ref type="bibr" target="#b5">[6]</ref>, DenseNet <ref type="bibr" target="#b8">[9]</ref>, Inception <ref type="bibr" target="#b27">[28]</ref>, etc. renders the inference speed slower. Though many advanced CNNs deliver higher accuracy, the complicated multi-branch designs (e.g., residual-addition in ResNet and branch-concatenation in Inception) make the models difficult to implement and customize, slowing down the inference and reducing memory utilization. The depth-wise separable convolutions used in MobileNets <ref type="bibr" target="#b6">[7]</ref> also reduce the upper limit of the GPU inference speed. In addition, 3 × 3 regular convolution is highly optimized by some modern computing libraries. Consequently, VGG <ref type="bibr" target="#b25">[26]</ref> is still heavily used for real-world applications in both research and industries.</p><p>RepVGG <ref type="bibr" target="#b1">[2]</ref> is an extension of VGG via reparametrization to accelerate inference time. RepVGG uses a multi-branch topological architecture during the training phase, which is then reparameterized to a simplified single-branch architecture during the inference phase. In terms of the optimization strategy of network training, reparameterization was introduced in YOLOv6 <ref type="bibr" target="#b15">[16]</ref>, YOLOv7 <ref type="bibr" target="#b30">[31]</ref>, and YOLOv6 v3.0 <ref type="bibr" target="#b16">[17]</ref>. YOLOv6 and YOLOv6 v3.0 employ reparameterization from RepVGG. RepConv, a RepVGG without an identity connection, is converted from RepVGG during inference time in YOLOv6, YOLOv6 v3.0, and YOLOv7 (named RepConvN in YOLOv7). Due to the removal of identity connections in RepConv, direct access to ResNet or the concatenation in DenseNet can provide more diversity of gradients for different feature maps. Grouped convolutions, which use a group of convolutions with multiple kernels per layer, like RepVGG, can also significantly reduce the computational complexity of the model, but there is no information communication between groups, which limits the ability of feature extraction of the convolution operator. In order to overcome the disadvantage of grouped convolutions, ShuffleNet V1 <ref type="bibr" target="#b33">[34]</ref> and V2 <ref type="bibr" target="#b20">[21]</ref> introduced the channel shuffle operation to facilitate information flows across different feature channels. In addition, when comparing Spatial Pyramid Pooling &amp; Cross Stage Partial Network plus ConvBNSiLU (SPPCSPC) in YOLOv7 with Spatial Pyramid Pooling Fast (SPPF) in YOLOv5 <ref type="bibr" target="#b9">[10]</ref> and YOLOv8 <ref type="bibr" target="#b10">[11]</ref>, it is found that more convolution layers in SPPCSPC architecture slow down the computation of the network. Nevertheless, SPP <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The architecture of the proposed RCS-YOLO network is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. It incorporates a new module-RCS-OSA in the backbone and neck of the YOLObased object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RepVGG/RepConv ShuffleNet</head><p>Inspired by ShuffleNet, we design a structural reparameterized convolution based on channel shuffle. Figure <ref type="figure" target="#fig_1">2</ref> shows the structural schematic diagram of RCS.</p><p>Given that the feature dimensions of an input tensor are C × H × W , after the channel split operator, it is divided into two different channel-wise tensors with equal dimensions of C×H ×W . For one of the tensors, we use the identity branch, 1 × 1 convolution, and 3 × 3 convolution to construct the training-time RCS. At the inference stage, the identity branch, 1 × 1 convolution, and 3 × 3 convolution are transformed to 3 × 3 RepConv by using structural reparameterization. The multi-branch topology architecture can learn abundant information about features during the training time, simplified single-branch architecture can save memory consumption during the inference time to achieve fast inference. After the multi-branch training of one of the tensors, it is concatenated to the other tensor in a channel-wise manner. The channel shuffle operator is also applied to enhance information fusion between two tensors so that the depth measurement between different channel features of the input can be realized with low computational complexity. When there is no channel shuffle, the output feature of each group only relates to the input feature within a group of grouped convolutions, and outputs from a certain group only relate to the input within the group. This blocks information flow between channel groups and weakens the ability of feature extraction. When channel shuffle is used, input and output features are fully related where one convolution group takes data from other groups, enabling more efficient feature information communication between different groups. The channel shuffle operates on stacked grouped convolutions and allows more informative feature representation. Moreover, assuming that the number of groups is g, for the same input feature, the computational complexity of channel shuffle is 1 g times that of a generic convolution.</p><p>Compared with the popular 3 × 3 convolution, during the inference stage, RCS uses the operators including channel split and channel shuffle to reduce the computational complexity by a factor of 2, while keeping the inter-channel information exchange. Moreover, using structural reparameterization enables deep representation learning from input features during the training stage, and reduction of inference-time memory consumption to achieve fast inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RCS-Based One-Shot Aggregation</head><p>The One-Shot Aggregation (OSA) module has been proposed to overcome the inefficiency of dense connections in DenseNet, by representing diversified features with multi-receptive fields and aggregating all features only once in the last feature maps. VoVNet V1 <ref type="bibr" target="#b13">[14]</ref> and V2 <ref type="bibr" target="#b14">[15]</ref> used the OSA module within its architecture to construct both lightweight and large-scale object detectors, which outperform the widely-used ResNet backbone with faster speed and better energy efficiency.</p><p>We develop an RCS-OSA module by incorporating RCS developed in Sect. 2.1 for OSA, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The RCS modules are stacked repeatedly to ensure the reuse of features and to enhance the information flow among different channels between features of adjacent layers. At different locations of the network, we set a different number of stacked modules. To reduce the level of network fragmentation, only three feature cascades are maintained on the one-shot aggregate path, which can mitigate the amount of network calculation burden and reduce the memory footprint. In terms of multi-scale feature fusion, inspired by the idea of Path Aggregation Network (PANet) <ref type="bibr" target="#b18">[19]</ref>, RCS-OSA + Upsampling and RCS-OSA + RepVGG/RepConv undersampling carry out the alignment of feature maps of different sizes to allow information exchange between the two prediction feature layers. This enables high-accuracy fast inference in object detection. Moreover, RCS-OSA maintains the same number of input channels and minimum output channels, thus reducing the memory access cost (MAC). For network building, we perpetuate max-pooling undersampling 32 times of YOLOv7 to construct a backbone network and adopt RepVGG/RepConv with a step of 2 to achieve undersampling. Due to the diversified feature representation of the RCS-OSA module and low-cost memory consumption, we use a different number of stacked RCS in RCS-OSA modules to achieve semantic information extraction during different stages of both backbone and neck networks. The common evaluation metric of computation efficiency (or time complexity) is floating-point operations (FLOPs). FLOPs are only the indirect indicator to measure the speed of inference. However, the object detector with a DenseNet backbone shows rather slow speed and low energy efficiency because the linearly increasing number of channels by dense connection leads to heavy MAC, which causes considerable computation overhead. Given input features of dimension M × M , the convolution kernel of size K × K, number of input channels C 1 , and the number of output channels C 2 , FLOPs and MAC can be calculated as:</p><formula xml:id="formula_0">F LOP s = M 2 K 2 C 1 C 2 (1) MAC = M 2 (C 1 + C 2 ) + K 2 C 1 C 2<label>(2)</label></formula><p>Assuming n to be 4, FLOPs of the proposed RCS-OSA and Efficient Layer Aggregation Networks (ELAN) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33]</ref> are 20.25C 2 M 2 and 40C 2 M 2 respectively. Compared with ELAN, FLOPs of RCS-OSA are reduced by nearly 50%. The MAC of RCS-OSA (i.e., 6CM 2 + 20.25C 2 ) is also reduced compared to that of ELAN (i.e., 17CM 2 + 40C 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detection Head</head><p>To further reduce inference time, we decrease the number of detection heads comprised of RepVGG and IDetect from 3 to 2. The YOLOv5, YOLOv6, YOLOv7, and YOLOv8 have three detection heads. However, we use only two feature layers for prediction, reducing the number of original nine anchors with different scales to four and using the K-means unsupervised clustering method to regenerate anchors with different scales. The corresponding scales are (87, 90), (127, 139), (154, 171), <ref type="bibr">(191,</ref><ref type="bibr">240)</ref>. This not only reduces the number of convolution layers and computational complexity of RCS-YOLO but also reduces the overall computational requirements of the network during the inference stage and the computational time of postprocessing non-maximum suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Details</head><p>To evaluate the proposed RCS-YOLO model, we used the brain tumor detection 2020 dataset (Br35H) <ref type="bibr" target="#b2">[3]</ref>, with a total of 701 images in the 'train' and 'val' two folders, 500 images of which are the 'train' folder were selected as the training set, while the other 201 images in the 'val' folder as the testing set. For the input size of 640×640 image, the actual corresponding size is 44×32. The small object is defined as the object whose pixel size is less than 32 × 32 defined by the MS COCO dataset <ref type="bibr" target="#b17">[18]</ref>, so there are no small objects in the brain tumor medical image data sets, and the scale change of the target boxes is smooth, almost square. The label boxes of the brain images were normalized (See supplementary material Sect. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>For model training and inference, we used Ubuntu 18.04 LTS, Intel R Xeon R Gold 5218 CPU processor, CUDA 12.0, and cuDNN 8.2. GPU is GeForce RTX 3090 with 24G memory size. The networking development framework is Pytorch 1.9.1. The Integrated Development Environment (IDE) is PyCharm. We uniformly set epoch 150, the batch size as 8, image size as 640 × 640. Stochastic Gradient Descent (SGD) optimizer was used with an initial learning rate of 0.01 and weight decay of 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Metrics</head><p>In this paper, we choose precision, recall, AP 50 , AP 50:95 , FLOPs, and Frames Per Second (FPS) as comparative metrics of detection effect to determine the advantages and disadvantages of the model. Taking IoU = 0.5 as the standard, precision, and recall can be calculated by the following equations:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P recision = T P T P + F P</head><p>(3)</p><formula xml:id="formula_1">Recall = T P (T P + P N) (<label>4</label></formula><formula xml:id="formula_2">)</formula><p>where T P represents the number of positive samples correctly identified as positive samples, F P represents the number of negative samples incorrectly identified as positive samples and F N represents the number of positive samples incorrectly identified as negative samples. AP 50 is the area under the precision-recall (PR) curve formed by precision and recall. For AP 50:95 , divide 10 IoU threshold of 0.5:0.05:0.95 to acquire the area under the PR curve, then average the results. FPS represents the number of images detected by the model per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>To highlight the accuracy and rapidity of the proposed model for the detection of brain tumor medical image data set, Table <ref type="table" target="#tab_1">1</ref> shows the performance comparison between our proposed detector and other state-of-the-art object detectors. The time duration of FPS includes data preprocessing, forward model inference, and post-processing. The long border of the input images is set as 640 pixels. The short border adaptively scales without distortion, whilst keeping the grey filling with 32 times the pixels of the short border.</p><p>It can be seen that RCS-YOLO with the advantages of incorporating the RCS-OSA module performs well. Compared with YOLOv7, the FLOPs of the object detectors of this paper decrease by 8.8G, and the inference speed improves by 43.4 FPS. In terms of detection rate, precision improves by 0.024; AP 50 increases by 0.01; AP 50:95 by 0.006. Also, RCS-YOLO is faster and more accurate than YOLOv6-L v3.0 and YOLOv8l. Although the AP 50:95 of RCS-YOLO equals that of YOLOv8l, it doesn't obscure the essential advantage of RCS-YOLO. The results clearly show the superior performance and efficiency of our method, compared to the state-of-the-art for brain tumor detection. As shown in supplementary material Fig. <ref type="figure" target="#fig_1">2</ref>, brain tumor regions are accurately detected from MRI by using the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We demonstrate the effectiveness of the proposed RCS-OSA module in YOLObased object detectors. The results of RepVGG-CSP in Table <ref type="table" target="#tab_2">2</ref>, where RCS-OSA in the RCS-YOLO is replaced with the Cross Stage Partial Network (CSP-Net) <ref type="bibr" target="#b31">[32]</ref> used in existing YOLOv4-CSP <ref type="bibr" target="#b29">[30]</ref> architecture, are decreased than RCS-YOLO except GFLOPs. Because the parameters of RepVGG-CSP (22.2M) are less than half those of RCS-YOLO (45.7M), the computation amount (i.e., GFLOPs) of RepVGG-CSP is accordingly smaller than RCS-YOLO. Nevertheless, RCS-YOLO still performs better in actual inference speed measured by FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We developed an RCS-YOLO network for fast and accurate medical object detection, by leveraging the reparameterized convolution operator RCS based on channel shuffle in the YOLO architecture. We designed an efficient one-shot aggregation module RCS-OSA based on RCS, which serves as a computational unit in the backbone and neck of a new YOLO network. Evaluation of the brain MRI dataset shows superior performance for brain tumor detection in terms of both speed and precision, as compared to YOLOv6, YOLOv7, and YOLOv8 models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of RCS-YOLO. The architecture of RCS-YOLO is mainly comprised of the RCS-OSA (blue) and RepVGG (orange) modules. n represents the number of stacked RCS modules. n cls represents the number of classes in detected objects. IDetect [29] from YOLOv7 denotes detection layers using 2D convolutional neural networks. (Color figure online)</figDesc><graphic coords="3,65,31,153,08,294,04,248,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The structure of RCS. (a) RepVGG at the training stage. (b) RepConv during model inference (or deployment). A rectangle with a black outer border represents the specific modular operation of the tensor; a rectangle with a gradient color represents the specific feature of the tensor, and the width of the rectangle represents the channel of the tensor.</figDesc><graphic coords="4,106,98,243,86,238,99,235,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The structure of RCS-OSA. n represents the number of stacked RCS modules.</figDesc><graphic coords="6,136,98,54,41,178,84,185,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This leads to fast propagation of accurate localization information to feature hierarchy in both the backbone and neck networks.3) We apply the proposed RCS-YOLO model for a challenging task of brain tumor detection. To our best knowledge, this is the first work to leverage on</figDesc><table><row><cell>module</cell></row><row><cell>achieves the fusion of local features and global features by max-pooling different</cell></row><row><cell>convolution kernels' sizes in the neck networks.</cell></row><row><cell>Aiming at a faster and high accuracy object detector for medical images,</cell></row><row><cell>we propose a new YOLO architecture called RCS-YOLO by leveraging on the</cell></row><row><cell>RepVGG/RepConv. The contributions of this work are summarized as follows:</cell></row><row><cell>1) We first develop a RepVGG/RepConv ShuffleNet (RCS) by combining the</cell></row><row><cell>RepVGG/RepConv with a ShuffleNet which benefits from reparameteriza-</cell></row><row><cell>tion to provide more feature information in the training stage and reduce</cell></row><row><cell>inference time. Then, we build an RCS-based One-Shot Aggregation (RCS-</cell></row><row><cell>OSA) module which allows not only low-cost memory consumption but also</cell></row><row><cell>semantic information extraction.</cell></row><row><cell>2) We design new backbone and neck networks of YOLO architecture by incor-</cell></row><row><cell>porating the developed RCS-OSA and RepVGG/RepConv with path aggre-</cell></row><row><cell>gation to shorten the information path between feature prediction layers.</cell></row></table><note><p>YOLO-based model for fast brain tumor detection. Evaluation on a publicly available brain tumor detection annotated dataset shows superior detection accuracy and speed compared to other state-of-the-art YOLO architectures.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of different methods. The best results are shown in bold.</figDesc><table><row><cell>Model</cell><cell cols="4">Params Precision Recall AP50 AP50:95 GFLOPs FPS</cell></row><row><cell>YOLOv6-L [17]</cell><cell>59.6M 0.907</cell><cell>0.920 0.929 0.709</cell><cell>150.5</cell><cell>64.0</cell></row><row><cell>YOLOv7 [31]</cell><cell>36.9M 0.912</cell><cell>0.925 0.936 0.723</cell><cell>103.3</cell><cell>71.4</cell></row><row><cell>YOLOv8l [11]</cell><cell>43.9M 0.934</cell><cell cols="2">0.920 0.944 0.729 164.8</cell><cell>76.2</cell></row><row><cell cols="2">RCS-YOLO (Ours) 45.7M 0.936</cell><cell cols="2">0.945 0.946 0.729 94.5</cell><cell>114.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on proposed RCS-OSA module. The best results are shown in bold.</figDesc><table><row><cell>Model</cell><cell cols="4">Params Precision Recall AP50 AP50:95 GFLOPs FPS</cell></row><row><cell cols="2">RepVGG-CSP (w/o RCS-OSA) 22.6M 0.926</cell><cell>0.930 0.933 0.689</cell><cell>43.3</cell><cell>6.1</cell></row><row><cell cols="2">RCS-YOLO (w/ RCS-OSA) 45.7M 0.936</cell><cell cols="2">0.945 0.946 0.729 94.5</cell><cell>114.8</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_57.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brain tumor detection and classification using machine learning: a comprehensive survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haldorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3161" to="3183" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RepVGG: making VGGstyle ConvNets great again</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Br35H : : brain tumor detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hamada</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/ahmedhamada0/brain-tumor-detection" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>Kaggle</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10578-9_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10578-9_23" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8691</biblScope>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tumor-like lesions of the brain</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Huisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="10" to="S13" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Special issue A</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">DenseNet: implementing efficient ConvNet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">YOLOv5 (6.0/6.1) brief summary</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/yolov5/issues/6998" />
	</analytic>
	<monogr>
		<title level="j">GitHub</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/ultralytics" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>YOLO by Ultralytics</publisher>
		</imprint>
	</monogr>
	<note>version 8.0.0). GitHub</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Brain lesion detection and analysis -a review</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G K</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 Fifth International Conference on I-SMAC (IoT in Social. Mobile, Analytics and Cloud) (I-SMAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigating brain tumor segmentation and detection techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An energy and GPU-computation efficient backbone network for real-time object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="752" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CenterMask: real-time anchor-free instance segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="13903" to="13912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">YOLOv6: a single-stage object detection framework for industrial applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.02976</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05586</idno>
		<title level="m">YOLOv6 v3.0: a full-scale reloading</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Brain tumor detection using machine learning and deep learning: a review</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lotlikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satpute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="604" to="622" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: practical guidelines for efficient CNN architecture design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01264-9_8" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Role of deep learning in brain tumor detection and classification (2015 to 2020): a review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khurshid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">101940</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey of medical image analysis using deep learning approaches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 2nd International Conference on Smart Electronics and Communication (ICOSEC)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1115" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brain tumor detection using supervised learning: a survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shanishchara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1159" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on brain tumor detection using machine learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shirwaikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hiremath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Forensics. Analytics, Big Data, Security (FABS)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations (ICLR</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Survey on brain tumor detection using machine learning and deep learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sravya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Computer Communication and Informatics (ICCCI)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/WongKinYiu/yolov7/blob/main/cfg/training/yolov7.yaml" />
		<title level="m">Yolov7.yaml. GitHub</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/WongKinYiu/yolov7/blob/main/cfg/baseline/yolov4-csp.yaml" />
		<editor>yaml. GitHub</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02696</idno>
		<title level="m">YOLOv7: trainable bag-offreebies sets new state-of-the-art for real-time object detectors</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CSPNet: a new backbone that can enhance learning capability of CNN</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1571" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Yeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.04800</idno>
		<title level="m">Designing network design strategies through gradient path analysis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ShuffleNet: an extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
