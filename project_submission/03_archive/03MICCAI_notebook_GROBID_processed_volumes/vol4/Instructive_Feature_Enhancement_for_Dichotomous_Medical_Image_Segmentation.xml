<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instructive Feature Enhancement for Dichotomous Medical Image Segmentation</title>
				<funder ref="#_ZvuCjjY">
					<orgName type="full">Warwick-QU</orgName>
				</funder>
				<funder ref="#_4xUZhEN">
					<orgName type="full">Shenzhen Science and Technology Innovations Committee</orgName>
				</funder>
				<funder ref="#_8VWsMRK #_aYvbguA">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_gDaCFVh #_PSr43Gt #_n7nQ22c #_D93asJS #_y3yPaH7 #_Q3jNH5J #_8jpjDCq #_2XGumwj">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_2XpHWh3">
					<orgName type="full">Shenzhen-Hong Kong Joint Research Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiongquan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sijing</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenlong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen RayShape Medical Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Computer Vision Lab (CVL)</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<email>xinyang@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instructive Feature Enhancement for Dichotomous Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="437" to="447"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5ABE78227E0FB1261CE0C57D067220C6</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have been widely applied in dichotomous medical image segmentation (DMIS) of many anatomical structures in several modalities, achieving promising performance. However, existing networks tend to struggle with task-specific, heavy and complex designs to improve accuracy. They made little instructions to which feature channels would be more beneficial for segmentation, and that may be why the performance and universality of these segmentation models are hindered. In this study, we propose an instructive feature enhancement approach, namely IFE, to adaptively select feature channels with rich texture cues and strong discriminability to enhance raw features based on local curvature or global information entropy criteria. Being plug-and-play and applicable for diverse DMIS tasks, IFE encourages the model to focus on texture-rich features which are especially important for the ambiguous and challenging boundary identification, simultaneously achieving simplicity, universality, and certain interpretability. To evaluate the proposed IFE, we constructed the first large-scale DMIS dataset Cosmos55k, which contains 55,023 images from 7 modalities and 26 anatomical structures. Extensive experiments show that IFE can improve the performance of classic segmentation networks across different anatomies and modalities with only slight modifications. Code is available at https://github.com/yezi-66/IFE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>computer-aided diagnosis. With the rapid development of intelligent medicine, MIS involves an increasing number of imaging modalities, raising requirements for the accuracy and universality of deep models.</p><p>Medical images have diverse modalities owing to different imaging methods. Images from the same modality but various sites can exhibit high similarity in overall structure but diversity in details and textures. Models trained on specific modalities or anatomical structure datasets may not adapt to new datasets. Similar to dichotomous image segmentation tasks <ref type="bibr" target="#b26">[26]</ref>, MIS tasks typically input an image and output a binary mask of the object, which primarily relies on the dataset. To facilitate such dichotomous medical image segmentation (DMIS) task, we constructed Cosmos55k, a large-scale dataset of 55,023 challenging medical images covering 26 anatomical structures and 7 modalities (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Most current MIS architectures are carefully designed. Some increased the depth or width of the backbone network, such as UNet++ <ref type="bibr" target="#b37">[36]</ref>, which uses nested and dense skip connections, or DeepLabV3+ <ref type="bibr" target="#b9">[9]</ref>, which combines dilated convolutions and feature pyramid pooling with an effective decoder module. Others have created functional modules such as Inception and its variants <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">31]</ref>, depthwise separable convolution <ref type="bibr" target="#b17">[17]</ref>, attention mechanism <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b34">33]</ref>, and multi-scale feature fusion <ref type="bibr" target="#b8">[8]</ref>. Although these modules are promising and can be used flexibly, they typically require repeated and handcrafted adaptation for diverse DMIS tasks. Alternatively, frameworks like nnUNet <ref type="bibr" target="#b18">[18]</ref> developed an adaptive segmentation pipeline for multiple DMIS tasks by integrating key dataset attributes and achieved state-of-the-art. However, heavy design efforts are needed for nnUNet and the cost is very expensive. More importantly, previous DMIS networks often ignored the importance of identifying and enhancing the determining and instructive feature channels for segmentation, which potentially limits their performance in general DMIS tasks.</p><p>We observed that the texture-rich and sharp-edge cues in specific feature channels are crucial and instructive for accurately segmenting objects. Curvature <ref type="bibr" target="#b15">[15]</ref> can represent the edge characteristics in images. Information entropy <ref type="bibr" target="#b0">[1]</ref> can describe the texture and content complexity of images. In this study, we focus on exploring their roles in quantifying feature significance. Based on curvature and information entropy, we propose a simple approach to balance accuracy and universality with only minor modifications of the classic networks. Our contribution is three folds. First, we propose the novel 2D DMIS task and construct a large-scale dataset (Cosmos55k ) to benchmark the goal and we will release the dataset to contribute to the community. Second, we propose a simple, generalizable, and effective instructive feature enhancement approach (IFE ). With extensive experiments on Cosmos55k, IFE soundly proves its advantages in promoting various segmentation networks and tasks. Finally, we provide an interpretation of which feature channels are more beneficial to the final segmentation results. We believe IFE will benefit various DMIS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Overview. Figure <ref type="figure" target="#fig_1">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref> illustrate two choices of the proposed IFE. It is general for different DMIS tasks. 1) We introduce a feature quantization method based on either curvature (Fig. <ref type="figure" target="#fig_1">2</ref>) or information entropy (Fig. <ref type="figure" target="#fig_2">3</ref>), which characterizes the content abundance of each feature channel. The larger these parameters, the richer the texture and detail of the corresponding channel feature. 2) We select a certain proportion of channel features with high curvature or information entropy and combine them with raw features. IFE improves performance with minor modifications to the segmentation network architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Curvature-Based Feature Selection</head><p>For a two-dimensional surface embedded in Euclidean space R 3 , two curvatures exist: Gaussian curvature and mean curvature. Compared with the Gaussian curvature, the mean curvature can better reflect the unevenness of the surface. Gong <ref type="bibr" target="#b15">[15]</ref> proposed a calculation formula that only requires a simple linear convolution to obtain an approximate mean curvature, as shown below:</p><formula xml:id="formula_0">C = C 1 C 2 C 3 * X,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">C 1 = [α, β, α] T , C 2 = [β, γ, β] T , C 3 = [α, β, α]</formula><p>T , the values of α, β, and γ are -1/16, 5/16, -1. * denotes convolution, X represents the input image, and C is the mean curvature. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the process, showing that the curvature image can effectively highlight the edge details in the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Entropy-Based Feature Selection</head><p>As a statistical form of feature, information entropy <ref type="bibr" target="#b0">[1]</ref> reflects the spatial and aggregation characteristics of the intensity distribution. It can be formulated as:</p><formula xml:id="formula_2">E = - 255 i=0 255 j=0 P i,j log 2 (P i,j ) , P i,j = f (i n , j n ) / (H × W ) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>i n denotes the gray value of the center pixel in the n th 3 × 3 sliding window and j n denotes the average gray value of the remaining pixels in that window (teal blue box in Fig. <ref type="figure" target="#fig_2">3</ref>). The probability of (i n , j n ) occurring in the entire image is denoted by P i,j , and E represents the information entropy.</p><p>Each pixel on images corresponds to a gray or color value ranging from 0 to 255. However, each element in the feature map represents the activation level of the convolution filter at a particular position in the input image. Given an input feature F x , as shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the tuples (i, j) obtained by the sliding windows are transformed to a histogram, representing the magnitude of the activation level and distribution within the neighborhood. This involves rearranging the activation levels of the feature map. The histogram converting method histc and information entropy E are presented in Algorithm 1. Note that the probability P hist(i,j ) will be used to calculate the information entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Instructive Feature Enhancement</head><p>Although IFE can be applied to various deep neural networks, this study mainly focuses on widely used segmentation networks. Figure <ref type="figure" target="#fig_3">4</ref> shows the framework of IFE embedded in representative networks, e.g., DeepLabV3+ <ref type="bibr" target="#b9">[9]</ref>, UNet <ref type="bibr" target="#b29">[28]</ref>, nnUNet <ref type="bibr" target="#b18">[18]</ref>, and SINetV2 <ref type="bibr" target="#b14">[14]</ref>. The first three are classic segmentation networks. Because the MIS task is similar to the camouflage object detection, such as low contrast and blurred edges, we also consider SINetV2 <ref type="bibr" target="#b14">[14]</ref>. According to <ref type="bibr" target="#b28">[27]</ref>, we implement IFE on the middle layers of UNet <ref type="bibr" target="#b29">[28]</ref> and nnUNet <ref type="bibr" target="#b18">[18]</ref>, on the lowlevel features of DeepLabV3+ <ref type="bibr" target="#b9">[9]</ref>, and on the output of the TEM of SINetV2 <ref type="bibr" target="#b14">[14]</ref>.</p><p>While the input images are encoded into the feature space, the different channel features retain textures in various directions and frequencies. Notably, the information contained by the same channel may vary across different images, which can be seen in Fig. <ref type="figure" target="#fig_4">5</ref>. For instance, the 15 th channel of the lung CT feature map contains valuable texture and details, while the same channel in the aortic CT feature map may not provide significant informative content. However, Algorithm 1. Information entropy of features with histogram.</p><formula xml:id="formula_4">Input:Fx ∈ R C×H×W , bins = 256, kernel_size = 3 Output:E ∈ R C z = unf old (F , kernel_size) Sliding window operation. i = f latten (z) [(H × W ) //2] j = (sum (z) -i) /((H × W ) -1) f hist(i,j) = histc ((i, j) , bins)</formula><p>Compute the histogram of (i, j). ext_k = kernel_size//2 P h i s t (i , j ) = f hist(i,j) / ((H + ext k ) × (W + ext k )) E = sum -P h i s t (i , j ) × log2 P h i s t (i , j ) their 2 nd channel features both focus on the edge details. By preserving the raw features, the channel features that contribute more to the segmentation of the current object can be enhanced by dynamically selecting from the input features. Naturally, it is possible to explicitly increase the sensitivity of the model to the channel information. Specifically, for input image X, the deep feature</p><formula xml:id="formula_5">F x = [f 1 , f 2 , f 3 , • • • , f C ] ∈ R C * H * W</formula><p>can be obtained by an encoder with the weights θ x : F x = Encoder (X, θ x ), our IFE can be expressed as:</p><formula xml:id="formula_6">F x = max{S (F x ) , r}, (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>F x is the selected feature map and S is the quantification method (see Fig. <ref type="figure" target="#fig_1">2</ref> or Fig. <ref type="figure" target="#fig_2">3</ref>), and r is the selected proportion. As discussed in <ref type="bibr" target="#b5">[6]</ref>, enhancing the raw features through pixel-wise addition may introduce unwanted background noise and cause interference. In contrast, the concatenate operation directly joins the features, allowing the network to learn how to fuse them automatically, reducing the interference caused by useless background noises. Therefore, we used the concatenation and employed the concatenated features F = [F x , F x ] as the input to the next stage of the network. Only the initialization channel number of the corresponding network layer needs to be modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Cosmos55k. To construct the large-scale Cosmos55k, 30 publicly available datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b36">35]</ref> were collected and processed with organizers' permission. The processing procedures included uniform conversion to PNG format, cropping, and removing mislabeled images. Cosmos55k (Fig. <ref type="figure" target="#fig_0">1</ref>) offers 7 imaging modalities, including CT, MRI, X-ray, fundus, etc., covering 26 anatomical structures such as the liver, polyp, melanoma, and vertebra, among others. Images contain just one labeled object, reducing confusion from multiple objects with different structures.</p><p>Implementation Details. Cosmos55k comprises 55,023 images, with 31,548 images used for training, 5,884 for validation, and 17,591 for testing. We conducted experiments using Pytorch for UNet <ref type="bibr" target="#b29">[28]</ref>, DeeplabV3+ <ref type="bibr" target="#b9">[9]</ref>, SINetV2 <ref type="bibr" target="#b14">[14]</ref>, and nnUNet <ref type="bibr" target="#b18">[18]</ref>. The experiments were conducted for 100 epochs on an RTX 3090 GPU. The batch sizes for the first three networks were 32, 64, and 64, respectively, and the optimizer used was Adam with an initial learning rate of 10 -4 . Every 50 epochs, the learning rate decayed to 1/10 of the former. Considering the large scale span of the images in Cosmos55k, the images were randomly resized to one of seven sizes (224, 256, 288, 320, 352, or 384) before being fed into the network for training. During testing, the images were resized to a fixed size of 224. Notably, the model set the hyperparameters for nnUNet <ref type="bibr" target="#b18">[18]</ref> automatically.  Quantitative and Qualitative Analysis. To demonstrate the efficacy of IFE, we employ the following metrics: Conformity (Con) <ref type="bibr" target="#b6">[7]</ref>, Dice Similarity Coefficient (DSC) <ref type="bibr" target="#b4">[5]</ref>, Jaccard Distance (JC) <ref type="bibr" target="#b12">[12]</ref>, F1 <ref type="bibr" target="#b1">[2]</ref>, Human Correction Efforts (HCE) <ref type="bibr" target="#b26">[26]</ref>, Mean Absolute Error (MAE) <ref type="bibr" target="#b25">[25]</ref>, Hausdorff Distance (HD) <ref type="bibr" target="#b35">[34]</ref>, Average Symmetric Surface Distance (ASD) <ref type="bibr" target="#b13">[13]</ref>, Relative Volume Difference(RVD) <ref type="bibr" target="#b13">[13]</ref>. The quantitative results for UNet <ref type="bibr" target="#b29">[28]</ref>, DeeplabV3+ <ref type="bibr" target="#b9">[9]</ref>, SINetV2 <ref type="bibr" target="#b14">[14]</ref>, and nnUNet <ref type="bibr" target="#b18">[18]</ref> are presented in Table <ref type="table" target="#tab_0">1</ref>. From the table, it can be concluded that IFE can improve the performance of networks on most segmentation metrics. Besides, Fig. <ref type="figure">6</ref> shows that IFE helps models perform better in most modalities and anatomical structures. Figure <ref type="figure">7</ref> presents a qualitative comparison. IFE aids in locating structures in an object that may be difficult to notice and enhances sensitivity to edge gray variations. IFE can substantially improve the segmentation accuracy of the base model in challenging scenes.</p><p>Ablation Studies. Choosing a suitable selection ratio r is crucial when applying IFE to different networks. Different networks' encoders are not equally capable of extracting features, and the ratio of channel features more favorable to the segmentation result varies. To analyze the effect of r, we conducted experiments using UNet <ref type="bibr" target="#b29">[28]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>, either too large or too small r will lead to a decline in the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In order to benchmark the general DMIS, we build a large-scale dataset called Cosmos55k. To balance universality and accuracy, we proposed an approach (IFE) that can select instructive feature channels to further improve the segmentation over strong baselines against challenging tasks. Experiments showed that IFE can improve the performance of classic models with slight modifications in the network. It is simple, universal, and effective. Future research will focus on extending this approach to 3D tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Statistics, modalities, and examples of anatomical structures in Cosmos55k.</figDesc><graphic coords="2,47,31,54,35,329,14,213,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of feature selection using curvature.</figDesc><graphic coords="3,89,97,431,66,272,17,126,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of feature selection using 2D information entropy. (Color figure online)</figDesc><graphic coords="4,76,29,53,72,271,96,130,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Implementation of IFE in exemplar networks. c is concatenation.</figDesc><graphic coords="5,61,98,54,05,328,84,113,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Feature maps visualization of the lung (first row) and aortic (second row) CT images from stage 3 of SINetV2. 2, 15, and 17 are the indexes of channels. The information contained by the same channel may vary across different images.</figDesc><graphic coords="6,56,31,54,62,311,08,127,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. DSC of UNet (blue) and UNet+E (pink) in modalities (a) and anatomic structures (b). Hist, Colo, and Derm are Histopathology, Colonoscopy, and Dermoscopy. (Color figure online)</figDesc><graphic coords="8,74,31,178,28,275,53,177,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison on IFE. +C, +E means curvature-or information entropy-based IFE. DLV3+ is DeepLabV3+. Bolded means the best group result. * denotes that DSC passed t-test, p &lt; 0.05.</figDesc><table><row><cell cols="7">Method Con(%)↑ DSC (%)↑ JC (% )↑ F1 (%)↑ HCE ↓ MAE (%)↓ HD↓</cell><cell>ASD↓ RVD ↓</cell></row><row><cell>UNet</cell><cell>84.410</cell><cell>93.695</cell><cell>88.690</cell><cell cols="2">94.534 1.988 1.338</cell><cell cols="2">11.464 2.287 7.145</cell></row><row><cell>+C</cell><cell>85.666</cell><cell>94.003  *</cell><cell>89.154</cell><cell>94.528</cell><cell>1.777 1.449</cell><cell cols="2">11.213 2.222 6.658</cell></row><row><cell>+E</cell><cell cols="2">86.664 94.233  *</cell><cell>89.526</cell><cell>94.466</cell><cell>1.610 1.587</cell><cell cols="2">11.229 2.177 6.394</cell></row><row><cell cols="3">nnUNet -53.979 92.617</cell><cell>87.267</cell><cell>92.336</cell><cell>2.548 0.257</cell><cell cols="2">19.963 3.698 9.003</cell></row><row><cell>+C</cell><cell cols="2">-30.908 92.686</cell><cell>87.361</cell><cell cols="2">92.399 2.521 0.257</cell><cell cols="2">19.840 3.615 8.919</cell></row><row><cell>+E</cell><cell cols="2">-34.750 92.641</cell><cell>87.313</cell><cell>92.367</cell><cell>2.510 0.257</cell><cell cols="2">19.770 3.637 8.772</cell></row><row><cell cols="2">SINetV2 80.824</cell><cell>93.292</cell><cell>88.072</cell><cell>93.768</cell><cell>2.065 1.680</cell><cell cols="2">11.570 2.495 7.612</cell></row><row><cell>+C</cell><cell cols="2">84.883 93.635  *</cell><cell>88.525</cell><cell cols="2">94.152 1.971 1.573</cell><cell cols="2">11.122 2.402 7.125</cell></row><row><cell>+E</cell><cell>83.655</cell><cell>93.423</cell><cell>88.205</cell><cell>93.978</cell><cell>2.058 1.599</cell><cell cols="2">11.418 2.494 7.492</cell></row><row><cell cols="2">DLV3+ 88.566</cell><cell>94.899</cell><cell>90.571</cell><cell>95.219</cell><cell>1.289 1.339</cell><cell>9.113</cell><cell>2.009 5.603</cell></row><row><cell>+C</cell><cell cols="2">88.943 95.000  *</cell><cell>90.738</cell><cell cols="2">95.239 1.274 1.369</cell><cell cols="2">8.885 1.978 5.391</cell></row><row><cell>+E</cell><cell>88.886</cell><cell>95.002  *</cell><cell>90.741</cell><cell>95.103</cell><cell>1.257 1.448</cell><cell>9.108</cell><cell>2.011 5.468</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies of UNet about ratio r. Bolded means the best result.</figDesc><table><row><cell cols="2">Method Ratio</cell><cell cols="6">Con (%)↑ DSC (%)↑ JC (%)↑ F1 (%)↑ HCE ↓ MAE (%)↓ HD↓</cell><cell>ASD↓ RVD ↓</cell></row><row><cell>UNet</cell><cell>0,0</cell><cell>84.410</cell><cell>93.695</cell><cell>88.690</cell><cell cols="2">94.534 1.988 1.338</cell><cell>11.464 2.287 7.145</cell></row><row><cell></cell><cell>1.0,1.0</cell><cell>84.950</cell><cell>93.787</cell><cell>88.840</cell><cell>94.527</cell><cell>1.921 1.376</cell><cell>11.417 2.271 6.974</cell></row><row><cell>+C</cell><cell cols="2">0.75,0.75 84.979</cell><cell>93.855</cell><cell>88.952</cell><cell>94.528</cell><cell>1.865 1.405</cell><cell>11.378 2.246 6.875</cell></row><row><cell></cell><cell cols="2">0.75,0.50 85.666</cell><cell>94.003</cell><cell>89.154</cell><cell>94.528</cell><cell>1.777 1.449</cell><cell>11.213 2.222 6.658</cell></row><row><cell></cell><cell cols="2">0.50,0.50 84.393</cell><cell>93.742</cell><cell>88.767</cell><cell>94.303</cell><cell>1.908 1.497</cell><cell>11.671 2.305 7.115</cell></row><row><cell>+E</cell><cell cols="2">0.75,0.75 85.392</cell><cell>93.964</cell><cell>89.106</cell><cell>94.290</cell><cell>1.789 1.597</cell><cell>11.461 2.252 6.712</cell></row><row><cell></cell><cell cols="2">0.75,0.50 83.803</cell><cell>93.859</cell><cell>88.949</cell><cell>94.420</cell><cell>1.877 1.471</cell><cell>11.351 2.260 6.815</cell></row><row><cell></cell><cell cols="2">0.50,0.50 86.664</cell><cell>94.233</cell><cell cols="2">89.526 94.466</cell><cell>1.610 1.587</cell><cell>11.229 2.177 6.394</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors of this paper sincerely appreciate all the challenge organizers and owners for providing the public MIS datasets including AbdomenCT-1K, ACDC, <rs type="grantNumber">AMOS 2022</rs>, <rs type="grantNumber">BraTS20</rs>, CHAOS, CRAG, crossMoDA, EndoTect 2020, ETIS-Larib Polyp DB, iChallenge-AMD, iChallenge-PALM, IDRiD 2018, ISIC 2018, <rs type="grantNumber">I2CVB</rs>, <rs type="grantNumber">KiPA22</rs>, <rs type="person">KiTS19&amp; KiTS21</rs>, <rs type="person">Kvasir-SEG</rs>, <rs type="grantNumber">LUNA16</rs>, <rs type="institution">Multi-Atlas Labeling Beyond the Cranial Vault (Abdomen), Montgomery County CXR Set, M&amp;Ms, MSD</rs>, <rs type="grantNumber">NCI-ISBI 2013</rs>, <rs type="grantNumber">PROMISE12</rs>, <rs type="grantNumber">QUBIQ 2021</rs>, <rs type="institution">SIIM-ACR</rs>, <rs type="grantNumber">SLIVER07</rs>, <rs type="person">VerSe19 &amp; VerSe20</rs>, <rs type="funder">Warwick-QU</rs>, and WORD. This work was supported by the grant from <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62171290</rs>, <rs type="grantNumber">62101343</rs>), <rs type="funder">Shenzhen-Hong Kong Joint Research Program</rs> (No. <rs type="grantNumber">SGDX20201103095613036</rs>), and <rs type="funder">Shenzhen Science and Technology Innovations Committee</rs> (No. <rs type="grantNumber">20200812143441001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gDaCFVh">
					<idno type="grant-number">AMOS 2022</idno>
				</org>
				<org type="funding" xml:id="_PSr43Gt">
					<idno type="grant-number">BraTS20</idno>
				</org>
				<org type="funding" xml:id="_n7nQ22c">
					<idno type="grant-number">I2CVB</idno>
				</org>
				<org type="funding" xml:id="_D93asJS">
					<idno type="grant-number">KiPA22</idno>
				</org>
				<org type="funding" xml:id="_y3yPaH7">
					<idno type="grant-number">LUNA16</idno>
				</org>
				<org type="funding" xml:id="_Q3jNH5J">
					<idno type="grant-number">NCI-ISBI 2013</idno>
				</org>
				<org type="funding" xml:id="_8jpjDCq">
					<idno type="grant-number">PROMISE12</idno>
				</org>
				<org type="funding" xml:id="_2XGumwj">
					<idno type="grant-number">QUBIQ 2021</idno>
				</org>
				<org type="funding" xml:id="_ZvuCjjY">
					<idno type="grant-number">SLIVER07</idno>
				</org>
				<org type="funding" xml:id="_8VWsMRK">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_aYvbguA">
					<idno type="grant-number">62101343</idno>
				</org>
				<org type="funding" xml:id="_2XpHWh3">
					<idno type="grant-number">SGDX20201103095613036</idno>
				</org>
				<org type="funding" xml:id="_4xUZhEN">
					<idno type="grant-number">20200812143441001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A two-dimensional image segmentation method based on genetic algorithm and entropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abdel-Khalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ishak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Omer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Obada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="414" to="422" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LiTS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature-fused SSD: fast detection for small objects</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE ICGIP</title>
		<imprint>
			<biblScope unit="volume">10615</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performance measure characterization for evaluating neuroimage segmentation algorithms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="135" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_49" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xception: deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized overlap measures for evaluation and validation in medical image analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Crum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1451" to="1461" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A modified Hausdorff distance for object matching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICPR</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Concealed object detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6024" to="6042" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curvature filters efficiently reduce certain variational energies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Sbalzarini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1786" to="1798" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for MobileNetV3</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37734-2_37" />
	</analytic>
	<monogr>
		<title level="m">MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11962</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning calibrated medical image segmentation via multi-rater agreement modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">AMOS: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WORD: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102642</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Saliency filters: contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highly accurate dichotomous image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13678</biblScope>
			<biblScope unit="page" from="38" to="56" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19797-0_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19797-0_3" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12116" to="12128" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Verse: a vertebrae labelling and segmentation benchmark for multi-detector CT images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102166</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A coarse-to-fine framework for the 2021 kidney and kidney tumor segmentation challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-98385-7_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-98385-7_8" />
	</analytic>
	<monogr>
		<title level="m">KiTS 2021</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Trofimova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Tejpaul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Weight</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13168</biblScope>
			<biblScope unit="page" from="53" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeeze-and-attention networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iou loss for 2D/3D object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prior-aware neural network for partially-supervised multi-organ segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UNet++: redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
