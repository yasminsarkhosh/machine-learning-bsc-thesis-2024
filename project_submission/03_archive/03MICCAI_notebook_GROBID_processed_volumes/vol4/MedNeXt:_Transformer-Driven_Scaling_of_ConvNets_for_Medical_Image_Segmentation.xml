<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Saikat</forename><surname>Roy</surname></persName>
							<email>saikat.roy@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gregor</forename><surname>Koehler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Constantin</forename><surname>Ulrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Center for Tumor Diseases (NCT)</orgName>
								<orgName type="institution">NCT Heidelberg</orgName>
								<address>
									<country>A Partnership Between</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">DKFZ and University Medical Center Heidelberg</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Baumgartner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Helmholtz Imaging</orgName>
								<orgName type="institution">German Cancer Research Center</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Petersen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Isensee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Helmholtz Imaging</orgName>
								<orgName type="institution">German Cancer Research Center</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>JÃ¤ger</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Helmholtz Imaging</orgName>
								<orgName type="institution">German Cancer Research Center</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Interactive Machine Learning Group</orgName>
								<orgName type="institution">German Cancer Research Center</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Medical Image Computing (MIC)</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="laboratory">Pattern Analysis and Learning Group</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="405" to="415"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0806FE2ADD9EEF1D27F1744C023AC0B4</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Image Segmentation</term>
					<term>Transformers</term>
					<term>MedNeXt</term>
					<term>Large Kernels</term>
					<term>ConvNeXt</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been exploding interest in embracing Transformerbased architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformerinspired large kernel segmentation network which introduces -1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https://github.com/MIC-DKFZ/MedNeXt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> have seen wide-scale adoption in medical image segmentation as either components of hybrid architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> or standalone techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> for state-of-the-art performance. The ability to learn longrange spatial dependencies is one of the major advantages of the Transformer architecture in visual tasks. However, Transformers are plagued by the necessity of large annotated datasets to maximize performance benefits owing to their limited inductive bias. While such datasets are common to natural images (ImageNet-1k <ref type="bibr" target="#b5">[6]</ref>, ImageNet-21k <ref type="bibr" target="#b25">[26]</ref>), medical image datasets usually suffer from the lack of abundant high quality annotations <ref type="bibr" target="#b18">[19]</ref>. To retain the inherent inductive bias of convolutions while taking advantage of architectural improvements of Transformers, the ConvNeXt <ref type="bibr" target="#b21">[22]</ref> was recently introduced to re-establish the competitive performance of convolutional networks for natural images. The Con-vNeXt architecture uses an inverted bottleneck mirroring that of Transformers, composed of a depthwise layer, an expansion layer and a contraction layer (Sect. 2.1), in addition to large depthwise kernels to replicate their scalability and long-range representation learning. The authors paired large kernel Con-vNeXt networks with enormous datasets to outperform erstwhile state-of-the-art Transformer-based networks. In contrast, the VGGNet <ref type="bibr" target="#b27">[28]</ref> approach of stacking small kernels continues to be the predominant technique for designing ConvNets in medical image segmentation. Out-of-the-box data-efficient solutions such as nnUNet <ref type="bibr" target="#b12">[13]</ref>, using variants of a standard UNet <ref type="bibr" target="#b4">[5]</ref>, have still remained effective across a wide range of tasks.</p><p>The ConvNeXt architecture marries the scalability and long-range spatial representation learning capabilities of Vision <ref type="bibr" target="#b6">[7]</ref> and Swin Transformers <ref type="bibr" target="#b20">[21]</ref> with the inherent inductive bias of ConvNets. Additionally, the inverted bottleneck design allows us to scale width (increase channels) while not being affected by kernel sizes. Effective usage in medical image segmentation would allow benefits from -1) learning long-range spatial dependencies via large kernels, 2) less intuitively, simultaneously scaling multiple network levels. To achieve this would require techniques to combat the tendency of large networks to overfit on limited training data. Despite this, there have been recent attempts to introduce large kernel techniques to the medical vision domain. In <ref type="bibr" target="#b17">[18]</ref>, a large kernel 3D-UNet <ref type="bibr" target="#b4">[5]</ref> was used by decomposing the kernel into depthwise and depthwise dilated kernels for improved performance in organ and brain tumor segmentationexploring kernel scaling, while using constant number of layers and channels. The ConvNeXt architecture itself was utilized in 3D-UX-Net <ref type="bibr" target="#b16">[17]</ref>, where the Transformer of SwinUNETR <ref type="bibr" target="#b7">[8]</ref> was replaced with ConvNeXt blocks for high performance on multiple segmentation tasks. However, 3D-UX-Net only uses these blocks partially in a standard convolutional encoder, limiting their possible benefits.</p><p>In this work, we maximize the potential of a ConvNeXt design while uniquely addressing challenges of limited datasets in medical image segmentation. We present the first fully ConvNeXt 3D segmentation network, MedNeXt, which is a scalable Encoder-Decoder network, and make the following contributions: MedNeXt achieves state-of-the-art performance against baselines consisting of Transformer-based, convolutional and large kernel networks. We show performance benefits on 4 tasks of varying modality (CT, MRI) and sizes (ranging from 30 to 1251 samples), encompassing segmentation of organs and tumors. We propose MedNeXt as a strong and modernized alternative to standard ConvNets for building deep networks for medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fully ConvNeXt 3D Segmentation Architecture</head><p>In prior work, ConvNeXt <ref type="bibr" target="#b21">[22]</ref> distilled architectural insights from Vision Transformers <ref type="bibr" target="#b6">[7]</ref> and Swin Transformers <ref type="bibr" target="#b20">[21]</ref> into a convolutional architecture. The ConvNeXt block inherited a number of significant design choices from Transformers, designed to limit computation costs while scaling the network, which demonstrated performance improvements over standard ResNets <ref type="bibr" target="#b9">[10]</ref>. In this work, we leverage these strengths by adopting the general design of ConvNeXt as the building block in a 3D-UNet-like <ref type="bibr" target="#b4">[5]</ref> macro architecture to obtain the MedNeXt. We extend these blocks to up and downsampling layers as well (Sect. 2.2), resulting in the first fully ConvNeXt architecture for medical image segmentation. The macro architecture is illustrated in Fig. <ref type="figure" target="#fig_0">1a</ref>. MedNeXt blocks (similar to ConvNeXt blocks) have 3-layers mirroring a Transformer block and are described for a C-channel input as follows:</p><p>1. Depthwise Convolution Layer: This layer contains a Depthwise Convolution with kernel size k Ãk Ãk, followed by normalization, with C output channels. We use channel-wise GroupNorm <ref type="bibr" target="#b31">[32]</ref> for stability with small batches <ref type="bibr" target="#b26">[27]</ref>, instead of the original LayerNorm. The depthwise nature of convolutions allow large kernels in this layer to replicate a large attention window of Swin-Transformers, while simultaneously limiting compute and thus delegating the "heavy lifting" to the Expansion Layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Expansion Layer:</head><p>Corresponding to a similar design in Transformers, this layer contains an overcomplete Convolution Layer with CR output channels, where R is the expansion ratio, followed by a GELU <ref type="bibr" target="#b11">[12]</ref> activation. Large values of R allow the network to scale width-wise while 1 Ã 1 Ã 1 kernel limits compute. It is important to note that this layer effectively decouples width scaling from receptive field (kernel size) scaling in the previous layer. 3. Compression Layer: Convolution layer with 1 Ã 1 Ã 1 kernel and C output channels performing channel-wise compression of the feature maps.</p><p>MedNeXt is convolutional and retains the inductive bias inherent to Conv-Nets that allows easier training on sparse medical datasets. Our fully ConvNeXt architecture also enables width (more channels) and receptive field (larger kernels) scaling at both standard and up/downsampling layers. Alongside depth scaling (more layers), we explore these 3 orthogonal types of scaling to design a compound scalable MedNeXt for effective medical image segmentation (Sect. 2.4).  </p><formula xml:id="formula_0">B R1 = R9 = 2 R2 = R8 = 3 R3-7 = 4 M B1 = B9 = 3 B2-8 = 4 L B1 = B9 = 3 B2 = B8 = 4 B3-7 = 8 R1 = R9 = 3 R2 = R8 = 4 R3-7 = 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Resampling with Residual Inverted Bottlenecks</head><p>The original ConvNeXt design utilizes separate downsampling layers which consist of standard strided convolutions. An equivalent upsampling block would be standard strided transposed convolutions. However, this design does not implicitly take advantage of width or kernel-based ConvNeXt scaling while resampling. We improve upon this by extending the Inverted Bottleneck to resampling blocks in MedNeXt. This is done by inserting the strided convolution or transposed convolution in the first Depthwise Layer for Downsampling and Upsampling MedNeXt blocks respectively. The corresponding channel reduction or increase is inserted in the last compression layer of our MedNeXt 2Ã Up or Down block design as in Fig. <ref type="figure" target="#fig_0">1a</ref>. Additionally, to enable easier gradient flow, we add a residual connection with 1 Ã 1 Ã 1 convolution or transposed convolution with stride of 2.</p><p>In doing so, MedNeXt fully leverages the benefits from Transformer-like inverted bottlenecks to preserve rich semantic information in lower spatial resolutions in all its components, which should benefit dense medical image segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">UpKern: Large Kernel Convolutions Without Saturation</head><p>Large convolution kernels approximate the large attention windows in Transformers, but remain prone to performance saturation. ConvNeXt architectures in classification of natural images, despite the benefit of large datasets such as ImageNet-1k and ImageNet-21k, are seen to saturate at kernels of size 7 Ã 7 Ã 7 <ref type="bibr" target="#b21">[22]</ref>. Medical image segmentation tasks have significantly less data and performance saturation can be a problem in large kernel networks. To propose a solution, we borrow inspiration from Swin Transformer V2 <ref type="bibr" target="#b19">[20]</ref> where a largeattention-window network is initialized with another network trained with a smaller attention window. Specifically, Swin Transformers use a bias matrix B â R (2M -1)Ã(2M -1) to store learnt relative positional embeddings, where M is the number of patches in an attention window. On increasing the window size, M increases and necessitates a larger B. The authors proposed spatially interpolating an existing bias matrix to the larger size as a pretraining step, instead of training from scratch, which demonstrated improved performance. We propose a similar approach but customized to convolutions kernels, as seen in Fig. <ref type="figure" target="#fig_0">1b</ref>, to overcome performance saturation. UpKern allows us to iteratively increase kernel size by initializing a large kernel network with a compatible pretrained small kernel network by trilinearly upsampling convolutional kernels (represented as tensors) of incompatible size. All other layers with identical tensor sizes (including normalization layers) are initialized by copying the unchanged pretrained weights. This leads to a simple but effective initialization technique for Med-NeXt which helps large kernel networks overcome performance saturation in the comparatively limited data scenarios common to medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Compound Scaling of Depth, Width and Receptive Field</head><p>Compound scaling <ref type="bibr" target="#b28">[29]</ref> is the idea that simultaneous scaling on multiple levels (depth, width, receptive field, resolution etc.) offers benefits beyond that of scaling at one single level. The computational requirements of indefinitely scaling kernel sizes in 3D networks quickly becomes prohibitive and leads us to investigate simultaneous scaling at different levels. Keeping with Fig. <ref type="figure" target="#fig_0">1a</ref>, our scaling is tested for block count (B), expansion ratio (R) and kernel size (k) -corresponding to depth, width and receptive field size. We use 4 model configurations of the MedNeXt to do so, as detailed in Table <ref type="table" target="#tab_1">1</ref> (Left). The basic functional design (MedNeXt-S) uses number of channels (C) as 32, R = 2 and B = 2. Further variants increase on just R (MedNeXt-B) or both R and B (MedNeXt-M).</p><p>The largest 70-MedNext-block architecture uses high values of both R and B (MedNeXt-L) and is used to demonstrate the ability of MedNeXt to be significantly scaled depthwise (even at standard kernel sizes). We further explore large kernel sizes and experiment with k = {3, 5} for each configuration, to maximize performance via compound scaling of the MedNeXt architecture.</p><p>3 Experimental Design</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Configurations, Implementation and Baselines</head><p>We use PyTorch <ref type="bibr" target="#b23">[24]</ref> for implementing our framework. We experiment with 4 configurations of the MedNeXt with 2 kernel sizes as detailed in Sect. 2.4. The GPU memory requirements of scaling are limited via -1) Mixed precision training with PyTorch AMP, 2) Gradient Checkpointing. <ref type="bibr" target="#b3">[4]</ref>. Our experimental framework uses the nnUNet <ref type="bibr" target="#b12">[13]</ref> as a backbone -where the training schedule (epochs = 1000, batches per epoch = 250), inference (50% patch overlap) and data augmentation remain unchanged. All networks, except nnUNet, are trained with AdamW <ref type="bibr" target="#b22">[23]</ref> as optimizer. The data is resampled to 1.0 mm isotropic spacing during training and inference (with results on original spacing), using input patch size of 128 Ã 128 Ã 128 and 512 Ã 512, and batch size 2 and 14, for 3D and 2D networks respectively. The learning rate for all MedNeXt models is 0.001, except kernel:5 in KiTS19, which uses 0.0001 for stability. For baselines, all Swin models and 3D-UX-Net use 0.0025, while ViT models use 0.0001. We use Dice Similarity Coefficient (DSC) and Surface Dice Similarity (SDC) at 1.0 mm tolerance for volumetric and surface accuracy. 5-fold cross-validation (CV) mean performance for supervised training using 80:20 splits for all models are reported. We also provide test set DSC scores for a 5-fold ensemble of MedNeXt-L (kernel: 5 Ã 5 Ã 5) without postprocessing. Our extensive baselines consist of a high-performing convolutional network (nnUNet <ref type="bibr" target="#b12">[13]</ref>), 4 convolution-transformer hybrid networks with transformers in the encoder (UNETR <ref type="bibr" target="#b8">[9]</ref>, SwinUNETR <ref type="bibr" target="#b7">[8]</ref>) and in intermediate layers <ref type="bibr">(TransBTS [31]</ref>, TransUNet <ref type="bibr" target="#b2">[3]</ref>), a fully transformer network (nnFormer <ref type="bibr" target="#b33">[34]</ref>) as well as a partially ConvNeXt network (3D-UX-Net <ref type="bibr" target="#b16">[17]</ref>). TransUNet is a 2D network while the rest are 3D networks. The uniform framework provides a common testbed for all networks, without incentivizing one over the other on aspects of patch size, spacing, augmentations, training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We use 4 popular tasks, encompassing organ as well as tumor segmentation tasks, to comprehensively demonstrate the benefits of the MedNeXt architecture -1) Beyond-the-Cranial-Vault (BTCV) Abdominal CT Organ Segmentation <ref type="bibr" target="#b15">[16]</ref>, 2) AMOS22 Abdominal CT Organ Segmentation <ref type="bibr" target="#b13">[14]</ref> 3) Kidney Tumor Segmentation Challenge 2019 Dataset (KiTS19) <ref type="bibr" target="#b10">[11]</ref>, 4) Brain Tumor Segmentation Challenge 2021 (BraTS21) <ref type="bibr" target="#b0">[1]</ref>. BTCV, AMOS22 and KiTS19 datasets contain 30, 200 and 210 CT volumes with 13, 15 and 2 classes respectively, while the BraTS21 dataset contains 1251 MRI volumes with 3 classes. This diversity shows the effectiveness of our methods across imaging modalities and training set sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Ablation of Architectural Improvements</head><p>We ablate the MedNeXt-B configuration on AMOS22 and BTCV datasets to highlight the efficacy of our improvements and demonstrate that a vanilla ConvNeXt is unable to compete with existing segmentation baselines such as nnUNet. The following are observed in ablation tests in  3. The performance boost in large kernels is seen to be due to the combination of UpKern with a larger kernel and not merely a longer effective training schedule (Upkern vs Trained 2Ã), as a trained MedNeXt-B with kernel 3Ã3Ã3 retrained again is unable to match its large kernel counterpart.</p><p>This highlights that the MedNeXt modifications successfully translate the ConvNeXt architecture to medical image segmentation. We further establish the performance of the MedNeXt architecture against our baselines -comprising of convolutional, transformer-based and large kernel baselines -on all 4 datasets. We discuss the effectiveness of the MedNeXt on multiple levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison to Baselines</head><p>There are 2 levels at which MedNeXt successfully overcomes existing baselines -5 fold CV and public testset performance. In 5-fold CV scores in Table <ref type="table" target="#tab_4">2</ref>, Med-NeXt, with 3 Ã 3 Ã 3 kernels, takes advantage of depth and width scaling to provide state-of-the-art segmentation performance against every baseline on all 4 datasets with no additional training data. MedNeXt-L outperforms or is competitive with smaller variants despite task heterogeneity (brain and kidney tumors, organs), modality (CT, MRI) and training set size (BTCV: 18 samples vs BraTS21: 1000 samples), establishing itself as a powerful alternative to established methods such as nnUNet. With UpKern and 5 Ã 5 Ã 5 kernels, MedNeXt takes advantage of full compound scaling to improve further on its own small kernel networks, comprehensively on organ segmentation (BTCV, AMOS22) and in a more limited fashion on tumor segmentation (KiTS19, BraTS21).</p><p>Furthermore, in leaderboard scores on official testsets (Fig. <ref type="figure" target="#fig_0">1c</ref>), 5-fold ensembles for MedNeXt-L (kernel: 5 Ã 5 Ã 5) and nnUNet, its strongest competitor are compared -1) BTCV: MedNeXt beats nnUNet and, to the best of our knowledge, is one of the leading methods with only supervised training and no extra training data (DSC: 88.76, HD95: 15.34), 2) AMOS22: MedNeXt not only surpasses nnUNet, but is also Rank 1 (date: 09.03.23) currently on the leaderboard (DSC: 91.77, NSD: 84.00), 3) KITS19: MedNeXt exceeds nnUNet performance (DSC: 91.02), 4) BraTS21: MedNeXt surpasses nnUNet in both volumetric and surface accuracy (DSC: 88.01, HD95: 10.69). MedNeXt attributes its performance solely to its architecture without leveraging techniques like transfer learning (3D-UX-Net) or repeated 5-fold ensembling (UNETR, SwinUNETR), thus establishing itself as the state-of-the-art for medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In comparison to natural image analysis, medical image segmentation lacks architectures that benefit from scaling networks due to inherent domain challenges such as limited training data. In this work, MedNeXt is presented as a scalable Transformer-inspired fully-ConvNeXt 3D segmentation architecture customized for high performance on limited medical image datasets. We demonstrate Med-NeXt's state-of-the-art performance across 4 challenging tasks against 7 strong baselines. Additionally, similar to ConvNeXt for natural images <ref type="bibr" target="#b21">[22]</ref>, we offer the compound scalable MedNeXt design as an effective modernization of standard convolution blocks for building deep networks for medical image segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Architectural design of the MedNeXt. The network has 4 Encoder and Decoder layers each, with a bottleneck layer. MedNeXt blocks are present in Up and Downsampling layers as well. Deep Supervision is used at each decoder layer, with lower loss weights at lower resolutions. All residuals are additive while convolutions are padded to retain tensor sizes. (b) Upsampled Kernel (UpKern) initialization of a pair of MedNeXt architectures with similar configurations (Î¸) except kernel size (k1, k2). (c) MedNeXt-L (5 Ã 5 Ã 5) leaderboard performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>-</head><label></label><figDesc>We utilize an architecture composed purely of ConvNeXt blocks which enables network-wide advantages of the ConvNeXt design. (Sect. 2.1) -We introduce Residual Inverted Bottlenecks in place of regular up and</figDesc><table><row><cell>downsampling blocks, to preserve contextual richness while resampling to</cell></row><row><cell>benefit dense segmentation tasks. The modified residual connection in partic-</cell></row><row><cell>ular improves gradient flow during training. (Sec. 2.2)</cell></row><row><cell>-We introduce a simple but effective technique of iteratively increasing kernel</cell></row><row><cell>size, UpKern, to prevent performance saturation on large kernel MedNeXts</cell></row><row><cell>by initializing with trained upsampled small kernel networks. (Sect. 2.3)</cell></row><row><cell>-We propose applying Compound Scaling [29] of multiple network parame-</cell></row><row><cell>ters owing to our network design, allowing orthogonality of width (channels),</cell></row><row><cell>receptive field (kernel size) and depth (number of layers) scaling. (Sect. 2.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>(Left) MedNeXt configurations from scaling Block Counts (B) and Expansion Ratio (R) as in Fig. 1a. (Right) MedNext-B ablations (Sect. 4.1).</figDesc><table><row><cell cols="3">Config. # Blocks (B) Exp. Rat. (R)</cell></row><row><cell>S</cell><cell>Ball = 2</cell><cell>Rall = 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 (</head><label>1</label><figDesc>Right) -1. Residual Inverted Bottlenecks, specifically in Up and Downsampling layers, functionally enables MedNeXt (MedNeXt-B Resampling vs Standard Resampling) for medical image segmentation. In contrast, absence of these modified blocks lead to considerably worse performance. This is possibly owing to preservation of semantic richness in feature maps while resampling. 2. Training large kernel networks for medical image segmentation is a non-trivial task, with large kernel MedNeXts trained from scratch failing to perform in seen in MedNeXt-B (UpKern vs From Scratch). UpKern improves performance in kernel 5 Ã 5 Ã 5 on both BTCV and AMOS22, whereas large kernel performance is indistinguishable from small kernels without it.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b4">5</ref>-fold CV results of MedNeXt at kernel sizes: {3, 5} outperforming 7 baselines -consisting of convolutional, transformer and large kernel networks.[val (bold): better than or equal to (â¥) top baseline, val (underline): better than (&gt;) kernel: 3 of same configuration]</figDesc><table><row><cell>Networks</cell><cell>Cat.</cell><cell>BTCV</cell><cell>AMOS22</cell><cell>KiTS19</cell><cell>BraTS21</cell><cell>AVG</cell></row><row><cell></cell><cell></cell><cell cols="5">DSC SDC DSC SDC DSC SDC DSC SDC DSC SDC</cell></row><row><cell>nnUNet</cell><cell cols="6">Baselines 83.56 86.07 88.88 91.70 89.88 86.88 91.23 90.46 88.39 88.78</cell></row><row><cell>UNETR</cell><cell></cell><cell cols="5">75.06 75.00 81.98 82.65 84.10 78.05 89.65 88.28 82.36 81.00</cell></row><row><cell>TransUNet</cell><cell></cell><cell cols="5">76.72 76.64 85.05 86.52 80.82 72.90 89.17 87.78 82.94 80.96</cell></row><row><cell>TransBTS</cell><cell></cell><cell cols="5">82.35 84.33 86.52 88.84 87.03 83.53 90.66 89.71 86.64 86.60</cell></row><row><cell>nnFormer</cell><cell></cell><cell cols="5">80.76 82.37 84.20 86.38 89.09 85.08 90.42 89.83 86.12 85.92</cell></row><row><cell>SwinUNETR</cell><cell></cell><cell cols="5">80.95 82.43 86.83 89.23 87.36 83.09 90.48 89.56 86.41 86.08</cell></row><row><cell>3D-UX-Net</cell><cell></cell><cell cols="5">80.76 82.30 87.28 89.74 88.39 84.03 90.63 89.63 86.77 86.43</cell></row><row><cell cols="7">MedNeXt-S kernel: 3 83.90 86.60 89.03 91.97 90.45 87.80 91.27 90.46 88.66 89.21</cell></row><row><cell>MedNeXt-B</cell><cell></cell><cell cols="5">84.01 86.77 89.14 92.10 91.02 88.24 91.30 90.51 88.87 89.41</cell></row><row><cell>MedNeXt-M</cell><cell></cell><cell cols="5">84.31 87.34 89.27 92.28 90.78 88.22 91.57 90.78 88.98 89.66</cell></row><row><cell>MedNeXt-L</cell><cell></cell><cell cols="5">84.57 87.54 89.58 92.62 90.61 88.08 91.57 90.81 89.08 89.76</cell></row><row><cell cols="7">MedNeXt-S kernel: 5 83.92 86.80 89.27 92.26 90.08 87.04 91.40 90.57 88.67 89.17</cell></row><row><cell>MedNeXt-B</cell><cell></cell><cell cols="5">84.23 87.06 89.38 92.36 90.30 87.40 91.48 90.70 88.85 89.38</cell></row><row><cell>MedNeXt-M</cell><cell></cell><cell cols="5">84.41 87.48 89.58 92.65 90.87 88.15 91.49 90.67 89.09 89.74</cell></row><row><cell>MedNeXt-L</cell><cell></cell><cell cols="5">84.82 87.85 89.87 92.95 90.71 87.85 91.46 90.73 89.22 89.85</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">TransUNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ã</forename><surname>ÃiÃ§ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2021</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-2_22" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12962</biblScope>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: results of the KiTS19 challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101821</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">AMOS: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolution-free medical image segmentation using transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Vasylechko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Miccai multiatlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI Multi-Atlas Labeling Beyond Cranial Vault Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">D UX-Net: a large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15076</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-kernel attention for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swin transformer v2: scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3D tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_16" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ImageNet-21k pretraining for the masses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are 2.5 d approaches superior to 3D deep networks in whole brain segmentation?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>KÃ¼gler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reuter</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="988" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CoTr: efficiently bridging CNN and transformer for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">nnFormer: Interleaved transformer for volumetric segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03201</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
