<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenqi</forename><surname>He</surname></persName>
							<idno type="ORCID">0009-0000-2265-7159</idno>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<addrLine>Pok Fu Lam</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Unberath</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Ke</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
							<idno type="ORCID">0000-0001-7866-3339</idno>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="206" to="215"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1FEC19273CFB1F3A7A4851470AF7917E</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lightweight Multi-Task Framework</term>
					<term>Shared Attention Heads</term>
					<term>Nuclei</term>
					<term>Edge and Clustered Edge Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nuclei appear small in size, yet, in real clinical practice, the global spatial information and correlation of the color or brightness contrast between nuclei and background, have been considered a crucial component for accurate nuclei segmentation. However, the field of automatic nuclei segmentation is dominated by Convolutional Neural Networks (CNNs), meanwhile, the potential of the recently prevalent Transformers has not been fully explored, which is powerful in capturing local-global correlations. To this end, we make the first attempt at a pure Transformer framework for nuclei segmentation, called TransNuSeg. Different from prior work, we decouple the challenging nuclei segmentation task into an intrinsic multi-task learning task, where a tri-decoder structure is employed for nuclei instance, nuclei edge, and clustered edge segmentation respectively. To eliminate the divergent predictions from different branches in previous work, a novel self distillation loss is introduced to explicitly impose consistency regulation between branches. Moreover, to formulate the high correlation between branches and also reduce the number of parameters, an efficient attention sharing scheme is proposed by partially sharing the self-attention heads amongst the tri-decoders. Finally, a token MLP bottleneck replaces the over-parameterized Transformer bottleneck for a further reduction in model complexity. Experiments on two datasets of different modalities, including MoNuSeg have shown that our methods can outperform state-of-the-art counterparts such as CA 2.5 -Net by 2-3% Dice with 30% fewer parameters. In conclusion, TransNuSeg confirms the strength of Transformer in the context of nuclei segmentation, which thus can serve as an efficient solution for real clinical practice. Code is available at https://github.com/zhenqi-he/ transnuseg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate cancer diagnosis, grading, and treatment decisions from medical images heavily rely on the analysis of underlying complex nuclei structures <ref type="bibr" target="#b6">[7]</ref>. Yet, due to the numerous nuclei contained in a digitized whole-slide image (WSI), or even in an image patch of deep learning input, dense annotation of nuclei contouring is extremely time-consuming and labor-expensive <ref type="bibr" target="#b10">[11]</ref>. Consequently, automated nuclei segmentation approaches have emerged to satisfy a broad range of computer-aided diagnostic systems, where the deep learning methods, particularly the convolutional neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> have received notable attention due to their simplicity and generalization ability.</p><p>In the literature work, the sole-decoder design in these UNet variants (Fig. <ref type="figure" target="#fig_0">1(a)</ref>) is susceptible to failures in splitting densely clustered nuclei when precise edge information is absent. Hence, deep contour-aware neural network (DCAN) <ref type="bibr" target="#b2">[3]</ref> with bi-decoder structure achieves improved instance segmentation performance by adopting multi-task learning, in which one decoder learns to segment the nuclei and the other recognizes edges as described in Fig. <ref type="figure" target="#fig_0">1(b</ref>). Similarly, CIA-Net <ref type="bibr" target="#b19">[20]</ref> extends DCAN with an extra information aggregator to fuse the features from two decoders for more precise segmentation. Much recently, CA 2.5 -Net <ref type="bibr" target="#b5">[6]</ref> shows identifying the clustered edges in a multiple-task learning manner can achieve higher performance, and thereby proposes an extra output path to learn the segmentation of clustered edges explicitly. A significant drawback of the aforementioned multi-decoder networks is the ignorance of the prediction consistency between branches, resulting in sub-optimal performance and missing correlations between the learned branches. Specifically, a prediction mismatch between the nuclei and edge branches is observed in previous work <ref type="bibr" target="#b7">[8]</ref>, implying a direction for performance improvement. To narrow this gap, we propose a consistency distillation between the branches, as shown by the dashed line in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Furthermore, to resolve the cost of involving more decoders, we propose an attention sharing scheme, along with an efficient token MLP bottleneck <ref type="bibr" target="#b15">[16]</ref>, which can both reduce the number of parameters.</p><p>Additionally, existing methods are CNN-based, and their intrinsic convolution operation fails to capture global spatial information or the correlation amongst nuclei <ref type="bibr" target="#b17">[18]</ref>, which domain experts rely heavily on for accurate nuclei allocation. It suggests the presence of long-range correlation in practical nuclei segmentation tasks. Inspired by the capability in long-range global context capturing by Transformers <ref type="bibr" target="#b16">[17]</ref>, we make the first attempt to construct a tri-decoder based Transformer model to segment nuclei. In short, our major contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We propose a novel multi-task framework for nuclei segmentation, namely TransNuSeg, as the first attempt at a fully Swin-Transformer driven architecture for nuclei segmentation. (2) To alleviate the prediction inconsistency between branches, we propose a novel self distillation loss that regulates the consistency between the nuclei decoder and normal edge decoder. <ref type="bibr" target="#b2">(3)</ref> We propose an innovative attention sharing scheme that shares attention heads amongst all decoders. By leveraging the high correlation between tasks, it can communicate the learned features efficiently across decoders and sharply reduce the number of parameters. Furthermore, the incorporation of a light-weighted MLP bottleneck leads to a sharp reduction of parameters at no cost of performance decline. Fig. <ref type="figure">2</ref>. The overall framework of the proposed TransNuSeg of three output branches to separate the nuclei, normal edges, and cluster edges, respectively. In the novel design, a pre-defined proportion of the attention heads are shared between the decoders via the proposed sharing scheme, which considerably reduces the number of parameters and enables more efficient information communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Network Architecture Overview. Figure <ref type="figure">2</ref> illustrates the overall architecture of the proposed multi-task tri-decoder Transformer network, named TransNuSeg. Both the encoder and decoders utilize the Swin Transformer <ref type="bibr" target="#b12">[13]</ref> as the building blocks to capture the long-range feature correlations in the nuclei segmentation context. Our network consists of three individual output decoder paths for nuclei segmentation, normal edges segmentation, and clustered edges segmentation. Given the high dependency between edge and clustered edge, we are inspired to propose a novel attention sharing scheme, which can communicate the information and share learned features across decoders while also reducing the number of parameters. Additionally, a token MLP bottleneck is incorporated to further increase the model efficiency.</p><p>Attention Sharing Scheme. To capture the strong correlation between nuclei segmentation and contour segmentation between multiple decoders <ref type="bibr" target="#b14">[15]</ref>, we introduce a novel attention sharing scheme that is designed as an enhancement to the multi-headed self-attention (MSA) module in the plain Transformer <ref type="bibr" target="#b16">[17]</ref>. Based on the attention sharing scheme, we design a shared MSA module, which is similar in structure to vanilla MSA. Specifically, it consists of a LayerNorm layer <ref type="bibr" target="#b0">[1]</ref>, residual connection, and feed-forward layer. Innovatively, it differs from the vanilla MSA by sharing a proportion of globally-shared self-attention (SA) heads amongst all the parallel Transformer blocks in decoders, while keeping the remaining SA heads unshared i. e. learn the weights separately. A schematic illustration of the shared MSA module in the Swin Transformer block is demonstrated in Fig. <ref type="figure" target="#fig_2">3</ref>, as is formally formulated as follows:</p><formula xml:id="formula_0">Shared-MSA(z) = SA s 1 (z), • • • , SA s m (z), SA u 1 (z), • • • , SA u n (z) U u MSA ,<label>(1)</label></formula><p>[•] writes for the concatenation, SA(•) denotes the self-attention head whose output dimension is D h , and U u MSA ∈ R (m+n)•D h ×D is a learnable matrix. The superscript s and u refer to the globally-shared and unshared weights across all decoders, respectively.</p><p>Token MLP Bottleneck. To reduce the complexity of the model, we leverage a token MLP bottleneck as a light-weight alternative for the Swin Transformer bottleneck. Specifically, this approach involves shifting the latent features extracted by the encoder via two MLP blocks across the width and height channels, respectively <ref type="bibr" target="#b15">[16]</ref>. The objective of this process is to attend to specific areas, which mimics the shifted window attention mechanism in Swin Transformer <ref type="bibr" target="#b12">[13]</ref>. The shifted features are then projected by a learnable MLP and normalized through a LayerNorm <ref type="bibr" target="#b0">[1]</ref> before being fed to a reprojection MLP layer.</p><p>Consistency Self Distillation. To alleviate the inconsistency between the contour generated from the nuclei segmentation prediction and the predicted edge, we propose a novel consistency self distillation loss, denoted as L SD . Formally, this regularization is defined as the dice loss between the contour generated from the nuclei branch prediction (y n ) using the Sobel operation (sobel(y n )) and the predicted edges y e from the normal edge decoder. Specifically, the self distillation loss L D is formulated by L sd = Dice(sobel(y n ), y e ).</p><p>Multi-task Learning Objective. We employ a multi-task learning paradigm to train the tri-decoder network, aiming to improve model performance by leveraging the additional supervision signal from edges. Particularly, the nuclei semantic segmentation is considered the primary task, while the normal edge and clustered edge semantic segmentation are viewed as auxiliary tasks. All decoder branches follow a uniform scheme that combines the cross-entropy loss and the dice loss, with the balancing coefficients set to 0.60 and 0.40 respectively, as previous work <ref type="bibr" target="#b5">[6]</ref>. Subsequently, the overall loss L is calculated as a weighted summation of semantic nuclei mask loss (L n ), normal edge loss (L e ), and clustered edge loss (L c ), and the self distillation loss (L SD ) i. e.</p><formula xml:id="formula_1">L = γ n • L n + γ e • L e + γ c • L c + γ sd • L sd</formula><p>, where coefficients γ n , γ e and γ c are set to 0.30, 0.35, 0.35 respectively, and γ sd is initially set to 1 with a 0.3 decrease for every 10 epochs until it reaches 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. We evaluated the applicability of our approach across multiple modalities by conducting evaluations on microscopy and histology datasets.   The private dataset contains 300 images sized at 512 × 512 tessellated from 50 WSIs scanned at 20×, and meticulously labeled by five pathologists according to the labeling guidelines of the MoNuSeg <ref type="bibr" target="#b9">[10]</ref>. For both datasets, we randomly split 80% of the samples on the patient level as the training set and the remaining 20% as the test set.  TransNuSeg demonstrates superior segmentation performance compared to its counterparts, which can successfully distinguish severely clustered nuclei from normal edges.</p><p>Implementations. All experiments are performed on one NVIDIA RTX 3090 GPU with 24 GB memory. We use Adam optimizer with an initial learning rate of 1 × 10 -4 . We compare TransNuSeg with UNet <ref type="bibr" target="#b13">[14]</ref>, UNet++ <ref type="bibr" target="#b20">[21]</ref>, Tran-sUNet <ref type="bibr" target="#b3">[4]</ref>, SwinUNet <ref type="bibr" target="#b1">[2]</ref>, and CA 2.5 -Net <ref type="bibr" target="#b5">[6]</ref>. We evaluate the results by using Dice Score (DSC), Intersection over Union (IoU), pixel-level accuracy (Acc), and F1-score(F1) as metrics, and ErCnt <ref type="bibr" target="#b7">[8]</ref>. To ensure statistical significance, we run all methods five times with different fixed seeds and report the results as mean ± standard deviation.</p><p>Results. Table <ref type="table" target="#tab_0">1</ref> shows the quantitative comparisons for the nuclei segmentation. The large margin between the SwinUNet and the other CNN-based or hybrid networks also confirms the superiority of the Transformer in fine-grained nuclei segmentation. More importantly, our method can outperform SwinUNet and the previous methods on both datasets. For example, in the histology image dataset, TransNuSeg improves the dice score, F1 score, accuracy, and IoU by 2.08%, 3.41%, 1.25%, and 2.70% respectively, over the second-best models. Similarly, in the fluorescence microscopy image dataset, our proposed model improves DSC by 0.96%, while also leading to 1.65%, 1.03% and 1.91% increment in F1 score, accuracy, and IoU to the second-best performance. For better visualization, representative samples and their segmentation results using different methods are demonstrated in Fig. <ref type="figure" target="#fig_3">4</ref>. Furthermore, Table <ref type="table" target="#tab_1">2</ref> compares the model complexity in terms of the number of parameters, floating point operations per second (FLOPs), and the training computational cost, where our approach can significantly reduce around 28% of the training time compared to the state-ofthe-art CNN multi-task method CA 2.5 -Net, while also boosting performance. Ablation. Our ablation study yields that token MLP bottleneck and attention sharing schemes can complementarily reduce the training cost while increasing efficiency, as shown in Table <ref type="table" target="#tab_1">2</ref> (the last 4 rows). To further show the effectiveness of these schemes, as well as consistency self distillation, we conduct a comprehensive ablation study on both datasets. As described in Table <ref type="table" target="#tab_2">3</ref>, each component proportionally contributes to the improvement to reach the overall performance boost. Moreover, self distillation can enhance the intrinsic consistency between two branches, as visualized in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we make the first attempt at an efficient but effective multi-task Transformer framework for modality-agnostic nuclei segmentation. Specifically, our tri-decoder framework TransNuSeg leverages an innovative self distillation regularization to impose consistency between the different branches. Experimental results on two datasets demonstrate the excellence of our TransNuSeg against state-of-the-art counterparts for potential real-world clinical deployment. Additionally, our work opens a new architecture to perform nuclei segmentation tasks with Swin Transformer, where further investigations can be performed to explore the generalizability to the top of our methods with different modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Semantic illustrations of the nuclei segmentation networks with different numbers of decoders. (a) Sole-decoder to perform a single task of nuclei segmentation. (b) Bi-decoder to segment nuclei and locate nuclei edges simultaneously. (c) Tri-decoder with the third encoder path to specify the challenging clustered edge (ours), where the consistency regularization is designed across the predictions from the other two branches (dashed line).</figDesc><graphic coords="3,74,46,53,87,303,91,60,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(1) Fluorescence Microscopy Image Dataset: This set combines three different data sources to simulate the heterogeneous nature of medical images [9]. It consists of 524 fluorescence images, each with a resolution of 512 × 512 pixels. (2) Histology Image Dataset: This set is the combination of the open dataset MoNuSeg [10] and another private histology dataset [8] of 462 images. We crop each image in the MoNuSeg dataset into four partially overlapping 512 × 512 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A schematic illustration of the proposed Attention Sharing scheme.</figDesc><graphic coords="6,79,29,447,02,265,30,130,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Exemplary samples and their segmentation results using different methods. TransNuSeg demonstrates superior segmentation performance compared to its counterparts, which can successfully distinguish severely clustered nuclei from normal edges.</figDesc><graphic coords="8,47,79,53,69,328,72,156,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The impact of self distillation regularization on mismatch reduction across three decoders. (a) Raw input image. Segmentation results by TransNuSeg trained (b) w/o self distillation, and (c) w/ self distillation. The predicted normal edges from the normal edge decoder are shown in green; while the edges generated from the nuclei decoder and processed with the Sobel operation are in red. The yellow color indicates the overlap between both. Accordingly, the numbers below images indicate the proportion of the pixels belonging to the three parts. Compared to the results without self distillation, the outputs with self distillation exhibit reduced mismatches, resulting in improved segmentation performance. (Color figure online)</figDesc><graphic coords="9,61,98,331,64,328,69,152,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,73,80,232,16,276,79,259,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with counterparts. The best performance with respect to each metric is highlighted in boldface. TransUNet 85.80 ± 0.20 72.87 ± 0.49 90.53 ± 0.27 60.21 ± 0.46 35.2 ± 0.8 SwinUNet 88.73 ± 0.90 78.11 ± 1.88 91.23 ± 0.73 64.41 ± 0.15 27.6 ± 2.3 CA 2.5 -Net 86.74 ± 0.18 77.42 ± 0.30 91.52 ± 0.78 66.79 ± 0.34 23.7 ± 0.7 Ours 90.81 ± 0.22 81.52 ± 0.44 92.77 ± 0.64 69.49 ± 0.17 11.4 ± 1.1</figDesc><table><row><cell>Dataset</cell><cell>Methods</cell><cell>DSC (%)</cell><cell>F1 (%)</cell><cell>Acc (%)</cell><cell>IoU (%)</cell><cell>ErCnt (%)</cell></row><row><cell cols="2">Microscopy UNet</cell><cell cols="5">85.51 ± 0.35 91.05 ± 0.13 92.19 ± 0.20 85.44 ± 0.29 55.2 ± 2.7</cell></row><row><cell></cell><cell>UNet++</cell><cell cols="5">94.14 ± 0.58 92.34 ± 0.63 93.87 ± 0.61 86.20 ± 1.02 69.3 ± 1.4</cell></row><row><cell></cell><cell cols="6">TransUNet 94.14 ± 0.47 92.31 ± 0.34 93.76 ± 0.50 86.16 ± 0.56 51.9 ± 1.0</cell></row><row><cell></cell><cell cols="6">SwinUNet 96.05 ± 0.27 95.02 ± 0.23 96.08 ± 0.23 91.06 ± 0.43 31.2 ± 0.6</cell></row><row><cell></cell><cell cols="6">CA 2.5 -Net 91.08 ± 0.49 90.05 ± 0.27 93.40 ± 0.14 86.89 ± 0.87 18.6 ± 1.3</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">97.01 ± 0.74 96.67 ± 0.60 97.11 ± 1.02 92.97 ± 0.41 9.78 ± 2.1</cell></row><row><cell>Histology</cell><cell>UNet</cell><cell cols="5">80.97 ± 0.75 72.17 ± 0.49 90.14 ± 0.24 61.63 ± 0.36 45.7 ± 1.6</cell></row><row><cell></cell><cell>UNet++</cell><cell cols="5">87.10 ± 0.16 75.20 ± 0.19 91.34 ± 0.14 62.89 ± 0.27 38.0 ± 2.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the model complexity in terms of the number of parameters, FLOPs, as well as the training cost in the form of the averaged training time per epoch. The average training time is computed using the same batch size for both datasets, with the first number indicating the averaged time on the Fluorescence Microscopy Image Dataset and the second on the Histology Image Dataset. The token MLP bottleneck and attention sharing scheme are denoted as 'MLP', and 'AS', respectively.</figDesc><table><row><cell>Methods</cell><cell cols="3">#Params (×10 6 ) FLOPs (×10 9 ) Training (s)</cell></row><row><cell>UNet [14]</cell><cell>31.04</cell><cell>219.03</cell><cell>43.4/27.7</cell></row><row><cell>UNet++ [21]</cell><cell>9.05</cell><cell>135.72</cell><cell>41.8/31.7</cell></row><row><cell>TransUNet [4]</cell><cell>67.87</cell><cell>129.97</cell><cell>37.1/34.5</cell></row><row><cell>SwinUNet [2]</cell><cell>27.18</cell><cell>30.67</cell><cell>37.8/35.2</cell></row><row><cell>CA 2.5 -Net [6]</cell><cell>24.27</cell><cell>460.70</cell><cell>73.8/70.2</cell></row><row><cell cols="2">Ours (w/o MLP &amp; w/o AS) 34.33</cell><cell>93.98</cell><cell>76.1/74.3</cell></row><row><cell>Ours (w/o MLP)</cell><cell>30.82</cell><cell>123.60</cell><cell>62.6/61.2</cell></row><row><cell>Ours (w/o AS)</cell><cell>21.33</cell><cell>116.95</cell><cell>53.1/51.2</cell></row><row><cell>Ours (full settings)</cell><cell>17.82</cell><cell>165.95</cell><cell>51.5/50.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The ablation on each functional block, where 'MLP', 'AS', and 'SD' represent the token MLP bottleneck, attention sharing scheme, and the self distillation.</figDesc><table><row><cell cols="2">MLP AS SD</cell><cell></cell><cell cols="2">Microscopy</cell><cell></cell><cell></cell><cell cols="2">Histology</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">DSC (%) F1 (%) Acc (%) IoU (%) DSC (%) F1 (%) Acc (%) IoU (%)</cell></row><row><cell>×</cell><cell cols="2">× × 95.31</cell><cell>94.05</cell><cell>96.06</cell><cell>90.05</cell><cell>88.76</cell><cell>78.20</cell><cell>90.96</cell><cell>64.48</cell></row><row><cell>•</cell><cell cols="2">× × 95.49</cell><cell>94.48</cell><cell>95.95</cell><cell>89.97</cell><cell>89.41</cell><cell>77.94</cell><cell>91.02</cell><cell>65.17</cell></row><row><cell>×</cell><cell cols="2">• × 95.88</cell><cell>93.51</cell><cell>96.11</cell><cell>90.55</cell><cell>90.23</cell><cell>80.46</cell><cell>92.03</cell><cell>67.84</cell></row><row><cell>•</cell><cell cols="2">• × 96.95</cell><cell>95.72</cell><cell>96.92</cell><cell>91.98</cell><cell>90.27</cell><cell>81.04</cell><cell>92.01</cell><cell>67.56</cell></row><row><cell>×</cell><cell>× •</cell><cell>96.99</cell><cell>95.74</cell><cell>97.02</cell><cell>92.22</cell><cell>90.25</cell><cell>80.81</cell><cell>92.45</cell><cell>68.14</cell></row><row><cell>•</cell><cell>× •</cell><cell>96.58</cell><cell>95.65</cell><cell>97.03</cell><cell>92.07</cell><cell>90.17</cell><cell>80.62</cell><cell>92.35</cell><cell>67.88</cell></row><row><cell>×</cell><cell>• •</cell><cell>96.89</cell><cell>95.78</cell><cell>97.12</cell><cell>92.08</cell><cell>90.34</cell><cell>80.88</cell><cell>92.49</cell><cell>68.05</cell></row><row><cell>•</cell><cell>• •</cell><cell>97.01</cell><cell cols="2">96.67 97.11</cell><cell>92.97</cell><cell>90.81</cell><cell cols="2">81.52 92.77</cell><cell>69.49</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DCAN: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno>CoRR, abs/1604.02677</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TransuNet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning with noise: mask-guided attention model for weakly supervised nuclei segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pagnucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_43" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CA 2.5 -net nuclei segmentation framework with a microscopy cell benchmark collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_43" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Methods for nuclei detection, segmentation, and classification in digital histopathology: a review-current status and future potential</title>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ClusterSeg: a crowd cluster pinpointed nucleus segmentation framework with cross-modality datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102758</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An annotated fluorescence image dataset for training nuclear segmentation methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kromp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">262</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review and comparison of breast tumor cell nuclei segmentation performances using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lagree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8025</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nucleisegnet: robust deep learning architecture for the nuclei segmentation of liver cancer histopathology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alabhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanfade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>CoRR, abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Federated learning for chronic obstructive pulmonary disease classification with partial personalized attention mechanism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16142</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UneXt: MLP-based rapid medical image segmentation network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DA-Net: dual branch transformer and adaptive strip upsampling for retinal vessels segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_51" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HistoSeg: quick attention with multi-loss function for multistructure segmentation in digital histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Fraz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 12th International Conference on Pattern Recognition Systems (ICPRS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cia-net: robust nuclei instance segmentation with contour-aware information aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno>CoRR, abs/1903.05358</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<editor>Stoyanov, D., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<idno>DLMIA/ML-CDS -2018</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
