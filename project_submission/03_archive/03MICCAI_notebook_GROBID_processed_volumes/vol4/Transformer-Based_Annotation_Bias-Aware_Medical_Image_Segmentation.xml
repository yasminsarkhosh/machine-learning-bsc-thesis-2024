<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer-Based Annotation Bias-Aware Medical Image Segmentation</title>
				<funder ref="#_HAZdnBe">
					<orgName type="full">Key Research and Development Program of Shaanxi Province, China</orgName>
				</funder>
				<funder ref="#_bKwstbF">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_PFRSR94">
					<orgName type="full">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</orgName>
				</funder>
				<funder ref="#_ARgaKQa">
					<orgName type="full">Key Technologies Research and Development Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zehui</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shishuai</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Xie</surname></persName>
							<email>yutong.xie678@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer-Based Annotation Bias-Aware Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="24" to="34"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">438CC947926DCD9A2BB4BE4175BD5ED8</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image segmentation</term>
					<term>Multiple annotators</term>
					<term>Transformer</term>
					<term>Multivariate normal distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Manual medical image segmentation is subjective and suffers from annotator-related bias, which can be mimicked or amplified by deep learning methods. Recently, researchers have suggested that such bias is the combination of the annotator preference and stochastic error, which are modeled by convolution blocks located after decoder and pixelwise independent Gaussian distribution, respectively. It is unlikely that convolution blocks can effectively model the varying degrees of preference at the full resolution level. Additionally, the independent pixel-wise Gaussian distribution disregards pixel correlations, leading to a discontinuous boundary. This paper proposes a Transformer-based Annotation Bias-aware (TAB) medical image segmentation model, which tackles the annotator-related bias via modeling annotator preference and stochastic errors. TAB employs the Transformer with learnable queries to extract the different preference-focused features. This enables TAB to produce segmentation with various preferences simultaneously using a single segmentation head. Moreover, TAB takes the multivariant normal distribution assumption that models pixel correlations, and learns the annotation distribution to disentangle the stochastic error. We evaluated our TAB on an OD/OC segmentation benchmark annotated by six annotators. Our results suggest that TAB outperforms existing medical image segmentation models which take into account the annotator-related bias. The code is available at https://github.com/Merrical/TAB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural networks (DCNNs) have significantly advanced medical image segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. However, their success relies heavily on accurately labeled training data <ref type="bibr" target="#b26">[27]</ref>, which are often unavailable for medical image segmentation tasks since manual annotation is highly subjective and requires the observer's perception, expertise, and concentration <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. In a study of liver lesion segmentation using abdominal CT, three trained observers delineated the lesion twice over a one-week interval, resulting in the variation of delineated areas up to 10% per observer and more than 20% between observers <ref type="bibr" target="#b20">[21]</ref>.</p><p>To analyze the annotation process, it is assumed that a latent true segmentation, called meta segmentation for this study, exists as the consensus of annotators <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Annotators prefer to produce annotations that are different from the meta segmentation to facilitate their own diagnosis. Additionally, stochastic errors may arise during the annotation process. To predict accurate meta segmentation and annotator-specific segmentation, research efforts have been increasingly devoted to addressing the issue of annotator-related bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Existing methods can be categorized into three groups. Annotator decision fusion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> methods model annotators individually and use a weighted combination of multiple predictions as the meta segmentation. Despite their advantages, these methods ignore the impact of stochastic errors on the modeling of annotator-specific segmentation <ref type="bibr" target="#b10">[11]</ref>. Annotator bias disentangling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> methods estimate the meta segmentation and confusion matrices and generate annotator-specific segmentation by multiplying them. Although confusion matrices characterize the annotator bias, their estimation is challenging due to the absence of ground truth. Thus, the less-accurate confusion matrices seriously affect the prediction of meta segmentation. Preference-involved annotation distribution learning (PADL) framework <ref type="bibr" target="#b13">[14]</ref> disentangles annotator-related bias into annotator preference and stochastic errors and, consequently, outperforms previous methods in predicting both meta segmentation and annotatorspecific segmentation. Although PADL has recently been simplified by replacing its multi-branch architecture with dynamic convolutions <ref type="bibr" target="#b5">[6]</ref>, it still has two limitations. First, PADL uses a stack of convolutional layers after the decoder to model the annotator preference, which may not be effective in modeling the variable degrees of preference at the full resolution level, and the structure of this block, such as the number of layers and kernel size, needs to be adjusted by trial and error. Second, PADL adopts the Gaussian assumption and learns the annotation distribution per pixel independently to disentangle stochastic errors, resulting in a discontinuous boundary.</p><p>To address these issues, we advocate extracting the features, on which different preferences focus. Recently, Transformer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> has drawn increasing attention due to its ability to model the long-range dependency. Among its variants, DETR <ref type="bibr" target="#b1">[2]</ref> has a remarkable ability to detect multiple targets at different locations in parallel, since it takes a set of different positional queries as conditions and focuses on features at different positions. Inspired by DETR, we suggest utilizing such queries to represent annotators' preferences, enabling Transformer to extract different preference-focused features. To further address the issue of missing pixel correlation, we suggest using a non-diagonal multivariate normal distribution <ref type="bibr" target="#b18">[19]</ref> to replace the pixel-wise independent Gaussian distribution.</p><p>In this paper, we propose a Transformer-based Annotation Bias-aware (TAB) medical image segmentation model, which can characterize the annotator preference and stochastic errors and deliver accurate meta segmentation and annotator-specific segmentation. TAB consists of a CNN encoder, a Preference Feature Extraction (PFE) module, and a Stochastic Segmentation (SS) head. The CNN encoder performs image feature extraction. The PFE module takes the image feature as input and produces R + 1 preference-focused features in parallel for meta/annotator-specific segmentation under the conditions of R + 1 different queries. Each preference-focused feature is combined with the image feature and fed to the SS head. The SS head produces a multivariate normal distribution that models the segmentation and annotator-related error as the mean and variance respectively, resulting in more accurate segmentation. We conducted comparative experiments on a public dataset (two tasks) with multiple annotators. Our results demonstrate the superiority of the proposed TAB model as well as the effectiveness of each component.</p><p>The main contributions are three-fold. (1) TAB employs Transformer to extract preference-focused features under the conditions of various queries, based on which the meta/annotator-specific segmentation maps are produced simultaneously. (2) TAB uses the covariance matrix of a multivariate normal distribution, which considers the correlation among pixels, to characterize the stochastic errors, resulting in a more continuous boundary. ( <ref type="formula">3</ref>) TAB outperforms existing methods in addressing the issue of annotator-related bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formalization and Method Overview</head><p>Let a set of medical images annotated by R annotators be denoted by</p><formula xml:id="formula_0">D = {x i , y i1 , y i2 , • • • , y iR } N i=1</formula><p>, where x i ∈ R C×H×W represents the i-th image with C channels and a size of H × W , and y ir ∈ {0, 1}</p><p>K×H×W is the annotation with K classes given by the r-th annotator. We simplify the K -class segmentation problem as K binary segmentation problems. Our goal is to train a segmentation model on D so that the model can generate a meta segmentation map and R annotator-specific segmentation maps for each input image.</p><p>Our TAB model contains three main components: a CNN encoder for image feature extraction, a PFE module for preference-focused feature production, and a SS head for segmentation prediction (see Fig. <ref type="figure" target="#fig_0">1</ref>). We now delve into the details of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CNN Encoder</head><p>The ResNet34 <ref type="bibr" target="#b6">[7]</ref> pre-trained on ImageNet is employed as the CNN encoder. We remove its average pooling layer and fully connected layer to adjust it to our tasks. Skip connections are built from the first convolutional block and the first three residual blocks in the CNN encoder to the corresponding locations of the decoder in the SS head. The CNN encoder takes an image x ∈ R C×H×W as its input and generates a high-level low-resolution feature map</p><formula xml:id="formula_1">f img ∈ R C ×H ×W , where C = 512, H = H 32 , W = W 32 .</formula><p>Moreover, f img is fed to a 1×1 convolutional layer for channel reduction, resulting in f re ∈ R d×H ×W , where d = 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PFE Module</head><p>The PFE module consists of an encoder-decoder Transformer and a multi-head attention block. Feeding the image feature f re to the PFE module, we have R+1 enhanced feature maps {f pref r } R r=0 , on which different preferences focus (r = 0 for meta segmentation and others for R annotator-specific segmentation). Note that meta segmentation is regarded as a special preference for simplicity.</p><p>The Transformer Encoder is used to enhance the image feature f re . The Transformer encoder consists of a multi-head self-attention module and a feedforward network. Since the encoder expects a sequence as input, we collapse the spatial dimensions of f re and reshape it into the size of d × H W . Next, f re is added to the fixed positional encodings E pos and fed to the encoder. The output of the Transformer encoder is denoted by f E and its size is d × H W .</p><p>The Transformer Decoder accepts f E and R+1 learnable queries {Q pref r } R r=0 of size d = 256 as its input. We aim to extract different preference-focused features based on the conditions provided by {Q pref r } R r=0 , which are called 'preference queries' accordingly. This decoder consists of a multi-head self-attention module for the intercommunication between queries, a multi-head attention module for feature extraction under the conditions of queries, and a feed-forward network. And it produces R + 1 features {f D r } R r=0 of size d = 256 in parallel.</p><p>Multi-head Attention Block has m heads in it. It takes the output of the Transformer decoder {f D r } R r=0 as its input and computes multi-head attention scores of f D r over the output of the encoder f E , generating m attention heatmaps per segmentation. The output of this block is denoted as {f pref r } R r=0 , and the size of</p><formula xml:id="formula_2">f pref r is m × H × M . Then, {f pref r } R</formula><p>r=0 are individually decoded by SS head, resulting in R + 1 different preference-involved segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">SS Head</head><p>The SS head aims to disentangle the annotator-related bias and produce meta and annotator-specific segmentation maps. Given an input image, we assume that the annotation distribution over annotators follows a multivariant normal distribution. Thus, we can learn the annotation distribution and disentangle the annotator-related bias by modeling it as the covariance matrix that considers pixel correlation. First, we feed the concatenation of f re and f pref r to a CNN decoder, which is followed by batch normalization and ReLU, and obtain the feature map f seg r ∈ R 32×H×W . Second, we establish the multivariant normal distribution N (μ(x), Σ(x)) via predicting the μ(x) ∈ R K×H×W and Σ(x) ∈ R (K×H×W ) 2 based on f seg r . However, the size of the covariance matrix scales with (K × H × W )</p><p>2 , making it computationally intractable. To reduce the complexity, we adopt the low-rank parameterization of the covariance matrix [19]</p><formula xml:id="formula_3">Σ = P × P T + D<label>(1)</label></formula><p>where P ∈ R (K×H×W )×α is the covariance factor, α defines the rank of the parameterization, D is a diagonal matrix with K × H × W diagonal elements.</p><p>We employ three convolutional layers with 1 × 1 kernel size to generate μ(x), P , and D, respectively. In addition, the concatenation of μ(x) and f seg r is fed to the P head and D head, respectively, to facilitate the learning of P and D. Finally, we get R+1 distributions. Among them, N (μ MT (x), Σ MT (x)) is for meta segmentation and N (μ r (x), Σ r (x)), r = 1, 2, ..., R are for annotator-specific segmentation. The probabilistic meta/annotator-specific segmentation map (ŷ MT /ŷ r ) is calculated by applying the sigmoid function to the estimated μ MT /μ r . We can also produce the probabilistic annotator bias-involved segmentation maps ŷs MT /ŷ s r by applying the sigmoid function to the segmentation maps sampled from the established distribution N (μ MT (x), Σ MT (x))/N (μ r (x), Σ r (x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss and Inference</head><p>The loss of our TAB model contains two items: the meta segmentation loss L MT and annotator-specific segmentation loss L AS , shown as follows</p><formula xml:id="formula_4">L = L MT (y s , ŷs MT ) + R r=1 L AS (y r , ŷs r )<label>(2)</label></formula><p>where L MT and L AS are the binary cross-entropy loss, y s is a randomly selected annotation per image, y r is the delineation given by annotator A r .</p><p>During inference, the estimated probabilistic meta segmentation map ŷMT is evaluated against the mean voting annotation. The estimated probabilistic annotator-specific segmentation map ŷr is evaluated against the annotation y r given by the annotator A r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setup</head><p>Dataset. The RIGA dataset <ref type="bibr" target="#b0">[1]</ref> is a public benchmark for optic disc and optic cup segmentation, which contains 750 color fundus images from three sources, including 460 images from MESSIDOR, 195 images from BinRushed, and 95 images from Magrabia. Six ophthalmologists from different centers labeled the optic disc/cup contours manually on each image. We use 655 samples from BinRushed and MESSIDOR for training and 95 samples from Magrabia for test <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. The 20% of the training set that is randomly selected is used for validation.</p><p>Implementation Details. All images were normalized via subtracting the mean and dividing by the standard deviation. The mean and standard deviation were counted on training cases. We set the batch size to 8 and resized the input image to 256 × 256. The Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with default settings was adopted. The learning rate lr was set to 5e -5 and decayed according to the polynomial policy lr = lr 0 ×(1t/T ) 0.9 , where t is the epoch index and T = 300</p><p>is the maximum epoch. All results were reported over three random runs. Both mean and standard deviation are given.</p><p>Evaluation Metrics. We adopted Soft Dice (D s ) as the performance metric.</p><p>At each threshold level, the Hard Dice is calculated between the segmentation and annotation maps. Soft Dice is calculated via averaging the hard Dice values obtained at multiple threshold levels, i.e., (0.1, 0.3, 0.5, 0.7, 0.9) for this study.</p><p>Based on the Soft Dice, there are two performance metrics, namely Average and Mean Voting. Mean Voting is the Soft Dice between the predicted meta segmentation and the mean voting annotation. A higher Mean Voting represents better performance on modeling the meta segmentation. The annotator-specific predictions are evaluated against each annotator's delineations, and the average Soft Dice of all annotators is denoted as Average. A higher Average represents better performance on mimicking all annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparative Experiments</head><p>We compared our TAB to three baseline models and four recent segmentation models that consider the annotator-bias issue, including (1) the baseline 'Multi-Net' setting, under which R U-Nets (denoted by M r , r = 1, 2, ..., R, R = 6 for RIGA) were trained and tested with the annotations provided by annotator A r , respectively; (2) MH-UNet <ref type="bibr" target="#b4">[5]</ref>: a U-Net variant with multiple heads, each Table <ref type="table" target="#tab_1">1</ref>. Performance (D s disc (%), D s cup (%)) of our TAB, seven competing models, and two variants of TAB on the RIGA dataset. From left to right: Performance in mimicking the delineations of each annotator (Ar, r = 1, 2, ..., 6), Average, and Mean Voting. The standard deviation is shown as the subscript of the mean. Except for two variants of TAB and the 'Multi-Net' setting (Mr), the best results in Average and Mean Voting columns are highlighted in blue. accounting for imitating the annotations from a specific annotator; (3) MV-UNet <ref type="bibr" target="#b2">[3]</ref>: a U-Net trained with the mean voting annotations; (4) MR-Net <ref type="bibr" target="#b8">[9]</ref>: an annotator decision fusion method that uses an attention module to characterize the multi-rater agreement; (5) CM-Net <ref type="bibr" target="#b28">[29]</ref>: an annotator bias disentangling method that uses a confusion matrix to model human errors; (6) PADL <ref type="bibr" target="#b13">[14]</ref>: a multi-branch framework that models annotator preference and stochastic errors simultaneously; and (7) AVAP <ref type="bibr" target="#b5">[6]</ref>: a method that uses dynamic convolutional layers to simplify the multi-branch architecture of PADL. Analysis of the SS Head. The effect of each component in the SS head was accessed using Average and Mean Voting. Table <ref type="table" target="#tab_2">2</ref> gives the performance of the TAB with complete SS head and its three variants. We compare the stochastic errors modeling capacity of multivariate normal distribution N (μ, Σ) and the pixel-wise independent Gaussian distribution N (μ, σ) <ref type="bibr" target="#b13">[14]</ref>. Though both N (μ, Σ) and N (μ, σ) can reduce the impact of stochastic errors, N (μ, Σ) performs better than N (μ, σ) on the test set. We also explored the influence of μ prior. It reveals that the μ prior can facilitate the learning of N (μ, Σ) and improve the capacity of meta/annotator-specific segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose the TAB model to address the issue of annotatorrelated bias that existed in medical image segmentation. TAB leverages the Transformer with multiple learnable queries on extracting preference-focused features in parallel and the multivariate normal distribution on modeling stochastic annotation errors. Extensive experimental results on the public RIGA dataset with annotator-related bias demonstrate that TAB achieves better performance than all competing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of TAB model, which consists of a CNN encoder, a PFE module, and a SS head. 'trans.' means the transpose operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,55,98,53,84,340,30,163,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Ours 96.320.05,86.130.18 96.210.07,85.900.17 96.900.08,84.640.14 97.010.07,89.400.12 Oursw/o PFE 95.670.03,81.690.13 95.120.02,80.160.11 96.080.04,79.420.16 96.400.02,78.530.17 Oursw/o SS 96.390.11,84.820.46 95.550.02,83.680.47 96.410.05,82.730.29 96.770.07,88.210.47 UNet 97.090.17,78.690.90 96.820.41,75.890.32 96.540.09,80.840.41 97.390.06,85.270.09 MV-UNet 95.850.04,78.640.20 95.620.02,74.740.15 95.400.02,77.110.28 97.450.04,86.080.12</figDesc><table><row><cell></cell><cell></cell><cell>A2</cell><cell>A3</cell><cell>A4</cell></row><row><cell>Mr</cell><cell cols="4">96.200.05,84.430.16 95.510.02,84.810.36 96.560.04,83.150.11 96.800.03,87.890.09</cell></row><row><cell cols="5">MH-UNet 96.250.29,83.310.49 95.270.10,81.820.24 96.720.28,77.320.35 97.000.11,88.030.29</cell></row><row><cell cols="5">MV-UNet 95.110.02,76.850.34 94.530.05,78.450.44 95.570.03,77.680.22 95.700.02,76.270.56</cell></row><row><cell>MR-Net</cell><cell cols="4">95.330.46,81.960.47 94.720.43,81.130.78 95.650.14,79.040.81 95.940.03,84.132.61</cell></row><row><cell>CM-Net</cell><cell cols="4">96.260.07,84.500.14 95.410.08,81.460.52 96.550.88,81.800.41 96.801.17,87.500.51</cell></row><row><cell>PADL</cell><cell cols="4">96.430.03,85.210.26 95.600.05,85.130.25 96.670.02,82.740.37 96.880.11,88.800.10</cell></row><row><cell>AVAP</cell><cell cols="4">96.200.13,85.790.59 95.440.03,84.500.51 96.470.03,81.650.95 96.820.02,89.610.20</cell></row><row><cell>Methods</cell><cell>A5</cell><cell>A6</cell><cell>Average</cell><cell>Mean Voting</cell></row><row><cell>Mr</cell><cell cols="4">96.700.03,83.270.15 97.000.00,80.450.01 96.460.03,84.000.16 /</cell></row><row><cell>MH-MR-Net</cell><cell cols="4">95.870.20,79.000.25 95.710.06,76.180.27 95.540.19,80.240.70 97.500.07,87.210.19</cell></row><row><cell>CM-Net</cell><cell cols="4">96.281.06,82.430.25 96.851.39,78.770.33 96.360.06,82.740.55 96.410.34,81.210.12</cell></row><row><cell>PADL</cell><cell cols="4">96.800.08,83.420.39 96.890.02,79.760.36 96.530.01,84.320.06 97.770.03,87.770.08</cell></row><row><cell>AVAP</cell><cell cols="4">96.290.04,83.630.35 96.680.02,80.060.48 96.320.13,84.210.46 97.860.02,87.600.27</cell></row><row><cell>Ours</cell><cell cols="4">96.800.06,84.500.13 96.990.04,82.550.16 96.700.04,85.520.06 97.820.04,88.220.07</cell></row><row><cell cols="5">Oursw/o PFE 96.400.04,79.840.15 96.250.01,75.690.16 95.990.05,79.220.14 97.710.04,87.510.14</cell></row><row><cell cols="5">Oursw/o SS 96.630.09,82.880.60 96.810.06,79.900.52 96.430.03,83.710.37 97.180.02,85.360.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>It shows that our TAB achieves the second highest D s</figDesc><table><row><cell>The soft Dice of optic</cell></row></table><note><p><p><p><p>cup</p>on Average. We also visualize the segmentation maps predicted by TAB and other competing methods (see Fig.</p>2</p>). It reveals that TAB can produce the most accurate segmentation map compared to the ground truth. Fig. 2. Visualization of predicted meta segmentation maps obtained by applying six competing methods and our TAB to four cases from the RIGA dataset, together with ground truths (GTs, i.e., mean voting annotations).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>PerformanceBoth the PFE module and SS head play an essential role in the TAB model, characterizing the annotators' preferences and stochastic errors independently. To evaluate the contributions of them, we compared TAB with two of its variants, i.e., 'Ours w/o PFE' and 'Ours w/o SS'. In 'Ours w/o SS', the SS head is replaced by the CNN decoder, which directly converts image features to a segmentation map. 'Ours w/o PFE' contains a CNN encoder and an SS head. This variant is, of course, not able to model the preference of each annotator. The performance of our TAB and its two variants was given in Table1. It reveals that, when the PFE module was removed, the performance in reconstructing each annotator's delineations drops obviously. Meanwhile, without the SS head, Mean Voting score drops from 97.82% to 97.18% for optic disc segmentation and from 88.22% to 85.36% for optic cup segmentation. It indicates that the PFE module contributes substantially to the modeling of each annotator's pref-erence, and the SS head can effectively diminish the impact of stochastic errors and produce accurate meta/annotator-specific segmentation.</figDesc><table><row><cell cols="4">N (μ, σ) N (μ, Σ) μ prior Average</cell><cell>Mean Voting</cell></row><row><cell>√</cell><cell>√ √</cell><cell>√</cell><cell cols="2">96.430.03, 83.710.37 97.180.02, 85.360.67 96.540.02, 84.560.09 97.660.02, 87.180.12 96.670.06, 85.240.12 97.760.07, 87.870.09 96.700.04, 85.520.06 97.820.04, 88.220.07</cell></row><row><cell cols="2">3.3 Ablation Analysis</cell><cell></cell><cell></cell></row></table><note><p>of the TAB with complete SS head and its three variants on the RIGA dataset. The Average and Mean Voting (D s disc (%), D s cup (%)) are used as the performance metrics. The standard deviation is shown as the subscript of the mean.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62171377</rs>, in part by the <rs type="funder">Key Technologies Research and Development Program</rs> under Grant <rs type="grantNumber">2022YFC2009903 / 2022YFC2009900</rs>, in part by the <rs type="funder">Key Research and Development Program of Shaanxi Province, China</rs>, under Grant <rs type="grantNumber">2022GY-084</rs>, and in part by the <rs type="funder">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</rs> under Grant <rs type="grantNumber">CX2022056</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bKwstbF">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funding" xml:id="_ARgaKQa">
					<idno type="grant-number">2022YFC2009903 / 2022YFC2009900</idno>
				</org>
				<org type="funding" xml:id="_HAZdnBe">
					<idno type="grant-number">2022GY-084</idno>
				</org>
				<org type="funding" xml:id="_PFRSR94">
					<idno type="grant-number">CX2022056</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Agreement among ophthalmologists in marking the optic disc and optic cup in fundus images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almazroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="701" to="717" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A retrospective comparison of deep learning to manual annotations for optic disc and optic cup segmentation in fundus photographs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Vision Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Who said what: Modeling individual labelers improves classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling annotator variation and annotator preference for multiple annotations medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning calibrated medical image segmentation via multi-rater agreement modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12341" to="12351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inter-observer variability of manual contour delineation of structures in CT</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Caplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sosna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1391" to="1399" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the effect of inter-observer variability for a reliable estimation of uncertainty of medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jungo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_77</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_77" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="682" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning with noisy labels: exploring techniques and remedies in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101759</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Modeling human preference and stochastic error for medical image segmentation with multiple annotators</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13410</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of deep-learning-based medical image segmentation methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1224</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer V2: scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to segment skin lesions from noisy annotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mirikharaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-33391-1_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-33391-1_24" />
	</analytic>
	<monogr>
		<title level="m">DART/MIL3ID -2019</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11795</biblScope>
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic segmentation networks: modelling spatially correlated aleatoric uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12756" to="12767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding expert disagreement in medical data analysis through structured adjudication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
		<title level="m">Fundamentals of Medical Imaging, 3rd edn</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotation-efficient deep learning for automatic medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5915</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02602</idno>
		<title level="m">Pathological image segmentation with noisy labels</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust optic disc and cup segmentation with deep learning for glaucoma detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kanagasingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to segment when experts disagree</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_18" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="179" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangling human error from ground truth in segmentation of medical images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15750" to="15762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
