<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>She</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hebei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Beijing Tiantan Hospital</orgName>
								<orgName type="institution" key="instit2">Capital Medical University</orgName>
								<address>
									<postCode>100050</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
							<email>sunxiaoyan@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">923D325FCF20419C8DAF388A0D18C24C</idno>
					<idno type="DOI">10.1007/978-3-031-43901-832.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brain tumor segmentation</term>
					<term>Edge-oriented module</term>
					<term>Transformer Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate segmentation of brain tumors in MRI images requires precise detection of the edges. However, this crucial information has been overlooked by existing methods. In this paper, we introduce the Edge-oriented Transformer (EoFormer) which specifically captures and enhances edge information for brain tumor segmentation. Our approach incorporates a CNN-Transformer encoder to comprehensively improve the feature representation capability. The CNN structure captures lowlevel local features in the image, while the Transformer structure establishes long-range dependencies between features to generate high-level global features. Additionally, the decoder of our approach utilizes two edge sharpening modules, the Edge-oriented Sobel and Laplacian modules, which enhance the edge information. We also introduce efficient attention and re-parameterization techniques that make EoFormer computationally efficient. Experimental results on the BraTS 2020 dataset and a private medulloblastoma dataset demonstrate the superiority of our approach compared with existing state-of-the-art methods. Moreover, our method achieves this with limtied model parameters and lower FLOPs, making it a promising approach for future research. The code is available at https://github.com/sd0809/EoFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate segmentation of brain tumors from MRI images is of great significance as it enables more accurate assessment of tumor morphology, size, location, and distribution range, thereby providing clinicians with a reliable basis for diagnosis and treatment <ref type="bibr" target="#b16">[16]</ref>. Physicians manually delineate the tumor regions based on the varying signal intensities between diseased and normal tissues. This signal disparity constitutes the edge information in the images, making it essential for accurate tumor segmentation.</p><p>CNN-based networks, such as UNet <ref type="bibr" target="#b1">[2]</ref>, SegResNet <ref type="bibr" target="#b15">[15]</ref>, and nnUNet <ref type="bibr" target="#b7">[8]</ref>, have made significant progress in the field of medical image segmentation, including brain tumor segmentation. With the emergence of Transformer <ref type="bibr" target="#b19">[19]</ref>, which is capable of modeling long-range dependencies that CNNs struggle with, a number of CNN-Transformer hybrid networks have been proposed, such as TransBTS <ref type="bibr" target="#b21">[21]</ref>, UNETR <ref type="bibr" target="#b6">[7]</ref>, Swin-UNETR <ref type="bibr" target="#b5">[6]</ref> and NestedFormer <ref type="bibr" target="#b23">[23]</ref>, leading to further improvements in brain tumor segmentation. However, the performance of existing brain tumor segmentation methods are still unsatisfactory, especially for the segmentation of edges between tumor lesion and normal tissues.</p><p>Considerable advancement has been achieved in the field of natural image segmentation by focusing on the edge information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b25">25]</ref>, and this idea is also being applied to medical image segmentation. Some methods utilize the distance-dependent objective functions to generate more accurate edge predictions. Karimi et al. <ref type="bibr" target="#b8">[9]</ref> design a Hausdorff-based metric loss function to minimize Hausdorff distance (HD), which is used to measure the edge distance between two point sets. Other methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22]</ref> involve post-processing uncertain regions to more accurately segment pixels near edges. For example, BAT <ref type="bibr" target="#b20">[20]</ref> considers global context to coarsely locate lesion area and paying special attention to the ambiguous area to specify the exact edges of the skin cancers. Similarly, Xie et al. <ref type="bibr" target="#b22">[22]</ref> use the confidence map to evaluate the uncertainty of each pixel to enhance the segmentation of the ambiguous edges of ultrasound images. However, the methods mentioned above are not suitable for brain tumor segmentation for two main reasons. (1) Efficiency. For instance, Karimi et al. <ref type="bibr" target="#b8">[9]</ref> require the calculation of the HD at each iteration, which is both time-consuming and computationally demanding. Moreover, processing every slice of large volumes of MRI images at the pixel-level is impractical. (2) Task Complexity. Unlike many other medical image segmentation tasks that involve the segmentation of a single ROI, brain tumor segmentation requires the simultaneous segmentation of three regions: the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET) regions. Therefore, in addition to focusing on the edge between the tumor lesion and normal tissue to segment the WT, it is also necessary to consider the edges within the tumor in order to segment the TC and ET regions.</p><p>In this paper, we propose an Edge-oriented transFormer (EoFormer), for efficient and accurate brain tumor segmentation. We design a CNN-Transformer based encoder for more effective feature representation, called Efficient Hybrid Encoder (EHE). Specifically, the input image is first processed by the CNN blocks to extract low-level local features. Then, the extracted features are fed into the transformer blocks to create long-range dependencies, resulting in the formation of high-level semantic features. In addition, to provide more accurate edge predictions, we design two edge sharpening modules in the decoder, called Edgeoriented Sobel (EoS) and Laplacian (EoL) modules. By implicitly embedding Sobel and Laplacian filters into the convolution layers to extract 1st-order and 2nd-order differential features, the two modules could enhance the edge information contained in the feature maps. In order to reduce the computational and memory complexity of the model, we replace the vanilla attention module with our extended efficient attention module <ref type="bibr" target="#b17">[17]</ref>. To simplify the model architecture and reduce inference time, we also introduce the re-parameterization technique <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Our model has been evaluated on both the publicly BraTS 2020 dataset and a private medulloblastoma segmentation dataset. The results demonstrate that EoFormer clearly outperforms the state-of-the-art methods with limited model parameters and lower FLOPs (see more in supplementary material). (2) a decoder which incorporates edge-oriented modules to enhance the edge information in features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Efficient Hybrid Encoder</head><p>The EHE, shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), comprises four stages, each of which consists of a feature extraction module and a downsampling module. All four feature extraction modules follow the same paradigm of the general transformer architecture (see Fig. <ref type="figure" target="#fig_1">1(b)</ref>), which regards the attention module in the transformer as a token mixer <ref type="bibr" target="#b24">[24]</ref>. In the first two stages of EHE, we use depth-wise convolution (DWConv) to instantiate the token mixer, called the ConvFormer block. In the third stage and bottleneck, we use the multi-head self-attention (MSA) to instantiate the token mixer, which is the typical transformer block. For each stage i, given an input feature map X, the output of the i th block X is computed as follows:</p><formula xml:id="formula_0">X = TokenMixer i (Norm(X)) + X, X = MLP(Norm(X )) + X , (1)</formula><p>where the TokenMixer i (•) corresponds to DWConv (i ∈ {0, 1}) and MSA (i ∈ {2, 3}), Norm( • ) represents layer normalization, and MLP(•) denotes the Multilayer Perceptron. Our approach combines the strengths of CNN and transformer to create a more powerful encoder that can extract both local and global information from input data.</p><p>We address the computational and memory complexity issues that arise from 3D input by replacing the vanilla attention with our extended 3D efficient attention. Assuming the size of the input feature is n and the dimensionality is d, the input feature X ∈ R n×d pass through three linear layers to generate the queries</p><formula xml:id="formula_1">Q ∈ R n×d k , keys K ∈ R n×d k and values V ∈ R n×dv . The vanilla attention D(•)</formula><p>and the efficient attention E(•) are computed as follows:</p><formula xml:id="formula_2">D(Q,K,V) = ρ(QK T )V, E(Q,K,V) = ρ(Q)(ρ(K) T V ),<label>(2)</label></formula><p>where ρ(•) is the softmax activation function, T represents the matrix transpose operator. The efficient attention reduces the memory complexity and computational complexity of vanilla attention from O(n 2 ) and O(dn 2 ) to O(dn + d 2 ) and O(nd 2 ), where</p><formula xml:id="formula_3">d = d v = 2d k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Edge-Oriented Transformer Decoder</head><p>We design the EoFormer block (see Fig. <ref type="figure" target="#fig_1">1(c</ref>)) in the decoder, which instantiates the token mixer with our proposed Edge-oriented Sobel module (EoS) and Edgeoriented Laplacian module (EoL). Each edge-oriented module includes a normal 3 × 3 × 3 convolution and an edge detection path to extract the 1st-order or the 2nd-order spatial derivatives from intermediate features. This design allows the edge-oriented module to efficiently extract the edges and textures of the features. Moreover, to boost the segmentation performance without sacrificing efficiency, we incorporate the re-parameterization technique in the decoder.</p><p>Edge-Oriented Sobel Module. We use a dual-branch structure, where the input feature X is simultaneously processed by two different branches. The first branch contains a 3 × 3 × 3 convolution that extracts basic features from the input. The second branch, which is responsible for edge extraction, first uses a C × C × 1 × 1 × 1 convolution to enhance the interaction between channel features of X, then utilizes a learnable scaled Sobel filter to extract the 1storder differentiation edge information from X. This filter is capable of detecting edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so it comprises three filters M x , M y , and M z , each of which is represented by a 3 × 3 × 3 array. Take M x as an example, which is described as:</p><formula xml:id="formula_4">Mx[0, :, :] = ⎡ ⎣ -1 0 +1 -2 0 +2 -1 0 +1 ⎤ ⎦ , Mx[1, :, :] = ⎡ ⎣ -2 0 +2 -4 0 +4 -2 0 +2 ⎤ ⎦ , Mx[2, :, :] = ⎡ ⎣ -1 0 +1 -2 0 +2 -1 0 +1 ⎤ ⎦ .</formula><p>We then apply a learnable scaling matrix S ∈ R C×1×1×1 to M x , which allows for dynamic adjustment of the scaling factor in each channel. The resulting feature extracted from the scaled Sobel-x filter is denoted as:</p><formula xml:id="formula_5">F x = DWConv S•Mx (Conv 1×1×1 (X)),<label>(3)</label></formula><p>where the '•' denotes channel-wise multiplication; the DWConv S•Mx indicates that DWConv(•) applies a S • M x learnable scaled filter as its kernel weight. Similarly, F y and F z are processed in the same way. The final output of the EoS module, denoted as F sob , is given by:</p><formula xml:id="formula_6">F sob = Norm(Conv 3×3×3 (X) + F x + F y + F z ).<label>(4)</label></formula><p>Edge-Oriented Laplacian Module. Different from the Sobel filter that only extracts edges in the horizontal, vertical, and orthogonal directions, the Laplacian filter can extract edges in all directions. After extracting the 1st-order differentiation edge information, the intermediate features are then fed into the EoL module for extracting the 2nd-order differentiation edge information. Similarly, the feature F , obtained from the learnable scaling Laplacian filter, and the final output of the EoL module, denoted as F lap , are defined as:</p><formula xml:id="formula_7">F = DWConv S•M lap (Conv 1×1×1 (X)), F lap = Norm(Conv 3×3×3 (X) + F). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Re-parameterization of the Edge-Oriented Modules. We introduce the re-parameterization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> into the edge-oriented modules to boost the segmentation performance while maintaining high efficiency. Specifically, we explain the re-parameterization of the EoL module as follows:</p><formula xml:id="formula_9">W rep = W conv3×3×3 + W conv1×1×1 * W conv lap<label>(6)</label></formula><formula xml:id="formula_10">B rep = B conv3×3×3 + W conv lap * up(B conv1×1×1 ) + B conv lap<label>(7)</label></formula><p>where ' * ' represents the convolution operation, W conv means the weights of the convolution and B conv denotes the bias, and up(•) is the spatial broadcasting operation ,which upgrades the bias B ∈ R 1×C×1×1×1 into up(B) ∈ R 1×C×3×3×3 . In the inference stage, the output feature F is produced by a normal 3 × 3 × 3 convolution as follows: </p><formula xml:id="formula_11">F = W rep * X + B rep . (<label>8</label></formula><formula xml:id="formula_12">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metric</head><p>In order to validate the performance of EoFormer, we conduct extensive experiments on both the publicly available BraTS 2020 dataset and a private medulloblastoma segmentation dataset (MedSeg). The BraTS 2020 dataset <ref type="bibr" target="#b14">[14]</ref> consists of MRI image data from 369 patients, with each patient having four modalities (T1, T1ce, T2 and T2-FLAIR) of skull-striped MRI, which are aligned to a standard brain template. The training/validation/test split follows 315/16/37 according to recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>The MedSeg dataset includes MRI images of T1, T1ce, T2, and T2 FLAIR modalities from 255 patients with medulloblastoma. The dataset includes manual annotations of the WT and ET regions. These annotated masks are reviewed by two experienced physicians to ensure the accuracy of the annotated results. The images are registered to the size of 24 × 256 × 256. The training/validation/test split ratio is 3:1:1. Four-fold cross-validation is performed on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implement EoFormer in Pytorch 1.11. Our model is trained from scratch for 300 epochs using two NVIDIA GTX 3090 GPUs. We select a combination of soft dice loss and cross-entropy as the loss function and utilize the AdamW optimizer <ref type="bibr" target="#b13">[13]</ref> with a weight decay of 1×10 -5 . The initial learning rate is 2×10 -4 . For data augmentation, we apply image croping, flipping, identity scaling and shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We compare EoFormer with seven methods, including CNN-based methods (3D-UNet <ref type="bibr" target="#b1">[2]</ref>, SegResNet <ref type="bibr" target="#b15">[15]</ref> and nnUNet <ref type="bibr" target="#b7">[8]</ref>) and Transformer-based methods (TransBTS <ref type="bibr" target="#b21">[21]</ref>, UNETR <ref type="bibr" target="#b6">[7]</ref>, Swin-UNETR <ref type="bibr" target="#b5">[6]</ref>, NestedFormer <ref type="bibr" target="#b23">[23]</ref>). The results are reproduced on our data split.</p><p>Table <ref type="table" target="#tab_0">1</ref> displays the performance comparison of EoFormer against other methods on the BraTS 2020 dataset. EoFormer achieves the highest Dice scores on TC, ET, and the average. In addition, EoFormer attains the best HD95 scores on TC, ET, and the average. HD95 measures the edge distance between prediction and annotation, which is more sensitive to boundaries. Table <ref type="table" target="#tab_1">2</ref> illustrates the performance of EoFormer and other methods on MedSeg. EoFormer outperforms the second-ranked NestedFormer by an average of 1.59% on Dice and achieves the top performance for both WT and ET on HD95. Furthermore, compared to the second-ranked SegResNet, EoFormer demonstrates an </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation</head><p>We evaluate the effectiveness of our proposed EoFormer framework by conducting ablation experiments on the BraTS 2020. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose the EoFormer, a novel approach for brain tumor segmentation. Our method comprises the Efficient Hybrid Encoder and the Edgeoriented Transformer Decoder. The encoder effectively extracts features from images by striking a balance between CNN and Transformer architectures. The decoder integrates the Sobel and Laplacian edge detection filters into our edgeoriented modules that enhance the extraction capability of edge and texture information. Besides, we introduce the efficient attention mechanism and the re-parameterization technology to improve the model efficiency. Our EoFormer outperforms other state-of-the-art methods on both BraTS 2020 and MedSeg. Our model is computationally efficient and can be readily applied to other 3D medical image segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Overview of the proposed EoFormer. (b) The general transformer architecture and its variants. (c) The EoFormer block, the Edge-oriented Sobel Module (EoS) and the Edge-oriented Laplacian Module (EoL).</figDesc><graphic coords="3,55,98,53,99,340,15,223,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1(a) presents an overview of the proposed EoFormer architecture, which comprises two components: (1) an EHE encoder and bottleneck which are used to capture low-level local features and learn a comprehensive feature representation.(2) a decoder which incorporates edge-oriented modules to enhance the edge information in features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative Comparison of segmentation results on BraTS and MedSeg. The red region represents WT, the yellow means TC and the white denotes ET (Color figure online).</figDesc><graphic coords="7,56,97,54,56,338,20,211,15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison on BraTS 2020 dataset. The ' * ' means the FLOPs we recalculate.</figDesc><table><row><cell>Method</cell><cell cols="2">Param(M) FLOPs(G)</cell><cell></cell><cell cols="2">Dice (%) ↑</cell><cell></cell><cell></cell><cell>HD95 (mm) ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>Ave</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>Ave</cell></row><row><cell>3D-UNet [2]</cell><cell>5.75</cell><cell>1449.59</cell><cell cols="6">90.01 80.68 79.18 83.29 8.591 10.91 5.932 8.477</cell></row><row><cell>SegResNet [15]</cell><cell>18.79</cell><cell>185.23</cell><cell cols="6">87.59 85.50 81.20 84.77 4.941 4.653 3.822 4.472</cell></row><row><cell>nnUNet [8]</cell><cell>5.75</cell><cell>1449.59</cell><cell cols="6">91.15 86.12 81.67 86.32 3.532 4.901 3.561 3.998</cell></row><row><cell>TransBTS [21]</cell><cell>32.99</cell><cell>333.00</cell><cell cols="6">90.54 84.93 79.91 85.12 3.916 4.843 4.501 4.420</cell></row><row><cell>UNETR [7]</cell><cell>102.06</cell><cell>203.32</cell><cell cols="6">90.77 84.11 79.12 84.67 4.917 5.054 3.943 4.638</cell></row><row><cell cols="2">Swin-UNETR [6] 61.98</cell><cell>793.92</cell><cell cols="6">91.50 84.06 80.98 85.51 3.386 5.080 3.640 4.035</cell></row><row><cell cols="2">NestedFormer [23] 10.48</cell><cell>209.58  *</cell><cell cols="6">91.07 85.30 80.10 85.49 3.583 4.735 4.391 4.236</cell></row><row><cell>EoFormer</cell><cell>6.28</cell><cell>91.81</cell><cell cols="6">90.84 86.38 83.22 86.81 3.974 4.500 3.432 3.968</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison on MedSeg dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Dice (%) ↑</cell><cell></cell><cell cols="2">HD95 (mm) ↓</cell></row><row><cell></cell><cell>WT</cell><cell>ET</cell><cell>Ave</cell><cell>WT</cell><cell>ET</cell><cell>Ave</cell></row><row><cell>3D-UNet [2]</cell><cell cols="6">61.52 50.71 56.11 17.43 14.62 16.03</cell></row><row><cell>SegResNet [15]</cell><cell cols="6">76.76 55.60 66.18 7.810 9.411 8.611</cell></row><row><cell>TransBTS [21]</cell><cell cols="6">72.35 55.56 63.96 11.09 11.19 11.14</cell></row><row><cell>UNETR [7]</cell><cell cols="6">73.38 56.02 64.70 9.112 12.70 10.90</cell></row><row><cell cols="7">Swin-UNETR [6] 70.10 60.79 65.44 9.766 9.339 9.552</cell></row><row><cell cols="7">NestedFormer [23] 79.89 55.76 67.83 7.099 12.08 9.587</cell></row><row><cell>EoFormer</cell><cell cols="6">79.74 59.10 69.42 6.978 7.104 7.041</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the encoder and decoder design. It is worth mentioning that EoFormer has the lowest FLOPs and limited model size. Fig.2represents the segmentation results on the BraTS 2020 and MedSeg datasets. The visualisation demonstrates that the EoFormer achieves the closest segmentation results to the ground truth. Specifically, EoFormer accurately segments both TC and ET region boundaries.</figDesc><table><row><cell cols="3">Index Encoder/Decoder Dice (%) ↑</cell><cell></cell><cell></cell><cell cols="2">HD95 (mm) ↓</cell><cell></cell></row><row><cell></cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>Ave</cell><cell>WT</cell><cell>TC</cell><cell>ET</cell><cell>Ave</cell></row><row><cell>Enc1 UNet encoder</cell><cell cols="8">83.29 83.43 79.01 84.00 6.232 8.018 6.697 6.983</cell></row><row><cell>Enc2 CF×4</cell><cell cols="8">88.51 83.42 82.56 84.83 7.131 8.772 5.641 7.181</cell></row><row><cell>Enc3 CF×2+TF×2</cell><cell cols="8">90.92 86.63 81.05 86.20 3.906 5.227 5.637 4.923</cell></row><row><cell>Enc4 CF×2+ETF×2</cell><cell cols="8">90.84 86.38 83.22 86.81 3.974 4.500 3.432 3.968</cell></row><row><cell>Enc5 ETF×4</cell><cell cols="8">90.24 84.15 85.40 86.59 3.951 5.651 3.405 4.336</cell></row><row><cell>Dec1 UNet decoder</cell><cell cols="8">89.75 84.09 80.12 84.66 7.562 7.332 6.427 7.107</cell></row><row><cell>Dec2 IF×3</cell><cell cols="8">90.27 86.07 80.09 85.48 5.448 6.069 4.929 5.482</cell></row><row><cell>Dec3 CF×3</cell><cell cols="8">90.63 85.63 81.61 85.96 4.098 4.467 3.023 4.842</cell></row><row><cell>Dec4 EoF×3</cell><cell cols="8">90.84 86.38 83.22 86.81 3.974 4.500 3.432 3.968</cell></row><row><cell cols="9">average HD95 improvement of 1.57 mm, highlighting its superior performance</cell></row><row><cell cols="2">in tumor boundary prediction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>, the abbreviations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Encoder and Bottleneck Design.</head><label></label><figDesc>We compare our proposed EHE with different encoders and bottleneck in Table3. Enc1 utilizes UNet encoder. Enc2 -5 have the same encoder and bottleneck as EHE but with different configurations. Our results show that EHE outperforms other methods, with high average Dice and low average HD95. This is because a full CNN encoder (Enc2) is not good at capturing global dependencies, while a full Transformer encoder (Enc5) is inadequate at capturing low-level features. Our proposed EHE balances the strengths of both and achieves the best segmentation performance. Additionally, our extended efficient attention achieves better performance compared with Enc3 because it has a better ability to capture the periphery of objects<ref type="bibr" target="#b17">[17]</ref>. We compare the performance of various decoders in Table3. Dec1 -4 share the same EHE encoder, but employ different decoders: Dec1 uses the UNet decoder, Dec2 has three IdentityFormer blocks, Dec3 replaces the IdentityFormer blocks with ConvFormer blocks, and Dec4 is our proposed EoFormer decoder. Our results show that the EoFormer decoder achieves the highest Dice scores, and achieves the lowest average HD95 score due to the incorporation of the EoS and EoL modules within the EoFormer block.</figDesc><table><row><cell>Decoder Design.</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boundary-assisted region proposal networks for nucleus segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-127" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6819" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ACNet: strengthening the kernel skeletons for powerful CNN via asymmetric convolution blocks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1911" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RepVGG: making VGGstyle convnets great again</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-222" />
	</analytic>
	<monogr>
		<title level="m">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09-27">27 September 2021. 2022</date>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the hausdorff distance in medical image segmentation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Salcudean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="499" to="513" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orthogonal ensemble networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Larrazabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_56</idno>
		<idno>978-3-030-87199-4 56</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via decoupled body and edge supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-58520-4_26</idno>
		<idno>978-3-030-58520-4 26</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">12362</biblScope>
			<biblScope unit="page" from="435" to="452" />
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BSDA-Net: a boundary shape and distance aware joint learning framework for segmenting and classifying OCTA images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D MRI brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11726-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11726-928" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2018, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11384</biblScope>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated medical image segmentation techniques</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Phys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient attention: attention with linear complexities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3531" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look closer to segment better: boundary patch refinement for instance segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13926" to="13935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boundary-aware transformers for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-220" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-211" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cascade network for ultrasound image segmentation with ambiguous boundary</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-826" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NestedFormer: nested modalityaware transformer for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-914" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MetaFormer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boundary-aware CNN for semantic segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="114520" to="114528" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
