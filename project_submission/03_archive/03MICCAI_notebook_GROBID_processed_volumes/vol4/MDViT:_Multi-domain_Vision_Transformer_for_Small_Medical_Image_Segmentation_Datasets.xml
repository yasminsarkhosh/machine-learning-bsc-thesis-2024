<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyi</forename><surname>Du</surname></persName>
							<idno type="ORCID">0000-0002-9961-4533</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nourhan</forename><surname>Bayasi</surname></persName>
							<email>nourhanb@ece.ubc.ca</email>
							<idno type="ORCID">0000-0003-4653-6081</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ghassan</forename><surname>Hamarneh</surname></persName>
							<email>hamarneh@sfu.ca</email>
							<idno type="ORCID">0000-0001-5040-7448</idno>
							<affiliation key="aff1">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafeef</forename><surname>Garbi</surname></persName>
							<email>rafeef@ece.ubc.ca</email>
							<idno type="ORCID">0000-0001-6224-0876</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="448" to="458"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F0BE7454381F92E93AB77DB2E8ACE481</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision Transformer</term>
					<term>Data-efficiency</term>
					<term>Multi-domain Learning</term>
					<term>Medical Image Segmentation</term>
					<term>Dermatology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite its clinical utility, medical image segmentation (MIS) remains a daunting task due to images' inherent complexity and variability. Vision transformers (ViTs) have recently emerged as a promising solution to improve MIS; however, they require larger training datasets than convolutional neural networks. To overcome this obstacle, data-efficient ViTs were proposed, but they are typically trained using a single source of data, which overlooks the valuable knowledge that could be leveraged from other available datasets. Naïvly combining datasets from different domains can result in negative knowledge transfer (NKT), i.e., a decrease in model performance on some domains with non-negligible inter-domain heterogeneity. In this paper, we propose MDViT, the first multi-domain ViT that includes domain adapters to mitigate data-hunger and combat NKT by adaptively exploiting knowledge in multiple small data resources (domains). Further, to enhance representation learning across domains, we integrate a mutual knowledge distillation paradigm that transfers knowledge between a universal network (spanning all the domains) and auxiliary domain-specific network branches. Experiments on 4 skin lesion segmentation datasets show that MDViT outperforms state-of-the-art algorithms, with superior segmentation performance and a fixed model size, at inference time, even as more domains are added. Our code is available at https://github.com/siyi-wind/MDViT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation (MIS) is a crucial component in medical image analysis, which aims to partition an image into distinct regions (or segments) that are semantically related and/or visually similar. This process is essential for clinicians to, among others, perform qualitative and quantitative assessments of various anatomical structures or pathological conditions and perform imageguided treatments or treatment planning <ref type="bibr" target="#b1">[2]</ref>. Vision transformers (ViTs), with their inherent ability to model long-range dependencies, have recently been considered a promising technique to tackle MIS. They process images as sequences of patches, with each patch having a global view of the entire image. This enables a ViT to achieve improved segmentation performance compared to traditional convolutional neural networks (CNNs) on plenty of segmentation tasks <ref type="bibr" target="#b16">[16]</ref>. However, due to the lack of inductive biases, such as weight sharing and locality, ViTs are more data-hungry than CNNs, i.e., require more data to train <ref type="bibr" target="#b31">[31]</ref>. Meanwhile, it is common to have access to multiple, diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for the same MIS task, e.g., PH2 <ref type="bibr" target="#b25">[25]</ref> and ISIC 2018 <ref type="bibr" target="#b11">[11]</ref> in dermatology, LiTS <ref type="bibr" target="#b6">[6]</ref> and CHAOS <ref type="bibr" target="#b18">[18]</ref> in liver CT, or OASIS <ref type="bibr" target="#b24">[24]</ref> and ADNI <ref type="bibr" target="#b17">[17]</ref> in brain MRI. As each dataset alone is too small to properly train a ViT, the challenge becomes how to effectively leverage the different datasets. Various strategies have been proposed to address ViTs' data-hunger (Table <ref type="table" target="#tab_0">1</ref>), mainly: Adding inductive bias by constructing a hybrid network that fuses a CNN with a ViT <ref type="bibr" target="#b39">[39]</ref>, imitating CNNs' shifted filters and convolutional operations <ref type="bibr" target="#b7">[7]</ref>, or enhancing spatial information learning <ref type="bibr" target="#b22">[22]</ref>; sharing knowledge by transferring knowledge from a CNN <ref type="bibr" target="#b31">[31]</ref> or pertaining ViTs on multiple related tasks and then fine-tuning on a down-stream task <ref type="bibr" target="#b37">[37]</ref>; increasing data via augmentation <ref type="bibr" target="#b34">[34]</ref>; and non-supervised pre-training <ref type="bibr" target="#b8">[8]</ref>. Nevertheless, one notable limitation in these approaches is that they are not universal, i.e., they rely on separate training for each dataset rather than incorporate valuable knowledge from related domains. As a result, they can incur additional training, inference, and memory costs, which is especially challenging when dealing with multiple small datasets in the context of MIS tasks. Multi-domain learning, which trains a single universal model to tackle all the datasets simultaneously, has been found promising for reducing computational demands while still leveraging information from multiple domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21]</ref>. To the best of our knowledge, multi-domain universal models have not yet been investigated for alleviating ViTs' data-hunger.</p><p>Given the inter-domain heterogeneity resulting from variations in imaging protocols, scanner manufacturers, etc. <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b21">21]</ref>, directly mixing all the datasets for training, i.e., joint training, may improve a model's performance on one dataset while degrading performance on other datasets with non-negligible unrelated domain-specific information, a phenomenon referred to as negative knowledge transfer (NKT) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">38]</ref>. A common strategy to mitigate NKT in computer vision is to introduce adapters aiding the model to adapt to different domains, i.e., multi-domain adaptive training (MAT), such as domain-specific mechanisms <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref>, and squeeze-excitation layers <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b35">35]</ref> (Table <ref type="table" target="#tab_0">1</ref>). However, those MAT techniques are built based on CNN rather than ViT or are scalable, i.e., the models' size at the inference time increases linearly with the number of domains.</p><p>To address ViTs' data-hunger, in this work, we propose MDViT, a novel fixedsize multi-domain ViT trained to adaptively aggregate valuable knowledge from multiple datasets (domains) for improved segmentation. In particular, we introduce a domain adapter that adapts the model to different domains to mitigate negative knowledge transfer caused by inter-domain heterogeneity. Besides, for better representation learning across domains, we propose a novel mutual knowledge distillation approach that transfers knowledge between a universal network (spanning all the domains) and additional domain-specific network branches.</p><p>We summarize our contributions as follows: (1) To the best of our knowledge, we are the first to introduce multi-domain learning to alleviate ViTs' data-hunger when facing limited samples per dataset. <ref type="bibr" target="#b1">(2)</ref> We propose a multi-domain ViT, MDViT, for medical image segmentation with a novel domain adapter to counteract negative knowledge transfer and with mutual knowledge distillation to enhance representation learning. <ref type="bibr" target="#b2">(3)</ref> The experiments on 4 skin lesion segmentation datasets show that our multi-domain adaptive training outperforms separate and joint training (ST and JT), especially a 10.16% improvement in IOU on the skin cancer detection dataset compared to ST and that MDViT outperforms state-of-the-art data-efficient ViTs and multi-domain learning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Let X ∈ R H×W ×3 be an input RGB image and Y ∈ {0, 1} H×W be its groundtruth segmentation mask. Training samples {(X, Y )} come from M datasets, each representing a domain. We aim to build and train a single ViT that performs well on all domain data and addresses the insufficiency of samples in any of the datasets. We first introduce our baseline (BASE), a ViT with hierarchical transformer blocks (Fig. <ref type="figure" target="#fig_0">1-a</ref>). Our proposed MDViT extends BASE with 1) a domain adapter (DA) module inside the factorized multi-head self-attention (MHSA) to adapt the model to different domains (Fig. <ref type="figure" target="#fig_0">1-b,c</ref>), and 2) a mutual knowledge distillation (MKD) strategy to extract more robust representations across domains (Fig. <ref type="figure" target="#fig_0">1-d</ref>). We present the details of MDViT in Sect. 2.1. BASE is a U-shaped ViT based on the architecture of U-Net <ref type="bibr" target="#b27">[27]</ref> and pyramid ViTs <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b19">19]</ref>. It contains encoding (the first four) and decoding (the last four) transformer blocks, a two-layer CNN bridge, and skip connections. As described in <ref type="bibr" target="#b19">[19]</ref>, the ith transformer block involves a convolutional patch embedding layer with a patch size of 3 × 3 and L i transformer layers with factorized MHSA in linear complexity, the former of which converts a feature map X i-1 into a sequence of patch embeddings z i ∈ R Ni×Ci , where</p><formula xml:id="formula_0">N i = H 2 i+1 W 2 i+1 , 1 ≤ i ≤ 4</formula><p>is the number of patches and C i is the channel dimension. We use the same position embedding as <ref type="bibr" target="#b19">[19]</ref> and skip connections as <ref type="bibr" target="#b27">[27]</ref>. To reduce computational complexity, following <ref type="bibr" target="#b19">[19]</ref>, we add two and one CNN layer before and after transformer blocks, respectively, enabling the 1st transformer block to process features starting from a lower resolution: H 4 × W 4 . We do not employ integrated and hierarchical CNN backbones, e.g., ResNet, in BASE as data-efficient hybrid ViTs <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b39">39]</ref>, to clearly evaluate the efficacy of multi-domain learning in mitigating ViTs' data-hunger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MDViT</head><p>MDViT consists of a universal network (spanning M domains) and M auxiliary network branches, i.e., peers, each associated with one of the M domains. The universal network is the same as BASE, except we insert a domain adapter (DA) in each factorized MHSA to tackle negative knowledge transfer. Further, we employ a mutual knowledge distillation (MKD) strategy to transfer domain-specific and shared knowledge between peers and the universal network to enhance representation learning. Next, we will introduce DA and MKD in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adapter (DA):</head><p>In multi-domain adaptive training, some methods build domain-specific layers in parallel with the main network <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref>. Without adding domain-specific layers, we utilize the existing parallel structure in ViTs, i.e., MHSA, for domain adaptation. The H parallel heads of MHSA mimic how humans examine the same object from different perspectives <ref type="bibr" target="#b10">[10]</ref>. Similarly, our intuition of inserting the DA into MHSA is to enable the different heads to have varied perspectives across domains. Rather than manually designate each head to one of the domains, guided by a domain label, MDViT learns to focus on the corresponding features from different heads when encountering a domain. DA contains two steps: Attention Generation and Information Selection (Fig. <ref type="figure" target="#fig_0">1-c</ref>).</p><p>Attention Generation generates attention for each head. We first pass a domain label vector m (we adopt one-hot encoding m ∈ R M but other encodings are possible) through one linear layer with a ReLU activation function to acquire a domain-aware vector d ∈ R K r . K is the channel dimension of features from the heads. We set the reduction ratio r to 2. After that, similar to <ref type="bibr" target="#b20">[20]</ref>, we calculate attention for each head:</p><formula xml:id="formula_1">a h = ψ(W h d) ∈ R K , h = 1, 2, ...H,</formula><p>where ψ is a softmax operation across heads and</p><formula xml:id="formula_2">W h ∈ R K× K r .</formula><p>Information Selection adaptively selects information from different heads. After getting the feature</p><formula xml:id="formula_3">U h = [u h 1 , u h 2 , ..., u h K ] ∈ R N ×K</formula><p>from the hth head, we utilize a h to calibrate the information along the channel dimension: ũh</p><formula xml:id="formula_4">k = a h k • u h k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual Knowledge Distillation (MKD):</head><p>Distilling knowledge from domainspecific networks has been found beneficial for universal networks to learn more robust representations <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b40">40]</ref>. Moreover, mutual learning that transfers knowledge between teachers and students enables both to be optimized simultaneously <ref type="bibr" target="#b15">[15]</ref>. To realize these benefits, we propose MKD that mutually transfers knowledge between auxiliary peers and the universal network. In Fig. <ref type="figure" target="#fig_0">1-d</ref>, the mth auxiliary peer is only trained on the mth domain, producing output Ŷ m , whereas the universal network's output is Ŷ . Similar to <ref type="bibr" target="#b21">[21]</ref>, we utilize a symmetric Dice loss L am mkd = Dice( Ŷ , Ŷ m ) as the knowledge distillation loss. Each peer is an expert in a certain domain, guiding the universal network to learn domain-specific information. The universal network experiences all the domains and grasps the domain-shared knowledge, which is beneficial for peer learning.</p><p>Each Auxiliary Peer is trained on a small, individual dataset specific to that peer (Fig. <ref type="figure" target="#fig_0">1-d</ref>). To achieve a rapid training process and prevent overfitting, particularly when working with numerous training datasets, we adapt a lightweight multilayer perception (MLP) decoder designed for ViT encoders <ref type="bibr" target="#b36">[36]</ref> to our peers' architecture. Specifically, multi-level features from the encoding transformer blocks (Fig. <ref type="figure" target="#fig_0">1-a</ref>) go through an MLP layer and an up-sample operation to unify the channel dimension and resolution to H 4 × W 4 , which are then concatenated with the feature involving domain-shared information from the universal network's last transformer block. Finally, we pass the fused feature to an MLP layer and do an up-sample to obtain a segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Objective Function</head><p>Similar to Combo loss <ref type="bibr" target="#b29">[29]</ref>, BASE's segmentation loss combines Dice and binary cross entropy loss: L seg = L Dice +L bce . In MDViT, we use the same segmentation loss for the universal network and auxiliary peers, denoted as L u seg and L a seg , respectively. The overall loss is calculated as follows.</p><formula xml:id="formula_5">L total = L u seg (Y , Ŷ ) + α M m=1 L am seg (Y , Ŷ m ) + β M m=1 L am mkd ( Ŷ , Ŷ m ). (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>We set both α and β to 0.5. L am seg does not optimize DA to avoid interfering with the domain adaptation learning. After training, we discard the auxiliary peers and only utilize the universal network for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Evaluation Metrics:</head><p>We study 4 skin lesion segmentation datasets collected from varied sources: ISIC 2018 (ISIC) <ref type="bibr" target="#b11">[11]</ref>, Dermofit Image Library (DMF) <ref type="bibr" target="#b2">[3]</ref>, Skin Cancer Detection (SCD) <ref type="bibr" target="#b14">[14]</ref>, and PH2 <ref type="bibr" target="#b25">[25]</ref>, which contain 2594, 1300, 206, and 200 samples, respectively. To facilitate a fairer performance comparison across datasets, as in <ref type="bibr" target="#b4">[4]</ref>, we only use the 1212 images from DMF that exhibited similar lesion conditions as those in other datasets. We perform 5-fold cross-validation and utilize Dice and IOU metrics for evaluation as <ref type="bibr" target="#b33">[33]</ref>.</p><p>Implementation Details: We conduct 3 training paradigms: separate (ST), joint (JT), and multi-domain adaptive training (MAT), described in Sect. 1, to train all the models from scratch on the skin datasets. Images are resized to 256 × 256 and then augmented through random scaling, shifting, rotation, flipping, Gaussian noise, and brightness and contrast changes. The encoding transformer blocks' channel dimensions are [64, 128, 320, 512] (Fig. <ref type="figure" target="#fig_0">1-a</ref>). We use two transformer layers in each transformer block and set the number of heads in MHSA to 8. The hidden dimensions of the CNN bridge and auxiliary peers are 1024 and 512. We deploy models on a single TITAN V GPU and train them for 200 epochs with the AdamW <ref type="bibr" target="#b23">[23]</ref> optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an initial learning rate of 1×10 -4 , which changes through a linear decay scheduler whose step size is 50 and decay factor γ = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing Against BASE:</head><p>In Table <ref type="table" target="#tab_1">2</ref>-a,b, compared with BASE in ST, BASE in JT improves the segmentation performance on small datasets (PH2 and SCD) but at the expense of diminished performance on larger datasets (ISIC and DMF). This is expected given the non-negligible inter-domain heterogeneity between skin lesion datasets, as found by Bayasi et al. <ref type="bibr" target="#b5">[5]</ref>. The above results demonstrate that shared knowledge in related domains facilitates training a ViT on small datasets while, without a well-designed multi-domain algorithm, causing negative knowledge transfer (NKT) due to inter-domain heterogeneity, i.e., the model's performance decreases on other datasets. Meanwhile, MDViT fits all the domains without NKT and outperforms BASE in ST by a large margin; significantly increasing Dice and IOU on SCD by 6.4% and 10.16%, showing that MDViT smartly selects valuable knowledge when given data from a certain domain. Additionally, MDViT outperforms BASE in JT across all the domains, with average improvements of 0.82% on Dice and 1.4% on IOU.  <ref type="bibr" target="#b7">[7]</ref>, UTNet <ref type="bibr" target="#b13">[13]</ref>, BAT <ref type="bibr" target="#b33">[33]</ref>, TransFuse <ref type="bibr" target="#b39">[39]</ref>, and Swin UNETR <ref type="bibr" target="#b30">[30]</ref>. We implement ResNet-34 as the backbone of BAT for fair comparison (similar model size). As illustrated in Table <ref type="table" target="#tab_1">2</ref>-a,b,c, these SOTA models are superior to BASE in ST. This is expected since they are designed to reduce data requirements. Nevertheless, in JT, these models also suffer from NKT: They perform better than models in ST on some datasets, like SCD, and worse on others, like ISIC. Finally, MDViT achieves the best segmentation performance in average Dice and IOU without NKT and has the best results on SCD and PH2. Figure <ref type="figure" target="#fig_1">2</ref> shows MDViT's excellent performance on ISIC and DMF and that it achieves the closest results to ground truth on SCD and PH2. More segmentation results are presented in the supplementary material. Though BAT and TransFuse in ST have better results on some datasets like ISIC, they require extra compute resources to train M models as well as an M -fold increase in memory requirements. The above results indicate that domain-shared knowledge is especially beneficial for training relatively small datasets such as SCD.</p><p>We employ the two fixed-size (i.e., independent of M ) multi-domain algorithms proposed by Rundo et al. <ref type="bibr" target="#b28">[28]</ref> and Wang et al. <ref type="bibr" target="#b35">[35]</ref> on BASE. We set the number of parallel SE adapters in <ref type="bibr" target="#b35">[35]</ref> to 4. In Table <ref type="table" target="#tab_1">2</ref>-b,d, MDViT outperforms both of them on all the domains, showing the efficacy of MDViT and that multi-domain methods built on ViTs might not perform as well as on CNNs. We also apply the domain-specific normalization <ref type="bibr" target="#b21">[21]</ref> to BASE and MDViT to get BASE † and MDViT † , respectively. In Table 2-d, BASE † confronts NKT, which lowers the performance on DMF compared with BASE in ST, whereas MDViT † not only addresses NKT but also outperforms BASE † on average Dice and IOU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies and Plug-in Capability of DA:</head><p>We conduct ablation studies to demonstrate the efficacy of DA, MKD, and auxiliary peers. Table <ref type="table" target="#tab_2">3</ref>b reveals that using one-direction knowledge distillation (KD) or either of the critical components in MDViT, i.e., DA or MKD, but not together, could not achieve the best results. Table <ref type="table" target="#tab_2">3</ref>-c exemplifies that, for building the auxiliary peers, our proposed MLP architecture is more effective and has fewer parameters (1.6M) than DeepLabv3's decoder <ref type="bibr" target="#b9">[9]</ref> (4.7M) or BASE's decoding layers (10.8M). Finally, we incorporate DA into two ViTs: TransFuse and DosViT (the latter includes the earliest ViT encoder <ref type="bibr" target="#b12">[12]</ref> and a DeepLabv3's decoder). As shown in Table <ref type="table" target="#tab_2">3</ref>-a,b, DA can be used in various ViTs but is more advantageous in MDViT with more transformer blocks in the encoding and decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a new algorithm to alleviate vision transformers (ViTs)' datahunger in small datasets by aggregating valuable knowledge from multiple related domains. We constructed MDViT, a robust multi-domain ViT leveraging novel domain adapters (DAs) for negative knowledge transfer mitigation and mutual knowledge distillation (MKD) for better representation learning. MDViT is nonscalable, i.e., has a fixed model size at inference time even as more domains are added. The experiments on 4 skin lesion segmentation datasets show that MDViT outperformed SOTA data-efficient medical image segmentation ViTs and multi-domain learning methods. Our ablation studies and application of DA on other ViTs show the effectiveness of DA and MKD and DA's plug-in capability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall architecture of MDViT, which is trained on multi-domain data by optimizing two types of losses: Lseg and L mkd . MDViT extends BASE (a) with DA inside factorized MHSA (b), which is detailed in (c), and MKD (d).</figDesc><graphic coords="4,71,76,65,72,296,35,197,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual result comparison of MDViT, BASE and SOTA data-efficient MIS ViTs in ST and JT training paradigms on four datasets. The green and red contours present the ground truth and segmentation results, respectively. (Color figure online)</figDesc><graphic coords="7,50,82,54,32,331,18,94,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Related works on mitigating ViTs' data-hunger or multi-domain adaptive learning. U (universal) implies a model spans multiple domains. F means the model's size at inference time remains fixed even when more domains are added.</figDesc><table><row><cell>Method ViT Mitigate ViTs' data-hunger [7, 22, 39] √ √ by adding inductive bias [31, 37] √ √ by knowledge sharing [34] √ √ by increasing dataset size [8] √ √ by unsupervised pretraining × -U F × -× -× -[28, 35] × √ √ × [21, 26] × × √ × [32] √ × √ × MDViT √ √ by multi-domain learning √ √</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Segmentation results comparing BASE, MDViT, and SOTA methods. We report the models' parameter count at inference time in millions (M). T means training paradigms. † represents using domain-specific normalization.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param. (millions) (M) T</cell><cell>Segmentation Results in Test Sets (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dice ↑</cell><cell>IOU ↑</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ISIC DMF SCD PH2 avg ± std</cell><cell>ISIC DMF SCD PH2 avg ± std</cell></row><row><cell>(a) BASE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BASE</cell><cell>27.8×</cell><cell>ST</cell><cell cols="2">90.18 90.68 86.82 93.41 90.27 ± 1.16 82.82 83.22 77.64 87.84 82.88 ± 1.67</cell></row><row><cell>BASE</cell><cell>27.8</cell><cell>JT</cell><cell cols="2">89.42 89.89 92.96 94.24 91.63 ± 0.42 81.68 82.07 87.03 89.36 85.04 ± 0.64</cell></row><row><cell cols="2">(b) Our Method</cell><cell></cell><cell></cell></row><row><cell>MDViT</cell><cell>28.5</cell><cell cols="3">MAT 90.29 90.78 93.22 95.53 92.45 ± 0.65 82.99 83.41 87.80 91.57 86.44 ± 0.94</cell></row><row><cell cols="2">(c) Other Data-efficient MIS ViTs</cell><cell></cell><cell></cell></row><row><cell>SwinUnet</cell><cell>41.4×</cell><cell>ST</cell><cell cols="2">89.25 90.69 88.58 94.13 90.66 ± 0.87 81.51 83.25 80.40 89.00 83.54 ± 1.27</cell></row><row><cell>SwinUnet</cell><cell>41.4</cell><cell>JT</cell><cell cols="2">89.64 90.40 92.98 94.86 91.97 ± 0.30 81.98 82.80 87.08 90.33 85.55 ± 0.50</cell></row><row><cell>UTNet</cell><cell>10.0×</cell><cell>ST</cell><cell cols="2">89.74 90.01 88.13 93.23 90.28 ± 0.62 82.16 82.13 79.87 87.60 82.94 ± 0.82</cell></row><row><cell>UTNet</cell><cell>10.0</cell><cell>JT</cell><cell cols="2">90.24 89.85 92.06 94.75 91.72 ± 0.63 82.92 82.00 85.66 90.17 85.19 ± 0.96</cell></row><row><cell>BAT</cell><cell>32.2×</cell><cell>ST</cell><cell cols="2">90.45 90.56 90.78 94.72 91.63 ± 0.68 83.04 82.97 83.66 90.03 84.92 ± 1.01</cell></row><row><cell>BAT</cell><cell>32.2</cell><cell>JT</cell><cell cols="2">90.06 90.06 92.66 93.53 91.58 ± 0.33 82.44 82.18 86.48 88.11 84.80 ± 0.53</cell></row><row><cell>TransFuse</cell><cell>26.3×</cell><cell>ST</cell><cell cols="2">90.43 91.04 91.37 94.93 91.94 ± 0.67 83.18 83.86 84.91 90.44 85.60 ± 0.95</cell></row><row><cell>TransFuse</cell><cell>26.3</cell><cell>JT</cell><cell cols="2">90.03 90.48 92.54 95.14 92.05 ± 0.36 82.56 82.97 86.50 90.85 85.72 ± 0.56</cell></row><row><cell cols="2">Swin UNETR 25.1×</cell><cell>ST</cell><cell cols="2">90.29 90.95 91.10 94.45 91.70 ± 0.51 82.93 83.69 84.16 89.59 85.09 ± 0.79</cell></row><row><cell cols="2">Swin UNETR 25.1</cell><cell>JT</cell><cell cols="2">89.81 90.87 92.29 94.73 91.93 ± 0.29 82.21 83.58 86.10 90.11 85.50 ± 0.44</cell></row><row><cell cols="4">(d) Other Multi-domain Learning Methods</cell></row><row><cell cols="2">Rundo et al. 28.2</cell><cell cols="3">MAT 89.43 89.46 92.62 94.68 91.55 ± 0.64 81.73 81.40 86.71 90.12 84.99 ± 0.90</cell></row><row><cell>Wang et al.</cell><cell>28.1</cell><cell cols="3">MAT 89.46 89.62 92.62 94.47 91.55 ± 0.54 81.79 81.59 86.71 89.76 84.96 ± 0.74</cell></row><row><cell>BASE  †</cell><cell>27.8(.02×)</cell><cell cols="3">MAT 90.22 90.61 93.69 95.55 92.52 ± 0.45 82.91 83.14 88.28 91.58 86.48 ± 0.74</cell></row><row><cell>MDViT</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>† 28.6(.02×) MAT 90.24 90.71 93.38 95.90 92.56 ± 0.52 82.97 83.31 88.06 92.19 86.64 ± 0.76</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies of MDViT and experiments of DA's plug-in capability. KD means general knowledge distillation, i.e., we only transfer knowledge from auxiliary peers to the universal network. D or B refers to using DeepLabv3's decoder or BASE's decoding layers as auxiliary peers.</figDesc><table><row><cell>Model</cell><cell cols="2">#Param. (M) T</cell><cell>Dice ↑</cell><cell>IOU ↑</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ISIC DMF SCD PH2 avg ± std</cell><cell>ISIC DMF SCD PH2 avg ± std</cell></row><row><cell cols="2">(a) Plug-in Capability of DA</cell><cell></cell><cell></cell></row><row><cell>DosViT</cell><cell>14.6</cell><cell>JT</cell><cell cols="2">88.66 89.72 90.65 94.26 90.82 ± 0.43 80.45 81.73 83.29 89.26 83.68 ± 0.68</cell></row><row><cell>DosViT+DA</cell><cell>14.9</cell><cell cols="3">MAT 89.22 89.91 90.73 94.42 91.07 ± 0.32 81.28 82.00 83.44 89.57 84.07 ± 0.50</cell></row><row><cell>TransFuse</cell><cell>26.3</cell><cell>JT</cell><cell cols="2">90.03 90.48 92.54 95.14 92.05 ± 0.36 82.56 82.97 86.50 90.85 85.72 ± 0.56</cell></row><row><cell cols="2">TransFuse+DA 26.9</cell><cell cols="3">MAT 90.13 90.47 93.62 95.21 92.36 ± 0.38 82.80 82.94 88.16 90.97 86.22 ± 0.64</cell></row><row><cell cols="4">(b) Ablation Study for DA and MKD</cell></row><row><cell>BASE</cell><cell>27.8</cell><cell>JT</cell><cell cols="2">89.42 89.89 92.96 94.24 91.63 ± 0.42 81.68 82.07 87.03 89.36 85.04 ± 0.64</cell></row><row><cell>BASE+DA</cell><cell>28.5</cell><cell cols="3">MAT 89.96 90.66 93.36 95.46 92.36 ± 0.51 82.52 83.24 87.98 91.43 86.29 ± 0.72</cell></row><row><cell>BASE+MKD</cell><cell>27.8</cell><cell>JT</cell><cell cols="2">89.27 89.53 92.66 94.83 91.57 ± 0.53 81.45 81.49 86.81 90.42 85.04 ± 0.74</cell></row><row><cell cols="2">BASE+DA+KD 28.5</cell><cell cols="3">MAT 90.03 90.59 93.26 95.63 92.38 ± 0.39 82.67 83.12 87.85 91.72 86.34 ± 0.51</cell></row><row><cell cols="4">(c) Ablation Study for Auxiliary Peers</cell></row><row><cell>MDViT D</cell><cell>28.5</cell><cell cols="3">MAT 89.64 90.25 92.24 95.36 91.87 ± 0.45 82.10 82.55 86.12 91.24 85.50 ± 0.67</cell></row><row><cell>MDViT B</cell><cell>28.5</cell><cell cols="3">MAT 90.03 90.73 92.72 95.32 92.20 ± 0.50 82.66 83.35 87.01 91.17 86.05 ± 0.70</cell></row><row><cell>MDViT</cell><cell>28.5</cell><cell cols="3">MAT 90.29 90.78 93.22 95.53 92.45 ± 0.65 82.99 83.41 87.80 91.57 86.44 ± 0.94</cell></row></table><note><p>Comparing Against State-of-the-Art (SOTA) Methods: We conduct experiments on SOTA data-efficient MIS ViTs and multi-domain learning methods. Previous MIS ViTs mitigated the data-hunger in one dataset by adding inductive bias, e.g., SwinUnet</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 43.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on data-efficient algorithms in big data era</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep semantic segmentation of natural and medical images: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asgari Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="137" to="178" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A color and texture based hierarchical K-NN approach to the classification of non-melanoma skin lesions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ballerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color medical image analysis</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Celebi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dordrecht</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-007-5389-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-94-007-5389-14" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Culprit-prune-net: efficient continual sequential multi-domain learning with application to skin lesion classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bayasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garbi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-216" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BoosterNet: improving domain generalization of deep neural nets using culpability-ranked features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bayasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="538" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LiTS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-89" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10728</idno>
		<title level="m">Training vision transformers with only 2040 images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What does BERT look at? an analysis of BERT&apos;s attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">276</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UTNet: a hybrid transformer architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MSIM: multistage illumination modeling of dermatological photographs for illumination-corrected skin lesion analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Glaister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amelard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1873" to="1883" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge distillation: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on vision transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="110" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mag. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CHAOS challenge-combined (CT-MR) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Gezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barış</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Groza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MPViT: multi-path vision transformer for dense prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7287" to="7296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MS-Net: multi-site network for improving prostate segmentation with heterogeneous MRI data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2713" to="2724" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient training of visual transformers with small datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="23818" to="23830" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PH 2-A dermoscopic image database for research and benchmarking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Marcal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMBC 2013</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="5437" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">USE-Net: incorporating squeeze-and-excitation blocks into u-net for prostate zonal segmentation of multi-institutional MRI datasets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rundo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="31" to="43" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Combo loss: handling input and output imbalance in multi-organ segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of Swin transformers for 3d medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Task adaptive parameter sharing for multi-task learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wallingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7561" to="7570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Boundary-aware transformers for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-220" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards data-efficient detection transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20077-9_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20077-96" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13669</biblScope>
			<biblScope unit="page" from="88" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards universal object detection by domain attention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7289" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SegFormer: simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UniMiSS: universal medical self-supervised learning via breaking dimensionality barrier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-833" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page" from="558" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on negative transfer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Automatica Sinica</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">TransFuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel multi-domain machine reading comprehension model with domain interference mitigation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="791" to="798" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
