<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Certification of Deep Learning Models for Medical Image Segmentation</title>
				<funder ref="#_a3tAf2N">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Othmane</forename><surname>Laousy</surname></persName>
							<email>othmane.laousy@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MICS</orgName>
								<orgName type="institution" key="instit2">CentraleSupélec, Paris-Saclay University</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Hôpital Cochin</orgName>
								<orgName type="institution" key="instit2">AP-HP</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Cité University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inria Saclay</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Araujo</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">New York University</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Chassagnon</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Hôpital Cochin</orgName>
								<orgName type="institution" key="instit2">AP-HP</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Cité University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Therapanacea</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie-Pierre</forename><surname>Revel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Hôpital Cochin</orgName>
								<orgName type="institution" key="instit2">AP-HP</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Cité University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Vakalopoulou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MICS</orgName>
								<orgName type="institution" key="instit2">CentraleSupélec, Paris-Saclay University</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inria Saclay</orgName>
								<address>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Certification of Deep Learning Models for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="611" to="621"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3F313F83544F8F44C4EAD0F873428150</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Certified Robustness</term>
					<term>Randomized Smoothing</term>
					<term>Denoising Diffusion Models</term>
					<term>Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In medical imaging, segmentation models have known a significant improvement in the past decade and are now used daily in clinical practice. However, similar to classification models, segmentation models are affected by adversarial attacks. In a safety-critical field like healthcare, certifying model predictions is of the utmost importance. Randomized smoothing has been introduced lately and provides a framework to certify models and obtain theoretical guarantees. In this paper, we present for the first time a certified segmentation baseline for medical imaging based on randomized smoothing and diffusion models. Our results show that leveraging the power of denoising diffusion probabilistic models helps us overcome the limits of randomized smoothing. We conduct extensive experiments on five public datasets of chest X-rays, skin lesions, and colonoscopies, and empirically show that we are able to maintain high certified Dice scores even for highly perturbed images. Our work represents the first attempt to certify medical image segmentation models, and we aspire for it to set a foundation for future benchmarks in this crucial and largely uncharted area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the past decade, deep neural networks have dominated the computer vision community and provided near human performance on many different tasks, including classification <ref type="bibr" target="#b19">[18]</ref>, segmentation <ref type="bibr" target="#b25">[24]</ref>, and image generation <ref type="bibr" target="#b17">[16]</ref>. Given these impressive results, convolutional neural networks are now used on a daily basis in fields like healthcare, self-driving cars, and robotics, to cite a few. In medical imaging, convolutional neural networks are particularly used to segment organs or regions of interest on different modalities such as X-rays, CT scans, MRIs, or ultrasound <ref type="bibr" target="#b37">[36]</ref>. Indeed, segmentation techniques and variations of 2D and 3D U-Nets are currently the state-of-the-art to identify and isolate tumors, blood vessels, organs, or other structures within an image and provide crucial help to physicians for medical diagnosis, screening, and prognosis <ref type="bibr" target="#b33">[32]</ref>.</p><p>Nowadays, segmentation models are gaining widespread adoption in modern clinical practice and are being used with increasing frequency, making the results of these models critical for many patients. However, it is now commonly known that neural networks can be vulnerable to adversarial attacks <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b35">34]</ref>, i.e., small input perturbations invisible to humans crafted specifically such that the network performs errors. Over the past few years, a large body of work has devised empirical defenses against adversarial attacks for classification tasks <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b26">25]</ref>, as well as segmentation tasks <ref type="bibr" target="#b38">[37]</ref>, including applications on medical imaging <ref type="bibr" target="#b28">[27]</ref>. Although state-of-the-art empirical defenses provide significant robustness, these defenses do not guarantee theoretical robustness and stronger attacks can be crafted to break them <ref type="bibr" target="#b5">[5]</ref>. Recently, certified defenses, for classification <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b27">26]</ref> and segmentation <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b24">23]</ref>, have been proposed to guarantee the accuracy and reliability of neural networks. However, certified defenses for segmentation in the context of medical imaging are still lacking, even if models are getting market approvals (e.g., FDA, CE) and are already adopted in clinical practice.</p><p>In this paper, we provide the first method for certified robustness in the context of segmentation for medical imaging. We leverage the randomized smoothing strategy <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b16">15]</ref>, and the recent work on diffusion models <ref type="bibr" target="#b7">[7]</ref> to achieve stateof-the-art certified robustness for segmentation models. Randomized smoothing consists in convolving the neural network with a Gaussian distribution (i.e., by adding noise to the input) in order to obtain a smooth segmentation model. From the smoothness properties of the segmentation model, we can derive a robustness guarantee and compute a certified Dice score. We go even further by using diffusion models to first denoise the perturbed input and boost the certified robustness. By extension, we show that current diffusion models, trained on 'classical images' generalize well to medical datasets for denoising tasks. Extensive experiments on five public medical datasets of chest X-rays <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b32">31]</ref>, skin lesions <ref type="bibr" target="#b10">[10]</ref>, and colonoscopies <ref type="bibr" target="#b6">[6]</ref>, and different popular segmentation models, prove the potential of our method. We hope that this study will provide the first step towards robustness guarantees for medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since the discovery of adversarial attacks <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b35">34]</ref>, numerous defenses <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b26">25]</ref> and attacks have been devised <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b26">25]</ref>, demonstrating that neural networks are sensitive to small input perturbation and vulnerable to attacks. Adversarial training, which has been acknowledged as one of the most successful empirical defenses, consists in training a network directly on adversarial examples <ref type="bibr" target="#b26">[25]</ref>. However, it is now known that even strong defenses can be bypassed by adaptive attacks <ref type="bibr" target="#b12">[12]</ref>. Paschali et al. <ref type="bibr" target="#b28">[27]</ref> were among the first to study adversarial attacks in the context of medical imaging. They conducted experiments using several neural network architectures <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b34">33]</ref> (i.e., Inception V3, V4, MobileNet) and several attacks <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b26">25]</ref> to demonstrate that the vulnerability of neural networks is extended to medical images.</p><p>More specifically, in the context of classification, a previous work <ref type="bibr" target="#b4">[4]</ref> has analyzed the robustness of neural networks for chest X-ray images and showed that gradient-based attacks were successful in fooling both machines and humans. In a similar line of work, Yao et al. <ref type="bibr" target="#b39">[38]</ref> proposed an add-on to known attacks that bypasses state-of-the-art adversarial detectors making current defenses even less robust. On the other hand, several works have been focused on crafting defense strategies specifically in the context of medical imaging. For example, Almalik et al. <ref type="bibr" target="#b0">[1]</ref> proposed a self-ensembling method to enhance the robustness of Vision Transformers in the presence of adversarial attacks. In the context of segmentation in medical imaging, <ref type="bibr" target="#b31">[30]</ref> introduced a vector quantization approach by learning a discrete representation in a low dimensional embedding space and improving the robustness of a segmentation model. Finally, Daza et al. <ref type="bibr" target="#b13">[13]</ref> proposed a lattice architecture that segments organs and lesions on MRI and CT scans and leveraged an efficient approach of adversarial training to defend against adversarial examples.</p><p>Although a large body of work has focused on constructing defenses for classification and segmentation tasks in the context of medical imaging, certified defenses are under-studied by the medical community. In this paper, we propose to leverage randomized smoothing and diffusion models for certified segmentation on medical datasets, setting the first baseline for this challenging problem and certifying popular segmentation architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Randomized Smoothing</head><p>Randomized smoothing is a model agnostic technique, proposed by Cohen et al. <ref type="bibr" target="#b11">[11]</ref>, used to improve and certify the robustness of neural networks against adversarial attacks. This method consists in adding random noise (e.g., noise generated from a Gaussian distribution) to the input data and then classifying the perturbed data using the neural network. Let D = X × Y denote the data distribution where X ⊂ R d and Y = {1, . . . , k} represent the input space and target space respectively and k is the number of classes. Let f : X → Y be a neural network such that for (x, y) ∈ D, the classifier correctly classifies if</p><formula xml:id="formula_0">f (x) = y. An adversarial attack is a small norm-bounded perturbation δ ∈ R d with δ 2 ≤ such that: f (x + δ) = y.</formula><p>Randomized smoothing is a procedure to construct a new smooth classifier g given any base classifier f . Let N (0, σ 2 I) be a Gaussian distribution of mean 0 and variance σ, then, the smooth classifier g is defined as follows:</p><formula xml:id="formula_1">g(x) = P η∼N (0,σ 2 I) [f (x + η) = y]</formula><p>Cohen et al. <ref type="bibr" target="#b11">[11]</ref> have shown that if R = σΦ -1 (g(x)) where Φ is the cumulative distribution function of the standard Gaussian distribution and R can be considered the certified radius, then, g(x + δ) = y for all δ satisfying δ 2 ≤ R.</p><p>However, since it is not possible to compute g at x exactly, they proposed using Monte Carlo algorithms as an alternative approach for estimating g(x) using random sampling. In order to obtain a reliable estimate of the probability g(x), they also suggested a method that involves generating n samples of η from a normal distribution N (0, σ 2 I) and evaluating f (x + η) for each sample. The resulting counts for each class in Y are then used to estimate probability p y and the radius R with confidence 1 -α (where α is a value between 0 and 1). If the confidence level cannot be achieved (for example, due to insufficient samples), the method will abstain from providing an estimate. More recently, Fischer et al. <ref type="bibr" target="#b16">[15]</ref> built upon the work of <ref type="bibr" target="#b11">[11]</ref> by introducing SegCertify, the first certified approach for image segmentation. The segmentation process involves assigning a segmentation class to every pixel in the image, which can be viewed as a form of classification at the pixel level. In the segmentation settings, the output space consists of regions or categories to be segmented, such as cars, roads, pedestrians, etc. The classifier function f : X → Y d determines the class for each pixel and categorizes each component independently. In this context, the certification algorithm proposed by Cohen et al. <ref type="bibr" target="#b11">[11]</ref> can be extended to accommodate the segmentation task.</p><p>To obtain a smooth classifier, it is necessary to add random noise to the input of the classifier. However, this creates a trade-off between accuracy and robustness. If low variance noise is added, accuracy won't be impacted significantly, but the certified radius will remain low. Conversely, adding high variance noise can improve certificates but at the expense of accuracy. To address this issue, Cohen et al. proposed a simple trick of training the network with noise injection during the training phase. While this method may reduce accuracy when evaluating the classifier with noise during the certification process, it can also help mitigate the trade-off between accuracy and robustness. One can note that during training, the network's objective is to learn to ignore the noise and classify at the same time. To improve the natural as well as the certified accuracy, Salman et al. <ref type="bibr" target="#b30">[29]</ref> proposed to separate the two tasks with two networks trained separately. First, a network, h : X → X , is trained to denoise the data such that for η ∼ N (0, σ 2 I), we have h(x + η) ≈ x, then, the output of the denoiser is given to the classifier.</p><p>In this paper, we leverage randomized smoothing and diffusion probabilistic models to obtain state-of-the-art results on certified segmentation for medical imaging. To the best of our knowledge, we are the first to propose a comprehensive study on certified segmentation for medical imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Diffusion Probabilistic Models for Certification</head><p>The training of a Denoising Diffusion Probabilistic Model (DDPM) is an iterative process that involves adding a small amount of noise at every step of the diffusion process until random noise is reached. The reverse process then starts from random noise and generates a new image that conforms to the data distribution. Since DDPMs are inherently iterative denoising models, we can leverage this property for randomized smoothing. The idea would be to start the reverse process with a noisy image, rather than Gaussian noise, enabling the DDPM to output an image that resembles the original image.</p><p>Similar to Carlini et al. <ref type="bibr" target="#b7">[7]</ref>, our proposed pipeline is composed of two main steps: we denoise, then we certify. In order to complete the denoising, we need to first map between the noise model utilized in diffusion models and the one used in randomized smoothing. Randomized smoothing needs a data point that is enhanced with Gaussian noise added to it, given by x rs = x + δ with δ ∼ N (x, σ 2 I). On the other hand, diffusion models suppose the noise model for Since randomized smoothing is applied to each pixel separately with a probability of 1 -α, considering the entire segmentation region would imply considering a union bound with significantly reduced confidence. Similar to Fischer et al. <ref type="bibr" target="#b16">[15]</ref>, we leverage the Holm-Bonferroni method <ref type="bibr" target="#b20">[19]</ref> and perform multipletesting corrections. For each image, we repeat this process n = 100 times, identifying pixels on which the model abstains, and computing the certified scores. We extend the work of Fischer et al. <ref type="bibr" target="#b16">[15]</ref> to also compute a certified Dice score that is calculated ignoring the abstain class ( ). Our approach has a significant advantage compared to SegCertify since it leverages off-the-shelf and stateof-the-art pre-trained denoisers and segmentation models. SegCertify, on the other hand, relies on models trained with Gaussian noise.</p><formula xml:id="formula_2">x DDPM ∼ N ( √ α t x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Datasets: We perform experiments on 5 different publicly available datasets. All datasets were divided to 70% for training, 10% for validation, and 20% for testing. The testing set is the one used to compute certified results.</p><p>Chest X-rays Datasets: JSRT dataset <ref type="bibr" target="#b32">[31]</ref> with annotations of lung, heart, and clavicles provided by <ref type="bibr" target="#b36">[35]</ref> is used. This dataset contains 247 images. For lung segmentation only, we use both the Montgomery and Shenzen datasets <ref type="bibr" target="#b22">[21]</ref>. Montgomery consists of 138 and Shenzen of 662 annotated images.</p><p>Skin Lesion: Skin images with their annotations provided by the ISIC 2018 boundary segmentation challenge <ref type="bibr" target="#b10">[10]</ref> were used. This dataset consists of 2694 RGB dermatoscopy images. Colonoscopy Images: CVC-ClinicDB dataset <ref type="bibr" target="#b6">[6]</ref> containing 612 colonoscopy images in RGB together with their annotations were utilized.</p><p>Implementation Details: We train three different segmentation models namely, a UNet <ref type="bibr" target="#b29">[28]</ref>, a ResUNet++ <ref type="bibr" target="#b23">[22]</ref>, and a DeeplabV2 <ref type="bibr" target="#b9">[9]</ref> with and without noise. The models trained without noise are used exclusively with our method. The models trained with a Gaussian noise of 0.25 are used to compute SegCertify scores. All 6 models use an image input size of 512 × 512 for X-ray images, 384×512 for skin lesions, and 288×384 for colonoscopy. As a denoiser, we use an off-the-shelf denoising diffusion probabilistic model provided by <ref type="bibr" target="#b15">[14]</ref>. We perform our experiments with the 256 × 256 class unconditional denoiser with a linear scheduler and without timestep respacing. For each noise level, our method follows the steps described in the previous section and uses n 0 = 10, n=100 for each image, and α = 0.001, and τ = 0.75. Our code is made publicly available at: https://github.com/othmanela/medical_cert_seg.</p><p>Results and Discussion: For all five datasets, we compute a certified Dice score and certified mean Intersection over Union (IoU). We also report the percentage of abstentions (% ) representing the mean number of pixels on which the model's prediction confidence was insufficient with respect to the radius R.</p><p>The lower the percentage of abstentions the better the segmentation model is.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we compare our method using 3 different and popular architectures (UNet, ResUNet++, and DeeplabV2) on the chest X-rays datasets. We  <ref type="table" target="#tab_2">S2</ref> of the supplementary material. Overall, for both methods, ResUNet++ is the most robust architecture followed by UNet and then DeeplabV2 for all σ and R combinations. Moreover, certified metrics for lungs and heart remain high for our method, even with high levels of noise. However, the increasing level of noise affects the clavicles since these are smaller structures. A comparison of our method and SegCertify using the ResUNet++ architecture is presented in Table <ref type="table" target="#tab_2">2</ref> for the three chest X-ray datasets. We observe that we outperform SegCertify, especially for high sigma values. For σ = 0.25, SegCertify performs slightly better. This is due to the fact that the model used with SegCertify is trained with a noise level of 0.25. The main drawback however is that its Dice on unperturbed images drops considerably (e.g., from 0.96 to 0.91 on lung segmentation). On the other hand, our pipeline does not require training a segmentation model with noise or even a denoising model. Our methodology relies only on off-the-shelf models. For the highest noise level of σ = 1.0, we notice that the certified Dice and IoU with SegCertify both drop to 0 whereas our proposed method is able to maintain high certified scores.</p><p>Qualitative results are provided in Fig. <ref type="figure" target="#fig_0">1</ref> for our proposed method and SegCertify for the different datasets and different levels of noise. Regarding the structures to segment, we notice that the abstentions around the clavicles (the smallest benchmarked region of interest on chest X-rays) get bigger. We also notice that the fine segmentation boundaries (e.g., area around the skin lesion) may not be as sharp after denoising. As we increase the noise, the decision boundary is harder to find for all models. This may be due to the fact that fine details on the image are lost after the denoising step. However, our method is still able to segment the large majority of pixels properly on the image, contrary to its competitor, especially for high noise levels (third row on chest X-rays).</p><p>Table <ref type="table" target="#tab_3">3</ref> reports certified segmentation results for skin lesions and colonoscopy on both techniques. We notice that our method is still performing better than  SegCertify. This supports our claim that DDPMs generalize quite well to medical images and that harnessing their potential boosts the state-of-the-art.</p><p>Regarding the denoiser, we used a single-step denoising strategy, i.e., we perform a single call to the DDPM to compute the denoised image from t * to t = 0. Another strategy could be to iteratively denoise from t * , t * -1, ... until t = 0. However, this implies predicting a denoised image multiple times and in the end, may result in images with unwanted artifacts. We perform multi-step denoising experiments and report results in Table <ref type="table" target="#tab_1">S1</ref> of the supplementary material. We note that the single-step denoising performs best since it relies more on the denoising power of DDPMs rather than their generative capabilities, and is also faster than the multi-step approach. Finally, we perform a comparison with another denoiser architecture. We train three UNet models (one for each noise level) on the JSRT dataset. We report results in Table <ref type="table" target="#tab_3">S3</ref> and notice that even with custom-trained denoisers, the DDPM outperforms the UNet denoising architecture. A comparison of denoised images is provided in Figure <ref type="figure" target="#fig_0">S1</ref>. We notice that the DDPM is able to keep high-fidelity images compared to the UNet and is therefore more relevant for certified medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present the first work on certified segmentation for medical imaging, and extensively evaluate it on five different datasets and three deep learning segmentation models. Our technique leverages off-the-shelf denoising and segmentation models and provides the highest certified Dice and mIoU on multi-class and binary segmentation of five different datasets. With that, we are able to remove the overhead of having to train and fine-tune models specifically for robustness. This paradigm shift alleviates the dilemma of having to choose between highly accurate segmentation models or models robust to attacks. We hope that this work serves as a baseline for the unexplored yet critical topic of certified segmentation in medical imaging. Future work will involve extending our approach to 3D medical imaging modalities as well as exploring the realm of certified classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Qualitative results of SegCertify and our method on colonoscopy, skin lesion, and chest X-ray images. From left to right: image with added noise, ground truth, SegCertify segmentation, our segmentation. White pixels denote abstention areas of the segmentation models. We increase the noise level from top to bottom: σ = 0.25, 0.5, and 1.0.</figDesc><graphic coords="8,55,29,178,43,313,69,192,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, (1 -α t )I). Programmatically, we start by adding Gaussian noise to an image x, obtaining x rs . Then the timestep t * on which we can use the diffusion model for randomized smoothing is defined. Depending on the scheduler of the denoiser, we compute t * such that σ 2 = 1-α t * α t * (obtained by scaling x rs with √ α t and pairing the variances). We then calculate x DDPM = √ α t After that, we apply a single-step denoiser and predict the completely denoised image. A single-step denoising involves directly predicting the image from t</figDesc><table /><note><p>* (x + δ), δ ∼ N (0, σ 2 I). * to t = 0. A multi-step denoising strategy implies iteratively predicting all images at t * , t * -1, . . . until t = 0. Both techniques are explored in the next section and supplementary material.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our approach with three different model architectures on chest X-ray datasets. We report certified Dice, IoU and percentage of abstentions (% ) for different noise levels σ and radii R.</figDesc><table><row><cell></cell><cell></cell><cell>JSRT</cell><cell>Montgomery</cell><cell>Schenzen</cell></row><row><cell>σ</cell><cell>R</cell><cell>Lung Dice IoU Dice IoU Dice IoU % Heart Clavicles</cell><cell>Lung Dice IoU %</cell><cell>Lung Dice IoU %</cell></row><row><cell cols="2">UNet [28]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25</cell><cell>0.17</cell><cell cols="3">0.94 0.91 0.88 0.79 0.75 0.63 0.07 0.93 0.89 0.07 0.95 0.90 0.05</cell></row><row><cell>0.50</cell><cell>0.34</cell><cell cols="3">0.90 0.83 0.88 0.79 0.61 0.45 0.09 0.89 0.80 0.07 0.93 0.90 0.02</cell></row><row><cell>1.00</cell><cell>0.67</cell><cell cols="3">0.87 0.79 0.84 0.75 0.23 0.15 0.15 0.88 0.80 0.14 0.89 0.83 0.10</cell></row><row><cell cols="3">ResUNet++ [22]</cell><cell></cell><cell></cell></row><row><cell>0.25</cell><cell>0.17</cell><cell cols="3">0.95 0.91 0.93 0.87 0.78 0.65 0.05 0.96 0.93 0.02 0.95 0.91 0.01</cell></row><row><cell>0.50</cell><cell>0.34</cell><cell cols="3">0.94 0.88 0.91 0.83 0.63 0.48 0.08 0.94 0.89 0.03 0.93 0.90 0.02</cell></row><row><cell>1.00</cell><cell>0.67</cell><cell cols="3">0.90 0.82 0.87 0.77 0.28 0.19 0.12 0.89 0.83 0.07 0.90 0.85 0.06</cell></row><row><cell cols="2">DeeplabV2 [9]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25</cell><cell>0.17</cell><cell cols="3">0.94 0.91 0.91 0.86 0.85 0.75 0.04 0.93 0.91 0.07 0.80 0.71 0.07</cell></row><row><cell>0.50</cell><cell>0.34</cell><cell cols="3">0.88 0.81 0.87 0.79 0.63 0.49 0.10 0.91 0.87 0.02 0.34 0.25 0.15</cell></row><row><cell>1.00</cell><cell>0.67</cell><cell cols="3">0.88 0.80 0.83 0.74 0.20 0.11 0.14 0.85 0.79 0.17 0.04 0.02 0.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Certified segmentation results of our technique and SegCertify<ref type="bibr" target="#b16">[15]</ref> on the chest X-ray JSRT dataset. We report Dice, IoU, and percentage of abstentions (% ) for each class.notice that our method maintains overall good results on all three model backbones. A similar table with SegCertify results is provided in Table</figDesc><table><row><cell>Model</cell><cell>Trained with noise</cell><cell>σ</cell><cell>R</cell><cell>Lung Dice IoU</cell><cell>Heart Dice IoU</cell><cell>Clavicles Dice IoU %</cell></row><row><cell>ResUNet++ [22]</cell><cell>✗</cell><cell cols="3">0.00 0.00 0.97 0.94</cell><cell>0.94 0.91</cell><cell>0.93 0.91 0.00</cell></row><row><cell></cell><cell>✓</cell><cell cols="3">0.25 0.00 0.91 0.90</cell><cell>0.89 0.87</cell><cell>0.84 0.79 0.00</cell></row><row><cell>SegCertify [15]</cell><cell>✓</cell><cell cols="5">0.25 0.17 0.96 0.92 0.93 0.88 0.83 0.72 0.04</cell></row><row><cell></cell><cell>✓</cell><cell cols="3">0.50 0.34 0.89 0.84</cell><cell>0.85 0.79</cell><cell>0.58 0.43 0.13</cell></row><row><cell></cell><cell>✓</cell><cell cols="3">1.00 0.67 0.07 0.04</cell><cell>0.02 0.01</cell><cell>0.00 0.00 0.24</cell></row><row><cell></cell><cell>✗</cell><cell cols="4">0.25 0.17 0.95 0.91 0.93 0.87</cell><cell>0.78 0.65 0.05</cell></row><row><cell>Ours</cell><cell>✗</cell><cell cols="5">0.50 0.34 0.94 0.88 0.91 0.83 0.63 0.48 0.08</cell></row><row><cell></cell><cell>✗</cell><cell cols="5">1.00 0.67 0.90 0.82 0.87 0.77 0.28 0.19 0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results on skin lesions<ref type="bibr" target="#b10">[10]</ref> and CVC-ClinicDB<ref type="bibr" target="#b6">[6]</ref> segmentation.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell>σ</cell><cell>R</cell><cell>Skin Lesions Dice IoU %</cell><cell>CVC-ClinicDB Dice IoU %</cell></row><row><cell></cell><cell>SegCertify [15]</cell><cell cols="2">0.25 0.17</cell><cell>0.79 0.68 0.07</cell><cell>0.63 0.56 0.05</cell></row><row><cell>ResUNet++ [22]</cell><cell></cell><cell cols="2">0.50 0.34</cell><cell>0.41 0.27 0.06</cell><cell>0.15 0.10 0.01</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00 0.67</cell><cell>0.00 0.00 0.01</cell><cell>0.00 0.00 0.00</cell></row><row><cell></cell><cell></cell><cell cols="2">0.25 0.17</cell><cell>0.85 0.77 0.03</cell><cell>0.65 0.57 0.04</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">0.50 0.34</cell><cell>0.83 0.76 0.04</cell><cell>0.45 0.39 0.07</cell></row><row><cell></cell><cell></cell><cell cols="2">1.00 0.67</cell><cell>0.77 0.69 0.06</cell><cell>0.26 0.23 0.14</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was granted access to the <rs type="institution">HPC resources of IDRIS</rs> under the allocation 2023-AD011013308R1 made by <rs type="institution">GENCI</rs> and it was partially supported by the <rs type="funder">ANR Hagnodice</rs> <rs type="grantNumber">ANR-21-CE45-0007</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_a3tAf2N">
					<idno type="grant-number">ANR-21-CE45-0007</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_58.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-ensembling vision transformer (sevit) for robust medical image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Almalik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="376" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_36" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified algebraic perspective on lipschitz neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Havens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Delattre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advocating for multiple defense strategies against adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Negrevergne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vulnerability analysis of chest x-ray image classification against adversarial attacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asgari Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">iMIMIC</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">(certified!!) adversarial robustness for free!</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE on Security and Privacy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C F</forename><surname>Codella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards robust general medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_1" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable certified segmentation via randomized smoothing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two public chest x-ray datasets for computer-aided screening of pulmonary diseases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X J</forename><surname>Wáng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Resunet++: an advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE on Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards better certified segmentation via diffusion models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Laousy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>UAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A dynamical system perspective for lipschitz neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Delattre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalizability vs. robustness: investigating medical imaging networks using adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Conjeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_56" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="493" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Denoised smoothing: a provable defense for pretrained classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vector quantisation for robust segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santhirasekaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rockall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_63" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists&apos; detection of pulmonary nodules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net and its variants for medical image segmentation: a review of theory and applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paheding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Elkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Devabhaktuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="82031" to="82057" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19" to="40" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13120v3</idno>
		<title level="m">Medical image segmentation using deep learning: a survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A hierarchical feature constraint to camouflage medical adversarial attacks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_4" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
