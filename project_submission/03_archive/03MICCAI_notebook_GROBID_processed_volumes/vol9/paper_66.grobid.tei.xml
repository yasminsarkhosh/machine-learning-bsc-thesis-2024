<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Fonds de Recherche du Québec Nature et Technologies</orgName>
					<orgName type="abbreviated">FRQNT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soorena</forename><surname>Salari</surname></persName>
							<email>soorena.salari@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amirhossein</forename><surname>Rasoulian</surname></persName>
							<email>ah.rasoulian@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Rivaz</surname></persName>
							<email>hassan.rivaz@concordia.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Xiao</surname></persName>
							<email>yiming.xiao@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="689" to="698"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CF768D6B7F35425C965DD82957D3A7F7</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Registration</term>
					<term>Inter-modal</term>
					<term>Error estimation</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In brain tumor resection, accurate removal of cancerous tissues while preserving eloquent regions is crucial to the safety and outcomes of the treatment. However, intra-operative tissue deformation (called brain shift) can move the surgical target and render the presurgical plan invalid. Intra-operative ultrasound (iUS) has been adopted to provide real-time images to track brain shift, and inter-modal (i.e., MRI-iUS) registration is often required to update the pre-surgical plan. Quality control for the registration results during surgery is important to avoid adverse outcomes, but manual verification faces great challenges due to difficult 3D visualization and the low contrast of iUS. Automatic algorithms are urgently needed to address this issue, but the problem was rarely attempted. Therefore, we propose a novel deep learning technique based on 3D focal modulation in conjunction with uncertainty estimation to accurately assess MRI-iUS registration errors for brain tumor surgery. Developed and validated with the public RESECT clinical database, the resulting algorithm can achieve an estimation error of 0.59 ± 0.57 mm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Resection of early-stage brain tumors can greatly reduce the mortality rate of patients. During the surgery, brain tissue deformation (called brain shift) can occur due to various causes, such as gravity, drug administration, and pressure change after craniotomy. While modern magnetic resonance imaging (MRI) techniques can provide rich anatomical and physiological information with various contrasts (e.g., fMRI) for more elaborate pre-surgical planning, intra-operative MRI that can track brain shift requires a complex setup and is costly. In contrast, intra-operative ultrasound (iUS) has gained popularity for real-time imaging during surgery to monitor tissue deformation and surgical tools because of its lower cost, portability, and flexibility <ref type="bibr" target="#b0">[1]</ref>. Accurate and robust MRI-iUS registration techniques <ref type="bibr" target="#b1">[2]</ref> can greatly enhance the value of iUS for updating pre-surgical plans and guiding the interpretation of iUS, which has an unintuitive contrast and non-standard orientations. This can greatly enhance the safety and outcomes of the surgical procedure by allowing maximum brain tumor removal while avoiding eloquent regions <ref type="bibr" target="#b2">[3]</ref>. However, as the true underlying tissue deformation is unknown due to the 3D nature of the surgical data and the time constraint, real-time manual inspection of MRI-iUS registration results is challenging and error-prone, especially for precision-sensitive neurosurgery. Therefore, algorithms that can detect and quantify unreliable inter-modal medical image registration results are highly beneficial.</p><p>Recently, automatic quality assessment for medical image registration has attracted increasing attention <ref type="bibr" target="#b3">[4]</ref> from the domains of big medical data analysis and surgical interventions. With high efficiency, machine, and deep learning techniques have been proposed to allow automatic grading and dense estimation of medical image registration errors. Early endeavors on this topic primarily relied on hand-crafted features, including information theory-based metrics <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. More recently, deep learning (DL) techniques that learn task-specific features have also been adopted in automatic evaluation of medical image registration, with a primary focus on intra-contrast/modal applications, including CT <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and MRI <ref type="bibr" target="#b10">[11]</ref>. Unfortunately, so far, error grading and estimation in inter-contrast/modal registration have rarely been explored, despite the particular demand in surgical applications. In this direction, Bierbrier et al. <ref type="bibr" target="#b11">[12]</ref> made the first attempt using simulated iUS from MRI to train 3D convolutional neural networks (CNNs) to perform dense error regression for MRI-iUS registration in brain tumor resection. Although their algorithm performed well in simulated cases, the results on real clinical scans still required improvements. In this paper, we propose a novel 3D CNN to perform patch-wise error estimation for MRI-iUS registration in neurosurgery, by using focal modulation <ref type="bibr" target="#b12">[13]</ref>, a recent alternative DL technique to self-attention <ref type="bibr" target="#b13">[14]</ref> for encoding contextual information, and uncertainty estimation. We call our method FocalErrorNet, which has three main novelties. First, we adapted the focal modulation network <ref type="bibr" target="#b12">[13]</ref> from 2D to 3D and employed the technique in registration error assessment for the first time. Second, we incorporated uncertainty estimation using Monte Carlo (MC) dropouts <ref type="bibr" target="#b14">[15]</ref> to offer assurance for error regression. Lastly, we developed and thoroughly evaluated our technique against a recent baseline model <ref type="bibr" target="#b11">[12]</ref> using real clinical data and showed excellent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset and Preprocessing</head><p>For methodological development and assessment, we used the RESECT (REtro-Spective Evaluation of Cerebral Tumors) dataset <ref type="bibr" target="#b15">[16]</ref>, which has pre-operative MRI, and iUS scans at different surgical stages from 23 subjects who underwent low-grade glioma resection surgeries. As it is still challenging to model iUS scans with tissue resection, we took 22 cases with T2FLAIR MRI that better depicts tumor boundaries and iUS acquired before resection. An example of an MRI-iUS pair from a patient is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We hypothesized that directly leveraging clinical iUS could help learn more realistic image features with potentially better outcomes in clinical applications than with simulated contrasts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. However, since the true brain shift model is impossible to obtain, we followed the strategy of creating silver ground truths for image alignment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, upon which simulated misalignment is augmented in the iUS to build and test our DL model. To create the silver registration ground truths, we used the homologous landmarks between MRI and iUS in the RESECT dataset to perform landmark-based 3D B-Spline nonlinear registration to register iUS to the corresponding MRI for all 22 cases. To tackle the limited field of view (FOV) in iUS, we cropped the T2FLAIR MRI to the same FOV of the iUS, which was resampled to a 0.5 × 0.5 × 0.5 mm 3 resolution. To perform spatial misalignment augmentation, we continued to leverage 3D B-Spline transformation, similar to earlier reports on the same topic <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. In short, B-Spline transformation can be modeled by a grid of regularly spaced control points and the associated parameters to allow various levels of nonlinear deformation. While the spacing of the control points determines the levels of details in local deformation fields, the displacement parameters control the magnitude of the deformation. To ensure that simulated registration errors are of different varieties and sizes, we randomly selected the number of control points and the associated displacements (in each 3D axis) with a maximum of 20 points and 30 mm, respectively. Note that the control point grid is isotropic, and the density is arbitrarily determined per deformation in our case. Each coregistered iUS scan was deformed ten times. After misalignment augmentation on the previously co-registered iUS, matching pairs of 3D image patches of size 33 × 33 × 33 voxels were taken from both the iUS volume and the corresponding MRI. As iUS has limited FOV and may contain no anatomical features, to ensure that the patches we extracted contain useful information (e.g. to avoid the dark background) in iUS, we focused on acquiring patches centered around the anatomical landmark locations available through the RESECT database. Since B-spline transformation offers a displacement vector at each voxel of the iUS volume, we directly considered the norm of the vector as the simulated registration error at the associated voxel. In our design, we determined the registration error of the image patch pair as the mean of all voxel-wise errors within the iUS patch. Finally, the image patch pairs, along with corresponding registration errors were then fed to the proposed DL algorithm for training and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>We proposed a novel 3D neural network, named FocalErrorNet, based on the recent focal modulation networks <ref type="bibr" target="#b12">[13]</ref> that was originally proposed for 2D vision tasks to estimate the registration error between MRI and iUS patches. With a similar goal as the Vision Transformer (ViT), the focal modulation network was designed to model contextual information in images. It incorporates three main elements to achieve the goal: 1) focal contextualization that comprises a stack of depth-wise convolutional layers to account for long-to short-range dependencies, 2) gated aggregation to collect contexts into a modulator for individual query tokens, and 3) element-wise affine transformation to inject the modulator into the query. In the architecture of FocalErrorNet (see Fig. <ref type="figure" target="#fig_1">2</ref>), all layers contain two focal modulator blocks, where two depth-wise convolutional layers focally extract contexts around each voxel, selectively aggregate and inject them into the query, and pass the information to the next block. We designed the FocalErrorNet as a ResNet-like variant of the focal modulation network to better encode relevant features across the input image and ensure a better gradient flow. Finally, the information from the backbone was propagated to a multi-layer perceptron (MLP) to regress registration errors, and two MC dropout layers were added to the MLP to allow uncertainty quantification for the results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uncertainty Quantification</head><p>For registration error regression in surgical applications, knowledge regarding the reliability of the automated results is instrumental for the safety and wellbeing of the patients. Uncertainty estimation has gained popularity in probing the trustworthiness and credence of DL algorithms. Although the concept has been widely applied in image segmentation and classification, it has not been employed for registration error estimation, especially in the case of multi-modal situations, such as MRI-iUS alignment. Therefore, we incorporated uncertainty estimation in our proposed FocalErrorNet. For each MRI-iUS patch pair, 200 regression samples were collected by random sampling from MC dropouts <ref type="bibr" target="#b14">[15]</ref> at test time. While the final patch registration error was obtained as the mean of all the samples, the sample standard deviation was used as the uncertainty metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experimental Setup and Implementation</head><p>From the transformation augmentation, we acquired 3380 samples of MRI-iUS pairs. For our experiments, we arbitrarily split the subjects into training, validation, and test sets with the proportion of 60%, 20%, and 20%, respectively. To prevent information leakage, we ensured that each patient was included in only one of the split sets. For model training, we adopted the Adam optimization with a learning rate of 5 × 10 -5 and a batch size of 64. For the loss function, we used mean squared error (MSE) to minimize the difference between the predicted MRI-iUS registration error and the ground truths. Furthermore, in addition to the transformation augmentation, we also included additional data augmentation, including random noise addition and random image flipping on training sets to mitigate overfitting and increase the model's generalizability. To assess our proposed FocalErrorNet, we compared it against a 3D CNN [9,12] (see Fig. <ref type="figure" target="#fig_2">3</ref>) that was employed for medical image registration error regression. The two DL models were trained with the same dataset and procedure, and their prediction accuracies, measured as the absolute error between the predicted and ground truths mis-registration on the test set were compared with two-sided pairedsamples t-tests to confirm the superiority of the proposed method, in addition to correlations between their estimated and ground truth errors. To validate the proposed uncertainty estimation method, we calculated the correlation between the uncertainty measure and absolute error of FocalErrorNet, and the correlation between the uncertainty and mutual information between MRI and iUS, which is often used to measure the information overlap in multi-modal registration. Finally, to test the robustness of the FocalErrorNet, we acquired additional MRI-iUS patch pairs from the test subjects, by introducing random linear shifts (the max displacement from landmark locations is 10 voxels) from the selected locations in the original set, and evaluated the DL model performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Error Regression Accuracy</head><p>The accuracy comparison between the proposed FocalErrorNet and the baseline 3D CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> is shown in Table <ref type="table" target="#tab_0">1</ref>. Across all samples in the testing data, we achieved an accuracy of 0.59 ± 0.57 mm, while the counterpart obtained a prediction error of 1.69 ± 1.37 mm. With the t-test, our FocalErrorNet outperformed the 3D CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> (p &lt; 1e-4). In addition, the correlations between the predicted and ground truths errors are 0.82 (p &lt; 1e-4) and 0.61 (p &lt; 1e-3) for FocalErrorNet and 3D CNN, respectively, further confirming the advantage of the proposed technique. To allow a qualitative comparison, scatter plots for predicted vs. ground truth errors of the two models are depicted in Fig. <ref type="figure" target="#fig_3">4a</ref> and<ref type="figure" target="#fig_4">5a</ref>. At larger error levels, it is evident that the point clouds exhibit a wider shape. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Validation of the Uncertainty Evaluation</head><p>We obtained correlations of 0.70 (p &lt; 1e-4) and 0.34 (p &lt; 1e-4) between estimated uncertainty and prediction error for FocalErrorNet and the baseline 3D CNN, respectively. Additionally, the uncertainty vs. mutual information uncertainties was assessed at -0.67 (p &lt; 1e-4) for our proposed method and -0.18 (p &lt; 1e-3) for the baseline. To allow better visual comparisons, the associated scatter plots are illustrated in Fig. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref>. These metrics proved the validity of our uncertainty measure and further confirmed the performance of FocalErrorNet. Note the scatter plots for uncertainty measure validation were performed using value binning (with 20 values per bin) for each axis to better reveal the trends of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Robustness of the Proposed Model</head><p>To examine the performance of our proposed method for image regions that contain fewer potent anatomical features, we acquired additional image pairs from test subjects, according to Sect. 2.4. With the new test set, the prediction errors for our method and the baseline model were 1.28 ± 0.99 mm and 2.49 ± 1.87 mm, respectively. Furthermore, the correlations between estimated and true error were calculated at 0.41 for FocalErrorNet and 0.20 for the baseline. These results supported the benefits of focal modulation in registration error estimation. In this test, patches can contain large areas of zeros (image content out of the scanning FOV of the iUS). The main reason for the observed performance decline is due to the reduction in sufficient image features in iUS. However, despite these challenges, we saw an acceptable outcome from FocalEr-rorNet (absolute error = 1.28 mm or ∼1 voxel in clinical MRIs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In image-guided interventions, there is an urgent need for automatic assessment of image registration quality. Multi-modal registration quality evaluation poses major challenges due to three main factors. First, dissimilar contrasts between images require more elaborate strategies to derive relevant features for error assessment. Second, unlike segmentation or classification, the ground truths of registration errors are difficult to obtain. Finally, compared with classification, regression tasks tend to be more error-prone for deep learning algorithms. To tackle these challenges, we employed 3D focal modulation with depth-wise convolution to encode contextual information for the image pair. Compared with the ViT and its variants, focal modulation allows a more lightweight setup, which could be desirable for 3D data. Although we admit that residual errors still remain after landmark-based B-Spline nonlinear alignment, this approach has been adopted in different prior studies, considering the residual landmark registration error is fairly low (mTRE of 0.0008 ± 0.0010mm). Although simulated ultrasound has been used to provide a perfect alignment with MRIs, the fidelity of the simulated results is still suboptimal, and this may explain the underperformance of the previous technique in real clinical data <ref type="bibr" target="#b11">[12]</ref>. To ensure the performance of our FocalErrorNet, we opted to regress the mean registration error of image patches than simplistic error grades or voxel-wise error maps. We believe that this design choice offers a more stable performance, which is supported by our validation. We adopted uncertainty estimation in inter-modal registration error assessment for the first time. While other techniques exist to provide model uncertainty <ref type="bibr" target="#b17">[18]</ref>, MC dropout is more flexible for various DL models. Furthermore, the use of standard deviation as an uncertainty measurement maintains the same unit as the regressed errors, thus making the interpretation more intuitive. From quantitative and qualitative evaluations using correlation coefficients and scatter plots to assess the association of uncertainty measures with the prediction errors and image entropy, we confirmed the validity of the proposed uncertainty estimation approach. For our FocalErrorNet, we achieved a prediction error of 0.59 ± 0.57 mm, which is on par with the image resolution (0.5 mm). Additionally, the standard deviation of our results is lower than the baseline model <ref type="bibr" target="#b11">[12]</ref>. These signify a robust performance of the FocalErrorNet. One limitation of our work lies in the limited patient data, as public iUS datasets are scarce, while the settings and properties of US scanners can vary, potentially affecting the DL model designs. Therefore, we created random deformations for patch-wise error estimation, and will further explore data-efficient approaches for registration error assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed FocalErrorNet, a novel DL model for uncertainty-aware inter-modal registration error estimation in iUS-guided neurosurgery, leveraging the latest focal modulation technique and MC dropout. With thorough assessments of the accuracy and uncertainty measures, we have confirmed the performance of the proposed method against a baseline model previously adopted for the same task. As the first to introduce uncertainty measures and 3D focal modulation in registration error evaluation, our work provides the first step for fast and reliable feedback in inter-modal medical image registration to guide clinical decisions in surgery. We plan to adapt the presented framework for other inter-modal/contrast image registration applications in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left to right: demonstration of sample pre-operative MRI, perfectly registered, and deformed iUS with a mean registration error of 1.4 mm.</figDesc><graphic coords="3,59,46,266,27,333,49,75,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed FocalErrorNet for registration error and uncertainty estimation.</figDesc><graphic coords="4,54,81,410,87,314,17,158,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The baseline 3D CNN [9, 12] with the added MC dropout layer.</figDesc><graphic coords="5,70,98,53,90,310,54,87,79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left to right: scatter plots and correlations between true registration error vs. predicted registration error, uncertainty metric vs. absolute error, and mutual information vs. uncertainty metric for the proposed FocalErrorNet.</figDesc><graphic coords="6,43,29,264,86,337,33,132,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Left to right: scatter plots and correlations between true registration error vs. predicted registration error, uncertainty metric vs. absolute error, and mutual information vs. uncertainty metric for the 3D CNN [9, 12].</figDesc><graphic coords="7,58,98,54,02,334,12,132,67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy comparison of different models for registration error estimation.</figDesc><table><row><cell>Model</cell><cell cols="2">Absolute error (mm) Correlation</cell></row><row><cell cols="2">3D CNN [9, 12] 1.69 ± 1.37</cell><cell>0.61</cell></row><row><cell cols="2">FocalErrorNet 0.59 ± 0.57</cell><cell>0.82</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. We acknowledge the support of the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs> and <rs type="funder">Fonds de Recherche du Québec Nature et Technologies (FRQNT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Near real-time robust non-rigid registration of volumetric ultrasound images for neurosurgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="587" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of MRI to ultrasound registration methods for brain shift correction: the CuRIOUS2018 challenge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="777" to="786" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extent of resection of glioblastoma revisited: personalized survival modeling facilitates more accurate survival prediction and supports a maximum-safe-resection approach to surgery</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Marko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Sawaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">774</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating medical image registration error and confidence: a taxonomy and scoping review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bierbrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-E</forename><surname>Gueziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="page">102531</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Assessment of rigid registration quality measures in ultrasound-guided radiotherapy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hébert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="428" to="437" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local-search based prediction of medical image registration error</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saygili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Perception, Observer Performance, and Technology Assessment</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10577</biblScope>
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantitative error prediction of medical image registration using regression forests</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saygili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="110" to="121" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accuracy estimation for medical image registration using regression forests</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saygili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46726-9_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46726-913" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9902</biblScope>
			<biblScope unit="page" from="107" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Error estimation of deformable image registration of pulmonary CT scans using convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<idno>024 003-024 003</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical prediction of registration misalignment using a convolutional LSTM: application to chest CT scans</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Elmahdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DARQ: deep learning of quality control for stereotaxic registration of human brain MRI to the T1w MNI-ICBM 152 template</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Fonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Adni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-A</forename><forename type="middle">R G</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page">119266</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards estimating MRI-Ultrasound registration error in image-guided neurosurgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bierbrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eskandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ferroelectrics, and Frequency Control</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Ultrasonics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal modulation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4203" to="4217" />
			<date type="published" when="2022">2022</date>
			<publisher>Curran Associates Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Retrospective evaluation of cerebral tumors (RESECT): a clinical database of pre-operative MRI and intra-operative ultrasound in low-grade glioma surgeries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Unsgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reinertsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3875" to="3882" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving probabilistic image registration via reinforcement learning and uncertainty evaluation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-02267-3_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-02267-324" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2013</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8184</biblScope>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of uncertainty quantification in deep learning: techniques, applications and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="243" to="297" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
