<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Baoru</forename><surname>Huang</surname></persName>
							<email>baoru.huang18@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hamlyn Centre for Robotic Surgery</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery &amp; Cancer</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yicheng</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hamlyn Centre for Robotic Surgery</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery &amp; Cancer</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<settlement>Liverpool</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stamatia</forename><surname>Giannarou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hamlyn Centre for Robotic Surgery</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery &amp; Cancer</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Elson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Hamlyn Centre for Robotic Surgery</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery &amp; Cancer</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="260" to="270"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DBD5F08AFEC0176BCCB53C2E90543476</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Laparoscopic Image-guided Intervention</term>
					<term>Minimally Invasive Surgery</term>
					<term>Detection of Sensing Area</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In surgical oncology, it is challenging for surgeons to identify lymph nodes and completely resect cancer even with pre-operative imaging systems like PET and CT, because of the lack of reliable intraoperative visualization tools. Endoscopic radio-guided cancer detection and resection has recently been evaluated whereby a novel tethered laparoscopic gamma detector is used to localize a preoperatively injected radiotracer. This can both enhance the endoscopic imaging and complement preoperative nuclear imaging data. However, gamma activity visualization is challenging to present to the operator because the probe is nonimaging and it does not visibly indicate the activity origination on the tissue surface. Initial failed attempts used segmentation or geometric methods, but led to the discovery that it could be resolved by leveraging highdimensional image features and probe position information. To demonstrate the effectiveness of this solution, we designed and implemented a simple regression network that successfully addressed the problem. To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system. Through intensive experimentation, we demonstrated that our method can successfully and effectively detect the sensing area, establishing a new performance benchmark. Code and data are available at https://github.com/br0202/Sensing area detection.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cancer remains a significant public health challenge worldwide, with a new diagnosis occurring every two minutes in the UK (Cancer Research UK <ref type="foot" target="#foot_0">1</ref> ). Surgery is one of the main curative treatment options for cancer. However, despite substantial advances in pre-operative imaging such as CT, MRI, or PET/SPECT to aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect cancerous tissues and disease metastases intra-operatively due to the lack of reliable intraoperative visualization tools. In practice, imprecise intraoperative cancer tissue detection and visualization results in missed cancer or the unnecessary removal of healthy tissues, which leads to increased costs and potential harm to the patient. There is a pressing need for more reliable and accurate intraoperative visualization tools for minimally invasive surgery (MIS) to improve surgical outcomes and enhance patient care. A recent miniaturized cancer detection probe (i.e., 'SENSEI R ' developed by Lightpoint Medical Ltd.) leverages the cancer-targeting ability of nuclear agents typically used in nuclear imaging to more accurately identify cancer intraoperatively from the emitted gamma signal (see Fig. <ref type="figure" target="#fig_0">1b</ref>) <ref type="bibr" target="#b5">[6]</ref>. However, the use of this probe presents a visualization challenge as the probe is non-imaging and is air-gapped from the tissue, making it challenging for the surgeon to locate the probe-sensing area on the tissue surface.</p><p>It is crucial to accurately determine the sensing area, with positive signal potentially indicating cancer or affected lymph nodes. Geometrically, the sensing area is defined as the intersection point between the gamma probe axis and the tissue surface in 3D space, but projected onto the 2D laparoscopic image. However, it is not trivial to determine this using traditional methods due to poor textural definition of tissues and lack of per-pixel ground truth depth data. Similarly, it is also challenging to acquire the probe pose during the surgery.</p><p>Problem Redefinition. In this study, in order to provide sensing area visualization ground truth, we modified a non-functional 'SENSEI' probe by adding a miniaturized laser module to clearly optically indicate the sensing area on the laparoscopic images -i.e. the 'probe axis-surface intersection'. Our system consists of four main components: a customized stereo laparoscope system for capturing stereo images, a rotation stage for automatic phantom movement, a shutter for illumination control, and a DAQ-controlled switchable laser module (see Fig. <ref type="figure" target="#fig_0">1a</ref>). With this setup, we aim to transform the sensing area localization problem from a geometrical issue to a high-level content inference problem in 2D. It is noteworthy that this remains a challenging task, as ultimately we need to infer the probe axis-surface intersection without the aid of the laser module to realistically simulate the use of the 'SENSEI' probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Laparoscopic images play an important role in computer-assisted surgery and have been used in several problems such as object detection <ref type="bibr" target="#b8">[9]</ref>, image segmentation <ref type="bibr" target="#b22">[23]</ref>, depth estimation <ref type="bibr" target="#b19">[20]</ref> or 3D reconstruction <ref type="bibr" target="#b12">[13]</ref>. Recently, supervised or unsupervised depth estimation methods have been introduced <ref type="bibr" target="#b13">[14]</ref>. Ye et al. <ref type="bibr" target="#b21">[22]</ref> proposed a deep learning framework for surgical scene depth estimation in self-supervised mode and achieved scalable data acquisition by incorporating a differentiable spatial transformer and an autoencoder into their framework. A 3D displacement module was explored in <ref type="bibr" target="#b20">[21]</ref> and 3D geometric consistency was utilized in <ref type="bibr" target="#b7">[8]</ref> for self-supervised monocular depth estimation. Tao et al. <ref type="bibr" target="#b18">[19]</ref> presented a spatiotemporal vision transformer-based method and a selfsupervised generative adversarial network was introduced in <ref type="bibr" target="#b6">[7]</ref> for depth estimation of stereo laparoscopic images. Recently, fully supervised methods were summarized in <ref type="bibr" target="#b0">[1]</ref> for depth estimation. However, acquiring per-pixel ground truth depth data is challenging, especially for laparoscopic images, which makes it difficult for large-scale supervised training <ref type="bibr" target="#b7">[8]</ref>.</p><p>Laparoscopic segmentation is another important task in computer-assisted surgery as it allows for accurate and efficient identification of instrument position, anatomical structures, and pathological tissue. For instance, a unified framework for depth estimation and surgical tool segmentation in laparoscopic images was proposed in <ref type="bibr" target="#b4">[5]</ref>, with simultaneous depth estimation and segmentation map generation. In <ref type="bibr" target="#b11">[12]</ref>, self-supervised depth estimation was utilized to regularize the semantic segmentation in knee arthroscopy. Marullo et al. <ref type="bibr" target="#b15">[16]</ref> introduced a multi-task convolutional neural network for event detection and semantic segmentation in laparoscopic surgery. The dual swin transformer U-Net was proposed in <ref type="bibr" target="#b10">[11]</ref> to enhance the medical image segmentation performance, which leveraged the hierarchical swin transformer into both the encoder and the decoder of the standard U-shaped architecture, benefiting from the self-attention computation in swin transformer as well as the dual-scale encoding design.</p><p>Although the intermediate depth information was not our final aim and can be bypassed, the 3D surface information was necessary in the intersection point inference. ResNet <ref type="bibr" target="#b2">[3]</ref> has been commonly used as the encoder to extract the image features and geometric information of the scene. In particular, in <ref type="bibr" target="#b20">[21]</ref>, concatenated stereo image pairs were used as inputs to achieve better results, and such stereo image types are also typical in robot-assisted minimally invasive surgery with stereo laparoscopes. Hence, stereo image data was also adopted in this paper.</p><p>If the problem of inferring the intersection point is treated as a geometric problem, both data collection and intra-operative registration would be difficult, which inspired us to approach this problem differently. In practice, we utilize the laser module to collect the ground truth of the intersection points when the laser is on. We note that the standard illumination image from the laparoscopic probe is also captured with the same setup when the laser module is on. Therefore, we can establish a dataset with an image pair (RGB image and laser image) that shares the same intersection point ground truth with the laser image (see Fig. <ref type="figure" target="#fig_1">2a</ref> and Fig. <ref type="figure" target="#fig_1">2b</ref>). The assumptions made are that the probe's 3D pose when projected into the two 2D images is the observed 2D pose, and that the intersection point is located on its axis. Hence, we input these axes to the network as another branch and randomly sampled points along them to represent the probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>To validate our proposed solution for the newly formulated problem, we acquired and publicly released two new datasets. In this section, we introduce the hardware and software design that was used to achieve our final goal, while Fig. <ref type="figure" target="#fig_1">2</ref> shows a sample from our dataset. Data Collection. Two miniaturized, high-resolution cameras were coupled onto a stereo laparoscope using a custom-designed connector. The accompanying API allowed for automatic image acquisition, exposure time adjustment, and white balancing. An electrically controllable shutter was incorporated into the standard laparoscopic illumination path. To indicate the probe axis-surface intersection, we incorporated a DAQ controlled cylindrical miniature laser module into a 'SENSEI' probe shell so that the adapted tool was visually identical to the real probe. The laser module emitted a red laser beam (wavelength 650 nm) that was visible as a red spot on the tissue surface. We acquired the dataset on a silicone tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color manually by hand to be visually realistic. The phantom was placed on a rotation stage that stepped 10 times per revolution to provide views separated by a 36-degree angle. At each position, stereo RGB images were captured i) under normal laparoscopic illumination with the laser off; ii) with the laparoscopic light blocked and the laser on; and iii) with the laparoscopic light blocked and the laser off. Subtraction of the images with laser on and off readily allowed segmentation of the laser area and calculation of its central point, i.e. the ground truth probe axis-surface intersection.</p><p>All data acquisition and devices were controlled by Python and LABVIEW programs, and complete data sets of the above images were collected on visually realistic phantoms for multiple probe and laparoscope positions. This provided 10 tissue surface profiles for a specific camera-probe pose, repeated for 120 different camera-probe poses, mimicking how the probe may be used in practice. Therefore, our first newly acquired dataset, named Jerry, contains 1200 sets of images. Since it is important to report errors in 3D and in millimeters, we recorded another dataset similar to Jerry but also including ground truth depth map for all frames by using structured-lighting system <ref type="bibr" target="#b7">[8]</ref>-namely the Coffbee dataset.</p><p>These datasets have multiple uses such as:</p><p>-Intersection point detection: detecting intersection points is an important problem that can bring accurate surgical cancer visualization. We believe this is an under-investigated problem in surgical vision. -Depth estimation: corresponding ground truth will be released.</p><p>-Tool segmentation: corresponding ground truth will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probe Axis-Surface Intersection Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>The problem of detecting the intersection point is trivial when the laser is on and can be solved by training a deep segmentation network. However, segmentation requires images with a laser spot as input, while the real gamma probe produces no visible mark and therefore this approach produces inferior results.</p><p>An alternative approach to detect the intersection point is to reconstruct the 3D tissue surface and estimate the pose of the probe in real time. A tracking and pose estimation method for the gamma probe <ref type="bibr" target="#b5">[6]</ref> involved attaching a dualpattern marker to the probe to improve detection accuracy. This enabled the derivation of a 6D pose, comprising a rotation matrix and translation matrix with respect to the laparoscope camera coordinate. To obtain the intersection point, the authors used the Structure From Motion (SFM) method to compute the 3D tissue surface, combining it with the estimated pose of the probe, all within the laparoscope coordinate system. However, marker-based tracking and pose estimation methods have sterilization implications for the instrument, and the SFM method requires the surgeon to constantly move the laparoscope, reducing the practicality of these methods for surgery.</p><p>In this work, we propose a simple, yet effective regression approach to address this problem. Our approach relies solely on the 2D information and works well without the need for the laser module after training. Furthermore, this simple methodology facilitated an average inference time of 50 frames per second, enabling real-time sensing area map generation for intraoperative surgery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intersection Detection as Segmentation</head><p>We utilized different deep segmentation networks as a first attempt to address our problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Please refer to the Supplementary Material for the implementation details of the networks. We observed that when we do not use images with the laser, the network was not able to make any good predictions. This is understandable as the red laser spot provides the key information for the segmentation. Therefore the network does not have any visual information to make predictions from images of the gamma probe. We note that to enable real-world applications, we need to estimate the intersection point using the images when the laser module is turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Intersection Detection as Regression</head><p>Problem Formulation. Formally, given a pair of stereo images I l , I r , n points {P l 1 , P l 2 , ..., P l n } were sampled along the principal axis of the probe, P l i ∈ R 2 from the left image. The same process was repeated for the right image. The goal was to predict the intersection point P intersect on the surface of the tissue. During the training, the ground truth intersection point position was provided by the laser source, while during testing the intersection was estimated solely based on visual information without laser guidance (see Fig. <ref type="figure">3</ref>).</p><p>Network Architecture. Unlike the segmentation approach, the intersection point was directly predicted using a regression network. The images fed to the network were 'laser off' stereo RGB, but crucially, the intersection point for these images was known a priori from the paired 'laser on' images. The raw image resolution was 4896×3680 but these were binned to 896×896. Principal Component Analysis (PCA) <ref type="bibr" target="#b14">[15]</ref> was used to extract the central axis of the probe and 50 points were sampled along this axis as an extra input dimension. A network was designed with two branches, one branch for extracting visual features from the image and one branch for learning the features from the sequence of principal points using ResNet <ref type="bibr" target="#b2">[3]</ref> and Vision Transformer (ViT) <ref type="bibr" target="#b1">[2]</ref> as two backbones. The principal points were learned through a multi-layer perception (MLP) or a long short-term memory (LSTM) network <ref type="bibr" target="#b3">[4]</ref>. The features from both branches were concatenated and used for regressing the intersection point (see Fig. <ref type="figure">4</ref>). Finally, the whole network is trained end-to-end using the mean square error loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>Evaluation Metrics. To evaluate sensing area location errors, Euclidean distance was adopted to measure the error between the predicted intersection points and the ground truth laser points. We reported the mean absolute error, the standard derivation, and the median in pixel units.</p><p>Implementation Details. The networks were implemented in PyTorch <ref type="bibr" target="#b16">[17]</ref>, with an input resolution of 896 × 896 and a batch size of 12. We partitioned the Jerry dataset into three subsets, the training, validation, and test set, consisting of 800, 200, and 200 images, respectively, and the same for the Coffbee dataset. The learning rate was set to 10 -5 for the first 300 epochs, then halved until epoch 400, and quartered until the end of the training. The model was trained for 700 epochs using the Adam optimizer on two NVIDIA 2080 Ti GPUs, taking approximately 4 h to complete. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Quantitative results on the released datasets are shown in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> with different backbones for extracting image features, ResNet and ViT. For the 2D error on two datasets, among the different settings, the combination of ResNet and MLP gave the best performance with a mean error of 70.5 pixels and a standard deviation of 56.8. The median error of this setting was 59.8 pixels while the R2 score was 0.82 (higher is better for R2 score). Comparing the Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, we found that the ResNet backbone was better than the ViT backbone in the image processing task, while MLP was better than LSTM in probe pose representation. ResNet processed the input images as a whole, which was better suited for utilizing the global context of a unified scene composed of the tissue and the probe, compared to the ViT scheme, which treated the whole scene as several patches. Similarly, the sampled 50 principal points on the probe axis were better processed using the simple MLP rather than using a recurrent procedure LSTM. It is worth noting that the results from stereo inputs exceeded those from mono inputs, which can be attributed to the essential 3D information included in the stereo image pairs.</p><p>For the 3D error, the ResNet backbone still gave generally better performance than the ViT backbone while under the ResNet backbone, LSTM and MLP gave competitive results and they are all in sub-milimeter level. We note that the 3D error subjected to the quality of the acquired ground truth depth maps, which had limited resolution and non-uniformly distributed valid data due to hardware constraints. Hence, we used the median depth value of a square area of 5 pixels around the points where depth value was not available.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> shows visualization results of our method using ResNet and MLP. This figure illustrates that our proposed method successfully detected the intersection point using solely standard RGB laparoscopic images as the input. Furthermore, based on the simple design, our method achieved the inference time of 50 frames per second, making it well-suitable for intraoperative surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, a new framework for using a laparoscopic drop-in gamma detector in manual or robotic-assisted minimally invasive cancer surgery was presented, where a laser module mock probe was utilized to provide training guidance and the problem of detecting the probe axis-tissue intersection point was transformed to laser point position inference. Both the hardware and software design of the proposed solution were illustrated and two newly acquired datasets were publicly released. Extensive experiments were conducted on various backbones and the best results were achieved using a simple network design, enabling real time inference of the sensing area. We believe that our problem reformulation and dataset release, together with the initial experimental results, will establish a new benchmark for the surgical vision community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Hardware set-up for experiments, including a customized portable stereo laparoscope system and the 'SENSEI' probe, a rotation stage, a laparoscopic lighting source, and a phantom; (b) An example of the use of the 'SENSEI' probe in MIS.</figDesc><graphic coords="2,56,46,244,10,339,49,119,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example data. (a) Standard illumination left RGB image; (b) left image with laser on and laparoscopic light off; same for (c) and (d) but for right images.</figDesc><graphic coords="4,56,97,407,27,338,32,61,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Sensing area detection. (a) The input RGB image, (b) The estimated line using PCA for obtaining principal points, (c) The image with laser on that we used to detect the intersection ground truth.</figDesc><graphic coords="6,56,46,195,62,339,58,83,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative results. (a) and (c) are standard illumination images and (b) and (d) are images with laser on and laparoscopic light off. The predicted intersection point is shown in blue and the green point indicates the ground truth, which are further indicated by arrows for clarity. (Color figure online)</figDesc><graphic coords="7,42,30,54,59,339,55,61,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results using ResNet50. Grey color denotes the Jerry dataset and Blue color is for Coffbee dataset (2D errors are in pixels and 3D errors are in mm).</figDesc><table><row><cell>ResNet</cell><cell></cell></row><row><cell>MLP</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell></row><row><cell>Stereo</cell><cell></cell></row><row><cell>Mono</cell><cell></cell></row><row><cell cols="2">2D Mean E. 73.5 70.5 73.7 75.6 76.7</cell></row><row><cell>2D Std.</cell><cell>65.1 56.8 62.1 62.9 64.4</cell></row><row><cell cols="2">2D Median 57.5 59.8 56.9 58.8 68.4</cell></row><row><cell cols="2">2D Mean E. 63.2 52.9 62.0 55.8 60.2</cell></row><row><cell>2D Std.</cell><cell>71.4 42.9 63.4 55.3 42.1</cell></row><row><cell cols="2">2D Median 44.9 44.6 43.4 42.5 52.3</cell></row><row><cell cols="2">R2 Score 0.55 0.82 0.63 0.73 0.78</cell></row><row><cell cols="2">3D Mean E. 8.5 7.4 6.5 6.4 11.2</cell></row><row><cell>3D Std.</cell><cell>15.7 6.7 6.8 7.1 18.2</cell></row><row><cell cols="2">3D Median 4.5 4.6 4.0 4.3 5.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results using ViT. Grey color denotes the Jerry dataset and Blue color is for Coffbee dataset (2D errors are in pixels and 3D errors are in mm).</figDesc><table><row><cell>ViTNet</cell><cell></cell></row><row><cell>MLP</cell><cell></cell></row><row><cell>LSTM</cell><cell></cell></row><row><cell>Stereo</cell><cell></cell></row><row><cell>Mono</cell><cell></cell></row><row><cell cols="2">2D Mean E. 77.9 92.3 80.9 87.7 112.1</cell></row><row><cell>2D Std.</cell><cell>69.1 71.0 67.4 68.6 84.2</cell></row><row><cell cols="2">2D Median 59.0 75.0 64.8 74.9 90.0</cell></row><row><cell cols="2">2D Mean E. 76.3 75.0 88.0 56.5 82.7</cell></row><row><cell>2D Std.</cell><cell>69.8 60.6 83.3 75.8 63.9</cell></row><row><cell cols="2">2D Median 59.9 59.6 68.3 34.5 69.1</cell></row><row><cell cols="2">R2 Score 0.58 0.66 0.33 0.65 0.60</cell></row><row><cell cols="2">3D Mean E. 7.9 9.1 11.4 11.6 7.7</cell></row><row><cell>3D Std.</cell><cell>6.9 8.2 16.7 21.3 7.0</cell></row><row><cell cols="2">3D Median 6.0 5.9 7.1 5.3 6.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.cancerresearchuk.org/health-professional/cancer-statistics-for-the-uk.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 25.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01133</idno>
		<title level="m">Stereo correspondence and reconstruction of endoscopic data challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous depth estimation and surgical tool segmentation in laparoscopic images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Robot. Bionics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="338" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking and visualization of the sensing area for a tethered laparoscopic gamma probe</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-020-02205-z</idno>
		<ptr target="https://doi.org/10.1007/s11548-020-02205-z" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1389" to="1397" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised generative adversarial network for depth estimation in laparoscopic images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-122" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised depth estimation in laparoscopic image using 3d geometric consistency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust real-time detection of laparoscopic instruments in robot surgery using convolutional neural networks with motion vector prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">2865</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Siamese neural networks for oneshot image recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DS-TransUNet: dual Swin transformer u-net for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised depth estimation to regularise semantic segmentation in knee arthroscopy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jonmohamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-858" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="594" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sage: slam with appearance and geometry prior for endoscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5587" to="5593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense depth estimation in monocular endoscopy with self-supervised learning methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1438" to="1447" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal components analysis (PCA)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maćkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ratajczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Geosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="342" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multi-task convolutional neural network for semantic segmentation and event detection in laparoscopic surgery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porpiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vezzetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personalized Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in pytorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SVT-SDE: spatiotemporal vision transformers-based self-supervised depth estimation in stereoscopic surgical videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Robot. Bionics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stereo depth estimation via self-supervised contrastive representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tukra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-158" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation with 3-D displacement module for laparoscopic images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Elson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Robot. Bionics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="334" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08260</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Surgical scene segmentation using semantic image synthesis with a virtual surgery environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-153" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
