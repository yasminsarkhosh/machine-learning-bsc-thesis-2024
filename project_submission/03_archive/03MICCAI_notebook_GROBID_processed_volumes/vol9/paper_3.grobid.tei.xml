<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Surgical Video Captioning with Mutual-Modal Concept Alignment</title>
				<funder ref="#_ewVZWaC #_Evk9FZs #_azDxZjX #_qxTZAeW #_tXHmEDb #_KDZStMX">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_aqcadAs">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_uuVfTma">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Chen</surname></persName>
							<email>zhen.chen@cair-cas.org.hk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="department" key="dep2">Institute of Science and Innovation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hong Kong, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="department" key="dep2">Institute of Science and Innovation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hong Kong, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leo</forename><forename type="middle">K T</forename><surname>Yeung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danny</forename><forename type="middle">T M</forename><surname>Chan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Surgery</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="department" key="dep2">Institute of Science and Innovation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hong Kong, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongbin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="department" key="dep2">Institute of Science and Innovation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hong Kong, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
							<email>jqwang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre for Artificial Intelligence and Robotics (CAIR)</orgName>
								<orgName type="department" key="dep2">Institute of Science and Innovation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hong Kong, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Wuhan AI Research</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">ObjectEye Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Surgical Video Captioning with Mutual-Modal Concept Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="24" to="34"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">03A1F71BA2AA1781E4B0840DB814A445</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neurosurgery</term>
					<term>Video caption</term>
					<term>Surgical concept</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic surgical video captioning is critical to understanding surgical procedures, and can provide the intra-operative guidance and the post-operative report generation. As the overlap of surgical workflow and vision-language learning, this cross-modal task expects precise text descriptions of complex surgical videos. However, current captioning algorithms neither fully leverage the inherent patterns of surgery, nor coordinate the knowledge of visual and text modalities well. To address these problems, we introduce the surgical concepts into captioning, and propose the Surgical Concept Alignment Network (SCA-Net) to bridge the visual and text modalities via surgical concepts. Specifically, to enable the captioning network to accurately perceive surgical concepts, we first devise the Surgical Concept Learning (SCL) to predict the presence of surgical concepts with the representations of visual and text modalities, respectively. Moreover, to mitigate the semantic gap between visual and text modalities of captioning, we propose the Mutual-Modality Concept Alignment (MC-Align) to mutually coordinate the encoded features with surgical concept representations of the other modality. In this way, the proposed SCA-Net achieves the surgical concept alignment between visual and text modalities, thereby producing more accurate captions with aligned multi-modal knowledge. Extensive experiments on neurosurgery videos and nephrectomy images confirm the effectiveness of our SCA-Net, which outperforms the state-of-the-arts by a large margin. The source code is available at https://github.com/franciszchen/SCA-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic surgical video captioning is critical to understanding the surgery with complicated operations, and can produce the natural language description with given surgical videos <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. In this way, these techniques can reduce the workload of surgeons with multiple applications, such as providing the intraoperative surgical guidance <ref type="bibr" target="#b16">[17]</ref>, generating the post-operative surgical report <ref type="bibr" target="#b3">[4]</ref>, and even training junior surgeons <ref type="bibr" target="#b7">[8]</ref>.</p><p>To generate text descriptions from input videos, existing captioning works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> mostly consist of a visual encoder for visual representations and a text decoder for text generation. Some early works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> adopted a fixed object detector as the visual encoder to capture object representations for text decoding. This paradigm in Fig. <ref type="figure" target="#fig_0">1</ref>(a) requires auxiliary annotations (e.g., bounding box) to pre-train the visual encoder, and cannot adequately train the entire network for captioning. To improve performance with high efficiency in practice, recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> followed the detector-free strategy, and opened up the joint optimization of visual encoder and text decoder towards captioning, as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Despite great progress in this field, these works can be further improved with two limitations of surgical video captioning.</p><p>First, existing surgical captioning works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> did not fully consider the inherent patterns of surgery to facilitate captioning. Due to the variability of lesions and surgical operations, surgical videos contain complex visual contents, and thus it is difficult to directly learn the mapping from the visual input to the text output. In fact, the same type of surgery has relatively fixed semantic patterns, such as using specific surgical instruments for a certain surgical action. Therefore, we introduce the surgical concepts (e.g., surgical instruments, operated targets and surgical actions) from a semantic perspective, and guide the surgical captioning network to perceive these surgical concepts in the input video to generate more accurate surgical descriptions. Second, existing studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> simply processed visual and text modalities in sequential, while ignoring the semantic gap between these two modalities. This restricts the integration of visual and text modality knowledge, thereby damaging the captioning performance. Considering that both visual and text modalities revolve around the same set of surgical concepts, we aim to align the features in the visual and text modalities with each other through surgical concepts, and achieve more efficient multi-modal fusion for accurate text predictions.</p><p>To address these two limitations in surgical video captioning, we propose the Surgical Concept Alignment Network (SCA-Net) to bridge the visual and text modalities through the surgical concepts, as illustrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Specifically, to enable the SCA-Net to accurately perceive surgical concepts, we first devise the Surgical Concept Learning (SCL) to predict the presence of surgical concepts with the representations of visual and text modalities, respectively. Moreover, to mitigate the semantic gap between visual and text modalities of captioning, we propose the Mutual-Modality Concept Alignment (MC-Align) to mutually coordinate the encoded features with surgical concept representations of the other modality. In this way, the proposed SCA-Net achieves the surgical concept alignment between visual and text modalities, thereby producing more accurate captions with aligned multi-modal knowledge. To the best of our knowledge, this work represents the first effort to introduce the surgical concepts for the surgical video captioning. Extensive experiments are performed on neurosurgery video and nephrectomy image datasets, and demonstrate the effectiveness of our SCA-Net by remarkably outperforming the state-of-the-art captioning works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Surgical Concept Alignment Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of SCA-Net</head><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the Surgical Concept Alignment Network (SCA-Net) follows the advanced captioning architecture <ref type="bibr" target="#b24">[25]</ref>, and consists of visual and text encoders, and a multi-modal decoder. We implement the visual encoder with VideoSwin <ref type="bibr" target="#b14">[15]</ref> to capture the discriminative spatial and temporal representations from input videos, and utilize the Vision Transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> with causal mask <ref type="bibr" target="#b5">[6]</ref> as the text encoder to exploit text semantics with merely previous text tokens. The multi-modal decoder with ViT structure takes both visual and text tokens as input, and finally generates the caption of the input video. Moreover, to accurately perceive surgical concepts in SCL (Sect. 2.2), the SCA-Net learns from surgical concept labels using separate projection heads after the visual and text encoders. In the MC-Align (Sect. 2.3), the visual and text tokens from two encoders are mutually aligned with the concept representations of the other modality for better multi-modal decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Surgical Concept Learning</head><p>Previous surgical captioning works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> generated surgical descriptions directly from input surgical videos. Considering the variability of lesions and surgical operations, these methods may struggle to understand complex visual contents and generate erroneous surgical descriptions, thereby hindering performance to meet clinical requirements. In fact, both the surgical video and surgical caption represent the same surgical semantics in different modalities. Therefore, we decompose surgical operations into surgical concepts, and guide these two modalities to accurately perceive the presence of surgical concepts, so as to better complete this cross-modal task. Given a type of surgery, we regard the surgical instruments, surgical actions and the operated targets used in surgical videos as surgical concepts. Considering that both the visual input and the shifted text input contain the same set of surgical concepts, we find out which surgical concepts appear in the input surgical video by parsing the caption label. In this way, the presence of surgical concepts can be represented in a multi-hot surgical concept label y cpt ∈ {0, 1} C , where the surgical concepts that appear in the video are marked as 1 and the rest are marked as 0, and C is the number of possible surgical concepts. For example, the surgical video in Fig. <ref type="figure" target="#fig_1">2</ref> contains the instrument cutting forcep, the action removing and the target bone, and thus the surgical concept label y cpt represents these surgical concepts in corresponding dimensions.</p><p>To guide the visual modality to perceive surgical concepts, we aggregate visual tokens generated by the visual encoder in average, and add a linear layer to predict the surgical concepts of input videos, where the normalized output p v ∈ [0, 1] C estimates the probability of each surgical concept. We perform the multi-label classification using binary sigmoid cross-entropy loss, as follows:</p><formula xml:id="formula_0">L v SCL = - C c=1 y cpt c log (p v c ) + 1 -y cpt c log (1 -p v c ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>In this way, the visual tokens are supervised to contain discriminative semantics related to valid surgical concepts, which can reduce prediction errors in surgical descriptions. For the text modality, we also perform SCL for surgical concept prediction p t c ∈ [0, 1] C and calculate the loss L t SCL in the same way. By optimizing L SCL = L v SCL + L t SCL , the SCL enables visual and text encoders to exploit multi-modal features with the perception of surgical concepts, thereby facilitating the SCA-Net towards the captioning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mutual-Modality Concept Alignment</head><p>With the help of SCL in Sect. 2.2, our SCA-Net can perceive the shared set of surgical concepts in both visual and text modalities. However, given the differences between two modalities with separate encoders, it is inappropriate for the decoder to directly explore the cross-modal relationship between visual and text tokens <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. To mitigate the semantic gap of two modalities, we devise the MC-Align to bridge these tokens in different modalities through surgical concept representations for better multi-modal decoding, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>To align these two modalities, we first collect surgical concept representations for each modality. Note that text tokens are separable for surgical concepts, while visual tokens are part of the input video containing multiple surgical concepts. For text modality, we parse the label of each text token and average text tokens of each surgical concept as t c , and update the historical text concept representations { tc } C c=1 using Exponential Moving Average (EMA), as tc ← γ tc +(1-γ)t c , where the coefficient γ controls the updating for stable training and is empirically set as 0.9. For visual modality, we average visual tokens as the representation of each surgical concept present in the input video (i.e., v c if surgical concept label y cpt c = 1), and update the historical visual concept representations {v c } C c=1 with EMA, as vc ← γ vc + (1γ)v c . In this way, we obtain the text and visual concept representations with tailored strategies for the alignment.</p><p>Then, we mutually align visual and text concept representations with corresponding historical ones in another modality. For visual-to-text alignment, visual concept representations are expected to be similar to corresponding text concept representations, while differing from other text concept representations as possible. Thus, we calculate the alignment objective L v→t MCA with regard to surgical concepts <ref type="bibr" target="#b9">[10]</ref>, and the visual encoder can be optimized with the gradients of visual concept representations in backward, thereby gradually aligning visual modality to text modality. Similarly, text concept representations are also aligned to the historical visual ones, as text-to-visual alignment L t→v MCA . The MC-Align is summarized as follows:</p><formula xml:id="formula_2">L MCA = - vi∈V log exp (v i • ti ) C c=1 exp (v i • tc ) L v→t MCA - tj ∈T log exp (t j • vj ) C c=1 exp (t j • vc ) L t→v MCA ,<label>(2)</label></formula><p>where V and T denote all visual and text representations respectively, and • is the inner product of vectors. In this way, the MC-Align aligns visual and text representations with each other modality according to the surgical concept, thus benefiting multi-modal decoding for captioning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>For the surgical captioning task, we adopt standard captioning loss L Cap to optimize the cross-entropy of each predicted word based on previous words y &lt;t and input video x, as follows:</p><formula xml:id="formula_3">L Cap = - T t=1 log p (y t | y &lt;t , x) , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where T is the length of caption prediction. Overall, the final objective of SCA-Net is summarized as L = L Cap + λ 1 L SCL + λ 2 L MCA , where loss coefficients λ 1 and λ 2 control the trade-off of SCL and MC-Align. By optimizing this final objective L, the proposed SCA-Net can achieve multi-modal concept alignment, and generate superior descriptions for the surgical video captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>Neurosurgery Video Captioning Dataset. To evaluate the effectiveness of surgical video captioning, we collect a large-scale dataset with 41 surgical videos of endonasal skull base neurosurgery. These surgical videos are recorded at the Prince of Wales Hospital, Chinese University of Hong Kong, where surgeons remove pituitary tumors through the endonasal corridor to the skull base. After necessary data cleaning, we divide these surgical videos with resolution of 1, 920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes. These video clips are annotated under Tool-Tissue Interaction (TTI) principle <ref type="bibr" target="#b17">[18]</ref>, and include a total of 16 instruments, 8 targets, and 10 surgical actions. The annotation preprocessing follows <ref type="bibr" target="#b25">[26]</ref> using NLTK <ref type="bibr" target="#b15">[16]</ref> toolkit. The proportion of surgical concepts is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. We split these video clips at patientlevel, where the video clips of 31 patients are used for training and the rest of 10 patients are utilized for test.</p><p>EndoVis Image Captioning Dataset. We further compare our method with state-of-the-arts on the public EndoVis-2018 Image Captioning Dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. This dataset reveals robotic nephrectomy procedures acquired by the da Vinci X or Xi system, and is annotated with surgical actions between 9 possible tools and surgical targets <ref type="bibr" target="#b22">[23]</ref>. We follow the official split in <ref type="bibr" target="#b23">[24]</ref> with 11 sequences for training and 3 sequences for test. In this way, these two datasets can comprehensively evaluate the captioning tasks under both surgical videos and images.</p><p>Implementation Details. We implement our SCA-Net and state-of-the-art captioning methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> in PyTorch <ref type="bibr" target="#b19">[20]</ref>. We optimize the SCA-Net and compared captioning methods using Adam with the batch size of 12 for both captioning datasets. All models are trained for 20 and 50 epochs in neurosurgery and EndoVis datasets, respectively. We adopt the step-wise learning rate decay strategy to facilitate training convergence, where the learning rate is initialized as 1 × 10 -2 and halved after every 5 epochs. The loss coefficients λ 1 of L SCL and λ 2 of of L MCA are empirically set to 0.1 and 0.01, respectively. All experiments are performed on a single NVIDIA A100 GPU.</p><p>Evaluation Metrics. To evaluate the captioning performance, we adopt standard metrics, including BLEU@4 <ref type="bibr" target="#b18">[19]</ref>, METEOR <ref type="bibr" target="#b2">[3]</ref>, SPICE <ref type="bibr" target="#b1">[2]</ref>, ROUGE <ref type="bibr" target="#b11">[12]</ref> and CIDEr <ref type="bibr" target="#b21">[22]</ref>. Specifically, BLEU@4 <ref type="bibr" target="#b18">[19]</ref> evaluates the 4-gram precision of the predicted caption, and CIDEr <ref type="bibr" target="#b21">[22]</ref> is based on the n-gram similarity with TF-IDF weights. METEOR <ref type="bibr" target="#b2">[3]</ref> considers both precision and recall. ROUGE <ref type="bibr" target="#b11">[12]</ref> and SPICE <ref type="bibr" target="#b1">[2]</ref> measure the matching between predictions and ground truth. The higher scores of these metrics indicate better performance in surgical captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison on Neurosurgery Video Captioning</head><p>To evaluate the performance of our SCA-Net, we perform a comprehensive comparison with the state-of-the-art captioning methods, including Self-Seq <ref type="bibr" target="#b20">[21]</ref>, AOANet <ref type="bibr" target="#b8">[9]</ref>, SIG-Former <ref type="bibr" target="#b25">[26]</ref>, M 2 Transformer <ref type="bibr" target="#b4">[5]</ref>, and SwinMLP-TranCAP <ref type="bibr" target="#b23">[24]</ref>.</p><p>As illustrated in Table <ref type="table" target="#tab_0">1</ref>, our SCA-Net achieves the best performance, with the overwhelming BLEU@4 of 48.1%, METEOR of 35.1% and CIDEr of 368.4%. Noticeably, our SCA-Net outperforms the surgical captioning work, SwinMLP-TranCAP <ref type="bibr" target="#b23">[24]</ref>, by a large margin, e.g., <ref type="bibr" target="#b15">16</ref>.9% in SPICE and 13.0% in ROUGE. This advantage confirms that the proposed surgical concept alignment can alleviate the modalities gap in surgical captioning. Moreover, compared with the second-best M 2 Transformer <ref type="bibr" target="#b4">[5]</ref> with meshed attention between the visual encoder and the text decoder, our SCA-Net obtains superior performance with a remarkable increase of 9.5% in SPICE and 7.1% in ROUGE. These experimental results demonstrate the performance advantage of our SCA-Net over state-of-the-arts in the neurosurgery video captioning.</p><p>Ablation Study. To further validate the effectiveness of SCL and MC-Align, we perform the detailed ablation study in Table <ref type="table" target="#tab_0">1</ref>. Specifically, we implement three ablative baselines of the proposed SCA-Net, by removing the MC-Align (denoted as w/o MC-Align) and the SCL (denoted as w/o SCL) individually, as well as removing both (denoted as w/o SCL, MC-Align). As illustrated in Table <ref type="table" target="#tab_0">1</ref>, the proposed SCL and MC-Align can bring an individual improvement of 4.0% and 5.5% in BLEU@4, respectively, to the baseline of 40.3%. Furthermore, the SCL and MC-Align can work together to facilitate the captioning, with a BLEU@4 gain of 7.8%. These ablation experiments confirm that the proposed SCL and MC-Align play an important role in solving the modality gap in surgical video captioning, resulting in the performance advantage of our SCA-Net.</p><p>Qualitative Analysis. We present qualitative results of our SCA-Net and state-of-the-arts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> on neurosurgery video captioning. In Fig. <ref type="figure" target="#fig_3">4</ref>(a), SwinMLP-TranCAP <ref type="bibr" target="#b23">[24]</ref> and M 2 Transformer <ref type="bibr" target="#b4">[5]</ref> incorrectly predict the operated targets and ignore important surgical instruments, respectively, and both methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> cannot recognize the rare instrument ultrasound probe as well as the corresponding surgical action in Fig. <ref type="figure" target="#fig_3">4(b)</ref>. With the help of surgical concept alignment, our SCA-Net can perceive the surgical concepts present in the surgical videos and thus generate correct descriptions in these two complex videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison on EndoVis Image Captioning</head><p>To further confirm the effectiveness of surgical captioning, we perform the comparison on the public EndoVis image captioning dataset. As shown in Table <ref type="table" target="#tab_1">2</ref>, the end-to-end captioning methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> outperform the detector-based works using instrument bounding box as auxiliary annotations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, by optimizing the visual encoder to meet the requirement of the captioning task. In particular, our SCA-Net with Swin Transformer <ref type="bibr" target="#b13">[14]</ref> as visual encoder achieves the best performance of four metrics (e.g., 47.6% in BLEU@4 and 58.4% in SPICE), and outperforms the surgical state-of-the-art <ref type="bibr" target="#b23">[24]</ref> with the advantage of 7.3% in BLEU@4 and 5.1% in METEOR. These comparisons confirm that our SCA-Net with surgical concept alignment can produce more accurate surgical captions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To achieve accurate surgical video captioning, we propose the SCA-Net to mitigate the semantic gap of visual and text modalities with surgical concepts. Specifically, we devise the SCL to enable the SCA-Net with the perception of surgical concepts in visual and text modalities, respectively. Moreover, we propose the MC-Align to mutually coordinate visual and text representations with surgical concept representations of the other modality for multi-modal decoding, thereby generating more accurate captions with aligned multi-modal knowledge.</p><p>Extensive experiments on neurosurgery and nephrectomy datasets confirm the advantage of our SCA-Net over state-of-the-arts on the surgical captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Differences between existing captioning works (a) and (b), and our method (c). Different from directly mapping from visual input to text output in (a) and (b), we perform the surgical concept learning of two modality-specific encoders and mutually align two modalities for better multi-modal decoding.</figDesc><graphic coords="2,75,39,68,45,312,49,69,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Surgical Concept Alignment Network (SCA-Net) includes visual and text encoders, and a multi-modal decoder. The SCL supervises two encoders with projection heads by surgical concept labels, and the MC-Align mutually coordinates two modalities with concept representations for better multi-modal decoding.</figDesc><graphic coords="4,57,48,107,69,331,66,196,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Surgical concepts and proportions in neurosurgery video captioning dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The qualitative comparison between our SCA-Net and state-of-the-arts. With surgical concept alignment, our SCA-Net generates more accurate surgical descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison on neurosurgery video captioning dataset. Best and second best results are highlighted and underlined.</figDesc><table><row><cell>Method</cell><cell cols="5">BLEU@4 METEOR SPICE ROUGE CIDEr</cell></row><row><cell cols="2">VideoSwin + Self-Seq [21] 34.4</cell><cell>24.8</cell><cell>39.7</cell><cell>53.5</cell><cell>183.8</cell></row><row><cell cols="2">VideoSwin + AOANet [9] 41.5</cell><cell>29.7</cell><cell>46.5</cell><cell>58.0</cell><cell>288.1</cell></row><row><cell>SIG-Former [26]</cell><cell>36.2</cell><cell>30.1</cell><cell>35.8</cell><cell>52.0</cell><cell>181.7</cell></row><row><cell cols="2">SwinMLP-TranCAP [24] 39.8</cell><cell>28.7</cell><cell>39.2</cell><cell>51.9</cell><cell>195.9</cell></row><row><cell>M 2 Transformer [5]</cell><cell>43.2</cell><cell>30.9</cell><cell>46.6</cell><cell>57.8</cell><cell>317.8</cell></row><row><cell cols="2">Ours w/o SCL, MC-Align 40.3</cell><cell>29.6</cell><cell>46.4</cell><cell>55.7</cell><cell>279.8</cell></row><row><cell>Ours w/o MC-Align</cell><cell>44.3</cell><cell>32.4</cell><cell>52.6</cell><cell>61.8</cell><cell>298.9</cell></row><row><cell>Ours w/o SCL</cell><cell>45.8</cell><cell>32.9</cell><cell>53.2</cell><cell>62.7</cell><cell>325.1</cell></row><row><cell>Ours</cell><cell>48.1</cell><cell>35.1</cell><cell>56.1</cell><cell>64.9</cell><cell>368.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison on EndoVis-2018 image captioning dataset. Best and second best results are highlighted and underlined.</figDesc><table><row><cell>Method</cell><cell cols="5">Aux. Anno BLEU@4 METEOR SPICE CIDEr</cell></row><row><cell>FasterRCNN + Self-seq [21]</cell><cell></cell><cell>29.5</cell><cell>28.3</cell><cell>49.6</cell><cell>180.1</cell></row><row><cell>FasterRCNN + AOANet [9]</cell><cell></cell><cell>37.7</cell><cell>32.4</cell><cell>58.0</cell><cell>181.1</cell></row><row><cell>SIG-Former [26]</cell><cell>✗</cell><cell>42.6</cell><cell>33.5</cell><cell>52.4</cell><cell>282.6</cell></row><row><cell>SwinMLP-TranCAP [24]</cell><cell>✗</cell><cell>40.3</cell><cell>31.3</cell><cell>54.7</cell><cell>250.4</cell></row><row><cell>M 2 Transformer [5]</cell><cell>✗</cell><cell>43.0</cell><cell>32.5</cell><cell>55.3</cell><cell>245.2</cell></row><row><cell>Ours</cell><cell>✗</cell><cell>47.6</cell><cell>36.4</cell><cell>58.4</cell><cell>300.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>. This work is supported by <rs type="funder">National Key R&amp;D Program of China</rs> under Grant No. <rs type="grantNumber">2021YFE0205700</rs>, <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62276260</rs>, <rs type="grantNumber">62076235</rs>, <rs type="grantNumber">62176254</rs>, <rs type="grantNumber">61976210</rs>, <rs type="grantNumber">62002356</rs>, <rs type="grantNumber">62006230</rs>), sponsored by <rs type="person">Zhejiang Lab</rs> (No. <rs type="grantNumber">2021KH0AB07</rs>) and the <rs type="programName">InnoHK program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aqcadAs">
					<idno type="grant-number">2021YFE0205700</idno>
				</org>
				<org type="funding" xml:id="_ewVZWaC">
					<idno type="grant-number">62276260</idno>
				</org>
				<org type="funding" xml:id="_Evk9FZs">
					<idno type="grant-number">62076235</idno>
				</org>
				<org type="funding" xml:id="_azDxZjX">
					<idno type="grant-number">62176254</idno>
				</org>
				<org type="funding" xml:id="_qxTZAeW">
					<idno type="grant-number">61976210</idno>
				</org>
				<org type="funding" xml:id="_tXHmEDb">
					<idno type="grant-number">62002356</idno>
				</org>
				<org type="funding" xml:id="_KDZStMX">
					<idno type="grant-number">62006230</idno>
				</org>
				<org type="funding" xml:id="_uuVfTma">
					<idno type="grant-number">2021KH0AB07</idno>
					<orgName type="program" subtype="full">InnoHK program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46454-124" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: an automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generation of surgical reports using keyword-augmented next sequence prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Direct. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="387" to="390" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Meshed-memory transformer for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10578" to="10587" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OperA: attention-regularized transformers for surgical phase recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-158" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surgical training fit for the future: the need for a change</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Postgrad. Med. J</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="820" to="823" />
			<date type="published" when="1165">1165. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention on attention for image captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SGT: scene graph-guided transformer for surgical report generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-148" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="507" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ROUGE: a package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SwinBERT: end-to-end transformers with sparse attention for video captioning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17949" to="17958" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno>arXiv preprint cs/0205028</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Artificial intelligence for intraoperative guidance: using semantic segmentation to identify surgical anatomy during laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CholecTriplet 2021: a benchmark challenge for surgical action triplet recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102803</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
		<respStmt>
			<orgName>ACL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m">PyTorch: an imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CIDEr: consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class-incremental domain adaptation with smoothing and calibration for surgical report generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-126" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking surgical captioning: end-to-end windowbased MLP transformer using patches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-136" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="376" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CoCa: contrastive captioners are image-text foundation models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Trans</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Surgical instruction generation with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-128" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
