<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery</title>
				<funder>
					<orgName type="full">NIHR Academic Clinical Fellowship</orgName>
				</funder>
				<funder ref="#_FmQ6x6W">
					<orgName type="full">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
					<orgName type="abbreviated">WEISS</orgName>
				</funder>
				<funder ref="#_bvxnMfD">
					<orgName type="full">WEISS</orgName>
				</funder>
				<funder>
					<orgName type="full">Royal Academy of Engineering Chair in Emerging Technologies Scheme</orgName>
				</funder>
				<funder ref="#_pDVWTPH #_YrQzead #_K399yqm #_zqYf2gr">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
					<orgName type="abbreviated">EPSRC</orgName>
				</funder>
				<funder ref="#_z3643Bg">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_FUwjfNm">
					<orgName type="full">Cancer Research UK (CRUK)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Adrito</forename><surname>Das</surname></persName>
							<email>adrito.das.20@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danyal</forename><forename type="middle">Z</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Hanrahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anouk</forename><surname>Borg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><forename type="middle">L</forename><surname>Dorward</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophia</forename><surname>Bano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hani</forename><forename type="middle">J</forename><surname>Marcus</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Neurosurgery</orgName>
								<orgName type="department" key="dep2">National Hospital for Neurology and Neurosurgery</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="472" to="482"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E6F26DC3F5BD019B66AF661DA0F42231</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>minimally invasive surgery</term>
					<term>semantic segmentation</term>
					<term>surgical AI</term>
					<term>surgical vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pituitary tumours are in an anatomically dense region of the body, and often distort or encase the surrounding critical structures. This, in combination with anatomical variations and limitations imposed by endoscope technology, makes intra-operative identification and protection of these structures challenging. Advances in machine learning have allowed for the opportunity to automatically identifying these anatomical structures within operative videos. However, to the best of the authors' knowledge, this remains an unaddressed problem in the sellar phase of endoscopic pituitary surgery. In this paper, PAINet (Pituitary Anatomy Identification Network), a multi-task network capable of identifying the ten critical anatomical structures, is proposed. PAINet jointly learns: (1) the semantic segmentation of the two most prominent, largest, and frequently occurring structures (sella and clival recess); and (2) the centroid detection of the remaining eight less prominent, smaller, and less frequently occurring structures. PAINet utilises an EfficientNetB3 encoder and a U-Net++ decoder with a convolution layer for segmentation and pooling layer for detection. A dataset of 64-videos (635 images) were recorded, and annotated for anatomical structures through multi-round expert consensus. Implementing 5-fold cross-validation, PAINet achieved 66.1% and 54.1% IoU for sella and clival recess semantic segmentation respectively, and 53.2% MPCK-20% for centroid detection of the remaining eight structures, improving on single-task performances. This therefore demonstrates automated identification of anatomical critical structures in the sellar phase of endoscopic pituitary surgery is possible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A difficulty faced by surgeons performing endoscopic pituitary surgery is identifying the areas of the bone which are safe to open. This is of particular importance during the sellar phase as there are several critical anatomical structures within close proximity of each other <ref type="bibr" target="#b8">[9]</ref>. The sella, behind which the pituitary tumour is located, is safe to open. However, the smaller structures surrounding the sella, behind which the optic nerves and internal carotid arteries are located, carry greater risk. Failure to appreciate these critical parasellar neurovascular structures can lead to their injury, and adverse outcomes for the patient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. The human identification of these structures relies on visual clues, inferred from the impressions made on the bone, rather than direct visualisations of the structures <ref type="bibr" target="#b10">[11]</ref>. This is especially challenging as the pituitary tumour often compresses; distorts; or encases the surrounding structures <ref type="bibr" target="#b10">[11]</ref>. Neurosurgeons utilise identification instruments, such as a stealth pointer or micro-doppler, to aid in this task <ref type="bibr" target="#b8">[9]</ref>. However, once an identification instrument is removed, identification is lost upon re-entry with a different instrument, and so the identification can only be used in referenced to the more visible anatomical landmarks. Automatic identification from endoscopic vision may therefore aid surgeons in this effort while minimising disruption to the surgical workflow <ref type="bibr" target="#b10">[11]</ref>. This is a challenging computer vision task due to the narrow camera angles enforced by minimally invasive surgery, which lead to: (i) structure occlusions by instruments and biological factors (e.g., blood); and (ii) image blurring caused by rapid camera movements. Additionally, in this specific task there are: (iii) numerous small structures; (iv) visually similar structures; and (v) unclear structure boundaries. Hence, the task can be split into two sub-tasks to account for these difficulties in identification: (1) the semantic segmentation of the two larger, visually distinct, and frequently occurring structures (sella and clival recess); and (2) the centroid detection of the eight smaller structures (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>To solve both tasks simultaneously, PAINet (Pituitary Anatomy Identification Network) is proposed. This paper's contribution is therefore:</p><p>1. The automated identification of the ten critical anatomical structures in the sellar phase of endoscopic pituitary surgery. To the best of the authors' knowledge, this is the first work addressing the problem at this granularity. 2. The creation of PAINet, a multi-task neural network capable of simultaneously semantic segmentation and centroid detection of numerous anatomical structures within minimally invasive surgery. PAINet uniquely utilises two loss functions for improved performance over single-task neural networks due to the increased information gain from the complementary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Encoder-decoder architectures are the leading models in semantic segmentation and landmark detection <ref type="bibr" target="#b3">[4]</ref>, with common architectures for anatomy identification including the U-Net and DeepLab families <ref type="bibr" target="#b5">[6]</ref>. Improvements to these models include: adversarial training to limit biologically implausible predictions <ref type="bibr" target="#b13">[14]</ref>; spatial-temporal transformers for scene understanding across consecutive frames <ref type="bibr" target="#b4">[5]</ref>; transfer learning from similar anatomical structures <ref type="bibr" target="#b2">[3]</ref>; and graph neural networks for global image understanding <ref type="bibr" target="#b1">[2]</ref>. Multi-task networks improve on the baseline models by leveraging common characteristics between sub-tasks, increasing the total information provided to the network <ref type="bibr" target="#b14">[15]</ref>, and are effective at instrument segmentation in minimally invasive surgery <ref type="bibr" target="#b9">[10]</ref>.</p><p>The most clinically similar works to this paper are: <ref type="bibr" target="#b0">(1)</ref> The semantic segmentation of 3-anatomical-structures in the nasal phase of endoscopic pituitary surgery <ref type="bibr" target="#b11">[12]</ref>. Here, U-Net was weakly-supervised on centroids, outputting segmentation masks for each structure. Training on 18-videos (367-images), the model achieved statistically significant results (P &lt; 0.001) on the hold-out testing dataset of 5-videos (182-images) when compared to a location prior baseline model <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b1">(2)</ref> The semantic segmentation of: 2-zones (safe or dangerous); and 3-anatomical-structures in laparoscopic cholecystectomy <ref type="bibr" target="#b6">[7]</ref>. Here, two PSPNets were fully-supervised on 290-videos (2627 images) using 10-fold cross-validation, achieving 62% mean intersection over union (MIoU) for the 2-zones; and 74% MIoU for the 3-structures <ref type="bibr" target="#b6">[7]</ref>. These works are extended in this paper by increasing the number of anatomical structures and the identification granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>PAINet: A multi-task encoder-decoder network is proposed to improve performance by exchanging information between the semantic segmentation and centroid detection tasks. EfficientNetB3, pre-trained on ImageNet, is used as the encoder because of its accuracy, computational efficiency and proven generalisation capabilities <ref type="bibr" target="#b12">[13]</ref>. The decoder is based on U-Net++, a state-of-the-art segmentation network widely used in medical applications <ref type="bibr" target="#b15">[16]</ref>. The encoderdecoder architecture is modified to output both segmentation and centroid predictions by sending the decoder output into two separate layers: (1) a convolution for segmentation prediction; and (2) an average pooling layer for centroid prediction. Different loss functions were minimised for each sub-task (Fig. <ref type="figure" target="#fig_1">2</ref>). Ablation studies and granular details are provided below. The priority was to find the optimal sella segmentation model, as it is required to be opened to access the pituitary tumour behind it, indicating the surgical "safe-zone".</p><p>Semantic Segmentation: First, single-class sella segmentation models were trialed. 8-encoders (pre-trained convolution neural networks) and 15-decoders were used, with their selection based off architecture variety. Two loss functions were also used: (1) distribution-based logits cross-entropy; and (2) region-based Jaccard loss. Boundary-based loss functions were not trialed as: (1) the boundary of the segmentation masks are not well-defined; and (2) in the cases of split structures (Fig. <ref type="figure" target="#fig_0">1c</ref>), boundary-based loss functions are not appropriate <ref type="bibr" target="#b7">[8]</ref>. The decoder output is passed through a convolution layer and sigmoid activation.</p><p>For multi-class sella and clival recess segmentation, the optimal single-class model was extended by: (1) sending through each class to the loss function separately (multi-class separate); and (2) sending both classes through together (multi-class together). An extension of logits cross-entropy, logits focal loss, was used instead as it accounts for data imbalance between classes.</p><p>Centroid Detection: 5-models were trialed: 3-models consisted of encoders with a convolution layer and linear activation; and 2-models consisted of encoderdecoders with an average pooling layer and sigmoid activation with 0.3 dropout. Two distance-based loss functions were trialed: (1) mean squared error (MSE); and (2) mean absolute error (MAE). Loss was calculated for all structures simultaneously as a 16 dimensional output (8 centroids × 2 coordinates) and set to 0 for a structure if ground-truth centroids of that structure was not present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Evaluation Metrics: For sella segmentation the evaluation metric is intersection over union (IoU), as commonly used in the field <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. For multi-class segmentation, the model that optimises clival recess IoU without reducing the previously established sella IoU is chosen. Precision and recall are also given.</p><p>For centroid detection, the evaluation metric is mean percentage of correct keypoints (MPCK) with the threshold set to 20%, indicating the mean number of predicted centroids falling within 144 pixels of the ground-truth centroid. This is commonly used in anatomical detection tasks as it ensures the predictions are close to the ground-truth while limiting overfitting <ref type="bibr" target="#b0">[1]</ref>. MPCK-40% and MPCK-10%, along with the mean percentage of centroids that fall within their corresponding segmentation mask (mean percentage of centroid masks (MPCM)) are given as secondary metrics. For multi-task detection, MPCK-20% is optimised such that sella IoU does not drop from the previously established optimal IoU.</p><p>Network Parameters: 5-fold cross-validation was implemented with no holdout testing. To account for structure data imbalance, images were randomly split such that the number of structures in each fold is approximately even. Images from a singular video were present in either the training or validation dataset.</p><p>Each model was run for with a batch size of 5 for 20 epochs, where the epoch with the best primary evaluation metric on the validation dataset was kept. The optimising method was Adam with varying initial learning rates, with a separate optimiser for each loss function during multi-task training.</p><p>All images were scaled to 736 × 1280 pixels for model compatibility, and training images were randomly augmented within the following parameters: shift in any direction by up to 10%; zooming in or out about the image center by up to 10%; rotation about the image center clockwise or anticlockwise by up to π/6; increasing or decreasing brightness, contrast, saturation, and hue by up to 10%.</p><p>The code is written in Python 3.8 using PyTorch 1.8.1, run on a single NVIDIA Tesla V100 Tensor Core 32-GB GPU using CUDA 11.2, and is available at https://github.com/dreets/pitnet-anat-public. For PAINet, a batch size of 5 utilised 29-GB and the runtime was approximately 5-min per epoch. Valuation runtime is under 0.1-s per image and therefore a real-time overlay on-top of the endoscope video feed is feasible intra-operatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset Description</head><p>Images: Images come from 64-videos of endoscopic pituitary surgery where the sellar phase is present <ref type="bibr" target="#b8">[9]</ref>, recorded between 30 Aug 2018 and 20 Feb 2021 from The National Hospital of Neurology and Neurosurgery, London, United Kingdom. All patients have provided informed consent, and the study was registered with the local governance committee. A high-definition endoscope (Hopkins Telescope, Karl Storz Endoscopy) was used to record the surgeries at 24 frames per second (fps), with at least 720p resolution, and stored as mp4 files. 10-images corresponding to 10-s of the sellar phase immediately preceding sellotomy were extracted from each video at 1 fps, and stored as 720p png files. Video upload and annotation was performed using Touch Surgery TM Enterprise. Annotations: Expert neurosurgeons identified 10-anatomical-structures as critical based on the literature (Fig. <ref type="figure" target="#fig_0">1a</ref>) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. 640-images were manually segmented to obtain ground-truth segmentations. A two-stage process was used: (1) two neurosurgeons segmented each image, with any differences settled through discussion; (2) two consultant neurosurgeons independently peer-reviewed the segmentations. Only visible structures were annotated (Fig. <ref type="figure" target="#fig_0">1b</ref>); if the structures were occluded, the segmentation boundaries were drawn around these occlusions (Fig. <ref type="figure" target="#fig_0">1c</ref>); and if an image is too blurry to see the structures no segmentation boundaries were drawn -this excluded 5 images (Fig. <ref type="figure" target="#fig_0">1d</ref>). The center of mass of each segmentation mask was defined as the centroid.</p><p>The sella is present in all 635-images (Fig. <ref type="figure" target="#fig_2">3</ref>). Other than the clival recess, the remaining 8-structures are found in less than 65% of images, with planum sphenoidal found in less than 25% of images. Moreover, the area covered by these 8-structures are small, with several covering less than 10% of the total area covered by all structures in a given image. Furthermore, most smaller structures boundaries are ambiguous as they are hard to define even by expert neurosurgeons. This emphasizes the challenge of identifying smaller structure in computer vision, and supports the need for detection and multi-task solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>Quantitative evaluation is calculated for: single-class sella segmentation (Table <ref type="table" target="#tab_0">1</ref>); single-class, multi-class, and PAINet 2-structures segmentation (Table <ref type="table" target="#tab_1">2</ref>); multi-class, and PAINet 8-structures centroid detection (Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>).</p><p>The optimal model for single-class sella segmentation achieved 65.4% IoU, utilising an EfficientNetB3 encoder; U-Net++ decoder; Jaccard loss; and a 0.001 initial learning rate. Reductions in IoU are seen when alternative parameters are used, highlighting their impact on model performance. Using the optimal sella model configuration, 53.4% IoU is achieved for singleclass clival recess segmentation. Extending this to multi-class and PAINet training improves both sella and clival recess IoU to 66.1% and 54.1% respectively.</p><p>The optimal model for centroid detection achieves 51.7% MPCK-20%, with minor deviations during model parameter changes. This model, ResNet18 with MSE loss, outperforms the more sophisticated models, as these models over-learn image features in the training dataset. However, PAINet leverages the additional information from segmentation masks to achieve an improved 53.2%.</p><p>The per structure PCK-20% indicate performance is positively correlated with the number of images where the structure is present. This implies the limiting factor is the number of images rather than architectural design.   Qualitative predictions of the best performing model, PAINet, are displayed in Fig. <ref type="figure" target="#fig_3">4</ref>. The segmentation predictions look strong, with small gaps from the ground-truth. However, this is expected as structure boundaries are not welldefined. The centroid predictions are weaker: in (a) the planum sphenoidale (grey) is predicted within the segmentation mask; in (b) three structures are within their segmentation mask, but the left optic-carotid recess (orange) is predicted in a biologically implausible location; and in (c) this is repeated for the right carotid (pink) and no structures are within their segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Identification of critical anatomical structures by neurosurgeons during endoscopic pituitary surgery remains a challenging task. In this paper, the potential of automating anatomical structure identification during surgery was shown. The proposed multi-task network, PAINet, designed to incorporate identification of both large prominent structures and numerous smaller less prominent structures, was trained on images of the sellar phase of endoscopic pituitary surgery. Using 635-images from 64-surgeries annotated by expert neurosurgeons and various model configurations, the robustness of the PAINet was shown over single task networks. PAINet achieved 66.1% (+0.7%) and 54.1% IoU (+0.7%) for sella and clival recess segmentation respectively, a higher performance than other minimally invasive surgeries <ref type="bibr" target="#b6">[7]</ref>. PAINet also achieved 53.2% MPCK-20% (+1.5%) for detection of the remaining 8-structures. The most important structures to identify and avoid, the carotids and optic protuberances, have high performance, and therefore demonstrate the success of PAINet. This performance is greater than similar studies in endoscopic pituitary surgery for different structures <ref type="bibr" target="#b11">[12]</ref> but lower than anatomical detection in other surgeries <ref type="bibr" target="#b0">[1]</ref>. Collecting data from more pituitary surgeries will support incorporating anatomy variations and achieving generalisability. Furthermore, introducing modifications to the model architecture, such as the use of temporal networks <ref type="bibr" target="#b4">[5]</ref>, will further boost performance required for real-time video clinical translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. 10-anatomical-structures semantic segmentation of the sellar phase in endoscopic pituitary surgery. Names and mask colour are given in the legend, with centroids displayed as dots. Example images where each visible critical anatomical structures' mask is annotated by neurosurgeons, are displayed: (a) an image where all structures are clearly seen; (b) an image where only the sella is visible; (c) an image where some structures are occluded by an instrument and biological factors; and (d) an image where none of the structures are identifiable due to blurriness caused by camera movement. (Color figure online)</figDesc><graphic coords="2,55,98,194,30,340,24,144,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multi-task (semantic segmentation and centroid detection) architecture diagram. Notice the two output layers (convolution and pooling) and two loss functions.</figDesc><graphic coords="4,55,98,215,87,340,24,129,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distribution of 10 anatomical structures in 635-images: The top bars display structure frequency; and the bottom bars display each structures area relative to the total area covered by all structures in a given image (mean-averaged across all images).</figDesc><graphic coords="6,55,98,220,07,340,12,142,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. PAINet predictions for three videos, displaying images with: (a) strong; (b) typical; and (c) poor performances. (The color map is given in Fig. 1.)</figDesc><graphic coords="8,55,98,430,10,340,18,132,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Selected models performance on the single-class sella segmentation task (5fold cross validation). The model with the highest IoU is given in top-most row (in bold), with each row changing one model parameter (in italics), where the highest and lowest IoU for a given model parameter change is shown. Complete results for all models can be found in Supplementary Material Table1. *Initial learning rate.</figDesc><table><row><cell>Decoder</cell><cell>Encoder</cell><cell>Loss</cell><cell cols="2">Rate* IoU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell cols="3">U-Net++ EfficientNetB3 Jaccard</cell><cell cols="4">0.001 65.4±1.6 79.3±2.6 79.5±3.9</cell></row><row><cell cols="2">DeepLabv3+ EfficientNetB3</cell><cell>Jaccard</cell><cell cols="4">0.001 63.7 ± 2.9 77.0 ± 4.7 79.5 ± 2.1</cell></row><row><cell>PSPNet</cell><cell>EfficientNetB3</cell><cell cols="5">Cross-Entropy 0.001 54.5 ± 4.1 74.3 ± 1.9 68.4 ± 3.5</cell></row><row><cell>U-Net++</cell><cell>EfficientNetB3</cell><cell cols="5">Cross-Entropy 0.001 64.8 ± 2.3 79.4 ± 2.5 78.6 ± 2.6</cell></row><row><cell>U-Net++</cell><cell>Xception</cell><cell>Jaccard</cell><cell cols="4">0.001 60.1 ± 4.6 78.2 ± 2.9 77.9 ± 4.3</cell></row><row><cell>U-Net++</cell><cell>ResNet18</cell><cell>Jaccard</cell><cell cols="4">0.001 56.3 ± 3.2 73.3 ± 4.0 74.2 ± 5.2</cell></row><row><cell>U-Net++</cell><cell>EfficientNetB3</cell><cell>Jaccard</cell><cell cols="4">0.0001 63.4 ± 1.3 72.7 ± 4.7 83.6 ± 6.3</cell></row><row><cell>U-Net++</cell><cell>EfficientNetB3</cell><cell>Jaccard</cell><cell>0.01</cell><cell cols="3">34.7 ± 9.5 68.7 ± 8.2 69.6 ± 8.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Selected models performance for sella and clival recess segmentation (5-fold cross-validation). All models use an EfficientNetB3 encoder and U-Net++ decoder. The best performing network, as determined by sella IoU, is displayed in bold.</figDesc><table><row><cell>Training</cell><cell>Loss</cell><cell>Sella IoU Clival Recess IoU</cell></row><row><cell>Single-Class</cell><cell cols="2">Jaccard 65.4 ± 1.6 53.4 ± 5.9</cell></row><row><cell>Multi-Class Separate</cell><cell>Focal</cell><cell>65.4 ± 2.1 54.2 ± 8.5</cell></row><row><cell>Multi-Class Together</cell><cell>Focal</cell><cell>65.6 ± 2.6 49.9 ± 4.8</cell></row><row><cell cols="3">Multi-task (PAINet) Focal 66.1 ± 2.3 54.1 ± 5.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Selected models performance for centroid detection (5-fold cross-validation). The model with the highest MPCK-20% is given in the last row (in bold). Complete results for all models can be found in Supplementary Material Table2.</figDesc><table><row><cell>Training</cell><cell>Decoder</cell><cell>Encoder</cell><cell>Loss MPCK-20 MPCK-40 MPCK-10 MPCM</cell></row><row><cell cols="2">Multi-Class -</cell><cell>ResNet18</cell><cell>MSE 51.7 ± 9.2 58.0 ± 7.0 28.4 ± 5.9 09.7 ± 2.8</cell></row><row><cell cols="2">Multi-Class -</cell><cell>ResNet18</cell><cell>MAE 51.4 ± 9.4 57.9 ± 7.8 34.2 ± 5.3 10.9 ± 2.5</cell></row><row><cell cols="2">Multi-Class -</cell><cell>EfficientNetB3</cell><cell>MSE 46.8 ± 8.3 55.5 ± 5.7 19.5 ± 3.2 06.3 ± 3.0</cell></row><row><cell cols="3">Multi-Class DeepLabv3+ ResNet18</cell><cell>MSE 50.4 ± 6.2 57.8 ± 5.8 20.1 ± 5.4 06.5 ± 1.8</cell></row><row><cell cols="2">Multi-Class U-Net++</cell><cell>ResNet18</cell><cell>MSE 50.1 ± 4.2 58.0 ± 6.8 26.1 ± 2.7 09.7 ± 1.2</cell></row><row><cell cols="2">Multi-Class U-Net++</cell><cell>EfficientNetB3</cell><cell>MSE 48.</cell></row></table><note><p><p><p>1 ± 3.7 56.2 ± 4.7 26.4 ± 4.8 06.8 ± 1.6</p>PAINet</p>U-Net++ EfficientNetB3 MSE 53.2 ± 5.9 58.0 ± 6.9 39.6 ± 3.2 13.4 ± 2.9</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>PAINet 8-structures centroid detection performance (5-fold cross validation).</figDesc><table><row><cell>Structure Left</cell><cell>Left Optic-</cell><cell>Left Optic</cell><cell>Planum</cell><cell cols="2">Tuberculum Right Optic Right Optic-</cell><cell>Right</cell></row><row><cell>Carotid</cell><cell cols="4">Carotid Recess Protuberance Sphenoidale Sellae</cell><cell cols="2">Protuberance Carotid Recess Carotid</cell></row></table><note><p>PCK-20% 68.3 ± 9.2 37.6 ± 4.8 53.9 ± 7.3 27.6 ± 2.4 76.1 ± 2.8 72.0 ± 9.1 34.8 ± 3.0 55.4 ± 8.5</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was funded in whole, or in part, by the <rs type="funder">Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)</rs> [<rs type="grantNumber">203145/Z/16/Z</rs>]; the <rs type="funder">Engineering and Physical Sciences Research Council (EPSRC)</rs> [<rs type="grantNumber">EP/P027938/1</rs>, <rs type="grantNumber">EP/R004080/1</rs>, <rs type="grantNumber">EP/P012841/1</rs>, <rs type="grantNumber">EP/W00805X/1</rs>]; and the <rs type="funder">Royal Academy of Engineering Chair in Emerging Technologies Scheme</rs>. AD is supported by <rs type="funder">EPSRC</rs> [<rs type="grantNumber">EP/S021612/1</rs>]. HJM is supported by <rs type="funder">WEISS</rs> [<rs type="grantNumber">NS/A000050/1</rs>] and by the <rs type="institution">National Institute for Health and Care Research (NIHR) Biomedical Research Centre at University College London</rs> (<rs type="affiliation">UCL</rs>). DZK and JGH are supported by the <rs type="funder">NIHR Academic Clinical Fellowship</rs>. DZK is supported by the <rs type="funder">Cancer Research UK (CRUK)</rs> <rs type="grantName">Predoctoral Fellowship</rs>. With thanks to <rs type="institution">Digital Surgery Ltd</rs>, a Medtronic company, for access to <rs type="institution">Touch Surgery TM Enterprise</rs> for both video recording and storage.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FmQ6x6W">
					<idno type="grant-number">203145/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_pDVWTPH">
					<idno type="grant-number">EP/P027938/1</idno>
				</org>
				<org type="funding" xml:id="_YrQzead">
					<idno type="grant-number">EP/R004080/1</idno>
				</org>
				<org type="funding" xml:id="_K399yqm">
					<idno type="grant-number">EP/P012841/1</idno>
				</org>
				<org type="funding" xml:id="_zqYf2gr">
					<idno type="grant-number">EP/W00805X/1</idno>
				</org>
				<org type="funding" xml:id="_z3643Bg">
					<idno type="grant-number">EP/S021612/1</idno>
				</org>
				<org type="funding" xml:id="_bvxnMfD">
					<idno type="grant-number">NS/A000050/1</idno>
				</org>
				<org type="funding" xml:id="_FUwjfNm">
					<orgName type="grant-name">Predoctoral Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_45.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automating periodontal bone loss measurement via dental landmark localisation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Danks</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02431-z</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02431-z" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1189" to="1199" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gaggion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mansilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mosquera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Milone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2022.3224660</idno>
		<ptr target="https://doi.org/10.1109/tmi.2022.3224660" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="546" to="556" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contrastive semi-supervised learning for domain adaptive segmentation across similar anatomical structures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2022.3209798</idno>
		<ptr target="https://doi.org/10.1109/tmi.2022.3209798" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="256" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A brief survey on semantic segmentation with deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.11.118</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2019.11.118" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring intra-and inter-video relation for surgical semantic scene segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2022.3177077</idno>
		<ptr target="https://doi.org/10.1109/tmi.2022.3177077" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2991" to="3002" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anatomy-aided deep learning for medical image segmentation: a review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N J</forename><surname>Veldhuis</surname></persName>
		</author>
		<idno type="DOI">10.1088/1361-6560/abfbf4</idno>
		<idno>11TR01</idno>
		<ptr target="https://doi.org/10.1088/1361-6560/abfbf4" />
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Artificial intelligence for intraoperative guidance using semantic segmentation to identify surgical anatomy during laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<idno type="DOI">10.1097/sla.0000000000004594</idno>
		<ptr target="https://doi.org/10.1097/sla.0000000000004594" />
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="363" to="369" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Metrics reloaded: pitfalls and recommendations for image analysis validation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Godau</surname></persName>
		</author>
		<idno type="DOI">10.48550/arxiv.2206.01653</idno>
		<idno>2206.01653</idno>
		<ptr target="https://doi.org/10.48550/arxiv" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pituitary society expert Delphi consensus: operative workflow in endoscopic transsphenoidal pituitary adenoma resection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11102-021-01162-3</idno>
		<ptr target="https://doi.org/10.1007/s11102-021-01162-3" />
	</analytic>
	<monogr>
		<title level="j">Pituitary</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="839" to="853" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-task convolutional neural network for semantic segmentation and event detection in laparoscopic surgery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porpiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vezzetti</surname></persName>
		</author>
		<idno type="DOI">10.3390/jpm13030413</idno>
		<ptr target="https://doi.org/10.3390/jpm13030413" />
	</analytic>
	<monogr>
		<title level="j">J. Personal. Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skull base anatomy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Fernandez-Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.otc.2015.09.001</idno>
		<idno>09.001</idno>
		<ptr target="https://doi.org/10.1016/j.otc.2015" />
	</analytic>
	<monogr>
		<title level="j">Otolaryngol. Clin. North Am</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="20" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine vision for real-time intraoperative anatomic guidance: a proof-of-concept study in endoscopic pituitary surgery</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Staartjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volokitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Regli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Serra</surname></persName>
		</author>
		<idno type="DOI">10.1093/ons/opab187</idno>
		<ptr target="https://doi.org/10.1093/ons/opab187" />
	</analytic>
	<monogr>
		<title level="j">Oper. Neurosurg</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="242" to="247" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1905.11946</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1905.11946" />
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CAT: constrained adversarial training for anatomically-plausible semi-supervised segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2023.3243069</idno>
		<ptr target="https://doi.org/10.1109/tmi.2023.3243069" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1093/nsr/nwx105</idno>
		<ptr target="https://doi.org/10.1093/nsr/nwx105" />
	</analytic>
	<monogr>
		<title level="j">Natl. Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="43" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
