<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing</title>
				<funder ref="#_zpV6aUW">
					<orgName type="full">Wellcome/EPSRC</orgName>
				</funder>
				<funder ref="#_dxd6UE8">
					<orgName type="full">Medtronic/RAEng Research Chair</orgName>
				</funder>
				<funder ref="#_GxP8Ny9">
					<orgName type="full">NIHR</orgName>
				</funder>
				<funder ref="#_yDB4GXh">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_YyUPdyk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Charlie</forename><surname>Budd</surname></persName>
							<email>charles.budd@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianrong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Craniofacial and Regenerative Biology</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oscar</forename><surname>Maccormac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">King&apos;s College Hospitals</orgName>
								<address>
									<addrLine>Denmark Hill</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Huber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Mower</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mirek</forename><surname>Janatka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Hypervision Surgical Limited</orgName>
								<address>
									<addrLine>1st Floor 85 Great Portland Street</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Trotouin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Hypervision Surgical Limited</orgName>
								<address>
									<addrLine>1st Floor 85 Great Portland Street</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Shapey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Hypervision Surgical Limited</orgName>
								<address>
									<addrLine>1st Floor 85 Great Portland Street</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mads</forename><forename type="middle">S</forename><surname>Bergholt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Craniofacial and Regenerative Biology</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Biomedical Engineering and Imaging Science</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Hypervision Surgical Limited</orgName>
								<address>
									<addrLine>1st Floor 85 Great Portland Street</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="658" to="667"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E9B4ED57B34C8A173D29AB663834227A</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Autofocus</term>
					<term>Deep Reinforcement Learning</term>
					<term>Hyperspectral Imaging</term>
					<term>Computer Assisted Intervention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperspectral imaging (HSI) captures a greater level of spectral detail than traditional optical imaging, making it a potentially valuable intraoperative tool when precise tissue differentiation is essential. Hardware limitations of current optical systems used for handheld realtime video HSI result in a limited focal depth, thereby posing usability issues for integration of the technology into the operating room. This work integrates a focus-tunable liquid lens into a video HSI exoscope, and proposes novel video autofocusing methods based on deep reinforcement learning. A first-of-its-kind robotic focal-time scan was performed to create a realistic and reproducible testing dataset. We benchmarked our proposed autofocus algorithm against traditional policies, and found our novel approach to perform significantly (p &lt; 0.05) better than traditional techniques (0.070 ± .098 mean absolute focal error compared to 0.146 ± .148). In addition, we performed a blinded usability trial by having two neurosurgeons compare the system with different autofocus policies, and found our novel approach to be the most favourable, making our system a desirable addition for intraoperative HSI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Traditional optical imaging samples the visual spectrum in three diffuse spectral bands (RGB), while hyperspectral imaging (HSI) provides much more detailed spectral information. This information is potentially valuable for making intraoperative decisions, particularly in cases where tissue differentiation is critical but challenging to perform using traditional visualisation techniques. In the case of brain tumour excision, fluorescence-guided resection is commonly used to minimize damage to healthy tissue <ref type="bibr" target="#b1">[2]</ref> but is limited to high-grade gliomas, and results in added cost and workflow disruptions. Thanks to a more detailed definition between tissue types <ref type="bibr" target="#b4">[5]</ref>, HSI is seen as a promising alternative with wider applicability and smoother integration into the workflow.</p><p>While HSI has been integrated into surgical microscope systems <ref type="bibr" target="#b10">[11]</ref>, it is suggested that handheld systems are better suited to translational research <ref type="bibr" target="#b3">[4]</ref>. Such handheld systems consist of an exoscope coupled to a draped optical stack, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The optics in the exoscope typically result in a short focal depth, making manual focusing tricky, particularly as the tuning must be performed through the drape. As such, these systems are commonly left at a fixed focal power and the surgeon must keep the working distance fixed to keep the subject in focus. Furthermore, the narrow spectral bands of HSI sensors reduce the amount of light collected <ref type="bibr" target="#b11">[12]</ref>. To avoid increasing exposure time, a large aperture size is needed, at a cost of further reducing focal depth. This exacerbates the focusing issues, making current real-time handheld HSI imaging systems particularly challenging to focus, posing significant usability issues. Figure <ref type="figure" target="#fig_0">1</ref> highlights the limited focal depth of our system, and shows a typical target that the surgeon must manually bring into focus during surgery.</p><p>The issue of reduced focal depth in real-time HSI systems could be mitigated by the introduction of a video autofocus system. Autofocus methods are divided into active methods, which use transmission to probe the scene, and passive methods, which rely only on incoming light. Passive methods are further split into phase-based, which require specialised hardware, and contrast-based, which compare images captured at different focal powers. Our investigation focuses on contrast-based methods, which require minimal hardware development. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Autofocusing Works</head><p>While autofocusing systems are prevalent in consumer device, the scientific literature is sparse, especially for dynamic video autofocusing. Many publications in the field are concerned with benchtop microscope autofocus systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>. This environment is conducive to autofocus as the scene is typically static with a single focal plane across the whole image. Additionally, the focus can be adjusted easily by moving the stage vertically. <ref type="bibr" target="#b7">[8]</ref> take a traditional approach, making use of a Laplacian focal metric combined with a modified hill-climber optimisation scheme. <ref type="bibr" target="#b14">[15]</ref> input a stack of sequential images to a 3d convolutional neural network (CNN) trained as a deep reinforcement agent trained to output changes in stage height. <ref type="bibr" target="#b6">[7]</ref> train a CNN to regress the optimal focal power from just two images taken at different focal powers. Beyond benchtop microscopy, <ref type="bibr" target="#b5">[6]</ref> also use a CNN to directly regress optimal focal powers, this time from varying number of samples from the full focal stacks. <ref type="bibr" target="#b0">[1]</ref> take a novel approach by using pre-trained object detection models to generate latent vector representations of images and using these as inputs to a deep reinforcement agent. <ref type="bibr" target="#b13">[14]</ref> train two CNNs, one to regress focal steps from a single image, the other to determine if the current image is in focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Contributions</head><p>This work aims to improve intraoperative handheld HSI systems by alleviating one of their main usability drawbacks, that of shortened focal depth. We introduce an autofocus system to an existing handheld intraoperative real-time HSI system <ref type="bibr" target="#b3">[4]</ref>. The focus adjustments are handled by a focus tunable liquid lens which is integrated into the setup. We propose autofocusing policies based on deep reinforcement learning and compare these to traditional heuristic approaches. Our final model is similar to that presented in <ref type="bibr" target="#b14">[15]</ref> but differs in its use of a weight shared image encoder, software simulated defocusing for training data, and small input patch size. In addition, our method is designed and trained to handle dynamic environments, something entirely missing in the literature. We performed a robotic focal-time scan to create a reproducible testing benchmark and allow quantitative comparison of autofocus policies. Finally, we demonstrate the utility of our approach in a blinded user study involving two neurosurgeons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Optical System</head><p>Our intraoperative HSI system, shown in Fig. <ref type="figure" target="#fig_1">2</ref>, builds on our existing system <ref type="bibr" target="#b3">[4]</ref> by integrating an Optotune EL-10-30-Ci focus-tunable liquid lens to allow electrical control of the focal length. The hyperspectral camera is based on an IMEC 2/3" snapshot mosaic CMV2K-SSM4X4-VIS sensor, which acquires 16 spectral bands in a 4 × 4 mosaic between the spectral range of 460 nm and 600 nm. With a sensor resolution of 2048 × 1088 pixels, hyperspectral data is acquired with a spatial resolution of 512 × 272 pixels per spectral band. Video-rate imaging of snapshot data is achieved with a speed of up to 50 FPS depending on acquisition parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>Software Simulated Focal-Time Scans. We define a focal-time scan as a time series of focal stacks, with a focal stack being a single image captured at multiple focal lengths. In order to assemble a large and diverse focal-time scan dataset, we choose to simulate focal-time scans using existing in-focus video data. To ensure the resulting focal-time scan features diverse camera motion, we implement a smooth random walk to step a cropping rectangle across the video after each frame. This also allows for the construction of plausible focaltime scans from single images, although features such as dynamic subjects or imaging noise will be missing. In order to simulate defocus, we implement another random walk to simulate a dynamic optimal focal power. When an agent is interacting with the simulated scan, a Gaussian filter is used to approximate focal blurring with σ = σ 0 |f *f | where f and f * are the current and optimal focal powers and σ 0 is chosen randomly from the range 2-8 for each scan. We use this technique to create a training and testing dataset consisting of 1000 and 200 simulated focal-time scans based on 200 10-second video clips sampled from Cholec80 <ref type="bibr" target="#b12">[13]</ref>, a popular endoscopic dataset. In addition, we created simulated focal-time scans from 200 in focus images taken of a brain phantom with our HSI system. These act as a validation dataset to help prevent over fitting and aid generalisation. While Gaussian blur is a reasonable approximation, we note that more rigours methods exist to simulate defocus blur that may produce better simulated data <ref type="bibr" target="#b8">[9]</ref>.</p><p>Robotic Focal-Time Scan. As a testing dataset similar to our intended use case, we chose to approximate a real focal-time scan by controlling conditions during capture of the individual focal stacks. Our optical system was fixed to a robotic arm, which was then used in a compliant control mode to record a natural hand-guided trajectory whilst imaging a brain phantom. The motion was performed to try to emulate typical usage during a surgery, whilst also trying to cover the range of plausible working distances. The focal range of the liquid lens is discretised into a set of focal powers, and the recorded trajectory is discretised into a sequence of 1184 poses. For each discrete pose, the robotic arm is fixed, and an image captured for each focal power. We randomise the order of the focal powers to reduce systematic bias caused by the response of the liquid lens. Auto-exposure was implemented in order to ensure good exposure across all working distances. To ensure consistency within a given focal stack, autoexposure was only stepped in-between discrete poses. The robotic arm holding our optical system and a sample of the resulting focal-time scan can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>. The optimal focus for all focal stacks was computed via global search of a traditional focal metric (mean gradient magnitude) as detailed below. This was then validated visually and corrected where appropriate.</p><p>Integration and Usability Trial. To ensure the validity of our quantitative evaluation, and to get feedback on the system in general, a blinded trial was set up with two practising neurosurgeons. A set was made containing two repeats of three selected autofocus policies. This set was then shuffled, and the surgeons remained blinded to the autofocus policy until after the trial. Each surgeon used our optical system to inspect a brain phantom with each policy in the set. The surgeon was made aware when the policy was changed and prompted to make comments throughout the trial, which were recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Autofocus Policies</head><p>As seen in Fig. <ref type="figure" target="#fig_0">1</ref>, the area of surgical interest can make up a rather small amount of the overall image, as such, we limit ourselves to a patch size of just 32 × 32 pixels. The positioning of the patch could be dictated by a second algorithm or user input, but this is outside the scope of this work. Here, we simply position the patch at the centre of the circular content area, which is detected using the method presented in <ref type="bibr" target="#b2">[3]</ref>. All of our autofocus policies deal with the grayscale reconstruction of the HSI images. Throughout this work, we further deal with a normalised focal power range (0-1).</p><p>Traditional Approach. We implement two traditional autofocus policy based on different focal metrics combined with a simple hill-climber optimisation policy. We choose mean gradient magnitude (MGM) and mean local ratio (MLR). Two focal metrics which are conceptually simple but competitive <ref type="bibr" target="#b5">[6]</ref> and implemented in quite different ways. They are defined as</p><formula xml:id="formula_0">φ MGM (I) = 1 n p I x 2 (p) + I y 2 (p) (1) φ MLR (I) = 1 n p max G σ (I)(p) + 1 I(p) + 1 , I(p) + 1 G σ (I)(p) + 1 (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where p is the set of all pixels in the image, I x and I y are defined as the x and y responses of a Sobel filter, and G σ is a Gaussian blur. The kernel size is chosen as σ = 4 for all our experiments. Our hill-climber optimisation policy O HC sets the focal power f at time t + 1 based on information at time t and is defined as</p><formula xml:id="formula_2">f t+1 = O HC (φ t , f t , φ t-1 ) = f t + d prev h, if 0 &lt; f t &lt; 1 and φ t &gt; φ t-1 f t -d prev h, otherwise<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">d prev = sign(f t -f t-1</formula><p>) is the direction of the previous step and h is a step size which we set to h = 0.05 for all our experiments. We note that our definition is different from standard hill-climber. A normal hill-climber will repeat a step while the focal metric is increasing, and either stop or change direction with a smaller step size when the focal metric decreases, but this does not translate to a continuous and dynamic environment.</p><p>Learned Optimisation Policy. Due to our dynamic environment, it seems likely that considering a sequence of the N last focal metrics, rather than the last two, would help to build a strong optimisation policy. However, as N increases, it quickly becomes unclear how to incorporate this information effectively. It is likely that a learning based solution would uncover a better strategy than heuristic approaches. While regression based approcahes may work, reinforcement learning provides a natural framework for this problem by allowing the policy to model the trade-off between maximisation and exploration. By modelling the autofocus task as a Markov process, we can define a Q-function Q(s, a) which maps state-action pairs to expected future rewards. We define our state, actions, and reward function as</p><formula xml:id="formula_4">s t = {φ t , f t , ..., φ t-(N -1) , f t-(N -1) } A = {-h, 0, +h} r t = -|f * t -f t |</formula><p>where f * t is the optimal focal power at t which can only be known in controlled environment. As before, we take h = 0.05. Our learned optimisation policy O RL can then be defined as</p><formula xml:id="formula_5">f t+1 = O RL (f t , s t ) = f t + max a Q(s t , a)<label>(4)</label></formula><p>To model Q(s, a), we use an MLP consisting of 2 hidden layers of 256 ReLUs each and a third layer with 3 outputs corresponding to the 3 possible actions. The MLP takes as input the state vector s containing the N most recent focal metrics and focal powers, we take N = 8 for all our experiments. To train the model, we use Deep Q Learning following the recommendations set out by the DQN method <ref type="bibr" target="#b9">[10]</ref> to improve training stability. We use an experience memory with size 2.5 × 10 6 , and an -greedy exploration policy where exponentially decays from 1.0 to 0.1 over the first 2 × 10 6 experiences. Our target model is updated with exponential moving average (EMA) weight updates with a β = 0.005, and we use γ = 0.99 in our Bellman equation. Finally, we use a smoothed L1 loss function and optimise with RMSProp with learning rate 1×10 -5 and momentum 0.95. We trained on our software simulated focal-time scans created from real endoscopy videos and validated against our simulated focal-time scans created with HSI images taken with our optical system mounted on a robotic arm.</p><p>End-to-End Model. In addition to learning the optimisation policy, we can also learn the focal metric. By learning the two together, we are no longer constrained to a scalar metric and can instead learn a latent vector encoding of the image patches. To do this, we construct a CNN consisting of 4 convolutions with 8 filters each and a stride of 2, outputting a vector of 8 logits for our patch size of 32 × 32. The CNN is run on each of the N most recent image patches as a batch during training, but only the most recent during inference, with the previous encodings stored between steps. The encodings are concatenated with the N most recent focal powers and fed into an MLP. The MLP and training procedure are the same as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We evaluated each autofocus policies on both our simulated focal-time scan test set, and the robotically recorded focal-time scan. The mean focal errors are shown in Table <ref type="table" target="#tab_0">1</ref>. The scores show an improvement in almost all cases by the introduction of a learned optimiser. The paths taken and the focal error over time for the robotic focal-time scan for a selection of policies are plotted in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>During the usability trial, the surgeon participants were positive about all presented policies. In line with our quantitative results, the participants both showed preference for the CNN-based policy. It was thought by both to be smoother and more deliberate in its adjustments, and felt more robust to minor accidental motions inherent to hand-operated system. One commented that it felt slower to focus but more stable, going on to state that this was desirable behaviour. All algorithms handled the brain fissure well, this is likely due to the small patch size used, allowing for precise targeting. Overall, the surgeons were very positive about the integration of autofocus into optical imaging systems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have successfully designed a handheld intraoperative HSI imaging system with autofocusing capability. We developed a novel CNN-based autofocus policy suitable for video data. In addition, we performed a robotic focal-time scan to evaluate our methods. Our novel method significantly outperforms a traditional baseline on our robotic focal-time scan, and performs preferably in a usability trial by two neurosurgeons. The comments from the usability trial also suggest that the dynamic video autofocusing systems will be well received among surgeons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Left) Existing fixed-focus HSI system being used during neurosurgery in an ethically approved study. Right) RGB reconstruction of an image taken with the fixedfocus HSI system following a craniotomy. The focus has been manually adjusted for the cavity visible through the craniotomy (circled).</figDesc><graphic coords="2,56,46,423,02,339,40,124,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic diagram of our intraoperative video HSI system with focus-tunable liquid lens, allowing electrically controllable focal length. The handheld portion of the system is shown in the dashed line box.</figDesc><graphic coords="4,85,65,67,40,225,67,66,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left) Robotic arm holding our optical system imaging a brain phantom. Right) Sample from our robotic focal-time scan, with the columns representing sequential focal stacks sampled at focal powers of 0.2, 0.5 and 0.8 (top to bottom). For low focal powers, the focal plane is behind the phantom (upper row). As the focal power increases, the focal plane intersects with the fissure (middle row), and then with the area surrounding the fissure (bottom row).</figDesc><graphic coords="5,42,30,54,05,339,52,156,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Focal path (top) and error in focal power (bottom) for three autofocus policies on the robotic focal-time scan. The optimal focal power is shown in black. All paths have been smoothed with a moving average with a window of 5 frames for visualisation purposes.</figDesc><graphic coords="8,82,56,231,77,308,05,177,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean absolute focal power error (0 to 1), and the percentage of in focus frames (focal power error &lt; 0.1), for different autofocus policies on both the simulated and robotic focal-time scan testing sets.</figDesc><table><row><cell cols="2">Autofocus policy</cell><cell cols="2">Focal power error (MAE)</cell><cell>In focus frames</cell></row><row><cell cols="3">Metric Optimiser Simulated</cell><cell>Robotic</cell><cell>Simulated Robotic</cell></row><row><cell>n/a</cell><cell>fixed</cell><cell cols="3">0.236 ± .152 0.262 ± .161 19.0%</cell><cell>26.4%</cell></row><row><cell cols="5">MGM hill-climber 0.102 ± .138 0.146 ± .148 67.9%</cell><cell>46.4%</cell></row><row><cell cols="5">MLR hill-climber 0.092 ± .118 0.163 ± .168 68.2%</cell><cell>44.1%</cell></row><row><cell cols="2">MGM learned</cell><cell cols="3">0.085 ± .115 0.126 ± .118 70.4%</cell><cell>50.8%</cell></row><row><cell cols="2">MLR learned</cell><cell cols="3">0.098 ± .120 0.156 ± .131 66.4%</cell><cell>39.5%</cell></row><row><cell cols="2">CNN learned</cell><cell cols="3">0.049 ± .072 0.070 ± .099 84.9%</cell><cell>79.1%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This study/project is funded by the <rs type="funder">NIHR</rs> [<rs type="grantNumber">NIHR202114</rs>]. This work was supported by core funding from the <rs type="funder">Wellcome/EPSRC</rs> [<rs type="grantNumber">WT203148/Z/16/Z</rs>; <rs type="grantNumber">NS/A000049/1</rs>]. This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No <rs type="grantNumber">101016985</rs> (<rs type="projectName">FAROS</rs> project). TV is supported by a <rs type="funder">Medtronic/RAEng Research Chair</rs> [<rs type="grantNumber">RCSRF1819\7\34</rs>]. For the purpose of open access, the authors have applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission. TV is a co-founder and shareholder of Hypervision Surgical.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GxP8Ny9">
					<idno type="grant-number">NIHR202114</idno>
				</org>
				<org type="funding" xml:id="_zpV6aUW">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_yDB4GXh">
					<idno type="grant-number">NS/A000049/1</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funded-project" xml:id="_dxd6UE8">
					<idno type="grant-number">101016985</idno>
					<orgName type="project" subtype="full">FAROS</orgName>
				</org>
				<org type="funding" xml:id="_YyUPdyk">
					<idno type="grant-number">RCSRF1819\7\34</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 63.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Anikina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Y</forename><surname>Rogov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12842</idno>
		<title level="m">Dasha: decentralized autofocusing system with hierarchical agents</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Increased brain tumor resection using fluorescence image guidance in a preclinical model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bogaards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lasers Surg. Med</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rapid and robust endoscopic content area estimation: a lean gpu-based pipeline and curated benchmark dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Budd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C G P</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intraoperative hyperspectral label-free imaging: from system design to first-in-patient translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ebner</surname></persName>
		</author>
		<idno type="DOI">10.1088/1361-6463/abfbf6</idno>
		<ptr target="https://doi.org/10.1088/1361-6463/abfbf6.https://www.scopus.com/inward/record.url?scp=85107008535&amp;partnerID=8YFLogxK" />
	</analytic>
	<monogr>
		<title level="j">J. Phys. D Appl. Phys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">29</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-vivo and ex-vivo tissue analysis through hyperspectral imaging techniques: revealing the invisible features of cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Halicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fabelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Callico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fei</surname></persName>
		</author>
		<idno type="DOI">10.3390/cancers11060756</idno>
		<ptr target="https://www.mdpi.com/2072-6694/11/6/756" />
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to autofocus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFocus: a deep learning model for focusing microscope systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sharoukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Putman</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2568990</idno>
		<ptr target="https://doi.org/10.1117/12.2568990" />
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Zelinski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Howe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A S</forename><surname>Awwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">11511</biblScope>
			<biblScope unit="page">1151103</biblScope>
		</imprint>
	</monogr>
	<note>Applications of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autofocus algorithm using optimized laplace evaluation function and enhanced mountain climbing search algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="10299" to="10311" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating generalized gaussian blur kernels for out-of-focus image deblurring</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2020.2990623</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2020.2990623" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="829" to="843" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intraoperative video-rate hemodynamic response assessment in human cortex using snapshot hyperspectral optical imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pichette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurophotonics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45003</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intraoperative multispectral and hyperspectral label-free imaging: a systematic review of in vivo clinical studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shapey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biophoton</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">201800455</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno>CoRR abs/1602.03012</idno>
		<ptr target="https://arxiv.org/abs/1602.03012" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning for camera autofocus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCI.2021.3059497</idno>
		<ptr target="https://doi.org/10.1109/TCI.2021.3059497" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="258" to="271" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A robotic auto-focus system based on deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
