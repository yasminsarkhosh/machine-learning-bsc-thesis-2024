<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Tissue to Sound: Model-Based Sonification of Medical Imaging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sasan</forename><surname>Matinfar</surname></persName>
							<email>sasan.matinfar@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>85748</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Nuklearmedizin rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>81675</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehrdad</forename><surname>Salehi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>85748</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shervin</forename><surname>Dehghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>85748</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Nuklearmedizin rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>81675</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>85748</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Tissue to Sound: Model-Based Sonification of Medical Imaging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7084C4FACF46E7075DF7962467030279</idno>
					<idno type="DOI">10.1007/978-3-031-43996-420.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sonification</term>
					<term>Auditory Display</term>
					<term>Auditive Augmented Reality</term>
					<term>Model-based Sonification</term>
					<term>Physical Modeling Sound Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a general design framework for the interactive sonification of multimodal medical imaging data. The proposed approach operates on a physical model that is generated based on the structure of anatomical tissues. The model generates unique acoustic profiles in response to external interactions, enabling the user to learn about how the tissue characteristics differ from rigid to soft, dense to sparse, structured to scattered. The acoustic profiles are attained by leveraging the topological structure of the model with minimal preprocessing, making this approach applicable to a diverse array of applications. Unlike conventional methods that directly transform low-dimensional data into global sound features, this approach utilizes unsupervised mapping of features between an anatomical data model and a sound model, allowing for the processing of high-dimensional data. We verified the feasibility of the proposed method with an abdominal CT volume. The results show that the method can generate perceptually discernible acoustic signals in accordance with the underlying anatomical structure. In addition to improving the directness and richness of interactive sonification models, the proposed framework provides enhanced possibilities for designing multisensory applications for multimodal imaging data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Carrying out a surgical procedure requires not only spatial coordination but also the ability to maintain temporal continuity during time-critical situations, underscoring the crucial importance of auditory perception due to the temporal nature of sound. Perceptual studies have shown that stimuli in different sensory modalities can powerfully interact under certain circumstances, affecting perception or behavior <ref type="bibr" target="#b0">[1]</ref>. In multisensory perception, sensory experiences are integrated when they occur simultaneously, i.e., when complementary or redundant signals are received from the same location at the same time. Previous studies have shown that combining independent but causally correlated sources of information facilitates information processing <ref type="bibr" target="#b1">[2]</ref>. It has been demonstrated that multisensory integration, particularly auditory cues embedded in complex sensory scenes, improves performance on a wide range of tasks <ref type="bibr" target="#b2">[3]</ref>. Biologically, humans have evolved to process spatial dimensions visually <ref type="bibr" target="#b3">[4]</ref>. At the same time, their auditory system, which works in an omnidirectional manner, helps to maintain a steady pace in dynamic interaction, facilitating hand-eye coordination. It has been shown that bimanual coordination augmented with auditory feedback, compared to visual, leads to enhanced activation in brain regions involved in motor planning and execution <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, the integration of auditory systems with visual modalities in biomedical research and applications has not been fully achieved. This could be due to the challenges involved in providing perceptually unequivocal and precise sound while reflecting high-resolution and high-dimensional information, as available in medical data. This paper proposes a novel approach, providing an intelligent modeling as a design methodology for interactive medical applications. Considering the limitations of the state-of-the-art, we investigate potential design approaches and the possibility of establishing a new research direction in sonifying high-resolution and multidimensional medical imaging data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State-of-the-Art of Sonification in the Medical Domain</head><p>Sonification is the data-dependent generation of sound, if the transformation is systematic, objective and reproducible, so that it can be used as scientific method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Researchers have investigated the impact of sound as an integral part of the user experience in interactive systems and interfaces <ref type="bibr" target="#b7">[8]</ref>. In this form of interaction design, sound is used to create rich, dynamic, and immersive user experiences and to convey information, provide feedback, and shape user behavior. The use of sonification has expanded in recent years to a wide range of tasks, such as navigation, process monitoring, data exploration, among others <ref type="bibr" target="#b6">[7]</ref>. A variety of sonification techniques for time-indexed data such as audification, parameter-mapping sonification (PMSon), and model-based sonification (MBS) translate meaningful data patterns of interaction into perceptual patterns, allowing listeners to gain insight into those patterns and be aware of their changes.</p><p>In PMSon, data features are input parameters of a mapping function to determine synthesis parameters. PMSon allows explicit definition of mapping functions in a flexible and adaptive manner, which has made it the most popular approach for medical applications. Medical sonification, as demonstrated by pioneering works such as <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, has predominantly used PMSon for sonifying the position or state of surgical instruments with respect to predetermined structures or translating spatial characteristics of medical imaging data into acoustic features. Research in surgical sonification <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> has mainly focused on image-guided navigation, showing significant results in terms of accuracy and facilitating guidance. Fundamental research behind these studies has aimed to expand mapping dimensionality beyond 1D <ref type="bibr" target="#b15">[15]</ref> and 2D <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref> up to three orthogonal dimensions <ref type="bibr" target="#b18">[18]</ref>. A sonification method combining two orthogonal mappings in two alignment phases has been proposed in <ref type="bibr" target="#b19">[19]</ref> for the placement of pedicle screws with four degrees of freedom. In addition to expanding the data bandwidth, there have been studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref> which address the issues of integrability and pleasantness of the resulting sound signal. This is arguably one of the most significant challenges in surgical sonification, which still requires further research.</p><p>Medical sonification has shown great potential, despite being a relatively new field of research. However, there are limitations in terms of design and integrability. The tedious and case-specific process of defining a mapping function that can be perceptually resolved, even in low dimensional space, poses a significant challenge, rendering the achievement of a generalized method nearly impossible. Consequently, practitioners endeavor to minimize data complexity and embed data space into low dimensional space, giving rise to sonification models that offer only an abstract and restricted understanding of the data, which is inadequate in the case of complex medical data. Although these methods achieve adequate perceptual resolution and accuracy, they still are not optimized for integration into the surgical workflow and demonstrate low learning rates, which also demand increased cognitive load and task duration. Furthermore, fine-tuning these models involves a considerable amount of artistic creativity in order to avoid undesirable effects like abrasiveness and fatigue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Towards Model-Based Sonic Interaction with Multimodal Medical Imaging Data</head><p>The need for an advanced multisensory system becomes more critical in scenarios where the anatomy is accessed through limited means, as in minimally invasive surgery. In such cases, an enriched auditory feedback system that conveys precise information about the tissue or surrounding structures, capturing spatial characteristics of the data (such as density, solidity, softness, or sparsity), can enhance the surgeon's perception and, on occasion, compensate for the lack of haptic feedback in such procedures. Embedding such use cases into a low-dimensional space is not feasible. In the same way that tapping on wood or glass imparts information about the object's construction, an effective sonification design can convey information in a manner that is easily interpreted with minimal cognitive effort. Intuitive embodiment of information in auditory feedback, facilitates the learning process to the extent that subconscious association of auditory cues with events can be achieved. Despite the fact that anatomical structures do not produce sound in human acoustic ranges, and surgeons do not have a direct perceptual experience of them, these structures still adhere to the physical principles of dynamics. The hypothesis of this paper is that sounds based on the principles of physics are more straightforward to learn and utilize due to their association with real-world rules. MBS is a technique that utilizes mathematical models to represent data and then convert those models into audible sounds. This approach is often used in scientific or data analysis applications such as clustering <ref type="bibr" target="#b22">[22]</ref> or data scanning <ref type="bibr" target="#b23">[23]</ref>, where researchers aim to represent complex data sets or phenomena through sound. MBS employs a mapping approach that associates data features with the features of a sound-capable system, facilitating generalization across different cases. MBS is often coupled with physical modeling synthesis that uses algorithms to simulate the physical properties of real-world instruments and generate sound in a way that mimics the behavior of physical objects <ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref>. This approach is often used to create realistic simulations of acoustic instruments or explore new sonic possibilities that go beyond the limitations of traditional instruments. Sound is a physical phenomenon that arises from the vibration of objects in a medium, such as air, resulting in a complex waveform composed of multiple frequencies at different amplitudes. This vibrational motion can be generated by mechanical impacts, such as striking, plucking, or blowing, applied to a resonant object capable of sustaining vibration. Physical modeling has been previously introduced to the biomedical research community as a potential solution for the issue of unintuitive sounds, as reported in <ref type="bibr" target="#b21">[21]</ref>. However, as mentioned earlier, it is limited by the shortcomings of the current state-of-the-art, particularly with respect to reduced data dimensionality.</p><p>Physical modeling can be approached using the mass interaction method, which is characterized by its modular design and the capacity to incorporate direct gestural interaction, as demonstrated in <ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref>. This approach offers the advantage of describing highly intricate virtual objects as a construction of elementary physical components. Additionally, such an approach is highly amenable to the iterative and exploratory design of "physically plausible" virtual objects, which are grounded in the principles of Newtonian physics and three laws of motion but are not necessarily limited to the mechanical constraints of the physical world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>This paper presents a novel approach for transforming spatial features of multimodal medical imaging data into a physical model that is capable of generating distinctive sound. The physical model captures complex features of the spatial domain of data, including geometric shapes, textures, and complex anatomical structures, and translates them to sound. This approach aims to enhance experts' ability to interact with medical imaging data and improve their mental mapping regarding complex anatomical structures with an unsupervised approach. The unsupervised nature of the proposed approach facilitates generalization, which enables the use of varied input data for the development of versatile soundcapable models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>The proposed method involves multimodal imaging data serving as input, a topology to capture spatial structures, and an interaction module to establish temporal progress. We consider medical imaging data as members of a R d+m space, where d is the data dimensionality in space domain, and m the dimension of measured features by the imaging systems. A sequence of physics-based sound signals, S, expressed as</p><formula xml:id="formula_0">S = n i=0 P i (act(pos i , F i ), f(A i ), T i ),<label>(1)</label></formula><p>can be achieved by the sum of n excitations of a physical model P with user interaction act at the position pos with applied force of F , where f transfers a region of interest (RoI) A ∈ R d+m to parameters of the physical model P using the topology matrix T . These components are described in detail in Sect. 4.1.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> provides an illustration of the method overview and data pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Physical Model</head><p>The mass-interaction physics methodology <ref type="bibr" target="#b28">[28]</ref> allows the formulation of physical systems, such as the linear harmonic oscillator, which comprise two fundamental constituents: masses, representing material points within a 3D space, with corresponding inertial behaviors, and connecting springs, signifying specific types of physical couplings such as viscoelastic and collision between two mass elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of a Mass-Interaction System in Discrete Time.</head><p>To represent and compute discretized modular mass-interaction systems, a widely used method involves applying a second-order central difference scheme to Newton's second law, which states that force F is equal to mass m times acceleration a, or the second derivative of its position vector x with respect to time t. The total force exerted by the dampened spring, denoted as</p><formula xml:id="formula_1">F (t n ) = F s (t n ) + F d (t n ) at time t n ,</formula><p>where F s represents the elastic force exerted by a linear spring (the interaction) with stiffness K, connecting two masses m 1 , m 2 located at positions x 1 , x 2 , can be expressed using the discrete-time equivalent of Hooke's law. Similarly, the friction force F d applied by a linear damper with damping parameter z can be derived using the Backward Euler difference scheme with the discrete-time inertial parameter Z = z/ΔT . F (t n ) is applied symmetrically to each mass in accordance with Newton's third law:</p><formula xml:id="formula_2">F 2→1 (t n ) = -F (t n ) and F 1→2 (t n ) = + F (t n ).</formula><p>The combination of forces applied to masses and the connecting spring yields a linear harmonic oscillator as described in</p><formula xml:id="formula_3">X(t n+1 ) = (2 - K + Z M )X(t n ) + ( Z M -1)X(t n-1 ) + F (t n ) M , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>which is a fundamental type of the mass-interaction system. This system is achieved by connecting a dampened spring between a mass and a fixed point</p><formula xml:id="formula_5">x 1 (t n ) = 0.</formula><p>A mass-interaction system can be extended to a physical model network with an arbitrary topology by connecting the masses via dampened springs. The connections are formalized as a routing matrix T of dimensions r × c, where r denotes the number of mass points in the physical model network, and c represents the number of connecting springs, each having only two connections. A single mass can be connected to multiple springs in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction</head><p>Module. An interaction module, act, excites the model P by applying force to one or more input masses of the model I ∈ T r×c . f maps the intensities of the input data to M, K, Z, and F . Therefore, the input force is propagated through the network according to the Eq. 2 and observed by the output masses O ∈ T r×c .</p><p>To summarize, the output masses are affected by the oscillation of all masses activated in the model with various frequencies and corresponding amplitudes, resulting in a specific sound profile, i.e., tone color. This tone color represents the spatial structure and physical properties of the RoI, which is transformed into the features of the output sound. The wave propagation is significantly influenced by the model's topology and the structure of the inter-mass connections, which have great impact on activating spatial relevant features and sound quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment and Results</head><p>The objective is to evaluate the feasibility of the proposed method in creating a model that is capable of generating discernible sound profiles in accordance with the underlying anatomical structures. In particular, we aim to differentiate between the sound of a set of tissue types. Through empirical experimentation on an abdominal CT volume <ref type="bibr" target="#b31">[31]</ref>, we determined a model configuration that achieves stability and reduces noise to the desired level. The shape of the topology is a 3D cube of size 7 mm 3 . The inter-mass connections are established at the grid spacing distance of 1 mm between each mass and its adjacent neighbor masses. All the masses located on the surface of the model are set as fixed points. To excite the model, equal forces are applied to the center of the model in a 3D direction and observed at the same position. The RoI is obtained by selecting 3D cubes with the same topology and size in the CT volume. The intensities in the RoI are transformed to define the model parameters. The spring parameters K and Z are derived by averaging the intensities of their adjacent CT voxels, by a linear mapping and M and F are set to constant values. We defined a sequence of RoI starting in the heart, passing through lung, liver, bone, muscle, and ending in the air in a 16-steps trajectory, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. For visualizing the trajectory and processing image intensities, we used ImFusion Suite <ref type="bibr" target="#b32">[32]</ref>. For generating physical models and generating sound, we used mass-interaction physical modelling library <ref type="bibr" target="#b29">[29]</ref> for the Processing sketching environment: https://processing.org/. Visual demonstrations of the models, along with the corresponding sound samples, are provided in the supplementary material, along with additional explanations.</p><p>A mel spectrogram is a visual representation of the frequency content of an audio signal, where the frequencies are mapped to a mel scale, which is a perceptual frequency scale based on how humans hear sounds. Therefore, we used this representation to show the frequency content of the resulting sound of the trajectory, presented in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>This paper introduced a general framework for MBS of multimodal medical imaging data. The preliminary study demonstrates perceptually distinguishable sound profiles between various anatomical tissue types. These profiles are achieved through a minimal preprocessing based on the model topology and a basic mapping definition. This indicates that the model is effective in translating intricate imaging data into a discernible auditory representation, which can conveniently be generalized to a wide range of applications. In contrast to the traditional methods that directly convert low-dimensional data into global sound features, this approach maps features of a data model to the features of a sound model in an unsupervised manner and enables processing of high-dimensional data.</p><p>The proposed method presents opportunities for several enhancements and future directions. In the case of CT imaging, it may be feasible to establish modality-specific generalized configurations and tissue-type-specific transfer functions to standardize the approach to medical imaging sonification. Such efforts could lead to the development of an auditory equivalent of 3D visualization for medical imaging, thereby providing medical professionals with a more immersive and intuitive experience. Another opportunity could be in the sonification of intraoperative medical imaging data, such as ultrasound or fluoroscopy, to augment the visual information that a physician would receive by displaying the tissue type or structure which their surgical instruments are approaching. To accommodate various application cases, alternative interaction modes can be designed that simulate different approaches to the anatomy with different tool materials. Additionally, supervised features can be integrated to improve both local and global perception of the model, such as amplifying regions with anomalies. Different topologies and configurations can be explored for magnifying specific structures in data, such as pathologies, bone fractures, and retina deformation. To achieve a realistic configuration of the model regarding the physical behavior of the underlying anatomy, one can use several modalities which correlate with physical parameters of the model. For instance, the masses can be derived by intensities of CT and spring stiffness from magnetic resonance elastography, both registered as a 3D volume.</p><p>An evaluation of the model's potential in an interactive setting can be conducted to determine its impact on cognitive load and interaction intuitiveness. Such an assessment can shed light on the practicality of the model's application by considering the user experience, which is influenced by cognitive psychological factors. As with any emerging field, there are constraints associated with the methodology presented in this paper. One of the constraints is the requirement to manually configure the model parameters. Nevertheless, a potential future direction would be to incorporate machine learning techniques and dynamic modeling to automatically determine these parameters from underlying physical structure. For instance, systems such as <ref type="bibr" target="#b30">[30]</ref> can provide a reliable reference for such investigations. Considering the surgical auditory scene understanding is vital when incorporating the sonification model into the surgical workflow. Future studies can investigate this by accounting for realistic surgical environments, including existing sound sources, and considering auditory masking effects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The diagram shows the transformation of multimodal medical imaging data to sound. The registration method can be arbitrarily chosen depending on the application and data modalities.</figDesc><graphic coords="5,56,46,275,87,339,37,172,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The spectrogram illustrates the sound profiles of the tissues corresponding to the sequence of RoIs marked as yellow on the abdominal CT. The green spheres highlight the excitation points. (Color figure online)</figDesc><graphic coords="7,56,46,387,56,339,55,105,61" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multisensory perception: from integration to remapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Luca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Sensory Cue Integration</publisher>
			<biblScope unit="page" from="224" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Benefits of multisensory learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="411" to="417" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audiovisual events capture attention: evidence from temporal order judgments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Burg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Olivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Bronkhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Theeuwes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sound localization by human listeners</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Middlebrooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="159" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motor learning with augmented feedback: modality-dependent behavioral and neural consequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ronsse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1283" to="1294" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taxonomy and definitions for sonification and auditory display</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Community for Auditory Display</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Sonification Handbook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Neuhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Logos Verlag</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m">Sonic Interaction Design</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Franinovic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Serafin</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Surgical navigation using audio feedback</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Karron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medicine Meets Virtual Reality</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="450" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sonification of optical coherence tomography data and images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Adie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Boppart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="9934" to="9944" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auditory support for resection guidance in navigated liver surgery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Robot. Comput. Assist. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surgical soundtracks: towards automatic musical augmentation of surgical procedures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matinfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="673" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_76</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-876" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of auditory display in image-guided interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nabavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-017-1547-z</idno>
		<ptr target="https://doi.org/10.1007/s11548-017-1547-z" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1665" to="1676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audiovisual AR concepts for laparoscopic subsurface structure navigation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Joeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Razavizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of sonification strategies for guidance tasks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parseihian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gondre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ystad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="674" to="686" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Psychoacoustic sonification design for navigation in surgical interventions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ziemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schultheis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Meetings on Acoustics</title>
		<meeting>Meetings on Acoustics</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">50005</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Psychoacoustical interactive sonification for short range navigation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ziemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schultheis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Acust. Acust</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1075" to="1093" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Psychoacoustical signal processing for three-dimensional sonification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ziemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schultheis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Georgia Institute of Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sonification as a reliable alternative to conventional visual surgical navigation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matinfar</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/s41598-023-32778-z" />
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5930</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sonification for process monitoring in highly sensitive surgical tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matinfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fürnstahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nordic Sound and Music Computing Conference</title>
		<meeting>the Nordic Sound and Music Computing Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. Nordic SMC 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SonifEye: sonification of visual information using physical modeling sound synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Roodaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2366" to="2371" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Listen to your data: model-based sonification for data analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Computing and Multimedia Systems</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tangible data scanning sonification model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bovermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Georgia Institute of Technology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Physical modeling using digital waveguides</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Music. J</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="74" to="91" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Physically informed sonic modeling (PhISM): synthesis of percussive sounds</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Music. J</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Physical audio signal processing: for virtual musical instruments and audio effects</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>W3K Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Physical modelling concepts for a collection of multisensory virtual musical instruments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cadoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Interfaces for Musical Expression</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mass-interaction physical models for sound and multisensory creation: starting anew</title>
		<author>
			<persName><forename type="first">J</forename><surname>Villeneuve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Sound &amp; Music Computing Conference</title>
		<meeting>the 16th Sound &amp; Music Computing Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="https://github.com/mi-creative/miPhysicsProcessing" />
		<title level="m">Mass Interaction Physics in Java/Processing Homepage</title>
		<imprint>
			<date type="published" when="2023-03">Mar 2023</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Novel clinical device tracking and tissue event characterization using proximally placed audio signal acquisition and processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Illanes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12070</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">WORD: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102642</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recent advances in point-of-care ultrasound using the ImFusion Suite for real-time image analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zettinig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POCUS/BIVPCS/CuRIOUS/CPM -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11042</biblScope>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01045-4_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01045-46" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
