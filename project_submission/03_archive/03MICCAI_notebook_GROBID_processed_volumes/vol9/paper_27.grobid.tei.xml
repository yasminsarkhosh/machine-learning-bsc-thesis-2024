<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery</title>
				<funder ref="#_U99J2Yz">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_jyJAMxZ">
					<orgName type="full">Hong Kong Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
				<funder ref="#_qKYrSRa">
					<orgName type="full">Shun Hing Institute of Advanced Engineering</orgName>
				</funder>
				<funder ref="#_qSHWUES">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lalithkumar</forename><surname>Seenivasan</surname></persName>
							<idno type="ORCID">0000-0002-0103-1234</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mobarakol</forename><surname>Islam</surname></persName>
							<idno type="ORCID">0000-0002-7162-2822</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">WEISS</orgName>
								<orgName type="institution" key="instit2">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gokul</forename><surname>Kannan</surname></persName>
							<idno type="ORCID">0000-0002-2284-703X</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Production Engineering</orgName>
								<orgName type="institution">National Institute of Technology</orgName>
								<address>
									<settlement>Tiruchirappalli</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<email>hlren@ee.cuhk.edu.hk</email>
							<idno type="ORCID">0000-0002-6488-1551</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Shun Hing Institute of Advanced Engineering</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C70322992D329D90E70556201E577EB5</idno>
					<idno type="DOI">10.1007/978-3-031-43996-427.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advances in GPT-based large language models (LLMs) are revolutionizing natural language processing, exponentially increasing its use across various domains. Incorporating uni-directional attention, these autoregressive LLMs can generate long and coherent paragraphs. However, for visual question answering (VQA) tasks that require both vision and language processing, models with bi-directional attention or models employing fusion techniques are often employed to capture the context of multiple modalities all at once. As GPT does not natively process vision tokens, to exploit the advancements in GPT models for VQA in robotic surgery, we design an end-to-end trainable Language-Vision GPT (LV-GPT) model that expands the GPT2 model to include vision input (image). The proposed LV-GPT incorporates a feature extractor (vision tokenizer) and vision token embedding (token type and pose). Given the limitations of unidirectional attention in GPT models and their ability to generate coherent long paragraphs, we carefully sequence the word tokens before vision tokens, mimicking the human thought process of understanding the question to infer an answer from an image. Quantitatively, we prove that the LV-GPT model outperforms other state-ofthe-art VQA models on two publically available surgical-VQA datasets (based on endoscopic vision challenge robotic scene segmentation 2018 and CholecTriplet2021) and on our newly annotated dataset (based on the holistic surgical scene dataset). We further annotate all three datasets to include question-type annotations to allow sub-type analysis. Furthermore, we extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent evolution of large language models (LLMs) is revolutionizing natural language processing and their use across various sectors (e.g., academia, healthcare, business, and IT) and daily applications are being widely explored. In medical diagnosis, recent works <ref type="bibr" target="#b23">[23]</ref> have also proposed employing the LLM models to generate condensed reports, interactive explanations, and recommendations based on input text descriptions (predicted disease and report). While the current single-modality (language) LLMs can robustly understand the questions, they still require prior text descriptions to generate responses and are unable to directly infer responses based on the medical image. Although language-only models can greatly benefit the medical domain in language processing, there is a need for robust multi-modality models to process both medical vision and language. In the surgical domain, in addition to the scarcity of surgical experts, their daily schedules are often overloaded with clinical and academic work, making it difficult for them to dedicate time to answer inquiries from students and patients on surgical procedures <ref type="bibr" target="#b2">[3]</ref>. Although various computer-assisted solutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed and recorded surgical videos have been made available for students to sharpen their skills and learn from observation, they still heavily rely on surgical experts to answer their surgery-specific questions. In such cases, a robust and reliable surgical visual question answering (VQA) model that can respond to questions by inferring from context-enriched surgical scenes could greatly assist medical students, and significantly reduce the medical expert's workload <ref type="bibr" target="#b18">[19]</ref>.</p><p>In the medical domain, MedfuseNet <ref type="bibr" target="#b18">[19]</ref>, an attention-based model, was proposed for VQA in medical diagnosis. Utilizing the advancements in the transformer models, VisualBert RM <ref type="bibr" target="#b17">[18]</ref>, a modified version of the VisualBert <ref type="bibr" target="#b11">[12]</ref> model was also proposed for VQA in robotic surgery. Compared to most VQA models that require a region proposal network to propose vision patches, the VisualBert RM <ref type="bibr" target="#b17">[18]</ref> performed VQA based on features extracted from the whole image, eliminating the need for a region proposal network. However, they were extracted using a non-trainable fixed feature extractor. While VisualBert <ref type="bibr" target="#b11">[12]</ref> models and LLMs are transformer models, there are fundamentally different. VisualBert <ref type="bibr" target="#b11">[12]</ref> transformers are bidirectional encoder models and are often employed for multi-modality tasks. In contrast, ChatGPT<ref type="foot" target="#foot_0">1</ref> (GPT3.5) and BARD (LaMDA <ref type="bibr" target="#b19">[20]</ref>) are language-only uni-directional transformer decoder models employed for language generation. As they are proving to be robust in language generation, exploiting them to process the questions and enabling them to process vision could greatly improve performance in VQA tasks.</p><p>In this work, we develop an end-to-end trainable SurgicalGPT model by exploiting a pre-trained LLM and employing a learnable feature extractor to generate vision tokens. In addition to word tokens, vision tokens (embedded with token type and pose embedding) are introduced into the GPT model, resulting in a Language-Vision GPT (LV-GPT) model. Furthermore, we carefully sequence the word and vision tokens to leverage the GPT model's robust language processing ability to process the question and better infer an answer based on the vision tokens. Through extensive experiments, we show that the SurgicalGPT(LV-GPT) outperforms other state-of-the-art (SOTA) models by ∼ 3-5% on publically available EndoVis18-VQA <ref type="bibr" target="#b17">[18]</ref> and Cholec80-VQA surgical-VQA <ref type="bibr" target="#b17">[18]</ref> datasets. Additionally, we introduce a novel PSI-AVA-VQA dataset by adding VQA annotations to the publically available holistic surgical scene dataset(PSI-AVA) and observe similar performance improvement. Furthermore, we study and present the effects of token sequencing, where model performance improved by ∼ 2-4% when word tokens are sequenced earlier. Finally, we also study the effects of token type and pose embedding for vision tokens in the LV-GPT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>GPT2 <ref type="bibr" target="#b5">[6]</ref>, a predecessor to GPT3.5 (ChatGPT), is a transformer decoder model that performs next-word prediction. Auto-regressive in nature, its self-attention blocks attend to earlier word tokens to predict the next word token iteratively, allowing the model to generate complex paragraphs <ref type="bibr" target="#b14">[15]</ref>. Although robust in language generation, due to its unidirectional attention <ref type="bibr" target="#b12">[13]</ref>, in a given iteration, the generated token knows all earlier tokens but does not know any subsequent token (Fig. <ref type="figure" target="#fig_1">1(a)</ref>), restricting the model's ability to capture the entire context between all tokens. VisualBert <ref type="bibr" target="#b11">[12]</ref>, fundamentally different from GPT models, is a non-auto-regressive transformer encoder model. Its bidirectional self-attention blocks attend in both directions (earlier and subsequent tokens) <ref type="bibr" target="#b12">[13]</ref>, allowing the model to capture the entire context all at once (Fig. <ref type="figure" target="#fig_1">1(b)</ref>). Due to this, bi-directional attention models are often preferred for multi-modality tasks.</p><p>Vision-Language Processing: Employed mostly for language-only tasks, GPT models do not natively process vision tokens <ref type="bibr" target="#b7">[8]</ref>. While it supports robust word embedding, it lacks vision tokenizer and vision embedding layers. This limits exploiting its language processing ability for multi-modality tasks. Alternate to GPT, as the VisualBert model is often preferred for multi-modality tasks, it encompasses dedicated embedding layers for both vision and word tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LV-GPT: Language-Vision GPT</head><p>Overall Network: We design an end-to-end trainable multi-modality (language and vision) LV-GPT model (Fig. <ref type="figure" target="#fig_2">2</ref>) for surgical VQA. We integrate a vision tokenizer (feature extractor) module and vision embedding with the GPT model to exploit its language processing ability in performing VQA tasks. Current token iteration  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language-Vision Processing:</head><p>The questions are tokenized using the inherent GPT2 tokenizer. The word tokens are further embedded based on token-id, token type (0) and token position by the inherent GPT2 word embedding layers. To tokenize the input surgical scene (image) into vision tokens, the LV-GPT includes a vision tokenizer (feature extractor): ResNet18 (RN18) <ref type="bibr" target="#b8">[9]</ref>/Swin <ref type="bibr" target="#b13">[14]</ref>/ViT <ref type="bibr" target="#b6">[7]</ref>. Given an image, the tokenizer outputs vision tokens, each holding visual features from an image patch. Additionally, the vision tokens are further embedded based on token type (1) and token position (pos = 0) embeddings. The final embedded word and vision tokens (w e and v e ) can be formulated as:</p><formula xml:id="formula_0">w e = T t=0 (w x ) + P pos (w x ) + w x ; pos = 0, 1, 2, 3, ..., n. v e = T t=1 (v x ) + P pos=0 (v x ) + v x ; v x = v t , dim(v i t ) = dim(w i x ) f (v t ), e l s e<label>(1)</label></formula><p>where, T t () is type embedding, P pos () is pose embedding, w x and v x are initial word and vision embedding, and v t are vision tokens. Initial word embeds (w x ) are obtained using word embedding based on word token id. Depending on the size (dim) of each vision token, they undergo additional linear layer embedding (f ()) to match the size of the word token.</p><p>Token Sequencing: LLMs are observed to process long sentences robustly and hold long-term sentence knowledge while generating coherent paragraphs/reports. Considering GPT's superiority in sequentially processing large sentences and its uni-directional attention, the word tokens are sequenced before the vision tokens. This is also aimed at mimicking human behaviour, where the model understands the question before attending to the image to infer an answer.</p><p>Classification: Finally, the propagated multi-modality features are then passed through a series of linear layers for answer classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>EndoVis18-VQA: We employ publically available EndoVis18-VQA <ref type="bibr" target="#b17">[18]</ref> dataset to benchmark the model performance. We use the classification subset that includes classification-based question-and-answer (Q&amp;A) pairs for 14 robotic nephrectomy procedure video sequences of the MICCAI Endoscopic Vision Challenge 2018 <ref type="bibr" target="#b1">[2]</ref> dataset. The Q&amp;A pairs are based on the tissue, actions, and locations of 8 surgical tools. The dataset includes 11783 Q&amp;A pairs based on 2007 surgical scenes. The answers consist of 18 classes (1 kidney, 13 tool-tissue interactions, and 4 tool locations). Additionally, we further annotated the validation set (video sequences 1, 5, and 16) on question types to assist in additional analysis. We followed the EndoVis18-VQA <ref type="bibr" target="#b17">[18]</ref> dataset's original train/test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cholec80-VQA:</head><p>The classification subset of the Cholec80-VQA <ref type="bibr" target="#b17">[18]</ref> is also employed for model evaluation. It contains Q&amp;A pairs for 40 video sequences of the Cholec80 dataset <ref type="bibr" target="#b20">[21]</ref>. The subset consists of 43182 Q&amp;A pairs on the surgical phase and instrument presence for 21591 frames. The answers include 13 classes (2 instrument states, 4 on tool count, and 7 on surgical phase). We additionally annotated the validation set (video sequences: 5, 11, 12, 17, 19, 26, 27 and 31) on the Q&amp;A pairs types for further model analysis. The VQA <ref type="bibr" target="#b17">[18]</ref> dataset's original train/test split is followed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSI-AVA-VQA:</head><p>We introduce a novel PSI-AVA-VQA dataset that consists of Q&amp;A pairs for key surgical frames of 8 cases of the holistic surgical scene dataset (PSI-AVA dataset) <ref type="bibr" target="#b21">[22]</ref>. The questions and answers are generated in sentence form and single-word (class) response form, respectively. They are generated based on the surgical phase, step, and location annotation provided in the PSI-AVA dataset <ref type="bibr" target="#b21">[22]</ref>. The PSI-AVA-VQA consists of 10291 Q&amp;A pairs and with 35 answer classes (4 locations, 11 surgical phases, and 21<ref type="foot" target="#foot_1">1</ref> surgical steps). The Q&amp;A pairs are further annotated into 3 types (location, phase, and step). The fold-1 train/test split of parent PSI-AVA <ref type="bibr" target="#b21">[22]</ref> dataset is followed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All variants of our models<ref type="foot" target="#foot_2">2</ref> are trained based on cross-entropy loss and optimized using the Adam optimizer. The models were trained for 80 epoch, with a batch size of 64, except for LV-GPT (ViT) ( batch size = 32 due to GPU limitation). learning rates lr = 1×10 -5 , 1×10 -5 and 5×10 -6 are used for EndoVis18-VQA, PSI-AVA-VQA and Cholec80-VQA dataset, respectively. The SOTA Visu-alBert <ref type="bibr" target="#b11">[12]</ref> and VisualBert RM <ref type="bibr" target="#b17">[18]</ref> models were implemented using their official code repositories. The Block <ref type="bibr" target="#b4">[5]</ref>, MUTAN <ref type="bibr" target="#b3">[4]</ref>, MFB <ref type="bibr" target="#b24">[24]</ref> and MFH <ref type="bibr" target="#b25">[25]</ref> were implemented using the official codes of Block <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>All our proposed LV-GPT model variants are quantitatively benchmarked (Table <ref type="table" target="#tab_0">1</ref>) against other attention-based/bi-directional encoder-based SOTA models on EndoVis18-VQA, Cholec80-VQA and PSI-AVA-VQA datasets based VisualBert <ref type="bibr" target="#b11">[12]</ref> 0.6143 0.4282 0.3745 0.9007 0.6294 0.6300 0.5853 0.3307 0.3161 VisualBert RM <ref type="bibr" target="#b17">[18]</ref> 0.6190 0.4079 0.3583 0.9001 0.6573 0.6585 0.6016 0.3242 0.3165 Block <ref type="bibr" target="#b4">[5]</ref> 0.6088 0.4884 0.4470 0.8948 0.6600 0.6413 0.5990 0.5136 0.4933 Mutan <ref type="bibr" target="#b3">[4]</ref> 0.6303 0.4969 0.4565 0.8699 0.6332 0.6106 0.4971 0.3912 0.3322 MFB <ref type="bibr" target="#b24">[24]</ref> 0.5238 0.4205 0.3622 0.8410 0.5303 0.4588 0.5712 0.4379 0.4066 MFH <ref type="bibr" target="#b25">[25]</ref> 0 VisualBert RM <ref type="bibr" target="#b17">[18]</ref>, Block <ref type="bibr" target="#b4">[5]</ref>, and our LV-GPT (Swin) models against the ground truth based on input surgical scene and question.</p><p>on the accuracy (Acc), recall, and Fscore. In most cases, all our variants, LV-GPT (Swin), LV-GPT (RN18) and LV-GPT (ViT), are observed to significantly outperform SOTA models on all three datasets in terms of Acc. Specifically, the LV-GPT (Swin) variant (balanced performance across all datasets) is observed to outperform all SOTA models on all datasets and significantly improve the performance (∼ 3-5% improvement) on EndoVis18-VQA and Cholec80-VQA dataset.</p><p>Additionally, it should be noted our model variants can be trained end-to-end, whereas, most of the SOTA models requires a region proposal network to process input image into vision tokens. Figure <ref type="figure" target="#fig_3">3</ref> shows the qualitative performance of LV-GPT (Swin) against SOTA models on three datasets. A Comparison of our LV-GPT model performance on the EndoVis18-VQA dataset with default test queries vs rephrased test queries is presented in supplementary materials that highlight the model's robustness in language reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early Vision vs Early Word:</head><p>The performance of LV-GPT based on word and vision token sequencing (Table <ref type="table" target="#tab_1">2</ref>) is also studied. While all three variants of the LV-GPT models processing vision tokens earlier are observed to perform on par with SOTA models reported in Table <ref type="table" target="#tab_0">1</ref>, in most cases, their performances on both datasets further improved by ∼ 2-4% when word tokens are processed earlier. This improvement could be attributed to LLM's ability to hold sentence (question) context before processing the vision tokens to infer an answer. This behaviour, in our view, mimics the human thought process, where we first understand the question before searching for an answer from an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Embedding for Vision Tokens:</head><p>The influence of positional embedding of the vision tokens (representing a patch region) in all the LV-GPT variants is studied by either embedded with position information (pos = 1, 2, 3, .., n.) or zero-position (pos = 0). Table <ref type="table" target="#tab_2">3</ref> shows the difference in the performance of the best-performing LV-GPT variant in each dataset, with its vision tokens   embedded with actual-position or zero-position. While we expected the positional embedding to improve the performance (dataset Q&amp;A pairs related to tool location), from the results, we observe that embedding vision tokens with zero-position embedding results in better performance. In-depth analysis shows that our CNN-based LV-GPT (RN18) model improved with positional embedding (Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref>). In the transformer-based LV-GPT (Swin)/LV-GPT (ViT) models, positional embedding is already incorporated at the vision tokenizer (VIT/Swin) layer, and adding positional embedding at the GPT level results in double Position embedding. Thus, "zero-position" can be interpreted as "LV-GPT only requires one layer of positional embedding". A sub-type analysis (Fig. <ref type="figure" target="#fig_4">4</ref>) is also performed on the model performance to analyze the effect of positional embedding of the vision tokens. The model in which the vision tokens were embedded with zero-position (at the GPT level), performed marginally better/similar on all sub-types in the Cholec80-VQA dataset. However, its performance improvement was significant in the PSI-AVA-VQA dataset sub-types, including the 'tool location' sub-types that contain questions on tool location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on Vision Token</head><p>Embedding: An ablation study on the vision token embedding in the LV-GPT model on the EndoVis18-VQA dataset is also shown in Table <ref type="table" target="#tab_3">4</ref>. VB-VE refers to vision token embedding using Visu-alBert vision embedding. The C-VE refers to custom embedding, where, in LV-GPT (RN18), the vision token undergoes additional linear layer embedding to match the word-token dimension, and in other variants, vision tokens from the Swin/VIT are directly used. The subsequent VT-TY + VT-PE and VT-TY + VT-ZPE refers to the additional vision token type (TY) and actual-position (PE)/zero-position (ZPE) embedding. We observe that employing C-VE with VT-TY + VT-ZPE results in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We design an end-to-end trainable SurgicalGPT, a multi-modality Language-Vision GPT model, for VQA tasks in robotic surgery. In addition to GPT's inherent word embeddings, it incorporates a vision tokenizer (trainable feature extractor) and vision token embedding (type and pose) to perform multimodality tasks. Furthermore, by carefully sequencing the word tokens earlier to vision tokens, we exploit GPT's robust language processing ability, allowing the LV-GPT to significantly perform better VQA. Through extensive quantitative analysis, we show that the LV-GPT outperforms other SOTA models on three surgical-VQA datasets and sequencing word tokens early to vision tokens significantly improves the model performance. Furthermore, we introduce a novel surgical-VQA dataset by adding VQA annotations to the publically available holistic surgical scene dataset. While multi-modality models that process vision and language are often referred to as "vision-language" models, we specifically name our model "language-vision GPT" to highlight the importance of the token sequencing order in GPT models. Integrating vision tokens into GPT also opens up future possibilities of generating reports directly from medical images/videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Uni-directional attention in GPT language model vs bi-direction attention in VisualBert multi-modality model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. End-to-End LV-GPT for Surgical VQA: The input question and surgical scene are tokenized, embedded, and sequenced to predict the answer.</figDesc><graphic coords="4,53,52,54,68,285,91,91,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative analysis: Comparison of answers predicted by VisualBERT [12],VisualBert RM<ref type="bibr" target="#b17">[18]</ref>, Block<ref type="bibr" target="#b4">[5]</ref>, and our LV-GPT (Swin) models against the ground truth based on input surgical scene and question.</figDesc><graphic coords="6,80,85,226,40,300,97,97,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sub-type performance analysis of LV-GPT model variants on Cholec80-VQA and PSI-AVA-VQA embedded with zero-pose embedding vs actual-pose embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitaive comparison of our LV-GPT (Swin), LV-GPT (RN18), and (LV-GPT (ViT) against state-of-the-art models.</figDesc><table><row><cell>MODELS</cell><cell cols="2">EndoVis18-VQA [18]</cell><cell cols="2">Cholec80-VQA [18]</cell><cell>PSI-AVA-VQA</cell></row><row><cell></cell><cell>Acc</cell><cell cols="2">Recall FScore Acc</cell><cell>Recall FScore Acc</cell><cell>Recall FScore</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of LV-GPT model performance when vision tokens are sequenced earlier vs when word tokens are sequenced earlier.</figDesc><table><row><cell cols="2">Token sequencing Model</cell><cell cols="2">EndoVis18-VQA</cell><cell>PSI-AVA-VQA</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Recall FScore Acc</cell><cell>Recall FScore</cell></row><row><cell>Early vision</cell><cell cols="3">LV-GPT (RN18) 0.6338 0.3600 0.3510 0.5542 0.2879 0.2886</cell></row><row><cell></cell><cell cols="3">LV-GPT (Swin) 0.6208 0.4059 0.3441 0.6068 0.4195 0.3813</cell></row><row><cell></cell><cell cols="3">LV-GPT (ViT) 0.6493 0.4362 0.3701 0.6023 0.2802 0.2628</cell></row><row><cell>Early word</cell><cell cols="3">LV-GPT (RN18) 0.6811 0.4649 0.4649 0.5933 0.3183 0.3168</cell></row><row><cell></cell><cell cols="3">LV-GPT (Swin) 0.6613 0.4460 0.4537 0.6033 0.4137 0.3767</cell></row><row><cell></cell><cell cols="3">LV-GPT (ViT) 0.6659 0.4920 0.4336 0.6549 0.4132 0.3971</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of model performances on EndoVis18-VQA, Cholec80-VQA and PSI-AVA-VQA datasets when vision tokens are embedded with zero-positional embedding vs actual pose embedding.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">Zero Pose Embedding</cell><cell>Actual Pose Embedding</cell></row><row><cell></cell><cell>Best LV-GPT</cell><cell>Acc</cell><cell cols="2">Recall FScore Acc</cell><cell>Recall FScore</cell></row><row><cell cols="5">EndoVis18-VQA LV-GPT (RN18) 0.6811 0.4649 0.4649 0.6811 0.4720 0.4681</cell></row><row><cell>Cholec80-VQA</cell><cell cols="4">LV-GPT (Swin) 0.9429 0.7339 0.7439 0.9414 0.7251 0.7360</cell></row><row><cell cols="5">PSI-AVA-VQA LV-GPT (ViT) 0.6549 0.4132 0.3971 0.5905 0.3742 0.3463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on vision token (VT) embedding.</figDesc><table><row><cell cols="4">VB-VE C-VE VT-TY +VT-PE VT-TY +VT-ZPE LV-GPT (RN18)</cell><cell>LV-GPT (ViT)</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Recall FScore Acc</cell><cell>Recall FScore</cell></row><row><cell>✓</cell><cell></cell><cell cols="2">0.6287 0.4061 0.4063 0.6147 0.4199 0.3679</cell></row><row><cell>✓</cell><cell></cell><cell cols="2">0.6728 0.4366 0.4455 0.6504 0.4792 0.4323</cell></row><row><cell>✓</cell><cell>✓</cell><cell cols="2">0.6811 0.4720 0.4681 0.6259 0.4306 0.3805</cell></row><row><cell>✓</cell><cell>✓</cell><cell cols="2">0.6811 0.4649 0.4649 0.6659 0.4920 0.4336</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>chat.openai.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>One class shares a common name with a surgical phase class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Code available: github.com/lalithjets/SurgicalGPT</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">Hong Kong Research Grants Council (RGC) Collaborative Research Fund</rs> (<rs type="grantNumber">CRF C4026-21GF</rs> and <rs type="grantNumber">CRF C4063-18G</rs>) and <rs type="funder">Shun Hing Institute of Advanced Engineering</rs> (<rs type="grantNumber">BME-p1-21/8115064</rs>) at the <rs type="affiliation">Chinese University of Hong Kong</rs>. M. Islam was funded by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">[EP/W00805X/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jyJAMxZ">
					<idno type="grant-number">CRF C4026-21GF</idno>
				</org>
				<org type="funding" xml:id="_qKYrSRa">
					<idno type="grant-number">CRF C4063-18G</idno>
				</org>
				<org type="funding" xml:id="_U99J2Yz">
					<idno type="grant-number">BME-p1-21/8115064</idno>
				</org>
				<org type="funding" xml:id="_qSHWUES">
					<idno type="grant-number">[EP/W00805X/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computer-assisted surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graphics Appl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="51" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gawande</surname></persName>
		</author>
		<title level="m">Error in medicine: what have we learned?</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2612" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Block bilinear superdiagonal fusion for visual question answering and visual relationship detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8102" to="8109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advance in Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From images to textual prompts: zero-shot VQA with frozen large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10846</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simulation-based surgical training systems in laparoscopic surgery: a current review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rozenblit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="491" to="510" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simulation in surgical training: educational issues and practical implications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kneebone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Educ</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="277" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: a simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SOLOIST: few-shot task-oriented dialog with a single pretrained auto-regressive model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shayandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.052983</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer-assisted learning versus a lecture and feedback seminar for teaching a basic surgical technical skill</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Howdieshell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Surg</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="508" to="510" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simulation and surgical training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Clin. Pract</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2120" to="2125" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Surgical-VQA: Visual question answering in surgical scenes using transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-14" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MedFuseNet: an attention-based multimodal deep learning model for visual question answering in the medical domain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LAMDA: language models for dialog applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards holistic surgical scene understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Valderrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="442" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-142" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ChatCAD: interactive computer-aided diagnosis on medical image using large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07257</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with coattention learning for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond bilinear: generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5947" to="5959" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
