<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change</title>
				<funder ref="#_VyXJtvV #_6vWAj2J #_fVSNugv">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Lampen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daeseung</forename><surname>Kim</surname></persName>
							<email>dkim@houstonmethodist.org</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianshu</forename><surname>Kuang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hannah</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">A K</forename><surname>Liebschner</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurosurgey</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaime</forename><surname>Gateno</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E94BC6900F7701D82BA876DBE21A0D1</idno>
					<idno type="DOI">10.1007/978-3-031-43996-454.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical Planning</term>
					<term>Deep Learning</term>
					<term>Spatiotemporal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate surgical planning for orthognathic surgical procedures requires biomechanical simulation of facial soft tissue changes. Simulations must be performed quickly and accurately to be useful in a clinical pipeline, and surgeons may try several iterations before arriving at an optimal surgical plan. The finite element method (FEM) is commonly used to perform biomechanical simulations. Previous studies divided FEM simulations into incremental steps to improve convergence and model accuracy. While incremental simulations are more realistic, they greatly increase FEM simulation time, preventing integration into clinical use. In an attempt to make simulations faster, deep learning (DL) models have been developed to replace FEM for biomechanical simulations. However, previous DL models are not designed to utilize temporal information in incremental simulations. In this study, we propose Spatiotemporal Incremental Mechanics Modeling (SIMM), a deep learning method that performs spatiotemporally-aware incremental simulations for mechanical modeling of soft tissues. Our method uses both spatial and temporal information by combining a spatial feature extractor with a temporal aggregation mechanism. We trained our network using incremental FEM simulations of 18 subjects from our repository. We compared SIMM to spatial-only incremental and single-step simulation approaches. Our results suggest that adding spatiotemporal information may improve the accuracy of incremental simulations compared to methods that use only spatial information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Orthognathic surgery corrects facial skeletal deformities that may cause functional and aesthetic impairments. In orthognathic surgical procedures, the jaws are cut into several bony segments and repositioned to desired locations to achieve an ideal alignment. Surgeons plan the movement of osteotomized bony segments before surgery to obtain the best surgical outcome. While surgeons do not operate on the soft tissue of the face during the procedure, it is passively moved by the underlying bone, causing a change in facial appearance. To visualize the proposed outcome, simulations of the planned procedure may be performed to predict the final facial tissue appearance. Current simulation techniques use the finite element method (FEM) to estimate the change of the facial tissue caused by the movement of the bony segments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Subjectspecific FEM models of the facial tissue are created from preoperative Computed Tomography (CT) imaging. The planned bony displacement is fed to the model as an input boundary condition. Previous studies divided the input bony displacement into smaller, incremental steps to improve model accuracy <ref type="bibr" target="#b6">[7]</ref>. While incremental FEM simulations are very accurate, they require significant computational time, sometimes approaching 30 min to perform a single simulation <ref type="bibr" target="#b6">[7]</ref>. This restricts the utility of FEM since surgeons may try several iterations before arriving at an optimal surgical plan, and simulations must be performed quickly to be useful in a clinical pipeline. Previous studies have implemented deep learning (DL) methods to shorten soft tissue simulations. Physics-informed neural networks (PINNs) learn the constitutive model of soft tissues, however, these models do not generalize well to new settings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. Other studies learned a general biomechanical model by training a UNet-style network on FEM simulations, but these methods are limited to grid-like structures and cannot represent irregular meshes used in FEM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. PointNet++ has also been used to learn biomechanical deformation based on point cloud data representing nodes within a mesh <ref type="bibr" target="#b9">[10]</ref>. While PointNet++ can represent nodes in an irregular mesh, it does not capture the connections (edges) between nodes. PhysGNN utilizes edge information by learning biomechanical deformation based on graphs created from FE meshes <ref type="bibr" target="#b15">[16]</ref>. However, the main limitation of all the aforementioned DL methods is that they perform "single-step" simulations (Fig. <ref type="figure" target="#fig_0">1</ref>), which are not ideal for modeling non-linear materials such as facial soft tissue, especially when large deformation (&gt; 1mm) is involved <ref type="bibr" target="#b6">[7]</ref>. Incremental simulations help deal with the non-linearity of soft tissue by simulating several transitional states, providing a more realistic and traceable final postoperative prediction.</p><p>A simplistic approach to performing incremental simulations using DL is to break a large simulation into smaller steps and perform them in sequential order, but independently. However, this does not allow the model to correct errors, causing them to accumulate over incremental steps. One previous work implemented a data augmentation strategy to minimize network error in incremental simulations of soft tissue <ref type="bibr" target="#b16">[17]</ref>. However, the network used was not designed to utilize temporal information or capture temporal trends across incremental steps, such as deformation that follows a general trajectory, which can be utilized to improve incremental predictions. While another previous work incorporated long short-term memory into a convolutional neural network (CNN) to learn across multiple timesteps, the use of a CNN means the network is limited to representing regular grids without integration of edge information <ref type="bibr" target="#b3">[4]</ref>. Therefore, a method is needed which can perform incremental simulations using both spatial and temporal information while accepting irregular mesh geometry in the form of graphs. There are several technical challenges to combining spatial and temporal information in a deep learning network. First, it can be challenging to capture spatiotemporal features that effectively represent the data, especially in cases where the objects change shape over time. Second, spatiotemporal networks are often computationally complex and prone to overfitting, which limits the model's clinical utility. For this reason, an explainable spatiotemporal network, i.e. one that learns temporal trends from already extracted spatial features, while minimizing computational complexity is needed.</p><p>In this study, we hypothesize that utilizing both spatial and temporal information for incremental simulations can improve facial prediction accuracy over DL networks that perform single-step simulations or incremental simulations while only considering spatial information. Therefore, the purpose of this study is to introduce a spatiotemporal deep learning approach for incremental biomechanics simulation of soft tissue deformation. We designed a network that learns spatial features from incremental simulations and aggregates them across multiple incremental simulations to establish sequential continuity. The contributions of this work are (1) a method for combining spatial and temporal learning in an incremental manner (2) a method for performing incremental simulations while preserving knowledge from previous increments. Our proposed method successfully implements spatiotemporal learning by observing temporal trends from spatial features while minimizing computational complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The Spatiotemporal Incremental Mechanics Modeling (SIMM) method predicts the incremental deformation of a three-dimensional (3D) soft tissue mesh based on incrementally planned input displacement. The SIMM network consists of two main operations: 1) spatial feature encoding and 2) temporal aggregation of spatial features (Fig. <ref type="figure" target="#fig_1">2</ref>). The SIMM method is designed to first encode spatial features using a graph neural network optimized for spatial tasks, then observe how these features change over time using a temporal aggregation mechanism. SIMM is designed to be explainable and efficient by using the already extracted spatial features to observe temporal trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Representation</head><p>The SIMM method uses an incremental simulation scheme, shown in Fig. <ref type="figure" target="#fig_1">2</ref>. SIMM is designed to accept input data in graph format that represents volumetric meshes used in FEM. In the input graphs, vertices correspond to nodes in an FE mesh and the edges correspond to the connections between nodes within an element <ref type="bibr" target="#b12">[13]</ref>. For this reason, we will refer to our input data as "meshes" consisting of "nodes" and "edges" in the rest of this manuscript. In the SIMM method, the planned bony displacement d B for a surgical procedure is broken into smaller steps and applied incrementally. The incremental planned bony displacement d</p><formula xml:id="formula_0">(t) B</formula><p>and geometric information from an input facial mesh m</p><formula xml:id="formula_1">(t)</formula><p>F are fed to the spatial sub-network to predict the incremental facial deformation d (t) F,s . The geometric information from the input facial mesh consists of an adjacency matrix a F and a set of edge weights w (t) F . The adjacency matrix is a binary matrix describing the connections between nodes. The edge weights are calculated as the inverse of the Euclidean distance between nodes, providing the network with spatial information. Spatial features x s are extracted in each GNN layer and aggregated using jumping-knowledge connections <ref type="bibr" target="#b17">[18]</ref>. In the temporal sub-network, the stack of encoded spatial features [x F,gt . The spatiotemporally predicted facial deformation is then used to update the facial mesh for the subsequent timestep. After all incremental steps are applied (at t = N ), the final predicted deformation is used to calculate the predicted postoperative facial mesh m N F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network</head><p>Spatial sub-network: We used a PhysGNN network to learn the spatial features <ref type="bibr" target="#b15">[16]</ref>. PhysGNN consists of six GNN layers that extract features in an increasing neighborhood around a given node. PhysGNN also uses jumpingknowledge connections which aggregate information from the first three and last three GNN layers. The jumping-knowledge connections use an aggregation function to fuse features across the GNN layers using the equation:</p><formula xml:id="formula_2">y s = f aggr (x (0) , ..., x (l) )<label>(1)</label></formula><p>where y s is the spatially aggregated features, and x (l) are the features from layers 0 to l. f aggr can be any form of aggregation function, such as concatenation.</p><p>In this work, we use a long-short-term memory (LSTM) attention aggregation mechanism to achieve optimal results, which has been demonstrated in our ablation study (Sect. 3.4) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal sub-network:</head><p>To capture temporal trends, we implemented additional aggregation layers to process features across a sequence of incremental timepoints. Instead of aggregating features across several spatial "neighborhoods" as seen in Eq. ( <ref type="formula" target="#formula_2">1</ref>), these additional aggregation layers aggregate features from a given spatial neighborhood across several sequential timepoints (t), as seen in Eq. ( <ref type="formula">2</ref>):</p><formula xml:id="formula_3">y t = f aggr (x (t) , ..., x (t-N ) ) (2)</formula><p>where y t is the temporally aggregated features and x (t) are the features to be aggregated across timepoints (t) to (t -N ). We hypothesized that including more than one previous timepoint in the aggregation layer may improve the network performance (see Sect. 3.4). We used the same LSTM-attention aggregation mechanism as used in the PhysGNN spatial sub-network.</p><p>Training Strategy: The SIMM network produces two predictions of the incremental tissue deformation, one from the spatial sub-network and the other after spatiotemporal aggregation (Fig. <ref type="figure" target="#fig_1">2</ref>). We used incremental FEM simulations as ground truth while training our network. The loss was calculated as the meansquared error between the predicted deformation and the ground truth FEM deformation. We use the following equation to calculate the loss of the network:</p><formula xml:id="formula_4">l SIMM = l s + l st (3)</formula><p>where l SIMM is the total loss of the SIMM network, l s is the loss of the spatial sub-network predicted deformation d</p><formula xml:id="formula_5">(t)</formula><p>F,s , and l st is the loss of the spatiotemporal predicted deformation d (t) F,st . The combined loss was used to train the network to ensure the spatial sub-network still learns to adequately encode the spatial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The SIMM method was evaluated on a dataset of incremental FEM simulations from 18 subjects using a leave-one-out cross-validation (LOOCV). The subjects were chosen from our digital archive of patients who underwent doublejaw orthognathic surgery (IRB# PRO00008890). FEM meshes were generated from CT scans and FE simulations were performed using an existing clinical pipeline <ref type="bibr" target="#b6">[7]</ref>. Boundary conditions including bony movements and sliding effect were applied to the FEM mesh. FEM simulation was performed to simulate the facial change with at least 200 incremental steps to have enough training data. The total number of incremental steps varied, depending on the stability of each incremental simulation step. Meshes for all subjects consisted of 3960 nodes and 2784 elements. The incremental bony displacement information was fed to the network as an input feature vector for each node. For facial nodes assumed to move together with the underlying moving bony surface, the input displacement vector was equal to the bony surface displacement d (t) B , which describes the boundary condition between the moving bony structures and the facial soft tissue. For facial nodes not on the surface of a moving bony segment, or that are on the surface of a fixed bony segment, the input displacement vector was initialized as all zeros. To increase the number of simulations for training, we further split the simulation for a given subject into several "sub-simulations" consisting of a subset of the incremental timesteps. The timesteps for each sub-simulation were determined by using a maximum deformation threshold, d max . For example, starting from timepoint 0, the first incremental timepoint with a maximum deformation exceeding the threshold would be included in the sub-simulation (Fig. <ref type="figure" target="#fig_0">S1</ref>). The starting point of each sub-simulation was unique, however, all sub-simulations ended on the postoperative face as the last incremental step, allowing for a direct comparison between all sub-simulations on the postoperative mesh for a given subject. We used a maximum deformation threshold of 0.5mm when splitting each subject's simulation into sub-simulations, which was chosen based on an ablation study (see Sect. 3.4). The number of sub-simulations for each subject can be found in Tab. S1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details and Evaluation Metrics</head><p>All networks were trained in pytorch using the Adam optimizer with an initial learning rate of 5e-3 for 100 epochs on an NVIDIA Tesla V100. We trained all networks in leave-one-out cross-validation across all 18 subjects. The loss was calculated as the mean squared error between the network-predicted and FEM-predicted deformation vectors for each node within the facial mesh. The final accuracy was calculated as the Euclidean distance between the networkpredicted and FEM-predicted node coordinates on the final postoperative face. The distribution of the Euclidean distances was not found to be normal using the Kolmogorov-Smirnov test, so statistical significance between methods was tested using the Wilcoxon signed-rank test and a p-value of 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Baselines</head><p>We compared our SIMM method with two baseline methods: 1) a single-step method, and 2) an incremental method. For the single-step method, we trained a PhysGNN network to predict the total facial deformation in a single step. The same sub-simulations used in the incremental and SIMM methods (see Sect. 3.1) were used to train the single-step network, however, all intermediate timepoints between the first and final timepoints were removed. For the incremental method, we used PhysGNN to perform incremental simulations. The incremental method used the prediction from timepoint t-1 to update the edge weights in timepoint t, similar to the feedback loop in SIMM (Fig. <ref type="figure" target="#fig_1">2</ref>). However, no temporal aggregation or separate spatiotemporal prediction was used.</p><p>Our SIMM method achieved a mean error of 0.42 mm on all subjects (Table <ref type="table" target="#tab_0">1</ref>) with subject-specific mean error between 0.23 and 0.77 mm (Tab. S1). In comparison, the single-step method achieved a mean error of 0.44 mm on all subjects (Table <ref type="table" target="#tab_0">1</ref>) with subject-specific mean error between 0.30 and 0.91 mm (Tab. S1). The incremental method achieved a mean error of 0.47 mm on all subjects (Table <ref type="table" target="#tab_0">1</ref>) with subject-specific mean error between 0.25 and 1.00 mm (Tab. S1). Statistical analysis showed SIMM performed significantly better than the singlestep and incremental methods, while the single-step and incremental methods were not significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-step</head><p>Incremental SIMM 332 nodes &gt;1mm error 230 nodes &gt;1mm error 130 nodes &gt;1mm error Fig. <ref type="figure">3</ref>. Error of a) the single-step method, b) incremental method, and c) SIMM method for subject 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>We also performed ablation studies to investigate the effects of several key components in the SIMM method. First, when splitting subject simulations into sub-simulations, we varied the maximum deformation threshold d max to 1.0, 0.5, and 0.1 mm. We found a d max of 0.5 achieved the best performance in the incremental method, although the best d max may change for different cases (Tab. S2). Second, we investigated the effect of the type of aggregation mechanism used in the PhysGNN network of the incremental method by replacing the LSTM-attention aggregation mechanism with a concatenation aggregation mechanism. The mean error increased to 1.25 mm when using a concatenation aggregation. Third, we tried increasing the number of previous timepoints in the spatial features stack to be used in the temporal aggregation layers from 1 previous timepoint to 5 previous timepoints. We hypothesized that including multiple previous timepoint in the aggregation layer may improve performance.</p><p>The mean error increased to 0.44 mm when using 5 previous timepoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>The SIMM method achieved a lower mean error than the single-step and incremental methods, as seen in the quantitative results (Table <ref type="table" target="#tab_0">1</ref>). The results of the incremental method suggest the network accumulates errors across incremental steps, as seen in a plot of the predicted deformation vectors over time (Fig. <ref type="figure" target="#fig_1">S2</ref>). Figure <ref type="figure">3</ref> shows an example of the improvement of SIMM over the single-step and incremental methods. The results of the ablation study showed concatenation aggregation did not perform as well as LSTM-attention aggregation, which is consistent with other studies that investigated aggregation methods in GNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Our ablation study also demonstrated that increasing the number of previous timepoints in the temporal feature aggregation did not improve network performance. We think this result is likely caused by over-smoothing the temporal information when aggregating features from many previous timepoints. One limitation of our SIMM method is that it requires incremental FEM simulations to be trained. This greatly increases training requirements over single-step methods, which can feasibly be trained only on preoperative and postoperative facial data without performing incremental FEM simulations. However, this is true of any incremental simulation method. Once SIMM is trained, the inference simulation time is considerably faster than FEM, which can take several minutes to perform an incremental simulation. The SIMM method could be easily extended to biomechanical simulations of many other types of soft tissues and clinical applications.</p><p>In conclusion, we successfully created a spatiotemporal incremental mechanics modeling (SIMM) method for simulating incremental facial deformation for surgical planning. The SIMM method shows potential to improve simulation accuracy over methods based on spatial information alone, suggesting the importance of spatiotemporal learning for incremental simulations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An overview of the difference between the "single-step" DL method and the incremental DL method, which replicates incremental FEM simulations.</figDesc><graphic coords="2,55,98,348,59,340,24,142,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed framework of Spatiotemporal Incremental Mechanics Modeling (SIMM) for incremental mechanical modeling of facial tissue deformation.</figDesc><graphic coords="4,56,46,57,92,339,40,138,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.] are aggregated across several incremental steps, resulting in a separate spatiotemproal prediction of the incremental facial tissue deformation d (t) F,st . Both the spatial-only and spatiotemproal predictions are compared to the ground truth FEM predicted incremental facial deformation d (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean Euclidean distance error, mean simulation time, and model size comparison between SIMM, Incremental, and Single-step deep learning methods. StdDev [mm] 0.44 ± 0.14 a 0.47 ± 0.19 a 0.42 ± 0.13 b Values with different superscript letters are significantly different (p&lt; 0.05)</figDesc><table><row><cell>Method</cell><cell cols="3">Single-Step Incremental SIMM</cell></row><row><cell>Mean Error ± Mean Simulation Time [s]</cell><cell>0.08</cell><cell>1.14</cell><cell>1.36</cell></row><row><cell>Model Size [MB]</cell><cell>2052</cell><cell>2774</cell><cell>3322</cell></row><row><cell># of Parameters [M]</cell><cell>0.9</cell><cell>0.9</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>*  </p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by <rs type="funder">NIH</rs> awards <rs type="grantNumber">R01 DE022676</rs>, <rs type="grantNumber">R01 DE027251</rs>, and <rs type="grantNumber">R01 DE021863</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VyXJtvV">
					<idno type="grant-number">R01 DE022676</idno>
				</org>
				<org type="funding" xml:id="_6vWAj2J">
					<idno type="grant-number">R01 DE027251</idno>
				</org>
				<org type="funding" xml:id="_fVSNugv">
					<idno type="grant-number">R01 DE021863</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personalising left-ventricular biophysical models of the heart using parametric physics-informed neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kozerke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patient specific finite element model of the face soft tissues for computer-assisted maxillofacial surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chabanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Luboz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Payan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="151" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2017-December</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time simulation of viscoelastic tissue behavior with physics-guided deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rivest-Hénault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">102165</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A clinically validated prediction method for facial soft-tissue changes following double-jaw surgery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4252" to="4261" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new approach of predicting facial changes following orthognathic surgery using realistic lip sliding effect</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel incremental simulation of facial changes following orthognathic surgery using FEM with realistic lip sliding effect</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102095</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three-dimensional soft tissue prediction in orthognathic surgery: a clinical comparison of Dolphin, ProPlan CMF, and probabilistic finite element modelling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Knoops</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Oral Maxillofac. Surgery</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="511" to="518" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel soft tissue prediction methodology for orthognathic surgery based on probabilistic finite element modelling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Knoops</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">197209</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for biomechanical modeling of facial tissue deformation in orthognathic surgical planning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lampen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surgery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generic physics-informed neural network-based constitutive model for soft biological tissues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">113402</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simulation of hyperelastic materials in real-time using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mendizabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101569</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning soft tissue behavior of organs for surgical navigation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1147" to="1155" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PhysGNN: a physics-driven graph neural network based model for predicting soft tissue deformation in image-guided neurosurgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giannacopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04352</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Soft-Tissue Simulation from Models and Observation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazanzides</surname></persName>
		</author>
		<idno>ISMR 2021</idno>
	</analytic>
	<monogr>
		<title level="m">2021 International Symposium on Medical Robotics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="8676" to="8685" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
