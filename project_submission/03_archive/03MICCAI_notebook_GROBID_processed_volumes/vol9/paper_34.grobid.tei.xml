<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yannik</forename><surname>Frisch</surname></persName>
							<email>yannik.frisch@gris.tu-darmstadt.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Fuchs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Sanner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><forename type="middle">Anton</forename><surname>Ucar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universitätsmedizin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
								<address>
									<settlement>Mainz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marius</forename><surname>Frenzel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universitätsmedizin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
								<address>
									<settlement>Mainz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joana</forename><surname>Wasielica-Poslednik</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universitätsmedizin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
								<address>
									<settlement>Mainz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Gericke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universitätsmedizin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
								<address>
									<settlement>Mainz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><forename type="middle">Mathias</forename><surname>Wagner</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Universitätsmedizin</orgName>
								<orgName type="institution" key="instit2">Johannes Gutenberg-Universität Mainz</orgName>
								<address>
									<settlement>Mainz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Dratsch</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Uniklinik Köln</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anirban</forename><surname>Mukhopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University Darmstadt</orgName>
								<address>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="354" to="364"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">16C2FF1D584909D6C633EF5971DBA86E</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Models</term>
					<term>Denoising Diffusion Models</term>
					<term>Cataract Surgery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cataract surgeries are amongst the most frequently performed treatments, with 4,000 to 10,000 annual operations per million people <ref type="bibr" target="#b26">[27]</ref>. The high demand naturally asks for automation and advanced assistance systems and has seen increasing attention within the CAI community in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Nevertheless, the publicly available data for training such systems is limited: given the nature of the surgeries, certain surgical phases take more time than others. Further, there are variances in their length based on the surgeon's skill and the patient's particular needs. Since surgical tool usage is strongly coupled with the surgical phase, certain phases and tools are shown more frequently than others, constituting an inherent imbalance in the data. As displayed in Fig. <ref type="figure" target="#fig_0">1</ref> for the CATARACTS dataset <ref type="bibr" target="#b0">[1]</ref>, such imbalances impact the performance on downstream tasks, e.g. surgical phase prediction or tool classification.</p><p>One cannot simply gather new data showing unusual tools to perform the required actions during a surgical step. Therefore, we must find different ways to represent them in the data and counteract the imbalance. The usual countermeasures in the form of oversampling can increase prediction accuracy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>. Still, they only alter the number of times an underrepresented sample is seen during training, resulting in a fragile representation of the tools and phases and hindering generalisation. Generative models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> can potentially solve this by synthesising unseen examples for underrepresented tool and phase combinations. Regarding image quality, generative models based on diffusion models reached superior performance over alternative methods in the recent past <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. Despite the successes, these models have not yet found application in Surgical Data Science. In the broader medical domain, they have been utilised to generate thorax CT scans <ref type="bibr" target="#b9">[10]</ref>, brain MRI <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> and breast and knee MRI scans <ref type="bibr" target="#b9">[10]</ref>.</p><p>Although these applications have shown promising results, there is a demand for conditional image generation for Surgical Data Science. Since most downstream applications consist of supervised methods, they require training targets, and the likelihood of unconditionally generating diverse samples for unusual cases is very low. For conditional generation with denoising diffusion models, classifier guidance has been introduced recently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> but requires computationally extensive parallel training of a separate classifier model. Instead, Classifier-Free Guidance (CFG) <ref type="bibr" target="#b7">[8]</ref> yields a simple trick to achieve class-constrained generative Fig. <ref type="figure">2</ref>. Ground truth toolset occurrence for the worst performing CATARACTS phases. Some toolsets, e.g. (Bonn Forceps, Capsulorhexis Forceps) during Implant Ejection (left), are rarely present and poorly detected, deteriorating the overall performance. We focus on such rare toolsets to generate new samples for a phase. E.g. for Manual Aspiration (middle), we mainly want additional samples showing the Hydrodissection Canulla. The chord diagram for the Suturing phase (right) shows the complexity of such occurrences.</p><p>results with diffusion models. Conditional diffusion models have been applied to generate medical images from a few binary label inputs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. Peng et al. <ref type="bibr" target="#b16">[17]</ref> synthesise 3D volumes from 2D reference slides. Sagers et al. <ref type="bibr" target="#b22">[23]</ref> have built on DALL•E2 for the targeted generation of images of skin diseases, and Moghadam et al. <ref type="bibr" target="#b12">[13]</ref> have generated histopathology images with genotype guidance.</p><p>Precise conditioning beyond a few binary labels is crucial for synthesising valuable surgical data. Instead, we need to train a model that can generate diverse examples based on multi-class or multi-label conditions, e.g. certain surgical phases, combinations of surgical tools, or both. We show that using an adapted denoising diffusion model together with CFG can yield high-quality samples of cataract surgery data, even for rare cases such as the CATARACTS phase and tool combinations shown in Fig. <ref type="figure">2</ref>.</p><p>To the best of our knowledge, ours is the first work combining CFG with diffusion models to efficiently generate realistic cataract surgery data with a complex underlying label structure. Additionally, we examine the cataract video data for the worst-performing phases of a pre-trained tool usage classifier. We then leverage the conditional denoising diffusion model to generate unseen samples for these phases. Our conditioned tools are recognisable by the tool classifier and are hard to differentiate from real images, even for clinicians with more than five years of experience. Further, we demonstrate how our synthetically extended data can alleviate the data sparsity problem for the downstream task. Overall, our evaluations show that the model can generate valuable examples to build the bridge to clinical application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The following section describes how we build our generative diffusion model and integrate CFG to generate cataract surgery frames conditioned on surgical phases and tools. Furthermore, we provide an analysis of the worst-performing surgical steps for a pre-trained tool classifier model. Finally, we demonstrate the sampling procedure using the generative model to improve the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Denoising Diffusion Probabilistic Models</head><p>The fundamental underlying idea of Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b6">[7]</ref> is a forward diffusion process that gradually adds Gaussian noise to an image x. This process is defined by q(x t |x t-1 ) = N (x t ; √ 1β t x t-1 , β t I), which uses a pre-defined variance schedule {β t ∈ (0, 1)} T t=1 . Eventually, when T → ∞, x T becomes equivalent to an isotropic Gaussian distribution. When we can learn the reverse process q(x t-1 |x t ), we can generate samples starting from a simple Gaussian noise x T ∼ N (0, I). To achieve this, one can approximate the conditional probabilities by p θ (x t-1 |x t ) = N (x t-1 ; μ θ (x t , t), Σ θ (x t , t)). In practice and after some mathematical simplifications, this reduces the reverse process to estimating the noise t between x t and x t+1 , as shown by Ho et al. <ref type="bibr" target="#b6">[7]</ref>. Usually, the noise is parameterised by a UNet-type architecture θ , optimised by minimising the simplified objective</p><formula xml:id="formula_0">L t = E t∼[1,T ],x0, t [|| t -θ (x t , t)|| 2 ] = E t∼[1,T ],x0, t [|| t -θ ( √ α t x 0 + √ 1 -α t t , t)|| 2 ] (1)</formula><p>where</p><formula xml:id="formula_1">α 1:T ∈ (0, 1] T , α t = (1 -β t )α t-1 .</formula><p>Denoising Diffusion Implicit Models (DDIMs) <ref type="bibr" target="#b24">[25]</ref> are a generalisation of these formulations for non-Markovian, more efficient sampling. Instead of the complete Markov chain, they are defined on a reduced set of intermediate latents {x τ1 , ..., x τS }, where [τ 1 , ..., τ S ] ⊆ [1, ..., T ]. This reduction results in significantly fewer inference steps required to generate samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classifier-Free Guidance</head><p>By utilising Classifier-Free Guidance (CFG) <ref type="bibr" target="#b7">[8]</ref>, we can learn the unconditional model p θ (x) and the model p θ (x|y (p) , y (s) ) conditioned on phase y (p) and toolset y (s) using a single neural network. The corresponding gradient is given by</p><formula xml:id="formula_2">∇ xt log p(y (p) , y (t) |x t ) = ∇ xt log p(x t |y (p) , y (s) ) -∇ xt log p(x t ) = - 1 √ 1 -ᾱt ( θ (x t , t, y (p) , y (s) ) -θ (x t , t))<label>(2)</label></formula><p>This gradient yields </p><formula xml:id="formula_3">¯ θ (x t ,</formula><p>where w is a weighting hyperparameter. The weighted noise ¯ θ (x t , t, y (p) , y (s) ) can simply replace in Eq. 1. We add an embedding module emb (p) to the UNet architecture, which converts categorical phase labels y (p) into one-hot encoded vectors to include them into the input. To simultaneously include tool labels in the form of (non-exclusive) binary vectors, we compute projections emb (s) of the same size as the time-step and phase label embeddings using stacked dense layers. All embeddings are concatenated as {emb t (t), emb (p) (y (p ), emb (s) (y (s) )} and fed to the conditional denoising UNet model together with the noisy image x t . Figure <ref type="figure" target="#fig_1">3</ref> visualises the forward process, reverse process, and sampling procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tool Usage Analysis and Sample Generation</head><p>Following Roychowdhury et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>, we deploy a ResNet50 architecture to predict tools present in a given frame. An inspection of the phase-wise performance reveals that the model underperforms for underrepresented phases, as shown in Fig. <ref type="figure">2</ref>. Certain tool combinations are sparsely used in these phases, causing a significant drop in prediction performance. Appendix Fig. <ref type="figure">6</ref> displays the distribution of toolset labels for all surgical steps. We synthesise new examples for every phase to smooth out the distribution. We then re-train the classifier model on the original and extended data combined. Adding samples based on toolsets y s and phase labels y p requires a throughout pre-selection of query inputs due to the complex underlying latent structure. To automatise this process, we compute the joint probabilities p φ (y s , y p ) from the available CATARACTS annotations. We can then generate rare cases for a given phase by sampling tool labels from the inverse of p φ (y s , y p ) = p(y p )p(y s |y p ). This ensures we synthesise examples of underrepresented cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In this section we explain our experimental setup, demonstrate the synthesis of high-quality samples and show how these can improve the downstream model's performance on challenging phases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup and Dataset</head><p>To evaluate the quality of synthesised images, we generate 30,000 samples with phase and toolset conditions sampled from p -1 φ (y s , y p ) = (1p φ (y s , y p ))/ (1p φ (y s , y p )), as explained in Sect. 2.3. The resulting number of examples is close to the test split size of CATARACTS sampled at 3 FPS. We compare the proposed approach to state-of-the-art baselines for conditional generative modelling: A conditional LS-GAN (cLS-GAN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and VQ-VAE2 <ref type="bibr" target="#b19">[20]</ref>. For the latter, we deploy a PixelSNAIL prior <ref type="bibr" target="#b2">[3]</ref> for bottom-and top-level features. Every model is trained on two NVIDIA A40 GPUs for 500 epochs with about 45,000 training examples each. Other hyperparameters vary for each model and can be accessed next to the code to reproduce our results and the generated data at https:// github.com/MECLabTUDA/CataSynth. We take τ S = 200 denoising steps using the DDIM formulation <ref type="bibr" target="#b24">[25]</ref> to generate our images, yielding a reasonable inference speed and sample quality trade-off. The displayed and evaluated images are generated with a CFG weight of ω = 2.0. We use a random chance of p = 0.1 for the unconditional model during training. All models are trained on images of 128 × 128 pixels and up-sampled with bilinear interpolation to 270 × 480 for displaying purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Image Quality</head><p>We deploy a variety of quantitative metrics to assess the quality of generated images, for which Table <ref type="table" target="#tab_1">1</ref> lists the results. Firstly, FID <ref type="bibr" target="#b1">[2]</ref> and KID <ref type="bibr" target="#b1">[2]</ref> compare the spatial distribution of the synthesised images with the training set distribution of CATARACTS. Further, we use the classifier from Sect. 2.3 to obtain the Inception Score (IS) of the generated images. By using the scores of a classifier trained for tool recognition, this metric yields a measurement of tool realism. Additionally, we evaluate the F1 score for the pre-trained classifier identifying tools in the conditionally generated images. We denote this metric as CF1. Lastly, we also compute the perceptual LPIPS diversity <ref type="bibr" target="#b27">[28]</ref> to catch mode collapses, a common problem with generated models, leading to reduced image variability. In summary, our model generates images of superior quality regarding spatial properties, tool label preservation and diversity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Downstream Tool Classification</head><p>Finally, we re-train the tool-set classifier on a combined dataset of the original and the synthesised samples. As shown in Table <ref type="table" target="#tab_4">3</ref>, re-training on the combined data (Extended ) improves the tool-set classifier's prediction performance on the original test data compared to training solely on the original training data (Original ). For completeness, we also report the performance from fitting the classifier exclusively on the synthetic data and evaluating it on the test split of CATARACTS, denoted CAS <ref type="bibr" target="#b19">[20]</ref>. Figure <ref type="figure" target="#fig_3">5</ref> displays the individual differences in the classifier's F1 scores for the originally worst-performing phases. Extending the data with synthetic samples yields performance gains for five of the seven most critical phases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a generative model based on denoising diffusion models and classifierfree guidance, powerful enough to synthesise cataract surgery images that are hard to distinguish for a pre-trained tool classifier and clinical experts. For underrepresented phases, state-of-the-art baselines tend to produce frames that show eyes without correct anatomy or barely recognisable tools, resulting in a significant performance gap. Distortions in the dataset further deteriorate their learning. On the contrary, we demonstrate that the proposed approach outperforms these baselines in terms of image quality and tool preservation. As a limitation of our approach, we found that the generalisation capabilities must be strengthened to generate unreasonable samples, e.g. completely wrong tools during a phase. Such samples can happen if they are present in the data. Though, a targeted generation would require a more substantial representation. Additionally, while tool realism is significantly better for the proposed method, the CF1 and CAS scores indicate that it can be further improved. Besides, the underlying class imbalances and lack of available data are even more severe for the downstream task of anatomy and tool segmentation. In future work, we will extend the proposed method to generate segmentation targets, temporally connected data and deploy a tighter structure for conditioning. Overall, we are the first to have shown how conditional diffusion models can successfully be applied to mend data sparsity and generate high-quality cataract surgery images suitable for clinical application. These improvements can bring computer-assisted cataract surgery one step closer to the next level of automation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of CATARACTS phases. Except for Idle -which can appear anytime -all phases are displayed in the usual chronological order. The dataset yields severe class imbalances regarding the available frames per phase (darker blue), which results in performance drops for underrepresented phases (lighter blue). (Color figure online)</figDesc><graphic coords="2,56,46,291,44,339,16,91,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the Conditional Denoising UNet. The model θ is trained to reverse the Diffusion Process, mapping a noisy sample xt to a less noisy xt-1. Condition embeddings based on the surgical phase y (p) and the toolset y (s) are concatenated with the diffusion time step embedding and fed into every level of the UNet to guide targeted sample generation. We utilise the final model to synthesise realistic examples and improve the predictions ŷ(s) of a toolset classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative examples for the least common toolsets of the three most challenging phases of CATARACTS. The proposed method produces superior results compared to the baselines, which struggle with the rare combinations of tools and phases shown. The realistically generated tools are especially noteworthy.</figDesc><graphic coords="6,95,40,266,45,300,16,140,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Phase-wise performance changes for critical phases after retraining including synthetic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative image quality evaluation. Additional examples for the other phases and randomly chosen toolsets are displayed in the Appendix. The qualitative results reflect the quantitative metrics and illustrate our method's superior image quality and tool preservation. Besides qualitative evaluations, we conduct a user study to assess the realism of our generated images. Therefore, we let six clinicians survey 50 generated and 50 real images of size 235 × 132 pixels in a randomised side-by-side view. In every example, both images show the same phase and tool combination, and participants must distinguish the real image from the synthesised sample. We grouped the participants by domain experience, with domain experts (DE) having more than five years of domain expertise in cataract surgery. On average, the miss rate or false classification rate (FR) was 0.61. This result translates into clinical</figDesc><table><row><cell>Method</cell><cell cols="2">FID (↓) KID (↓)</cell><cell cols="2">C F 1( ↑) IS (↑)</cell><cell>LPIPS div. (↑)</cell></row><row><cell>cLS-GAN</cell><cell>284.5</cell><cell cols="2">0.319 ± 0.005 0.000</cell><cell>1.376 ± 0.009 0.559 ± 0.119</cell></row><row><cell>VQ-VAE2</cell><cell>88.9</cell><cell cols="2">0.096 ± 0.002 0.089</cell><cell>2.149 ± 0.035 0.502 ± 0.061</cell></row><row><cell cols="2">CFG + DDIM 43.7</cell><cell cols="2">0.030 ± 0.002 0.433</cell><cell>6.428 ± 0.115 0.595 ± 0.070</cell></row><row><cell cols="4">3.3 Qualitative Results and User Study</cell></row><row><cell cols="5">Figure 4 displays generated samples for the rarest toolsets of CATARACTS'</cell></row><row><cell cols="3">three most challenging phases.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>experts favouring the gener- ated images 61% of the time</head><label></label><figDesc>and highlights how realistic they appear. Their answers have an average Matthews Correlation Coefficient (MCC) of -0.216, showing low validity of the subjects' binary decisions. Individual MCC and FR scores are given in Table2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results for user study on image realism.</figDesc><table><row><cell cols="2">Clinician NDE1 NDE2 NDE3 DE1</cell><cell>DE2 DE3</cell></row><row><cell>MCC</cell><cell cols="2">-0.961 -0.288 -0.201 -0.233 0.098 0.288</cell></row><row><cell>FR</cell><cell cols="2">49/50 32/50 30/50 31/50 23/50 18/50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Tool-set prediction performance on the CATARACTS test split for different types of data.</figDesc><table><row><cell>Data</cell><cell cols="3">F1 (↑) AUROC (↑) Acc. (↑)</cell></row><row><cell>Original</cell><cell>0.897</cell><cell>0.986</cell><cell>0.9921</cell></row><row><cell>Extended</cell><cell cols="2">0.916 0.989</cell><cell>0.9924</cell></row><row><cell cols="2">Synthetic (CAS) 0.299</cell><cell>0.681</cell><cell>0.9502</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 34.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CATARACTS: challenge on automatic tool annotation for cataRACT surgery</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<title level="m">Demystifying MMD GANs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PixelSNAIL: an improved autoregressive generative model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-dimensional medical image synthesis with denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dorjsembe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Odonchimed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CaDIS: cataract dataset for surgical RGB-image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grammatikopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102053</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-generation and segmentation for generalized surgical instrument segmentation on unlabelled data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Aleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Salcudean</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-139" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part IV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Medical diffusion-denoising diffusion probabilistic models for 3D medical image generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khader</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.03364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A morphology focused diffusion probabilistic model for synthesis of histopathology images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Moghadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Müller-Franzes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.07501</idno>
		<title level="m">Diffusion probabilistic models beat GANs on medical images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">GLIDE: towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08034</idno>
		<title level="m">Generating realistic 3D brain MRIs using a conditional diffusion probabilistic model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating Large Labeled Data Sets for Laparoscopic Image Processing Tasks Using Unpaired Image-to-Image Translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32254-0_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32254-014" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part V</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brain imaging generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-212" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13609</biblScope>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Identification of surgical tools using deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>D-Wave Systems Inc</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving dermatology classifiers across populations using images generated by large diffusion models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Sagers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Adamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13352</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Surgical scene generation and adversarial networks for physics-based iOCT synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sommersperger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2414" to="2430" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A systematic comparison of generative models for medical images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Uzunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Forkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Handels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ehrhardt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-022-02567-6</idno>
		<ptr target="https://doi.org/10.1007/s11548-022-02567-6" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1213" to="1224" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cataract surgical rate and socioeconomics: a global study</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="5872" to="5881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
