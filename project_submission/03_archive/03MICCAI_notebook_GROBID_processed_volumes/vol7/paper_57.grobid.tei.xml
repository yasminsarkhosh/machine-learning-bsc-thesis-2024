<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Polar-Net: A Clinical-Friendly Model for Alzheimer&apos;s Disease Detection in OCTA Images</title>
				<funder ref="#_pQRD5tV">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_GemJMS3">
					<orgName type="full">Youth Innovation Promotion Association CAS</orgName>
				</funder>
				<funder ref="#_hEJXCuR #_wFS73Ve">
					<orgName type="full">National Science Foundation Program of China</orgName>
				</funder>
				<funder ref="#_fPzuYqt">
					<orgName type="full">A*STAR AME Programmatic Funding Scheme</orgName>
				</funder>
				<funder>
					<orgName type="full">A*STAR Central Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shouyue</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Cixi Institute of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Cixi Biomedical Research Institute</orgName>
								<orgName type="institution" key="instit2">Wenzhou Medical University</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinkui</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Cixi Institute of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
							<email>ywxu@ieee.org</email>
							<affiliation key="aff2">
								<orgName type="department">School of Future Technology</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<orgName type="institution">A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Cixi Institute of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yalin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Eye and Vision Science</orgName>
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<settlement>Liverpool</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghuai</forename><surname>Liu</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Edge Hill University</orgName>
								<address>
									<settlement>Ormskirk</settlement>
									<country key="GB">England</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Cixi Institute of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yitian</forename><surname>Zhao</surname></persName>
							<email>yitian.zhao@nimte.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Technology and Engineering</orgName>
								<orgName type="institution" key="instit1">Cixi Institute of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Ningbo Institute of Materials</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Polar-Net: A Clinical-Friendly Model for Alzheimer&apos;s Disease Detection in OCTA Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F6D8477CEF8EBCEC4596CF8E80AB2F16</idno>
					<idno type="DOI">10.1007/978-3-031-43990-257.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>OCTA</term>
					<term>Alzheimer&apos;s Disease</term>
					<term>Polar Transformation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical Coherence Tomography Angiography (OCTA) is a promising tool for detecting Alzheimer's disease (AD) by imaging the retinal microvasculature. Ophthalmologists commonly use region-based analysis, such as the ETDRS grid, to study OCTA image biomarkers and understand the correlation with AD. In this work, we propose a novel deep-learning framework called Polar-Net. Our approach involves mapping OCTA images from Cartesian coordinates to polar coordinates, which allows for the use of approximate sector convolution and enables the implementation of the ETDRS grid-based regional analysis method commonly used in clinical practice. Furthermore, Polar-Net incorporates clinical prior information of each sector region into the training process, which further enhances its performance. Additionally, our framework adapts to acquire the importance of the corresponding retinal region, which helps researchers and clinicians understand the model's decisionmaking process in detecting AD and assess its conformity to clinical observations. Through evaluations on private and public datasets, we have demonstrated that Polar-Net outperforms existing state-of-the-art methods and provides more valuable pathological evidence for the association between retinal vascular changes and AD. In addition, we also show that the two innovative modules introduced in our framework have a significant impact on improving overall performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alzheimer's disease (AD) is a progressive and debilitating neurological disorder that affects millions of people worldwide. Although primary detection of AD can be achieved through a combination of cognitive function tests and neuroimaging techniques, such as magnetic resonance imaging (MRI) and cerebrospinal fluid (CSF) analysis <ref type="bibr" target="#b13">[14]</ref>. However, these approaches suffer from being invasive, timeconsuming, or expensive, hindering their use in routine clinical practice. The convergence of tissue origin, structural characteristics, and functional mechanisms between the eyes and the brain has been previously reported <ref type="bibr" target="#b17">[18]</ref>. For example, patients with AD have significantly decreased blood vessel density in superficial parafoveal and choriocapillaris (CC) <ref type="bibr" target="#b24">[25]</ref>. To this end, the automated AD detection using fundus image has emerged as an active research field in the last two years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Color fundus photography (CFP) has commonly used for AD studies, but the CFP has limitations in capturing the information of deep layer vessels. Optical coherence tomography angiography (OCTA) is an innovative non-invasive technology that generates high-resolution images of depth-resolved retinal microvasculature projections <ref type="bibr" target="#b7">[8]</ref>, including SVC, DVC, and CC.</p><p>Studies on clinical biomarkers of OCTA images are mainly based on regional analysis, e.g., the early treatment of diabetic retinopathy study (ETDRS) grid, which divides a target area into 9 regions with three concentric circles and two orthogonal lines, as shown in the right three sub-figures in Fig. <ref type="figure" target="#fig_0">1</ref>. The regionbased analysis allows a more specific evaluation of retinal changes and their correlation with AD, which can provide a more nuanced understanding of the disease. Research following ETDRS and IE grid demonstrated the significance of many regions, e.g., in the three sub-regions of nasal-outer, superior-inner, and inferior-inner in inner vascular complexes, which present a substantial decrease in vascular area density and vascular length density for the AD participants <ref type="bibr" target="#b22">[23]</ref>.</p><p>Over the past few years, deep-learning-based algorithms have achieved remarkable success in the analysis of medical images. As for AD detection, several methods use an integration of multiple modalities <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. However, these methods rarely follow the clinical region-based analysis routine, which limits their ability to incorporate valuable clinical statistical findings and generate easily interpretable results. To address the above issues, we proposed a novel deep-learning framework to take full advantage of clinical region-based analysis, for AD detection in OCTA images. To obtain a more accurate and interpretable result, we specifically designed an approximate sector convolution, based on the polar transformation and a multi-kernel feature extraction module. The main contributions of the paper can be summarised as follows: (1) Based on the wellknown clinically used ETDRS grids for retinal image analysis, we incorporate the regional importance prior in the training process through a weight matrix, so as to better understand the correlations between retinal structure alterna-  We introduce an approximate sector convolution through polar transformation, to mimic the clinical region-based analysis, by mapping the OCTA image from the Cartesian system to the polar system, as shown in the left two sub-figures in Fig. <ref type="figure" target="#fig_0">1</ref>. (3) We further performed the explainability analysis on the well-trained model. The interpretable results showed consistency with the conclusions of the previous clinical studies, indicating that the proposed method can be a potential tool, to investigate the pathological evidence of the relationship between the fundus and AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the flowchart of our AD detection method using SVC, DVC, and CC projections of OCTA as input. First, we utilize VAFF-Net <ref type="bibr" target="#b5">[6]</ref> to locate the center of the FAZ on SVC. We then transform the original images into the polar coordinates with the FAZ center as the origin. The transformed images are then fed into our Polar-Net, which produces the final detection result and the corresponding region importance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Polar Coordinate Transformation for OCTA Image</head><p>We introduce a method called polar transformation, to realize region-based analysis. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, the polar transformation converts the region of interest (blue circle) into a polar coordinate system Fig. <ref type="figure" target="#fig_2">2(c</ref>), with the center of the FAZ (according to the definition <ref type="bibr" target="#b3">[4]</ref> of ETDRS), O c (u o , v o ) as the origin. The original image is represented as points in the Cartesian system p(u, v), and the corresponding points in the polar system are represented by p (θ, r). The relationship between these two coordinate systems is given by the following equations: </p><formula xml:id="formula_0">u = r cos θ v = r sin θ ⇔ r = √ u 2 + v 2 θ = tan -1 v/u . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The width of the transformed image is equal to the distance R, the minimal length from the center O c (u o , v o ) to the edge in the original image, and the height is 2πR. Since the corners are cropped, the outermost pixels of the region of interest are kept in order to preserve the original information as much as possible, and the part near O c is filled by nearest neighbor interpolation. The polar transformation represents the original image in the polar coordinate system by pixel-wise mapping <ref type="bibr" target="#b2">[3]</ref>, and has the following properties:</p><p>1) Approximate Sector-Shaped Convolution. Convolution is widely used in convolutional neural networks (CNNs), where the shape of the convolution kernel is always rectangular. However, in the real world, many semantics are non-rectangular, such as circle and sector, which makes the adaptability of the receptive field in CNNs suboptimal. For the polar transformation, the mapping relationship is fixed, enabling us to approximate the sector convolution with a rectangular convolution kernel at a lower computational cost. The mapping relationship shown in Fig. <ref type="figure" target="#fig_2">2</ref>(b)(d) explains this, and for the sake of simplicity and clarity, we use ETDRS girds as an example. When we perform convolution with a rectangular kernel along the T I → SI direction on the transformed image, it is equivalent to performing convolution with a sector-shaped kernel counterclockwise around the FAZ center in the original image.</p><p>2) Equivalent Augmentation. Applying data augmentation to the original image is the same as applying data augmentation in the polar system since the transformation is a pixel-wise mapping <ref type="bibr" target="#b2">[3]</ref>. For instance, by changing the start angle and the transformation center O c (u o , v o ), we can realize the drift cropping operation in the polar system. It is analogous to applying various cropping factors for data augmentation by changing the transformation radius R. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>In the transformed image, we can extract features around the FAZ. Rectangular features at different scales correspond to different sectoral features in the retina. Therefore, it is critical to extract information across different sizes of the visual field. To this end, we design the Polar-Net. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, it contains several branches, the number of which varies according to the number of projections.</p><p>Each branch starts with a polar feature extractor module (PFEM) and ends with a residual network. To take full advantage of all the branches, middle fusion is used. To generate the region importance matrix, a polar region importance module (PRIM) is proposed, which follows the residual network. Furthermore, Polar-Net can receive a prior knowledge matrix to utilize clinical knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polar Feature Extractor Module (PFEM):</head><p>To extract shallow features in different views, we propose PFEM, which consists of a multi-kernel atrous convolution module (MKAC), a multi-kernel pooling module (MKPM), and a convolutional block attention module (CBAM) <ref type="bibr" target="#b21">[22]</ref>. For each projection x i , MKAC H (•) applies multiple scale atrous convolutions to enlarge the field of view <ref type="bibr" target="#b23">[24]</ref>, and MKPM G (•) applies a series of max-pooling operations with different pooling kernels to discover microscopic changes, by extracting the most salient feature <ref type="bibr" target="#b8">[9]</ref>. Finally, CBAM T (•) is applied to exploit the inter-channel hidden information of features. During feature extraction, W n is used to adaptively adjust the weights of the above processes. The mathematical notation of the above is:</p><formula xml:id="formula_2">⎧ ⎨ ⎩ F MKAC = LeakyReLU [ n H (x i ) W n ] , F MKPM = LeakyReLU [x i + n G (x i ) W n ] , F PFEM = T (Concat (F MKAC , F MKPM )) .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polar Region Importance Module (PRIM):</head><p>To calculate the region importance, we implement PRIM by applying an average pooling after a Grad-CAM <ref type="bibr" target="#b15">[16]</ref>. In order to capture the importance of feature map k for class c, we denote a c k as the gradient of the score for class c, with respect to feature map activations A k of the last residual layer. The region importance matrix L c RI is given by:</p><formula xml:id="formula_3">L c RI = AvgPool[ReLU( k α c k A k )].<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Data Description: An in-house dataset was conducted for this study. It includes 199 images from 114 AD patients and 566 images from 291 healthy subjects. All data were collected with the approval of the relevant authorities and the consent of the patients, following the Declaration of Helsinki. All the patients conform to the standards of the National Institute on Aging and Alzheimer's Association (NIA-AA). The images were captured with a swept-source OCTA (VG200S, SVision Imaging). The images were captured in a 3 × 3 mm 2 area centered on the fovea. We make sure that the images from a single patient will only be used as training or testing sets once. In the cross-validation experiment subset, we sample the categories from each dataset at the same ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model ACC AUROC Kappa</head><p>ResNet-34 <ref type="bibr" target="#b6">[7]</ref> 0.8125 ± 0.0267 0.7960 ± 0.0479 0.4909 ± 0.0717 EfficientNet-B3 <ref type="bibr" target="#b16">[17]</ref> 0.7942 ± 0.0104 0.7908 ± 0.0157 0.4177 ± 0.0267 ConvNeXt-S <ref type="bibr" target="#b11">[12]</ref> 0.7562 ± 0.0138 0.5903 ± 0.0313 0.1660 ± 0.0437 HorNet-S GF <ref type="bibr" target="#b14">[15]</ref> 0.7602 ± 0.0113 0.5921 ± 0.0286 0.1738 ± 0.0458 VAN-B6 <ref type="bibr" target="#b4">[5]</ref> 0.7707 ± 0.0124 0.6911 ± 0.0298 0.2939 ± 0.0485 ViT-Base <ref type="bibr" target="#b1">[2]</ref> 0.7904 ± 0.0183 0.7726 ± 0.0286 0.3641 ± 0.0715 SwinV2-T <ref type="bibr" target="#b10">[11]</ref> 0.7601 ± 0.0117 0.7528 ± 0.0343 0.3242 ± 0.0448 MUCO-Net <ref type="bibr" target="#b19">[20]</ref> 0 Nvidia RTX 3090 GPUs. We employed Adam as the optimizer with an initial learning rate of 2e-5 and a batch size of 28. We also applied data augmentation by randomly rotating the images by ± 20 • around their centers. The model was trained for 200 epochs. During the transformation, we considered the difference between the left and right eyes and used nearest-neighbor interpolation.</p><p>The width of the transformed images was resized to 224 pixels. Five-fold crossvalidation was employed to fully utilize the data and make the results more reliable. Since there is no standard way to convert existing prior knowledge into matrices, for prior knowledge, we manually generated a 4 × 2 weight matrix according to the study <ref type="bibr" target="#b22">[23]</ref>. The weights were 1 by default. The regions with p-values less than 0.05 had a weight of 1.5, and regions with p-values less than 0.01 had a weight of 2. For the entire DVC, the weight was set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Interpretability Assessment:</head><p>We evaluate the performance of the model on the test set using the accuracy score (ACC), area under the receiver operating characteristic (AUROC), and kappa. To evaluate the performance, we compared our method with several state-of-the-art methods in the computer vision field and one in the AD detection field. Table <ref type="table" target="#tab_0">1</ref> shows that our method outperforms the others in ACC, AUROC, and Kappa, with an improvement of up to 4.07%, 5.63%, and 9.08%, respectively. Prior knowledge did not bring much performance improvement, partly due to the crude method of generating the prior matrices, and partly probably due to the fact that the network's adaptive algorithm may have already learned similar prior knowledge. During the testing phase, we activated PRIM and generated a 4 × 2 importance matrix for the entire testing set. An inverse operation of the polar transformation was applied to generate the importance map.</p><p>For the sake of simplicity, here we take the EDTRS grid for analysis. As shown in Fig. <ref type="figure" target="#fig_4">4</ref> (a), it can be seen that globally, the importance of CC is highest and DVC is the second. This matches the findings that AD patients have a considerably lower density in choriocapillaris flow <ref type="bibr" target="#b24">[25]</ref>. Meanwhile, the significance of DVC coincides with research findings that there is a considerable reduction in vascular area density and other factors in DVC <ref type="bibr" target="#b22">[23]</ref>. In the DVC and CC, the parafovea is more important. This may relate to the loss of ganglion cells in the parafoveal retina <ref type="bibr" target="#b18">[19]</ref>. For single projection (Fig. <ref type="figure" target="#fig_4">4 (b)</ref>), different regions have different importance and the contributions of NI, SI and II (illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>) are higher. In summary, we found a pattern that high importance always occurs where there are more micro-vessels, such as the CC, DVC, and the parafovea. This finding coincides with the conclusion that the microvasculature of the brain and retina is significantly decreased in AD patients <ref type="bibr" target="#b24">[25]</ref>. Our interpretable results roughly match the clinical study results, because we have made the network follow the clinical analysis method. This also proves the clinical relevance of ours. The minor difference is perhaps because the network unearthed the high-dimensional features that have not yet been discovered clinically.</p><p>Ablation Study: To evaluate the effectiveness of the polar transformation and Polar-Net, we performed an ablation study. To validate the proposed Polar-Net, we removed the PFEM. The results are shown at the bottom of Table <ref type="table" target="#tab_0">1</ref>. To  ViT-Base <ref type="bibr" target="#b1">[2]</ref> w/o trans 0.7904 ± 0.0183 0.7726 ± 0.0286 0.3641 ± 0.0715 w trans 0.7982 ± 0.0177 0.8025 ± 0.0509 0.4235 ± 0.0748 MUCO-Net <ref type="bibr" target="#b19">[20]</ref> w/o trans 0.7968 ± 0.0369 0.7773 ± 0.0414 0.3985 ± 0.0789 w trans 0.7916 ± 0.0255 0.7956 ± 0.0500 0.4271 ± 0.0659 validate the transformation, we used the transformed images and the original images respectively. The results are shown in Table <ref type="table" target="#tab_2">2</ref>. All the results showed the effectiveness of the proposed components and modules.</p><p>Extended Experiment: To further verify our detection method's stability and generalisability, we conducted an additional experiment on a public dataset OCTA-500 <ref type="bibr" target="#b9">[10]</ref>. It contains 189 images from 29 subjects with diabetic retinopathy and 160 healthy control. The details of the implementation are the same as the experiments on the in-house dataset. As shown in Table <ref type="table" target="#tab_3">3</ref>, our method achieved the best performances compared to the competitors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel framework for AD detection using retinal OCTA images, leveraging clinical prior knowledge and providing interpretable results. Our approach involves polar transformation, allowing for the use of approximate sector convolution and enabling the implementation of the regionbased analysis. Additionally, our framework, called Polar-Net, is designed to acquire the importance of the corresponding retinal region, facilitating the understanding of the model's decision-making process in detecting AD and assessing its conformity to clinical observations. We evaluate the performance of our method on both private and public datasets, and the results demonstrate that Polar-Net outperforms state-of-the-art methods. Importantly, our approach produces clinically interpretable results, providing a potential tool for disease research to investigate the underlying pathological mechanisms. Our work presents a promising approach to using OCTA imaging for AD detection. Furthermore, we highlight the importance of incorporating clinical knowledge into AI models to improve interpretability and clinical applicability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed AD detection model utilizes the polar transformation (left two) inspired by ETDRS grids commonly used in ophthalmic analysis. This model allows for easy understanding by ophthalmologists, as the output of the Polar-Net can be interpreted through an intuitive color map illustration (right) that indicates different levels of significance.</figDesc><graphic coords="3,58,98,53,72,334,51,59,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>tions and AD. (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustrations of the Cartesian coordinate system (a), an ETDRS grid on an OCTA projection (b), the polar coordinate system (c), and the mapping relationship after the transformation (d). Definition: temporal-inner (TI), temporal-external (TE), superior-inner (SI), superior-external (SE), nasal-inner (NI), nasal-external (NE), inferior-inner (II), and inferior-external (IE).</figDesc><graphic coords="4,73,80,53,87,276,25,75,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The details of Polar-Net. It contains multiple input branches, and each branch starts with a polar feature extractor module (PFEM) and ends with a residual network. PFEM consists of a multi-kernel atrous convolution module (MKAC), a multikernel pooling module (MKPM), and a convolutional block attention module (CBAM). Parameters K and D denote the kernel size and dilation respectively.</figDesc><graphic coords="5,58,98,53,84,334,51,134,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Importance maps according to ETDRS, IE, and hemispheric grids. Group (a) takes into account the differences in importance between the projections, while (b) reflects the relative importance of regions within a single projection. The redder the color, the greater the importance. (c) shows a set of examples using the Grad-CAM visualization method, based on ResNet-34 in AD detection.</figDesc><graphic coords="8,44,79,53,72,334,51,150,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The detection results (mean ± std) over the in-house dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The ablation results (mean ± std) of the polar transformation over the inhouse dataset.</figDesc><table><row><cell>Model</cell><cell>Input</cell><cell>ACC</cell><cell>AUROC</cell><cell>Kappa</cell></row><row><cell>ResNet-34 [7]</cell><cell cols="2">w/o trans 0.8125 ± 0.0267</cell><cell>0.7960 ± 0.0479</cell><cell>0.4909 ± 0.0717</cell></row><row><cell></cell><cell cols="4">w trans 0.8244 ± 0.0226 0.8471 ± 0.0279 0.5137 ± 0.0417</cell></row><row><cell cols="3">EfficientNet-B3 [17] w/o trans 0.7942 ± 0.0104</cell><cell>0.7908 ± 0.0157</cell><cell>0.4177 ± 0.0267</cell></row><row><cell></cell><cell cols="4">w trans 0.8335 ± 0.0157 0.8295 ± 0.0279 0.5287 ± 0.0600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The classification results (mean ± std) of different methods over the OCTA-500 dataset.</figDesc><table><row><cell>Model</cell><cell>ACC</cell><cell>AUROC</cell><cell>Kappa</cell></row><row><cell>ResNet-34 [7]</cell><cell>0.9641 ± 0.0389</cell><cell>0.9818 ± 0.0306</cell><cell>0.8412 ± 0.1757</cell></row><row><cell cols="2">EfficientNet-B3 [17] 0.9632 ± 0.0225</cell><cell>0.9741 ± 0.0245</cell><cell>0.8375 ± 0.1006</cell></row><row><cell>VAN-B6 [5]</cell><cell>0.9478 ± 0.0245</cell><cell>0.9517 ± 0.0300</cell><cell>0.7691 ± 0.1334</cell></row><row><cell>ViT-Base [2]</cell><cell>0.9692 ± 0.0281</cell><cell>0.9768 ± 0.0247</cell><cell>0.8694 ± 0.1199</cell></row><row><cell>MUCO-Net [20]</cell><cell>0.9529 ± 0.0204</cell><cell>0.9717 ± 0.0222</cell><cell>0.8086 ± 0.0885</cell></row><row><cell>Polar-Net</cell><cell cols="3">0.9898 ± 0.0140 0.9949 ± 0.0072 0.9604 ± 0.0544</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by the <rs type="funder">National Science Foundation Program of China</rs> (<rs type="grantNumber">62272444</rs>, <rs type="grantNumber">62103398</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (<rs type="grantNumber">LR22F020008</rs>), the <rs type="funder">Youth Innovation Promotion Association CAS</rs> (<rs type="grantNumber">2021298</rs>), the <rs type="funder">A*STAR AME Programmatic Funding Scheme</rs> Under Project <rs type="grantNumber">A20H4b0141</rs>, and <rs type="funder">A*STAR Central Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hEJXCuR">
					<idno type="grant-number">62272444</idno>
				</org>
				<org type="funding" xml:id="_wFS73Ve">
					<idno type="grant-number">62103398</idno>
				</org>
				<org type="funding" xml:id="_pQRD5tV">
					<idno type="grant-number">LR22F020008</idno>
				</org>
				<org type="funding" xml:id="_GemJMS3">
					<idno type="grant-number">2021298</idno>
				</org>
				<org type="funding" xml:id="_fPzuYqt">
					<idno type="grant-number">A20H4b0141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A deep learning model for detection of Alzheimer&apos;s disease based on retinal photographs: a retrospective, multicentre case-control study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="806" to="e815" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint optic disc and cup segmentation based on multi-label deep network and polar transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1597" to="1605" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Early photocoagulation for diabetic retinopathy: ETDRS report number 9</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="766" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retinal structure detection in octa image via voting-based multitask learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3969" to="3980" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Determining degenerative from vascular dementia using optical coherence tomography biomarkers for tomography and angiography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multiple Sclerosis</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="002" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NormAttention-PSN: a highfrequency region enhanced photometric stereo network with normalized attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3014" to="3034" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">IPN-V2 and OCTA-500: methodology and dataset for retinal image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07261</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Swin transformer v2: scaling up capacity and resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12009" to="12019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ROSE: a retinal oct-angiography vessel segmentation dataset and new model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="928" to="939" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cognitive impairment in older adults: epidemiology, diagnosis, and treatment</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychiatr. Clin</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HorNet: efficient highorder spatial interactions with recursive gated convolutions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cerebral and retinal neurovascular changes: a biomarker for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V R</forename><surname>Teja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Berendschot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steinbusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Webers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathuranath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gerontol. Geriatr. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Posterior pole analysis and ganglion cell layer measurements in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Un</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alpaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonmez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hosp. Pract</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="282" to="288" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Screening of dementia on octa images via multi-projection consistency and complementarity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-766" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MIC-CAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural network to identify symptomatic Alzheimer&apos;s disease using multimodal retinal imaging</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Wisely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="388" to="395" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
		<idno>978- 3-030-01234-2 1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep segmentation of octa for evaluation and association of changes of retinal microvasculature with Alzheimer&apos;s disease and mild cognitive impairment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1136/bjo-2022-321399</idno>
		<ptr target="https://doi.org/10.1136/bjo-2022-321399" />
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RecepNet: network with large receptive field for real-time semantic segmentation and application for blue-green algae</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">5315</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Choriocapillaris changes are correlated with disease duration and MoCA score in early-onset dementia</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Aging Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">656750</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
