<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Magdalena</forename><surname>Bachmaier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthcare GmbH</orgName>
								<address>
									<settlement>Forchheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Rohleder</surname></persName>
							<idno type="ORCID">0000-0002-1758-9056</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthcare GmbH</orgName>
								<address>
									<settlement>Forchheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benedict</forename><surname>Swartman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department for Trauma and Orthopaedic Surgery</orgName>
								<orgName type="department" key="dep2">BG Trauma Center Ludwigshafen</orgName>
								<address>
									<settlement>Ludwigshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maxim</forename><surname>Privalov</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department for Trauma and Orthopaedic Surgery</orgName>
								<orgName type="department" key="dep2">BG Trauma Center Ludwigshafen</orgName>
								<address>
									<settlement>Ludwigshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
							<idno type="ORCID">0000-0002-9550-5284</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Holger</forename><surname>Kunze</surname></persName>
							<email>holger.hk.kunze@siemens-healthineers.com</email>
							<idno type="ORCID">0000-0002-7021-2370</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Healthcare GmbH</orgName>
								<address>
									<settlement>Forchheim</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="446" to="455"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">AB0396343D04F7A6EF87C3B9E8099841</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Image Rotation</term>
					<term>Hough Transform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standardized image rotation is essential to improve reading performance in interventional X-ray imaging. To minimize user interaction and streamline the 2D imaging workflow, we present a new automated image rotation method. Image rotation can follow two steps: First, an anatomy specific centerline image is predicted which depicts the desired anatomical axis to be aligned vertically after rotation. In a second step, the necessary rotation angle is calculated from the orientation of the predicted line image. We propose an end-to-end trainable model with the Hough transform (HT) and a differentiable spatial-to-angular transform (DSAT) embedded as known operators. This model allows to robustly regress a rotation angle while maintaining an explainable inner structure and allows to be trained with both a centerline segmentation and angle regression loss. The proposed method is compared to a Hu moments-based method on anterior-posterior X-ray images of spine, knee, and wrist. For the wrist images, the HT based method reduces the mean absolute angular error (MAE) from 9.28 • using the Hu momentsbased method to 3.54 • . Similar results for the spinal and knee images can be reported. Furthermore, a large improvement of the 90 th percentile of absolute angular error by a factor of 3 indicates a better robustness and reduction of outliers for the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intra-operative X-ray imaging supports the assessment of fracture reduction and implant positioning, which significantly increases surgical precision <ref type="bibr" target="#b6">[7]</ref> and the overall outcome <ref type="bibr" target="#b0">[1]</ref>. Therefore, standardized image rotation is essential to improve reading performance and interpretation. While in diagnostic imaging, this can be reached by careful alignment of the imaging system with the patient, in intra-operative setups, this alignment is hard to achieve due to mechanical constraints of the widely used C-arm devices and limited space in the operating room. The rotation of the images must then be corrected in a post-processing step <ref type="bibr" target="#b11">[11]</ref>. Therefore, a common task in the radiography workflow is to manually rotate digital X-ray images to a preferred orientation suitable for diagnostic reading. However, during a surgery, user interaction of the surgeon with the system must be minimized due to sterility considerations. Thus, even improved user interaction with the system does not improve the situation for the surgeon much. Supporting staff is often not well trained in operating the vendorspecific systems and changes job positions frequently. Therefore, the speed and quality of the image alignment depends on the experience of the operator and leaves room for mistakes. An automatic X-ray alignment system can help ensure proper alignment resulting in improved reading performance and interpretation also <ref type="bibr" target="#b1">[2]</ref>. Previous attempts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref> to automate this rotation typically only allow the identification of a limited set of orientations. In addition, they are often specially designed for certain examinations. More recent publications <ref type="bibr" target="#b3">[4]</ref> work on the regression of the correct angular offset. Conventional vision algorithms based on feature extraction with subsequent feature registration to an atlas, typically fail to automate the image rotation of intra-operative images due to the enormous variety of images with different collimation settings, fracture types, and instruments/casts that obscure many image features. Additionally, they hardly generalize to a large number of body regions due to the nature of the hand crafted image features. In this paper, we extend the indirect regression approach of Kunze et al. <ref type="bibr" target="#b11">[11]</ref>. It bases on the insight that for many anatomical structures a line can be defined that strongly corresponds with the upright position of the X-ray image. After this line has been determined as a heatmap by a Convolutional neural network (CNN) according to the idea of Kordon et al. <ref type="bibr" target="#b7">[8]</ref> this heatmap can be post-processed to calculate the orientation of the line. Kunze et al. <ref type="bibr" target="#b11">[11]</ref> have demonstrated that this indirect approach is more robust than the direct regression approaches for angle regression <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> based on ResNet or PoseNet. In applications with different crops of anatomic regions, Kordon et al. <ref type="bibr" target="#b10">[10]</ref> have shown that the quality of the heatmap can be improved if a companion objective function is integrated into the training that optimizes the regression target directly. Therefore, the derivable calculation of the position and direction based on the Hu moments (HM) was added to the cost function of the segmentation task, enabling end-to-end gradient flow. This approach can be also adapted for the angle regression. However, the HM are an indirect measure leaving much room for the algorithm to shape the heatmap in an uncontrolled manner. Therefore, to limit the space of solutions, we propose to substitue the HM based angle regression with a Hough transform (HT) based one. By doing so, the HM based cost term is replaced by a term, that favors straight, narrow lines. Thereto, we present an algorithm to implement the computation of the line angle based on the HT differentiable to keep end-to-end training based on angular values.</p><p>The main contribution of the paper can be summarized as follows:</p><p>-proposition of a derivable calculation of the image rotation angle derived from the HT heatmap in the manner of a differentiable spatial to numerical transform (DSNT) <ref type="bibr" target="#b16">[16]</ref> -ablation study which examines the properties of the proposed method and shows that the proposed method increases the robustness of the image rotation compared to state-of-the-art indicated by the 90 th percentile of the absolute angular error. -verification of the approach on a the body regions spine, wrist, and knee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Determination of Rotation Angle</head><p>In Kunze et al. <ref type="bibr" target="#b11">[11]</ref>, two classes of algorithms are compared with which the rotation angle of images can be derived: The direct regression method, uses an encoder structure determining festures of the image followed by a regression network converting them into an angular valued, represented by its sine and cosine value for stability reasons. The indirect method computes a heatmap of a line which represents the upwards direction of the image. Analysing the direction of the line returns the rotation angle of the image. While the first approach consisting of a modified version of ResNet-34 with the fully connected (FC) layer being substituted by two FC layers serves as baseline algorithm <ref type="bibr" target="#b11">[11]</ref>, in the following the angle determination of the indirect approch is revised.</p><p>Angle Regression Exploiting the Hough Transform. As pointed out in the introduction, the training with a cost function based on the HM does not enforce a narrow destinct line, but any structure with a defined main axis contributes to a low cost. A cost function which enforces this behaviour would potentially more distinctive heatmap.</p><p>A common approach to detect lines in pattern recognition applications is the HT: Given a line L ρ,θ , with ρ the angle between line and the x-axis and θ its distance from the origin, its corresponding pixels {x ρ,θ,i , y ρ,θ,i } vote in the Hough space for the closest bin (ρ, θ). In contrast to the HM based direction calculation, the maximum of the HT is only obtained for a straight line of given extent. The extent can be controlled by the bin width of the HT. This feature of the HT enables the training of a narrow structure with a defined width punishing wide structures. However, the HT does not return an angle itself but returns a new 2-dimensional representation of the heatmap, with the line to be sought represented as its maximum. When this representation shall be integrated in an end-to-end trainable network, a differentiable calculation of the position of this maximum is needed. Thereto, we follow the idea of the DSNT layer introduced by Nibali et al. <ref type="bibr" target="#b16">[16]</ref> that calculates the channel-wise centroid of the input. For the current scenario, merely the centroid θ of the θ coordinate indicating the direction of the line is of interest. The θ can be written as</p><formula xml:id="formula_0">θ = θ ρ θg(θ, ρ) θ ρ g(θ, ρ) = θ θ ρ g(θ, ρ) θ ρ g(θ, ρ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Equation ( <ref type="formula" target="#formula_0">1</ref>) involves a summation over ρ coordinate. Since the zeroth-order Helgason-Ludwig condition of the Radon transform <ref type="bibr" target="#b13">[13]</ref> states that the sum over each projection is constant, also the Hough histogram yields a constant first-order moment. This renders the direct implementation incapable for the angle computation.</p><p>To overcome this shortcoming, A non-linear function needs to be applied to the Hough histogram to assign high pixel values a greater weight than small ones. Keeping pixel values between 0 and 1 while weighting maximum values and suppressing low values, a steep sigmoid function is employed, which maps input values to a range from 0 to 1. At best, only values from roughly 0.8 to 1 should be used. The steeper the sigmoid function is, the more it approximates the step function, which ideally assigns a weight of 1 to values of 0.8 and higher.</p><p>For the end-to-end training, the same issue holds as for the direct regression: the angular value θ has a discontinuity for θ = -90 • and θ = 90 • . Both θ values represent the same line. To pass this boundy problem, the cosine-and sine-value are computed instead of the angular value itself. This can be incorporated in Eq. ( <ref type="formula" target="#formula_0">1</ref>) by using a sine-and cosine-weighting instead of linear one. So the total angular value calculation given the HT can be written as</p><formula xml:id="formula_2">(cos( θ), sin( θ)) = θ cosθ ρ ĝ(θ, ρ) θ ρ ĝ(θ, ρ) , θ sinθ ρ ĝ(θ, ρ) θ ρ ĝ(θ, ρ) (2) with ĝ(θ, ρ) = sigmoid (α • g(θ, ρ) -β) ,<label>(3)</label></formula><p>where α = 20 and β = 17 were determined heuristically. θ can be calculated by the atan2 function from its sine and cosine value. We call the layer, that implements Eq. ( <ref type="formula">2</ref>) in combination with the atan2 method differential spatial to angular transform (DSAT).</p><p>Neither the HT layer parameters nor the DSAT layer have parameters, that need to be trained: The HT can be written as a multiplication of a vector representing the values of the image with a sparse matrix mapping the input image into a [N ρ × N θ ] Hough histogram Y , where N ρ and N θ are the numbers of discrete offsets and angles <ref type="bibr" target="#b12">[12]</ref>. For the DSAT layer, Eq. 1 can be implemented. So both parts of the algorithms can be treated as known operators <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>For the experiments, three different datasets containing the spine, the knee and the wrist, were used. The spinal dataset consists of 958 images derived from 148 patients, where 289 images represent the cervical, 150 the lumbar, and 519 the thoracic region. The wrist dataset contains 257 and the knee dataset 113 images. The 16-bit, gray-scale X-ray images were selected randomly retrospectively from anonymized databases which were acquired using the Cios Spin mobile C-arm system of Siemens Healthcare GmbH during orthopedic surgeries. The images are depicted in the AP view and may contain screws, plates, and other surgical tools. In these images, the centerline was marked using the labelme tool <ref type="bibr" target="#b19">[19]</ref> by a trained medical engineer. During annotation, special care has been taken to ensure that the rotation angle of the image can be calculated as the angle between the line and the y-axis by consistently drawing the orientation of the line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Protocol, Experiments, and Evaluation</head><p>To identify the effect of the post processing method on the angle regression error, an ablation study was set up. Following Kunze et al. <ref type="bibr" target="#b11">[11]</ref>, for the indirect (segmentation-based) method, a D-LinkNet <ref type="bibr" target="#b20">[20]</ref> was chosen as underlying network architecture. The output of the D-LinkNet, which is the heatmap of the line, is fed into the HT layer and its result is processed by the DSAT layer. As loss function for the training, the weighted sum of the segmentation loss and the regression loss was chosen. The segmentation loss consists of the sum of Binary Cross Entropy and Mean Squared Error(MSE). For the regression loss, the MSE of the regressed rotation's sine-and cosine-values was selected. Thus, the total loss can be defined as</p><formula xml:id="formula_3">L = λ • L seg + (1 -λ) • L reg ,<label>(4)</label></formula><p>where λ ∈ R is a multiplicative weighting term. Using this we examined the following 3 cases:</p><p>1. For λ = 1, the training corresponds to one without the regression loss. In this case, the performance of the HM based and the HT based angle regression methods can be compared on the heatmap. 2. With a constant weighting term λ = 0.5, the regression loss is considered along with the segmentation loss during the training. 3. When decreasing lambda over the epochs from 1 to 0, we obtain a training which is kick-started using the segmentation loss only. But in the end, only the regression loss is used for the training.</p><p>For the parameter selection of the HT layer, a grid search was performed on synthetic line heatmaps, resulting in discretization values dθ = 0.4 • and dρ = 1 8px.</p><p>For comparison reasons, the direct regression method based on a ResNet-34 was trained using the MSE based on the sine-and cosine-vales as cost function.</p><p>During all trainings, online augmentation for rotation (α ∈ [-45 • , 45 • ], p = 0.5), inverting (p = 0.5), shifting (s ∈ [-40px, 40px], p = 0.5) and contrast enhancement was applied. After the augmentation, the images were scaled to the dimension [H:256×W:256] px, using zero-padding the image to preserve the ratio. Thereafter, sample-wise data normalization using z-scoring was used and combined with batch normalization <ref type="bibr">[9]</ref>. The heatmaps for segmentation training were created by placing a Gaussian function of width 7px in a neighborhood of 21px on the line connecting the augmented start and end point of the center line. The Gaussian was scaled such that the center point obtains a value of 1 <ref type="bibr" target="#b11">[11]</ref>.</p><p>For the evaluation, different matrices based on the angular error were used. The mean error (ME) reveals a directional offset of the algorithm. Further on, the mean absolute error (MSE) and the standard deviation of the absolute error (Std.) are reported. Practically more relevant are the percentiles P 50 , P 90 and P 95 based on the absolute error as they reveal to what extent manual rotation corrections are required after the automatic correction was performed. Based on the MAE a Wilcoxon signed-rank test was performed to verify the significance of the observed differences.</p><p>To evaluate the performance of the presented methods, a 5-fold crossvalidation scheme is employed. The mean performance score across all folds served as final result of the evaluated algorithm. Training was performed using the Adam optimizer for 500 epochs with a learning rate of l = 0.001 divided by two every 50 epochs. The batch size was selected to be 8. The weights of the segmentation network as well as of the direct regression model were initialized by He's method <ref type="bibr" target="#b4">[5]</ref>. Implementation was done in PyTorch v1.11 (Python v3.9.12, CUDA v11.0), training was performed on a Windows 10 system with 64 GB RAM and 24 GB NVIDIA Titan RTX. Reproducibility was confirmed by repeated trainings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results of the experiments are shown in Table <ref type="table" target="#tab_0">1</ref>. A comparison of the segmentation results is made in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The results show for all three anatomies, that the HT based methods are superior to the HM based method. So already as post-processing method, it significantly outperforms the HM based method (spine: p = 1.0 × 10 -95 , wrist: p = 5.6 × 10 -21 , knee: p = 6.7 × 10 -8 ). Further improvements of the angle regression results can be realized by incorporating the regression loss in the training (spine: p = 9.5 × 10 -91 , wrist: p = 5.7 × 10 -21 , knee: p = 4.5 × 10 -9 ). Best results except for wrist are obtained when the final training is performed on the regression loss only (spine: p = 1.3 × 10 -2 , wrist: p = 7.7 × 10 -1 , knee: p = 2.1 × 10 -2 ). For the wrist adding the regression loss to the overall loss does not improve the training. The comparable P 50 value for most of the anatomies shows, that HT based and HM based angle regression both work well for standard images. For all anatomies, an improvement in the 90 th percentile of the absolute angular error can be observed. While for the spine the improvement is about 17%, for the wrist and knee, this value an reduction by a factor of 4.6 and 3.1, respectively, can be observed.</p><p>A case-by-case analysis shows that on images for which the centerline is well determined, the HT analysis returns roughly the same result as the HM analysis. However, since the dataset also contains many challenging cases, e.g., images with cropped anatomical structures of interest or images containing metal implants, the centerline cannot always be well determined by the segmentation network. Then occasionally, the heatmap generates short and fragmented estimates of the centerline. For these images, the error of the regressed angle by the HT based method typically is thinner compared to that by the HM computed one (Fig. <ref type="figure" target="#fig_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We introduced a new HT based method to calculate the rotation of straight heatmaps. For the implementation, special care needs to be taken to cope with constant first-order moments of the HT and boundary problems at θ = ±90 • . Doing so, the proposed method has similar performance to HM for X-ray images of good quality. In more difficult cases, when the segmentation algorithm returns multiple and only short segments, like for the wrist or knee, the proposed algorithm is more robust than the HM based one.</p><p>The largest benefit of the HT based image rotation calculation can be achieved with its embedding into the cost function of a segmentation network. Then it helps to improve the accuracy compared to a segmentation-only approach. Especially outliers can be reduced resulting in a lower MAE and standard deviation of the error. The presented results confirm the work of Kordon et al. <ref type="bibr" target="#b10">[10]</ref> that direct optimization of the target measure in addition to a proxy objective is beneficial to the underlying heatmap-generating problem. That is especially true for body regions like the wrist or knee. For these regions, the lengths of the structures and the annotated lines defining the orientation of the image vary. Then, the algorithm can benefit from a more generic description. For the spine, a line from the top to the bottom of the image can be drawn. Therefore, the regression-related term provides less additional information during the training.</p><p>The inspection of the generated heatmaps reveals that the HT based approach can enforce more pronounced, thinner heatmaps compared to the HM -as desired. This observation can be explained by the property of the HT based cost function: it penalizes structures that are not oriented along the desired direction. Only thin straight heatmaps along the direction have a positive effect on the loss term. Structures that are parallel, like the centerlines of the radius and ulna for the wrist images, are penalized. In contrast to that, the HM derived cost term does not punish broad heatmaps as long as the largest main axis points toward the correct direction. Thus, this regression term does not enforce one single, thin line. Also, clusters are tolerated as the results depict. That explains the reduced number of outliers for the HT bases method compared to the HM based, indicated by P 90 and P 95 values. The fact that the adaptation of the parameter λ during the training has only a small effect can be explained by the regression loss being larger compared to the segmentation loss by magnitudes as soon as a certain segmentation level is reached. So the segmentation loss only at the beginning of the training has a distinct influence on the training. Thus, the training process is adapting the influence of the segmentation loss by itself.</p><p>Finally, the results confirm the finding of Kunze et al. <ref type="bibr" target="#b11">[11]</ref> that the segmentation-based rotation regression is more robust compared to the direct regression by a ResNet-34, at the cost of the uncertainty of an up-down flip. For the wrist, slightly worse results are reported compared to <ref type="bibr" target="#b11">[11]</ref>. That can be attributed to the dataset for this study containing only intra-operative X-ray images which are acquired not as standardized as the diagnostic images used in <ref type="bibr" target="#b11">[11]</ref>.</p><p>Data Use Declaration: The data was obtained retrospectively from anonymized databases and not generated intentionally for the study. The acquisition of data from patients had a medical indication. Informed consent was not required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Regression results for different calculations of the image rotation based on the Hough transform (HT). Left: Linear central moment on HT, Middle: Linear central moment on sigmoid activated HT, Right: Sine-Cosine weighted central moment on sigmoid activated HT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of the segmentation results for upper ankle, wrist and spine (top to bottom) trained with segmentation loss only, Hu moments, Hough loss, Hu moments with loss adaption, and Hough loss with loss adaption (column 3-6) with corresponding input images (1st column) and ground truth heatmaps (2nd column).</figDesc><graphic coords="8,57,48,54,65,337,42,145,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Angular error in degree for the different network topologies and angle calculation methods. The direct regression uses the values obtained by the fully connected layers, while the Hu moments and Hough transform-based methods analyze the detected line at the heatmap output and are integrated as post-processing step as well as additional layers.</figDesc><table><row><cell cols="2">Anatomy Model</cell><cell>ME</cell><cell>MAE Std. P50 P90</cell><cell>P95</cell></row><row><cell>spine</cell><cell>ResNet-34</cell><cell>0.07</cell><cell cols="2">6.95 6.32 5.37 14.54 18.95</cell></row><row><cell></cell><cell>Seg loss, HM post-proc</cell><cell cols="3">-0.17 2.14 3.23 1.19 4.87 7.22</cell></row><row><cell></cell><cell>Seg loss, HT post-proc</cell><cell>0.13</cell><cell cols="2">1.80 2.36 1.08 3.92 5.69</cell></row><row><cell></cell><cell>Seg loss &amp; HM loss</cell><cell cols="3">-0.01 2.05 2.91 1.28 4.33 6.08</cell></row><row><cell></cell><cell>Seg loss &amp; HT loss</cell><cell>0.10</cell><cell cols="2">1.79 2.50 1.13 3.61 5.21</cell></row><row><cell></cell><cell cols="4">adapt. Seg loss &amp; HM loss -0.08 1.98 3.50 1.24 4.31 5.97</cell></row><row><cell></cell><cell cols="2">adapt. Seg loss &amp; HT loss 0.15</cell><cell cols="2">1.61 2.19 1.05 3.40 4.72</cell></row><row><cell>wrist</cell><cell>ResNet-34</cell><cell>0.32</cell><cell cols="2">7.31 5.12 6.25 14.43 17.28</cell></row><row><cell></cell><cell>Seg loss, HM post-proc</cell><cell>2.26</cell><cell cols="2">9.28 16.02 3.13 26.51 43.80</cell></row><row><cell></cell><cell>Seg loss, HT post-proc</cell><cell>0.39</cell><cell cols="2">3.73 7.72 1.97 7.63 10.01</cell></row><row><cell></cell><cell>Seg loss &amp; HM loss</cell><cell>4.40</cell><cell cols="2">11.24 16.93 3.80 31.13 48.33</cell></row><row><cell></cell><cell>Seg loss &amp; HT loss</cell><cell>0.23</cell><cell cols="2">3.54 6.60 2.07 6.75 11.46</cell></row><row><cell></cell><cell cols="2">adapt. Seg loss &amp; HM loss 3.24</cell><cell cols="2">9.93 16.75 3.66 24.81 49.09</cell></row><row><cell></cell><cell cols="2">adapt. Seg loss &amp; HT loss 0.63</cell><cell cols="2">4.80 11.67 2.05 6.97 11.47</cell></row><row><cell>knee</cell><cell>ResNet-34</cell><cell>0.22</cell><cell cols="2">5.63 4.97 4.01 12.48 14.27</cell></row><row><cell></cell><cell>Seg loss, HM post-proc</cell><cell>4.99</cell><cell cols="2">8.77 9.18 5.71 19.53 29.97</cell></row><row><cell></cell><cell>Seg loss, HT post-proc</cell><cell>0.15</cell><cell cols="2">4.43 11.83 1.68 7.16 10.73</cell></row><row><cell></cell><cell>Seg loss &amp; HM loss</cell><cell>6.00</cell><cell cols="2">10.00 15.27 4.77 24.57 40.45</cell></row><row><cell></cell><cell>Seg loss &amp; HT loss</cell><cell>0.44</cell><cell cols="2">3.71 4.39 2.23 7.76 11.13</cell></row><row><cell></cell><cell cols="2">adapt. Seg loss &amp; HM loss 0.49</cell><cell cols="2">8.31 13.14 3.68 16.72 32.62</cell></row><row><cell></cell><cell cols="2">adapt. Seg loss &amp; HT loss 1.05</cell><cell cols="2">3.48 8.07 1.71 6.29 8.34</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The use of intraoperative three-dimensional imaging (ISO-C-3D) in fixation of intraarticular fractures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Atesok</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.injury.2007.06.014</idno>
		<ptr target="https://doi.org/10.1016/j.injury.2007.06.014" />
	</analytic>
	<monogr>
		<title level="j">Injury</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1163" to="1169" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Orientation regression in hand radiographs: a transfer learning approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Baltruschat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saalbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jockel</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2291620</idno>
		<ptr target="https://doi.org/10.1117/12.2291620" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of SPIE Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page" from="473" to="480" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition of chest radiograph orientation for picture archiving and communications systems display using neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Boone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Steiner</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF03167769</idno>
		<ptr target="https://doi.org/10.1007/BF03167769" />
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="193" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic orientation identification of pediatric chest x-rays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Freire Sobrinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Soares</surname></persName>
		</author>
		<idno type="DOI">10.1109/COMPSAC48688.2020.00-51</idno>
		<ptr target="https://doi.org/10.1109/COMPSAC48688.2020.00-51" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1449" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward automatic c-arm positioning for standard projections in orthopedic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kausch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-020-02204-0</idno>
		<ptr target="https://doi.org/10.1007/s11548-020-02204-0" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1095" to="1105" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Intra-operative imaging in trauma surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Swartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Grützner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franke</surname></persName>
		</author>
		<idno type="DOI">10.1302/2058-5241.3.170074</idno>
		<ptr target="https://doi.org/10.1302/2058-5241.3.170074" />
	</analytic>
	<monogr>
		<title level="j">EFORT Open Rev</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="541" to="549" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task localization and segmentation for x-ray guided planning in knee surgery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kordon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-769" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="622" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved x-ray bone segmentation by normalization and augmentation strategies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bildverarbeitung für die Medizin</title>
		<imprint>
			<biblScope unit="page" from="104" to="109" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wiesbaden</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-658-25326-4_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-658-25326-424" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep geometric supervision improves spatial generalization in orthopedic surgery planning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Swartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Privalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>El Barbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kunze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Direct and indirect image rotation estimation methods of orthopedic X-ray images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Breininger</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2606045</idno>
		<ptr target="https://doi.org/10.1117/12.2606045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings SPIE Medical Imaging</title>
		<meeting>SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised lane detection with deep hough transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gemert</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP42928.2021.9506299</idno>
		<ptr target="https://doi.org/10.1109/ICIP42928.2021.9506299" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1514" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The radon transform on Euclidean space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ludwig</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpa.3160190105</idno>
		<ptr target="https://doi.org/10.1002/cpa.3160190105" />
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="81" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust online orientation correction for radiographs in PACs environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2006.880677</idno>
		<ptr target="https://doi.org/10.1109/tmi.2006.880677" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1370" to="1379" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning with known operators reduces maximum error bounds</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maier</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0077-5</idno>
		<ptr target="https://doi.org/10.1038/s42256-019-0077-5" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Numerical coordinate regression with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1801.07372</idno>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1801.07372" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple method for identifying image orientation of chest radiographs by use of the center of gravity of the image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12194-012-0155-4</idno>
		<ptr target="https://doi.org/10.1007/s12194-012-0155-4" />
	</analytic>
	<monogr>
		<title level="j">Radiol. Phys. Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orientation correction for chest images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pietka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF03167768</idno>
		<ptr target="https://doi.org/10.1007/BF03167768" />
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="189" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Labelme: image polygonal annotation with python</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">D-LinkNet: LinkNet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00034</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2018.00034" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="192" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
