<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen</title>
				<funder ref="#_VfdnS7e #_pGjf9SX">
					<orgName type="full">National Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_tqR8zcd">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_nzubREG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_dGuAJj5">
					<orgName type="full">Natural Science Foundation of Jiangsu Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daoqiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Rongjun</forename><surname>Ge</surname></persName>
							<email>rongjun.ge@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="23" to="32"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">870FA660294A03E64C2DA1A0AEF98E1E</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Eye-tracking</term>
					<term>Multiorgan segmentation</term>
					<term>Computer-Aided Diagnosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-organ segmentation of the abdominal region plays a vital role in clinical such as organ quantification, surgical planning, and disease diagnosis. Due to the dense distribution of abdominal organs and the close connection between each organ, the accuracy of the label is highly required. However, the dense and complex structure of abdominal organs necessitates highly professional medical expertise to manually annotate the organs, leading to significant costs in terms of time and effort. We found a cheap and easily accessible form of supervised information. Recording the areas by the eye tracker where the radiologist focuses while reading abdominal images, gaze information is able to force the network model to focus on relevant objects or features required for the segmentation task. Therefore how to effectively integrate image information with gaze information is a problem to be solved. To address this issue, we propose a novel network for abdominal multi-organ segmentation, which incorporates radiologists' gaze information to boost high-precision segmentation and weaken the demand for high-cost manual labels. Our network includes three special designs: 1) a dual-path encoder to further integrate gaze information; 2) a cross-attention transformer module (CATM) that embeds human cognitive information about the image into the network model; and 3) multi-feature skip connection (MSC), which combines spatial information during down-sampling to offset the internal details of segmentation. Additionally, our network utilizes discrete wavelet transform (DWT) to further provide information on organ location and edge in different directions. Extensive experiments performed on the publicly available Synapse dataset demonstrate that our proposed method can integrate effectively gaze information and achieves Dice similarity coefficient (DSC) up to 81.87% and Hausdorff distance (HD) reduction to 11.96%, as well as gain high-quality readable visualizations. Code will be available at https://github.com/code-Porunacabeza/gaze seg/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automatic segmentation of abdominal multiple organs is clinically significant in extremely that can significantly reduce clinical resource costs. However, the task of abdominal organ segmentation is difficult. The number of abdominal organs is large, and these multiple organs show diverse characteristics among themselves. For example, the shape of the stomach varies greatly even in the same individual at different times, making precise pixel segmentation extremely challenging. Accurate and automatic segmentation of readable results from abdominal multiple organs can provide accurate evidence of reality for surgical navigation, visual enhancement, radiation therapy, and biomarker measurement systems. Therefore, how to accurately make the segmentation results more readable in the case of multiple organs influencing each other has a great contribution to clinical examination and diagnosis.</p><p>Abdominal multi-organ network models based on deep neural networks (DNN) are difficult to train. Training such a good enough model usually requires a large amount of labeled data, or the model performance is likely to meet a heavy drop. However, manual annotation of organs requires doctors to make accurate judgments based on their professional knowledge and rich experience, this leads to making manual labeling both expensive and time-consuming. In addition to pixel-level annotated datasets, deep neural networks can also benefit from other types of supervision. For example, boundary-level annotation can provide more detailed boundary information. In addition, weakly supervised <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14]</ref> learning techniques can be used, such as training with pixel-level labels and unlabeled data. Additionally, visual perceptual <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">7]</ref> supervision can be employed by utilizing visual perceptual theory in the training of deep networks to increase their sensitivity to image features. Furthermore, pre-trained models can be utilized for transfer learning, which allows the model to learn features from previous tasks and improve its performance. In summary, deep neural networks can benefit from various types of supervision, which can improve their performance in a variety of visual tasks. These studies have demonstrated that incorporating finer-grained additional supervision can enhance the accuracy of deep neural networks and improve the interpretability of network models.</p><p>However, the practical process of collecting additional annotations remains challenging, as it may require clinicians to repeatedly provide specific and refined annotations to fine-tune the network model. There is a need to minimize the impact of the annotation process on clinical work. To address this, we investigate novel annotation information that can be used for abdominal multi-organ segmentation. In the context of medical image analysis, it has been observed that radiologists tend to focus their attention on specific regions of interest (ROIs) or lesions when interpreting medical images. Specifically, our method utilizes eye gaze information collected by an eye-tracker during radiologists' image interpretation as a source of additional supervision. In clinical practice, experienced radiologists can usually quickly locate specific organs when reading abdominal images. In this process, the doctor's eye movement information can reflect the location information of organs to a certain extent. Compared with manual label-ing, this information is cheap and fast and can be used as effective supervision information to assist the localization and segmentation of each organ. The literature studies have implied that the potential of the radiologist's gaze data can be high in improving disease diagnosis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">17]</ref>. Recently, Wang et al. <ref type="bibr" target="#b16">[16]</ref> applied eye-tracking technology to diagnose knee osteoarthritis, while Men et al. <ref type="bibr" target="#b9">[9]</ref> used eye-trackers to provide visual guidance to sonographers during ultrasound scanning. It can be seen that the use of eye movement attention information has great value and potential in automated auxiliary diagnosis.</p><p>In this paper, we propose a novel eye-guided multi-organ segmentation network for diverse abdominal organ images. The network model is forced to focus on relevant objects or features required for the segmentation task by fully and synergistically utilizing the radiologist's cognitive information about the abdominal image. This method of information collection is convenient and can make the positioning of each organ more accurate. The overall architecture is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The proposed network has three special designs: 1) a dual-path encoder that integrates human cognitive information; 2) a cross-attention transformer module (CATM) that communicates information in network semantic perception and human semantic perception; and 3) multi-feature skip connection (MSC), which effectively combines spatial information during down-sampling to offset the internal details of segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The proposed network adopts an encoder-decoder structure, where the encoder part consists of parallel dual paths that utilize multi-feature skip connection (MSC) to combine spatial information during down-sampling to offset the internal details of segmentation. A cross-attention transformer module (CATM) is designed at the bottleneck stage to effectively communicate information in network perception and human perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Wavelet Transform for Composite Information</head><p>Wavelet transform is able to obtain global information and edge information in different directions in gaze attention heatmaps so that the network can effectively fuse the composite information in the heatmaps. In the clinic, when radiologists read abdominal images, the more important location, the longer the radiologists' gaze. We convert this information into a heatmap representation. The heatmap reflects the rough position information of the target to be segmented. The single heatmap is unable to reflect the composite information it contains, therefore DWT is utilized for extracting it. Discrete wavelet transform <ref type="bibr" target="#b8">[8]</ref> (DWT) is applied to decompose the approximation coefficients and detail coefficients of abdominal organ distribution information on the gaze attention heatmap to locate the position and edge of multiple organs in the abdomen. In the decoding phase, we fuse the approximation coefficient in the gaze heatmap so that compensates for the global topological information of decoding features at the final segmentation. The detail coefficients are input into the image encoder together with the original image, which is used to guide the dual-path encoder to reserve detailed information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-feature Skip Connection</head><p>Multi-feature skip connection (MSC) comprehensively utilizes multiple features to guide the segmentation results of each abdominal organ toward accurate internal details. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we choose to integrate the encoding features on the two paths concatenated with features in the up-sampling process to offset the internal details of segmentation. Instead of using a simple concatenated and fusion strategy, we use multiple composite splicing and fusion to obtain matching features. The residual connection can aggregate the features of different levels to avoid additional noise and thus improve the network performance. The MSC can be expressed as follows:</p><formula xml:id="formula_0">F feat = F i + Conv(Conv(Conv(F i ⊕ F g )) ⊕ Conv(Conv(F g ⊕ F i ))),<label>(1)</label></formula><p>where F i and F g represent the output features from the down-sampling layers of the two paths, and F fusion denotes the final multiple fusion features. Following the MSC, the dimension of the concatenated multiple features remains the same as the dimension of the upsampled features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Attention Transformer Module</head><p>The cross-attention transformer module (CATM) creatively enables the communication between network semantic perception and human semantic perception. Different from the traditional self-attention mechanism in transformer block <ref type="bibr" target="#b15">[15]</ref>, by using CATM, information interactive collaboration on two paths is enabled effectively. CATM, which is a multi-path structure, is embedded in the bottleneck layer between the encoder and decoder. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, it consists of two paths: the image attention path and the gaze attention path. In our work, CATM is composed of L (we set L = 6) cross-attention transformer blocks (CTB) and Conv2D. The expression of CATM can be represented as:</p><formula xml:id="formula_1">F out = Conv((CT B(F i ) ⊕ CT B(F g )) i ),<label>(2)</label></formula><p>where F i and F g represent the final output features of the encoder on the image attention and gaze attention encoding pathways. Our experimental results demonstrate that the cross-attention operation within the CATM design efficiently enhances the communication of information between these two paths.</p><p>The convolution operation of CATM is used to fuse the feature information from two paths, which makes up for the possible information shortage in the decoding process. The CTB is the core design of the CATM, which is a variant of the Transformer <ref type="bibr" target="#b15">[15]</ref> that exchanges the network semantic perception and human semantic perception on the two different paths of the image and gaze attention. The key of cross-attention is to exchange Q, K and V of respective features between different path features and fuse them. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the K and V in the image attention path exchange with the gaze attention path, and each original Q fuse with the exchanged K and V . It is represented by a formula:</p><formula xml:id="formula_2">Attention(Q i , K g , V g ) = Sof tmax(Q i K g / √ d + B)V g , (<label>3</label></formula><p>)</p><formula xml:id="formula_3">Attention(Q g , K i , V i ) = Sof tmax(Q g K i / √ d + B)V i , (<label>4</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">Q i , K i , V i and Q g , K g , V g represent Q, K</formula><p>, and V in the image and gaze attention path, respectively; B denotes the learnable relative positional encoding; d is the dimension of K, and we set the number of head of multi-headed self-attention is 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation</head><p>Our experiments use the Synapse multi-organ segmentation dataset(Synapse). Each CT volume consists of 85 -198 slices of 512 × 512 pixels, with a voxel spatial resolution of ([0.54 -0.54] × [0.98 -0.98] × [2.5 -5.0]) mm 3 . We use the 30 abdominal CT scans and split it 18 training cases and 12 testing cases randomly. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, all 3D volumes are inferenced in a slice-by-slice fashion and the predicted 2D slices are stacked together to reconstruct the 3D prediction. We use the average Dice-Similarity coefficient(DSC) and average Hausdorff distance (HD) as the evaluation metric to evaluate our method on the full resolution of the original slice.  2). TransUnet and SwinUnet (without gaze attention information) predict coarser edges and shapes compared to our method; 3). In the 3rd row, our method correctly identifies the stomach, while SwinUnet (with and without gaze attention information) failed to predict the shape of the stomach.</p><p>Comparison with Existing Methods Performance. As shown in Table <ref type="table" target="#tab_0">1</ref>, We also train Unet gaze , TransUnet gaze , and SwinUnet gaze networks with gaze attention information by simply concatenating the gaze attention information with the input image. Our experimental results reveal that this approach of introducing gaze attention information as an auxiliary supervision mechanism leads to an appreciable improvement in network segmentation performance. Specifically, Unet, TransUnet, and SwinUnet have an improvement of approximately 1% in terms of the DSC evaluation metric and 2-4% in terms of the HD evaluation metric by concatenating the gaze attention. Furthermore, our method surpasses the SwinUnet (without gaze attention information) by approximately 3% in terms of the DSC evaluation metric and approximately 10% in terms of the HD evaluation metric. Comparing our method with U-Net, TransUnet, and SwinUnet (with gaze attention information), we also observe a significant improvement of approximately 2-4% in terms of the DSC evaluation metric and 7-24% in terms of the HD evaluation metric, respectively.</p><p>Qualitative Visualization Results. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>. 1st row of the figure highlights that the approach leveraging gaze attention as auxiliary supervision yields fewer erroneous labels in comparison to the other methods, implying that eye-tracking attention can aid the model in attending to relevant objects or features. TransUnet (with gaze attention information) predicts coarser edges and shapes compared to our method (e.g. in the 2nd row, the model's prediction for the liver). In the 3rd row, our method correctly identifies the stomach, while SwinUnet (with and without gaze attention information) failed to predict the shape of the stomach. The results demonstrate that our method can leverage gaze attention as auxiliary supervision and better segment and retain edge and shape information. Ablation Study. We verified the DWT, MSC, and CATM separately using three different network configurations. We summarize the experimental results in Table <ref type="table" target="#tab_2">2</ref>. It can be seen that the w/o DWT performs the worst, indicating that the detail coefficients extracted from eye-tracking heatmaps can effectively locate organs in the image and provide strong support for edge segmentation. The w/o CATM does not effectively fuse the image features and eye-tracking attention features using CATM, resulting in a less-than-ideal improvement in segmentation results. In the results of w/o MSC and the full version of the network, we observed that MSC can further improve segmentation results by integrating down-sampling information from both paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel network that can realize the interactive communication between network semantic perception and human semantic perception, and apply it to the task of abdominal multi-organ segmentation for information interactive collaboration. The network is innovatively built with 1) a dual-path encoder that integrates human cognitive information; 2) a cross-attention transformer module (CATM) that communicates information in network semantic perception and human semantic perception; and 3) multi-feature skip connection (MSC), which effectively combines spatial information during down-sampling to offset the internal details of segmentation. Extensive experiments with promising results reveal gaze attention has great clinical value and potential in multi-organ segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the network architecture, detail coefficients x dc , image ximg and gaze information xgaze are input into the network for segmentation. In the decoding phase, approximation coefficients xac are fused to compensate the global information. MSC: multi-feature skip connection. CATM: cross-attention transformer module</figDesc><graphic coords="3,55,98,315,05,340,27,128,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MSC fuses the encoding features on the two paths concatenated with features in the up-sampling process to offset the internal details of segmentation.</figDesc><graphic coords="4,109,80,331,79,204,34,85,66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CATM utilizes cross-attention efficiently enhances information interactive collaboration between network semantic perception and human semantic perception. The convolution operation fuses the feature information from two different paths.</figDesc><graphic coords="5,62,97,179,09,326,65,146,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. 1). 1st row of the figure highlights that the approach leveraging gaze attention as auxiliary supervision yields fewer erroneous labels in comparison to the other methods; 2). TransUnet and SwinUnet (without gaze attention information) predict coarser edges and shapes compared to our method; 3). In the 3rd row, our method correctly identifies the stomach, while SwinUnet (with and without gaze attention information) failed to predict the shape of the stomach.</figDesc><graphic coords="7,55,98,171,38,340,27,168,25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison on the Synapse multi-organ CT dataset (average dice score % and average Hausdorff distance in mm, and dice score % for each organ).</figDesc><table><row><cell>Methods</cell><cell cols="6">DSC↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach</cell></row><row><cell>V-Net [10]</cell><cell>68.81 -</cell><cell>75.34 51.87</cell><cell>77.10</cell><cell>80.75</cell><cell>87.84 40.05</cell><cell>80.56 56.98</cell></row><row><cell>DARR [5]</cell><cell>69.77 -</cell><cell>74.74 53.77</cell><cell>72.31</cell><cell>73.24</cell><cell>94.08 54.18</cell><cell>89.90 45.96</cell></row><row><cell>R50 U-Net [4]</cell><cell cols="2">74.68 36.87 87.74 63.66</cell><cell>80.60</cell><cell>78.19</cell><cell>93.74 56.90</cell><cell>85.87 74.16</cell></row><row><cell>U-Net [13]</cell><cell cols="2">76.85 39.70 89.07 69.72</cell><cell>77.77</cell><cell>68.60</cell><cell>93.43 53.98</cell><cell>86.67 75.58</cell></row><row><cell>U-Netgaze</cell><cell cols="2">77.94 35.50 89.66 65.04</cell><cell>81.25</cell><cell>75.91</cell><cell>93.57 59.94</cell><cell>84.66 73.53</cell></row><row><cell cols="3">R50 Att-UNet [4] 75.57 36.97 55.92 63.91</cell><cell>79.20</cell><cell>72.71</cell><cell>93.56 49.37</cell><cell>87.19 74.95</cell></row><row><cell>Att-UNet [11]</cell><cell cols="2">77.77 36.02 89.55 68.88</cell><cell>77.98</cell><cell>71.11</cell><cell>93.57 58.04</cell><cell>87.30 75.75</cell></row><row><cell>R50 ViT [4]</cell><cell cols="2">71.29 32.87 73.73 55.13</cell><cell>75.80</cell><cell>72.20</cell><cell>91.51 45.99</cell><cell>81.99 73.95</cell></row><row><cell>TransUnet [4]</cell><cell cols="2">77.48 31.69 87.23 63.13</cell><cell>81.87</cell><cell>77.02</cell><cell>94.08 55.86</cell><cell>85.08 75.62</cell></row><row><cell>TransUnetgaze</cell><cell cols="2">78.29 27.00 88.57 61.89</cell><cell>83.07</cell><cell>75.99</cell><cell>94.56 59.44</cell><cell>85.54 77.27</cell></row><row><cell>SwinUnet [3]</cell><cell cols="2">79.13 21.55 85.47 66.53</cell><cell>83.28</cell><cell>79.61</cell><cell>94.29 56.58</cell><cell>90.66 76.60</cell></row><row><cell>SwinUnetgaze</cell><cell cols="2">80.02 19.47 86.58 66.97</cell><cell>84.05</cell><cell>81.80</cell><cell>94.22 61.33</cell><cell>89.22 76.00</cell></row><row><cell>Ours</cell><cell cols="2">81.87 11.96 89.88 64.16</cell><cell>86.62</cell><cell>83.89</cell><cell>94.77 66.97</cell><cell>87.53 81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.16 3.2 Results and Analysis Overall Performance.</head><label></label><figDesc>As the last row shown in Table1. Our experimental results demonstrate that leveraging gaze attention as an auxiliary supervision mechanism for network training achieves superior segmentation performance, as evidenced by segmentation accuracies of 81.87% (Dice similarity coefficient) and 11.96% (Hausdorff distance).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the different variant network under the Synapse multiorgan CT dataset (average dice score % and average Hausdorff distance in mm).</figDesc><table><row><cell>Variant</cell><cell>Modules</cell><cell>Metrics</cell></row><row><cell></cell><cell cols="2">DWT MSC CATM DSC HD</cell></row><row><cell cols="2">w/o DWT ✕</cell><cell>75.75 32.35</cell></row><row><cell>w/o MSC</cell><cell>✕</cell><cell>81.21 8.99</cell></row><row><cell>w/o CATM</cell><cell>✕</cell><cell>80.46 12.34</cell></row><row><cell>Full Version</cell><cell></cell><cell>81.87 11.96</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This study was supported by the <rs type="funder">National Natural Science Foundation</rs> (No. <rs type="grantNumber">62101249</rs> and No. <rs type="grantNumber">62136004</rs>), the <rs type="funder">Natural Science Foundation of Jiangsu Province</rs> (No. <rs type="grantNumber">BK20210291</rs>), and the <rs type="funder">China Postdoctoral Science Foundation</rs> (No. <rs type="grantNumber">2021TQ0149</rs> and No. <rs type="grantNumber">2022M721611</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VfdnS7e">
					<idno type="grant-number">62101249</idno>
				</org>
				<org type="funding" xml:id="_pGjf9SX">
					<idno type="grant-number">62136004</idno>
				</org>
				<org type="funding" xml:id="_dGuAJj5">
					<idno type="grant-number">BK20210291</idno>
				</org>
				<org type="funding" xml:id="_tqR8zcd">
					<idno type="grant-number">2021TQ0149</idno>
				</org>
				<org type="funding" xml:id="_nzubREG">
					<idno type="grant-number">2022M721611</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eye movements of radiologists reflect expertise in CT study interpretation: a potential tool to measure resident development</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bertram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="815" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of eye tracking for understanding and improving diagnostic interpretation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Brunyé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41235-019-0159-2</idno>
		<ptr target="https://doi.org/10.1186/s41235-019-0159-2" />
	</analytic>
	<monogr>
		<title level="j">Cogn. Res. Princ. Implic</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-89" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptive relational reasoning for 3D multi-organ segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-864" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02349</idno>
		<title level="m">Tapas: weakly supervised table parsing via pre-training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using gaze-tracking data and mixture distribution analysis to support a holistic model for the detection of cancers on mammograms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Kundel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Nodine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Krupinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mello-Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="881" to="886" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WavTrans: synergizing wavelet and cross-attention transformer for multi-contrast MRI super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodalguidenet: gaze-probe bidirectional guidance in obstetric ultrasound scanning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-110" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-net: learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning hierarchical attention for weakly-supervised chest Xray abnormality localization and diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2698" to="2710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09772</idno>
		<title level="m">Weakly supervised disentanglement with guarantees</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Follow my eye: using gaze to supervise computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1688" to="1698" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eye movements in medical image perception: a selective review of past, present and future</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
