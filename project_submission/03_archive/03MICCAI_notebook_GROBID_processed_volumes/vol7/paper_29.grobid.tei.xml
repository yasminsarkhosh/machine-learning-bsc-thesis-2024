<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingxuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University in the City of New York</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Obstetrics and Gynecology Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mian</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="304" to="314"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">37B7097C6A7B805CB2007F8852E4B418</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Placenta Accreta Spectrum</term>
					<term>MRI</term>
					<term>Multi-Instance Learning</term>
					<term>Hierarchical Attention</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Placenta Accreta Spectrum (PAS) can lead to high risks like excessive blood loss at the delivery. Therefore, prenatal screening with MRI is essential for delivery planning that ensures better clinical outcomes. For computer-aided PAS diagnosis, existing work mostly extracts radiomics features directly from ROI while ignoring the context information, or learns global semantic features with limited awareness of the focal area. Moreover, they usually select single or few MRI slices to represent the whole sequences, which can result in biased decisions. To deal with these issues, a novel end-to-end Hierarchical Attention and Contrastive Learning Network (HACL-Net) is proposed under the formulation of a multi-instance problem. Slice-level attention module is first designed to extract context-aware deep semantic features. These slice-wise features are then aggregated via the patient-level attention module into taskspecific patient-wise representation for PAS prediction. A plug-and-play contrastive learning module is introduced to further improve the discriminating power of extracted features. Extensive experiments with ablation studies on a real clinical dataset show that HACL-Net can achieve stateof-the-art prediction performance with the effectiveness of each module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Placenta Accreta Spectrum (PAS) occurs when the placenta becomes abnormally adherent to the myometrium rather than the uterine decidua <ref type="bibr" target="#b14">[15]</ref>. The primary risk of PAS is hemorrhage and even life-threatening associated complications <ref type="bibr" target="#b14">[15]</ref>. Particularly, it can lead to excessive blood loss and transfusions of blood products at the delivery <ref type="bibr" target="#b8">[9]</ref>. Prenatal screening helps identify women with potential PAS and allows for appropriate delivery planning, which is critical for better clinical outcomes. Meanwhile, Magnetic Resonance Imaging (MRI) features high resolution and sensitivity, playing an increasingly important role in prenatal screening <ref type="bibr" target="#b11">[12]</ref>. MRI is often used after an inconclusive ultrasonic examination to assess the invasive condition of PAS <ref type="bibr" target="#b8">[9]</ref>. A detailed background illustration is included in Section A of supplementary material.</p><p>In the field of computer-aided PAS diagnosis for prenatal screening, some work based on traditional machine learning has been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> and achieved promising progress. In these works, radiomics features were first extracted from the Region of Interest (ROI, placenta) of raw images and a classifier was then fitted between the features and clinical outcomes. In <ref type="bibr" target="#b11">[12]</ref>, Random Forests (RF), K Nearest Neighbor (KNN), Naive Bayes (NB), and Multi-Layer Perception (MLP) were compared. Statistical analysis and multivariate logistic regression were employed in <ref type="bibr" target="#b12">[13]</ref>. The features are mostly extracted only on placenta ROI with Pyradiomics <ref type="bibr" target="#b16">[17]</ref> or predefined rules, ignoring the context information such as the location of the placenta and its interaction with nearby organs that are important in PAS diagnosis <ref type="bibr" target="#b9">[10]</ref>.</p><p>On the other hand, deep learning stands out in terms of automatic deep feature extraction <ref type="bibr" target="#b23">[24]</ref> and end-to-end learning fashion <ref type="bibr" target="#b2">[3]</ref>. Deep learning has been recently explored for image-based PAS diagnosis <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. A deep dynamic convolution neural network with autoencoder training manner was proposed in <ref type="bibr" target="#b19">[20]</ref> to extract deep features. In <ref type="bibr" target="#b22">[23]</ref> the encoder of a placenta segmentation network was employed to capture the semantic features. To enhance such deep features with the awareness of the focal area, attention mechanisms <ref type="bibr" target="#b24">[25]</ref> are considered in this paper. The ROI relevant to the prediction task can then be automatically localized, leading to more comprehensive features that capture both semantic and context information. Moreover, prior knowledge about the focal area can be incorporated by annotating a limited number of images with ROIs. That is more efficient than directly including the localization task in addition to the prediction task, which requires extra pixel-level annotation of each image.</p><p>Another gap to fill is the inherent diversity of MRI sequences. There are T1weighted and T2-weighted MRI scans, upon which three planes with multiple slices are further present. Therefore, each patient corresponds to dozens of slice images. The physicians usually need to inspect a large portion of these images in order to identify the focus of suspected PAS and reach the patient-level diagnosis. For the existing work, a single (or few) slice image is selected to represent the information of a patient, which can lead to potentially biased results. Multi-Instance Learning (MIL), as a potential solution, has been widely adopted in lesion classification and localization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Some recent works, such as slice attention transformer and convolutional transformer-based MIL, have studied the problem of patient as bag and images as instances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. It remains to explore how MIL can benefit the computer-aided PAS diagnosis.</p><p>To deal with the issues above, a novel end-to-end Hierarchical Attention and Contrastive Learning Network (HACL-Net) is proposed under the formulation of a multi-instance problem. Slice-level spatial residual attention with prior knowledge is designed to extract context-aware deep semantic features from individual MRI slices of a patient. Patient-level attention-based pooling is then applied to aggregate these features into patient representation for PAS prediction. To make such features more discriminative, a plug-and-play contrastive loss is further included. Extensive experiments with ablation study show the proposed network can achieve state-of-the-art performance on a real clinical dataset involving 359 distinct patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Consider each patient bag P i includes K i valid 2D MRI slices from axial, sagittal, and coronal views (possibly both T1-weighted and T2-weighted scans exist). In the training dataset, there is an associated binary label Y i indicating whether PAS is reported at delivery. Denote</p><formula xml:id="formula_0">P i = {X i , Y i }, X i = {X (1) i , • • • , X (Ki) i }, where X (j)</formula><p>i is the j-th MRI slice of i-th patient. Note that the number K i can vary with different patients. The proposed HACL-Net aims to learn the relationship between X i and Y i in a weekly-supervised and end-to-end manner (Fig. <ref type="figure" target="#fig_0">1</ref>). The slice-level attention module is designed to extract context-aware deep semantic features from each MRI slice. Then the slice-wise features are fused in a taskspecific way via the patient-level attention module, after which being connected to FC layer for PAS prediction. A plug-and-play contrastive learning module is presented to facilitate discriminative patient-wise features between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Slice-Level Spatial Residual Attention</head><p>Trunk Branch. The trunk branch aims to extract global semantic information from each MRI slice that is important for the PAS prediction task. It is composed of the top convolution layer, batch normalization layer, and ReLU layer from ResNet18. For each grayscale MRI slice X (j) i ∈ R 256×256×1 , a preliminary feature map T (j) i ∈ R 128×128×64 is generated. Note that to realize the spatial residual attention with the mask branch, shallow features are first derived since they can be aligned with the context features. Deep features will be extracted later after the spatial residual attention layer.</p><p>Mask Branch. The mask branch aims to extract the context information that indicates the location of ROI. Unlike the traditional implementation where a general bottom-up top-down network is employed, here the U-Net dedicated for medical image segmentation <ref type="bibr" target="#b13">[14]</ref> is considered because the placenta area accounts for most of the ROI. Automatic segmentation of placenta with U-Net has proved effective in recent years <ref type="bibr" target="#b3">[4]</ref>. The original network consists of 4 downsampling and 4 upsampling modules. To preserve the context information surrounding the placenta, a partial U-Net is constructed, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The last upsampling module to generate the single-channel mask (R 256×256×1 ) is removed and the intermediate feature map M (j) i ∈ R 128×128×64 is obtained as the output of mask branch. Note that the entire U-Net is first pre-trained with a small number of placenta annotated MRI slices. Then the partial U-Net gets fine-tuned with a small learning rate in the end-to-end prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Residual Attention and Deep Feature Extraction. The mask branch output M (j)</head><p>i has been aligned with the trunk branch output T (j) i in terms of the field of view. Therefore, M (j) i is multiplied in an element-wise manner with T (j) i as the attention mechanism. Besides, to avoid potential performance drop with attention, residual learning is further considered <ref type="bibr" target="#b17">[18]</ref>. The shallow features after spatial residual attention become H</p><formula xml:id="formula_1">(j) i = (1 + M (j) i ) T (j)</formula><p>i . Note that the attention is not performed directly on deep features because the ROI considered has explicit physical meanings instead of being "hidden".</p><p>The remaining layers of pre-trained ResNet18 (excluding the top three layers used previously) are employed to further extract deep features, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. These layers are fine-tuned in end-to-end learning so that task-specific deep features can be obtained. For each patient bag, a set of slice-wise deep features</p><formula xml:id="formula_2">F i = {F (1) i , • • • , F (Ki) i</formula><p>} are eventually generated, where F (j) i ∈ R 512 . These features capture the context-aware semantic information of MRI slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Patient-Level Attention-Based Pooling</head><p>Due to the MIL nature of the studied problem, MRI slices in a patient bag contribute unevenly to the PAS diagnosis. There are no detailed annotations to reflect the role of each slice. Therefore, patient-level attention mechanism is further proposed to aggregate the deep features from each slice in a task-specific manner, i.e., deriving the patient-wise features G i based on a set of slice-wise features F i . Some recent work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> has proved that attention-based pooling is useful for feature aggregation in MIL problems. It shows certain similarities with channel attention which has been widely applied in image classification. Besides, such aggregators can be easily embedded in the network as a trainable layer. Concretely, the attention-based pooling layer works as follows:</p><formula xml:id="formula_3">G i = Ki j=1 a j F (j) i , a j = exp{w tanh(VF (j) i )} Ki m=1 exp{w tanh(VF (m) i )} , (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>where the calculation of attention weight a j involves trainable parameters w ∈ R 64×1 and V ∈ R 64×512 . Such weight quantifies the degrees of "activation" toward the final prediction for each MRI slice. Note that the weight calculation resembles the softmax operation, and can deal with the varying number of slices for different patients unless the variation is not significant. The aggregated patient-wise features G i are then mapped to an output vector Ŷi with an FC layer. Binary Cross Entropy (BCE) is used as the classification loss function,</p><formula xml:id="formula_5">L e = - 1 N b N b i=1 Y i log( Ŷi ) + (1 -Y i ) log(1 -Ŷi ) . (<label>2</label></formula><formula xml:id="formula_6">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning for Discriminative Deep Features</head><p>For the studied problem, the MRI slices of a positive patient (with PAS) that reflect the clues about PAS can account for only a small portion of all slices, while most ones look normal. Meanwhile, some slices of a negative patient (without PAS) can manifest suspected patterns. Therefore, the extracted patient-wise deep features should be more discriminative under the existence of inherent noises. In addition to aggregating the slice-wise features with attention, contrastive learning is further considered to realize that <ref type="bibr" target="#b1">[2]</ref>. Specifically, the representation G i needs to keep certain distances between positive and negative patients while getting relatively closer for patients in the same class. Since the proposed network does not require self-supervised learning, a plug-and-play contrastive loss is employed to avoid additional capacity. Concretely, to make up each batch, a positive patient and a negative patient are first sampled from the dataset. Then another patient is randomly picked up. It is thus guaranteed that a matching pair and a non-matching pair exist, respectively. Assume the third patient is positive, the patient-wise features are denoted as G ia , G in , G ip . The triplet loss <ref type="bibr" target="#b0">[1]</ref> is calculated between these feature maps (G ia is the anchor, G in is the non-matching sample and G ip is the matching sample). The triplet loss is used as the contrastive loss:</p><formula xml:id="formula_7">L c = max G ia -G ip 2 -G ia -G in 2 + c, 0<label>(3)</label></formula><p>where c is the positive margin constant that quantifies the distance. It is added to the BCE losses of the samples in a batch: L total = L e + L c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>The experiments are conducted on a real clinical dataset collected from a large obstetrics and gynecology hospital. The dataset involves 359 distinct patient subjects with a total of 15,186 MRI slices. A 5-fold cross-validation is performed and the average metrics values with standard deviations over the folds are reported.</p><p>To pre-train the U-Net in the mask branch, 185 MRI slices are annotated with placenta by the experts. The ResNet18 is pre-trained on ImageNet. HACL-Net is trained end to end with Adam optimizer and batch size N b = 3 for 100 epochs.</p><p>The learning rate of the slice-level attention module and final FC layer is set as 10 -6 and 10 -4 respectively, while that of the rest parts is set as 10 -5 . PyTorch 1.10.1, and an NVIDIA GeForce RTX 2080 Ti GPU with CUDA 11.3 are used to train our method. More details can be found in Section B of supplementary.</p><p>The code is publicly available on https://github.com/LouieBHLu/HACL-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison of PAS Diagnosis</head><p>Section C of supplementary material illustrates the effect of different selections of trunk branch. Table <ref type="table" target="#tab_0">1</ref> shows the performance of the pre-trained baseline U-Net HACL-Net 84.2 ± 0.9 84.9 ± 0.6 79.2 ± 5.9 86.9 ± 2.8 compared with other three common networks. It turns out such network choice does not have a significant impact. The placenta area can be accurately captured by U-Net, which justifies the usage of Partial U-Net in the mask branch. Visual ROI prediction can be found in Section D of supplementary material.</p><p>To evaluate the performance of machine learning methods with Pyradiomics features and deep learning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>, the slice-wise features are aggregated with simple mean to derive the patient-wise features. The accuracy, area under the ROC curve (AUC), sensitivity, and specificity are employed as the metrics. As shown in Table <ref type="table" target="#tab_1">2</ref>, HACL-Net achieves the best 84.9% AUC and 79.2% sensitivity with 23.1% and 44.2% improvement respectively over the second-best approach. The outcomes are reflected in Fig. <ref type="figure" target="#fig_3">4</ref>, where the compared methods suffer from very low sensitivity and high specificity. Since only a small portion of MRI slices have clues about a patient's being positive, traditional aggregation of slices tends to dilute the effective information. As the features are extracted from a segmentation network in <ref type="bibr" target="#b22">[23]</ref>, the sensitivity is relatively improved but with highly degraded specificity. In general deep features even perform worse than ROI-based radiomics features, partially due to the increased complexity of input is not properly explained by the clinical outcome of PAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Table <ref type="table" target="#tab_2">3</ref> and Fig. <ref type="figure" target="#fig_4">5</ref> illustrate the effect of each module in HACL-Net. For experiments without patient-level attention, slice-wise features are also aggregated   with simple mean to derive patient-level features. The combination of mask branch and trunk branch achieves 15.4% AUC improvement over a single trunk branch, indicating the gain of additional context information. A 12.3% AUC improvement is witnessed when patient-level attention is used as an aggregator instead of mean. Contrastive learning manifests 5.7% AUC and 16.3% sensitivity improvement with 11.8% specificity drop as a trade-off. It is believed that sensitivity can be more important from a clinical perspective, and the trade-off can be tuned according to specific demands. Moreover, the softmax activation is replaced with sigmoid, and different confidence thresholds are used. As the threshold gets lower, a non-monotonic trend of sens-spec trade-off is observed, while the optimal balance lies around 0.4. That reveals the potential effect of class imbalance, i.e., fewer positive cases. To this end, contrastive learning can help separate the decision boundary between two classes. Sigmoid results differ from softmax due to the explicitly set decision threshold. The predicted scores between positive and negative patients can be close for hard samples, leading to large deviations in sigmoid results. Figure <ref type="figure" target="#fig_5">6</ref> shows feature maps of a positive </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a novel end-to-end HACL-Net for MRI-based computeraided PAS diagnosis that facilitates more efficient prenatal screening. The proposed network utilizes slice-level spatial residual attention to extract contextaware deep semantic features, and aggregate them as a comprehensive representation of a patient with patient-level attention-based pooling. Moreover, a contrastive loss is added to improve the discriminating power of learned patient-wise features. Extensive experiments on a real clinical dataset show that HACL-Net achieves superior performance, with great potential for clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An architecture overview of the proposed HACL-Net.</figDesc><graphic coords="3,49,80,53,69,324,64,166,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The details of slice-level attention module with a two-branch structure.</figDesc><graphic coords="4,108,30,88,73,234,76,104,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Remaining layers of pre-trained ResNet18 except for the top three layers.</figDesc><graphic coords="5,100,29,60,65,214,60,61,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. ROC of HACL-Net and existing methods for PAS diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. ROC of ablation study of each component of HACL-Net. PatAttn is patient-level attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Feature maps generated by HACL-Net of a positive patient. Figures from left to right are the MRI slice, the spatial residual attention feature map, the trunk branch feature map, and the mask branch feature map.</figDesc><graphic coords="9,43,29,53,78,337,24,80,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of pre-trained ROI networks. PA is pixel accuracy.</figDesc><table><row><cell cols="2">Method Train</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>DICE</cell><cell>IOU</cell><cell>PA</cell><cell>DICE</cell><cell>IOU</cell><cell>PA</cell></row><row><cell>U-Net</cell><cell cols="6">80.6 ± 0.5 89.2 ± 0.3 78.3 ± 0.9 77.1 ± 0.9 87.0 ± 0.5 72.4 ± 1.2</cell></row><row><cell>MaNet</cell><cell cols="6">82.2 ± 0.6 90.2 ± 0.3 79.8 ± 0.7 80.4 ± 1.0 89.1 ± 0.6 76.2 ± 1.0</cell></row><row><cell>FPN</cell><cell cols="6">85.4 ± 0.5 92.1 ± 0.4 81.7 ± 0.7 80.7 ± 0.9 89.3 ± 0.4 72.8 ± 0.9</cell></row><row><cell cols="7">LinkNet 79.5 ± 0.3 88.5 ± 0.2 77.0 ± 0.7 76.8 ± 0.6 86.9 ± 0.4 72.8 ± 1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of HACL-Net and existing methods for PAS diagnosis. The results in bold denote the best performance. Radio+KNN 68.2 ± 5.7 60.2 ± 3.4 24.9 ± 8.3 87.0 ± 3.2 Radio+MLP 70.4 ± 6.4 61.7 ± 6.3 26.7 ± 8.8 89.4 ± 4.5</figDesc><table><row><cell>Method</cell><cell>Accuracy AUC</cell><cell cols="2">Sensitivity Specificity</cell></row><row><cell>Radio+RF</cell><cell cols="3">70.1 ± 5.4 61.8 ± 4.3 15.4 ± 4.2 93.9 ± 4.2</cell></row><row><cell>Radio+NB</cell><cell cols="2">69.9 ± 4.0 48.1 ± 3.9 0.0 ± 0.0</cell><cell>100.0 ± 0.0</cell></row><row><cell>Xuan [20]</cell><cell cols="2">65.7 ± 1.2 51.4 ± 1.0 1.7 ± 2.4</cell><cell>91.3 ± 2.5</cell></row><row><cell>Ye [23]</cell><cell cols="3">55.7 ± 6.5 46.7 ± 1.4 35.0 ± 8.2 64.0 ± 12.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of ablation study for each module in the proposed HACL-Net.</figDesc><table><row><cell cols="2">Mask Branch Trunk Branch Patient Attention CL Accuracy</cell><cell>AUC</cell><cell>Sensitivity Specificity</cell></row><row><cell></cell><cell cols="3">60.4 ± 13.5 50.1 ± 1.3 26.0 ± 32.2 74.1 ± 31.7</cell></row><row><cell></cell><cell cols="3">71.6 ± 0.2 50.0 ± 1.5 7.5 ± 2.5</cell><cell>94.1 ± 2.0</cell></row><row><cell></cell><cell cols="3">71.6 ± 1.4 51.5 ± 1.5 5.0 ± 3.5</cell><cell>98.0 ± 2.4</cell></row><row><cell>Sigmoid w/ 0.50 threshold</cell><cell cols="3">66.0 ± 9.6 60.9 ± 8.2 38.7 ± 4.1 84.8 ± 12.8</cell></row><row><cell>Sigmoid w/ 0.45 threshold</cell><cell cols="3">68.3 ± 9.8 68.1 ± 8.4 69.2 ± 6.6 67.0 ± 13.3</cell></row><row><cell>Sigmoid w/ 0.40 threshold</cell><cell cols="3">78.0 ± 8.0 82.1 ± 4.4 77.0 ± 10.1 81.3 ± 14.0</cell></row><row><cell>Sigmoid w/ 0.35 threshold</cell><cell cols="3">77.8 ± 8.4 81.6 ± 8.2 75.6 ± 0.9 78.5 ± 15.2</cell></row><row><cell>Sigmoid w/ 0.30 threshold</cell><cell cols="3">70.6 ± 3.1 66.3 ± 7.0 87.8 ± 8.7 49.7 ± 13.8</cell></row></table><note><p>70.7 ± 2.7 53.9 ± 5.2 12.5 ± 10.9 95.2 ± 4.4 77.7 ± 0.6 66.9 ± 1.5 35.0 ± 4.1 94.7 ± 1.0 80.7 ± 4.1 79.2 ± 5.2 62.9 ± 1.9 98.7 ± 1.3 84.2 ± 0.9 84.9 ± 0.6 79.2 ± 5.9 86.9 ± 2.8</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was supported by <rs type="person">Dr. Hao Zhu</rs>'s team from <rs type="institution">Obstetrics and Gynecology Hospital</rs> affiliated to Fudan University in China.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 29.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end deep learning of optimization heuristics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="219" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic segmentation of human placenta images with u-net</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="180083" to="180092" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mal: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-817" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implementing machine learning in radiology practice and research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Prevedello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Geis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="754" to="760" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Satr: slice attention with transformer for universal lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What we know about placenta accreta spectrum (PAS)</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Obstet. Gynecol. Reprod. Biol</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Placenta previa, placenta accreta, and vasa previa</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oyelese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Smulian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prediction of placenta accreta spectrum using texture analysis on coronal and sagittal T2-weighted imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Abdom. Radiol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="5344" to="5352" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine learning analysis of MRI-derived texture features to predict placenta accreta spectrum in patients with placenta previa</title>
		<author>
			<persName><forename type="first">V</forename><surname>Romeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="71" to="76" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prediction of placenta accreta spectrum in patients with placenta previa using clinical risk factors, ultrasound and magnetic resonance imaging findings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Romeo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11547-021-01348-6</idno>
		<idno>11547-021-01348-6</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Radiol. Med. (Torino)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1216" to="1225" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Placenta accreta spectrum. Protocols for High-Risk Pregnancies: an evidence-based approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lyell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="571" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-89" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational radiomics system to decode the radiographic phenotype</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Griethuysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Res</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="104" to="e107" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RMDL: recalibrated multi-instance deep learning for whole slide gastric image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101549</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prenatal prediction and typing of placental invasion using MRI deep and radiomic features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-instance learning for survival prediction from whole slide images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-755" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jonnagaddala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101789</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prediction of placenta accreta spectrum by combining deep learning and radiomics using t2wi: a multicenter study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Abdominal Radiol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4205" to="4218" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating feature selection and feature extraction methods with deep learning to predict clinical outcome of breast cancer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="28936" to="28944" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatiotemporal attention for early prediction of hepatocellular carcinoma based on longitudinal ultrasound images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="534" to="543" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
