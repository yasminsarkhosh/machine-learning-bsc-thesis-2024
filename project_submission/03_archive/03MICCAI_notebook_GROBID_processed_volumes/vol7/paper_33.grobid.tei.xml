<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TabAttention: Learning Attention Conditionally on Tabular Data</title>
				<funder ref="#_frVhZzY">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">International Research Agendas programme of the Foundation for Polish Science</orgName>
				</funder>
				<funder>
					<orgName type="full">European Regional Development Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michal</forename><forename type="middle">K</forename><surname>Grzeszczyk</surname></persName>
							<email>m.grzeszczyk@sanoscience.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sano Centre for Computational Medicine</orgName>
								<address>
									<settlement>Cracow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Szymon</forename><surname>Płotka</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Beata</forename><surname>Rebizant</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">The Medical Centre of Postgraduate Education</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katarzyna</forename><surname>Kosińska-Kaczyńska</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">The Medical Centre of Postgraduate Education</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michał</forename><surname>Lipa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Brawura-Biskupski-Samaha</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">The Medical Centre of Postgraduate Education</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Przemysław</forename><surname>Korzeniowski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomasz</forename><surname>Trzciński</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Warsaw University of Technology</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">IDEAS NCBR</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Tooploox, Wroclaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arkadiusz</forename><surname>Sitek</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TabAttention: Learning Attention Conditionally on Tabular Data</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="347" to="357"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2B2D85103FCAC40DEFC432308861D7AB</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attention</term>
					<term>Fetal Ultrasound</term>
					<term>Tabular Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical data analysis often combines both imaging and tabular data processing using machine learning algorithms. While previous studies have investigated the impact of attention mechanisms on deep learning models, few have explored integrating attention modules and tabular data. In this paper, we introduce TabAttention, a novel module that enhances the performance of Convolutional Neural Networks (CNNs) with an attention mechanism that is trained conditionally on tabular data. Specifically, we extend the Convolutional Block Attention Module to 3D by adding a Temporal Attention Module that uses multihead self-attention to learn attention maps. Furthermore, we enhance all attention modules by integrating tabular data embeddings. Our approach is demonstrated on the fetal birth weight (FBW) estimation task, using 92 fetal abdominal ultrasound video scans and fetal biometry measurements. Our results indicate that TabAttention outperforms clinicians and existing methods that rely on tabular and/or imaging data for FBW prediction. This novel approach has the potential to improve computeraided diagnosis in various clinical workflows where imaging and tabular data are combined. We provide a source code for integrating TabAttention in CNNs at https://github.com/SanoScience/Tab-Attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many clinical procedures involve collecting data samples in the form of imaging and tabular data. New deep learning (DL) architectures fusing image and nonimage data are being developed to extract knowledge from both sources of information and improve predictive capabilities <ref type="bibr" target="#b8">[9]</ref>. While concatenation of tabular and imaging features in final layers is widely used <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, this approach limits the interaction between them. To facilitate better knowledge transfer between these modalities more advanced techniques have been proposed. Duanmu et al. <ref type="bibr" target="#b4">[5]</ref> presented the Interactive network in which tabular features are passed through a separate branch and channel-wise multiplied with imaging features at different stages of Convolutional Neural Network (CNN). Pölsterl et al. <ref type="bibr" target="#b15">[16]</ref> proposed a Dynamic Affine Feature Map Transform (DAFT) to shift and scale feature maps conditionally on tabular data. In <ref type="bibr" target="#b5">[6]</ref>, Guan et al. presented a method for transforming tabular data and processing them together with 3D feature maps via VisText self-attention module. The importance of the attention mechanism on DL models' performance has been extensively studied <ref type="bibr" target="#b22">[23]</ref>. Convolutional Block Attention Module (CBAM) <ref type="bibr" target="#b24">[25]</ref> has been shown to improve the performance of DL models on high dimensional data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. Despite these advances, few studies have explored the potential of incorporating attention maps with imaging and tabular data simultaneously.</p><p>We develop such a solution and as an example of application, we use fetal birth weight (FBW) prediction from ultrasound (US) data. It is a challenging task requiring clinicians to collect US videos of fetal body parts and fetal biometry measurements. Currently, abdominal circumference (AC), head circumference (HC), biparietal diameter (BPD), and femur length (FL) are used to estimate FBW with heuristic formulae <ref type="bibr" target="#b6">[7]</ref>. The predicted weight is the indicator of perinatal health prognosis or complications in pregnancy and has an impact on the method of delivery (vaginal or Cesarean) <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, the current approach to FBW estimation is often imprecise and can lead to a mean absolute percentage error (MAPE) of 10%, even if performed by experienced sonographers <ref type="bibr" target="#b19">[20]</ref>. An ensemble of Machine Learning algorithms was proposed by Lu et al. <ref type="bibr" target="#b11">[12]</ref> for solving this task. CNNs are applied for fetal biometry measurements estimation from US standard planes <ref type="bibr" target="#b0">[1]</ref> or US videos <ref type="bibr" target="#b14">[15]</ref>. Tao et al. <ref type="bibr" target="#b20">[21]</ref> approach this problem with a recurrent network utilizing temporal features of fetal weight changes over weeks concatenated with fetal parameters. P lotka et al. <ref type="bibr" target="#b13">[14]</ref> developed BabyNet, a hybrid CNN with Transformer layers to estimate FBW directly from US videos. Recent studies show that there is a strong correlation between the image features of the abdominal plane and the estimated fetal weight, indicating that it can serve as a dependable indicator for evaluating fetal growth <ref type="bibr" target="#b2">[3]</ref>. We utilize the US videos of the abdomen (imaging data) and biometry measurements with other numerical values (tabular data) during our experiments.</p><p>In this work, we introduce TabAttention, a novel module designed to enhance the performance of CNNs by incorporating tabular data. TabAttention extends the CBAM to the temporal dimension by adding a Temporal Attention Module (TAM) that leverages Multi-Head Self-Attention (MHSA) <ref type="bibr" target="#b22">[23]</ref>. Our method utilizes pooled information from imaging feature maps and tabular data (represented as tabular embeddings) to generate attention maps through Channel Attention Module (CAM), Spatial Attention Module (SAM), and TAM. By incorporating tabular data, TabAttention enables the network to better identify what, where, and when to focus on, thereby improving performance. We evaluate our method on the task of estimating FBW from abdominal US  videos and demonstrate that TabAttention is at least on par with existing methods, including those based on tabular and/or imaging data, as well as clinicians.</p><p>The main contributions of our work are: 1) the introduction of TabAttention, a module for conditional attention learning with tabular data, 2) the extension of CBAM to the temporal dimension via the TAM module, and 3) the validation of our method on the FBW estimation task, where we demonstrate that it is competitive with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce the fundamental components of the TabAttention module. We detail the development of CBAM augmented with a Temporal Attention Module. Then, we elaborate on how TabAttention leverages tabular embeddings to modulate the creation of attention maps and outline how the module can be seamlessly incorporated into the residual block of ResNet. Figure <ref type="figure" target="#fig_1">1</ref> presents the overview of the TabAttention module. Given US video sequence S ∈ R T0×1×H0×W0 of height H 0 , width W 0 and frame number T 0 as the input, 3D CNN produces intermediate temporal feature maps S ∈ R T ×C×H×W where C is the number of channels. In our setting, the CBAM block generates T 1D channel attention maps M c ∈ R T ×C×1×1 and T 2D spatial attention maps M s ∈ R T ×1×H×W . We create attention maps separately for every temporal feature map as the information of what is meaningful and where it is important to focus on might change along the temporal dimension. To account for the temporal changes and focus on when is the informative part we add TAM which infers temporal attention map M t ∈ R T ×1×1×1 . Intermediate temporal feature maps S are refined with attention maps in the following way:</p><formula xml:id="formula_0">S = M c (S ) ⊗ S S = M s (S ) ⊗ S O = M t (S ) ⊗ S (1)</formula><p>Here O denotes the output of the module and ⊗ is an element-wise multiplication during which attention maps are broadcasted along all unitary dimensions.  In general, attention maps are computed based on information aggregated by average-and max-pooling along specified dimensions which are then passed through shared layers for refinement (Fig. <ref type="figure" target="#fig_3">2</ref>). Then, these refined descriptors are passed through the sigmoid function to create final attention maps. To account for the tabular information during attention maps computing, we embed the input tabular data T ab ∈ R D , where D is the number of numerical features, with two linear layers and Rectified Linear Unit (ReLU) activation in between. The tabular data is embedded to the size of pooled feature maps. The embedding is passed through shared layers in the same way as pooled feature maps. Therefore, the attention maps are computed conditionally on tabular data. Thus, the output of TabAttention O t is computed as follows:</p><formula xml:id="formula_1">S t = M c (S , T ab) ⊗ S S t = M s (S t , T ab) ⊗ S t O t = M t (S t , T ab) ⊗ S t</formula><p>(2) Channel Attention Module. We follow the design of the original CBAM <ref type="bibr" target="#b24">[25]</ref>. We split temporal feature maps into T feature maps F i where i ∈ 1, ..., T so that each of them is passed through CAM separately. To compute the channel attention (M c ), we aggregate the spatial information through average-and maxpooling to produce descriptors (F c avgi , F c maxi ∈ R C×1×1 ). We pass the tabular data through a multi-layer perceptron (MLP embc ) with one hidden layer (of size R C z , where z is the reduction ratio set to 16) and ReLU activation to embed it into the same dimension as spatial descriptors. Then, both descriptors, with tabular embedding are passed through the shared network which is MLP with a hidden activation size of R C z and one ReLU activation. After the MLP is applied, the output vectors are element-wise summed to produce the attention map. We concatenate attention maps of all feature maps to produce M c :</p><formula xml:id="formula_2">M c (S , T ab) = [M f c (F i , T ab)] i=1,...,T M f c (F i , T ab) = σ(MLP (F c maxi ) + MLP (F c avgi ) + MLP (MLP embc (T ab)))<label>(3)</label></formula><p>Spatial Attention Module. After splitting the temporal feature maps, we average-and max-pool them along channel dimension to produce feature descriptors (F s avgi , F s maxi ∈ R 1×H×W ). We pass the tabular data through MLP embs with one hidden layer of size R H×W 2</p><p>and ReLU activation to embed it into the same dimension as spatial descriptors. We reshape this embedding to the size of feature descriptors and concatenate it with them. We pass the following representation through a 2D convolution layer and the sigmoid activation:</p><formula xml:id="formula_3">M s (S , T ab) = [M f s (F i , T ab)] i=1,...,T M f s (F i , T ab) = σ(Conv([F s maxi , F s avgi , Reshape(MLP embs (T ab))]))<label>(4)</label></formula><p>Temporal Attention Module. We create temporal descriptors by averageand max-pooling temporal feature maps along all non-temporal dimensions (F t avgi , F t maxi ∈ R T ×1×1×1 ). We embed tabular data with MLP embt with one hidden layer of size R T 2 into the same dimension. We concatenate created vectors and treat them as the embedding of the US sequence which we pass to the MHSA layer (with 2 heads). We create the query (Q), key (K) and value (V) with linear layers and an output size of d (4). We add relative positional encodings <ref type="bibr" target="#b18">[19]</ref> r to K. After passing through MHSA, we squash the refined representation with one MLP layer and sigmoid function to create a temporal attention map M t :</p><formula xml:id="formula_4">MHSA(S emb ) = MLP sof tmax Q j (K j + r) T √ d V j j=1,2 M t (S , T ab) = σ(MHSA([F t max , F t avg , MLP embt (T ab)]))<label>(5)</label></formula><p>TabAttention can be integrated within any 3D CNN (or 2D CNN in case TAM is omitted). As illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>, we add TabAttention between the first ReLU and the second convolution in the residual block to integrate our module with 3D ResNet-18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>This section describes the dataset used and provides implementation details of our proposed method. We benchmark the performance of TabAttention against several state-of-the-art methods and compare them to results obtained by clinicians. Additionally, we conduct an ablation study to demonstrate the significance of each key component utilized in our approach.</p><p>Dataset. This study was approved by the Ethics Committee of the Medical University of Warsaw (Reference KB.195/2021) and informed consent was obtained for all subjects. The multi-site dataset was acquired using international standards approved by <ref type="bibr" target="#b17">[18]</ref>. The dataset consists of 92 2D fetal US video scans captured in the standard abdominal plane view. These scans were collected from 92 pregnant women (31.89 ± 4.76 years), across three medical centers, and obtained as part of routine US examination done less than 24 h before delivery. This allowed us to obtain the real ground truth which was baby weight soon after birth. Five experienced sonographers (14.2 ± 4.02 years of experience) acquired the data using a single manufacturer device (General Electric) and several models (GE Voluson E6, S8, P8, E10, and S10). The abdominal fetal US videos (5-10 seconds, 13-37 frames per second) were saved in the DICOM file format.</p><p>We resized the pixel spacing to 0. Implementation Details. We use 3D ResNet-18 as our base model. We implement all experiments with PyTorch and train networks using NVIDIA A100 80GB GPU for 250 epochs with a batch size of 16 and an initial learning rate chosen with grid search from the set of {1 × 10 -2 , 1 × 10 -3 , 1 × 10 -4 }. To minimize the Mean Squared Error loss function, we employ the Adam <ref type="bibr" target="#b9">[10]</ref> optimizer with L2 regularization of 1 × 10 -4 and cosine annealing learning rate scheduler. To evaluate the reliability of the regression algorithm, we conduct five-fold cross-validation (CV) and ensure that each patient's data is present in only one fold. To ensure similar birth weight distribution in all folds, we stratify them based on the assignment of data samples into three bins: &lt; 3000 g g, &gt; 4000 g g, and in-between. The input frames are of size 128 × 128 pixels. We follow the approach presented in <ref type="bibr" target="#b13">[14]</ref>, we set the number of input frames to 16 and average per-patient predictions of all 16 frame segments from the single video. Throughout the training process, we employ various data augmentation techniques such as rotation, random adjustments to brightness and contrast, the addition of Gaussian noise, horizontal flipping, image compression, and motion blurring for every batch. We standardize all numerical features to a mean of 0 Comparison with State-of-the-Art Methods. We compare TabAttention with several methods utilizing tabular data only (Linear Regression <ref type="bibr" target="#b12">[13]</ref>,</p><p>XGBoost <ref type="bibr" target="#b3">[4]</ref>), imaging data only (3D ResNet-18 <ref type="bibr" target="#b21">[22]</ref>, BabyNet <ref type="bibr" target="#b13">[14]</ref>), both types of data (Interactive <ref type="bibr" target="#b4">[5]</ref>, DAFT <ref type="bibr" target="#b15">[16]</ref>), and Clinicians. The predictions of Clinicians were achieved using Hadlock III <ref type="bibr" target="#b6">[7]</ref> formula and AC, HC, BPD, FL measurements. The comparison of results from the five-fold CV is presented in Table <ref type="table" target="#tab_0">1</ref>.</p><p>TabAttention achieves the lowest MAE, RMSE and MAPE (170 ± 26, 225 ± 37, 5.0 ± 0.8 respectively) among all tested methods. Our approach outperforms clinically utilized heuristic formulae, machine learning, and image-only DL methods (two-tailed paired t-test p-value &lt; 0.05). Results of TabAttention are also best compared with all DL models utilizing tabular and imaging modalities, however, the difference does not reach statistical significance with a p-value around 0.11.</p><p>Ablation Study. We conduct ablation experiments to validate the effectiveness of key components of our proposed method (Table <ref type="table" target="#tab_1">2</ref>). We employ 3D ResNet-18 as the baseline model. The integration of TAM or CBAM with attention maps learned conditionally on tabular data into the 3D ResNet-18 architecture improves the predictive performance of the network. Subsequently, the incorporation of full TabAttention further enhances its capabilities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusions</head><p>In this work, we present a novel method, TabAttention, that can effectively compete with current state-of-the-art image and/or tabular-based approaches in estimating FBW. We found that it outperformed Clinicians achieving mMAPE of 5.0% vs. 5.9% (p-value &lt; 0.05). A key advantage of our approach is that it does not require any additional effort from clinicians since the necessary data is already collected as part of standard procedures. This makes TabAttention an alternative to the heuristic formulas that are currently used in clinical practice. We should note that while TabAttention achieved the lowest metrics among the DL models we evaluated, the differences between our approach and other DL methods using tabular data were not statistically significant, partly due to the small performance change. This small difference in the performance is likely caused by the fact that the tabular features used in TabAttention are mainly derived from the same modality (i.e. US scans), so they do not carry additional information, but instead can be considered as refined features already present in the scans. To develop TabAttention, we used tabular data as a hint for the network to learn attention maps and gain additional knowledge about essential aspects presented in the scans. This approach significantly improved the performance of baseline methods and demonstrated its practical applicability. Accurate estimation of FBW is crucial in determining the appropriate delivery method, whether vaginal or Cesarean. Low birth weight (less than 2500 g g) is a major risk factor for neonatal death, while macrosomia (greater than 4000 g) can lead to delivery traumas and maternal complications, such as birth canal injuries, as reported by Benacerraf et al. <ref type="bibr" target="#b1">[2]</ref>. Thus, precise prediction of FBW is vital for very low and high weights. Notably, in this respect, our method is robust to outliers with high or low FBW since there is no correlation between true FBW and absolute prediction error (Pearson correlation coefficient of -0.029).</p><p>This study has limitations. Firstly, a relatively small study cohort was used, which may affect the accuracy and generalization of the results. To address this, future work will include a larger sample size by using additional datasets. Secondly, our dataset is limited to only Caucasian women and may not be rep-resentative of other ethnicities. It is important to investigate the performance of our method with datasets from different ethnic groups and US devices to obtain more robust and generalizable results. Lastly, our method relies on fetal biometry measurements that are subject to inter-and intra-observer variabilities. This variability could potentially affect the network's performance and influence the measurements' quality. Future studies should consider strategies to reduce measurement variabilities, such as standardized protocols or automated measurements, to improve the accuracy of the method.</p><p>To summarize, we have introduced TabAttention, a new module that enables the conditional learning of attention on tabular data and can be integrated with any CNN. Our method has many potential applications, including serving as a computer-aided diagnosis tool for various clinical workflows. We have demonstrated the effectiveness of TabAttention on the FBW prediction task, utilizing both US and tabular data, and have shown that it outperforms other methods, including clinically used ones. In the future, we plan to test the method in different clinical applications where imaging and tabular data are used together.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The overview of proposed TabAttention module inspired by CBAM<ref type="bibr" target="#b24">[25]</ref>. We add a Temporal Attention Module to the existing architecture to extend the method to 3D data processing. In our setting, channel, spatial and temporal attention maps are trained conditionally on tabular data. ⊗ indicates element-wise multiplication.</figDesc><graphic coords="3,57,96,54,05,336,37,92,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Details of TabAttention components: CAM (a), SAM (b) and TAM (c) with tabular embeddings. In TAM, only one attention head is visualised. TabAttention is integrated with residual block as presented in (d). ⊗ indicates matrix multiplication.</figDesc><graphic coords="4,308,01,72,11,67,33,129,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Exemplary input to our method (a) of tabular data and frames of abdominal scans. Fetal US scans of the head (b), abdomen (c), and femur (d) were used to obtain AC, HC, BPD and FL utilized in tabular data.</figDesc><graphic coords="6,43,29,77,72,337,54,56,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2 mm × 0.2 mm for all video clips. As tabular data, we used six numerical features: AC (34.51 ± 2.35 cm), HC (33.56 ± 1.41 cm), BPD (9.40 ± 0.46 cm), FL (7.28 ± 0.33 cm), GA (38.29 ± 1.47 weeks), and mother's age. The examples of how the measurements were obtained are presented in Fig.3. The actual birth weight of the fetus obtained right post-delivery (3495 ± 507 g) was used as the target of the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Five-fold cross-validation results of state-of-the-art methods utilizing imaging (Img.) and/or tabular (Tab.) data. The mean of MAE, RMSE, and MAPE across all folds are presented. The best results are bolded. and a standard deviation of 1. We use Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and MAPE to evaluate the regression performance.</figDesc><table><row><cell>Method</cell><cell cols="5">Img. Tab. mMAE [g] mRMSE [g] mMAPE [%]</cell></row><row><cell>BabyNet [14]</cell><cell>✓</cell><cell>✗</cell><cell>294 ± 30</cell><cell>386 ± 56</cell><cell>8.5 ± 1.0</cell></row><row><cell>3D ResNet-18 [22]</cell><cell>✓</cell><cell>✗</cell><cell>289 ± 38</cell><cell>373 ± 43</cell><cell>8.5 ± 1.1</cell></row><row><cell>XGBoost</cell><cell>✗</cell><cell>✓</cell><cell>259 ± 23</cell><cell>328 ± 26</cell><cell>7.6 ± 0.1</cell></row><row><cell>Linear Regression</cell><cell>✗</cell><cell>✓</cell><cell>207 ± 18</cell><cell>260 ± 19</cell><cell>6.0 ± 0.1</cell></row><row><cell>Clinicians</cell><cell>✗</cell><cell>✓</cell><cell>205 ± 14</cell><cell>253 ± 13</cell><cell>5.9 ± 0.0</cell></row><row><cell>DAFT [16]</cell><cell>✓</cell><cell>✓</cell><cell>175 ± 30</cell><cell>244 ± 42</cell><cell>5.3 ± 1.0</cell></row><row><cell>Interactive [5]</cell><cell>✓</cell><cell>✓</cell><cell>172 ± 27</cell><cell>230 ± 44</cell><cell>5.2 ± 0.9</cell></row><row><cell cols="2">TabAttention (ours) ✓</cell><cell>✓</cell><cell cols="2">170 ± 26 225 ± 37</cell><cell>5.0 ± 0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Five-fold cross-validation results of ablation study with key components of TabAttention. The first row is the result of the baseline method. The next rows refer to modules of TabAttention with or without tabular embeddings (Tab.) and the last one is full TabAttention.</figDesc><table><row><cell>Method</cell><cell cols="5">Img. Tab. mMAE [g] mRMSE [g] mMAPE [%]</cell></row><row><cell>3D ResNet-18 [22]</cell><cell>✓</cell><cell>✗</cell><cell>289 ± 38</cell><cell>373 ± 43</cell><cell>8.5 ± 1.1</cell></row><row><cell>+ TAM</cell><cell>✓</cell><cell>✗</cell><cell>288 ± 43</cell><cell>389 ± 65</cell><cell>8.4 ± 1.2</cell></row><row><cell>+ CBAM + Tab</cell><cell>✓</cell><cell>✓</cell><cell>271 ± 51</cell><cell>371 ± 99</cell><cell>7.7 ± 1.3</cell></row><row><cell>+ TAM + Tab</cell><cell>✓</cell><cell>✓</cell><cell>180 ± 32</cell><cell>237 ± 45</cell><cell>5.5 ± 0.1</cell></row><row><cell cols="2">+ TabAttention (ours) ✓</cell><cell>✓</cell><cell cols="2">170 ± 26 225 ± 37</cell><cell>5.0 ± 0.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement Sano No 857533 and the <rs type="funder">International Research Agendas programme of the Foundation for Polish Science</rs>, cofinanced by the <rs type="funder">European Union</rs> under the <rs type="funder">European Regional Development Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_frVhZzY">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AutoFB: automating fetal biometry estimation from standard ultrasound planes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bano</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-222" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sonographically estimated fetal weights: accuracy and limitation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Benacerraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Frigoletto</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1118" to="1121" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ultrasonic measurement of fetal abdomen circumference in the estimation of fetal weight</title>
		<author>
			<persName><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BJOG: Int. J. Obstet. Gynaecol</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="689" to="697" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">XGBoost: a scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939785" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction of pathological complete response to neoadjuvant chemotherapy in breast cancer using deep learning with integrative imaging, molecular and demographic data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Duanmu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-924" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting esophageal fistula risks using a multimodal selfattention network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimation of fetal weight with the use of head, body, and femur measurements-a prospective study</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Hadlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sharman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Deter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="337" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Endto-end learning of fused image and non-image features for improved breast cancer classification from MRI</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rahbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alessio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3294" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seyyedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">136</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint classification and regression via deep multi-task multi-channel learning for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1195" to="1206" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble machine learning for estimating fetal weight at varying gestational age</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9522" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BabyNet: residual transformer module for birth weight prediction on fetal ultrasound video</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-834" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning fetal ultrasound video model match human observers in biometric measurements</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45013</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining 3D image and tabular data via the dynamic affine feature map transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-366" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prediction of birth weight by ultrasound in the third trimester</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Pressman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bienstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Blakemore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="502" to="506" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ISUOG practice guidelines: ultrasound assessment of fetal biometry and growth</title>
		<author>
			<persName><forename type="first">L</forename><surname>Salomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="715" to="723" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of clinical and ultrasonic estimation of fetal weight</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tovbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bukovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Obstet. Gynecol</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="217" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fetal birthweight prediction with measured data by a temporal machine learning method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Informa. Decis. Making</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A spatiotemporal multi-stream learning framework based on attention mechanism for automatic modulation recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digit. Signal Process</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page">103703</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frequency and temporal convolutional attention for textindependent speaker recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6794" to="6798" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
