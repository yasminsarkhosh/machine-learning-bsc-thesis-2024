<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retinal Thickness Prediction from Multi-modal Fundus Photography</title>
				<funder ref="#_yHy2tAB">
					<orgName type="full">Institute for Intelligent Healthcare, Tsinghua University</orgName>
				</funder>
				<funder ref="#_rApMJpW">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_JVxhXRB">
					<orgName type="full">Tsinghua-Foshan Innovation Special Fund</orgName>
				</funder>
				<funder ref="#_uHqF2wh">
					<orgName type="full">Beijing Municipal Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_SAbjPzm #_8BnE9mm">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yihua</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Future Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seongho</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution" key="instit1">Kangbuk Samsung Hospital</orgName>
								<orgName type="institution" key="instit2">Sungkyunkwan University School of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><forename type="middle">Xing</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Ophthalmology and Visual Sciences Key Laboratory</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Ophthalmology</orgName>
								<orgName type="institution" key="instit2">Beijing Tongren Hospital</orgName>
								<orgName type="institution" key="instit3">Capital University of Medical Science</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Ophthalmology and Visual Sciences Key Laboratory</orgName>
								<orgName type="institution" key="instit1">Beijing Institute of Ophthalmology</orgName>
								<orgName type="institution" key="instit2">Beijing Tongren Hospital</orgName>
								<orgName type="institution" key="instit3">Capital University of Medical Science</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tien</forename><forename type="middle">Yin</forename><surname>Wong</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Tsinghua Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">National Eye Centre</orgName>
								<orgName type="institution">Singapore Eye Research Institute</orgName>
								<address>
									<country>Singapore, Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongen</forename><surname>Liao</surname></persName>
							<email>liao@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Su</forename><forename type="middle">Jeong</forename><surname>Song</surname></persName>
							<email>sjsong7@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution" key="instit1">Kangbuk Samsung Hospital</orgName>
								<orgName type="institution" key="instit2">Sungkyunkwan University School of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retinal Thickness Prediction from Multi-modal Fundus Photography</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="585" to="595"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8ED8CE220E6E7BBFF69A640F02BFF3C9</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Retinal thickness prediction</term>
					<term>Multi-modality</term>
					<term>Transformer</term>
					<term>Color fundus photography</term>
					<term>Infrared fundus photography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinal thickness map (RTM), generated from OCT volumes, provides a quantitative representation of the retina, which is then averaged into the ETDRS grid. The RTM and ETDRS grid are often used to diagnose and monitor retinal-related diseases that cause vision loss worldwide. However, OCT examinations can be available to limited patients because it is costly and time-consuming. Fundus photography (FP) is a 2D imaging technique for the retina that captures the reflection of a flash of light. However, current researches often focus on 2D patterns in FP, while its capacity of carrying thickness information is rarely explored. In this paper, we explore the capability of infrared fundus photography (IR-FP) and color fundus photography (C-FP) to provide accurate retinal thickness information. We propose a Multi-Modal Fundus photography enabled Retinal Thickness prediction network (M 2 FRT). We predict RTM from IR-FP to overcome the limitation of acquiring RTM with OCT, which boosts mass screening with a cost-effective and efficient solution. We first introduce C-FP to provide IR-FP with complementary thickness information for more precise RTM prediction. The misalignment of images from the two modalities is tackled by the Transformer-CNN hybrid design in M 2 FRT. Furthermore, we obtain the ETDRS grid prediction solely from C-FP using a lightweight decoder, S. J. Song and H. Liao are the co-corresponding authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retinal thickness map (RTM), generated from optical coherence tomography (OCT) volumes, provides a quantitative representation of various retina pathologic conditions <ref type="bibr" target="#b2">[3]</ref>. The ETDRS grid is an array comprising nine values representing the averaged thickness in nine regions in RTM <ref type="bibr" target="#b4">[5]</ref>. The RTM and ETDRS grid, are widely employed diagnostic and monitoring techniques for retinal disorders including age-related macular degeneration, glaucoma, and diabetic retinopathy <ref type="bibr" target="#b13">[14]</ref>, which are prevalent causes of visual impairment worldwide <ref type="bibr" target="#b5">[6]</ref>. On the other hand, OCT has been a critical diagnostic tool in ophthalmology due to its exceptional sensitivity and precision in identifying major eye diseases.</p><p>However, OCT exams are only available to limited patients as it is both costly and time-consuming, which impedes the acquisition of RTM and ETDRS grid. The recent advances in deep learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> have prompted research efforts aimed at addressing this limitation. There have been attempts to predict centerinvolving macular edema from color fundus photographs (C-FP) <ref type="bibr" target="#b16">[17]</ref>. Although these studies showed high sensitivity and specificity, they only provided a binary classification for the presence of macular edema. The lack of quantitative retina thickness prediction results mandated further study.</p><p>Fundus photography (FP) is widely used to image the retina, which captures the reflected signal of emitted signal from the retinal surface with a flash of light <ref type="bibr" target="#b12">[13]</ref>. As the retina is partially-transparent, a minority of light would pass through the surface <ref type="bibr" target="#b18">[19]</ref> and reflect back, which might carry information about the retinal thickness. This hypothesis motivates us to explore the connection between the RTM/ETDRS grid and the IR-FP/C-FP, which is rarely explored. Nonetheless, the FPs hold substantial clinical value in facilitating large-scale screening by acquiring RTM and ETDRS grid much faster and more affordable.</p><p>Recently, Holmberg et al. <ref type="bibr" target="#b9">[10]</ref> presented DeepRT, a convolutional neural network (CNN) designed for predicting retinal thickness using only infrared fundus photographs (IR-FP), disregarding C-FP. Exploring the capacity of C-FP to provide depth information has two major advantages: 1) More precise RTM prediction: Different from IR-FP, C-FP is acquired using light of multiple wavelengths that penetrate different depths in the retina <ref type="bibr" target="#b18">[19]</ref>. We assume that this can provide richer thickness information, which can lead to more precise RTM prediction when combined with IR-FP; 2) Clinical significance: C-FP is the most commonly used diagnostic tool in ophthalmology, and can be obtained even using a smartphone <ref type="bibr" target="#b6">[7]</ref>. The ability to derive thickness information from C-FP alone, without OCT scans, will make C-FP a potential tool for high functioning telemedicine platform which has the ability to diagnose, monitor treatment response, and even screen high-risk patients for diabetic macular edema (DME). In this paper, we explore the capability of IR-FP and C-FP to provide accurate retinal thickness information, with a cohort of patients with DME of different grades. We propose a Multi-Modal Fundus photography enabled Retinal Thickness prediction network (M 2 FRT). It is comprised of two separate encoders, a CNN E C and a Transformer E T , that encode localized information and rich depth information form IR-FP and C-FP respectively. We utilize the features extracted from E C to facilitate the learning process of E T in gathering 2D aligned thickness information via its attention mechanism. The enriched features are subsequently fed into a decoder to predict the RTM.</p><p>Furthermore, we obtain the ETDRS grid prediction, i.e. nine values representing averaged thickness in the predefined areas in Fig. <ref type="figure" target="#fig_1">1 (c 2</ref> ), solely from the C-FP by processing the features extracted from E T through another lightweight decoder, which has significant clinical implications. To the best of our knowledge, we are the first to demonstrate the benefit of C-FP for RTM prediction and derive the ETDRS grid prediction solely from C-FP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this study, we exclusively concentrate on DME to explore the predictive capacity of FPs regarding the retinal thickness. The rationale behind this is that, apart from DME, predicting retinal thickness itself has relatively less clinical value. For example, for age-related macular degeneration, the ophthalmologist needs to look for subtle changes in abnormal OCT features (e.g. subretinal fluid, pigmentary epithelial detachments <ref type="bibr" target="#b15">[16]</ref>), rather than just the retinal thickness.</p><p>In standard clinical settings, the ophthalmologist will acquire the C-FP upon patients' arrival. If RTM is deemed necessary for diagnosis, a separate device will capture IR-FP and conduct OCT scanning. Figure <ref type="figure" target="#fig_0">1</ref> (a) illustrates the acquisition process of RTM using OCT, where each B-scan is registered with the 2D positions in IR-FP. The ETDRS grid is an array comprising nine values indicating the average thickness (μm) in nine predefined regions in RTM (Fig. <ref type="figure" target="#fig_0">1</ref> (c 2 )).</p><p>Dovetailed with the clinical settings, M 2 FRT aims to predict the RTM corresponding to the IR-FP, utilizing enriched depth information from pre-collected C-FP. The RTM requires precise pixel-wise correspondences to the IR-FP, while the ETDRS grid is a regional concept. Therefore, we can manage to derive an ETDRS grid prediction using only easier acquired C-FP, even in the absence of IR-FP, which holds importance within clinical scenarios and telemedicine.</p><p>As mentioned above, the FPs from the two modalities are captured by different machines. So, the FPs are not registered and have a distinct field of view (FoV). The recent advances in vision Transformers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref> have inspired us to address this challenge, because the multi-head attention mechanism is location-agnostic, but rather leverages patch embedding and position encoding to introduce positional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>The overall pipeline of M 2 FRT is presented in Fig. <ref type="figure" target="#fig_0">1 (b</ref>). The notations used for the images in the modality of IR-FP and C-FP are I IR-F P and I C-F P , respectively. The objective is to predict the thickness map M in the FoV of I IR-F P and ETDRS grid G, which represents the central area of the retina and is the major concern in clinical practices.</p><p>The convolution and concatenation pose "hard" operations on the spatial dimensions. Thus, whether we concatenate I IR-F P and I C-F P as input or in the feature space under a CNN backbone, the misalignment of I IR-F P and I C-F P will deteriorate the performance for M prediction. In contrast, the spatial information is "softly" incorporated into the Transformer architecture, where the subsequent operations in the feature space are location-agnostic.</p><p>Therefore, we utilize a CNN encoder E C from U-Net <ref type="bibr" target="#b14">[15]</ref> to extract features from I IR-F P , and a Transformer encoder E T from 2D ViT/UNETR <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> to extract features from I C-F P . Notably, the deep features extracted by E T are spatially perturbed. M 2 FRT leverages attention mechanisms in E T to gather 2D aligned thickness information from I C-F P , guided by the features extracted from I IR-F P by E C . The extracted multi-level features from I IR-F P and I C-F P are denoted as f IR-F P and f C-F P respectively, as shown in the following equations:</p><formula xml:id="formula_0">f IR-F P = E C (I IR-F P ), f C-F P = E T (I C-F P ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder</head><p>M 2 FRT extracts 2D aligned depth information from C-FP, which enrich the depth representations acquired from IR-FP in an end-to-end learning manner.</p><p>The extracted features are fused by concatenation and passed to the decoder D M to generate the thickness map prediction M , where</p><formula xml:id="formula_1">M = D M (f IR-F P , f C-F P ).</formula><p>With fine-grained thickness information extracted for the RTM prediction task, the encoded features obtained from E T are ready to be decoded to predict the ETDRS grid using a lightweight decoder D G . In D G , the features from multiple levels are combined using a series of convolutions and concatenations. Then the final prediction for G is generated by a linear projection. The predicted ETDRS grid is denoted as G , where G = D G (f C-F P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Functions</head><p>The loss functions L M 1 and L G 1 are employed in the prediction of the RTM and ETDRS grid using L 1 criteria, respectively, as shown in the following equations,</p><formula xml:id="formula_2">L M 1 = M -M 1 , L G 1 = 1 9 9 i=1 G (i) -G (i) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where G (i) and G (i) are the i-th number in the ETDRS grid ground truth G and prediction G . The final loss function is</p><formula xml:id="formula_4">L = L M 1 + L G 1 .</formula><p>3 Experiments Implementation Details. The M 2 FRT is implemented with PyTorch <ref type="bibr" target="#b1">[2]</ref> and MONAI <ref type="bibr" target="#b0">[1]</ref>, and detailed configurations are in the supplementary material. Random flipping and rotation are utilized for data augmentation. We use the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with (β 1 , β 2 ) = (0.9, 0.999) for training for 300 epoches.</p><p>The initial learning rate is 0.001 and exponentially decayed with γ = 0.999.</p><p>Performance Metrics. For the RTM predictions, we use mean absolute error (MAE) and peak signal-to-noise ratio (PSNR) for evaluation in the areas G 1,2,3 as shown in Fig. <ref type="figure" target="#fig_0">1 (c 1</ref> ), where the peak signal is set to 800μm. For the ETDRS grid predictions, we calculate the MAE of the predictions of the nine grids, as shown in Fig. <ref type="figure" target="#fig_1">1 (c 2</ref> ). For the right eye, the grid must be mirrored horizontally, i.e.,</p><formula xml:id="formula_5">G 2 3 ↔ G 4 3 and G 2 2 ↔ G 4 2 .</formula><p>The Wilcoxon signed-rank test is employed to compare the performance of M 2 FRT with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative and Qualitative Evaluations on RTM Predictions</head><p>To better illustrate the problem and our solution, we begin with the most concise design, U-Net <ref type="bibr" target="#b14">[15]</ref>. In Table <ref type="table" target="#tab_0">1</ref>, the MAE/PSNR for U-Net with IR-FP as input are 27.84 µm/27.38 dB. By concatenating multi-modal IR-FP and C-FP as input to the U-Net, the performance improved to 25.16 µm/28.36 dB, indicating that C-FP has the potential of containing additional thickness information.</p><p>However, the multi-modal FPs are unregistered and have a distinct FoV, in which case a mere concatenation of these inputs would diminish the network's capacity to effectively exploit the thickness information from paired 2D positions. A simple solution is to encode the multi-modal FPs with two separated convolutional encoders E C , where the features are deeply fused along the downsampling path. The unregistered problem is eased by the higher-level features with a larger receptive field, and the MAE/PSNR are improved to 24.80 µm/28.54 dB.</p><p>After all, 2D convolution and concatenation pose a "hard" operation to the spatial dimensions, which is still interfered with by the unregistered problem. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, in (b) and (c), there are artifacts in the RTM predictions caused by the annular boundary (grey arrows) and misaligned vessels from C-FP (purple arrows). On the contrary, the attention operations in Transformer are location-agnostic, where the spatial information is more "softly" introduced into the network by patch embedding and position encoding <ref type="bibr" target="#b3">[4]</ref>.</p><p>Therefore, we employ distinct encoders of a CNN E C and a Transformer E T to IR-FP and C-FP respectively. The attention mechanism in E T is encouraged to gather 2D aligned thickness information from the perturbed patch embeddings, with the guidance from the decoder D M and the L M 1 loss function. With this CNN-Transformer hybrid design, the MAE/PSNR performance are  <ref type="table" target="#tab_0">1</ref>, and the network produced the best visual quality and smaller errors in Fig. <ref type="figure" target="#fig_1">2</ref> (f) and (g).</p><p>Since IR-FP acts as a localizer for the OCT scan and RTM, spatially perturbing the features from IR-FP with E T is not appropriate for the accurate prediction of RTM, and thus not yielding better quantitative results, as shown in Table <ref type="table" target="#tab_0">1</ref>. In Fig. <ref type="figure" target="#fig_1">2 (d)</ref>, the annular boundary artifacts from C-FP still exist (grey arrows). When both encoders are substituted by E T , in Fig. <ref type="figure" target="#fig_1">2</ref> (e), the 2D localizing information is degraded, in which case, there will be artifacts caused by the patch embedding (brown arrows).</p><p>Our proposed M 2 FRT utilizes a combination of multi-modal IR-FP and C-FP to predict the RTM. M 2 FRT outperforms the state-of-the-art (SOTA) RTM prediction technique, DeepRT <ref type="bibr" target="#b9">[10]</ref>, which uses mono IR-FP as input. Besides, methods with multi-modal FPs surpass methods with mono IR-FP as input, especially in the central G 1 area, as shown in Table <ref type="table" target="#tab_0">1</ref> and pink arrows in Fig. <ref type="figure" target="#fig_1">2</ref>. The results demonstrate that C-FP has the ability to provide complementary depth information with IR-FP. The effectiveness of our methodology is validated through the ablation study on the encoders and decoders, as presented in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Additionally, when E T is guided to gather aligned features for RTM using the attention mechanism, the deep features from E T are ready to be decoded by D G for ETDRS grid predictions, which involves computing the averaged thickness in nine predefined regions. Notably, the ETDRS grid prediction task does not have a significant impact on the performance of the RTM prediction (the last two rows in Table <ref type="table" target="#tab_0">1</ref>), while the ETDRS grid prediction task can benefit from the supervision provided by the RTM prediction task, which will be discussed in Sect. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative Evaluations on ETDRS Grid Predictions</head><p>Following the clinical settings, we predict the full RTM based on the IR-FP localizer in place of the OCT scanning procedure, which can boost mass screening. We gather enriched thickness information from C-FP and improve the performance with a hybrid CNN-Transformer design, as elaborated in Sect. 3.2.</p><p>In addition to identifying 2D disease patterns in C-FP, predicting the ETDRS grid solely from C-FP can exploit additional information in the C-FP and hold significant clinical value for rapid diagnosis, especially in the field of telemedicine.</p><p>To achieve this, we can adopt a conventional learning-based method to predict the nine numbers in the ETDRS grid, i.e. ResNet <ref type="bibr" target="#b8">[9]</ref>, as shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>However, simply approximating the nine numbers will neglect the fine-grained thickness information. To address this issue, following the design in Sect. 3.2, the encoder E T for C-FP is guided by the encoder E C from the IR-FP part for detailed RTM predictions. Therefore, E T has been trained to extract fine-grained depth information from C-FP, which can be decoded for the averaged thickness for ETDRS grid predictions with D G . The fine-grained thickness supervision from the RTM prediction task can benefit the ETDRS grid prediction task, as shown in the last two rows of Table <ref type="table" target="#tab_1">2</ref>. Besides, our proposed M 2 FRT outperforms its ablation and other baselines, as shown in Table <ref type="table" target="#tab_1">2</ref>. We can also observe from Table <ref type="table" target="#tab_0">1</ref> and 2 that the central thickness in G 1 area is more challenging to predict than the surrounding area for the RTM and ETDRS grid prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we demonstrate the advantages of leveraging multi-modal information from C-FP for RTM prediction with respect to IR-FP, which overcomes the limitations of OCT and has the potential to enhance mass screening. Additionally, we propose a novel method for predicting the ETDRS grids solely from C-FP, which has significant clinical importance for fast diagnosis, telemedicine, etc. Our results indicate that additional fine-grained supervision from the RTM prediction task is beneficial for ETDRS grid prediction, where the ETDRS grid is decoded from the encoder of C-FP by a lightweight decoder during the training procedure of the RTM prediction task. Further research could be conducted for: 1) Predicting RTM of multiple retinal layers simultaneously, and 2) Improving RTM prediction's resolution and detail by acquiring finer OCT as ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) M 2 FRT predicts the RTM with enhanced thickness information from mutimodal FPs without OCT scans, and the ETDRS grid can be predicted with C-FP only. (b) M 2 FRT utilizes Transformer/CNN hybrid encoders ET /EC for C-FP/IR-FP, to tackle the unregistered issue and extract 2D aligned thickness information in an end-to-end learning manner. A CNN decoder DM is employed to predict the RTM. Besides, a lightweight decoder DG is designed to predict the ETDRS grid base on the information from C-FP only, which is guided by the RTM prediction task during training. (c1) Areas for evaluations in the RTM prediction task, and (c2) in the ETDRS grid (left eye) prediction task. In (c1), G3 is the remaining area of G1,2 in RTM.</figDesc><graphic coords="3,72,96,125,45,306,28,145,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The use of IR-FP alone to predict RTM will cause larger errors, particularly in the central area (pink arrows). Artifacts can be generated by the annular boundary (grey arrows) and vessels (purple arrows) from C-FP when misaligned information is roughly fused. Additionally, the dual ET design weakens the localized 2D correspondence, where patch embedding can generate artifacts (brown arrows). Our proposed methods (f) and (g), with EC and ET extracting features for IR-FP and C-FP respectively, better leverage 2D aligned thickness information and lead to lower errors. (Color figure online)</figDesc><graphic coords="7,72,96,82,55,306,76,189,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A total of 967 retinal images were gathered from 361 distinct patients diagnosed with DME of different grades who underwent intravitreal injections. The dataset is collected in Kangbuk Samsung Hospital (IRB Approval Number: KBSMC 2022-12-016-001) between 2020 and 2021. The averaged retinal thickness (μm) in the dataset is 275.92 ± 20.91 (mean ± std.). For each patient, 31 B-scans are obtained by a Heidelberg OCT device, which are used to calculate the retinal thickness between the internal limiting membrane and the Bruch's membrane. The segmentations of the membrane layers are directly exported from the OCT machine. Images with poor fixation or OCTs with major segmentation errors are excluded by an experienced ophthalmologist. Quantitative comparison of different methods for RTM prediction, with MAE (µm) and PSNR (dB). The top-2 methods are highlighted in bold and underlined. By incorporating multi-modal FP as input, networks can access more comprehensive thickness information, resulting in improved performance. The most efficient way to tackle the unregistered problem is to utilize encoders EC and ET for IR-FP and C-FP respectively. Asterisks indicate M 2 FRT outperforms the baselines with p-values&lt;0.01.</figDesc><table><row><cell>3.1 Experimental Setup</cell></row><row><cell>Dataset.</cell></row></table><note><p>Data Pre-processing. For IR-FP, we center-crop the area corresponding to the OCT scanning area with a resolution of 544 × 544, and then calculate RTM Enc.: Encoder; Dec.: Decoder. M 2 FRT is comprised of EC , ET , DM , DG. ground truth within. With respect to the B-scans, the retinal thickness is calculated for 31 lines in the 2D IR-FP, and then linearly interpolated to match the resolution of IR-FP. For C-FP, we resize it to 544 × 544 from an original resolution of 3608 × 3608. The dataset is randomly split into training and test datasets at the patient level. The training/test dataset consisted of 657/310 images from 252/109 patients, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of different methods with MAE (µm). Our method incorporates pixel-wise supervision from the RTM prediction branch, and improves the MAE results. Asterisks indicate M 2 FRT outperforms the baselines with p-values&lt;0.05.</figDesc><table><row><cell>Methods</cell><cell cols="3">Mean Absolute Error (µm)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ETDRS Grid G1</cell><cell>G 1 2</cell><cell>G 2 2</cell><cell>G 3 2</cell><cell>G 4 2</cell><cell>G 1 3</cell><cell>G 2 3</cell><cell>G 3 3</cell><cell>G 4 3</cell></row><row><cell>ResNet-50 [9]</cell><cell>25.12*</cell><cell cols="8">37.46* 27.22 29.73* 25.89 26.43* 19.12* 22.27 17.65 20.36*</cell></row><row><cell>ResNet-101 [9]</cell><cell>25.06*</cell><cell cols="8">36.41* 27.05 29.29 25.76 26.18* 19.32 22.60* 17.86 21.11*</cell></row><row><cell>ET , DG</cell><cell>24.42*</cell><cell cols="8">34.88 26.70 29.29* 25.63 25.13 18.76 22.18* 17.38 19.85*</cell></row><row><cell cols="2">ET , DG, EC , DM 23.84</cell><cell cols="8">34.36 26.15 28.24 25.25 24.53 18.50 21.42 17.44 18.71</cell></row><row><cell cols="4">M 2 FRT is comprised of ET , DG, EC , DM .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">improved to 23.82 µm/28.91 dB in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The authors acknowledge supports from <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFC2405200</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">82027807</rs>, <rs type="grantNumber">U22A2051</rs>), <rs type="funder">Beijing Municipal Natural Science Foundation</rs> (<rs type="grantNumber">7212202</rs>), <rs type="funder">Institute for Intelligent Healthcare, Tsinghua University</rs> (<rs type="grantNumber">2022ZLB001</rs>), and <rs type="funder">Tsinghua-Foshan Innovation Special Fund</rs> (<rs type="grantNumber">2021THFS0104</rs>). We would like to thank <rs type="person">Hee Guan Khor</rs> for discussions on experiments and writing, and <rs type="person">Zhuxin Xiong</rs> for discussions on data pre-processing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rApMJpW">
					<idno type="grant-number">2022YFC2405200</idno>
				</org>
				<org type="funding" xml:id="_SAbjPzm">
					<idno type="grant-number">82027807</idno>
				</org>
				<org type="funding" xml:id="_8BnE9mm">
					<idno type="grant-number">U22A2051</idno>
				</org>
				<org type="funding" xml:id="_uHqF2wh">
					<idno type="grant-number">7212202</idno>
				</org>
				<org type="funding" xml:id="_yHy2tAB">
					<idno type="grant-number">2022ZLB001</idno>
				</org>
				<org type="funding" xml:id="_JVxhXRB">
					<idno type="grant-number">2021THFS0104</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://monai.io/" />
		<title level="m">Medical open network for artificial intelligence (MONAI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optical coherence tomography: a guide to interpretation of common macular diseases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="35" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Early Treatment Diabetic Retinopathy Study Research Group: grading diabetic retinopathy from stereoscopic color fundus photographs-an extension of the modified airlie house classification: ETDRS report number 10</title>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="786" to="806" />
			<date type="published" when="1991">1991</date>
			<pubPlace>Supplement</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global causes of blindness and distance vision impairment 1990-2020: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Glob. Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1221" to="e1234" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple, inexpensive technique for highquality smartphone fundus photography in human and animal eyes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Haddock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ophthalmol</title>
		<imprint>
			<biblScope unit="page">518479</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised retinal thickness prediction enables deep learning from unlabelled data to boost classification of diabetic retinopathy</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Holmberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="719" to="726" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transforming medical imaging with transformers? a comparative review of key properties, current progresses, and future perspectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102762</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fundus photography in the 21st century-a review of recent technological advances and their implications for worldwide healthcare</title>
		<author>
			<persName><forename type="first">N</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richhariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telemedicine and e-Health</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhanced grid-based visual analysis of retinal layer thickness with optical coherence tomography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Röhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Prakasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stüwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-NET: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention -MICCAI</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pigment epithelial detachment followed by retinal cystoid degeneration leads to vision loss in treatment of neovascular age-related macular degeneration</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Deak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="822" to="832" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting optical coherence tomography-derived diabetic macular edema grades from fundus photographs using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Varadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Biomedical Optics: Principles and Imaging</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="820" to="838" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of Medical Image Computing and Computer Assisted Intervention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<editor>Stoyanov, D., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<idno>DLMIA/ML-CDS -2018</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
