<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN</title>
				<funder ref="#_BJY6UY5">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_kwa2b73">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_dhNcQBj">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_ZerFSkQ">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_rapjvpW">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Dr8z9gp #_YcprX32">
					<orgName type="full">Applied Basic Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhaojie</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HDU-ITMO Joint Institute</orgName>
								<orgName type="institution" key="instit2">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HDU-ITMO Joint Institute</orgName>
								<orgName type="institution" key="instit2">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengxue</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Eye Hospital</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>518040</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangting</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Eye Hospital</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>518040</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaochong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Eye Hospital</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>518040</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Elazab</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gangyong</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiquan</forename><surname>Ge</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<postCode>310018</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changmiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Medical Big Data Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Research Institure of Big Data</orgName>
								<address>
									<postCode>518172</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="745" to="755"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">989ED1F7A943ECD52906E6D2910D6C1C</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fluorescein Angiography</term>
					<term>Cross-modality Image Generation</term>
					<term>Ultra-Wide-angle Fundus Imaging</term>
					<term>Conditional Generative Adversarial Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fundus photography is an essential examination for clinical and differential diagnosis of fundus diseases. Recently, Ultra-Wide-angle Fundus (UWF) techniques, UWF Fluorescein Angiography (UWF-FA) and UWF Scanning Laser Ophthalmoscopy (UWF-SLO) have been gradually put into use. However, Fluorescein Angiography (FA) and UWF-FA require injecting sodium fluorescein which may have detrimental influences. To avoid negative impacts, cross-modality medical image generation algorithms have been proposed. Nevertheless, current methods in fundus imaging could not produce high-resolution images and are unable to capture tiny vascular lesion areas. This paper proposes a novel conditional generative adversarial network (UWAT-GAN) to synthesize UWF-FA from UWF-SLO. Using multi-scale generators and a fusion module patch to better extract global and local information, our model can generate high-resolution images. Moreover, an attention transmit module is proposed to help the decoder learn effectively. Besides, a supervised approach is used to train the network using multiple new weighted losses on different scales of data. Experiments on an in-house UWF image dataset demonstrate the superiority of the UWAT-GAN over the state-of-the-art methods. The source code is available at: https://github.com/Tinysqua/ UWAT-GAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fluorescein Angiography (FA) is a commonly utilized imaging modality for detecting and diagnosing fundus diseases. It is widely used to image vascular structures and dynamically observe the circulation and leakage of contrast agents in blood vessels. Recently, the emergence of Ultra-Wide-angle Fundus (UWF) imaging has enabled its combination with FA and Scanning Laser Ophthalmoscopy (SLO), namely UWF-FA and UWF-SLO. The UWF-FA imaging enables simultaneous and high-contrast angiographic images of all 360 • C of the mid and peripheral retina <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>. However, both FA and UWF-FA require injecting a fluorescent dye (i.e., sodium fluorescein) into the anterior vein of the patient's hand or elbow, which then passes through the blood circulation to the fundus blood vessels. Some patients may experience adverse reactions such as vomiting and nausea during or after the examination. Moreover, it is not suitable for patients with serious cardiovascular and other systemic diseases.</p><p>Cross-modality medical image generation provides a new method for solving the aforementioned problems. Multi-scale feature maps from different input modalities usually have similar structures. Hence, different contrasts can be merged to generate target images based on multimodal deep learning to provide more information for diagnosis <ref type="bibr" target="#b14">[15]</ref>. Recently, the generative adversarial networks (GANs) <ref type="bibr" target="#b4">[5]</ref> and their variants have made breakthroughs in this field. The idea of PatchGAN <ref type="bibr" target="#b11">[12]</ref> was proposed to synthesize clearer images. Liu et al. <ref type="bibr" target="#b13">[14]</ref> proposed an end-to-end multi-input and multi-output deep adversarial learning network for MR image synthesis. Xiao et al. <ref type="bibr" target="#b21">[22]</ref> proposed a Transfer-GAN model by combining transfer learning and GANs to generate CT highresolution images. By merging multi-scale generators, these networks can explore fine and coarse features from images <ref type="bibr" target="#b9">[10]</ref>. Kamran et al. <ref type="bibr" target="#b8">[9]</ref> proposed a semisupervised model called VTGAN introducing transformer module into discriminators, helping the synthesis of vivid images. However, previous methods yielded lower-resolution situations and most discriminators can only take squared inputs (width equal to height) <ref type="bibr" target="#b17">[18]</ref>. Moreover, misaligned data and lower attention on disease-related regions as well as the correctness of synthesized lesions remain significant issues. Furthermore, the highly non-linear relationship between different modalities makes the mapping from one modality to another difficult to learn <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this paper, we present the Ultra-Wide-Angle Transformation GAN (UWAT-GAN), a supervised conditional GAN capable of generating UWF-FA from UWF-SLO. To address the image misalignment issue, we employ an automated image registration method and integrate the idea of pix2pixHD <ref type="bibr" target="#b19">[20]</ref> to use multi-scale discriminators. In addition, we use the multi-scale generators to synthesize high-resolution images as well as improve the ability to capture tiny vascular lesion areas and employ multiple new weighted losses on different scales of data to optimize model training. For evaluation metrics, we use Fréchet Inception Distance (FID) <ref type="bibr" target="#b5">[6]</ref>, Kernel Inception Distance (KID) <ref type="bibr" target="#b10">[11]</ref>, Inception Score (IS) <ref type="bibr" target="#b1">[2]</ref> and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b23">[24]</ref> to quantify the quality of images. Finally, we compare UWAT-GAN with the state-of-the-art image synthesis frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> for qualitative assessment and conduct an ablation study. Our main contributions are summarised as follows:</p><p>1). To the best of our knowledge, we present the first study to synthesize UWF-FA from UWF-SLO, overcoming the limitations of UWF-FA imaging. 2). We propose a novel UWAT-GAN utilizing multi-scale generators and multiple new weighted losses on different data scales to synthesize high-resolution images with the ability to capture tiny vascular lesion areas. 3). We assess the performance of the UWAT-GAN on a clinical in-house dataset and adopt an effective preprocessing method for image sharpening and registration to enhance the clarity of vascular regions and tackle the misalignment problem. 4). We demonstrate the superiority of the proposed UWAT-GAN against the state-of-the-art models through extensive experiments, comparisons, and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We propose a supervised conditional GAN for synthesizing UWF-FA from UWF-SLO images, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In order to achieve the desired outcome, we propose the concept of a fine-coarse level generator in whole architecture (Sect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1) and a fusion module works on result of both level generators (Sect. 2.2).</head><p>Then the Attention Transmit Module is put forward to improve the U-Net-like architecture (Sect. 2.3). Additionally, we provide a comprehensive description of up-down sampling process and architecture of multi-scaled discriminators (Sec. 2.4). Eventually, we discuss the proposed loss function terms and their impacts in detail (Sect. 2.5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>UWF-FA has global features such as eyeballs and long-thick blood vessels, as well as local features such as small lesions and capillary blood vessels. However, generating images with both global and local features using a single generator is challenging. To address this issue, we devise two different levels of generators.</p><p>The coarse generator Gen C extracts global information and generates a result based on this information, while the fine generator Gen F extracts local information. The results of global and local information can be used, alternately, as a reference for each other. Hence, this allows the extraction and utilization of both global and local information. In Fig. <ref type="figure" target="#fig_0">1</ref>, the original image is fed into the entire model. After down-sampling, the image is passed into Gen C . Then, we extract a patch from the original image as an input to Gen F as described in Sect. 2.3. Both generators share the down-sample residual block and attention concatenated modules. Note that both generators generate a UWF-FA image and pass it to the discriminators. However, the output of Gen F is the one we considered in the later experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Patch and Fusion Module</head><p>In Fig. <ref type="figure" target="#fig_0">1</ref>, Gen F receives a randomly cropped patch as the input instead of the original image. This is because directly inputting the original image would occupy a large amount of memory and significantly reduce the training speed. Therefore, we only feed one cropped patch of the original image to Gen F in each step. In Fig. <ref type="figure" target="#fig_1">2</ref>(A), a fusion block takes both patches from Gen F and Gen C . To get the same region from Gen F and Gen C , we resized the images to the same size, and cropped the patches from the same position. These two patches are concatenated at the depth level and passed into a two-layer fusion operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Transmit Module</head><p>In Fig. <ref type="figure" target="#fig_1">2</ref>, the attention module is designed based on U-Net-like <ref type="bibr" target="#b15">[16]</ref> architecture, whose sampling process can provide more information to the decoder. Whereas, when synthesizing UWF-FA from UWF-SLO, the information density of the source image is low. For instance, the eye sockets in the periphery of the image are sparsely distributed blood vessels in some areas. Therefore, completely passing the graphs from the down-sampling process to the up-sampling process is not appropriate. Subsequently, images that pass through Attention Transmit Module can first extract useful information so that the decoder uses this information, efficiently. The multi-head attention <ref type="bibr" target="#b18">[19]</ref> and the CNN-Attention blocks are shown in Fig. <ref type="figure">3</ref>. </p><formula xml:id="formula_0">Element</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Generator and Discriminator Architectures</head><p>After conducting multiple experiments, we choose three down-sample layers for Gen F and two down-sample layers for Gen C . In addition, each generator includes an initial block, down-sample block, up-sample block, residual block, and attention transmit module, which are shown in Fig. <ref type="figure">3</ref>. The initial block contains a reflection padding, a 2D convolution layer, and the Leaky-Relu activation function. The down/up-sampling blocks consist of a 2D convolution/transposed layer and the activation function combined with normalization. Additionally, the multi-scale discriminator in pix2pixHD <ref type="bibr" target="#b19">[20]</ref> is employed to evaluate the output of the generator. For generator Gen X , the first discriminator D X1 takes the original and generated image P 1 while the second discriminator D X2 takes the down-sampled version of P 1 . Although theoretically, a multi-level discriminator can be applied by generating an image pyramid for an image, we use D C1 and D C2 for Gen C , and D F for Gen F in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Proposed Loss Function</head><p>Denote the two generators Gen C and Gen F as G C and G F , the three discriminators as D C1 , D C2 , and D F , and the paired variables {(c i , x i )}, where c represents the distribution of original input as a condition and x represents the distribution of ground truth (i.e., real UWF-FA image). Given the conditional distribution c, we aim to maximize the loss of D C1 , D C2 , and D F while minimizing the loss of Gen C and Gen F using the following objective function:</p><formula xml:id="formula_1">min GC max DC1,DC2 k=1,2 L cGAN (G C , D Ck ) + min GenF max DF L cGAN (G F , D F ),<label>(1)</label></formula><p>where L cGAN is given by:</p><formula xml:id="formula_2">E (c,x) [log(D(c, x))] + E c [log(1 -D(G(c), c)]. (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>We adopt the feature mapping (FM) loss <ref type="bibr" target="#b19">[20]</ref> in our framework. Firstly, we collect the target images and their translated counterparts as a pair of images. Then, we split the discriminators into multiple layers and obtain the output from each layer. Denote D (i) as the ith-layer to extract the feature, the loss function is then defined as:</p><formula xml:id="formula_4">L F M (G.D k ) = E (c,x) T i=1 1 N i [ D i k (c, x) -D i k (c, G(c)) 1 ],<label>(3)</label></formula><p>where T is the total number of layers and N i represents each layer's number of elements. (e.g., convolution, normalization, Leaky-Relu means three elements). Minimizing this loss ensures that each layer can extract the same features from the paired images. Additionally, we use the perceptual loss <ref type="bibr" target="#b7">[8]</ref> in our framework, which is utilized by a pretrained VGG19 network <ref type="bibr" target="#b16">[17]</ref>, to extract the features from the paired images and it is defined as:</p><formula xml:id="formula_5">L V GG (G.D k ) = N i=1 1 M i [ V i (c, x) -V i (c, G(c)) 1 ],<label>(4)</label></formula><p>where N represents the total number of layers, M i denotes the elements in each layer, and V i is the ith-layer of the VGG19 network. The final cost function is as follows:</p><formula xml:id="formula_6">min GC ( max DC1,DC2 k=1,2 L cGAN (G C , D Ck ) + λ F MC k=1,2 L F M (G C , D Ck ) + λ V GGC k=1,2 L V GG (G C , D Ck )) + min GF (max DF L cGAN (G F , D F ) + λ F MF L F M (G F , D F ) + λ V GGF L V GG (G F , D F )).<label>(5)</label></formula><p>where λ F MC , λ V GGC , λ F MF , λ V GGF indicate adjustable weight parameters.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation and Preprocess</head><p>In our experiments, we utilized an in-house dataset of UWF images obtained from a local hospital, comprising UWF-FA and UWF-SLO images. The UWF-SLO are in 3-channel RGB format, whereas the UWF-FA images are in 1-channel format. Each image pair was collected from a unique patient. However, from a clinical perspective, images taken with an interval of more than one day or those with noticeable fresh bleeding were excluded. Additionally, images that contain numerous interfering factors affecting their quality were also discarded. After the quality check, we have 70 paired images with the size of 3900 × 3072, of which 70% were randomly allocated for training and 30% for testing, respectively. Furthermore, we employed image sharpening through histogram equalization to enhance the clarity of images. We then utilized automated image registration software, i2k Retina Pro, to register each pair of images which changed the image size. To standardize the size of each image, we resized the registered images to 2432 × 3702. Subsequently, we randomly cropped the resized images with a size of 608 × 768 into different patches. And 50 patches could be obtained for each image. Finally, we adopted data augmentation using random flip and rotation to increase the number of training images from 49 pairs to 1960 pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All our experiments were conducted on the PyTorch 1.12 framework and carried out on two Nvidia RTX 3090Ti GPUs. Our model was trained from scratch to 200 epochs. The parameters were optimized by the Adam optimizer algorithm <ref type="bibr" target="#b12">[13]</ref> with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We used a batch size of 2 to train our model and set λ F MF = λ F MC = λ V GGF = λ V GGC = 10 (Eq. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons</head><p>We first compared the performance of our model with some state-of-the-art GAN-based models including: Pix2pix <ref type="bibr" target="#b6">[7]</ref>, Pix2pixHD <ref type="bibr" target="#b19">[20]</ref> and StarGAN-v2 <ref type="bibr" target="#b2">[3]</ref>. For a fair comparison, we took the default parameters of the open-source codes of the competing methods, ensuring that the data volume matched the number of training cycles. We used the F ID(↓), KID(↓), LP IP S(↓) and IS(↑) to evaluate the generated UWF-FA. Table <ref type="table" target="#tab_1">1</ref> shows the generation performance of different methods. Overall, our method achieves the best in all metrics compared to other models. The Pix2Pix attained the worst performance in all evaluation metrics while PixPixHD and StarGAN had comparable performance. In general, our method outperformed the competing methods and improved FID, KID, IS, and LPIPS by at least 24.47%, 39.95%, 3.59%, and 14.04%, respectively. Although StarGAN-v2 yielded the second-best performance, it is still less comparable with the proposed UWAT-GAN due to the lesion generation module which could capture tiny image details and improve overall performance (see Fig. <ref type="figure" target="#fig_2">4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To evaluate the significance of the attention transmit module proposed in UWAT-GAN, we trained our model with and without this module, namely M A and M NA , respectively. Unlike the generated images of M A , we found that M NA was not so distinctive as some vessels were missing and some interference of eyelashes was incorrectly considered as vessels. In Fig. <ref type="figure" target="#fig_2">4</ref>, we showed the original pair of UWF-SLO and UWF-FA images and the generated images with M A and M NA . It is clear that the proposed method can generate good images and preserve small details. It becomes more distinctive when the attention module was used, as shown in the enlarged view of the red rectangle. It is also obvious that the FID and KID scores were improved by 22.25% and 18.46%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>To address the potential adverse effects of fluorescein injection during FA, we propose UWAT-GAN to synthesize UWF-FA from UWF-SLO. Our method can generate high-resolution images and enhance the ability to capture small vascular lesions. Comparison and ablation study on an in-house dataset demonstrate the superiority and effectiveness of our proposed method. However, our model still has a few limitations. First, not every pair of images can be registered since some paired images may have fewer available features, making registration difficult. Second, our model's accuracy in synthesizing very tiny lesions is not optimal, as some lesions cannot be well generalized. Third, the limited size of our dataset is relatively small and may affect the model performance. In the future, we aim to expand the size of our dataset and explore the use of the object detection model, especially for small targets, to push our model pay more attention to some lesions. After further validation, we aim to adopt this method as an auxiliary tool to diagnose and detect fundus diseases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall framework of UWAT-GAN.</figDesc><graphic coords="3,63,00,84,11,234,28,81,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Details of two-level generators and their building blocks. (A) GenF consists of two down and up processes, and ultimately sends its resulting patch to GenC ; (B) GenC comprises of three down and up processes before sending its resulting patch to GenF .</figDesc><graphic coords="4,41,79,53,72,340,57,122,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of original and generated images. From left to right: source UWF-SLO, UWF-FA, the proposed framework with and without the attention transmit module, and the pix2pixHD, respectively.</figDesc><graphic coords="8,70,29,182,57,283,57,113,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art methods using 4 evaluation metrics. The * means that the official code hasn't provided the way to measure it.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="3">FID(↓) KID(↓) IS(↑) LPIPS(↓)</cell></row><row><cell>Pix2Pix [7]</cell><cell></cell><cell cols="3">135.4038 0.1094 1.2772 0.4575</cell></row><row><cell cols="2">Pix2PixHD [20]</cell><cell>76.76</cell><cell cols="2">0.0491 1.0602 0.4451</cell></row><row><cell cols="2">StarGAN-v2 [3]</cell><cell>74.38</cell><cell>0.0433 *</cell><cell>0.4577</cell></row><row><cell cols="3">UWAT-GAN MNA 67.96</cell><cell cols="2">0.0308 1.2757 0.4086</cell></row><row><cell cols="2">UWAT-GAN</cell><cell>55.59</cell><cell cols="2">0.0260 1.323 0.3826</cell></row><row><cell>UWF-SLO</cell><cell>UWF-FA</cell><cell>A M A M</cell><cell>NA M NA M</cell><cell>pix2pixHD</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U20A20386</rs>, <rs type="grantNumber">U22A2033</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (No. <rs type="grantNumber">LY21F020017</rs>), <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), <rs type="person">GuangDong Basic</rs> and <rs type="funder">Applied Basic Research Foundation</rs> (No. <rs type="grantNumber">2022A1515110570</rs>), <rs type="projectName">Innovation teams of youth innovation in science and technology of high education institutions of Shandong province</rs> (No. <rs type="grantNumber">2021KJ088</rs>), the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>), and the <rs type="institution">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>.</p><p>Data statement. Dataset used in this work is privately collected from our collaborative hospital after ethical approval and data usage permission. Data maybe be available upon the permission of the related authority and adequate request to the corresponding author(s).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BJY6UY5">
					<idno type="grant-number">U20A20386</idno>
				</org>
				<org type="funding" xml:id="_ZerFSkQ">
					<idno type="grant-number">U22A2033</idno>
				</org>
				<org type="funding" xml:id="_dhNcQBj">
					<idno type="grant-number">LY21F020017</idno>
				</org>
				<org type="funding" xml:id="_Dr8z9gp">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funded-project" xml:id="_YcprX32">
					<idno type="grant-number">2022A1515110570</idno>
					<orgName type="project" subtype="full">Innovation teams of youth innovation in science and technology of high education institutions of Shandong province</orgName>
				</org>
				<org type="funding" xml:id="_kwa2b73">
					<idno type="grant-number">2021KJ088</idno>
				</org>
				<org type="funding" xml:id="_rapjvpW">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Applications -Vascular</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diabetic retinopathy and ultrawide field imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shokrollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Salongcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Seminars in Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2020">2020</date>
			<publisher>Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<ptr target="http://arxiv.org/abs/1801.01973" />
		<title level="m">A note on the inception score</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stargan v2: diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quantitative ultra-widefield angiography and diabetic retinopathy severity: an assessment of panretinal leakage index, ischemic index and microaneurysm count</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Ehlers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Boss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babiuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talcott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1527" to="1532" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II 14</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vtgan: Semi-supervised retinal image synthesis and disease prediction using vision transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3235" to="3245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rv-gan: Segmenting retinal vascular structure in fundus photographs using a novel multi-scale generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part VIII 24</title>
		<meeting>Part VIII 24<address><addrLine>Strasbourg, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
	<note>Assisted Intervention-MICCAI 2021: 24th International Conference</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative models with kernel distance in data space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Knop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Podolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">487</biblScope>
			<biblScope unit="page" from="119" to="129" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with Markovian generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-9_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simulation physics-informed deep neural network by adaptive adam optimization method to perform a comparative study of the system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lihua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">Suppl 2</biblScope>
			<biblScope unit="page" from="1111" to="1130" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal MR image synthesis using gradient prior and adversarial learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1176" to="1188" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on multimodal deep learning for image synthesis: applications, methods, datasets, evaluation metrics, and results comparison</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 the 5th International Conference on Innovation in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="108" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel deep learning conditional generative adversarial network for producing angiography images from retinal fundus photographs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakkoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Zuckerbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21580</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated grading of diabetic retinopathy with ultra-widefield fluorescein angiography and deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Diabetes Res</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer-gan: multimodal CT image super-resolution via transfer generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="195" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mri cross-modality image-to-image translation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3753</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
