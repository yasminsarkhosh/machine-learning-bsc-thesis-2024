<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computationally Efficient 3D MRI Reconstruction with Adaptive MLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Center for Magnetic Resonance Research</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yikang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Terrence</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shanhui</forename><surname>Sun</surname></persName>
							<email>shanhui.sun@uii-ai.com</email>
							<affiliation key="aff0">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">United Imaging Intelligence</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computationally Efficient 3D MRI Reconstruction with Adaptive MLP</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="195" to="205"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DC5F58D7492BEC53BB1D988C482A4545</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D MRI reconstruction</term>
					<term>Deep learning</term>
					<term>MLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared with 2D MRI, 3D MRI provides superior volumetric spatial resolution and signal-to-noise ratio. However, it is more challenging to reconstruct 3D MRI images. Current methods are mainly based on convolutional neural networks (CNN) with small kernels, which are difficult to scale up to have sufficient fitting power for 3D MRI reconstruction due to the large image size and GPU memory constraint. Furthermore, MRI reconstruction is a deconvolution problem, which demands long-distance information that is difficult to capture by CNNs with small convolution kernels. The multi-layer perceptron (MLP) can model such long-distance information, but it requires a fixed input size. In this paper, we proposed Recon3DMLP, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction, for 3D MRI reconstruction. We further utilized the circular shift operation based on MRI physics such that dMLP accepts arbitrary image size and can extract global information from the entire FOV. We also propose a GPU memory efficient data fidelity module that can reduce &gt;50% memory. We compared Recon3DMLP with other CNNbased models on a high-resolution (HR) 3D MRI dataset. Recon3DMLP improves HR 3D reconstruction and outperforms several existing CNNbased models under similar GPU memory consumption, which demonstrates that Recon3DMLP is a practical solution for HR 3D MRI reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Compared with 2D MRI, 3D MRI has superior volumetric spatial resolution and signal-to-noise ratio. However, 3D MRI, especially high resolution (HR) 3D MRI (e.g., at least 1 mm 3 voxel size), often takes much longer acquisition time than 2D scans. Therefore, it is necessary to accelerate 3D MRI by acquiring sub-sampled k-space. However, it is more challenging to reconstruct HR 3D MRI images than 2D images. For example, HR 3D MRI data can be as large as 380×294×138×64, which is more than 100X larger than common 2D MRI data <ref type="bibr" target="#b12">[13]</ref> (e.g., 320×320×1×15, hereafter data dimensions are defined as RO×PE×SPE×Coil, where RO stands for read-out, PE for phase-encoding, and SPE for slice-phase-encoding). Although deep learning (DL) based methods have shown superior reconstruction speed and image quality, they are constrained by GPU memory for 3D MRI reconstruction in the clinical setting. Due to the large 3D image size and computation constraint, the state-ofthe-art methods for 2D MRI reconstruction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> are not directly transferable to 3D MRI reconstruction. Instead of using 3D convolutions, <ref type="bibr" target="#b0">[1]</ref> proposed a 2D CNN on the PE-SPE plane for 3D MRI reconstruction. <ref type="bibr" target="#b30">[31]</ref> proposed to downsample the 3D volume and reconstruct the smaller 3D image, which is then restored to the original resolution by a super-resolution network. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> used 3D CNN models to reconstruct each coil of 3D MRI data independently. <ref type="bibr" target="#b10">[11]</ref> applied the gradient checkpointing technique to save the GPU memory during training. GLEAM <ref type="bibr" target="#b20">[21]</ref> splits the network into modules and updates the gradient on each module independently, which reduces memory usage during training.</p><p>The previous works on 3D MRI reconstruction have several limitations. First, all these methods are based on CNN. In the context of 3D reconstruction, deep CNN networks require significant GPU memory and are difficult to scale. As a result, many models are designed to be relatively small to fit within available resources <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23]</ref>. Given that a high-resolution 3D volume can contain over 100 million voxels, the model's fitting power is critical. Small models may lack the necessary fitting power, resulting in suboptimal performance in 3D MRI reconstruction. Second, due to network inductive bias, CNN prioritizes low-frequency information reconstruction and tends to generate smooth images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. Third, CNN has a limited receptive field due to highly localized convolutions using small kernels. The k-space sub-sampling is equivalent to convolving the underlying aliasing-free image using a kernel that covers the entire field of view (FOV) (orange arrow in Fig. <ref type="figure" target="#fig_0">1</ref>). Therefore, the contribution of aliasing artifacts for a voxel comes from all other voxels globally in the sub-sampling directions. Then reconstruction is deconvolution and it is desirable to utilize the global information along the sub-sampled directions (green arrow in Fig. <ref type="figure" target="#fig_0">1</ref>). Although convolution-based methods such as large kernels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, dilation, deformable convolution <ref type="bibr" target="#b4">[5]</ref> as well as attention-based methods such as Transformers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> can enlarge the receptive field, it either only utilizes limited voxels within the FOV or may lead to massive computation <ref type="bibr" target="#b21">[22]</ref>. Recently, multi-layer perceptron (MLP) based models have been proposed for various computer vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b29">30]</ref>. MLP models have better fitting power and less inductive bias than CNN models <ref type="bibr" target="#b15">[16]</ref>. MLP performs matrix multiplication instead of convolution, leading to enlarged receptive fields with lower memory and time cost than CNN and attention-based methods. However, MLP requires a fixed input image resolution and several solutions have been proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Nevertheless, these methods were proposed for natural image processing and failed to exploit global information from the entire FOV. Img2ImgMixer <ref type="bibr" target="#b18">[19]</ref> adapted MLP-Mixer <ref type="bibr" target="#b24">[25]</ref> to 2D MRI reconstruction but on fixed-size images. AUTOMAP <ref type="bibr" target="#b31">[32]</ref> employs MLP on whole k-space to learn the Fourier transform, which requires massive GPU memory and a fixed input size and thus is impractical even for 2D MRI reconstruction. Fourth, the methods to reduce GPU memory are designed to optimize gradient calculation for training, which is not beneficial for inference when deployed in clinical practice.</p><p>To tackle these problems, we proposed Recon3DMLP for 3D MRI reconstruction, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction. The dMLP improves the model fitting ability with almost the same GPU memory usage and a minor increase in computation time. We utilized the circular shift operation <ref type="bibr" target="#b17">[18]</ref> based on MRI physics such that the proposed dMLP accepts arbitrary image size and can extract global information from the entire FOV. Furthermore, we propose a memory-efficient data fidelity (eDF) module that can reduce &gt;50% memory. We also applied gradient checkpointing, RO cropping, and half-precision (FP16) to save GPU memory. We compared Recon3DMLP with other CNN-based models on an HR 3D multi-coil MRI dataset. The proposed dMLP improves HR 3D reconstruction and outperforms several existing CNN-based strategies under similar GPU memory consumption, which demonstrate that Recon3DMLP is a practical solution for HR 3D MRI reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recon3DMLP for 3D MRI Reconstruction</head><p>The MRI reconstruction problem can be solved as</p><formula xml:id="formula_0">x = arg min x ||y -MF Sx|| 2 2 + λ||x -g θ (x u )|| 2 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where y is the acquired measurements, x u is the under-sampled image, M and S are the sampling mask and coil sensitivities, F denotes FFT and λ is a weighting scalar. g θ is a neural network with the data fidelity (DF) module <ref type="bibr" target="#b23">[24]</ref>.</p><p>The proposed Recon3DMLP adapts the memory-friendly cascaded structure. Previous work has shown that convolutions with small kernels are essential for low-level tasks <ref type="bibr" target="#b26">[27]</ref>. Therefore, we added the dMLP module with large kernels after each 3D CNN with small kernels (k = 3) to increase the fitting capacity and utilize the global information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive MLP for Flexible Image Resolution</head><p>The dMLP module includes the following operations (Fig. <ref type="figure" target="#fig_1">2</ref>): 1) circular padding, 2) image patching, 3) FC layers, 4) circular shift, 5) shift alignment and 6) cropping. The input is circular-padded in order to be cropped into patches, and the shared 1D FC layers are applied over the patch dimension. The output is then un-patched into the original image shape. Next, the circular shift is applied along the patched dimension by a step size. The circular padding and shift are based on the DFT periodicity property of images. Then operations 2-4 (FC block) are stacked several times. Due to the shift operation in each FC block, the current patch contains a portion of information from two adjacent patches in the previous FC block, which allows information exchange between patches and thus dMLP can cover the entire FOV. In the end, the shift alignment is applied to roll back the previous shifts in the image domain. The padded region is then cropped out to generate the final output. Since the sub-sampling in k-space is a linear process that can be decomposed as 1D convolutions in the image domain along each sub-sampled direction, we use 1D dMLP for 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Efficient Data Fidelity Module</head><p>In the naive implementation of the DF module</p><formula xml:id="formula_2">d DF = S H F H [(I -M )F Sz + y],<label>(2)</label></formula><p>the coil combined image z is broadcasted to multi-coil data (I -M )F Sz and it increases memory consumption. Instead, we can process the data coil-by-coil</p><formula xml:id="formula_3">d eDF = c S H c F H [(I -M c )F S c z + y c ], (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where c is the coil index. Together with eDF, we also employed RO cropping and gradient checkpointing for training and half-precision for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiments</head><p>We collected a multi-contrast HR 3D brain MRI dataset with IRB approval, ranging from 224×220×96×12 to 336×336×192×32 <ref type="bibr" target="#b2">[3]</ref>. There are 751 3D multicoil images for training, 32 for validation, and 29 for testing.</p><p>We started with a small 3D CNN model (Recon3DCNN) with an expansion factor e = 6, where the channels increase from 2 to 12 in the first convolution layer and reduce to 2 in the last layer in each cascade. We then enlarged Recon3DCNN with increased width (e = 6,12,16,24) and depth (double convolution layers in each cascade). We also replaced the 3D convolution in Recon3DCNN with depth separable convolution <ref type="bibr" target="#b9">[10]</ref> or separate 1D convolution for each 3D dimension. We also adapted the reparameterization technique <ref type="bibr" target="#b5">[6]</ref> for Recon3DCNN such that the residual connection can be removed during inference to reduce the GPU memory. For comparison, we also adapted a 3D version of cascaded UNet, where each UNet  has five levels with e = 4 at the initial layer and the channels were doubled at each level. To demonstrate the effectiveness of dMLP, we built Recon3DMLP by adding two 1D dMLP on PE (k = 64) and SPE (k = 16) to the smallest Recon3DCNN (e = 6). Since GELU <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> has larger memory overhead, we used leaky ReLU for all models. We performed ablation studies on Recon3DMLP by sharing the FC blocks among shifts, removing shifts, reducing patch size to 3 as well as replacing the dMLP with large kernel convolutions (LKconv) using k = 65 for PE and k = 17 for SPE, as well as small kernel convolutions (SKconv) using k = 3. We attempted to adapt ReconFormer<ref type="foot" target="#foot_0">1</ref> , a transformer-based model, and Img2ImgMixer<ref type="foot" target="#foot_1">2</ref> , an MLP based model. Both models require to specify a fixed input image size when constructing the model and failed to run on datasets with various sizes, indicating the limitation of these methods. Note that the two models were originally demonstrated on the 2D datasets with the same size <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. All models were trained with loss = L1+SSIM and lr = 0.001 for 50 epochs using an NVIDIA A100 GPU with Pytorch 1.10 and CUDA 11.3. The pvalues were calculated by the Wilcoxon signed-ranks test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We first demonstrate the benefit of eDF and FP16 inference with a small CNN model Recon3DCNN (e = 6) (first and second panels in Table <ref type="table" target="#tab_0">1</ref>). Without eDF and FP16, the model takes &gt;40G inference GPU memory and fails to reconstruct the test data, which indicates the challenge of HR 3D MRI reconstruction. FP16 and eDF reduce at least 11% and 53% inference memory. However, the model with only eDF is slower than the model with only FP16. By combining eDF and FP16, the inference GPU memory is reduced by 71% to 11.5G, which makes the model feasible to be deployed with a mid-range GPU in practice. Hereafter, we applied eDF and FP16 to all models. Next, we aim to improve Recon3DCNN's performance by increasing the width and depth (third panel in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>). By making the model wider (increase e = 6 to e = 24), the PSNR/SSIM improves significantly (p &lt; 10 -7 ). However, the inference GPU memory also increases by 33%. On the other hand, doubling the depth also improves the performance (p &lt; 10 -5 ), but not as significantly as increasing the model width. Also, the former increases inference time (40%) more than the latter (21%). Also increasing the model depth does not affect the inference GPU memory. Next, we experimented with those commonly used techniques for efficient computation to modify the best CNN model Recon3DCNN (e = 24) (fourth panel in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>). All those variants lead to a performance drop compared to the original model (p &lt; 10 -7 ), because such methods reduce the model's fitting capacity. Those variants also result in memory increase except Recon3DCNN with reparameterization technique. These results indicate such methods proposed for natural image processing are not suitable for HR 3D MRI reconstruction.</p><p>The performance of Recon3DCNN improves when becoming larger (i.e., more parameters), which indicates CNN models lack fitting power for HR 3D MR reconstruction. Therefore, we performed an overfitting experiment where models were trained and tested on one data. Figure <ref type="figure" target="#fig_2">3</ref> confirms that Recon3DCNN can not overfit one test data in 10K iterations and models with better fitting ability tend to have better PSNR/SSIM (Table <ref type="table" target="#tab_0">1</ref>). The variants of Recon3DCNN indeed have lower fitting power than the original model. This motivates us to build Recon3DMLP by adding dMLP to Recon3DCNN (e = 6) to increase its capacity while maintaining low memory usage. Recon3DMLP has better fitting ability and less reconstruction error than all models (Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>). Compared to the smaller Recon3DCNN (e = 6), Recon3DMLP has similar GPU memory usage but better PSNR/SSIM (p &lt; 10 -7 ). Compared to the larger Recon3DCNN (e = 24), Recon3DMLP has 24% less GPU memory usage and better PSNR (p &lt; 10 -7 ) and only marginally worse SSIM (p = 0.05). The cascaded 3D UNet has less GPU memory consumption but lower fitting power, worse performance (p &lt; 10 -7 ) and longer inference time than Recon3DCNN (e = 24) and Recon3DMLP.</p><p>To investigate the source of the gain, we perform ablation studies on Recon3DMLP (last panel in Table <ref type="table" target="#tab_0">1</ref>). By removing the shift operations, the dMLP module can only utilize the global information within the large patch, which leads to a drop in PSNR/SSIM (p &lt; 10 -7 ). When reducing the patch  size to 3 but keeping the shift operations such that the model can only utilize the global information through the shift operations, the performance also drops (p &lt; 10 -7 ) but less than the previous one. This indicates the shift operations can help the model to learn the global information and thus improve the reconstruction results. Also, models with and without shift operations do not significantly differ in GPU memory and time, suggesting the shift operations are computationally efficient. By sharing the FC parameters among shifts, the model has much fewer parameters and performance drops slightly (p &lt; 10 -7 ) while GPU memory and time are similar to the original Recon3DMLP. We also replaced the dMLP modules in Recon3DMLP with convolutions using larger kernels and small kernels, respectively. Recon3DMLP (LKconv) and Recon3DMLP (SKconv) <ref type="foot" target="#foot_2">3</ref>have worse performance (p &lt; 10 -3 ) as well as longer time than their counterpart Recon3DMLP and Recon3DMLP (small patch), indicating the dMLP is better than the convolutions for HR 3D MRI reconstruction. We compared the Recon3DMLP with and without dMLP modules and Fig. <ref type="figure" target="#fig_4">5</ref> shows that dMLP modules help to learn the high-frequency information faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Although MLP has been proposed for vision tasks on natural images as well as 2D MRI reconstruction with fixed input size, we are the first to present a practical solution utilizing the proposed dMLP and eDF to overcome the computational constraint for HR 3D MRI reconstruction with various sizes. Compared with CNN based models, Recon3DMLP improves image quality with a little increase in computation time and similar GPU memory usage.</p><p>One limitation of our work is using the same shift and patch size without utilizing the multi-scale information. dMLP module that utilizes various patch and shift sizes will be investigated in future work. MLP-based models such as Recon3DMLP may fail if the training data is small.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Demonstration of k-space acquisition, which is equivalent to a convolution in the image domain, and reconstruction, which is a deconvolution process to recover the underlying image. The convolution kernel has the most energy at the center but spans the entire FOV, suggesting that global information is necessary for reconstruction. (Color figure online)</figDesc><graphic coords="2,94,80,244,19,234,28,87,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The proposed Recon3DMLP for 3D MRI reconstruction, which is a cascaded network and each cascade consists of a hybrid of CNN and dMLP modules. (b) The overall structure of dMLP module. (c) Circular padding is applied to ensure image can be patched. (d) Shared 1D FC layers is then applied to the patch dimension, followed by un-patch and shift operations. The FC blocks are stacked multiple times. The shiftalignment and crop operations are then applied to recover the original image shape.</figDesc><graphic coords="4,79,80,248,63,264,64,265,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The fitting power of various models on HR 3D MRI reconstruction. Models with lower loss indicate better fitting capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Reconstruction results and corresponding error maps.</figDesc><graphic coords="8,50,79,54,20,322,36,261,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The k-space difference between Recon3DMLP with and without dMLP across training iterations. Red areas in the outer k-space indicate Recon3DMLP with dMLP has recovered more high-frequency information and faster than that without dMLP. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of different models on HR 3D MRI reconstruction. The inference GPU memory and forward time were measured on a 3D image in 380×294×138×24.</figDesc><table><row><cell>Model</cell><cell cols="5">Memory Saving Parameters (K) GPU (G) Time (S) SSIM</cell><cell>PSNR</cell></row><row><cell>Recon3DCNN (e = 6)</cell><cell>None</cell><cell>65</cell><cell>&gt;40</cell><cell>Fail</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Recon3DCNN (e = 6)</cell><cell>FP16</cell><cell>65</cell><cell>35.5</cell><cell>1.17</cell><cell cols="2">0.9581 40.2790</cell></row><row><cell>Recon3DCNN (e = 6)</cell><cell>eDF</cell><cell>65</cell><cell>18.8</cell><cell>3.49</cell><cell cols="2">0.9581 40.2795</cell></row><row><cell>Recon3DCNN (e = 6)</cell><cell>FP16+eDF</cell><cell>65</cell><cell>11.5</cell><cell>3.04</cell><cell cols="2">0.9581 40.2785</cell></row><row><cell>Recon3DCNN (e = 6, conv = 10)</cell><cell>FP16+eDF</cell><cell>130</cell><cell>11.5</cell><cell>4.26</cell><cell cols="2">0.9597 40.5042</cell></row><row><cell>Recon3DCNN (e = 12)</cell><cell>FP16+eDF</cell><cell>247</cell><cell>11.6</cell><cell>3.14</cell><cell cols="2">0.9623 40.8118</cell></row><row><cell>Recon3DCNN (e = 16)</cell><cell>FP16+eDF</cell><cell>433</cell><cell>13.3</cell><cell>3.20</cell><cell cols="2">0.9636 40.9880</cell></row><row><cell>Recon3DCNN (e = 24)</cell><cell>FP16+eDF</cell><cell>960</cell><cell>15.2</cell><cell>3.67</cell><cell cols="2">0.9649 41.1503</cell></row><row><cell>Recon3DCNN-1DConv (e = 24)</cell><cell>FP16+eDF</cell><cell>386</cell><cell>16.6</cell><cell>5.51</cell><cell cols="2">0.9639 41.0473</cell></row><row><cell>Recon3DCNN-Rep (e = 24)</cell><cell>FP16+eDF</cell><cell>995</cell><cell>12.5</cell><cell>3.68</cell><cell cols="2">0.9613 40.4970</cell></row><row><cell cols="2">Recon3DCNN-DepthConv (e = 24) FP16+eDF</cell><cell>111</cell><cell>17.2</cell><cell>4.11</cell><cell cols="2">0.9594 40.4367</cell></row><row><cell>Recon3DCNN-UNet (e = 4)</cell><cell>FP16+eDF</cell><cell>7,056</cell><cell>10.6</cell><cell>4.16</cell><cell cols="2">0.9565 40.4229</cell></row><row><cell>Recon3DMLP (e = 6/8, SKconv)</cell><cell>FP16+eDF</cell><cell>72</cell><cell>10.5</cell><cell>4.38</cell><cell cols="2">0.9617 41.0456</cell></row><row><cell>Recon3DMLP (e = 6/8, LKconv)</cell><cell>FP16+eDF</cell><cell>157</cell><cell>11.5</cell><cell>4.55</cell><cell cols="2">0.9620 41.0741</cell></row><row><cell>Recon3DMLP (e = 6/8, k = 3)</cell><cell>FP16+eDF</cell><cell>115</cell><cell>11.5</cell><cell>3.37</cell><cell cols="2">0.9622 41.0627</cell></row><row><cell>Recon3DMLP (e = 6/8, share)</cell><cell>FP16+eDF</cell><cell>1,465</cell><cell>11.5</cell><cell>3.36</cell><cell cols="2">0.9627 41.1455</cell></row><row><cell>Recon3DMLP (e = 6/8, no shift)</cell><cell>FP16+eDF</cell><cell>11,264</cell><cell>11.5</cell><cell>3.36</cell><cell cols="2">0.9619 41.0853</cell></row><row><cell>Recon3DMLP (e = 6/8, proposed)</cell><cell>FP16+eDF</cell><cell>11,264</cell><cell>11.5</cell><cell>3.38</cell><cell cols="2">0.9637 41.1953</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/guopengf/ReconFormer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/MLI-lab/imaging MLPs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>These are CNN models but we consider them as ablated models of Recon3DMLP and slightly abuse the notation.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 19.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04674</idno>
		<title level="m">Deep learning-based reconstruction of highly accelerated 3D MRI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency bias in neural networks for input of non-uniform density</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kritchman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Accelerating 3D multiplex MRI reconstruction with deep learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08163</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10224</idno>
		<title level="m">CycleMLP: A MLP-like architecture for dense prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RepVGG: Making VGGstyle convnets great again</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ReconFormer: accelerated MRI reconstruction using recurrent transformer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09376</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memory-efficient learning for large-scale computational imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comp. Imag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1403" to="1414" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advancing machine learning for MR image reconstruction with an open competition: overview of the 2019 fastmri challenge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3054" to="3070" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">fastMRI: a publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">190007</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04454</idno>
		<title level="m">ConvMLP: hierarchical convolutional MLPs for vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">As-MLP: an axial shifted MLP architecture for vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08391</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for a new paradigm shift? a survey on visual deep MLP</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">100520</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">More convnets in the 2020s: scaling up kernels beyond 51×51 using sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03620</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Image-to-image MLP-mixer for image reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02018</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Results of the 2020 fastMRI challenge for machine learning MR image reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2306" to="2317" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gleam: greedy learning for large-scale accelerated MRI reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ozturkler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08393</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5301" to="5310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NC-PDNet: a densitycompensated unrolled network for 2D and 3D non-cartesian MRI reconstruction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ramzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chaithya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciuciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1625" to="1638" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep cascade of convolutional neural networks for MR image reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-951" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="647" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MLP-mixer: an all-MLP architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ResMLP: feedforward networks for image classification with data-efficient training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maxim: multi-axis MLP for image processing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5769" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UNeXt: MLP-based rapid medical image segmentation network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-93" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-828" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="292" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D MRI reconstruction based on 2D generative adversarial network super-resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2978</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7697</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
