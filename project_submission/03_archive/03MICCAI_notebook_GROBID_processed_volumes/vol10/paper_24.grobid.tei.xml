<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction</title>
				<funder ref="#_Td3fmBa">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_qak6g5R">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_DR2UScZ">
					<orgName type="full">ZJLab, Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
				<funder ref="#_vJ4Sb8E">
					<orgName type="full">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenglong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zilong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<idno type="ORCID">0000-0002-0604-3197</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University)</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shanghai Center for Brain Science and Brain-Inspired Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="250" to="259"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">73720E61B73532753DFCAD8AC5007851</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sparse-view CT</term>
					<term>CT reconstruction</term>
					<term>Fourier convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse-view computed tomography (CT) is a promising solution for expediting the scanning process and mitigating radiation exposure to patients, the reconstructed images, however, contain severe streak artifacts, compromising subsequent screening and diagnosis. Recently, deep learning-based image post-processing methods along with their dual-domain counterparts have shown promising results. However, existing methods usually produce over-smoothed images with loss of details due to i) the difficulty in accurately modeling the artifact patterns in the image domain, and ii) the equal treatment of each pixel in the loss function. To address these issues, we concentrate on the image postprocessing and propose a simple yet effective FREquency-band-awarE and SElf-guidED network, termed FreeSeed, which can effectively remove artifacts and recover missing details from the contaminated sparse-view CT images. Specifically, we first propose a frequency-band-aware artifact modeling network (FreeNet), which learns artifact-related frequencyband attention in the Fourier domain for better modeling the globally distributed streak artifact on the sparse-view CT images. We then introduce a self-guided artifact refinement network (SeedNet), which leverages the predicted artifact to assist FreeNet in continuing to refine the severely corrupted details. Extensive experiments demonstrate the superior performance of FreeSeed and its dual-domain counterpart over the state-of-the-art sparse-view CT reconstruction methods. Source code is made available at https://github.com/Masaaki-75/freeseed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>X-ray computed tomography (CT) is an established diagnostic tool in clinical practice; however, there is growing concern regarding the increased risk of cancer induction associated with X-ray radiation exposure <ref type="bibr" target="#b13">[14]</ref>. Lowering the dose of CT scans has been widely adopted in clinical practice to address this issue, following the "as low as reasonably achievable" (ALARA) principle in the medical community <ref type="bibr" target="#b8">[9]</ref>. Sparse-view CT is one of the effective solutions, which reduces the radiation by only sampling part of the projection data for image reconstruction. Nevertheless, images reconstructed by the conventional filtered back-projection (FBP) present severe artifacts, thereby compromising their clinical value.</p><p>In recent years, the success of deep learning has attracted much attention in the field of sparse-view CT reconstruction. Existing learning-based approaches mainly include image-domain methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref> and dual-domain ones <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, both involving image post-processing to restore a clean CT image from the low-quality one with streak artifacts. For the image post-processing, residual learning <ref type="bibr" target="#b2">[3]</ref> is often employed to encourage learning the artifacts hidden in the residues, which has become a proven paradigm for enhancing the performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref>. Unfortunately, existing image post-processing methods may fail to model the globally distributed artifacts within the image domain. They can also produce over-smoothed images due to the lack of differentiated supervision for each pixel. In this paper, we advance image post-processing to benefit both classical image-domain methods and the dominant dual-domain ones.</p><p>Motivation. We view the sparse-view CT image reconstruction as a two-step task: artifact removal and detail recovery. For the former, few work has investigated the fact that the artifacts exhibit similar pattern across different sparseview scenarios, which is evident in Fourier domain as shown in Fig. <ref type="figure" target="#fig_0">1</ref>: they are aggregated mainly in the mid-frequency band and gradually migrate from low to high frequencies as the number of views increases. Inspired by this, we propose a frequency-band-aware artifact modeling network (FreeNet) that learns the artifact-concentrated frequency components to remove the artifacts efficiently using learnable band-pass attention maps in the Fourier domain.</p><p>While Fourier domain band-pass maps help capture the pattern of the artifacts, restoring the image detail contaminated by strong artifacts may still be difficult due to the entanglement of artifacts and details in the residues. Consequently, we propose a self-guided artifact refinement network (SeedNet) that provides supervision signals to aid FreeNet in refining the image details contaminated by the artifacts. With these novel designs, we introduce a simple yet effective model termed FREquency-band-awarE and SElf-guidED network (FreeSeed), which enhances the reconstruction by modeling the pattern of artifacts from a frequency perspective and utilizing the artifact to restore the details. FreeSeed achieves promising results with only image data and can be further enhanced once the sinogram is available.</p><p>Our contributions can be summarized as follows: 1) a novel frequency-bandaware network is introduced to efficiently capture the pattern of global artifacts in the Fourier domain among different sparse-view scenarios; 2) to promote the restoration of heavily corrupted image detail, we propose a self-guided artifact refinement network that ensures targeted refinement of the reconstructed image and consistently improves the model performance across different scenarios; and 3) quantitative and qualitative results demonstrate the superiority of FreeSeed over the state-of-the-art sparse-view CT reconstruction methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Given a sparse-view sinogram with projection views N v , let I s and I f denote the directly reconstructed sparse-and full-view images by FBP, respectively. In this paper, we aim to construct an image-domain model to effectively recover I s with a level of quality close to I f . The proposed framework of FreeSeed is depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, which mainly consists of two designs: FreeNet that learns to remove the artifact and is built with band-pass Fourier convolution blocks that better capture the pattern of the artifact in Fourier domain; and SeedNet as a proxy module that enables FreeNet to refine the image detail under the guidance of the predicted artifact. Note that SeedNet is involved only in the training phase, additional computational cost will not be introduced in the application. The parameters of FreeNet and SeedNet in FreeSeed are updated in an iterative fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Frequency-Band-Aware Artifact Modeling Network</head><p>To learn the globally distributed artifact, FreeNet uses band-pass Fourier convolution blocks as the basic unit to encode artifacts from both spatial and frequency aspects. Technically, Fourier domain knowledge is introduced by fast Fourier convolution (FFC) <ref type="bibr" target="#b0">[1]</ref>, which benefits from the non-local receptive field and has shown promising results in various computer vision tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>. The features fed into FFC are split evenly along the channel into a spatial branch composed of vanilla convolutions and a spectral branch that applies convolution after real Fourier transform, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Despite the effectiveness, a simple Fourier unit in FFC could still preserve some low-frequency information that may interfere with the learning of artifacts, which could fail to accurately capture the banded pattern of the features of sparse-view artifacts in the frequency domain. To this end, we propose to incorporate learnable band-pass attention maps into FFC. Given an input spatial-domain feature map X in ∈ R Cin×H×W , the output X out ∈ R Cout×H×W through the Fourier unit with learnable band-pass attention maps is obtained as follows:</p><formula xml:id="formula_0">Z in = F real {X in }, Z out = f (Z in H) , X out = F -1 real {Z out }, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where F real and F -1 real denote the real Fourier transform and its inverse version, respectively. f denotes vanilla convolution. " " is the Hadamard product. Specifically, for c-th channel frequency domain feature Z (c) in ∈ C U ×V (c = 1, ..., C in ), the corresponding band-pass attention map H (c) ∈ R U ×V is defined by the following Gaussian transfer function:</p><formula xml:id="formula_2">H (c) = exp ⎡ ⎣ - D (c)2 -d (c)2 0 w (c) D (c) + 2 ⎤ ⎦ ,<label>(2)</label></formula><formula xml:id="formula_3">D (c) u,v = (u -U/2) 2 + (v -V /2) 2 max u ,v [(u -U/2) 2 + (v -V /2) 2 ] ,<label>(3)</label></formula><p>where D (c) is the c-th channel of the normalized distance map with entries denoting the distance from any point (u, v) to the origin. Two learnable parameters, w (c) &gt; 0 and d</p><formula xml:id="formula_4">(c) 0 ∈ [0, 1]</formula><p>, represent the bandwidth and the normalized inner radius of the band-pass map, respectively, and are initialized as 1 and 0, respectively. is set to 1 × 10 -12 to avoid division by zero. The right half part of the second row of Fig. <ref type="figure" target="#fig_0">1</ref> shows some samples of the band-pass maps.</p><p>The pixel-wise difference between the predicted artifact A of FreeNet and the groundtruth artifact A f = I s -I f is measured by 2 loss:</p><formula xml:id="formula_5">L art = A f -A 2 .</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-guided Artifact Refinement Network</head><p>Areas heavily obscured by the artifact should be given more attention, which is hard to achieve using only FreeNet. Therefore, we propose a proxy network SeedNet that provides supervision signals to focus FreeNet on refining the clinical detail contaminated by the artifact under the guidance of the artifact itself. SeedNet consists of residual Fourier convolution blocks. Concretely, given sparseview CT images I s , FreeNet predicts the artifact A and restored image I = I s -A; the latter is fed into SeedNet to produce targeted refined result I. To guide the network on refining the image detail obscured by heavy artifacts, we design the transformation T that turns A into a mask M using its mean value as threshold: M = T ( A), and define the following masked loss for SeedNet:</p><formula xml:id="formula_6">L mask = (I f -I) M 2 .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function for FreeSeed</head><p>FreeNet and SeedNet in our proposed FreeSeed are trained in an iterative fashion, where SeedNet is updated using L mask defined in Eq. ( <ref type="formula">5</ref>), and FreeNet is trained under the guidance of the total loss:</p><formula xml:id="formula_7">L total = L art + αL mask ,<label>(6)</label></formula><p>where α &gt; 0 is empirically set as 1. The pseudo-code for the training process and the exploration on the selection of α can be found in our Supplementary Material.</p><p>Once the training is complete, SeedNet can be dropped and the prediction is done by FreeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Extending FreeSeed to Dual-Domain Framework</head><p>Dual-domain methods are effective in the task of sparse-view CT reconstruction when the sinogram data are available. To further enhance the image reconstruction quality, we extend FreeSeed to the dominant dual-domain framework by adding the sinogram-domain sub-network from DuDoNet <ref type="bibr" target="#b6">[7]</ref>, where the resulting dual-domain counterpart shown in Fig. <ref type="figure" target="#fig_2">3</ref> is called FreeSeed dudo . The sinogramdomain sub-network involves a mask U-Net that takes in the linearly interpolated sparse sinogram S s , where a binary sinogram mask M s that outlines the unseen part of the sparse-view sinogram is concatenated to each stage of the U-Net encoder. The mask U-Net is trained using sinogram loss L sino and Radon consistency loss L rc . We refer the readers to Lin et al. <ref type="bibr" target="#b6">[7]</ref> for more information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We conduct experiments on the dataset of "the 2016 NIH-AAPM Mayo Clinic Low Dose CT Grand Challenge" <ref type="bibr" target="#b7">[8]</ref>, which contains 5,936 CT slices in 1 mm image thickness from 10 anonymous patients, where a total of 5,410 slices from 9 patients, resized to 256 × 256 resolution, are randomly selected for training and the 526 slices from the remaining one patient for testing without patient overlap. Fan-beam CT projection under 120 kVp and 500 mA is simulated using TorchRadon toolbox <ref type="bibr" target="#b10">[11]</ref>. Specifying the distance from the X-ray source to the rotation center as 59.5 cm and the number of detectors as 672, we generate sinograms from full-dose images with multiple sparse views N v ∈ {18, 36, 72, 144} uniformly sampled from full 720 views covering [0, 2π].</p><p>The models are implemented in PyTorch <ref type="bibr" target="#b9">[10]</ref> and are trained for 30 epochs with a mini-batch size of 2, using Adam optimizer <ref type="bibr" target="#b4">[5]</ref> with (β 1 , β 2 ) = (0.5, 0.999) and a learning rate that starts from 10 -4 and is halved every 10 epochs. Experiments are conducted on a single NVIDIA V100 GPU using the same setting. All sparse-view CT reconstruction methods are evaluated quantitatively in terms of root mean squared error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Performance</head><p>We compare our models (FreeSeed and FreeSeed dudo ) with the following reconstruction methods: direct FBP, DDNet <ref type="bibr" target="#b17">[18]</ref>, FBPConv <ref type="bibr" target="#b3">[4]</ref>, DuDoNet <ref type="bibr" target="#b6">[7]</ref>, and DuDoTrans <ref type="bibr" target="#b12">[13]</ref>. FBPConv and DDNet are image-domain methods, while DuDoNet and DuDoTrans are state-of-the-art dual-domain methods effective for CT image reconstruction. Table <ref type="table" target="#tab_1">1</ref> shows the quantitative evaluation.</p><p>Not surprisingly, we find that the performance of conventional image-domain methods is inferior to the state-of-the-art dual-domain method, mainly due to the failure of removing the global artifacts. We notice that dual-domain methods underperform FBPConv when N v = 18 because of the secondary artifact induced by the inaccurate sinogram restoration in the ultra-sparse scenario. Notably, FreeSeed outperforms the dual-domain methods in most scenarios. Figure <ref type="figure" target="#fig_3">4</ref> provides the visualization results for different methods. In general, FreeSeed successfully restores the tiny clinical structures (the spines in the first row, and the ribs in the second row) while achieving more comprehensive artifact removal (see the third row). Note that when the sinogram data are available, dual-domain counterpart FreeSeed dudo gains further improvements, showing the great flexibility of our model.  <ref type="formula" target="#formula_7">6</ref>) FreeNet trained with L mask using 1 norm (FreeSeed 1 ); and (7) FreeNet trained with L mask using 2 norm, i.e., the full version of our model <ref type="bibr">(FreeSeed)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><formula xml:id="formula_8">L 1+mask = (A f -A) (1 + M ) 2 (FreeNet 1+mask ); (</formula><p>By comparing the first two rows of Table <ref type="table" target="#tab_0">2</ref>, we find that simply applying FFC provides limited performance gains. Interestingly, we observe that the advantage of band-pass attention becomes more pronounced given more views, which can be seen in the last row of Fig. <ref type="figure" target="#fig_0">1</ref> where the attention maps are visualized by averaging all inner radii and bandwidths in different stages of FreeNet and calculating the map following Eq. ( <ref type="formula" target="#formula_2">2</ref>). Figure <ref type="figure" target="#fig_0">1</ref> shows that these maps successfully capture the banded pattern of the artifact, especially in the cases of N v = 36, 72, 144 where artifacts are less entangled with the image content and present a banded shape in the frequency domain. Thus, the band-pass attention maps lead to better convergence. The effectiveness of SeedNet can be seen by comparing Rows (1) and (3) and also Rows ( <ref type="formula">4</ref>) and <ref type="bibr" target="#b6">(7)</ref>. Both the baseline and FreeNet can benefit from the SeedNet supervision. Visually, clinical details in the image that are obscured by the heavy artifacts can be further refined by FreeNet; please refer to Fig. <ref type="figure" target="#fig_0">S1</ref> in our Supplementary Material for more examples and ablation study. We also find that FreeNet 1+mask does not provide stable performance gains, probably because directly applying a mask on the pixel-wise loss leads to the discontinuous gradient that brings about sub-optimal results, which, however, can be circumvented with the guidance of SeedNet. In addition, we trained FreeSeed with Eq. ( <ref type="formula" target="#formula_7">6</ref>) using 1 norm. From the last two rows in Table <ref type="table" target="#tab_0">2</ref> we find that 1 norm does not ensure stable performance gains when FFC is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed FreeSeed, a simple yet effective image-domain method for sparse-view CT reconstruction. FreeSeed incorporates Fourier knowledge into the reconstruction network with learnable band-pass attention for a better grasp of the globally distributed artifacts, and is trained using a self-guided artifact refinement network to further refine the heavily damaged image details. Extensive experiments show that both FreeSeed and its dual-domain counterpart outperformed the state-of-the-art methods. In future, we will explore FFC-based network for sinogram interpolation in sparse-view CT reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. First row: sparse-view CT images (left half) and the corresponding artifacts (right half); second row: real Fourier amplitude maps of artifacts (left half) and the learned band-pass attention maps (right half, with inner radius and bandwidth respectively denoted by d0 and w. Values greater than 0.75 are bounded by red dotted line) given different number of views Nv. (Color figure online)</figDesc><graphic coords="3,44,79,169,97,334,60,162,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed FreeSeed.</figDesc><graphic coords="4,58,98,191,93,334,60,130,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of dual-domain counterpart of FreeSeed.</figDesc><graphic coords="6,59,46,229,40,333,40,70,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of state-of-the-art methods. From top to bottom: Nv=36, 72 and 144; the display windows are [0,2000], [500,3000] and [50,500] HU, respectively.</figDesc><graphic coords="7,44,79,215,81,334,12,143,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>presents the effectiveness of each component in FreeSeed, where seven variants of FreeSeed are: (1) FBPConv upon which FreeNet is built (baseline); (2) FreeNet without band-pass attention maps nor SeedNet guidance L mask (baseline + Fourier); (3) FBPConv trained with L mask (baseline + SeedNet); (4) FreeNet trained without L mask (FreeNet); (5) FreeNet trained with simple masked loss</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation for state-of-the-art methods in terms of PSNR [dB], SSIM [%], and RMSE [×10 -2 ]. The best results are highlighted in bold and the secondbest results are underlined.</figDesc><table><row><cell>Methods</cell><cell>Nv = 18</cell><cell>Nv = 36</cell><cell>Nv = 72</cell><cell>Nv = 144</cell></row><row><cell></cell><cell cols="4">PSNR SSIM RMSE PSNR SSIM RMSE PSNR SSIM RMSE PSNR SSIM RMSE</cell></row><row><cell>FBP</cell><cell>22.88 36.59 7.21</cell><cell>26.44 49.12 4.78</cell><cell>31.63 66.23 2.63</cell><cell>38.51 86.23 1.19</cell></row><row><cell>DDNet</cell><cell>34.07 90.63 1.99</cell><cell>37.15 93.50 1.40</cell><cell>40.05 95.18 1.03</cell><cell>45.09 98.37 0.56</cell></row><row><cell>FBPConv</cell><cell>35.04 91.19 1.78</cell><cell>37.63 93.65 1.32</cell><cell>41.95 97.40 0.82</cell><cell>45.96 98.53 0.51</cell></row><row><cell>DuDoNet</cell><cell>34.42 91.07 1.91</cell><cell>38.18 93.45 1.24</cell><cell>42.80 97.21 0.73</cell><cell>47.79 98.96 0.41</cell></row><row><cell cols="2">DuDoTrans 34.89 91.08 1.81</cell><cell>38.55 94.82 1.19</cell><cell>43.13 97.67 0.70</cell><cell>48.42 99.15 0.38</cell></row><row><cell>FreeSeed</cell><cell>35.01 91.46 1.79</cell><cell>38.63 94.46 1.18</cell><cell>43.42 97.82 0.68</cell><cell>48.79 99.19 0.37</cell></row><row><cell cols="2">FreeSeed dudo 35.03 91.81 1.78</cell><cell>38.80 94.78 1.16</cell><cell>43.78 97.90 0.65</cell><cell>49.06 99.23 0.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>PSNR value of variants of FreeSeed. The best results are highlighted in bold and the second-best results are underlined.</figDesc><table><row><cell>Variants</cell><cell cols="4">Nv = 18 Nv = 36 Nv = 72 Nv = 144</cell></row><row><cell>(1) baseline</cell><cell>35.04</cell><cell>37.63</cell><cell>41.95</cell><cell>45.96</cell></row><row><cell cols="2">(2) baseline + Fourier 34.78</cell><cell>38.23</cell><cell>42.33</cell><cell>47.32</cell></row><row><cell cols="2">(3) baseline + SeedNet 34.49</cell><cell>38.35</cell><cell>42.89</cell><cell>48.64</cell></row><row><cell>(4) FreeNet</cell><cell>34.77</cell><cell>38.42</cell><cell>43.06</cell><cell>48.63</cell></row><row><cell>(5) FreeNet 1+mask</cell><cell>34.54</cell><cell>38.17</cell><cell>42.94</cell><cell>48.73</cell></row><row><cell>(6) FreeSeed 1</cell><cell>34.79</cell><cell>38.45</cell><cell>43.06</cell><cell>49.00</cell></row><row><cell>(7) FreeSeed (ours)</cell><cell>35.01</cell><cell>38.63</cell><cell>43.42</cell><cell>48.79</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62101136</rs>), <rs type="programName">Shanghai Sailing Program</rs> (No. <rs type="grantNumber">21YF1402800</rs>), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (No. <rs type="grantNumber">2018SHZDZX01</rs>) and <rs type="funder">ZJLab, Shanghai Municipal of Science and Technology Project</rs> (No. <rs type="grantNumber">20JC1419500</rs>), and <rs type="funder">Shanghai Center for Brain Science and Brain-inspired Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qak6g5R">
					<idno type="grant-number">62101136</idno>
					<orgName type="program" subtype="full">Shanghai Sailing Program</orgName>
				</org>
				<org type="funding" xml:id="_Td3fmBa">
					<idno type="grant-number">21YF1402800</idno>
				</org>
				<org type="funding" xml:id="_DR2UScZ">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_vJ4Sb8E">
					<idno type="grant-number">20JC1419500</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 24.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast fourier convolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4479" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep residual learning for compressed sensing CT reconstruction via persistent homology analysis</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06391</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep-neural-network-based sinogram synthesis for sparse-view CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DuDoNet: dual domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10512" to="10521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TU-FG-207A-04: overview of the low dose CT grand challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mccollough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3759" to="3760" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The ALARA principle in medical imaging</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="595" to="600" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14788</idno>
		<title level="m">TorchRadon: fast differentiable routines for computed tomography</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Resolution-robust large mask inpainting with Fourier convolutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Suvorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2149" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DuDoTrans: dual-domain transformer for sparse-view CT reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Medical Image Reconstruction</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An outlook on X-ray CT research and development</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1051" to="1064" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DRONE: dualdomain residual-based optimization network for sparse-view CT reconstruction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3002" to="3014" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11247</idno>
		<title level="m">SwinFIR: revisiting the SwinIR with fast fourier convolution and improved training for image super-resolution</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sparse-view CT reconstruction method based on combination of DenseNet and deconvolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1407" to="1417" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
