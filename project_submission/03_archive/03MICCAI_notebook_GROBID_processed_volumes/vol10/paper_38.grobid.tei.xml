<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">University of Dundee</orgName>
								<address>
									<settlement>Dundee</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Clinical Neurosciences</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bath</orgName>
								<address>
									<settlement>Bath</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">University of Dundee</orgName>
								<address>
									<settlement>Dundee</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Clinical Neurosciences</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="institution">University of Dundee</orgName>
								<address>
									<settlement>Dundee</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="398" to="408"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CC1B189407C1896876A06337A65E201F</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-modal MRI</term>
					<term>Medical image synthesis</term>
					<term>Latent space</term>
					<term>Diffusion models</term>
					<term>Structural guidance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MRI synthesis promises to mitigate the challenge of missing MRI modality in clinical practice. Diffusion model has emerged as an effective technique for image synthesis by modelling complex and variable data distributions. However, most diffusion-based MRI synthesis models are using a single modality. As they operate in the original image domain, they are memory-intensive and less feasible for multi-modal synthesis. Moreover, they often fail to preserve the anatomical structure in MRI. Further, balancing the multiple conditions from multi-modal MRI inputs is crucial for multi-modal synthesis. Here, we propose the first diffusion-based multi-modality MRI synthesis model, namely Conditioned Latent Diffusion Model (CoLa-Diff). To reduce memory consumption, we perform the diffusion process in the latent space. We propose a novel network architecture, e.g., similar cooperative filtering, to solve the possible compression and noise in latent space. To better maintain the anatomical structure, brain region masks are introduced as the priors of density distributions to guide diffusion process. We further present auto-weight adaptation to employ multi-modal information effectively. Our experiments demonstrate that CoLa-Diff outperforms other stateof-the-art MRI synthesis methods, promising to serve as an effective tool for multi-modal MRI synthesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) is critical to the diagnosis, treatment, and follow-up of brain tumour patients <ref type="bibr" target="#b26">[26]</ref>. Multiple MRI modalities offer complementary information for characterizing brain tumours and enhancing patient L. Jiang and Y. Mao-Contribute equally in this work.</p><p>management <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">27]</ref>. However, acquiring multi-modality MRI is time-consuming, expensive and sometimes infeasible in specific modalities, e.g., due to the hazard of contrast agent <ref type="bibr" target="#b14">[15]</ref>. Trans-modal MRI synthesis can establish the mapping from the known domain of available MRI modalities to the target domain of missing modalities, promising to generate missing MRI modalities effectively. The synthetic methods leveraging multi-modal MRI, i.e., many-to-one translation, have outperformed single-modality models generating a missing modality from another available modality, i.e., one-to-one translation <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b33">33]</ref>. Traditional multi-modal methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>, e.g., sparse encoding-based, patch-based and atlasbased methods, rely on the alignment accuracy of source and target domains and are poorly scalable. Recent generative adversarial networks (GANs) and variants, e.g., MM-GAN <ref type="bibr" target="#b23">[23]</ref>, DiamondGAN <ref type="bibr" target="#b12">[13]</ref> and ProvoGAN <ref type="bibr" target="#b30">[30]</ref>, have been successful based on multi-modal MRI, further improved by introducing multi-modal coding <ref type="bibr" target="#b31">[31]</ref>, enhanced architecture <ref type="bibr" target="#b6">[7]</ref>, and novel learning strategies <ref type="bibr" target="#b29">[29]</ref>.</p><p>Despite the success, GAN-based models are challenged by the limited capability of adversarial learning in modelling complex multi-modal data distributions <ref type="bibr" target="#b25">[25]</ref> Recent studies have demonstrated that GANs' performance can be limited to processing and generating data with less variability <ref type="bibr" target="#b0">[1]</ref>. In addition, GANs' hyperparameters and regularization terms typically require fine-tuning, which otherwise often results in gradient vanish and mode collapse <ref type="bibr" target="#b1">[2]</ref>.</p><p>Diffusion model (DM) has achieved state-of-the-art performance in synthesizing natural images, promising to improve MRI synthesis models. It shows superiority in model training <ref type="bibr" target="#b15">[16]</ref>, producing complex and diverse images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>, while reducing risk of modality collapse <ref type="bibr" target="#b11">[12]</ref>.For instance, Lyu et al. <ref type="bibr" target="#b13">[14]</ref> used diffusion and score-marching models to quantify model uncertainty from Monte-Carlo sampling and average the output using different sampling methods for CT-to-MRI generation; Ã–zbey et al. <ref type="bibr" target="#b19">[19]</ref> leveraged adversarial training to increase the step size of the inverse diffusion process and further designed a cycle-consistent architecture for unpaired MRI translation.</p><p>However, current DM-based methods focus on one-to-one MRI translation, promising to be improved by many-to-one methods, which requires dedicated design to balance the multiple conditions introduced by multi-modal MRI. Moreover, as most DMs operate in original image domain, all Markov states are kept in memory <ref type="bibr" target="#b8">[9]</ref>, resulting in excessive burden. Although latent diffusion model (LDM) <ref type="bibr" target="#b20">[20]</ref> is proposed to reduce memory consumption, it is less feasible for many-to-one MRI translation with multi-condition introduced. Further, diffusion denoising processes tend to change the original distribution structure of the target image due to noise randomness <ref type="bibr" target="#b13">[14]</ref>, rending DMs often ignore the consistency of anatomical structures embedded in medical images, leading to clinically less relevant results. Lastly, DMs are known for their slow speed of diffusion sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, challenging its wide clinical application.</p><p>We propose a DM-based multi-modal MRI synthesis model, CoLa-Diff, which facilitates many-to-one MRI translation in latent space, and preserve anatomical structure with accelerated sampling. Our main contributions include: -present a denoising diffusion probabilistic model based on multi-modal MRI.</p><p>As far as we know, this is the first DM-based many-to-one MRI synthesis model. -design a bespoke architecture, e.g., similar cooperative filtering, to better facilitate diffusion operations in the latent space, reducing the risks of excessive information compression and high-dimensional noise. -introduce structural guidance of brain regions in each step of the diffusion process, preserving anatomical structure and enhancing synthesis quality. -propose an auto-weight adaptation to balance multi-conditions and maximise the chance of leveraging relevant multi-modal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-conditioned Latent Diffusion Model</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the model design. As a latent diffusion model, CoLa-diff integrates multi-condition b from available MRI contrasts in a compact and low-dimensional latent space to guide the generation of missing modality x âˆˆ R HÃ—W Ã—1 . Precisely, b constitutes available contrasts and anatomical structure masks generated from the available contrasts. Similar to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">20]</ref>, CoLa-Diff invovles a forward and a reverse diffusion process. During forward diffusion, x 0 is encoded by E to produce Îº 0 , then subjected to T diffusion steps to gradually add noise and generate a sequence of intermediate representations: {Îº 0 , . . . , Îº T }.</p><p>The t-th intermediate representation is denoted as Îº t , expressed as:</p><formula xml:id="formula_0">Îº t = âˆš á¾±t Îº 0 + âˆš 1 -á¾±t , with âˆ¼ N (0, I)<label>(1)</label></formula><p>where á¾±t = t i=1 Î± i , Î± i denotes hyper-parameters related to variance. The reverse diffusion is modelled by a latent space network with parameters Î¸, inputting intermediate perturbed feature maps Îº t and y (compressed b) to predict noise level Î¸ (Îº t , t, y) for recovering feature maps Îºt-1 from previous,</p><formula xml:id="formula_1">Îºt-1 = âˆš á¾±t-1 ( Îº t - âˆš 1 -á¾±t â€¢ Î¸ (Îº t , t, y) âˆš á¾±t ) + 1 -á¾±t-1 â€¢ Î¸ (Îº t , t, y) (2)</formula><p>To enable effective learning of the underlying distribution of Îº 0 , the noise level needs to be accurately estimated. To achieve this, the network employs similar cooperative filtering and auto-weight adaptation strategies. Îº0 is recovered by repeating Eq. 2 process for t times, and decoding the final feature map to generate synthesis images x0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Space Network</head><p>We map multi-condition to the latent space network for guiding noise prediction at each step t. The mapping is implemented by N transformer blocks (Fig. <ref type="figure" target="#fig_0">1</ref> (D)), including global self-attentive layers, layer-normalization and position-wise MLP. Following the latent diffusion model (LDM) <ref type="bibr" target="#b20">[20]</ref>, the network Î¸ (Îº t , t, y) is trained to predict the noise added at each step using</p><formula xml:id="formula_2">L E := E E(x),y, âˆ¼N (0,1),t -Î¸ (Îº t , t, y) 2 2</formula><p>(3)</p><p>To mitigate the excessive information losses that latent spaces are prone to, we replace the simple convolution operation with a residual-based block (three sequential convolutions with kernels 1 * 1, 3 * 3, 1 * 1 and residual joins <ref type="bibr" target="#b7">[8]</ref>), and enlarge the receptive field by fusion (5 * 5 and 7 * 7 convolutions followed by AFF <ref type="bibr" target="#b5">[6]</ref>) in the down-sampling section. Moreover, to reduce high-dimensional noise generated in the latent space, which can significantly corrupt the quality of multi-modal generation. We design a similar cooperative filtering detailed below.</p><p>Similar Cooperative Filtering. The approach has been devised to filter the downsampled features, with each filtered feature connected to its respective upsampling component (shown in Fig. <ref type="figure" target="#fig_0">1 (F)</ref>). Given f , which is the downsampled feature of Îº t , suppose the 2D discrete wavelet transform Ï† <ref type="bibr" target="#b24">[24]</ref> decomposes the features into low frequency component f (i)</p><p>A and high frequency components</p><formula xml:id="formula_3">f (i) H , f (i) V , f (i) D , keep decompose f (i)</formula><p>A , where i is the number of wavelet transform layers. Previous work <ref type="bibr" target="#b4">[5]</ref> has shown to effectively utilize global information by considering similar patches. However, due to its excessive compression, it is less suitable for LDM. Here, we group the components and further filter by similar block matching Î´ <ref type="bibr" target="#b17">[18]</ref> or thresholding Î³, use the inverse wavelet transform Ï† -1 (â€¢) to reconstruct the denoising results, given f * .</p><formula xml:id="formula_4">f * = Ï† -1 (Î´(f (i) A ), Î´( i j=1 f (i) D ), Î³( i j=1 f (i) H ), Î³( i j=1 f (i) V )) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Guidance</head><p>Unlike natural images, medical images encompass rich anatomical information. Therefore, preserving anatomical structure is crucial for MRI generation. However, DMs often corrupt anatomical structure, and this limitation could be due to the learning and sampling processes of DMs that highly rely on the probability density function <ref type="bibr" target="#b8">[9]</ref>, while brain structures by nature are overlapping in MRI density distribution and even more complicated by pathological changes. Previous studies show that introducing geometric priors can significantly improve the robustness of medical image generation. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">28]</ref>. Therefore, we hypothesize that incorporating structural prior could enhance the generation quality with preserved anatomy. Specifically, we exploit FSL-FAST <ref type="bibr" target="#b32">[32]</ref> tool to segment four types of brain tissue: white matter, grey matter, cerebrospinal fluid, and tumour. The generated tissue masks and inherent density distributions (Fig. <ref type="figure" target="#fig_0">1 (E</ref>)) are then used as a condition y i to guide the reverse diffusion.</p><p>The combined loss function for our multi-conditioned latent diffusion is defined as</p><formula xml:id="formula_5">L MCL := L E + L KL (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where KL is the KL divergence loss to measure similarity between real q and predicted p Î¸ distributions of encoded images.</p><formula xml:id="formula_7">L KL := T -1 j=1 D KL (q (Îº j-1 | Îº j , Îº 0 ) p Î¸ (Îº j-1 | Îº j ))<label>(6)</label></formula><p>where D KL is the KL divergence function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Auto-Weight Adaptation</head><p>It is critical to balance multiple conditions, maximizing relevant information and minimising redundant information. For encoded conditions y âˆˆ R hÃ—wÃ—c , c is the number of condition channels. Set the value after auto-weight adaptation to á»¹, the operation of this module is expressed as (shown in Fig. <ref type="figure" target="#fig_0">1 (E)</ref>)</p><formula xml:id="formula_8">á»¹ = F (y|Î¼, Î½, o), with Î¼, Î½, o âˆˆ R c (7)</formula><p>The embedding outputs are adjusted by embedding weight Î¼. The autoactivation is governed by the learnable weight Î½ and bias o. </p><formula xml:id="formula_9">G c = Î¼ c y c 2 = Î¼ c h m=1 w n=1 (y m,n c ) 2 + 1 2 (8)</formula><p>where is a small constant added to the equation to avoid the issue of derivation at the zero point. The normalization method can establish stable competition between channels, G = {G c } S c=1 . We use L 2 normalization for cross-channel operations:</p><formula xml:id="formula_10">Äœc = âˆš SG c G 2 = âˆš SG c S c=1 G 2 c + 1 2<label>(9)</label></formula><p>where S denotes the scale. We use an activation mechanism for updating each channel to facilitate the maximum utilization of each condition during diffusion model training, and further enhance the synthesis performance. Given the learnable weight</p><formula xml:id="formula_11">Î½ = [Î½ 1 , Î½ 2 , ..., Î½ c ] and bias o = [o 1 , o 2 , ..., o c ] we compute á»¹c = y c [1 + S(Î½ c Äœc + o c )] (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>which gives new representations á»¹c of each compressed conditions after the automatic weighting. S(â€¢) denotes the Sigmoid activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparisons with State-of-the-Art Methods</head><p>Datasets and Baselines. We evaluated CoLa-Diff on two multi-contrast brain MRI datasets: BRATS 2018 and IXI datasets. The BRATS 2018 contains MRI scans from 285 glioma patients. Each includes four modalities: T1, T2, T1ce, and FLAIR. We split them into (190:40:55) for training/validation/testing. For each subject, we automatically selected axial cross-sections based on the perceptible effective area of the slices, and then cropped the selected slices to a size of 224 Ã— 224. The IXI<ref type="foot" target="#foot_0">1</ref> dataset consists of 200 multi-contrast MRIs from healthy brains, plit them into (140:25:35) for training/validation/testing. For preprocessing, we registered T2-and PD-weighted images to T1-weighted images using FSL-FLIRT <ref type="bibr" target="#b9">[10]</ref>, and other preprocessing are identical to the BRATS 2018. We compared CoLa-Diff with four state-of-the-art multi-modal MRI synthesis methods: MM-GAN <ref type="bibr" target="#b23">[23]</ref>, Hi-Net <ref type="bibr" target="#b33">[33]</ref>, ProvoGan <ref type="bibr" target="#b30">[30]</ref> and LDM <ref type="bibr" target="#b20">[20]</ref>. Implementation Details. Our code is publicly available at https://github. com/SeeMeInCrown/CoLa Diff MultiModal MRI Synthesis. The hyperparameters of CoLa-Diff are defined as follows: diffusion steps to 1000; noise schedule to linear; attention resolutions to 32, 16, 8; batch size to 8, learning rate to 9.6e -5.</p><p>The noise variances were in the range of Î² 1 = 10 -4 and Î² T = 0.02. An exponential moving average (EMA) over model parameters with a rate of 0.9999 was employed. The model is trained on 2 NVIDIA RTX A5000, 24 GB with Adam optimizer on PyTorch. An acceleration method <ref type="bibr" target="#b10">[11]</ref> based on knowledge distillation was applied for fast sampling. Quantitative Results. We performed synthesis experiments for all modalities, with each modality selected as the target modality while remaining modalities and the generated region masks as conditions. Seven cases were tested in two datasets (Table <ref type="table" target="#tab_0">1</ref>). The results show that CoLa-Diff outperforms other models by up to 6.01 dB on PSNR and 5.74% on SSIM. Even when compared to the best of other models in each task, CoLa-Diff is a maximum of 0.81 dB higher in PSNR and 0.82% higher in SSIM. Qualitative Results. The first three and last three rows in Fig. <ref type="figure" target="#fig_2">2</ref> illustrate the synthesis results of T1ce from BRATS and PD from the IXI, respectively. From the generated images, we observe that CoLa-Diff is most comparable to the ground truth, with fewer errors shown in the heat maps. The synthesis uncertainty for each region is derived by performing 100 generations of the same slice and calculating the pixel-wise variance. From the uncertainty maps, CoLa-Diff is more confident in synthesizing the gray and white matter over other comparison models. Particularly, CoLa-Diff performs better in generating complex brain sulcus and tumour boundaries. Further, CoLa-Diff could better maintain the anatomical structure over comparison models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study and Multi-modal Exploitation Capabilities</head><p>We verified the effectiveness of each component in CoLa-Diff by removing them individually. We experimented on BRATS T1+T1ce+FLAIRâ†’T2 task with four absence scenarios (Table <ref type="table" target="#tab_2">2</ref> top). Our results show that each component contributes to the performance improvement, with Auto-weight adaptation bringing a PSNR increase of 1.9450dB and SSIM of 4.0808%.</p><p>To test the generalizability of CoLa-Diff under the condition of varied inputs, we performed the task of generating T2 on two datasets with progressively increasing input modalities (Table <ref type="table" target="#tab_2">2</ref> bottom). Our results show that our model performance increases with more input modalities: SSIM has a maximum uplift value of 1.9603, PSNR rises from 26.6355 dB to 28.3126 dB in BRATS; from 32.164 dB to 32.8721 dB in IXI. The results could further illustrate the ability of CoLa-Diff to exploit multi-modal information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents CoLa-Diff, a DM-based multi-modal MRI synthesis model with a bespoke design of network backbone, similar cooperative filtering, structural guidance and auto-weight adaptation. Our experiments support that CoLa-Diff achieves state-of-the-art performance in multi-modal MRI synthesis tasks. Therefore, CoLa-Diff could serve as a useful tool for generating MRI to reduce the burden of MRI scanning and benefit patients and healthcare providers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of CoLa-Diff. During the forward diffusion, Original images x0 are compressed using encoder E to get Îº0, and after t steps of adding noise, the images turn into Îºt. During the reverse diffusion, the latent space network Î¸ (Îºt, t, y) predicts the added noise, and other available modalities and anatomical masks as structural guidance are encoded to y, then processed by the auto-weight adaptation block W and embedded into the latent space network. Sampling from the distribution learned from the network gives Îº0, then Îº0 are decoded by D to obtain synthesized images.</figDesc><graphic coords="3,44,79,54,41,334,57,181,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y c indicates each channel of y, where y c = [y m,n c ] hÃ—w âˆˆ R hÃ—w , y m,n c is the eigenvalue at position (m, n) in channel c. We use large receptive fields and contextual embedding to avoid local ambiguities, providing embedding weight Î¼ = [Î¼ 1 , Î¼ 2 ..., Î¼ c ]. The operation G c is defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of synthesized images, detail enlargements (row 1 and 4), corresponding error maps (row 2 and 5) and uncertainty maps (row 3 and 6).</figDesc><graphic coords="7,44,79,134,24,334,57,328,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance in BRATS (top) and IXI (bottom). PSNR (dB) and SSIM (%) are listed as meanÂ±std in the test set. Boldface marks the top models.</figDesc><table><row><cell></cell><cell cols="2">T2+T1ce+FLAIR</cell><cell cols="2">T1+T1ce+FLAIR</cell><cell cols="2">T2+T1+FLAIR</cell><cell cols="2">T2+T1ce+T1</cell></row><row><cell>Model (BRATS 2018)</cell><cell></cell><cell>â†’T1</cell><cell></cell><cell>â†’T2</cell><cell cols="2">â†’T1ce</cell><cell></cell><cell>â†’FLAIR</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM%</cell><cell>PSNR</cell><cell>SSIM%</cell><cell>PSNR</cell><cell>SSIM%</cell><cell>PSNR</cell><cell>SSIM%</cell></row><row><cell>MM-GAN</cell><cell cols="8">25.78Â±2.16 90.67Â±1.45 26.11Â±1.62 90.58Â±1.39 26.30Â±1.91 91.22Â±2.08 24.09Â±2.14 88.32Â±1.98</cell></row><row><cell>Hi-Net</cell><cell cols="8">27.42Â±2.58 93.46Â±1.75 25.64Â±2.01 92.59Â±1.42 27.02Â±1.26 93.35Â±1.34 25.87Â±2.82 91.22Â±2.13</cell></row><row><cell>ProvoGAN</cell><cell cols="8">27.79Â±4.42 93.51Â±3.16 26.72Â±2.87 92.98Â±3.91 29.26Â±2.50 93.96Â±2.34 25.64Â±2.77 90.42Â±3.13</cell></row><row><cell>LDM</cell><cell cols="8">24.55Â±2.62 88.34Â±2.51 24.79Â±2.67 88.47Â±2.60 25.61Â±2.48 89.18Â±2.55 23.12Â±3.16 86.90Â±3.24</cell></row><row><cell>CoLa-Diff (Ours)</cell><cell cols="8">28.26Â±3.13 93.65Â±3.02 28.33Â±2.27 93.80Â±2.75 29.35Â±2.40 94.18Â±2.46 26.68Â±2.74 91.89Â±3.11</cell></row><row><cell></cell><cell></cell><cell cols="2">T1+T2 â†’PD</cell><cell cols="2">T2+PD â†’T1</cell><cell></cell><cell cols="2">T1+PD â†’T2</cell></row><row><cell>Model (IXI)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSNR</cell><cell cols="2">SSIM%</cell><cell>PSNR</cell><cell>SSIM%</cell><cell>PSNR</cell><cell></cell><cell>SSIM%</cell></row><row><cell>MM-GAN</cell><cell cols="8">30.61Â±1.64 95.42Â±1.90 27.32Â±1.70 92.35Â±1.58 30.87Â±1.75 94.68Â±1.42</cell></row><row><cell>Hi-Net</cell><cell cols="8">31.79Â±2.26 96.51Â±2.03 28.89Â±1.43 93.78Â±1.31 32.58Â±1.85 96.54Â±1.74</cell></row><row><cell>ProvoGAN</cell><cell cols="8">29.93Â±3.11 94.62Â±2.40 24.21Â±2.63 90.46Â±3.58 29.19Â±3.04 94.08Â±3.87</cell></row><row><cell>LDM</cell><cell cols="8">27.36Â±2.48 91.52Â±2.39 24.19Â±2.51 88.75Â±2.47 27.04Â±2.31 91.23Â±2.24</cell></row><row><cell>CoLa-Diff (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>32.24Â±2.95 96.95Â±2.26 30.20Â±2.38 94.49Â±2.15 32.86Â±2.83 96.57Â±2.27</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation of four individual components (First four lines) and Multi-modal information utilisation (Last three lines). Boldface marks the best performing scenarios on each dataset.</figDesc><table><row><cell></cell><cell>PSNR</cell><cell>SSIM%</cell></row><row><cell cols="3">w/o Modified latent diffusion network 27.1074 90.1268</cell></row><row><cell>w/o Structural guidance</cell><cell cols="2">27.7542 91.4865</cell></row><row><cell>w/o Auto-weight adaptation</cell><cell cols="2">26.3896 89.7129</cell></row><row><cell>w/o Similar cooperative filtering</cell><cell cols="2">27.9753 92.1584</cell></row><row><cell>T1 (BRATS)</cell><cell cols="2">26.6355 91.7438</cell></row><row><cell>T1+T1ce (BRATS)</cell><cell cols="2">27.3089 92.9772</cell></row><row><cell>T1+T1ce+Flair (BRATS)</cell><cell cols="2">28.3126 93.7041</cell></row><row><cell>T1 (IXI)</cell><cell cols="2">32.1640 96.0253</cell></row><row><cell>T1+PD (IXI)</cell><cell cols="2">32.8721 96.5932</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://brain-development.org/ixi-dataset/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing what a GAN cannot generate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A closer look at the optimization landscapes of generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04848</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-infrared (NIR) tomography breast image reconstruction with a priori structural information from MRI: algorithm development for reconstructing heterogeneities</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Brooksby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Pogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Paulsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Quantum Electron</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="209" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Importance of multimodal MRI in characterizing brain tissue and its potential application for individual age prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cherubini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Caligiuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sabatini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1232" to="1239" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image restoration by sparse 3D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing: Algorithms and Systems VI</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6812</biblScope>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attentional feature fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oehmcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<idno>CoRR abs/2009.14082</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ResViT: residual vision transformers for multimodal medical image synthesis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dalmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2598" to="2614" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global optimisation method for robust affine registration of brain images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00132</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SRDiff: single image super-resolution with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">479</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DiamondGAN: unified multi-modal generative adversarial networks for MRI sequences synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_87</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-987" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="795" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conversion between CT and MRI images using diffusion and score-matching models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12104</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Chemistry of Contrast Agents in Medical Magnetic Resonance Imaging</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Merbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Helm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Toth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>MÃ¼ller-Franzes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.07501</idno>
		<title level="m">Diffusion probabilistic models beat gans on medical images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Block matching: a general framework to improve robustness of rigid registration of medical images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2000</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Delp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Digoia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Jaramaz</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1935</biblScope>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-40899-4_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-40899-457" />
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised medical image translation with adversarial diffusion models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ã–zbey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08208</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A compressed sensing approach for MR tissue contrast synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prince</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-22092-0_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-22092-0" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2011</title>
		<editor>
			<persName><forename type="first">G</forename><surname>SzÃ©kely</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Hahn</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6801</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Magnetic resonance image example-based contrast synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2348" to="2363" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Missing MRI pulse sequence synthesis using multimodal generative adversarial network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: wedding the a trous and Mallat algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting and mode collapse in GANs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Thanh-Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Magnetic Resonance Imaging: Theory and Practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vlaardingerbroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Boer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-modal learning for predicting the genotype of glioma</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ea-GANs: edgeaware generative adversarial networks for cross-modality MR image synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1750" to="1762" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MouseGAN++: unsupervised disentanglement and contrastive representation for multiple MRI modalities synthesis and structural segmentation of mouse brain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1197" to="1209" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressively volumetrized deep generative models for data-efficient contextual learning of MR image recovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ã–zbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tinaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102429</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-modal MRI image synthesis via GAN with multi-scale gate mergence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hi-Net: hybrid-fusion network for multi-modal MR image synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2772" to="2781" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
