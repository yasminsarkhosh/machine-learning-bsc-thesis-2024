<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Reconstruction for Deep Learning PET Head Motion Correction</title>
				<funder ref="#_D9AqgPT">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Institute Of Biomedical Imaging And Bioengineering</orgName>
					<orgName type="abbreviated">NIBIB</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Zeng</surname></persName>
							<email>tianyi.zeng@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiazhen</forename><surname>Zhang</surname></persName>
							<email>jiazhen.zhang@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">El√©onore</forename><forename type="middle">V</forename><surname>Lieffrig</surname></persName>
							<email>eleonore.lieffrig@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuotong</forename><surname>Cai</surname></persName>
							<email>zhuotong.cai@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuyao</forename><surname>Chen</surname></persName>
							<email>fuyao.chen@yale.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<email>chenyu.you@yale.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mika</forename><surname>Naganawa</surname></persName>
							<email>mika.naganawa@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yihuan</forename><surname>Lu</surname></persName>
							<email>yihuan.lu@united-imaging.com</email>
							<affiliation key="aff4">
								<orgName type="department">United Imaging Healthcare</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Onofrey</surname></persName>
							<email>john.onofrey@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Urology</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Reconstruction for Deep Learning PET Head Motion Correction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="710" to="719"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0DD23753CE5CA3FC84BAC203988A1F48</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>PET fast reconstruction</term>
					<term>Data-driven motion correction</term>
					<term>Brain PET</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Head motion correction is an essential component of brain PET imaging, in which even motion of small magnitude can greatly degrade image quality and introduce artifacts. Building upon previous work, we propose a new head motion correction framework taking fast reconstructions as input. The main characteristics of the proposed method are: (i) the adoption of a high-resolution short-frame fast reconstruction workflow; (ii) the development of a novel encoder for PET data representation extraction; and (iii) the implementation of data augmentation techniques. Ablation studies are conducted to assess the individual contributions of each of these design choices. Furthermore, multi-subject studies are conducted on an 18 F-FPEB dataset, and the method performance is qualitatively and quantitatively evaluated by MOLAR reconstruction study and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation. Additionally, we also compared our method with a conventional intensity-based registration method. Our results demonstrate that the proposed method outperforms other methods on all subjects, and can accurately estimate motion for subjects out of the training set. All code is publicly available on GitHub: https:// github.com/OnofreyLab/dl-hmc fast recon miccai2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron emission tomography (PET) has been widely used in human brain imaging, thanks to the availability of a vast array of specific radiotracers. These compounds allow for studying various neurotransmitters and receptor dynamics for different brain targets <ref type="bibr" target="#b10">[11]</ref>. Brain PET images are commonly used to diagnose and monitor neurodegenerative diseases, such as Alzheimer's disease, Parkinson's disease, epilepsy, and certain types of brain tumors <ref type="bibr" target="#b2">[3]</ref>. Head motion in PET imaging reduces brain image resolution, lowers tracer distribution estimation, and introduces attenuation correction (AC) mismatch artifacts <ref type="bibr" target="#b11">[12]</ref>. Consequently, the capability to monitor and correct head motion is of utmost importance in brain PET studies.</p><p>The first step of PET head motion correction is motion tracking. When head motion information is acquired, either frame-based motion correction or eventby-event (EBE) motion correction methods can be applied in the reconstruction workflow to derive motion-free PET images. EBE motion correction provides better results for real-time motion tracking compared to frame-based methods, as the latter does not allow for correction of motion that occurs within each dynamic frame <ref type="bibr" target="#b0">[1]</ref>. Currently, there are two main categories of head motion tracking methods, hardware-based motion tracking (HMT) and data-driven methods. For HMT, head motion is obtained from external devices. Generally, HMT systems offer accurate tracking results with high time resolution. Marker-based HMT such as Polaris Vicra (NDI, Canada) use light-reflecting markers on the patient's head and track the markers for motion correction <ref type="bibr" target="#b5">[6]</ref>. However, Vicra is not routinely used in the clinic, as setup and calibration of the tracking device can be complicated and attaching markers to each patient increases the logistical burden of the scan. In response, some researchers began to use markerless motion tracking systems for brain PET <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. These methods typically rely on the use of cameras and computer vision algorithms to detect and analyze the movement of a person's head in real-time, but these methods still require additional hardware setup. In data-driven motion tracking methods, head motion is estimated from PET reconstructions or raw data. With the development of commercial PET systems and technological advancements such as time of flight (TOF), data-driven head PET motion tracking has shown promising results in reducing motion artifacts and improving image quality. For instance, <ref type="bibr" target="#b11">[12]</ref> developed a novel data-driven head motion detection method based on the centroid of distribution (COD) of PET 3D point cloud image (PCI). Image registration methods that seek to align two or more images offer a data-driven solution for correcting head motion. Intensity-based registration methods have been used to track head motion using good-quality PET reconstruction frames to achieve stable performance <ref type="bibr" target="#b13">[14]</ref>. However, because of the dynamic change in PET images, current registration-based methods need to split the data into several discrete time frames, e.g., 5 min. Therefore, they will introduce a cumulative error when dealing with inter-frame motion. Finally, inspired by the development of deep learning-based registration methods, a deep learning head motion correction (DL-HMC) network using Vicra as ground truth was proposed <ref type="bibr" target="#b14">[15]</ref>. This study achieved accurate motion tracking on single subject testing data, but showed less accurate motion predictions for multi-subject motion studies. Meanwhile, the input images were low-resolution PCIs without TOF and had large voxel spacing, which can negatively affect motion tracking accuracy.</p><p>In this study, we proposed a new method to perform deep learning-based brain PET motion prediction across multiple subjects by utilizing high-resolution one-second fast reconstruction images (FRIs) with TOF. A novel encoder and data augmentation strategy was also applied to improve model performance. Ablation studies were conducted to assess the individual contributions of key method components. Multi-subject studies were conducted on a dataset of 20 subject and its results were quantitatively and qualitatively evaluated by MOLAR reconstruction studies and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We identified 20 18 F-FPEB studies from a database of brain PET scans acquired on a conventional mCT scanner (Siemens, Germany) at the Yale PET center. All subjects are healthy controls and the mean activity injected was 3.90 ¬± 1.02 mCi. The mean translational and rotational motions across time and scans are 3.75 ¬± 6.88 mm and 3.30 ¬± 8.77 ‚Ä¢ , respectively. PET list-mode data and Vicra motion tracking information are available for each subject, as well as T1-weighted MR images and MR-space to PET-space transformation matrices. We consider data acquired between 60 and 90 min post injection (30 min total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fast Reconstruction Images</head><p>To overcome the challenges when using low-quality, noisy PCI for motion correction, ultra-fast reconstruction techniques <ref type="bibr" target="#b13">[14]</ref> that generate one-second dynamic fast reconstruction images (FRIs), can be utilized as input for deep learning motion correction methods. Leveraging the availability of CPU-parallel reconstruction platforms <ref type="bibr" target="#b4">[5]</ref>, we develop a reconstruction package for one-second FRI. Our proposed method employs a TOF-based projector and utilizes pure maximum-likelihood expectation maximization (MLEM) for reconstruction <ref type="bibr" target="#b9">[10]</ref>. Attenuation correction (AC) and scatter correction are turned off to avoid AC mismatch and expensive computation. Normalization correction, random correction and decay correction are applied and the iteration number was set to two. Standard Uptake Value (SUV) calculation was also conducted to normalize the activities between different subjects. The final reconstructed image dimension is 150 √ó 150 √ó 111, with voxels spaced at 2.04 √ó 2.04 √ó 2.00 mm 3 . For comparison purposes, we also computed the same resolution PCI by TOF back-projection of the PET list-mode data along the line-of-response (LOR) with normalization for scanner sensitivity. Both FRI and PCI were resized to 96 √ó 96 √ó 64, with voxel spacing of 3.18 √ó 3.18 √ó 3.45 mm 3 . As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the resized FRI and PCI from the same time pairs are displayed. Due to the PET corrections, the FRI quality and noise level is superior to the PCI, particularly in areas outside of the head. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Motion Correction Framework</head><p>Network Architecture. We propose a modified version of the DL-HMC framework to learn rigid head motion in a supervised manner <ref type="bibr" target="#b14">[15]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref>). Our proposed method uses two FRIs I ref and I mov from two different time points t ref and t mov to predict the relative rigid motion transformation between the reference and moving time points with respect to the Vicra gold-standard. Our encoder consists of 3 convolution layers with kernel size of 5 3 and an intermediate convolution layer with convolution size of 3 3 . PReLU activation layers follow each convolution. In addition, we add dropout in the regression layers with rate 0.3. In this new architecture, the embedding space consists of feature maps of size 16 √ó 4 √ó 4 √ó 2, which preserves the spatial information in the FRI. No padding is applied to the images or feature maps. The extracted features are fed into the fully connected regression block to predict the six translation and rotation components of the relative rigid motion. Data Augmentation. To improve the performance and generalizability of our network, we use a task-specific data augmentation strategy to expose it to more varied and diverse training data. As a rule of thumb, translations of 2-5 mm and rotations of 2 ‚Ä¢ -3 ‚Ä¢ are common and larger magnitudes are expected without restraint or if non-customized supports are used <ref type="bibr" target="#b8">[9]</ref>. Due to our sampling strategy during model training, statistically, most of the sampled pairs will have small relative motion. However, during the inference, the relative motion between the moving frames at late time points and the reference frame at the beginning will be large due to the accumulation of motion. Therefore, the model may not be able to make accurate predictions when facing large relative motions. To take this problem into account, we perform data augmentation by simulating an additional relative motion that can be concatenated with the true relative motion. To be specific, the synthetic translation and rotation are uniformly sampled in the range of <ref type="bibr">[-10, 10]</ref> mm and [-5, 5] ‚Ä¢ , respectively. The randomly simulated motion T will be applied to the moving frame to generate a synthetic moving frame T ‚Ä¢ I ref and be concatenated with the real relative motion to acquire the synthetic relative motion between the reference and the synthetic moving frame. The synthetic moving frame and the synthetic relative motion will be used for training to increase the data variability.</p><p>Network Training and Inference. To train the network, we randomly sampled image pairs (t ref , t mov ) under the condition (t ref &lt; t mov ). The network was optimized by minimizing the mean square error (MSE) between the predicted motion estimate and Vicra parameters. More specifically, the prediction error for a given pair of reference and moving clouds is defined as L( Œ∏, Œ∏) = Œ∏Œ∏ 2 with Œ∏ = [t x , t y , t z , r x , r y , r z ] the Vicra information for the three translational and three rotational parameters (t x , t y , t z ) and (r x , r y , r z ), respectively, and Œ∏ the network prediction. After training the model, we perform motion tracking inference by setting the image from first time point t ref = 3,600 (60 min post-injection) as the reference image and predict the motion from this reference image to all subsequent one-second image frames in the next 30 min (1,800 one-second time points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We performed quantitative and qualitative experiments to validate our approach. We evaluated motion correction performance by comparing our proposed method to DL-HMC <ref type="bibr" target="#b14">[15]</ref>, intensity-based registration using the BioImage Suite (BIS) software package <ref type="bibr" target="#b6">[7]</ref> using the one-second FRIs, and ablation studies to demonstrate the effectiveness of our design choices. We qualitatively assessed motion correction performance by reconstructing the PET images with motion tracking result and comparing to DL-HMC and the Vicra gold-standard reconstruction results. We split our dataset of 20 subjects into distinct subsets for training and testing with 14 and 6 subjects, respectively. From the training cohort, we randomly sampled 10% of the time frames to be used as a validation set. Training the network required 6,000 epochs for convergence using a minibatch size of 64. Adam optimization was used with initial learning rate set to 5e-4, Œ≥ set to 0.98, and exponential decay with a step size of 150. All computations were performed on a server with Intel Xeon Gold 5218 processors, 256 GB RAM, and an NVIDIA Quadro RTX 8000 GPU (48 GB RAM). The network was implemented in Python (v3.9) using PyTorch (v1.13.1) and MONAI (v1.0.1).</p><p>Table <ref type="table">1</ref>. Quantitative motion correction results. We compared our proposed approach using fast reconstruction image (FRI) as input with DL-HMC and with using point cloud image (PCI) as input. An ablation study quantifies the effect of stochastic data augmentation (DA) and to standard intensity-based registration (BIS). Reported values are MSE (mean ¬± SD) comparing motion estimates to Vicra gold-standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Val. loss Test Set Total loss Translation (mm) Rotation ( Quantitative Evaluation. For quantitative comparisons of motion tracking, we compare MSE loss in the validation set and test set. We calculate MSE for the 6 parameter rigid motion as well as the translation and rotational components separately. To verify feasibility of traditional intensity-based registration method on FRIs, we use BIS with a multi-resolution hierarchical representation (3 levels) and minimize the sum of squared differences (SSD) similarity metric (Fig. <ref type="figure" target="#fig_0">S1</ref> shows a BIS result on a example testing subject). Compared to Vicra gold-standard, BIS fails to predict the motion. We evaluate the following motion prediction methods (Table <ref type="table">1</ref>): (i) DL-HMC with PCI as input (DL-HMC PCI);</p><p>(ii) DL-HMC with FRI as input (DL-HMC FRI); (iii) proposed network with PCI as input (Proposed PCI); (iv) proposed network with FRI as input (Proposed FRI); and (v) proposed method with FRI but without the data augmentation module (Proposed w/o DA); Results demonstrate that the proposed network with FRI input provides the best motion tracking performance in both validation and testing data. We also observes that using FRI yields a lower loss for the proposed network, indicating that high image quality enhanced the motion correction performance. For testing translation results, Proposed PCI outperforms Proposed FRI and has similar total motion loss, which indicates that the proposed network can still estimate motion on testing subjects even with noisy input. Figure <ref type="figure" target="#fig_1">2</ref> shows motion prediction results for different variations of the proposed method in a single test subject. These results show that the proposed FRI method is more similar to Vicra than the other methods, especially for translation in the x and z directions. However, the Proposed FRI method exhibits higher variance than other methods, which may be a result of the data augmentation distribution. Overall, our experiments demonstrate that the strategies in proposed FRI method enhance the motion tracking performance of the network.</p><p>Qualitative Reconstruction Evaluation. After inference, the 6 rigid degrees of freedom transformation estimated from the network were used to reconstruct the PET images using Motion-compensation OSEM List-mode Algorithm for Resolution-Recovery Reconstruction (MOLAR) algorithm <ref type="bibr" target="#b4">[5]</ref>. We applied PET head motion correction using the Proposed FRI model and compared with Vicra and no motion correction (NMC) reconstruction results. Figure <ref type="figure" target="#fig_2">3</ref> shows the reconstruction results for the same testing subject in quantitative evaluation.</p><p>Based on the tracer distribution of 18 F-PEB, we selected some frames from reconstructed images to illustrate the proposed FRI motion correction performance. In general, the Proposed FRI results in qualitatively enhanced anatomical interpretation of PET images. In Fig. <ref type="figure" target="#fig_2">3</ref>, NMC reconstruction has motion blurring on margins of the brain, while Proposed FRI reconstruction shows welldefined gyrus and sulcus comparable to Vicra reconstruction. 18 F-FPEB tracer is a metabotropic glutamate 5 receptor antagonist with moderate to high accumulation in multiple brain regions such as insula/caudate nucleus, thalamus and temporal lobe. Thus, we compared visualization of these regions among NMC, Proposed FRI and Vicra reconstructions (Fig. <ref type="figure" target="#fig_2">3</ref>). Specifically, our Proposed FRI method yields clear delineation of the insula/caudate nucleus, thalamus, and temporal lobe nearly indistinguishable from Vicra reconstructed images.</p><p>In addition, brain region of interest (ROI) analyses were also performed for quantitative use. Each subject's MR image was segmented into 74 regions using FreeSurfer software <ref type="bibr" target="#b1">[2]</ref>. These regions were then merged into twelve large grey matter (GM) ROIs. For all testing subjects, the SUV difference of 12 ROIs from DL-HMC, Proposed PCI, and Proposed FRI reconstruction were calculated for comparison with Vicra reference (Fig. <ref type="figure" target="#fig_2">3 right</ref>). The proposed FRI method yields the lowest absolute difference (0.8%) from Vicra images in SUV, while the absolute SUV difference for DL-HMC FRI is 1.5% and for NMC is 1.9%. Specifically, results of proposed FRI method are closest to Vicra results in regions such as thalamus, temporal lobe, insula (absolute activity difference are 0.5%, 0.7%, 0.9%, respectively), which are the target areas of 18 F-FPEB tracer with highest empirical tracer accumulation. The reconstruction and ROI evaluation results indicate that the proposed FRI method holds potential to improve clinical applicability through amelioration of PET motion correction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this work, we propose a new head motion correction approach using fast reconstructions as input. The proposed method outperforms other methods in a multi-subject cohort, and ablation studies demonstrate the effectiveness of our strategies. We apply our proposed FRI motion correction to get motion-free reconstruction using MOLAR. The proposed FRI method achieves good image quality and similar ROI evaluation results compared to Vicra gold-standard HMT. In this study, we showed that conventional intensity-based registration fails at performing motion tracking on FRI data. This is likely due to the PET dynamic changes and non-optimal registration parameters. Compared with previous deep learning motion correction <ref type="bibr" target="#b14">[15]</ref>, the training speed and GPU memory usage of the proposed method are much better thanks to the proposed shallower encoder architecture and our efficient training and testing strategies. Though HMT method such as Vicra achieves good accuracy and time resolution for PET head motion tracking, two common types of Vicra failure may occur: slipping and wobbling. Our method would be robust enough to compensate for the Vicra failure. Because of the limited Vicra data, in the future, we will develop semi-supervised deep learning methods for PET head motion correction. Our study used TOF PET data because it can yield high signal to noise ratio (SNR) for both FRI and PCI due to the better location identification of photons, thus the one-second FRI still retains some essential brain structures. Limitations of this work include partial limited tracking time and low time resolution compared to HMT methods mentioned in Sect. 1. In the future, with the development of PET techniques such as depth-of-interaction <ref type="bibr" target="#b7">[8]</ref>, higher resolution and sensitivity PET will be available. Such PET will give data-driven PET motion correction a revolutionary opportunity to have more accurate tracking and higher time resolution. We plan to apply the proposed method to other datasets, developing a generalized model for multi-tracer and multi-scanner PET data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed network architecture and different network inputs comparison. The proposed network takes two one-second FRIs as input to encoder block with shared weights. Stochastic motion T from the data augmentation block is applied to the moving reconstruction, and the reverse augmentation T -1 is applied on corresponding Vicra data to match the augmented FRI pairs. A regression block then estimates the rigid motion transformation parameters. PCI input is shown on the left for visual comparison.</figDesc><graphic coords="4,71,91,62,87,102,40,108,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Motion information comparison for testing subject inference. Columns show rigid transformation parameters from Vicra (Orange), DL-HMC FRI (yellow), proposed network without data augmentation (purple), and proposed FRI method (blue). (Color figure online)</figDesc><graphic coords="7,42,81,54,26,339,01,159,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. PET image reconstruction and ROI evaluation results. MOLAR reconstructed images using Vicra gold-standard motion correction, the proposed FRI motion correction, and no motion correction (NMC). The table on the right shows quantitative SUV difference values with respect to Vicra.</figDesc><graphic coords="8,70,47,54,32,311,38,170,38" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. Research reported in this publication was supported by the <rs type="funder">National Institute Of Biomedical Imaging And Bioengineering (NIBIB)</rs> of the <rs type="funder">National Institutes of Health (NIH)</rs> under Award Number <rs type="grantNumber">R21 EB028954</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the <rs type="funder">NIH</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_D9AqgPT">
					<idno type="grant-number">R21 EB028954</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 67.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Design of a motioncompensation OSEM list-mode algorithm for resolution-recovery reconstruction for the HRRT</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Liow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Nuclear Science Symposium. Conference Record</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3281" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freesurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FDG PET imaging in patients with pathologically verified dementia</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1920" to="1928" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marker-less and calibration-less motion correction method for brain pet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Phys. Technol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">List-mode reconstruction for the biograph MCT with physics modeling and event-by-event motion correction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">5567</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of frame-based and event-by-event motion-correction methods for awake monkey brain pet imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mulnix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sandiego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Carson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified framework for development, deployment and robust testing of neuroimaging algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Design and performance of SIAT APET: a uniform highresolution small animal pet scanner using dual-ended readout detectors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">235013</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion estimation and correction in SPECT, PET and CT</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Kyme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Fulton</surname></persName>
		</author>
		<idno>18TR02</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">18</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EM reconstruction algorithms for emission and transmission tomography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Assist. Tomogr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="306" to="316" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Positron emission tomography: human brain function and biochemistry</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mazziotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">4701</biblScope>
			<biblScope unit="page" from="799" to="809" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive data-driven motion detection and optimized correction for brain pet</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Revilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">252</biblScope>
			<biblScope unit="page">119031</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markerless motion tracking and correction for PET, MRI, and simultaneous PET/MRI</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Slipsager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">215524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ultra-fast listmode reconstruction of short pet frames and example applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Spangler-Bickell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Deller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bettinardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="292" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised deep learning for head motion correction in pet</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-819" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
