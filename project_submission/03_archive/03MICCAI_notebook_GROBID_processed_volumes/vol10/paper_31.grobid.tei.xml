<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images</title>
				<funder ref="#_amWDeGT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingjie</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Future Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulu</forename><surname>Gan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangxu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Future Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Future Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aimin</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dawei</forename><surname>Li</surname></persName>
							<email>lidawei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Future Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Transcend Vivoscope Biotech</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="323" to="332"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">765C172151236DEBA7A8E2A951CB1C59</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Isotropic reconstruction</term>
					<term>Unsupervised method</term>
					<term>Diffusion model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Three-dimensional microscopy is often limited by anisotropic spatial resolution, resulting in lower axial resolution than lateral resolution. Current State-of-The-Art (SoTA) isotropic reconstruction methods utilizing deep neural networks can achieve impressive super-resolution performance in fixed imaging settings. However, their generality in practical use is limited by degraded performance caused by artifacts and blurring when facing unseen anisotropic factors. To address these issues, we propose DiffuseIR, an unsupervised method for isotropic reconstruction based on diffusion models. First, we pre-train a diffusion model to learn the structural distribution of biological tissue from lateral microscopic images, resulting in generating naturally high-resolution images. Then we use low-axial-resolution microscopy images to condition the generation process of the diffusion model and generate high-axial-resolution reconstruction results. Since the diffusion model learns the universal structural distribution of biological tissues, which is independent of the axial resolution, DiffuseIR can reconstruct authentic images with unseen low-axial resolutions into a high-axial resolution without requiring re-training. The proposed DiffuseIR achieves SoTA performance in experiments on EM data and can even compete with supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Three-dimensional (3D) microscopy imaging is crucial in revealing biological information from the nanoscale to the microscale. Isotropic high resolution across M. Pan and Y. Gan-Equal contribution.</p><p>all dimensions is desirable for visualizing and analyzing biological structures. Most 3D imaging techniques have lower axial than lateral resolution due to physical slicing interval limitation or time-saving consideration <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. Effective isotropic super-resolution algorithms are critical for high-quality 3D image reconstructions, such as electron microscopy and fluorescence microscopy.</p><p>Recently, deep learning methods have made significant progress in image analysis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. To address the isotropic reconstruction problem, <ref type="bibr" target="#b8">[9]</ref> employs isotropic EM images to generate HR-LR pairs at axial and train a super-resolution model in a supervised manner, demonstrating the feasibility of inferring HR structures from LR images. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> use 3D point spread function (PSF) as a prior for self-supervised super-resolution. However, isotropic highresolution images or 3D point spread function (PSF) physical priors are difficult to obtain in practical settings, thus limiting these algorithms. Some methods like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> have skillfully used cycleGAN <ref type="bibr" target="#b31">[32]</ref> architecture to train axial superresolution models without depending on isotropic data or physical priors. They learn from unpaired matching between high-resolution 2D slices in the lateral plane and low-resolution 2D slices in the axial plane, achieving impressive performance. However, these methods train models in fixed imaging settings and suffer from degraded performance caused by artifacts and blurring when facing unseen anisotropic factors. This limits their generality in practice <ref type="bibr" target="#b5">[6]</ref>. In conclusion, a more robust paradigm needs to be proposed. Recently, with the success of the diffusion model in the image generation field <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>, researchers applied the diffusion model to various medical image generation tasks and achieved impressive results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Inspired by these works, we attempt to introduce diffusion models to address the isotropic reconstruction problem.</p><p>This paper proposes DiffuseIR, an unsupervised method based on diffusion models, to address the isotropic reconstruction problem. Unlike existing methods, DiffuseIR does not train a specific super-resolution model from low-axialresolution to high-axial-resolution. Instead, we pre-train a diffusion model θ to learn the structural distribution p θ (X lat ) of biological tissue from lateral microscopic images X lat , which resolution is naturally high. Then, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we propose a Sparse Spatial Condition Sampling (SSCS) to condition the reversediffusion process of θ . SSCS extracts sparse structure context from low-axialresolution slice x axi and generate reconstruction result x 0 ∼ p θ (X lat |x axi ). Since θ learns the universal structural distribution p θ , which is independent of the axial resolution, DiffuseIR can leverage the flexibility of SSCS to reconstruct authentic images with unseen anisotropic factors without requiring re-training. To further improve the quality of reconstruction, we propose a Refine-in-loop strategy to enhance the authenticity of image details with fewer sampling steps.</p><p>To sum up, our contributions are as follows:</p><p>(1) We are the first to introduce diffusion models to isotropic reconstruction and propose DiffuseIR. Benefiting from the flexibility of SSCS, DiffuseIR is naturally robust to unseen anisotropic spatial resolutions. <ref type="bibr" target="#b1">(2)</ref> We propose a Refine-in-loop strategy, which maintains performance with fewer sampling steps and better preserves the authenticity of the reconstructed image details. <ref type="bibr" target="#b2">(3)</ref> We perform extensive experiments on EM data with different imaging settings and achieve SOTA performance. Our unsupervised method is competitive with supervised methods and has much stronger robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, DiffuseIR address isotropic reconstruction by progressively conditions the denoising process of a pre-trained diffusion model θ . Our method consists of three parts: DDPM pre-train, Sparse Spatial Condition Sampling and Refine-in-loop strategy.</p><p>DDPM Pretrain on Lateral. Our method differs from existing approaches that directly train super-resolution models. Instead, we pre-train a diffusion model to learn the distribution of high-resolution images at lateral, avoiding being limited to a specific axial resolution. Diffusion models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> employ a Markov Chain diffusion process to transform a clean image x 0 into a series of progressively noisier images during the forward process. This process can be simplified as:</p><formula xml:id="formula_0">q(x t |x 0 ) = N (x t ; √ α t x 0 , (1 -α t )I),<label>(1)</label></formula><p>where α t controls the scale of noises. During inference, the model θ predicts x t-1 from x t . A U-Net θ is trained for denoising process p θ , which gradually reverses the diffusion process. This denoising process can be represented as:</p><formula xml:id="formula_1">p θ (x t-1 |x t ) = N (x t-1 ; θ (x t , t), σ 2 t I),<label>(2)</label></formula><p>During training, we use 2D lateral slices, which is natural high-resolution to optimize θ by mean-matching the noisy image obtained in Eq. 1 using the MSE loss <ref type="bibr" target="#b9">[10]</ref>. Only HR slices at lateral plane X lat were used for training, so the training process is unsupervised and independent of the specific axial resolution. So that θ learns the universal structural distribution of biological tissues and can generate realistic HR images following p θ (X lat ).</p><p>Sparse Spatial Condition Sampling on Axial. We propose Sparse Spatial Condition Sampling (SSCS) to condition the generation process of θ and generate high-axial-resolution reconstruction results. SSCS substitutes every reversediffusion step Eq. 2. We first transform the input axial LR slice x axi to match the lateral resolution by intra-row padding: (α -1) rows of zero pixels are inserted between every two rows of original pixels, where α is the anisotropic spatial factor. We denote M as the mask for original pixels in x con 0 , while (1 -M ) represents those empty pixels inserted. In this way, we obtain x con 0 , which reflects the sparse spatial content at axial, and further apply Eq. 1 to transform noise level:</p><p>Algorithm 1: Isotropic reconstruction using basic DiffuseIR</p><formula xml:id="formula_2">Input: axial slice xaxi, anisotropic factor α, refine-in-loop counts K 1 x con 0 , M ← padding(xaxi, α) 2 for t = T, ..., 1 do 3 x con t-1 ∼ N ( √ αtx con 0 , (1 -αt)I) 4 for i = 1, ..., K do 5 x * t-1 ∼ N (xt-1; θ (xt, t), σ 2 t I) 6 xt-1 = M * x con t-1 + (1 -M ) * x * t-1 7 if t &gt; 1 and i &lt; K then 8 xt ∼ N ( √ 1 -βtxt-1, βtI) 9 end 10 end 11 end 12 return x0 x con t-1 ∼ N ( √ α t x con 0 , (1 -α t )I)<label>(3)</label></formula><p>Then, SSCS sample x t-1 at any time step t, conditioned on x con t-1 . The process can be described as follows:</p><formula xml:id="formula_3">x t-1 = M x con t-1 + (1 -M ) x * t-1 )<label>(4)</label></formula><p>where x * t-1 is obtained by sampling from the model θ using Eq. 2, with x t of the previous iteration. x * t-1 and x con t-1 are combined with M . By iterative denoising, we obtain the reconstruction result x 0 . It conforms to the distribution p θ (X lat ) learned by the pre-trained diffusion model and maintains semantic consistency with the input LR axial slice. Since SSCS is parameter-free and decoupled from the model training process, DiffuseIR can adapt to various anisotropic spatial resolutions by modifying the padding factor according to α while other methods require re-training. This makes DiffuseIR a more practical and versatile solution for isotropic reconstruction.</p><p>Refine-in-Loop Strategy. We can directly use SSCS to generate isotropic results, but the reconstruction quality is average. The diffusion model is capable of extracting context from the sparse spatial condition. Still, we have discovered a phenomenon of texture discoordination at the mask boundaries, which reduces the reconstruction quality. For a certain time step t, the content of x * t-1 may be unrelated to x con t-1 , resulting in disharmony in x t-1 generated by SSCS. During the denoising of the next time step t-1, the model tries to repair the disharmony of x t-1 to conform to p θ distribution. Meanwhile, this process will introduce new inconsistency and cannot converge on its own. To overcome this problem, we propose the Refine-in-loop strategy: For x t-1 generated by SSCS at time step t, we apply noise to it again and obtain a new x t and then repeat SSCS at time step t. Our discovery showed that this uncomplicated iterative refinement method addresses texture discoordination significantly and enhances semantic precision.</p><p>The total number of inference steps in DiffuseIR is given by T total = T • K. As T total increases, it leads to a proportional increase in the computation time of our method. However, larger T total means more computational cost. Recent works such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref> have accelerated the sampling process of diffusion models by reducing T while maintaining quality. For DiffuseIR, adjusting the sampling strategy is straightforward. Lowering T and raising refinement iterations K improves outcomes with a fixed T total . We introduce and follow the approach presented in DDIM <ref type="bibr" target="#b23">[24]</ref> as an example and conducted detailed ablation experiments in Sec. 3 to verify this. Our experiments show that DiffuseIR can benefit from advances in the community and further reduce computational overhead in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussion</head><p>Dataset and Implement Details. To evaluate the effectiveness of our method, we conducted experiments on two widely used public EM datasets, FIB-25 <ref type="bibr" target="#b26">[27]</ref> and Cremi <ref type="bibr" target="#b4">[5]</ref>. FIB-25 contains isotropic drosophila medulla connectome data obtained with FIB-SEM. We partitioned it into subvolumes of 256 × 256 × 256 as ground truth and followed <ref type="bibr" target="#b8">[9]</ref> to perform average-pooling by factor α(x2,x4,x8) along the axis to obtain downsampled anisotropic data. Cremi consists of drosophila brain data with anisotropic spatial resolution. We followed <ref type="bibr" target="#b2">[3]</ref> to generate LR images with a degradation network and conduct experiments on lateral slices. All resulting images were randomly divided into the training (70%), validation (15%) and test (15%) set. For the pre-training of the diffusion model, we follow <ref type="bibr" target="#b18">[19]</ref> by using U-Net with multi-head attention and the same training hyper-parameters. We use 256×256 resolution images with a batch size of 4 and train the model on 8×V100 GPUs. For our sampling setting, we set T, K = 25, 40, which is a choice selected from the ablation experiments in Sec. 3 that balances performance and speed.</p><p>Quantitative and Visual Evaluation. To evaluate the effectiveness of our method, we compared DiffuseIR with SoTA methods and presented the quantitative results in Table <ref type="table">1</ref>. We use PSNR and SSIM to evaluate results. PSNR is calculated using the entire 3D stack. SSIM is evaluated slice by slice in XZ and YZ viewpoints, with scores averaged for the final 3D stack score, measuring quality and 3D consistency. We use cubic interpolation as a basic comparison. 3DSRUNet <ref type="bibr" target="#b8">[9]</ref> is a seminal isotropic reconstruction method, which requires HR and LR pairs as ground truth for supervised training. CycleGAN-IR <ref type="bibr" target="#b2">[3]</ref> proposed an unsupervised approach using a CycleGAN <ref type="bibr" target="#b31">[32]</ref> architecture, learning from unpaired axial and lateral slices. These methods train specialized models for a fixed anisotropic spatial setting and must be retrained for unseen anisotropic factors α, shown in Table <ref type="table">1</ref>. Despite being trained solely for denoising task and not exposed to axial slices, DiffuseIR outperforms unsupervised baselines and is competitive with supervised methods <ref type="bibr" target="#b8">[9]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our refine-in-loop strategy produces results with greater visual similarity to the Ground Truth, avoiding distortion and blurriness of details. Notably, the versatility afforded by Table <ref type="table">1</ref>. Quantitative evaluation of DiffuseIR against baselines. PSNR↑ and SSIM↑ are used as evaluation metrics. We evaluated the FIB25 and Cremi datasets, considering three anisotropic spatial resolutions, α = 2, 4, 8. Unlike other baselines which train a dedicated model for each α, our method only trains a single, generalizable model.   SSCS allows DiffuseIR to achieve excellent results using only one model, even under different isotropic resolution settings. This indicates that DiffuseIR overcomes the issue of generalization to some extent in practical scenarios, as users no longer need to retrain the model after modifying imaging settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methzod</head><p>Further Analysis on Robustness. We examined the robustness of our model to variations in both Z-axis resolutions and domain shifts. Specifically, we investigated the following: (a) Robustness to unseen anisotropic spatial factors. The algorithm may encounter unseen anisotropic resolution due to the need for different imaging settings in practical applications. To assess the model's robustness to unseen anisotropic factors, we evaluated the model trained with the anisotropic factor α = 4. Then we do inference under the scenario of anisotropic factor α = 8. For those methods with a fixed super-resolution factor, we use cubic interpolation to upsample the reconstructed result by 2x along the axis. (b) Robustness to the domain shifts. When encountering unseen data in the real world, domain shifts often exist, such as differences in biological structure features and physical resolution, which can impact the model's performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. To evaluate the model's ability to handle those domain shifts, we trained our model on one dataset and tested it on another dataset. Analysis: As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, DiffuseIR shows greater robustness than other methods. In scenario (a), other methods are trained on specific anisotropic factors for super-resolution of axial LR to lateral HR. This can result in model fragility during testing with unseen anisotropic resolutions. In contrast, DiffuseIR directly learns the universal structural distribution at lateral through generation task, applicable to various axial resolutions. All methods exhibit decreased performance in scenario (b). However, DiffuseIR shows a small performance degradation with the help of the multi-step generation of the diffusion model and sparse spatial constraints imposed by SSCS at each reverse-diffusion step. Further, compared to the pre-vious methods predicting the result by one step, DiffuseIR makes the generating process more robust and controllable by adding constraints at each step to prevent the model from being off-limit. The results show that when the number of total steps is fixed, increase K will lead to higher PSNR.</p><p>Ablation Study. We conducted extensive ablation experiments Fig. <ref type="figure" target="#fig_3">4</ref>. First, to demonstrate the effectiveness of SSCS, we use it only in partially alternate reverse-diffusion steps, such as 1/4 or 1/2 steps. As shown in Fig. <ref type="figure" target="#fig_3">4</ref> (a), increasing the frequency of SSCS significantly improves PSNR while bringing negligible additional computational costs. This indicates that SSCS have a vital effect on the model's performance. Second, for the Refine-in-loop strategy, results show that keeping the total number of steps unchanged (reducing the number of time steps T while increasing the refine iterations K) can markedly improve performance. Figure <ref type="figure" target="#fig_3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduce DiffuseIR, an unsupervised method for isotropic reconstruction based on diffusion models. To the best of our knowledge, We are the first to introduce diffusion models to solve this problem. Our approach employs Sparse Spatial Condition Sampling (SSCS) and a Refine-in-loop strategy to generate results robustly and efficiently that can handle unseen anisotropic resolutions. We evaluate DiffuseIR on EM data. Experiments results show our methods achieve SoTA methods and yield comparable performance to supervised methods. Additionally, our approach offers a novel perspective for addressing Isotropic Reconstruction problems and has impressive robustness and generalization abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitation</head><p>DiffuseIR leverages the powerful generative capabilities of pre-trained Diffusion Models to perform high-quality isotropic reconstruction. However, this inevitably results in higher computational costs. Fortunately, isotropic reconstruction is typically used in offline scenarios, making DiffuseIR's high computational time tolerable. Additionally, the community is continuously advancing research on accelerating Diffusion Model sampling, from which DiffuseIR can benefit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Method Pipeline. DiffuseIR progressively conditions the denoising process with SSCS. For SSCS, we perform intra-row padding on input X lat using the anisotropy factor α to obtain spatially aligned structural context, which is then merged with the diffusion model's output. Iterative SSCS refines reconstruction.</figDesc><graphic coords="3,58,98,53,78,334,48,95,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparisons on FIB-25 dataset (α = 4). DiffuseIR can generate competitive results compared to supervised methods, and the results appear more visually realistic.</figDesc><graphic coords="6,44,79,481,28,334,48,80,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Analysis on robustness. (a) Test on unseen anisotropic factor α. (b) Test on different datasets with domain shifts (e.g., train on FIB25, test on Cremi). Our method is robust against various anisotropic factors and domain shifts between two datasets.</figDesc><graphic coords="7,59,04,54,65,334,12,131,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ablation Study: (a)ablation on SSCS frequency.Experimental results demonstrates the importance of SSCS. When reducing the frequency of SSCS usage, performance will severely decline. (b)ablation on different refine-in-loop settings.The results show that when the number of total steps is fixed, increase K will lead to higher PSNR.</figDesc><graphic coords="8,44,31,107,39,335,80,107,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) have the following settings: T = {25, 100, 250, 1000} with K{40, 10, 4, 1} to achieve a total of 5000 steps. The results show that the model performs best when T = 25 and PSNR gradually increases with the increase of K. A balanced choice is {T = 25, K = 40}, which improves PSNR by 1.56dB compared to {T = 1000, K = 1} without using the Refine-in-loop strategy.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The authors acknowledge supports from <rs type="programName">Beijing Nova Program and Beijing Transcend Vivoscope Biotech Co</rs>., Ltd.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_amWDeGT">
					<orgName type="program" subtype="full">Beijing Nova Program and Beijing Transcend Vivoscope Biotech Co</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_31.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Score-based diffusion models for accelerated MRI</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chul Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102479</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<title level="m">Domain adaptation for visual applications: a comprehensive survey. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Isotropic reconstruction of 3D EM images with unsupervised degradation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<ptr target="http://cremi.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical flow driven interpolation for isotropic FIB-SEM reconstructions</title>
		<author>
			<persName><forename type="first">V</forename><surname>González-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>García-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page">106856</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ultrastructurally smooth thick partitioning and volume stitching for large-scale connectomics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Hayworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="319" to="322" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for isotropic super-resolution from non-isotropic 3d electron microscopy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bogovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medical image computing and computer assisted intervention</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Denoising diffusion restoration models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diffusion deformable model for 4d temporal medical image generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chul</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_51" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient meta-tuning for content-aware neural video delivery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19797-0_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19797-0_18" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13678</biblScope>
			<biblScope unit="page" from="308" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overfitting the data: compact neural video delivery via content-aware feature modulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv48922.2021.00459</idno>
		<ptr target="https://doi.org/10.1109/iccv48922.2021.00459" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">DPM-solver: a fast ode solver for diffusion probabilistic model sampling in around 10 steps</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dpm-solver++: fast solver for guided sampling of diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<title level="m">Repaint: inpainting using denoising diffusion probabilistic models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progress towards mammalian whole-brain cellular connectomics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mikula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroanat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised medical image translation with adversarial diffusion models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Özbey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3297</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<title level="m">Towards performant and reliable undersampled MR reconstruction via diffusion model sampling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brain-wide 3d imaging of neuronal activity in caenorhabditis elegans with sculpted light</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schrödel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aumayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaziri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Meth</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1013" to="1020" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Solving inverse problems in medical imaging with score-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Cornell University -arXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dual diffusion implicit bridges for imageto-image translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Synaptic circuits and their variations within different columns in the visual system of drosophila</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ya Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-resolution three-dimensional imaging of large specimens with light sheetbased microscopy</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Verveer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swoger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pampaloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H K</forename><surname>Stelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="311" to="313" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Isotropic reconstruction of 3D fluorescence microscopy images using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Myers</surname></persName>
		</author>
		<idno>arXiv</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Content-aware image restoration: pushing the limits of fluorescence microscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Three-dimensional virtual refocusing of fluorescence microscopy images using deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1323" to="1331" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
