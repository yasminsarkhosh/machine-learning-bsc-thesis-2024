<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images</title>
				<funder ref="#_9gfNKKZ #_XuYpUr5">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiyun</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenrong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zixu</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengjun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<email>lichizhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="66" to="76"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6C0C2A09071FCDDCF3744B0F494B9AC7</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-modality synthesis</term>
					<term>Super-resolution</term>
					<term>Magnetic resonance imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modality synthesis (CMS) and super-resolution (SR) have both been extensively studied with learning-based methods, which aim to synthesize desired modality images and reduce slice thickness for magnetic resonance imaging (MRI), respectively. It is also desirable to build a network for simultaneous cross-modality and super-resolution (CMSR) so as to further bridge the gap between clinical scenarios and research studies. However, these works are limited to specific fields. None of them can flexibly adapt to various combinations of resolution and modality, and perform CMS, SR, and CMSR with a single network. Moreover, alias frequencies are often treated carelessly in these works, leading to inferior detail-restoration ability. In this paper, we propose Alias-Free Co-Modulated network (AFCM ) to accomplish all the tasks with a single network design. To this end, we propose to perform CMS and SR consistently with co-modulation, which also provides the flexibility to reduce slice thickness to various, non-integer values for SR. Furthermore, the network is redesigned to be alias-free under the Shannon-Nyquist signal processing framework, ensuring efficient suppression of alias frequencies. Experiments on three datasets demonstrate that AFCM outperforms the alternatives in CMS, SR, and CMSR of MR images. Our codes are available at https://github.com/zhiyuns/AFCM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic Resonance Imaging (MRI) is widely recognized as a pivotally important neuroimaging technique, which provides rich information about brain tissue anatomy in a non-invasive manner. Several studies have shown that multi-modal MR images offer complementary information on tissue morphology, which help conduct more comprehensive brain region analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>. For instance, T1weighted images distinguish grey and white matter tissues, while FLAIR images differentiate edematous regions from cerebrospinal fluid <ref type="bibr" target="#b18">[19]</ref>. Furthermore, 3D MR imaging with isotropic voxels contains more details than anisotropic ones and facilitates the analyzing procedure <ref type="bibr" target="#b21">[22]</ref>, especially for automated algorithms whose performance degrades severely when dealing with anisotropic images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. However, the scanning protocols in the clinic are different from that in research studies. Due to the high costs of radiation examinations, physicians and radiologists prefer to scan a specific MR contrast, anisotropic MRI with 2D scanning protocols, or their combinations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. Such simplified protocols may hinder the potential possibility of utilizing them for further clinical studies. Therefore, there has been ever-growing interest in reconstructing images with the desired modality and resolution from the acquired images, which may have different modalities or resolutions.</p><p>Relevant literature in this area can be divided into cross-modality synthesis (CMS), super-resolution (SR), or cross-modality super-resolution (CMSR). Most of the works concerning CMS model modality-invariant and coherent high-frequency details to translate between modalities. For example, pGAN <ref type="bibr" target="#b4">[5]</ref> extended conditional GAN with perceptual loss and leveraged neighboring cross-sections to translate between T1-and T2-weighted images. AutoGAN <ref type="bibr" target="#b6">[7]</ref> explored NAS to automatically search for optimal generator architectures for MR image synthesis. ResViT <ref type="bibr" target="#b3">[4]</ref> proposed a transformer-based generator to capture long-range spatial dependencies for multi-contrast synthesis. On the other hand, SR methods are usually designed to build 3D models for reconstruction. DeepResolve <ref type="bibr" target="#b1">[2]</ref> employed a ResNet-based 3D CNN to reduce thickness for knee MR images. DCSRN <ref type="bibr" target="#b2">[3]</ref> designed a DenseNet-based 3D model to restore HR features of brain MR images. In more special clinical scenarios where both the CMS and SR need to be performed, current works prefer to build a single model that simultaneously performs the task, rather than performing them sequentially. For instance, SynthSR <ref type="bibr" target="#b9">[10]</ref> used a modified 3D-UNet to reconstruct MP-RAGE volumes with thickness of 1 mm from other MR images scanned with different protocols. WEENIE <ref type="bibr" target="#b8">[9]</ref> proposed a weakly-supervised joint convolutional sparse coding method to acquire high-resolution modality images with low-resolution ones with other modalities.</p><p>However, these works have two main limitations. On the one hand, these works are limited to specified fields, and cannot be flexibly switched to other tasks or situations. For instance, CMS models described above will fail to produce consistent 3D results due to the lack of modeling inter-plane correlations. Moreover, CMSR models only produce MR images with fixed thickness, which may constrain their further applications. On the other hand, high-frequency details with alias frequencies are often treated carelessly in these generative networks. This may lead to unnatural details and even aliasing artifacts, especially for the ill-posed inverse tasks (SR or CMSR) that require restoring high-frequency details based on low-resolution images.</p><p>In this paper, we propose an Alias-Free Co-Modulated network (AFCM) to address the aforementioned issues. AFCM is inspired by the recent advances in foundation models <ref type="bibr" target="#b23">[24]</ref>, which achieve superior results in various tasks and even outperform the models for specific tasks. To flexibly accomplish 2D-CMS and 3D-SR with a single model, we propose to extend style-based modulation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> with image-conditioned and position-embedded representations, termed as comodulation, to ensure consistency between these tasks. The design also enables more flexible SR by reconstructing slices in arbitrary positions with continuous positional embeddings. Moreover, the discrete operators in the generator are carefully redesigned to be alias-free under the Shannon-Nyquist signal processing framework. Our main contributions are as follows: 1) We propose a co-modulated network that can accomplish CMS, SR, and CMSR with a single model. 2) We propose the continuous positional embedding strategy in AFCM, which can synthesize high-resolution MR images with non-integer thickness. 3) With the redesigned alias-free generator, AFCM is capable of restoring high-frequency details more naturally for the reconstructed images. 4) AFCM achieves state-ofthe-art results for CMS, SR, and CMSR of MR images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Co-modulated Network</head><p>Here we propose a novel style-based co-modulated network to accomplish CMS, SR, and CMSR consistently. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the architecture design of our network is based on the style-based unconditional generative architecture <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Different from original stochastic style representation, we design both positionembedded representation and image-conditioned representation as the modulations, which control the generation process consistently. Remind that we aim to translate low-resolution MR images x i with modality i and thickness p into high-resolution MR images y j with modality j and thickness q. The positionembedded representation w is transformed from the latent code l ∼ N (0, I) and the relative slice index δ = kq/p -kq/p that controls the position k for each output slice y k j . This is accomplished with a mapping network M where the relative slice index is embedded using the class-conditional design <ref type="bibr" target="#b12">[13]</ref>. The imageconditioned representation E(x in i ) is obtained by encoding 2m adjacent slices (i.e.,</p><formula xml:id="formula_0">x in i = [x n-(m-1) i , ..., x n i , ..., x n+m i ]</formula><p>where n = kq/p ) with the encoder E. In this paper, m is experientially set to 2 following <ref type="bibr" target="#b11">[12]</ref>. The two representations are then concatenated and produce a style vector s with the affine transform A:</p><formula xml:id="formula_1">s = A(E(x in i ), M(l, δ)). (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>The style vector s is then used to modulate the weights of the convolution kernels in the synthesis network D <ref type="bibr" target="#b15">[16]</ref>. Moreover, skip connections are implemented between E and D to preserve the structure of conditional images. The target volume y j is created by stacking generated slices together. Note that our network is flexible to perform various tasks. It performs 2D-CMS or 3D-CMS alone when δ is fixed to 0, which is denoted as AFCM multi . In this case, if the information of adjacent cross-sections is unknown, we perform the one-to-one translation with AFCM one where only one slice is given as input (i.e., x in i = x k i ). Moreover, AFCM sr performs SR alone when i = j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alias-Free Generator</head><p>We notice that results produced by the vanilla generator are contaminated by "texture sticking", where several fine-grained details are fixed in pixel coordinates when δ changes, as illustrated in our ablation study. The phenomenon was found to originate from aliasing caused by carelessly designed operators (i.e., convolution, resampling, and nonlinearity) in the network <ref type="bibr" target="#b13">[14]</ref>. To solve the problem, we consider the feature map Z in the generator as a regularly sampled signal, which can represent the continuous signal z with limited frequencies under the Shannon-Nyquist signal processing framework <ref type="bibr" target="#b25">[26]</ref>. Therefore, the discrete operation F on Z has its continuous counterpart f (z):</p><formula xml:id="formula_3">F(Z) = X r f (φ r * Z) , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where X r is the two-dimensional Dirac comb with sampling rate r , and φ r is the interpolation filter with a bandlimit of r/2 so that z can be represented as φ r * Z. The operator denotes pointwise multiplication and * denotes continuous convolution. The operation F is alias-free when any frequencies higher than r /2 in the output signal, also known as alias frequencies, are efficiently suppressed. In this way, we re-design the generator by incorporating the aliasfree mechanism based on the analysis above, which consists of the antialiasing decoder and encoder which are further illustrated as follows.</p><p>Antialiasing Decoder. Considering that any details with alias frequencies need to be suppressed, the resampling and nonlinearity operators in the decoder are carefully redesigned following Alias-Free GAN <ref type="bibr" target="#b13">[14]</ref>. The convolution preserves the original form as it introduces no new frequencies. In low-resolution layers, the resampling filter is designed as non-critical sinc one whose cutoff frequency varies with the resolution of feature maps. Moreover, nonlinear operation (i.e., LeakyRelu here) is wrapped between upsampling and downsampling to ensure that any high-frequency content introduced by the operation is eliminated. Other minor modifications including the removal of noise, simplification of the network, and flexible layers are also implemented. Please refer to <ref type="bibr" target="#b13">[14]</ref> for more details. Note that Fourier features are replaced by feature maps obtained by the encoder E.</p><p>Antialiasing Encoder. Given that our network has a U-shaped architecture, the encoder also needs to be alias-free so that the skip connections would not introduce extra content with undesired frequency. The encoder consists of 14 layers, each of which is further composed of a convolution, a nonlinear operation, and a downsampling filter. The parameters of the filter are designed to vary with the resolution of the corresponding layer. Specifically, the cutoff frequency geometrically decreases from f c = r N /2 in the first non-critically sampled layer to f c = 2 in the last layer, where r N is the image resolution. The minimum acceptable stopband frequency starts at f t = 2 0.3 • r N /2 and geometrically decreases to f t = 2 2.1 , whose value determines the resolution r = min(ceil(2 • f t ), r N ) and the transition band half-width f h = max(r/2, f t ) -f c . The target feature map of each layer is surrounded by a 10-pixel margin, and the final feature map is resampled to 4 × 4 before formulating the image-conditioned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>The overall loss is composed of an adversarial loss and a pixel-wise L 1 loss:</p><formula xml:id="formula_5">L G = L adv + λL pix , (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where λ is used to balance the losses. The adversarial loss is defined as nonsaturation loss with R 1 regularization, and the discriminator preserves the architecture of that in StyleGAN2 <ref type="bibr" target="#b15">[16]</ref>. We combine x in i with real/fake y k j as the input for the discriminator, and embed δ with the projection discriminator strategy <ref type="bibr" target="#b20">[21]</ref>. Following the stabilization trick <ref type="bibr" target="#b13">[14]</ref>, we blur target and reconstructed images using a Gaussian filter with σ = 2 pixels in the early training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets. We evaluate the performance of AFCM on three brain MRI datasets: CSDH is an in-house dataset comprising 100 patients diagnosed with chronic subdural hematoma (CSDH). It includes T1-weighted images with spatial resolution of 0.75×0.75×1 mm 3 and T2-weighted flair images with spatial resolution of 0.75 × 0.75 × 8 mm 3 . Pixel-wise annotations of liquefied blood clots made by an experienced radiologist are used for segmentation accuracy evaluation.</p><p>ADNI<ref type="foot" target="#foot_1">2</ref> is composed of 50 patients diagnosed with Alzheimer's disease and 50 elderly controls. Each subject has one near-isotropic T1 MP-RAGE scan with thickness of 1 mm and one axial FLAIR scan with thickness of 5 mm. ADNI and CSDH are both further divided into 70, 10, and 20 subjects for training, validating, and testing. For all three datasets, MR images from the same subjects are co-registered using a rigid registration model with ANTs <ref type="bibr" target="#b0">[1]</ref>.</p><p>Implementation Details. We use the translation equivariant configuration of the Alias-Free GAN, and discard the rotational equivariance to avoid generating overly-symmetric images <ref type="bibr" target="#b24">[25]</ref>. AFCM is trained with batch size 16 on an NVIDIA GeForce RTX 3090 GPU with 24G memory for 100 epochs, which takes about 36 GPU hours for each experiment. The learning rates are initialized as 0.0025/0.0020 for the generator and the discriminator respectively in the first 50 epochs and linearly decrease to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparative Experiments</head><p>Cross-Modality Synthesis. In this section, we report three subtasks, namely CMS from T1 to T2, T2 to PD, and PD to T1-weighted MR images on the IXI dataset in Table <ref type="table" target="#tab_0">1</ref>. AFCM one achieves superior results for all tasks when performing one-to-one translation, where the corresponding slice for the target is taken as the input. The improvement in the generation quality can be attributed to the alias-free design of AFCM, which can restore high-frequency details more accurately, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Moreover, AFCM multi demonstrates higher generation quality and a significant improvement over other methods (p &lt; 0.05) when adjacent slices are used as extra guidance. The inverted translation results provided in the supplementary file also indicate the superiority of our method. Super-Resolution. The SR experiment is performed using the CSDH dataset with downsampling factor (DSF) of 8 and the ADNI dataset with DSF of 5. Following previous work <ref type="bibr" target="#b1">[2]</ref>, the low-resolution training data are simulated by filtering and downsampling thin-slice images. We compare with SR methods for MR images (Deepresolve <ref type="bibr" target="#b1">[2]</ref> and SynthSR <ref type="bibr" target="#b9">[10]</ref>), as well as the 3D version of an SR method for natural images (EDSR <ref type="bibr" target="#b17">[18]</ref>). It may seem counterintuitive to generate 2D images slice-by-slice instead of reconstructing the entire volume directly. However, results in Fig. <ref type="figure" target="#fig_3">3</ref>(right) suggest that AFCM sr yields higher quality than other 3D-based SR models due to the continuous position-embedded representation and the alias-free design for detail-restoration. Although the improvement is not significant compared with the SOTA methods, the qualitative results in the supplementary file demonstrate that AFCM sr produces coherent details compared to other methods whose results are blurred due to imperfect pre-resampling, especially when the DSF is high. We also evaluate whether the synthesized images can be used for downstream tasks. As observed from Fig. <ref type="figure" target="#fig_3">3</ref>(right), when using a pre-trained segmentation model to segment the liquefied blood clots in the reconstructed images, AFCM s r can produce most reliable results, which also indicates the superiority of our method for clinical applications.</p><p>Cross-Modality and Super-Resolution. As previously addressed, it is more challenging to implement CMS and SR simultaneously. One possible approach is to perform CMS and SR in a two-stage manner, which is accomplished with a combination of ResViT and DeepResolve. We also compare our approach with SynthSR <ref type="bibr" target="#b9">[10]</ref> (fully supervised version for fair comparison) that directly performs the task. As reported in Fig. <ref type="figure" target="#fig_3">3</ref>(right), the performance of the baseline method is improved when either DeepResolve is replaced with AFCM sr for SR (ResViT+AFCM sr ) or ResViT is replaced with AFCM multi for CMS (AFCM multi +DeepResolve). Additionally, although the two-stage performance is slightly lower than SynthSR, AFCM achieves the best results among all combinations (p &lt; 0.05, SSIM). We also qualitatively evaluate whether our network can perform CMSR with flexible target thickness. To this end, we directly use the model trained on CSDH to generate isotropic images with thickness of 0.75mm, which means 10.67× CMSR. As depicted in Fig. <ref type="figure" target="#fig_3">3</ref>   Ablation Study. In this section, we evaluate the impact of our alias-free design by performing CMSR on the CSDH dataset. Note that we replace the encoder and decoder in AFCM with vanilla ones <ref type="bibr" target="#b26">[27]</ref> as the baseline for comparison. Table in Fig. <ref type="figure" target="#fig_4">4</ref> indicates that although redesigning the decoder leads to the improvement of SSIM, it is when we also redesign the encoder that the dropped metrics (PSNR and Dice) recover, which highlights the importance of redesigning both the encoder and decoder to achieve optimal results. The qualitative results in Fig. <ref type="figure" target="#fig_4">4</ref> also demonstrate that our design successfully suppresses "texture sticking".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel alias-free co-modulated network for CMS, SR, and CMSR of MR images. Our method addresses the problems of task inconsistency between CMS and SR with a novel co-modulated design, and suppresses aliasing artifacts by a redesigned alias-free generator. AFCM is also flexible enough to reconstruct MR images with various non-integer target thickness. The experiments on three independent datasets demonstrate our state-of-the-art performance in CMS, SR, and CMSR of MR images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall architecture of AFCM. The encoder E embeds multiple slices into imageconditioned representation, while the mapping network M maps the latent code l and embedded δ into position-embedded representation w. The concatenated representations are transformed with affine transformation A and modulate the weights of the convolution kernels of the decoder D. The target MR image yj is generated by walking through all target positions and concatenating the generated slices. The resampling filter and nonlinearity in each layer are redesigned to suppress the corresponding alias frequencies and ensure alias-free generation.</figDesc><graphic coords="3,41,79,273,05,340,26,131,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results for cross-modality synthesis on the IXI dataset.</figDesc><graphic coords="7,41,79,148,73,340,24,140,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, when δ is set to values not encountered during training (fixed at [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875]), AFCM still generates reasonable results. Multiple views of the reconstructed MR image with reduced thickness of 0.75 mm also demonstrate the effectiveness of reducing the thickness to flexible values, which are even blind for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left: Qualitative results for arbitrary-scale cross-modality super-resolution on the CSDH dataset (input: anisotropic Flair, output: isotropic T1) with flexible target thickness (training: 1 mm, testing: 0.75 mm). Right: Quantitative results for superresolution and cross-modality super-resolution.</figDesc><graphic coords="8,55,98,304,04,170,15,199,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative (left) and quantitative (right) results for ablation study.</figDesc><graphic coords="9,41,79,54,26,204,16,65,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results for cross-modality synthesis on the IXI dataset. Except for AFCM multi , all experiments are performed under one-to-one translation protocol where only one slice is taken as the input.IXI 1 is a collection of multi-modality MRI images with spatial resolution of 0.94 × 0.94 × 1.2 mm 3 obtained from three hospitals. It includes images from 309 randomly selected healthy subjects, which are further divided into three subsets: training (285 subjects), validation (12 subjects), and testing (12 subjects).</figDesc><table><row><cell></cell><cell>T1-T2</cell><cell></cell><cell>T2-PD</cell><cell></cell><cell>PD-T1</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell cols="3">pix2pix [11] 23.35 ± 0.71 0.8754 ± 0.0126</cell><cell cols="2">30.09 ± 0.75 0.9558 ± 0.0063</cell><cell cols="2">25.92 ± 0.81 0.9258 ± 0.0094</cell></row><row><cell>pGAN [5]</cell><cell cols="2">26.09 ± 1.13 0.9205 ± 0.0130</cell><cell cols="2">32.87 ± 0.46 0.9737 ± 0.0023</cell><cell cols="2">27.38 ± 0.91 0.9424 ± 0.0089</cell></row><row><cell cols="3">AutoGAN [7] 26.33 ± 0.92 0.9193 ± 0.0174</cell><cell cols="2">33.01 ± 0.79 0.9669 ± 0.0115</cell><cell cols="2">27.49 ± 0.89 0.9384 ± 0.0088</cell></row><row><cell>ResViT [4]</cell><cell cols="2">26.17 ± 1.10 0.9209 ± 0.0131</cell><cell cols="2">33.15 ± 0.55 0.9753 ± 0.0024</cell><cell cols="2">27.73 ± 1.00 0.9443 ± 0.0091</cell></row><row><cell>AFCMone</cell><cell cols="2">26.38 ± 1.08 0.9297 ± 0.0119</cell><cell cols="4">33.56 ± 0.51 0.9779 ± 0.0020 27.88 ± 0.94 0.9453 ± 0.0088</cell></row><row><cell>AFCMmulti</cell><cell cols="6">26.92 ± 1.06 0.9355 ± 0.0196 33.58 ± 0.49 0.9779 ± 0.0019 28.76 ± 1.05 0.9525 ± 0.0074</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://brain-development.org/ixi-dataset/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://adni.loni.usc.edu/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62001292</rs> and No. <rs type="grantNumber">82227807</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9gfNKKZ">
					<idno type="grant-number">62001292</idno>
				</org>
				<org type="funding" xml:id="_XuYpUr5">
					<idno type="grant-number">82227807</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reproducible evaluation of ants similarity metric performance in brain image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2033" to="2044" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution musculoskeletal MRI using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2139" to="2154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Brain MRI super resolution using 3D deep densely connected neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ResViT: residual vision transformers for multimodal medical image synthesis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dalmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Çukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2598" to="2614" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image synthesis in multi-contrast MRI with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2375" to="2388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate and robust brain image alignment using boundary-based registration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Greve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AutoGAN-synthesizer: neural architecture search for cross-modality MRI synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_38" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="397" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast multi-contrast MRI reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Axel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1344" to="1352" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and crossmodality synthesis of 3D medical images using weakly-supervised joint convolutional sparse coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">118206</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FLAVR: flow-agnostic video representations for fast frame interpolation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2071" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Artifacts in magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekiesińska-Figatowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pol. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR) Workshops</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A review on brain tumor diagnosis from MRI images: practical implications, key achievements, and lessons learned</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Abd-Ellah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Khalaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Hamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="300" to="318" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-contrast, isotropic, single-slab 3D MR imaging in multiple sclerosis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moraal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroradiol. J</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>_suppl</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Bayesian model of shape and appearance for subcortical brain segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Patenaude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="907" to="922" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">StylegAN-XL: scaling StyleGAN to large diverse datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Communities in the presence of noise</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Instit. Radio Eng</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale image completion via co-modulated generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
