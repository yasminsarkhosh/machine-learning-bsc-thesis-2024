<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion</title>
				<funder ref="#_9kmsYPp">
					<orgName type="full">Hong Kong RGC CRF</orgName>
				</funder>
				<funder ref="#_P347dU9 #_hrkWKWV #_DK5N9QD #_pJMjxkX #_dHwM3j5 #_XGgqaMQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Long</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanan</forename><surname>Wu</surname></persName>
							<email>yananwu@cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mobarakol</forename><surname>Islam</surname></persName>
							<email>mobarakol.islam@ucl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<email>hlren@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Shun Hing Institute of Advanced Engineering</orgName>
								<orgName type="institution" key="instit2">CUHK</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="34" to="44"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6F034A9C373D05934C3467E4C2DBD185</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wireless capsule endoscopy (WCE) is a painless and noninvasive diagnostic tool for gastrointestinal (GI) diseases. However, due to GI anatomical constraints and hardware manufacturing limitations, WCE vision signals may suffer from insufficient illumination, leading to a complicated screening and examination procedure. Deep learningbased low-light image enhancement (LLIE) in the medical field gradually attracts researchers. Given the exuberant development of the denoising diffusion probabilistic model (DDPM) in computer vision, we introduce a WCE LLIE framework based on the multi-scale convolutional neural network (CNN) and reverse diffusion process. The multi-scale design allows models to preserve high-resolution representation and context information from low-resolution, while the curved wavelet attention (CWA) block is proposed for high-frequency and local feature learning. Moreover, we combine the reverse diffusion procedure to optimize the shallow output further and generate images highly approximate to real ones. The proposed method is compared with eleven state-of-the-art (SOTA) LLIE methods and significantly outperforms quantitatively and qualitatively. The superior performance on GI disease segmentation further demon-L. Bai and T. Chen-are co-first authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>strates the clinical potential of our proposed model. Our code is publicly accessible at github.com/longbai1006/LLCaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Currently, the golden standard of gastrointestinal (GI) examination is endoscope screening, which can provide direct vision signals for diagnosis and analysis. Benefiting from its characteristics of being non-invasive, painless, and low physical burden, wireless capsule endoscopy (WCE) has the potential to overcome the shortcomings of conventional endoscopy <ref type="bibr" target="#b21">[21]</ref>. However, due to the anatomical complexity, insufficient illumination, and limited performance of the camera, low-quality images may hinder the diagnosis process. Blood vessels and lesions with minor color changes in the early stages can be hard to be screened out <ref type="bibr" target="#b15">[15]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows WCE images with low illumination and contrast. The disease features clearly visible in the normal image become challenging to be found in the low-light images. Therefore, it is necessary to develop a low-light image enhancement framework for WCE to assist clinical diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal Images Low Light Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angiectasia Erosion</head><p>Foreign body Angiectasia Erosion Foreign body Many traditional algorithms (e.g., intensity transformation <ref type="bibr" target="#b7">[7]</ref>, histogram equalization <ref type="bibr" target="#b14">[14]</ref>, and Retinex theory <ref type="bibr" target="#b13">[13]</ref>) have been proposed for low-light image enhancement (LLIE). For WCE, Long et al. <ref type="bibr" target="#b15">[15]</ref> discussed adaptive fraction-power transformation for image enhancement. However, traditional methods usually require an ideal assumption or an effective prior, limiting their wider applications. Deep learning (DL) provides novel avenues to solve LLIE problems <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b16">16]</ref>. Some DL-based LLIE schemes for medical endoscopy have been proposed <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18]</ref>. Gomez et al. <ref type="bibr" target="#b5">[5]</ref> offered a solution for laryngoscope lowlight enhancement, and Ma et al. <ref type="bibr" target="#b18">[18]</ref> proposed a medical image enhancement model with unpaired training data.</p><p>Recently, denoising diffusion probabilistic model (DDPM) <ref type="bibr" target="#b8">[8]</ref> is the most popular topic in image generation, and has achieved success in various applications. Due to its unique regression process, DDPM has a stable training process and excellent output results, but also suffers from its expensive sampling procedure and lack of low-dimensional representation <ref type="bibr" target="#b19">[19]</ref>. It has been proved that DDPM can be combined with other existing DL techniques to speed up the sampling process <ref type="bibr" target="#b19">[19]</ref>. In our work, we introduce the reverse diffusion process of DDPM into our end-to-end LLIE process, which can preserve image details without introducing excessive computational costs. Our contributions to this work can be summarized as three-fold:</p><p>-We design a Low-Light image enhancement framework for Capsule endoscopy (LLCaps </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Multi-scale Residual Block. Multi-scale Residual Block (MSRB) <ref type="bibr" target="#b12">[12]</ref> constructs a multi-scale neuronal receptive field, which allows the network to learn multi-scale spatial information in the same layer. Therefore, the network can acquire contextual information from the low-resolution features while preserving high-resolution representations. We establish our CNN branch with six stacked multi-scale residual blocks (MSRB), and every two MSRBs are followed by a 2D convolutional layer (Conv2D). Besides, each MSRB shall require feature learning and the multi-scale feature aggregation module. Specifically, we propose our curved wavelet attention (CWA) module to conduct multi-scale feature learning, and employ the selective kernel feature fusion (SKFF) <ref type="bibr" target="#b29">[29]</ref> to combine multi-scale features, as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>.</p><p>Denoising Diffusion Probabilistic Models. Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b8">[8]</ref> can be summarised as a model consisting of a forward noise addition q(i 1:T |i 0 ) and a reverse denoising process p(i 0:T ), which are both parameterized Markov chains. The forward diffusion process gradually adds noise to the input image until the original input is destroyed. Correspondingly, the reverse process uses the neural network to model the Gaussian distribution and achieves image generation through gradual sampling and denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Methodology</head><p>Curved Wavelet Attention. Curved Wavelet Attention (CWA) block is the core component of our CNN branch, which is constructed via a curved dual attention mechanism and wavelet transform, as shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>). Firstly, the input feature map F in is divided into identity feature F identity and processing feature F p . Medical LLIE shall require high image details. In this case, we transform the F p into wavelet domain F w to extract high-frequency detail information based on discrete wavelet transform. F w is then propagated through the feature selector and dual attention module for deep representation learning. Finally, we conduct reverse wavelet transform (IWT) to get F p , and concatenate it with F identity before the final output convolution layer. We construct our curved dual attention module with parallel spatial and curved attention blocks. The spatial attention (SA) layer exploits the interspatial dependencies of convolutional features <ref type="bibr" target="#b29">[29]</ref>. The SA layer performs the global average pooling and max pooling on input features respectively, and concatenates the output F w(mean) and F w(max) to get F cat . Then the feature map will be dimensionally reduced and passed through the activation function.</p><p>However, literature <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b33">33]</ref> has discussed the problem of local illumination in LLIE. If we simply use a global computing method such as the SA layer, the model may not be able to effectively understand the local illumination/lack of illumination. Therefore, in order to compensate for the SA layer, we design the Curved Attention (CurveA) layer, which is used to model the high-order curve of the input features. Let IL n(c) denote the curve function, c denote the feature location coordinates, and Curve (n-1) denote the pixel-wise curve parameter, we can obtain the curve estimation equation as:</p><formula xml:id="formula_0">IL n(c) IL n-1(c) = Curve n-1 (1 -IL n-1(c) )<label>(1)</label></formula><p>The detailed CurveA layer is presented in the top of Fig. <ref type="figure" target="#fig_1">2</ref>(b), and the Eq. ( <ref type="formula" target="#formula_0">1</ref>) is related to the white area. The Curve Parameter Estimation module consists of a Sigmoid activation and several Conv2D layers, and shall estimate the pixelwise curve parameter at each order. The Feature Rescaling module will rescale the input feature into [0, 1] to learn the concave down curves. By applying the CurveA layer to the channels of the feature map, the CWA block can better estimate local areas with different illumination.</p><p>Reverse Diffusion Process. Some works <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b26">26]</ref> have discussed combining diffusion models with other DL-based methods to reduce training costs and be used for downstream applications. In our work, we combine the reverse diffusion process of DDPM in a simple and ingenious way, and use it to optimize the shallow output by the CNN branch. Various experiments shall prove the effectiveness of our design in improving image quality and assisting clinical applications.</p><p>In our formulation, we assume that i 0 is the learning target Y * and i T is the output shallow image from the CNN branch. Therefore, we only need to engage the reverse process in our LLIE task. The reverse process is modeled using a Markov chain:</p><formula xml:id="formula_1">p θ (i 0:T ) = p (i T ) T t=1 p θ (i t-1 | i t ) p θ (i t-1 | i t ) = N (i t-1 ; µ θ (i t , t) , Σ θ (i t , t)) (2)</formula><p>p θ (i t-1 | i t ) are parameterized Gaussian distributions whose mean µ θ (i t , t) and variance Σ θ (i t , t) are given by the trained network. Meanwhile, we simplify the network and directly include the reverse diffusion process in the end-toend training of the entire network. Shallow output is therefore optimized by the reverse diffusion branch to get the predicted image Y . We further simplify the optimization function and only employ a pixel-level loss on the final output image, which also improves the training and convergence efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Network</head><p>Architecture. An overview of our framework can be found in Fig. <ref type="figure" target="#fig_1">2</ref>. Our LLCaps contains a CNN branch (including a shallow feature extractor (SFE), multi-scale residual blocks (MSRBs), an output module (OPM)), and the reverse diffusion process. The SFE is a Conv2D layer that maps the input image into the high-dimensional feature representation F SF E ∈ R C×W ×H <ref type="bibr" target="#b27">[27]</ref>. Stacked MSRBs shall conduct deep feature extraction and learning. OPM is a Conv2D layer that recovers the feature space into image pixels. A residual connection is employed here to optimize the end-to-end training and converge process. Hence, given a low-light image x ∈ R 3×W ×H , where W and H represent the width and height, the CNN branch can be formulated as:</p><formula xml:id="formula_2">F SF E = H SF E (x) F MSRBs = H MSRBs (F SF E ), F OP M = H OP M (F MB ) + x (3)</formula><p>The shallow output F OP M ∈ R 3×W ×H shall further be propagated through the reverse diffusion process and achieve the final enhanced image Y ∈ R 3×W ×H . The whole network is constructed in an end-to-end mode and optimized by Charbonnier loss <ref type="bibr" target="#b1">[1]</ref>. The ε is set to 10 -3 empirically.</p><formula xml:id="formula_3">L (x, x * ) = Y -Y * 2 + ε 2<label>(4)</label></formula><p>in which Y and Y * denote the input and ground truth images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We conduct our experiments on two publicly accessible WCE datasets, the Kvasir-Capsule <ref type="bibr" target="#b22">[22]</ref> and the Red Lesion Endoscopy (RLE) dataset <ref type="bibr" target="#b3">[3]</ref>. Kvasir-Capsule dataset <ref type="bibr" target="#b22">[22]</ref> is a WCE classification dataset with three anatomy classes and eleven luminal finding classes. By following <ref type="bibr" target="#b2">[2]</ref>, we randomly select 2400 images from the Kvasir-Capsule dataset, of which 2000 are used for training and 400 for testing. To create low-light images, we adopt random Gamma correction and illumination reduction following <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b16">16]</ref>. Furthermore, to evaluate the performance on real data, we add an external validation on 100 real images selected from the Kvasir-Capsule dataset. These images are with low brightness and are not included in our original experiments.</p><p>Red Lesion Endoscopy dataset <ref type="bibr" target="#b3">[3]</ref> (RLE) is a WCE dataset for red lesion segmentation tasks (e.g., angioectasias, angiodysplasias, and bleeding). We randomly choose 1283 images, of which 946 images are used for training and 337 for testing. We adopt the same method in the Kvasir-Capsule dataset to generate low-light images. Furthermore, we conduct a segmentation task on the RLE test set to investigate the effectiveness of the LLIE models in clinical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We compare the performance of our LLCaps against the following state-of-theart (SOTA) LLIE methodologies: LIME <ref type="bibr" target="#b7">[7]</ref>, DUAL <ref type="bibr" target="#b31">[31]</ref>, Zero-DCE <ref type="bibr" target="#b6">[6]</ref>, Enlight-enGAN <ref type="bibr" target="#b10">[10]</ref>, LLFlow <ref type="bibr" target="#b24">[24]</ref>, HWMNet <ref type="bibr" target="#b4">[4]</ref>, MIRNet <ref type="bibr" target="#b29">[29]</ref>, SNR-Aware <ref type="bibr" target="#b28">[28]</ref>, Still-GAN <ref type="bibr" target="#b17">[17]</ref>, MIRNetv2 <ref type="bibr" target="#b30">[30]</ref>, and DDPM <ref type="bibr" target="#b8">[8]</ref>. Our models are trained using Adam optimizer for 200 epochs with a batch size of 4 and a learning rate of 1 × 10 -4 . For evaluation, we adopt three commonly used image quality assessment metrics: Peak Signal-to-Noise Ratio (PSNR) <ref type="bibr" target="#b9">[9]</ref>, Structural Similarity Index (SSIM) <ref type="bibr" target="#b25">[25]</ref>, and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b32">[32]</ref>. For the external validation set, we evaluate with no-reference metrics LPIPS <ref type="bibr" target="#b32">[32]</ref> and Perceptionbased Image Quality Evaluator (PIQE) <ref type="bibr" target="#b23">[23]</ref> due to the lack of ground truth images. To verify the usefulness of the LLIE methods for downstream medical tasks, we conduct red lesion segmentation on the RLE test set and evaluate the performance via mean Intersection over Union (mIoU), Dice similarity coefficient (Dice), and Hausdorff Distance (HD). We train UNet <ref type="bibr" target="#b20">[20]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We compare the performance of our LLCaps to the existing approaches, as demonstrated in Table <ref type="table">1</ref> and Fig. <ref type="figure" target="#fig_3">3</ref> quantitatively and qualitatively. Compared with other methods, our proposed method achieves the best performance among all metrics. Specifically, our method surpasses MIRNetv2 <ref type="bibr" target="#b30">[30]</ref> by 3.57 dB for the Kvasir-Capsule dataset and 0.33 dB for the RLE dataset. The SSIM of our method has improved to 96.34% in the Kvasir-Capsule dataset and 93.34% in the RLE dataset. Besides that, our method also performs the best in the noreference metric LPIPS. The qualitative results of the comparison methods and Table <ref type="table">1</ref>. Image quality comparison with existing methods on Kvasir-Capsule <ref type="bibr" target="#b22">[22]</ref> and RLE dataset <ref type="bibr" target="#b3">[3]</ref>. The 'External Val' denotes the external validation experiment conducted on 100 selected real low-light images from the Kvasir-Capsule dataset <ref type="bibr" target="#b22">[22]</ref>. The red lesion segmentation experiment is also conducted on RLE test set <ref type="bibr" target="#b3">[3]</ref>.</p><p>our method on the Kvasir-Capsule and RLE datasets are visualized in Fig. <ref type="figure" target="#fig_3">3</ref> with the corresponding heat maps. Firstly, we can see that directly performing LLIE training on DDPM <ref type="bibr" target="#b8">[8]</ref> cannot obtain good image restoration, and the original structures of the DDPM images are largely damaged. EnlightenGAN <ref type="bibr" target="#b10">[10]</ref> also does not perform satisfactorily in structure restoration. Our method successfully surpasses LLFlow <ref type="bibr" target="#b24">[24]</ref> and MIRNetv2 <ref type="bibr" target="#b30">[30]</ref> in illumination restoration. The error heat maps further reflect the superior performance of our method in recovering the illumination and structure from low-light images. Moreover, our solution yields the best on the real low-light dataset during the external validation, proving the superior performance of our solution in real-world applications.</p><p>Furthermore, a downstream red lesion segmentation task is conducted to investigate the usefulness of our LLCaps on clinical applications. As illustrated in Table <ref type="table">1</ref>, LLCaps achieve the best lesion segmentation results, manifesting the superior performance of our LLCaps model in lesion segmentation. Additionally, LLCaps surpasses all SOTA methods in HD, showing LLCaps images perform perfectly in processing the segmentation boundaries, suggesting that our method possesses better image reconstruction and edge retention ability.</p><p>Besides, an ablation study is conducted on the Kvasir-Capsule dataset to demonstrate the effectiveness of our design and network components, as shown in Table <ref type="table">2</ref>. To observe and compare the performance changes, we try to (i) remove the wavelet transform in CWA blocks, (ii) degenerate the curved attention (CurveA) layer in CWA block to a simple channel attention layer <ref type="bibr" target="#b29">[29]</ref>, and (iii) remove the reverse diffusion branch. Experimental results demonstrate that the absence of any component shall cause great performance degradation. The significant improvement in quantitative metrics is a further testament to the effectiveness of our design for each component.</p><p>Table <ref type="table">2</ref>. Ablation experiments of our LLCaps on the Kvasir-Capsule Dataset <ref type="bibr" target="#b22">[22]</ref>. In order to observe the performance changes, we (i) remove the wavelet transform, (ii) degenerate the CurveA layer, and (iii) remove the reverse diffusion branch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wavelet Transform</head><note type="other">Curve</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of normal images with low-light images. Obvious lesions are visible on normal images, but the same lesions can hardly be distinguished by human eyes in the corresponding low-light images.</figDesc><graphic coords="2,57,75,296,15,337,84,53,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overview of our proposed LLCaps. The CNN branch shall extract the shallow image output while the DDPM branch further optimizes the image via Markov chain inference. (a) represents the multi-scale residual block (MSRB), which allows the model to learn representation on different resolutions. (b) denotes our curved wavelet attention (CWA) block for attention learning and feature restoration. In (b), DWT and IWT denote discrete wavelet transform and inverse wavelet transform, respectively. The PReLU with two convolutional layers constructs the feature selector. 'MaxP' denotes max pooling, and 'GAP' means global average pooling.</figDesc><graphic coords="4,57,48,217,31,337,24,189,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>using Adam optimizer for 20 epochs. The batch size and learning rate are set to 4 and 1 × 10 -4 , respectively. All experiments are implemented by Python PyTorch and conducted on NVIDIA RTX 3090 GPU. Results are the average of 3-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The quantitative results for LLCaps compared with SOTA approaches on (a) Kvasir-Capsule dataset [22] and (b) RLE dataset [3]. The first row visualizes the enhanced images from different LLIE approaches, and the second row contains the reconstruction error heat maps. The blue and red represent low and high error, respectively. (Color figure online)</figDesc><graphic coords="7,66,90,234,77,301,48,166,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We present LLCaps, an end-to-end capsule endoscopy LLIE framework with multi-scale CNN and reverse diffusion process. The CNN branch is constructed by stacked MSRB modules, in which the core CWA block extracts high-frequency detail information through wavelet transform, and learns the local representation of the image via the Curved Attention layer. The reverse diffusion process further optimizes the shallow output, achieving the closest approximation to the real image. Comparison and ablation studies prove that our method and design bring about superior performance improvement in image quality. Further medical image segmentation experiments demonstrate the reliability of our method in clinical applications. Potential future works include extending our model to various medical scenarios (e.g., surgical robotics, endoscopic navigation, augmented reality for surgery) and clinical deep learning model deployment.</figDesc><table><row><cell></cell><cell>Reverse</cell><cell cols="2">PSNR ↑ SSIM ↑ LPIPS ↓</cell></row><row><cell>Attention</cell><cell>Diffusion</cell><cell></cell></row><row><cell></cell><cell></cell><cell>31.12</cell><cell>94.96</cell><cell>0.0793</cell></row><row><cell></cell><cell></cell><cell>32.78</cell><cell>96.26</cell><cell>0.0394</cell></row><row><cell></cell><cell></cell><cell>32.08</cell><cell>96.27</cell><cell>0.0415</cell></row><row><cell></cell><cell></cell><cell>33.10</cell><cell>94.53</cell><cell>0.0709</cell></row><row><cell></cell><cell></cell><cell>33.92</cell><cell>96.20</cell><cell>0.0381</cell></row><row><cell></cell><cell></cell><cell>34.07</cell><cell>95.61</cell><cell>0.0518</cell></row><row><cell></cell><cell></cell><cell>33.41</cell><cell>95.03</cell><cell>0.0579</cell></row><row><cell></cell><cell></cell><cell>35.24</cell><cell>96.34 0.0374</cell></row><row><cell>4 Conclusion</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">Hong Kong RGC CRF</rs> <rs type="grantNumber">C4063-18G</rs>, <rs type="grantNumber">CRF C4026-21GF</rs>, <rs type="grantNumber">RIF R4020-22</rs>, <rs type="grantNumber">GRF 14216022</rs>, <rs type="grantNumber">GRF 14211420</rs>, <rs type="grantNumber">NSFC/RGC JRS N CUHK420/22</rs>; <rs type="affiliation">Shenzhen-Hong Kong-Macau Technology Research Programme (Type C 202108233000303</rs>); GBABF #<rs type="grantNumber">2021B1515120035</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9kmsYPp">
					<idno type="grant-number">C4063-18G</idno>
				</org>
				<org type="funding" xml:id="_P347dU9">
					<idno type="grant-number">CRF C4026-21GF</idno>
				</org>
				<org type="funding" xml:id="_hrkWKWV">
					<idno type="grant-number">RIF R4020-22</idno>
				</org>
				<org type="funding" xml:id="_DK5N9QD">
					<idno type="grant-number">GRF 14216022</idno>
				</org>
				<org type="funding" xml:id="_pJMjxkX">
					<idno type="grant-number">GRF 14211420</idno>
				</org>
				<org type="funding" xml:id="_dHwM3j5">
					<idno type="grant-number">NSFC/RGC JRS N CUHK420/22</idno>
				</org>
				<org type="funding" xml:id="_XGgqaMQ">
					<idno type="grant-number">2021B1515120035</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Llcaps</surname></persName>
		</author>
		<idno>Ours) 35.24 96.34 0.0374 33.18 93.34 0.0721 0.3082 20.67 66.47 78.47 44.37 References</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic depth-aware network for endoscopy super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5189" to="5200" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deep learning approach for red lesions detection in video capsule endoscopies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93000-8_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93000-863" />
	</analytic>
	<monogr>
		<title level="m">ICIAR 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10882</biblScope>
			<biblScope unit="page" from="553" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Half wavelet attention on M-Net+ for low-light image enhancement</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3878" to="3882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-light image enhancement of high-speed endoscopic videos using a convolutional neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Semmler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schützenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Döllinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11517-019-01965-4</idno>
		<idno>11517-019-01965-4</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1451" to="1463" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIME: low-light image enhancement via illumination map estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scope of validity of PSNR in image/video quality assessment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="800" to="801" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">EnlightenGAN: deep light enhancement without paired supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-light image and video enhancement using deep learning: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9396" to="9416" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-revealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrast enhancement using stratified parametricoriented histogram equalization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1171" to="1181" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive image enhancement based on guide image and fraction-power transformation for wireless capsule endoscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="993" to="1003" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LLNet: a deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure and illumination constrained GAN for medical image enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3955" to="3967" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cycle structure and illumination constrained GAN for medical image enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-964" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DiffuseVAE: efficient, controllable and high-fidelity generation from low-dimensional latents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.00308</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flexible and capsule endoscopy for screening, diagnosis and treatment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Sliker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ciuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Rev. Med. Devices</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="649" to="666" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kvasir-capsule, a video capsule endoscopy dataset</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">142</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind image quality evaluation using perception based features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Venkatanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Praneeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Bh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Medasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Twenty First National Conference on Communications (NCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-light image enhancement with normalizing flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2604" to="2612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">MedSegDiff-V2: diffusion based medical image segmentation with transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11798</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30392" to="30400" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SNR-aware low-light image enhancement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17714" to="17724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning enriched features for real image restoration and enhancement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58595-2_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58595-230" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020, Part XXV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="492" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning enriched features for fast image restoration and enhancement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1934" to="1948" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual illumination estimation for robust exposure correction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="243" to="252" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LEDNet: joint low-light enhancement and deblurring in the dark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20068-7_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20068-733" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13666</biblScope>
			<biblScope unit="page" from="573" to="589" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
