<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training</title>
				<funder ref="#_drsxCRe">
					<orgName type="full">Federal Ministry for Education and Research of Germany</orgName>
				</funder>
				<funder ref="#_2zBQZaZ">
					<orgName type="full">Federal Ministry for Economic Affairs and Climate Action of Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Bigalke</surname></persName>
							<email>alexander.bigalke@uni-luebeck.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lasse</forename><surname>Hansen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">EchoScout GmbH</orgName>
								<address>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mattias</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
							<email>mattias.heinrich@uni-luebeck.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="677" to="687"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4E6243AA96B3C70876E3E173D887F0CC</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Registration</term>
					<term>Unsupervised learning</term>
					<term>Self-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art deep learning-based registration methods employ three different learning strategies: supervised learning, which requires costly manual annotations, unsupervised learning, which heavily relies on hand-crafted similarity metrics designed by domain experts, or learning from synthetic data, which introduces a domain shift. To overcome the limitations of these strategies, we propose a novel selfsupervised learning paradigm for unsupervised registration, relying on self-training. Our idea is based on two key insights. Feature-based differentiable optimizers 1) perform reasonable registration even from random features and 2) stabilize the training of the preceding feature extraction network on noisy labels. Consequently, we propose cyclical self-training, where pseudo labels are initialized as the displacement fields inferred from random features and cyclically updated based on more and more expressive features from the learning feature extractor, yielding a selfreinforcement effect. We evaluate the method for abdomen and lung registration, consistently surpassing metric-based supervision and outperforming diverse state-of-the-art competitors. Source code is available at https://github.com/multimodallearning/reg-cyclical-self-train.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image registration is a fundamental task in medical imaging with applications ranging from multi-modal data fusion to temporal data analysis. In recent years, deep learning has advanced learning-based registration methods <ref type="bibr" target="#b10">[11]</ref>, which achieve competitive performances at low runtimes and thus constitute a promising alternative to accurate but slow classical optimization methods. A decisive factor in successfully training deep learning-based methods is the choice of a suitable strategy to supervise the learning process. In the literature, there exist three different learning strategies. The first is supervised learning based on manual annotations such as landmark correspondences <ref type="bibr" target="#b8">[9]</ref> or semantic labels <ref type="bibr" target="#b15">[16]</ref>. However, manual annotations are costly and may introduce a label bias <ref type="bibr" target="#b1">[2]</ref>. Alternatively, a second strategy employs synthetic deformation fields to generate image pairs with precisely known displacement fields <ref type="bibr" target="#b6">[7]</ref>. However, this introduces a domain gap between synthetic training and real test pairs, limiting the performance at inference time. Elaborated deformation techniques can reduce the gap but require strong domain knowledge, are tailored to specific problems, and do not generalize across tasks. The third widely used training strategy is unsupervised metric-based learning, maximizing a similarity metric between fixed and warped moving images, e.g. implemented in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Popular metrics include normalized cross-correlation <ref type="bibr" target="#b19">[19]</ref> and MIND <ref type="bibr" target="#b12">[13]</ref>. However, the success of this strategy strongly depends on the specific hand-crafted metric, and the performance of the trained deep learning models is often inferior to a classical optimization-based counterpart. Considering the deficiencies of the above training techniques, in this work, we introduce a novel learning strategy for unsupervised registration based on the concept of self-training.</p><p>Self-training is a widespread training strategy for semi-supervised learning <ref type="bibr" target="#b25">[24]</ref> and domain adaptation <ref type="bibr" target="#b30">[29]</ref>. The core idea is to pre-train a network on available labeled data and subsequently apply the model to the unlabeled data to generate so-called pseudo labels. Afterwards, one alternates between re-training the model on the union of labeled and pseudo-labeled data and updating the pseudo labels with the current model. This general concept was successfully adapted to diverse tasks and settings, with methods in medical context primarily focusing on segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">18]</ref>. These methods resort to a special form of self-training, the Mean Teacher paradigm <ref type="bibr" target="#b22">[22]</ref>, where pseudo labels are continuously provided by a teacher model, representing a temporal ensemble of the learning network. A persistent problem of classical and Mean Teacher-based selftraining is the inherent noise of the pseudo labels, which can severely hamper the learning process. As a remedy, some works aim to filter reliable pseudo labels based on model uncertainty <ref type="bibr" target="#b29">[28]</ref>. Only recently, the Mean Teacher was adapted to the registration problem, tackling domain adaptation <ref type="bibr" target="#b2">[3]</ref> or complementing metric-based supervision for adaptive regularization weighting <ref type="bibr" target="#b26">[25]</ref>. Contrary to these methods, we introduce self-training for registration in a fully unsupervised setting, with pseudo labels as the single source of supervision.</p><p>Contributions. We introduce a novel learning paradigm for unsupervised registration by adapting the concept of self-training to the problem. This involves two principal challenges. First, labeled data for the pre-training stage is unavailable, raising the question of how to generate initial pseudo labels. Second, as a general problem in self-training, the negative impact of noise in the pseudo labels needs to be mitigated. In our pursuit to overcome these challenges, we made two decisive observations (see Fig. <ref type="figure" target="#fig_1">2</ref>) when exploring a combination of deep learning-based feature extraction with differentiable optimization algorithms for the displacement prediction, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">20]</ref>. First, we found that feature-based optimizers predict reasonable displacement fields and improve the initial registration even when applied to the output of random feature networks (orange line in Fig. <ref type="figure" target="#fig_1">2</ref>). We attribute this feature to the inductive bias of deep neural networks, which extract somewhat useful features even with random weights <ref type="bibr" target="#b3">[4]</ref>. These predicted displacements thus constitute meaningful initial pseudo labels, solving the first problem and leaving us with the second problem to overcome the noise in the labels. In this context, we made the second observation that the intrinsic regularizing capacity of the optimizers stabilizes the learning from noisy labels. Specifically, training the feature extractor on our initial pseudo labels yielded registrations surpassing the accuracy of the noisy labels used for training (green, red, purple, brown, and magenta lines in Fig. <ref type="figure" target="#fig_1">2</ref>). Consequently, we propose a cyclical self-training scheme, alternating between training the feature extractor and updating the pseudo labels. As such, our novel learning paradigm does not require costly manual annotations, prevents the domain shift of synthetic deformations, and is independent of hand-crafted similarity metrics. Moreover, our method significantly differs from previous uncertainty-based pseudo label filtering strategies since it implicitly overcomes the negative impact of noisy labels by combining deep feature learning with regularizing differentiable optimization. We evaluate the method for CT abdomen registration and keypoint-based lung registration, demonstrating substantial improvements over diverse state-of-theart comparison methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setup</head><p>Given a data pair (F , M ) of a fixed and a moving image as input, registration aims at finding a displacement field ϕ that spatially aligns M to F . We address the task in an unsupervised setting, where training data</p><formula xml:id="formula_0">T = {(F i , M i )} |T | i=1</formula><p>consists of |T | unlabeled data pairs. Given the training data, we aim to learn a function f with parameters θ f , (partially) represented by a deep network, which predicts displacement fields as φ = f (F , M ; θ f ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cyclical Self-training</head><p>We propose to solve the above problem with a cyclical self-training strategy visualized in Fig. <ref type="figure" target="#fig_0">1</ref>. While existing self-training methods assume the availability of some labeled data, annotations are unavailable in our unsupervised setting. To overcome this issue and generate an initial set of pseudo labels for the first stage of self-training, we parameterize the function f as the combination of a deep neural network g for feature extraction with a non-learnable but differentiable feature-based optimization algorithm h for displacement prediction, i.e.</p><formula xml:id="formula_1">f (F , M ; θ f ) = h(g(F , M ; θ g ))<label>(1)</label></formula><p>The approach is based on our empirical observation that a suitable optimization algorithm h can predict reasonable initial displacement fields φ(0) from random features provided by a network g (0) with random initialization θ (0) g , which is in</p><formula xml:id="formula_2">Image Pair ( , ) F M Deep Network g (t-1) Stage t Deep Network g (t)</formula><p>Deep Network (initialised with )  t) with pseudo labels generated based on the features from the network g (t-1) from the previous stage. For optimal feature learning, the pseudo displacements from the optimizer are further refined and regularized.</p><formula xml:id="formula_3">g (t+1) θ (t) g Augmentation Feature Pair ( , ) g (t-1) (F, θ (t-1) g ) g (t-1) (M, θ (t-1) g ) Feature Pair ( , ) g (t) (F, θ (t) g ) g (t) (M, θ (t) g )</formula><p>line with recent studies on the inductive bias of CNNs <ref type="bibr" target="#b3">[4]</ref>. We leverage these predicted displacements as pseudo labels to supervise the first stage of self-training, where the parameters of the feature extractor with different initialization θ (1)   g are optimized by minimizing the loss</p><formula xml:id="formula_4">L(θ (1) g ; T ) = 1 |T | i TRE h g F i , M i ; θ (1) g , φ<label>(0) i (2)</label></formula><p>with TRE( φ(1) i , φ(0) i ) denoting the mean over the element-wise target registration error between the displacement fields φ(1) i and φ(0) i . A critical problem of this basic setup is that the network might overfit the initial pseudo labels and learn to reproduce random features. Therefore, in the spirit of recent techniques from contrastive learning <ref type="bibr" target="#b5">[6]</ref>, we propose to improve the efficacy of feature learning by incorporating asymmetries into the learning and pseudo label streams at two levels. First, we apply different random augmentations to the input pairs in both streams. Second, we augment the pseudo label stream with additional (non-differentiable) fine-tuning and regularization steps after the optimizer to improve the pseudo displacement fields (see Sec. 2.3 for details). As demonstrated in our ablation experiments (Fig. <ref type="figure" target="#fig_1">2</ref>, Table <ref type="table" target="#tab_0">1</ref>), both strategies improve feature learning and strengthen the self-improvement effect.</p><p>Once the first stage of self-training has converged, we repeat the process T times. Specifically, at stage t, we generate refined pseudo labels with the trained network g (t-1) from the previous stage, initialize the learning network g (t) with the weights from g (t-1) and perform a warm restart on the learning rate to escape potential local minima from the previous stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Registration Framework</head><p>Our proposed self-training scheme is a flexible, modular framework, agnostic to the input modality and the specific implementation of feature extractor g and optimizer h. This section describes our specific design choices for g and h for image and point cloud registration, with the former being our main focus.</p><p>Image Registration. To extract features from 3D input volumes, we implement g a standard 3D CNN with six convolution layers with kernel sizes 3 × 3 × 3 and 32, 64, or 128 channels. Each convolution is followed by BatchNorm and ReLU, and every second convolution contains a stride of 2, yielding a downsampling factor of 8. The outputs for both images are mapped to 16-dimensional features using a 1 × 1 × 1 convolution and fed into a correlation layer <ref type="bibr" target="#b21">[21]</ref> that captures 125 discrete displacements.</p><p>As the optimizer, we adapt the coupled convex optimization for learningbased 3D registration from <ref type="bibr" target="#b20">[20]</ref>, which, given fixed and moving features, infers a displacement field that minimizes a combined objective of smoothness and feature dissimilarity. Our proposed refinement strategy in the pseudo label stream comprises three ingredients. 1) Forward-backward consistency additionally computes the reverse displacement field (F to M ) and then iteratively minimizes the discrepancy between both fields. 2) For a second warp, the moving image is warped with the inferred displacement field before repeating all previous steps. 3) Iterative instance optimization finetunes the final displacement field with Adam by jointly minimizing regularization cost and feature dissimilarity. For the latter, we use the CNN features after the second convolution block and map them with a 1×1×1 convolution to 16 channels. We apply the same refinement steps at test time. Moreover, we propose to leverage the difference between network-predicted and finetuned displacements to estimate the difficulty of the training samples. Consequently, we apply a weighted batch sampling at training that increases the probability of using less difficult registration pairs with a higher agreement between both fields. We rank all training pairs and use a sigmoid function with arguments ranging linearly from -5 to 5 for the weighted random sampler.</p><p>Point Cloud Registration. For point cloud registration, we implement the feature extractor as a graph CNN and rely on sparse loopy belief propagation for differentiable optimization, as introduced in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We conduct our main experiments for inter-patient abdomen CT registration using the corresponding dataset of the Learn2Reg (L2R) Challenge<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b14">[15]</ref>. The dataset contains 30 abdominal 3D CT scans of different patients with 13 manually labeled anatomical structures of strongly varying sizes. The original image data and labels are from <ref type="bibr" target="#b27">[26]</ref>. As part of L2R, they were affinely preregistered into a canonical space and resampled to identical voxel resolutions (2 mm) and spatial dimensions (192 × 160 × 256 vx). Following the data split of L2R, we use 20 scans (190 pairs) for training and the remaining 10 scans (45 pairs) for evaluation. Hence, data split and preprocessing are consistent with compared previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">27]</ref>. As metrics, we report the mean Dice overlap (DSC) between the semantic labels and the standard deviation of the logarithmic Jacobian determinant (SDlogJ).</p><p>We perform a second experiment for inhale-to-exhale lung CT registration on the DIR-Lab COPDGene dataset<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b4">[5]</ref>, which comprises 10 such scan pairs. For each pair, 300 expert-annotated landmark correspondences are available for evaluation. We pre-process all scans in multiple steps: 1) resampling to 1.75×1.00×1.25 mm for exhale and 1.75×1.25×1.75 mm for inhale, 2) cropping with fixed-size bounding boxes (192 × 192 × 208 vx), centered around automatically generated lung masks, 3) affine pre-registration, aligning the lung masks. Since we focus on keypoint-based registration of the lung CTs, we follow <ref type="bibr" target="#b8">[9]</ref> and extract distinctive keypoints from the CTs using the Förstner algorithm with non-maximum suppression, yielding around 1k points in the fixed and 2k points in the moving cloud. In our experiments, we perform 5-fold cross-validation, with each fold comprising eight data pairs for training and two for testing. We report the target registration error (TRE) at the landmarks and the SDlogJ as metrics.</p><p>Implementation Details. We implement all methods in Pytorch and optimize network parameters with the Adam optimizer. For abdomen registration, we train for T = 8 stages, each stage comprising 1000 iterations with a batch size of 2. The learning rate follows a cosine annealing warm restart schedule, decaying from 10 -3 to 10 -5 at each stage. Hyper-parameters were set based on the DSC on three cases from the training set. For lung registration, the model converged after T = 5 stages of 60 epochs with batch size 4, with an initial learning rate of 0.001, decreased by a factor of 10 at epochs 40 and 52. Here, hyper-parameters were adopted from <ref type="bibr" target="#b8">[9]</ref>. For both datasets, training requires 90-100 min and 8 GB on an RTX2080, and input augmentations consist of random affine transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Abdomen. First, we analyze our method in several ablation experiments. In Fig. <ref type="figure" target="#fig_1">2</ref>, we visualize the performance of our method on a subset of classes over several cycles of self-training. We observe consistent improvements over the stages, particularly pronounced at early stages while the performance converges later on. This highlights the self-reinforcing effect achieved through alternating pseudo label updates and network training. In the upper part of Table <ref type="table" target="#tab_0">1</ref>, we verify  the efficacy of incorporating asymmetries (input augmentations, finetuning of pseudo labels) into both streams and weighted sampling. The results confirm the importance of each component to reach optimal performance. In the lower part of Table <ref type="table" target="#tab_0">1</ref>, we evaluate our final model under different test configurations, highlighting the improvements through a second warp and Adam finetuning. Next, we compare our method to a comprehensive set of state-of-the-art unsupervised methods, including classical algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> and deep learningbased approaches, trained with MIND <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>/NCC <ref type="bibr" target="#b16">[17]</ref> supervision or contrastive learning <ref type="bibr" target="#b28">[27]</ref>. The results are collected from <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">27]</ref>. Moreover, we train our own registration framework with metric-based supervision (MIND <ref type="bibr" target="#b12">[13]</ref>, NCC <ref type="bibr" target="#b19">[19]</ref>) to directly verify the advantage of our self-training strategy. Results are shown in Table <ref type="table" target="#tab_1">2</ref>, Fig. <ref type="figure">3</ref>, and Supp., Fig. <ref type="figure" target="#fig_0">1</ref>. Our method substantially outperforms all comparison methods in terms of DSC (statistical significance is confirmed by a Fig. <ref type="figure">3</ref>. Qualitative results of selected methods on two cases of the Abdomen CT dataset (axial view). We show overlays of the warped segmentation labels with the fixed scan: liver , stomach , left kidney , right kidney , spleen , gall bladder , esophagus , pancreas , aorta , inferior vena cava , portal vein , left /right adrenal gland.  Lung. For point cloud-based lung registration, we compare our cyclical selftraining strategy to three alternative learning strategies: supervision with manually annotated landmark correspondences as in <ref type="bibr" target="#b8">[9]</ref>, metric-based supervision with Chamfer distance and local Laplacian penalties as in <ref type="bibr" target="#b23">[23]</ref>, and training on synthetic rigid/random field deformations. All strategies are implemented for the same baseline registration model from <ref type="bibr" target="#b8">[9]</ref>. Moreover, we report the performance of three unsupervised image-based deep learning methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref> trained with MIND supervision. Results are shown in Table <ref type="table" target="#tab_2">3</ref>, demonstrating the superiority of our self-training strategy over all competing learning strategies and the reported image-based SOTA methods. Qualitative results of the experiment are shown in Supp., Fig. <ref type="figure" target="#fig_1">2</ref>, demonstrating accurate and smooth displacements, as also confirmed by low values of SDlogJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced a novel cyclical self-training paradigm for unsupervised registration. To this end, we developed a modular registration pipeline of a deep feature extraction network coupled with a differentiable optimizer, stabilizing learning from noisy pseudo labels through regularization and iterative, cyclical refinement. That way, our method avoids pitfalls of popular metric supervision (NCC, MIND), which relies on shallow features or image intensities and is prone to noise and local minima. By contrast, our supervision through optimization-refined and -regularized pseudo labels promotes learning task-specific features that are more robust to noise, and our cyclical learning strategy gradually improves the expressiveness of features to avoid local minima. In our experiments, we demonstrated the efficacy and flexibility of our approach, which outperformed the competing state-of-the-art methods and learning strategies for dense image-based abdomen and point cloud-based lung registration. In summary, we did not only present the first fully unsupervised self-training scheme but also a new perspective on unsupervised learning-based registration. In particular, we consider our strategy complementary to existing techniques (metric-based and contrastive learning), opening up the potential for combined training schemes in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed cyclical self-training paradigm for unsupervised registration. The underlying registration pipeline comprises a deep network for feature extraction g and a differentiable optimizer h to predict the displacements. At stage t, we supervise the training of the network g(t) with pseudo labels generated based on the features from the network g (t-1) from the previous stage. For optimal feature learning, the pseudo displacements from the optimizer are further refined and regularized.</figDesc><graphic coords="4,41,79,53,96,340,33,160,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. "Opposite" cumulative distribution of Dice overlaps for Abdomen CT registration after different stages of self-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study for abdomen CT registration.</figDesc><table><row><cell>Method</cell><cell>DSC SDlogJ</cell></row><row><cell>prealign</cell><cell>25.9 -</cell></row><row><cell>w/o input augm</cell><cell>48.8 .129</cell></row><row><cell>w/o PL refinement</cell><cell>48.8 .200</cell></row><row><cell cols="2">w/o weighted sampling 50.1 .147</cell></row><row><cell>ours</cell><cell>51.1 .146</cell></row><row><cell>1 warp w/o Adam</cell><cell>38.6 .061</cell></row><row><cell>1 warp w/ Adam</cell><cell>49.6 .119</cell></row><row><cell>2 warps w/o Adam</cell><cell>41.1 .088</cell></row><row><cell cols="2">2 warps w/Adam (ours) 51.1 .146</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results for unsupervised abdomen CT registration.</figDesc><table><row><cell>Method</cell><cell cols="3">Dice [%] SDlogJ Time [s]</cell></row><row><cell>pre-aligned</cell><cell>25.9</cell><cell>-</cell><cell></cell></row><row><cell>Adam [10]</cell><cell>36.6</cell><cell>.080</cell><cell>1.6</cell></row><row><cell>Iter. LBP [10]</cell><cell>40.1</cell><cell>.093</cell><cell>0.6</cell></row><row><cell cols="2">ANTs (SyN) [1] 28.4</cell><cell>N/A</cell><cell>74.3</cell></row><row><cell>DEEDS [14]</cell><cell>46.5</cell><cell>N/A</cell><cell>45.4</cell></row><row><cell cols="2">VoxelMorph [2] 35.4</cell><cell>.134</cell><cell>0.2</cell></row><row><cell>PDD [12]</cell><cell>41.5</cell><cell>.129</cell><cell>1.4</cell></row><row><cell>LapIRN [17]</cell><cell>42.4</cell><cell>.089</cell><cell>3.8</cell></row><row><cell>SAME [27]</cell><cell>49.8</cell><cell>N/A</cell><cell>1.2</cell></row><row><cell>MIND sup</cell><cell>47.7</cell><cell>.237</cell><cell>1.2</cell></row><row><cell>NCC sup</cell><cell>48.1</cell><cell>.299</cell><cell>1.2</cell></row><row><cell>ours</cell><cell>51.1</cell><cell>.146</cell><cell>1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results for lung CT registration on the COPD dataset.Wilcoxon-signed rank test with p&lt;0.0001 for all competitors with public code, which excludes SAME) and sets a new state-of-the-art accuracy of 51.1% DSC. This highlights the advantages of our new learning paradigm over previous unsupervised strategies. Meanwhile, the smoothness of predicted displacement fields (SDlogJ) is comparable with most unsupervised deep learning-based methods<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> and superior to MIND-and NCC-supervision.</figDesc><table><row><cell>Method</cell><cell cols="2">TRE [mm] SDlogJ</cell></row><row><cell cols="2">initial (pre-aligned) 11.99</cell><cell>-</cell></row><row><cell>VoxelMorph [2]</cell><cell>7.98</cell><cell>N/A</cell></row><row><cell>LapIRN [17]</cell><cell>4.99</cell><cell>N/A</cell></row><row><cell>PDD [12]</cell><cell>2.16</cell><cell>N/A</cell></row><row><cell>rigid deform</cell><cell>2.98</cell><cell>.037</cell></row><row><cell>rnd. field deform</cell><cell>3.19</cell><cell>.035</cell></row><row><cell>metric sup. [23]</cell><cell>6.79</cell><cell>.042</cell></row><row><cell>landmark sup. [9]</cell><cell>2.27</cell><cell>.036</cell></row><row><cell>ours</cell><cell>1.93</cell><cell>.033</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://learn2reg.grand-challenge.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://med.emory.edu/departments/radiation-oncology/research-laboratories/ deformable-image-registration/downloads-and-reference-data/copdgene.html.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. We gratefully acknowledge the financial support by the <rs type="funder">Federal Ministry for Economic Affairs and Climate Action of Germany</rs> (FKZ: <rs type="grantNumber">01MK20012B</rs>) and by the <rs type="funder">Federal Ministry for Education and Research of Germany</rs> (FKZ: <rs type="grantNumber">01KL2008</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2zBQZaZ">
					<idno type="grant-number">01MK20012B</idno>
				</org>
				<org type="funding" xml:id="_drsxCRe">
					<idno type="grant-number">01KL2008</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_64.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting the mean teacher for keypointbased lung registration under geometric domain shifts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bigalke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_27" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A random CNN sees objects: one inductive bias of CNN and its applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The AAAI Conference On Artificial Intelligence</title>
		<meeting>Of The AAAI Conference On Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A reference dataset for deformable image registration spatial accuracy evaluation using the copdgene study archive</title>
		<author>
			<persName><forename type="first">R</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Bio</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2861</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pulmonary CT registration through supervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local and global structure-aware entropy regularized mean teacher model for 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_55" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020: 23rd International Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Lima, Peru; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">October 4-8, 2020. 2020</date>
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning based geometric registration for medical images: how accurate can we get without visual features?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-0_2" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-06-30">June 28-June 30, 2021. 2021</date>
			<biblScope unit="page" from="18" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting iterative highly efficient optimisation schemes in medical image registration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_20" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Closing the gap between deep and conventional image registration using probabilistic dense displacement networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-7_6" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2019: 22nd International Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Shenzhen, China; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">October 13-17, 2019. 2019</date>
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mind: modality independent neighbourhood descriptor for multi-modal deformable registration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MRF-based deformable registration and ventilation estimation of lung CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1239" to="1248" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learn2reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="697" to="712" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly-supervised convolutional neural networks for multimodal image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020: 23rd International Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Lima, Peru</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">October 4-8, 2020</date>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_21" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical imaging segmentation with self-ensembling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image registration by template matching using normalized cross-correlation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Sarvaiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bombaywala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 International Conference on Advances in Computing, Control, and Telecommunication Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="819" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learn to fuse input features for large-deformation registration with differentiable convex-discrete optimisation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Siebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-11203-4_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-11203-4_13" />
	</analytic>
	<monogr>
		<title level="m">Biomedical Image Registration: 10th International Workshop, WBIR 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">July 10-12, 2022. 2022</date>
			<biblScope unit="page" from="119" to="123" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PointPWC-Net: cost volume on point clouds for (self-)supervised scene flow estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020: 16th European Conference</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">August 23-28, 2020</date>
			<biblScope unit="page" from="88" to="107" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58558-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58558-7_6" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Double-uncertainty guided spatial and temporal consistency regularization weighting for learning-based abdominal registration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_2" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of six registration methods for the human abdomen on clinically acquired CT</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1563" to="1572" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sam: self-supervised learning of pixel-wise anatomical embeddings in radiological images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2658" to="2669" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_67" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2019: 22nd International Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Shenzhen, China; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">October 13-17, 2019. 2019</date>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
