<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trackerless Volume Reconstruction from Intraoperative Ultrasound Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sidaty</forename><surname>El Hadramy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Verde</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Karl-Philippe</forename><surname>Beaudet</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Stéphane</forename><surname>Cotin</surname></persName>
							<email>stephane.cotin@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trackerless Volume Reconstruction from Intraoperative Ultrasound Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="303" to="312"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DED7D05C8DE6074DD7873396058C2EF6</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intraoperative Ultrasound</term>
					<term>Liver Surgery</term>
					<term>Volume Reconstruction</term>
					<term>Recurrent Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a method for trackerless ultrasound volume reconstruction in the context of minimally invasive surgery. It is based on a Siamese architecture, including a recurrent neural network that leverages the ultrasound image features and the optical flow to estimate the relative position of frames. Our method does not use any additional sensor and was evaluated on ex vivo porcine data. It achieves translation and orientation errors of 0.449 ± 0.189 mm and 1.3 ± 1.5 • respectively for the relative pose estimation. In addition, despite the predominant non-linearity motion in our context, our method achieves a good reconstruction with final and average drift rates of 23.11% and 28.71% respectively. To the best of our knowledge, this is the first work to address volume reconstruction in the context of intravascular ultrasound. Source code of this work is publicly available at https://github. com/Sidaty1/IVUS Trakerless Volume Reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Liver cancer is the most prevalent indication for liver surgery, and although there have been notable advancements in oncologic therapies, surgery remains as the only curative approach overall <ref type="bibr" target="#b19">[20]</ref>.</p><p>Liver laparoscopic resection has demonstrated fewer complications compared to open surgery <ref type="bibr" target="#b20">[21]</ref>, however, its adoption has been hindered by several reasons, such as the risk of unintentional vessel damage, as well as oncologic concerns such as tumor detection and margin assessment. Hence, the identification of intrahepatic landmarks, such as vessels, and target lesions is crucial for successful and safe surgery, and intraoperative ultrasound (IOUS) is the preferred technique to accomplish this task. Despite the increasing use of IOUS in surgery, its integration into laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains challenging due to combined problems.</p><p>Performing IOUS during laparoscopic liver surgery poses significant challenges, as laparoscopy has poor ergonomics and narrow fields of view, and on the other hand, IOUS demands skills to manipulate the probe and analyze images. At the end, and despite its real-time capabilities, IOUS images are intermittent and asynchronous to the surgery, requiring multiple iterations and repetitive steps (probe-in -→ instruments-out -→ probe-out -→ instruments-in). Therefore, any method enabling a continuous and synchronous US assessment throughout the surgery, with minimal iterations required would significantly improve the surgical workflow, as well as its efficiency and safety.</p><p>To overcome these limitations, the use of intravascular ultrasound (IVUS) images has been proposed, enabling continuous and synchronous inside-out imaging during liver surgery <ref type="bibr" target="#b18">[19]</ref>. With an intravascular approach, an overall view and full-thickness view of the liver can quickly and easily be obtained through mostly rotational movements of the catheter, while this is constrained to the lumen of the inferior vena cava, and with no interaction with the tissue (contactless, a.k.a. standoff technique) as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. However, to benefit from such a technology in a computer-guided solution, the different US images would need to be tracked and possibly integrated into a volume for further processing. External US probes are often equipped with an electromagnetic tracking system to track its position and orientation in realtime. This information is then used to register the 3D ultrasound image with the patient's anatomy. The use of such an electromagnetic tracking system in laparoscopic surgery is more limited due to size reduction. The tracking system may add additional complexity and cost to the surgical setup, and the tracking accuracy may be affected by metallic devices in the surgical field <ref type="bibr" target="#b21">[22]</ref>.</p><p>Several approaches have been proposed to address this limitation by proposing a trackerless ultrasound volume reconstruction. Physics-based methods have exploited speckle correlation models between different adjacent frames <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> to estimate their relative position. With the recent advances in deep learning, recent works have proposed to learn a higher order nonlinear mapping between adjacent frames and their relative spatial transformation. Prevost et al. <ref type="bibr" target="#b8">[9]</ref> first demonstrated the effectiveness of a convolution neural network to learn the relative motion between a pair of US images. Xie et al. <ref type="bibr" target="#b9">[10]</ref> proposed a pyramid warping layer that exploits the optical flow features in addition to the ultrasound features in order to reconstruct the volume. To enable a smooth 3D reconstruction, a case-wise correlation loss based on 3D CNN and Pearson correlation coefficient was proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. Qi et al. <ref type="bibr" target="#b12">[13]</ref> leverages past and future frames to estimate the relative transformation between each pair of the sequence; they used the consistency loss proposed in <ref type="bibr" target="#b13">[14]</ref>. Despite the success of these approaches, they still suffer significant cumulative drift errors and mainly focus on linear probe motions. Recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> proposed to exploit the acceleration and orientation of an inertial measurement unit (IMU) to improve the reconstruction performance and reduce the drift error. Motivated by the weakness of the state-of-the-art methods when it comes to large non-linear probe motions, and the difficulty of integrating IMU sensors in the case of minimally invasive procedures, we introduce a new method for pose estimation and volume reconstruction in the context of minimally invasive trackerless ultrasound imaging. We use a Siamese architecture based on a Sequence to Vector(Seq2Vec) neural network that leverages image and optical flow features to learn relative transformation between a pair of images.</p><p>Our method improves upon previous solutions in terms of robustness and accuracy, particularly in the presence of rotational motion. Such motion is predominant in the context highlighted above and is the source of additional nonlinearity in the pose estimation problem. To the best of our knowledge, this is the first work that provides a clinically sound and efficient 3D US volume reconstruction during minimally invasive procedures. The paper is organized as follows: Sect. 2 details the method and its novelty, Sect. 3 presents our current results on ex vivo porcine data, and finally, we conclude in Sect. 4 and discuss future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this work, we make the assumption that the organ of interest does not undergo deformation during the volume acquisition. This assumption is realistic due to the small size of the probe. Let I 0 , I 1 ...I N -1 be a sequence of N frames. Our aim is to find the relative spatial transformation between each pair of frames I i and I j with 0 ≤ i ≤ j ≤ N -1. This transformation is denoted T (i,j) and is a six degrees of freedom vector representing three translations and three Euler angles. To achieve this goal, we propose a Siamese architecture that leverages the optical flow in the sequences in addition to the frames of interest in order to provide a mapping with the relative frames spatial transformation. The overview of our method is presented in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We consider a window of 2k + 3 frames from the complete sequence of length N , where 0 ≤ k ≤ N -3 2 is a hyper-parameter that denotes the number of frames between two frames of interest. Our method predicts two relative transformations between the pairs of frames (I 1 , I k+2 ) and (I k+2 , I 2k+3 ). The input window is divided into two equal sequences of length k + 2 sharing a common frame. Both deduced sequences are used to compute a sparse optical flow allowing to track the trajectory of M points. Then, Gaussian heatmaps are used to describe the motion of the M points in an image-like format(see Sect. 2.2). Finaly, a Siamese architecture based on two shared weights Sequence to Vector (Seq2Vec) network takes as input the Gaussian heatmaps in addition to the first and last frames and predicts the relative transformations. In the following we detail our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse Optical Flow</head><p>Given a sequence of frames I i and I i+k+1 , we aim at finding the trajectory of a set of points throughout the sequence. We choose the M most prominent points from the first frame using the feature selection algorithm proposed in <ref type="bibr" target="#b2">[3]</ref>. Points are then tracked throughout each pair of adjacent frames in the sequence by solving Eq. 1 which is known as the Optical flow equation. We use the pyramidal implementation of Lucas-Kanade method proposed in <ref type="bibr" target="#b3">[4]</ref> to solve the equation. Thus, yielding a trajectory matrix A ∈ R M ×(k+2)×2 that contains the position of each point throughout the sequence. Figure <ref type="figure" target="#fig_2">3</ref> illustrates an example where we track two points in a sequence of frames.</p><formula xml:id="formula_0">I i (x, y, t) = I i (x + dx, y + dy, t + dt) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gaussian Heatmaps</head><p>After obtaining the trajectory of M points in the sequence</p><formula xml:id="formula_1">{I i |1 ≤ i ≤ k + 2}</formula><p>we only keep the first and last position of each point, which corresponds to the positions in our frames of interest. We use Gaussian heatmaps H ∈ R H×W with the same dimension as the ultrasound frames to encode these points, they are more suitable as input for the convolutional networks. For a point with a position (x 0 , y 0 ), the corresponding heatmap is defined in the Eq. 2.</p><formula xml:id="formula_2">H(x, y) = 1 σ 2 √ 2π e -(x-x 0 ) 2 +(y-y 0 ) 2 2σ 2<label>(2)</label></formula><p>Thus, each of our M points are converted to a pair of heatmaps that represent the position in the first and last frames of the ultrasound sequence. These pairs concatenated with the ultrasound first and last frames form the recurrent neural network sequential input of size (M + 1, H, W, 2), where M + 1 is the number of channels (M heatmaps and one ultrasound frame), H and W are the height and width of the frames and finally 2 represents the temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Architecture</head><p>The Siamese architecture is based on a sequence to vector network. Our network maps a sequence of two images having M + 1 channel each to a six degrees of freedrom vector (three translations and three rotation angles). The architecture of Seq2Vec is illustrated in the Fig. <ref type="figure" target="#fig_3">4</ref>. It contains five times the same block composed of two Convolutional LSTMs (ConvLSTM) <ref type="bibr" target="#b4">[5]</ref> followed by a Batch Normalisation. Their output is then flattened and mapped to a six degrees of freedom vector through linear layers; ReLU is the chosen activation function for the first linear layer. We use an architecture similar to the one proposed in <ref type="bibr" target="#b4">[5]</ref> for the ConvLSTM layers. Seq2Vec networks share the same weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>In the training phase, given a sequence of 2k + 3 frames in addition to their ground truth transformations T(1,k+2) , T(k+2,2k+3) and T(1,2k+3) , the Seq2Vec's weights are optimized by minimising the loss function given in the Eq. 3. The loss contains two terms. The first represents the mean square error (MSE) between the estimated transformations (T (1,k+2) , T (k+2,2k+3) ) at each corner point of the frames and their respective ground truth. The second term represents the accumulation loss that aims at reducing the error of the volume reconstruction, the effectiveness of the accumulation loss have been proven in the literature <ref type="bibr" target="#b12">[13]</ref>. It is written as the MSE between the estimated T (1,2k+3) = T (k+2,2k+3) ×T <ref type="bibr">(1,k+2)</ref> at the corner points of the frames and the ground truth T <ref type="bibr">(1,2k+3)</ref> .</p><formula xml:id="formula_3">L = ||T (1,k+2) -T(1,k+2) || 2 +||T (k+2,2k+3) -T(k+2,2k+3) || 2 +||T (1,2k+3) -T(1,2k+3) || 2<label>(3)</label></formula><p>3 Results and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>To validate our method, six tracked sequences were acquired from an ex vivo swine liver. A manually manipulated IVUS catheter was used (8 Fr lateral firing AcuNav TM 4-10 MHz) connected to an ultrasound system (ACUSON S3000 HELX Touch, Siemens Healthineers, Germany), both commercially available. An electromagnetic tracking system (trakSTAR TM , NDI, Canada) was used along with a 6 DoF sensor (Model 130) embedded close to the tip of the catheter, and the PLUS toolkit <ref type="bibr" target="#b16">[17]</ref> along with 3D Slicer <ref type="bibr" target="#b17">[18]</ref> were used to record the sequences. The frame size was initially 480 × 640. Frames were cropped to remove the patient and probe characteristics, then down-sampled to a size of 128 × 128 with an image spacing of 0.22 mm per pixel. First and end stages of the sequences were removed from the six acquired sequences, as they were considered to be largely stationary, and aiming to avoid training bias. Clips were created by sliding a window of 7 frames (corresponding to a value of k = 2) with a stride of 1 over each continuous sequence, yielding a data set that contains a total of 13734 clips. The tracking was provided for each frame as a 4×4 transformation matrix.</p><p>We have converted each to a vector of six degrees of freedom that corresponds to three translations in mm and three Euler angles in degrees. For each clip, relative frame to frame transformations were computed for the frames number 0, 3 and 6. The distribution of the relative transformation between the frames in our clips is illustrated in the Fig. <ref type="figure" target="#fig_4">5</ref>. It is clear that our data mostly contains rotations, in particular over the axis x. Heatmaps were calculated for two points (M = 2) and with a quality level of 0.1, a minimum distance of 7 and a block size of 7 for the optical flow algorithm (see <ref type="bibr" target="#b3">[4]</ref> for more details). The number of heatmaps M and the frame jump k were experimentally chosen among 0, 2, 4, 6.</p><p>The data was split into train, validation and test sets by a ratio of 7:1.5:1.5. Our method is implemented in Pytorch<ref type="foot" target="#foot_0">1</ref> 1.8.2, trained and evaluated on a GeForce RTX 3090. We use an Adam optimizer with a learning rate of 10 -4 . The training process converges in 40 epochs with a batch size of 16. The model with the best performance on the validation data was selected and used for the testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics and Results</head><p>The test data was used to evaluate our method, it contains 2060 clips over which our method achieved a translation error of translation of 0.449 ± 0.189 mm, and an orientation error of orientation 1.3 ± 1.5 • . We have evaluated our reconstruction with a commonly used in state-of-the-art metric called final drift error, which measures the distance between the center point of the final frame according to the real relative position and the estimated one in the sequence. On this basis, each of the following metrics was reported over the reconstructions of our method. Final drift rate (FDR): the final drift divided by the sequence length. Average drift rate (ADR): the average cumulative drift of all frames divided by the length from the frame to the starting point of the sequence. Table <ref type="table" target="#tab_0">1</ref> shows the evaluation of our method over these metrics compared to the state-of-the-art methods MoNet <ref type="bibr" target="#b14">[15]</ref> and CNN <ref type="bibr" target="#b8">[9]</ref>. Both state-of-the-art methods use IMU sensor data as additional input to estimate the relative transformation between two relative frames. Due to the difficulty of including an IMU sensor in our IVUS catheter, the results of both methods were reported from the MoNet paper where the models have been trained on arm scans, see <ref type="bibr" target="#b14">[15]</ref> for more details. As the Table <ref type="table" target="#tab_0">1</ref> shows, our method is comparable with state-of-the-art methods in terms of drift errors without using any IMU and with non-linear probe motion as one may notice in our data distribution in the Fig. <ref type="figure" target="#fig_4">5</ref>. Figure <ref type="figure" target="#fig_5">6</ref> shows the volume reconstruction of two sequences of different sizes with our method in red against the ground truth slices. Despite the non-linearity of the probe motion, the relative pose estimation results obtained by our method remains very accurate. However, one may notice that the drift error increases with respect to the sequence length. This remains a challenge for the community even in the case of linear probe motions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed the first method for trackerless ultrasound volume reconstruction in the context of minimally invasive surgery. Our method does not use any additional sensor data and is based on a Siamese architecture that leverages the ultrasound image features and the optical flow to estimate relative transformations. Our method was evaluated on ex vivo porcine data and achieved translation and orientation errors of 0.449±0.189 mm and 1.3±1.5 • respectively with a fair drift error. In the future work, we will extend our work to further improve the volume reconstruction and use it to register a pre-operative CT image in order to provide guidance during interventions.</p><p>Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. left: IVUS catheter positioned in the lumen of the inferior vena cava in the posterior surface of the organ, and an example of the lateral firing and longitudinal beam-forming images; middle: anterior view of the liver and the rotational movements of the catheter providing full-thickness images; right: inferior view showing the rotational US acquisitions</figDesc><graphic coords="2,75,30,268,16,273,46,90,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed method. The input sequence is split into two equal sequences with a common frame. Both are used to compute a sparse optical flow.Gaussian heatmaps tracking M points are then combined with the first and last frame of each sequence to form the network's input. We use a Siamese architecture based on Sequence to Vector (Seq2Vec) network. The learning is done by minimising the mean square error between the output and ground truth transformations.</figDesc><graphic coords="4,44,79,53,72,334,51,125,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sparse Optical tracking of two points in a sequence, red points represent the chosen points to track, while the blue lines describe the trajectory of the points throughout the sequence. (Color figure online)</figDesc><graphic coords="5,84,15,54,14,284,80,78,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of Seq2Vec network. We use five blocks that contains each two ConvLSTM followed by Batch Normalisation. The output is flattened and mapped to a six degree-of-freedom translation and rotation angles through linear layers. The network takes as input a sequence of two images with M + 1 channel each, M heatmaps and an ultrasound frame. The output corresponds to the relative transformation between the blue and red frames. (Color figure online)</figDesc><graphic coords="6,73,80,53,96,276,13,101,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The distribution of the relative rotations and translations over the dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The reconstruction of two sequences of lengths 50 and 300 respectively with our method in red compared with the ground truth sequences. (Color figure online)</figDesc><graphic coords="8,65,79,435,14,289,24,98,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The mean and standard deviation FDR and ADR of our method compared with state-of-the-art models MoNet<ref type="bibr" target="#b14">[15]</ref> and CNN<ref type="bibr" target="#b8">[9]</ref> </figDesc><table><row><cell>Models</cell><cell>FDR(%)</cell><cell>ADR(%)</cell></row><row><cell>CNN [9]</cell><cell cols="2">31.88 (15.76) 39.71 (14.88)</cell></row><row><cell cols="3">MoNet [15] 15.67 (8.37) 25.08 (9.34)</cell></row><row><cell>Ours</cell><cell cols="2">23.11 (11.6) 28.71 (12.97)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://pytorch.org/docs/stable/index.html.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transjugular intravascular ultrasound for the evaluation of hepatic vasculature and parenchyma in patients with chronic liver disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Gottardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadengue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Giostra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spahr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC. Res. Notes</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fluoroless intravascular ultrasound image-guided liver navigation in porcine models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Urade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gunzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pessaux</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12876-021-01600-3</idno>
		<ptr target="https://doi.org/10.1186/s12876-021-01600-3" />
	</analytic>
	<monogr>
		<title level="j">BMC Gastroenterol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994. 1994. 1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramidal implementation of the affine Lucas Kanade feature tracker description of the algorithm</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Corporation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: a machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A review of calibration techniques for freehand 3-D ultrasound systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lindseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultras. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="449" to="471" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on 3D ultrasound reconstruction techniques</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Siang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Appl. Med. Biol</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freehand 3-D ultrasound imaging: a systematic review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mozaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultras. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2099" to="2124" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for sensorless 3D freehand ultrasound imaging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sprung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ladikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-871" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="628" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-based 3D ultrasound reconstruction with optical flow via pyramid warping network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual International Conference on IEEE Engineering in Medicine &amp; Biology Society</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sensorless freehand 3D ultrasound reconstruction via deep contextual learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_44</idno>
		<idno>978-3-030-59716-0 44</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultrasound volume reconstruction from freehand scans without tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="970" to="979" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.04867v2</idno>
		<title level="m">Trackerless freehand ultrasound with sequence modelling and auxiliary transformation over past and future frames</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probe localization from ultrasound image sequences using deep learning for volume reconstruction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohmiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kondo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Forum on Medical Imaging in Asia</title>
		<imprint>
			<biblScope unit="page">117920O</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Proceedings of the SPIE 11792</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep motion network for freehand 3D ultrasound reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-828" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial position estimation method for 3D ultrasound reconstruction based on hybrid transfomers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting><address><addrLine>Kolkata, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PLUS: opensource toolkit for ultrasound-guided intervention systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heffter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ungi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2527" to="2537" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D slicer as an image computing platform for the quantitative imaging network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<idno type="PMID">22770690</idno>
		<idno type="PMCID">PMC3466397</idno>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1323" to="1341" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fluoroless intravascular ultrasound image-guided liver navigation in porcine models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Urade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Verde</surname></persName>
		</author>
		<author>
			<persName><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Gastroenterol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long-term oncologic outcomes after laparoscopic versus open resection for colorectal liver metastases: a randomized trial</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Aghayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Kazaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Dagenborg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="182" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Laparoscopic versus open resection for colorectal liver metastases: the OSLO-COMET randomized controlled trial</title>
		<author>
			<persName><forename type="first">Å</forename><forename type="middle">A</forename><surname>Fretland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Dagenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M W</forename><surname>Bjørnelv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="207" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Electromagnetic tracking in medicine a review of technology, validation, and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Haidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Birkfellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1702" to="1725" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
