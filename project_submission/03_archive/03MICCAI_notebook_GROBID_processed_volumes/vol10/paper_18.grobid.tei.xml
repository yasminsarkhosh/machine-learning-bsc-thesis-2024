<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms</title>
				<funder ref="#_4N2eWfe">
					<orgName type="full">Opening Foundation of Agile and Intelligent Computing Key Laboratory of Sichuan Province</orgName>
				</funder>
				<funder ref="#_RFgKWVz">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_nVbJmyZ #_pdshw4a">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pinxian</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Chengdu University of Information Technology</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiliu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Chengdu University of Information Technology</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<email>wangyanscu@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dinggang.shen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Research and Development</orgName>
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="184" to="194"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BD61F0D9AACDD8C84C013A16919C0F0B</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Positron Emission Tomography (PET)</term>
					<term>Triple-Domain</term>
					<term>Vision Transformer</term>
					<term>Global Frequency Parser</term>
					<term>Direct Reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To obtain high-quality positron emission tomography (PET) images while minimizing radiation exposure, various methods have been proposed for reconstructing standard-dose PET (SPET) images from low-dose PET (LPET) sinograms directly. However, current methods often neglect boundaries during sinogram-to-image reconstruction, resulting in high-frequency distortion in the frequency domain and diminished or fuzzy edges in the reconstructed images. Furthermore, the convolutional architectures, which are commonly used, lack the ability to model long-range non-local interactions, potentially leading to inaccurate representations of global structures. To alleviate these problems, in this paper, we propose a transformer-based model that unites triple domains of sinogram, image, and frequency for direct PET reconstruction, namely TriDo-Former. Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Different from the vanilla transformer that splits an image into 2D patches, based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from prorogating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As an in vivo nuclear medical imaging technique, positron emission tomography (PET) enables the visualization and quantification of molecular-level activity and has been extensively applied in hospitals for disease diagnosis and intervention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In clinic, to ensure that more diagnostic information can be retrieved from PET images, physicians prefer standard-dose PET scanning which is obtained by injecting standard-dose radioactive tracers into human bodies. However, the use of radioactive tracers inevitably induces potential radiation hazards. On the other hand, reducing the tracer dose during the PET scanning will introduce unintended noise, thus leading to degraded image quality with limited diagnostic information. To tackle this clinical dilemma, it is of high interest to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) data (i.e., sinograms or images).</p><p>In the past decade, deep learning has demonstrated its promising potential in the field of medical images <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Along the research direction of PET reconstruction, most efforts have been devoted to indirect reconstruction methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> which leverage the LPET images pre-reconstructed from the original projection data (i.e., LPET sinograms) as the starting point to estimate SPET images. For example, inspired by the preeminent performance of generative adversarial network (GAN) in computer vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, Wang et al. <ref type="bibr" target="#b8">[9]</ref> proposed a 3D conditional generative adversarial network (3D-cGAN) to convert LPET images to SPET images. However, beginning from the pre-reconstructed LPET images rather than the original LPET sinograms, these indirect methods may lose or blur details such as edges and small-size organs in the pre-reconstruction process, leading to unstable and compromised performance.</p><p>To remedy the above limitation, several studies focus on the more challenging direct reconstruction methods <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> which complete the reconstruction from the original sinogram domain (i.e., LPET sinograms) to the image domain (i.e., SPET images). Particularly, Haggstrom et al. <ref type="bibr" target="#b18">[19]</ref> proposed DeepPET, employing a convolutional neural network (CNN)-based encoder-decoder network to reconstruct SPET images from LPET sinograms. Although these direct methods achieve excellent performance, they still have the following limitations. First, due to the lack of consideration for the boundaries, the reconstruction from the sinogram domain to the image domain often leads to distortion of the reconstructed image in the high-frequency part of the frequency domain, which is manifested as blurred edges. Second, current networks ubiquitously employ CNNbased architecture which is limited in modeling long-range semantic dependencies in data. Lacking such non-local contextual information, the reconstructed images may suffer from missing or inaccurate global structure.</p><p>In this paper, to resolve the first limitation above, we propose to represent the reconstructed SPET images in the frequency domain, then encourage them to resemble the corresponding real SPET images in the high-frequency part. As for the second limitation, we draw inspiration from the remarkable progress of vision transformer <ref type="bibr" target="#b27">[28]</ref> in medical image analysis <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Owing to the intrinsic self-attention mechanism, the transformer can easily correlate distant regions within the data and capture non-local information. Hence, the transformer architecture is considered in our work.</p><p>Overall, we propose an end-to-end transformer model dubbed TriDo-Former that unites triple domains of sinogram, image, and frequency to directly reconstruct the clinically acceptable SPET images from LPET sinograms. Specifically, our TriDo-Former is comprised of two cascaded transformers, i.e., a sinogram enhancement transformer (SE-Former) and a spatial-spectral reconstruction transformer (SSR-Former). The SE-Former aims to predict denoised SPET-like sinograms from LPET sinograms, so as to prevent the noise in sinograms from propagating into the image domain. Given that each row of the sinogram is essentially the projection at a certain imaging views angle, dividing it into 2D patches and feeding them directly into the transformer will inevitably break the continuity of each projection view. Therefore, to retain the inner-structure of sinograms and filter the noise, we split a sinogram by rows and obtain a set of 1D sequences of different imaging view angles. Then, the relations between view angles are modeled via the self-attention mechanism in the SE-Former. Note that the SE-Former is designed specifically for the sinogram domain of LPET to effectively reduce noise based on the imaging mechanisms of PET. The denoised sinograms can serve as a better basis for the subsequent sinogram-to-image reconstruction. The SSR-Former is designed to reconstruct SPET images from the denoised sinograms. In pursuit of better image quality, we construct the SSR-Former by adopting the powerful swin transformer <ref type="bibr" target="#b30">[31]</ref> as the backbone. To compensate for the easily lost high-frequency details, we propose a global frequency parser (GFP) and inject it into the SSR-Former. The GFP acts as a learnable frequency filter to globally modify the components of specific frequencies of the frequency domain, forcing the network to learn accurate high-frequency details and produce construction results with shaper boundaries. Through the above triple-domain supervision, our TriDo-Former exhausts the model representation capability, thereby achieving better reconstructions.</p><p>The contributions of our proposed method can be described as follows. (1) To fully exploit the triple domains of sinogram, image, and frequency while capturing global context, we propose a novel triple-domain transformer to directly reconstruct SPET images from LPET sinograms. To our knowledge, we are the first to leverage both triple-domain knowledge and transformer for PET reconstruction. <ref type="bibr" target="#b1">(2)</ref> We develop a sinogram enhancement transformer (SE-Former) that is tailored for the sinogram domain of LPET to suppress the noise while maintaining the inner-structure, thereby preventing the noise in sinograms from propagating into the image domain during the sinogram-to-image reconstruction. (3) To reconstruct high-quality PET images with clear-cut details, we design a spatial-spectral transformer (SS-Former) incorporated with the global frequency parser (GFP) which globally calibrates the frequency components in the frequency domain for recovering high-frequency details. (4) Experimental results demonstrate the superiority of our method both qualitatively and quantitatively, compared with other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overall architecture of our proposed TriDo-Former is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of two cascaded sub-networks, i.e., a sinogram enhancement transformer (SE-Former) and a spatial-spectral reconstruction transformer (SSR-Former). Overall, taking the LPET sinograms as input, the SE-Former first predicts the denoised SPET-like sinograms which are then sent to SSR-Former to reconstruct the estimated PET (denoted as EPET) images. A detailed description is given in the following sub-sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sinogram Enhancement Transformer (SE-Former)</head><p>As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(a), the SE-Former which is responsible for denoising in the input LPET sinograms consists of three parts, i.e., a feature embedding module, transformer encoder (TransEncoder) blocks, and a feature mapping module. Given that each row of sinogram is the 1D projection at an imaging view angle, we first divide the LPET sinograms by rows and perform linear projection in the feature embedding module to obtain a set of 1D sequences, each contains consistent information of a certain view angle. Then, we perform self-attention in the TransEncoder blocks to model the interrelations between projection view angles, enabling the network to better model the general characteristics under different imaging views which is crucial for sinogram denoising. After that, the feature mapping module predicts the residual between the LPET and SPET sinograms which is finally added to the input LPET sinograms to generate the EPET sinograms as the output of SE-Former. We argue that the introduction of residual learning allows the SE-Former to focus only on learning the difference between LPET and SPET sinograms, facilitating faster convergence.</p><p>Feature Embedding: We denote the input LPET sinogram as S L ∈ R C s ×H s ×W s , where H s , W s are the height, width and C s is the channel dimension. As each row of sinogram is a projection view angle, the projection at the i-th (i = 1, 2, . . . H s ) row can be defined as TransEncoder: Following the standard transformer architecture <ref type="bibr" target="#b27">[28]</ref>, each TransEncoder block contains a multi-head self-attention (MSA) module and a feed forward network (FFN) respectively accompanied by layer normalization (LN). For j-th (j = 1, 2, . . . , T ) TransEncoder block, the calculation process can be formulated as:</p><formula xml:id="formula_0">s i L ∈ R C s ×W s .</formula><formula xml:id="formula_1">F j = F j-1 + MSA LN F j-1 + FFN (LN (F j-1 + MSA(LN (F j-1 )))), (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where F j denotes the output of j-th TransEncoder block. After applying T identical TransEncoder blocks, the non-local relationship between projections at different view angles is accurately preserved in the output sequence F T ∈ R H s ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Mapping:</head><p>The feature mapping module is designed for projecting the sequence data back to the sinogram. Concretely,</p><formula xml:id="formula_3">F T is first reshaped to R C ×H s ×W s (C = d W s</formula><p>) and then fed into a linear projection layer to reduce the channel dimension from C to C s . Through these operations, the residual sinogram S R ∈ R C s ×H s ×W s of the same dimension as S L , is obtained. Finally, following the spirit of residual learning, S R is directly added to the input S L to produce the output of SE-Former, i.e., the predicted denoised sinogram</p><formula xml:id="formula_4">S E ∈ R C s ×H s ×W s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial-Spectral Reconstruction Transformer (SSR-Former)</head><p>The SSR-Former is designed to reconstruct the denoised sinogram obtained from the SE-Former to the corresponding SPET images. As depicted in Fig. <ref type="figure" target="#fig_0">1</ref> (b), SSR-Former adopts a 4-level U-shaped structure, where each level is formed by a spatial-spectral transformer block (SSTB). Furthermore, each SSTB contains two spatial-spectral transformer layers (SSTLs) and a convolution layer for both global and local feature extraction. Meanwhile, a 3 × 3 convolution is placed as a projection layer at the beginning and the end of the network. For detailed reconstruction and invertibility of sampling, we employ the pixelunshuffle and pixel-shuffle operators for down-sampling and up-sampling. In addition, skip connections are applied for multi-level feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-Spectral Transformer Layer (SSTL):</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>, an SSTL consists of a window-based spatial multi-head self-attention (W-SMSA) followed by FFN and LN. Following swin transformer <ref type="bibr" target="#b30">[31]</ref>, a window shift operation is conducted between the two SSTLs in each SSTB for cross-window information interactions. Moreover, to capture the high-frequency details which can be easily lost, we devise global frequency parsers (GFPs) that encourage the model to recover the high-frequency component of the frequency domain through the global adjustment of specific frequencies. Generally, the W-SMSA is leveraged to guarantee the essential global context in the reconstructed PET images, while GFP is added to enrich the high-frequency boundary details. The calculations of the core W-SMSA and GFP are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Window-based Spatial Multi-Head Self-Attention (W-SMSA):</head><p>Denoting the input feature embedding of certain W-SMSA as e in ∈ R C I ×H I ×W I , where H I ,W I and C I represent the height, width and channel dimension, respectively. As depicted in Fig. <ref type="figure" target="#fig_0">1(c</ref>), a window partition operation is first conducted in spatial dimension with a window size of M . Thus, the whole input features are divided into N (N = H I ×W I M 2 ) non-overlapping patches e * in = {e m in } N m=1 . Then, a regular spatial self-attention is performed separately for each window after partition. After that, the output patches are gathered through the window reverse operation to obtain the spatial representative feature e spa ∈ R C I ×H I ×W I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Frequency Parser (GFP):</head><p>After passing the W-SMSA, the feature e spa are already spatially representative, but still lack accurate spectral representations in the frequency domain. Hence, we propose a GFP module to rectify the high-frequency component in the frequency domain. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(e), the GFP module is comprised of a 2D discrete Fourier transform (DFT), an element-wise multiplication between the frequency feature and the learnable global filter, and a 2D inverse discrete Fourier transform (IDFT). Our GFP can be regarded as a learnable version of frequency filters. The main idea is to learn a parameterized attentive map applying on the frequency domain features. Specifically, we first convert the spatial feature e spa to the frequency domain via 2D DFT, obtaining the spectral feature e spe = DFT (e spa ). Then, we modulate the frequency components of e spe by multiplying a learnable parameterized attentive map A ∈ R C I ×H I ×W I to e spe , which can be formulated as:</p><formula xml:id="formula_5">e spe = A • e spe ,<label>(2)</label></formula><p>The parameterized attentive map A can adaptively adjust the frequency components of the frequency domain and compel the network to restore the high-frequency part to resemble that of the supervised signal, i.e., the corresponding real SPET images (ground truth), in the training process. Finally, we reverse e spe back to the image domain by adopting 2D IDFT, thus obtaining the optimized feature e spa = DFT (e spe ). In this manner, more high-frequency details are preserved for generating shaper constructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Function</head><p>The objective function for our TriDo-Former is comprised of two aspects: 1) a sinogram domain loss L sino and 2) an image domain loss L img .</p><p>The sinogram domain loss aims to narrow the gap between the real SPET sinograms S S and the EPET sinograms S E that are denoised from the input LPET sinograms. Considering the critical influence of sinogram quality, we apply the L2 loss to increase the error punishment, thus forcing a more accurate prediction. It can be expressed as:</p><formula xml:id="formula_6">L sino = E S S ,S E ∼p data(S S ,S E ) ||S S -S E || 2 ,<label>(3)</label></formula><p>For the image domain loss, the L1 loss is leveraged to minimize the error between the SPET images I S and the EPET images I E while encouraging less blurring, which can be defined as:</p><formula xml:id="formula_7">L img = E I S ,I E ∼p data(I S ,I E ) ||I S -I E || 1 ,<label>(4)</label></formula><p>Overall, the final objective function is formulated by the weighted sum of the above losses, which is defined as:</p><formula xml:id="formula_8">L total = L sino + λL img . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where λ is the hyper-parameters to balance these two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Details of Implementation</head><p>Our network is implemented by Pytorch framework and trained on an NVIDIA GeForce GTX 3090 with 24 GB memory. The whole network is trained end-to-end for 150 epochs in total using Adam optimizer with the batch size of 4. The learning rate is initialized to 4e-4 for the first 50 epochs and decays linearly to 0 for the remaining 100 epochs.</p><p>The number T of the TransEncoder in SE-Former is set to 2 and the window size M is set to 4 in the W-SMSA of the SSR-Former. The weighting coefficient λ in Eq. ( <ref type="formula">6</ref>) is empirically set as 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets: We train and validate our proposed TriDo-Former on a real human brain dataset including 8 normal control (NC) subjects and 8 mild cognitive impairment (MCI) subjects. All PET scans are acquired by a Siemens Biograph mMR system housed in Biomedical Research Imaging Center. A standard dose of 18F-Flurodeoxyglucose ([ 18 F] FDG) was administered. According to standard protocol, SPET sinograms were acquired in a 12-minute period within 60-minute of radioactive tracer injection, while LPET sinograms were obtained consecutively in a 3-min shortened acquisition time to simulate the acquisition at a quarter of the standard dose. The SPET images which are utilized as the ground truth in this study were reconstructed from the corresponding SPET sinograms using the traditional OSEM algorithm <ref type="bibr" target="#b31">[32]</ref>.</p><p>Experimental Settings: Due to the limited computational resources, we slice each 3D scan of size 128 × 128 × 128 into 128 2D slices with a size of 128 × 128. The Leave-One-Out Cross-Validation (LOOCV) strategy is applied to enhance the stability of the model with limited samples. To evaluate the performance, we adopt three typical quantitative evaluation metrics including peak signal-to-noise (PSNR), structural similarity index (SSIM), and normalized mean squared error (NMSE). Note that, we restack the 2D slices into complete 3D PET scans for evaluation.</p><p>Comparative Experiments: We compare our TriDo-Former with four direct reconstruction methods, including (1) OSEM <ref type="bibr" target="#b31">[32]</ref> (applied on the input LPET sinograms, serving as the lower bound), (2) DeepPET <ref type="bibr" target="#b18">[19]</ref>, (3) Sino-cGAN <ref type="bibr" target="#b22">[23]</ref>, and (4) LCPR-Net <ref type="bibr" target="#b23">[24]</ref> as well as one indirect reconstruction methods, i.e., (5) 3D-cGAN <ref type="bibr" target="#b8">[9]</ref>. The comparison results are given in Table <ref type="table" target="#tab_1">1</ref>, from which we can see that our TriDo-Former achieves the best results among all the evaluation criteria. Compared with the current state-of-the-art LCPR-Net, our proposed method still enhances the PSNR and SSIM by 0.599 dB and 0.002 for NC subjects, and 0.681 dB and 0.002 for MCI subjects, respectively. Moreover, our model also has minimal parameters and GLOPs of 38 M and 16.05, respectively, demonstrating its speed and feasibility in clinical applications. We also visualize the results of our method and the compared approaches in Fig. <ref type="figure" target="#fig_1">2</ref>, where the differences in global structure are highlighted with circles and boxes while the differences in edge details are marked by arrows. As can be seen, compared with other methods which have inaccurate structure and diminished edges, our TriDo-Former yields the best visual effect with minimal error in both global structure and edge details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Clinical Diagnosis:</head><p>To further prove the clinical value of our method, we further conduct an Alzheimer's disease diagnosis experiment as the downstream task. Specifically, a multi-layer CNN is firstly trained by real SPET images to distinguish between NC and MCI subjects with 90% accuracy. Then, we evaluate the PET images reconstructed by different methods on the trained classification model. Our insight is that, if the model can discriminate between NC and MCI subjects from the reconstructed images more accurately, the quality of the reconstructed images and SPET images (whose quality is preferred in clinical diagnosis) are closer. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the classification accuracy of our proposed method (i.e., 88.6%) is the closest to that of SPET images (i.e., 90.0%), indicating the huge clinical potential of our method in facilitating disease diagnosis.</p><p>Ablation Study: To verify the effectiveness of the key components of our TriDo-Former, we conduct the ablation studies with the following variants: (1) replacing SE-Former and SSR-Former with DnCNN <ref type="bibr" target="#b32">[33]</ref> (the famous CNN-based denoising network) and vanilla U-Net (denoted as DnCNN + UNet), (2) replacing DnCNN with SE-Former (denoted as SE-Former + UNet), (3) replacing the U-Net with our SSR-Former but removing GFP (denoted as Proposed w/o GFP), and (4) using the proposed TriDo-Former model (denoted as Proposed). According to the results in Table <ref type="table" target="#tab_2">2</ref>, the performance of our model progressively improves with the introduction of SE-Former and SSR-Former. Particularly, when we remove the GFP in SSR-Former, the performance largely decreases as the model fails to recover high-frequency details. Moreover, we conduct the clinical diagnosis experiment and the spectrum analysis to further prove the effectiveness of the GFP, and the results are included in supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we innovatively propose a triple-domain transformer, named TriDo-Former, for directly reconstructing the high-quality PET images from LPET sinograms. Our model exploits the triple domains of sinogram, image, and frequency as well as the ability of the transformer in modeling long-range interactions, thus being able to reconstruct PET images with accurate global context and sufficient high-frequency details. Experimental results on the real human brain dataset have demonstrated the feasibility and superiority of our method, compared with the state-of-the-art PET reconstruction approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed TriDo-Former.</figDesc><graphic coords="4,76,47,172,76,299,68,106,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison of the reconstruction methods.</figDesc><graphic coords="8,103,98,295,91,244,96,80,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results of the clinical diagnosis of Alzheimer's disease (NC/MCI).</figDesc><graphic coords="9,109,29,56,48,205,12,56,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>H s ×d , where d is the output dimension of the projection. To maintain the position information of different view angles, we introduce a learnable position embedding S pos = {s i pos } H s ×d and fuse it with S * L by element-wise addition, thus creating the input feature embedding F 0 = S pos + S * L which is further sent to T stacked TransEncoder blocks to model global characteristics between view angles.</figDesc><table><row><cell>H s i=1 ∈ R</cell></row></table><note><p>Therefore, by splitting the sinogram by rows, we obtain a set of 1D sequence data S * L = {s i L } H s i=1 ∈ R H s ×D , where H s is the number of projection view angles and D = C s × W s equals to the pixel number in each sequence data. Then, S * L is linearly projected to sequence S * L ∈ R</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with five PET reconstruction methods in terms of PSNR, SSIM, and NMSE. The best performance is marked as bold.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>NC subject SSIM</cell><cell>NMSE</cell><cell>PSNR</cell><cell>MCI subject SSIM</cell><cell>NMSE</cell><cell cols="2">Params GFLOPs</cell></row><row><cell>OSEM [32]</cell><cell>20.684</cell><cell>0.979</cell><cell>0.0530</cell><cell>21.541</cell><cell>0.977</cell><cell>0.0580</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepPET [19]</cell><cell>23.991</cell><cell>0.982</cell><cell>0.0248</cell><cell>24.125</cell><cell>0.982</cell><cell>0.0272</cell><cell>60M</cell><cell>49.20</cell></row><row><cell>Sino-cGAN [23]</cell><cell>24.191</cell><cell>0.985</cell><cell>0.0254</cell><cell>24.224</cell><cell>0.985</cell><cell>0.0269</cell><cell>39M</cell><cell>19.32</cell></row><row><cell>LCPR-Net [24]</cell><cell>24.313</cell><cell>0.985</cell><cell>0.0227</cell><cell>24.607</cell><cell>0.985</cell><cell>0.0257</cell><cell>77M</cell><cell>77.26</cell></row><row><cell>3D-cGAN [9]</cell><cell>24.024</cell><cell>0.983</cell><cell>0.0231</cell><cell>24.617</cell><cell>0.981</cell><cell>0.0256</cell><cell>127M</cell><cell>70.38</cell></row><row><cell>Proposed</cell><cell>24.912</cell><cell>0.987</cell><cell>0.0203</cell><cell>25.288</cell><cell>0.987</cell><cell>0.0228</cell><cell>38M</cell><cell>16.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison with models constructed in the ablation study in terms of PSNR, SSIM, and NMSE.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell><cell>NC subjects SSIM</cell><cell>NMSE</cell><cell>PSNR</cell><cell>MCI subjects SSIM</cell><cell>NMSE</cell></row><row><cell>DnCNN + UNet</cell><cell>23.872</cell><cell>0.981</cell><cell>0.0253</cell><cell>24.153</cell><cell>0.982</cell><cell>0.0266</cell></row><row><cell>SE-Former + UNet</cell><cell>24.177</cell><cell>0.982</cell><cell>0.0249</cell><cell>24.506</cell><cell>0.982</cell><cell>0.0257</cell></row><row><cell>Proposed w/o GFP</cell><cell>24.583</cell><cell>0.984</cell><cell>0.0235</cell><cell>24.892</cell><cell>0.984</cell><cell>0.0250</cell></row><row><cell>Proposed</cell><cell>24.912</cell><cell>0.987</cell><cell>0.0203</cell><cell>25.288</cell><cell>0.987</cell><cell>0.0228</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NSFC 62071314</rs>), <rs type="programName">Sichuan Science and Technology Program</rs> <rs type="grantNumber">2023YFG0263</rs>, <rs type="grantNumber">2023NSFSC0497</rs>, <rs type="grantNumber">22YYJCYJ0086</rs>, and <rs type="funder">Opening Foundation of Agile and Intelligent Computing Key Laboratory of Sichuan Province</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RFgKWVz">
					<idno type="grant-number">NSFC 62071314</idno>
					<orgName type="program" subtype="full">Sichuan Science and Technology Program</orgName>
				</org>
				<org type="funding" xml:id="_nVbJmyZ">
					<idno type="grant-number">2023YFG0263</idno>
				</org>
				<org type="funding" xml:id="_pdshw4a">
					<idno type="grant-number">2023NSFSC0497</idno>
				</org>
				<org type="funding" xml:id="_4N2eWfe">
					<idno type="grant-number">22YYJCYJ0086</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clinical applications of PET in brain tumors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1468" to="1481" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised tripled dictionary learning for standard-dose PET image prediction using low-dose PET and multimodal MRI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="579" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hi-net: hybrid-fusion network for multi-modal MR image synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2772" to="2781" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-scale transformer network with edge-aware pre-training for cross-modality MR image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tripled-uncertainty guided mean teacher model for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_42" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="450" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-constraint generative adversarial network for dose prediction in radiotherapy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102339</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting standard-dose PET image from low-dose PET and multimodal MR images using mapping-based sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="791" to="812" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Full-count PET recovery from low-count image using a dilated convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Spuhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Serrano-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4928" to="4938" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D conditional generative adversarial networks for highquality PET image estimation at low dose</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="550" to="562" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">auto-context-based locality adaptive multi-modality GANs for PET synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Locality adaptive multi-modality GANs for high-quality PET image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="329" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D Transformer-GAN for high-quality PET reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive rectification based adversarial network with spectrum constraint for high-quality PET image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102335</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification-aided high-quality PET image synthesis via bidirectional contrastive GAN with shared information maximization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="527" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D CVT-GAN: a 3D convolutional vision transformer-GAN for PET reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="516" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction of standard-dose PET from low-dose PET via dual-frequency supervision and global aggregation module</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the19th International Symposium on Biomedical Imaging Conference</title>
		<meeting>the19th International Symposium on Biomedical Imaging Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CT-only radiotherapy: an exploratory study for automatic dose prediction on rectal cancer patients via deep adversarial network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">875661</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explainable attention guided adversarial deep network for 3D radiotherapy dose distribution prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page">108324</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepPET: A deep encoder-decoder network for directly solving the PET image reconstruction inverse problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Häggström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Schmidtlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="253" to="262" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FBP-Net for direct reconstruction of dynamic PET images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">235008</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An encoder-decoder network for direct image reconstruction on sinograms of a long axial field of view PET</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="4464" to="4477" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DirectPET: full-size neural network PET reconstruction from sinogram data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Whiteley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32503</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">H: Deep-learning-based framework for PET image reconstruction from sinogram domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">8118</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LCPR-Net: low-count PET image reconstruction using the domain transform and cycle-consistent generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">749</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking PET image reconstruction: ultra-low-dose, sinogram and deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning based framework for direct reconstruction of PET images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_6" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TransEM: Residual swin-transformer based regularized PET image reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Con-ference on Computer Vision</title>
		<meeting>the IEEE/CVF International Con-ference on Computer Vision<address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TransCT: dual-path transformer for low dose computed tomography</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-transSP: Multimodal transformer for survival prediction of nasopharyngeal carcinoma patients</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accelerated image reconstruction using ordered subsets of projection data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="601" to="609" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
