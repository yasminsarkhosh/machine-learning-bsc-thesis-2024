<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Matteo</forename><surname>Ronchetti</surname></persName>
							<email>ronchetti@imfusion.com</email>
							<affiliation key="aff0">
								<orgName type="department">ImFusion GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wolfgang</forename><surname>Wein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ImFusion GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures (CAMP)</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oliver</forename><surname>Zettinig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ImFusion GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Prevost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ImFusion GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="761" to="770"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4E3BC3F1AC64503F2A23C08C4126CD62</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_72</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Registration</term>
					<term>Multimodal</term>
					<term>Metric Learning</term>
					<term>Differentiable</term>
					<term>Deformable Registration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal imaging has become increasingly popular in healthcare due to its ability to provide complementary anatomical and functional information. However, to fully exploit its benefits, it is crucial to perform accurate and robust registration of images acquired from different modalities. Multimodal image registration is a challenging task due to differences in image appearance, acquisition protocols, and physical properties of the modalities. This holds in particular if ultrasound (US) is involved, and has not been satisfactorily solved so far.</p><p>While simple similarity measures directly based on the images' intensities such as sum of absolute (L1) or squared (L2) differences and normalized crosscorrelation (NCC) <ref type="bibr" target="#b16">[16]</ref> work well in monomodal settings, a more sophisticated approach is needed when intensities cannot be directly correlated. Historically, a breakthrough in CT-MRI registration was achieved by Viola and Wells, who proposed Mutual Information <ref type="bibr" target="#b19">[19]</ref>. Essentially, it abstracts the problem to the statistical concept of information theory and optimizes image-wide alignment statistics. Broken down to patch level and inspired by ultrasound physics, the Linear Correlation of Linear Combination (LC 2 ) measure has shown to work well for US to MRI or CT registration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22]</ref>. While dealing well with US specifics, it is not differentiable and expensive to compute.</p><p>As an alternative to directly assessing similarity on the original images, various groups have proposed to first compute intermediate representations, and then align these with conventional L1 or L2 metrics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">20]</ref>. A prominent example is the Modality-Independent Neighbourhood Descriptor (MIND) <ref type="bibr" target="#b4">[5]</ref>, which is based on image self-similarity and has with minor adaptations (denoted MIND-SSC for self-similarity context) also been applied to US problems <ref type="bibr" target="#b6">[7]</ref>. Most recently, it has been shown that using 2D confidence maps-based weighting and adaptive normalization may further improve registration accuracy <ref type="bibr" target="#b21">[21]</ref>. Yet, such feature descriptors are not expressive enough to cope with complex US artifacts and exhibit many local optima, therefore requiring closer initialization.</p><p>More recently, multimodal registration has been approached using various Machine Learning (ML) techniques. Some of these methods involve the utilization of Convolutional Neural Networks (CNN) to extract segmentation volumes from the source data, transforming the problem into the registration of label maps <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24]</ref>. Although these methods have demonstrated promising results, they are anatomy-specific and require the identification and labeling of structures that are visible in both modalities. Other approaches are trained using ground truth registrations to directly predict the pose <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">12]</ref> or to establish keypoint correspondences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. However, these methods are not generalizable to different anatomies or modalities. Moreover, the paucity of precise and unambiguous ground truth registration, particularly in abdominal MR-US registration, exacerbates the overfitting problem, restricting generalization even within the same modality and anatomy. It has furthermore been proposed in the past to utilize CNNs as a replacement for a similarly metric. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">17]</ref>, the two images being registered are resampled into the same grid in each optimizer iteration, concatenated and fed into a network for similarity evaluation. While such a measure can directly be integrated into existing registration methods, it still suffers from similar limitations in terms of runtime performance and modality dependance.</p><p>In contrast, we propose in this work to use a small CNN to approximate an expensive similarity metric with a straightforward dot product in its feature space. Crucially, our method does not necessitate to evaluate the CNN at every optimizer iteration. This approach combines ML and classical multimodal image registration techniques in a novel way, avoiding the common limitations of ML approaches: ground truth registration is not required, it is differentiable and computationally efficient, and generalizes well across anatomies and imaging modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We formulate image registration as an optimization problem of a similarity metric s between the moving image M and the fixed image F with respect to the parameters α of a spatial transformation T α : Ω → Ω. Most multi-modal similarity metrics are defined as weighted sums of local similarities computed on patches. Denoting M • T α the deformed image, the optimization target can be expressed in the following way:</p><formula xml:id="formula_0">f (α) = p∈Ω w(p) s(F [p], M • T α [p]),<label>(1)</label></formula><p>where w(p) is the weight assigned to the point p, s(•, •) defines a local similarity and the [•] operator extracts a patch (or a pixel) at a given spatial location. This definition encompasses SSD but also other more elaborate metrics like LC 2 or MIND. The function w is typically used to reduce the impact of patches with ambiguous content (e.g. with uniform intensities), or can be chosen to encode prior information on the target application.</p><p>The core idea of our method is to approximate the similarity metric s(P 1 , P 2 ) of two image patches with a dot product φ(P 1 ), φ(P 2 ) where φ(•) is a function that extracts a feature vector, for instance in R 16 , from its input patch. When φ is a fully convolutional neural network (CNN), we can simply feed it the entire volume in order to pre-compute the feature vectors of every voxel with a single forward pass. The registration objective (Eq. 1) is then approximated as</p><formula xml:id="formula_1">f (α) ≈ p∈Ω w(p) φ(F )[p], φ(M ) • T α [p] ,<label>(2)</label></formula><p>thus converting the original problem into a registration of pre-computed feature maps using a simple and differentiable dot product similarity. This approximation is based on the assumption that the CNN is approximately equivariant to the transformation, i.e.</p><formula xml:id="formula_2">φ(M • T α )[p] ≈ φ(M ) • T α [p].</formula><p>Our experiments show that this assumption (implicitly made also by other descriptors like MIND) does not present any practical impediment. Our method exhibits a large capture range and can converge over a wide range of rotations and deformations.</p><p>Advantages. In contrast to many existing methods, our approach doesn't require any ground truth registration and can be trained using patches from unregistered pairs of images. This is particularly important for multi-modal deformable registration as ground truths are harder to define, especially on ultrasound. The simplicity of our training objective allows the use of a CNN with a limited number of parameters and a small receptive field. This means that the CNN has a negligible computational cost and can generalize well across anatomies and modalities: a single network can be used for all types of images and does not need to be retrained for a new task. Furthermore, the objective function (Eq. 2) can be easily differentiated without backpropagating the gradient through the CNN. This permits efficient gradient-based optimization, even when the original metric is either non-differentiable or costly to differentiate. Finally, we quantize the feature vectors to 8-bit precision further increasing the computational speed of registration without impacting accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We train our model to approximate the three-dimensional LC 2 similarity, as it showed good performance on a number of tasks, including ultrasound <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22]</ref>. The LC 2 similarity quantifies whether a target patch can be approximated by a linear combination of the intensities and the gradient magnitude of the source patch. In order to reduce the sensitivity on the scale, our target is actually the average LC 2 over different radiuses of 3, 5, and 7. In order to be consistent with the original implementation of LC 2 we use the same weighting function w based on local patch variance. Note that the network will be trained only once, on a fixed dataset that is fully independent of the datasets that will be used in the evaluation (see Sect. 4).</p><p>Dataset. Our neural network is trained using patches from the "Gold Atlas -Male Pelvis -Gentle Radiotherapy" <ref type="bibr" target="#b14">[14]</ref> dataset, which is comprised of 18 patients each with a CT, MR T1, and MR T2 volumes. We resample each volume to a spacing of 2 mm and normalize the voxel intensities to have zero mean and standard variation of one. Since our approach is unsupervised, we don't make use of the provided registration but leave the volumes in their standard DICOM orientation. As LC 2 requires the usage of gradient magnitude in one of the modalities, we randomly pick it from either CT or MR. We would like to report that, initially, we also made use of a proprietary dataset including US volumes. However, as our investigation progressed, we observed that the incorporation of US data did not significantly contribute to the generalization capabilities of our model. Consequently, for the purpose of ensuring reproducibility, all evaluations presented in this paper exclusively pertain to the model trained solely on the public MR-CT dataset.</p><p>Patch Sampling from Unregistered Datasets. For each pair of volumes (M, F ) we repeat the following procedure 5000 times: (1) Select a patch from M with probability proportional to its weight w; (2) Compute the similarity with all the patches of F ; (3) Uniformly sample t ∈ [0, 1]; (4) Pick the patch of F with similarity score closest to t. Running this procedure on our training data results in a total of 510000 pairs of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture and Training.</head><p>We use the same feed-forward 3D CNN to process all data modalities. The proposed model is composed of residual blocks <ref type="bibr" target="#b3">[4]</ref>, LeakyReLU activations <ref type="bibr" target="#b9">[10]</ref> and uses BlurPool <ref type="bibr" target="#b25">[25]</ref> for downsampling, resulting in a total striding factor of 4. We do not use any normalization layer, as this resulted in a reduction in performance. The output of the model is 16-channels volume with the norm of each voxel descriptor clipped at 1. The architecture consists of ten layers and a total of 90,752 parameters, making it notably smaller than many commonly utilized neural networks.</p><p>Augmentation on the training data is used to make the model as robust as possible while leaving the target similarity unchanged. In particular, we apply the same random rotation to both patches, randomly change the sign and apply random linear transformation on the intensity values. We train our model for 35 epochs using the L2 loss and batch size of 256. The training converges to an average patch-wise L2 error of 0.0076 on the training set and 0.0083 on the validation set. The total training time on an NVIDIA RTX4090 GPU is 5 h, and inference on a 256 3 volume takes 70 ms. We make the training code and preprocessed data openly available online<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We present an evaluation of our approach across tasks involving diverse modalities and anatomies. Notably, the experimental data utilized in our analysis differs significantly from our model's training data in terms of both anatomical structures and combination of modalities. To assess the effectiveness of our method, we compare it against LC 2 , which is the metric we approximate, and MIND-SSC <ref type="bibr" target="#b6">[7]</ref>. In all experiments, we use a Wilcoxon signed-rank test with p-value 10 -2 to establish the significance of our results.</p><p>As will be demonstrated in the next subsections, our method is capable of achieving comparable levels of accuracy as LC 2 while retaining the speed and flexibility of MIND-SSC. In particular, on abdominal US registration (Sect. 4.3) our method obtains a significantly larger capture range, opening new possibilities for tackling this challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Affine Registration of Brain US-MR</head><p>In this experiment, we evaluate the performance of different methods for estimating affine registration of the REtroSpective Evaluation of Cerebral Tumors (RESECT) MICCAI challenge dataset <ref type="bibr" target="#b23">[23]</ref>. This dataset consists of 22 pairs of pre-operative brain MRs and intra-operative ultrasound volumes. The initial pose of the ultrasound volumes exhibits an orientation close to the ground truth but can contain a significant translation shift. For both MIND-SSC and DISA-LC 2 , we resample the input volumes to 0.4 mm spacing and use the BFGS <ref type="bibr" target="#b18">[18]</ref> optimizer with 500 random initializations within a range of ±10 • and ±25 mm. We report the obtained Fiducial Registration Errors (FRE) in Table <ref type="table" target="#tab_0">1</ref>. DISA-LC 2 is significantly better than MIND-SSC while the difference with LC 2 is not significant. In conclusion, our experiments demonstrate that the proposed DISA-LC 2 , combined with a simple optimization strategy, is capable of achieving equivalent performance to manually tuned LC 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deformable Registration of Abdominal MR-CT</head><p>Our second application is the Abdomen MR-CT task of the Learn2Reg challenge 2021 <ref type="bibr" target="#b7">[8]</ref>. The dataset comprises 8 sets of MR and CT volumes, both depicting the abdominal region of a single patient and exhibiting notable deformations. We estimate dense deformation fields using the methodology outlined in <ref type="bibr" target="#b5">[6]</ref> (without inverse consistency) which first estimates a discrete displacement using explicit search and then iteratively enforces global smoothness. Segmentation maps of anatomical structures are used to measure the quality of the registration. In particular, we compute the 25th, 50th, and 75th quantile of the Dice Similarity Coefficient (DSC) and the 95th quantile of the Hausdorff distance (HD95) between the registered label maps. We compare MIND-SCC and DISA-LC 2 used with different strides and followed by a downsampling operation that brings the spacing of the descriptors volumes to 8 mm. The hyperparameters of the registration algorithm have been manually optimized for each approach. Table <ref type="table" target="#tab_1">2</ref> shows that our method obtains significantly better results than MIND-SCC on the DSC metrics while being not significantly better on HD95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deformable Registration of Abdominal US-CT and US-MR</head><p>As the most challenging experiment, we finally use our method to achieve deformable registration of abdominal 3D freehand US to a CT or MR volume.</p><p>We are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical vs. electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of the liver. All 3D ultrasound data sets are accurately calibrated, with overall system errors in the range of commercial ultrasound fusion options. Between 4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder, kidney) were manually annotated by an expert. In order to measure the capture range, we start the registration from 50 random rigid poses around the ground truth and calculate the Fiducial Registration Error (FRE) after optimization. For local optimization, LC 2 is used in conjunction with BOBYQA <ref type="bibr" target="#b15">[15]</ref> as in the original paper <ref type="bibr" target="#b22">[22]</ref>, while MIND-SCC and DISA-LC 2 are instead used with BFGS. Due to an excessive computation time, we don't do global optimization with LC 2 while with other methods we use BFGS with 500 random initializations within a range of ±40 • and ±150 mm. We use six parameters to define the rigid pose and two parameters to describe the deformation caused by the ultrasound probe pressure.</p><p>From the results shown in Table <ref type="table" target="#tab_2">3</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, it can be noticed that the proposed method obtains a significantly larger capture range than MIND-SCC and LC 2 while being more than 300 times faster per evaluation than LC 2 (the times reported in the table include not just the optimization but also descriptor extraction). The differentiability of our objective function allows our method to converge in fewer iterations than derivative-free methods like BOBYQA. Furthermore, the evaluation speed of our objective function allows us to exhaustively search the solution space, escaping local minima and converging to the correct solution with pose and deformation parameters at once, in less than two seconds.</p><p>Note that this registration problem is much more challenging than the prior two due to difficult ultrasonic visibility in the abdomen, strong deformations, and ambiguous matches of liver vasculature. Therefore, to the best of our knowledge, these results present a significant leap towards reliable and fully automatic fusion, doing away with cumbersome manual landmark placements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have discovered that a complex patch-based similarity metric can be approximated with feature vectors from a CNN with particularly small architecture, using the same model for any modality. The training is unsupervised and merely requires unregistered data. After features are extracted from the volumes, the actual registration comprises a simple iterative dot-product computation, allowing for global and derivative-based optimization. This novel combination of classical image processing and machine learning elevates multi-modal registration to a new level of performance, generality, but also algorithm simplicity.</p><p>We demonstrate the efficiency of our method on three different use cases with increasing complexity. In the most challenging scenario, it is possible to perform global optimization within seconds of both pose and deformation parameters, without any organ-specific distinction or successive increase of parameter sizes.</p><p>While we specifically focused on developing an unsupervised and generic method, a sensible extension would be to specialize our method by including global information, such as segmentation maps, into the approximated measure or by making use of ground-truth registration during training. Finally, the cross-modality feature descriptors produced by our model could be exploited by future research for tasks different from registration such as modality synthesis or segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Similarity maps across different modalities and anatomies. Each heatmap shows the similarity of the marked point on the source image to every point in the target image. Our method (DISA-LC 2 ) approximates LC 2 well in a fraction of the computation time and produces less ambiguous heatmaps than MIND.</figDesc><graphic coords="4,59,79,64,85,299,50,161,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Boxplot of fiducial registration errors for the different methods on deformable registration of abdominal US-CT and US-MR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on registration of brain US-MR data from the RESECT Challenge. FRE is the average of fiducial errors in millimeters across all cases, while FRE25, FRE50, and FRE75 refer to the 25th, 50th, and 75th percentiles.</figDesc><table><row><cell>Method</cell><cell cols="4">Mode Avg. FRE FRE25 FRE50 FRE75</cell></row><row><cell cols="2">MIND-SSC Rigid 5.05</cell><cell>1.69</cell><cell>2.20</cell><cell>3.31</cell></row><row><cell cols="2">MIND-SSC Affine 2.01</cell><cell>1.44</cell><cell>1.84</cell><cell>2.29</cell></row><row><cell>LC 2</cell><cell>Rigid 1.71</cell><cell>1.31</cell><cell>1.56</cell><cell>1.72</cell></row><row><cell>LC 2</cell><cell>Affine 1.73</cell><cell>1.32</cell><cell>1.67</cell><cell>1.89</cell></row><row><cell cols="2">DISA-LC 2 Rigid 1.82</cell><cell>1.37</cell><cell>1.65</cell><cell>1.80</cell></row><row><cell cols="2">DISA-LC 2 Affine 1.74</cell><cell>1.33</cell><cell>1.58</cell><cell>1.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on the Abdomen MR-CT task of the Learn2Reg challenge 2021. The best results and the ones not significantly different from them are in bold.</figDesc><table><row><cell>Method</cell><cell cols="2">Stride DSC25 DSC50 DSC75 HD95</cell></row><row><cell cols="2">MIND-SSC 4</cell><cell>42.3% 70.9% 84.9% 26.4 mm</cell></row><row><cell cols="2">MIND-SSC 2</cell><cell>49.8% 70.9% 84.9% 24.8 mm</cell></row><row><cell cols="2">MIND-SSC 1</cell><cell>48.8% 70.9% 84.9% 24.5 mm</cell></row><row><cell cols="2">DISA-LC 2 4</cell><cell>61.4% 72.7% 85.2% 23.6 mm</cell></row><row><cell cols="2">DISA-LC 2 2</cell><cell>61.5% 73.2% 85.5% 22.8 mm</cell></row><row><cell cols="2">DISA-LC 2 1</cell><cell>61.5% 74.0% 85.5% 22.6 mm</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on deformable registration of abdominal US-CT and US-MR. A case is considered "converged" if the FRE after registration is less than 15 mm. The best results and the ones not significantly different from them are highlighted in bold. (*)Time and evaluations for Global LC 2 are estimated by extrapolation.</figDesc><table><row><cell>Similarity</cell><cell cols="4">Search Converged cases w.r.t. initialization error</cell><cell cols="2">Time (s) Num. eval.</cell></row><row><cell></cell><cell cols="4">0-25 mm 25-50 mm 50-75 mm 75-100 mm</cell><cell></cell><cell></cell></row><row><cell cols="2">MIND-SSC Local 23.6%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.4</cell><cell>17</cell></row><row><cell>LC 2</cell><cell>Local 54.1%</cell><cell>14.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>1.9</cell><cell>98</cell></row><row><cell cols="2">DISA-LC 2 Local 70.3%</cell><cell>52.0%</cell><cell>21.1%</cell><cell>5.8%</cell><cell>0.9</cell><cell>70</cell></row><row><cell cols="2">MIND-SSC Global 17.9%</cell><cell>14.6%</cell><cell>5.3%</cell><cell>12.0%</cell><cell>1.3</cell><cell>26370</cell></row><row><cell>LC 2</cell><cell>Global</cell><cell></cell><cell>N/A</cell><cell></cell><cell>948.0*</cell><cell>38740*</cell></row><row><cell cols="2">DISA-LC 2 Global 75.5%</cell><cell>73.2%</cell><cell>65.0%</cell><cell>64.0%</cell><cell>1.8</cell><cell>29250</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/ImFusionGmbH/DISA-universal-multimodal-registration.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 72.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards fully automatic X-ray to CT registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-770" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic ultrasound-MRI registration for neurosurgery using the 2D and 3D LC2 metric</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fuerst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1312" to="1319" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep similarity metric for 3D MR-TRUS image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Haskins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mind: modality independent neighbourhood descriptor for multi-modal deformable registration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-parametric discrete registration with convex optimisation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Papież</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Handels</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-08554-8_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-08554-86" />
	</analytic>
	<monogr>
		<title level="m">WBIR 2014</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8545</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards realtime multimodal fusion for image-guided interventions using self-similarities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Papież</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40811-3_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40811-324" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2013</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8149</biblScope>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learn2reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="697" to="712" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orientation estimation of abdominal ultrasound images with multi-hypotheses networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Horstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zettinig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global multimodal 2D/3D registration via local descriptors learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Markova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zettinig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="269" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-026" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards multi-modal self-supervised video and ultrasound pose estimation for laparoscopic liver surgery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Montaña-Brown</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16902-1_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16902-118" />
	</analytic>
	<monogr>
		<title level="m">ASMUS 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Aylward</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Baum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Min</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13565</biblScope>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deriving anatomical context from 4D ultrasound</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Biannual Eurographics Workshop on Visual Computing for Biology and Medicine</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Nyholm</surname></persName>
		</author>
		<title level="m">Gold atlas -male pelvis -gentle radiotherapy</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Bobyqa algorithm for bound constrained optimization without derivatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
		<idno>NA2009/06</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">26</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Cambridge NA Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unifying maximum likelihood approaches in medical image registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sedghi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01565</idno>
		<title level="m">Semi-supervised deep metrics for image registration</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Limited memory BFGS for nonsmooth optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skajaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Courant Institute of Mathematical Science, New York University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Entropy and Laplacian images: structural representations for multi-modal registration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal registration of ultrasound and MR images using weighted self-similarity structure vector</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">106661</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic CTultrasound registration for diagnostic imaging and image-guided intervention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khamene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Callstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="585" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retrospective evaluation of cerebral tumors (resect): a clinical database of pre-operative MRI and intra-operative ultrasound in low-grade glioma surgeries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Unsgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reinertsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3875" to="3882" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning-based US-MR liver image registration with spatial priors</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_17</idno>
		<idno>978-3-031-16446-0 17</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
