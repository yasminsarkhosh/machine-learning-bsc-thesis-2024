<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images</title>
				<funder ref="#_F6hj3FD">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Jarvis Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Jarvis Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<addrLine>Shaan&apos;xi</addrLine>
									<settlement>Xi&apos;an</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wei</surname></persName>
							<email>donwei@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Jarvis Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuexiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Jarvis Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<email>yefengzheng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Jarvis Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="109" to="120"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BCE89C3B9E39F906B151BE3DCB1F4EFD</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sparse-view reconstruction</term>
					<term>Metal artifact reduction</term>
					<term>Rotation equivariance</term>
					<term>Proximal network</term>
					<term>Generalization capability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse-view computed tomography (CT) has been adopted as an important technique for speeding up data acquisition and decreasing radiation dose. However, due to the lack of sufficient projection data, the reconstructed CT images often present severe artifacts, which will be further amplified when patients carry metallic implants. For this joint sparse-view reconstruction and metal artifact reduction task, most of the existing methods are generally confronted with two main limitations: 1) They are almost built based on common network modules without fully embedding the physical imaging geometry constraint of this specific task into the dual-domain learning; 2) Some important prior knowledge is not deeply explored and sufficiently utilized. Against these issues, we specifically construct a dual-domain reconstruction model and propose a model-driven equivariant proximal network, called MEPNet. The main characteristics of MEPNet are: 1) It is optimization-inspired and has a clear working mechanism; 2) The involved proximal operator is modeled via a rotation equivariant convolutional neural network, which finely represents the inherent rotational prior underlying the CT scanning that the same organ can be imaged at different angles. Extensive experiments conducted on several datasets comprehensively substantiate that compared with the conventional convolution-based proximal network, such a rotation equivariance mechanism enables our proposed method to achieve better reconstruction performance with fewer network parameters. We will release the code at https://github.com/hongwang01/MEPNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computed tomography (CT) has been widely adopted in clinical applications. To reduce the radiation dose and shorten scanning time, sparse-view CT has drawn much attention in the community <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>. However, sparse data sampling inevitably degenerates the quality of CT images and leads to adverse artifacts. In addition, when patients carry metallic implants, such as hip prostheses and spinal implants <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>, the artifacts will be further aggravated due to beam hardening and photon starvation. For the joint sparse-view reconstruction and metal artifact reduction task (SVMAR), how to design an effective method for artifact removal and detail recovery is worthy of in-depth exploration.</p><p>For the sparse-view (SV) reconstruction, the existing deep-learning (DL)based methods can be roughly divided into three categories based on the information domain exploited, e.g., sinogram domain, image domain, and dual domains. Specifically, for the sinogram-domain methods, sparse-view sinograms are firstly repaired based on deep networks, such as U-Net <ref type="bibr" target="#b9">[10]</ref> and dense spatial-channel attention network <ref type="bibr" target="#b36">[37]</ref>, and then artifact-reduced CT images are reconstructed via the filtered-back-projection (FBP) process. For the image-domain methods, researchers have proposed to learn the clean CT images from degraded ones via various structures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Alternatively, both sinogram and CT images are jointly exploited for the dual reconstruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>For the metal artifact reduction (MAR) task, similarly, the current DL-based approaches can also be categorized into three types. To be specific, sinogramdomain methods aim to correct the sinogram for the subsequent CT image reconstruction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>. Image-domain-based works have proposed different frameworks, such as simple residual network <ref type="bibr" target="#b7">[8]</ref> and an interpretable structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, to learn artifact-reduced images from metal-affected ones. The dual-domain methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref> focus on the mutual learning between sinogram and CT image.</p><p>Albeit achieving promising performance, these aforementioned methods are sub-optimal for the SVMAR task. The main reasons are: 1) Most of them do not consider the joint influence of sparse data sampling and MAR, and do not fully embed the physical imaging constraint between the sinogram domain and CT image domain under the SVMAR scenario; 2) Although a few works focus on the joint SVMAR task, such as <ref type="bibr" target="#b35">[36]</ref>, the network structure is empirically built based on off-the-shelf modules, e.g., U-Net and gated recurrent units, and it does not fully investigate and embed some important prior information underlying the CT imaging procedure. However, for such a highly ill-posed restoration problem, the introduction of the proper prior is important and valuable for constraining the network learning and helping it evolve in a right direction <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-⋃ ⋃</head><p>Fig. <ref type="figure">1</ref>. Illustration of the elements in the model Eq. ( <ref type="formula" target="#formula_2">2</ref>) for easy understanding.</p><p>To alleviate these issues, in this paper, we propose a model-driven equivariant proximal network, called MEPNet, which is naturally constructed based on the CT imaging geometry constraint for this specific SVMAR task, and takes into account the inherent prior structure underlying the CT scanning procedure. Concretely, we first propose a dual-domain reconstruction model and then correspondingly construct an unrolling network framework based on a derived optimization algorithm. Furthermore, motivated by the fact that the same organ can be imaged at different angles making the reconstruction task equivariant to rotation <ref type="bibr" target="#b1">[2]</ref>, we carefully formulate the proximal operator of the built unrolling neural network as a rotation-equivariant convolutional neural network (CNN). Compared with the standard-CNN-based proximal network with only translationequivariance property <ref type="bibr" target="#b2">[3]</ref>, our proposed method effectively encodes more prior knowledge, e.g., rotation equivariance, possessed by this specific task. With such more accurate regularization, our proposed MEPNet can achieve higher fidelity of anatomical structures and has better generalization capability with fewer network parameters. This is finely verified by comprehensive experiments on several datasets of different body sites. To the best of our knowledge, we should be the first to study rotation equivariance in the context of SVMAR and validate its utility, which is expected to make insightful impacts on the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary Knowledge About Equivariance</head><p>Equivariance of a mapping w.r.t. a certain transformation indicates that executing the transformation on the input produces a corresponding transformation on the output <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. Mathematically, given a group of transformations G, a mapping Φ from the input feature space to the output feature space is said to be group equivariant about G if</p><formula xml:id="formula_0">Φ T g f = T g Φ f , ∀g ∈ G, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where f is any input feature map in the input feature space; T g and T g represent the actions of g on the input and output, respectively. The prior work <ref type="bibr" target="#b2">[3]</ref> has shown that adopting group equivariant CNNs to encode symmetries into networks would bring data efficiency and it can constrain the network learning for better generalization. For example, compared with the fullyconnected layer, the translational equivariance property enforces weight sharing for the conventional CNN, which makes CNN use fewer parameters to preserve the representation capacity and then obtain better generalization ability. Recently, different types of equivariant CNNs have been designed to preserve more symmetries beyond current CNNs, such as rotation symmetry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref> and scale symmetry <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. However, most of these methods do not consider specific designs for the SVMAR reconstruction. In this paper, we aim to build a physics-driven network for the SVMAR task where rotation equivariance is encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual-Domain Reconstruction Model for SVMAR</head><p>In this section, for the SVMAR task, we derive the corresponding dual domain reconstruction model and give an iterative algorithm for solving it. Dual-Domain Reconstruction Model. Given the captured sparse-view metal-affected sinogram Y svma ∈ R N b ×Np , where N b and N p are the number of detector bins and projection views, respectively, to guarantee the data consistency between the reconstructed clean CT image X ∈ R H×W and the observed sinogram Y svma , we can formulate the corresponding optimization model as <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_2">min X (1 -T r ∪ D) (PX -Y svma ) 2 F + μR(X), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where D ∈ R N b ×Np is the binary sparse downsampling matrix with 1 indicating the missing region; T r ∈ R N b ×Np is the binary metal trace with 1 indicating the metal-affected region; P is forward projection; R(•) is a regularization function for capturing the prior of X; ∪ is the union set; is the point-wise multiplication; H and W are the height and width of CT images, respectively; μ is a trade-off parameter. One can refer to Fig. <ref type="figure">1</ref> for easy understanding.</p><p>To jointly reconstruct sinogram and CT image, we introduce the dual regularizers R 1 (•) and R 2 (•), and further derive Eq. (2) as:</p><formula xml:id="formula_4">min S,X PX -S 2 F +λ (1 -T r ∪ D) (S -Y svma ) 2 F +μ 1 R 1 (S)+μ 2 R 2 (X), (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where S is the to-be-estimated clean sinogram; λ is a weight factor. Following <ref type="bibr" target="#b23">[24]</ref>, we rewrite S as Ȳ S for stable learning, where Ȳ and S are the normalization coefficient implemented via the forward projection of a prior image, and the normalized sinogram, respectively. Then we can get the final dual-domain reconstruction model for this specific SVMAR task as:</p><formula xml:id="formula_6">min S,X PX-Ȳ S 2 F +λ (1-T r∪D) ( Ȳ S-Y svma ) 2 F +μ 1 R 1 ( S)+μ 2 R 2 (X). (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>As observed, given Y svma , we need to jointly estimate S and X. For R 1 (•) and R 2 (•), the design details are presented below.</p><p>Iterative Optimization Algorithm. To solve the model (4), we utilize the classical proximal gradient technique <ref type="bibr" target="#b14">[15]</ref> to alternatively update the variables S and X. At iterative stage k, we can get the corresponding iterative rules:</p><formula xml:id="formula_8">Sk = prox μ 1 η 1 Sk-1 -η1 Ȳ Ȳ Sk-1 -PX k-1 +λ 1-T r∪D Ȳ Ȳ Sk-1 -Ysvma , X k = prox μ 2 η 2 X k-1 -η2P T PX k-1 -Ȳ Sk , (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where η i is stepsize; prox μiηi (•) is proximal operator, which relies on the regularization term R i (•). For any variable, its iterative rule in Eq. ( <ref type="formula" target="#formula_8">5</ref>) consists of two steps: an explicit gradient step to ensure data consistency and an implicit proximal computation prox μiηi (•) which enforces the prior R i (•) on the to-beestimated variable. Traditionally, the prior form R i (•) is empirically designed, e.g., l 1 penalty, which may not always hold in real complicated scenarios. Due to the high representation capability, CNN has been adopted to adaptively learn the proximal step in a data-driven manner for various tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. Motivated by their successes, in the next section, we will deeply explore the prior of this specific SVMAR task and carefully construct the network for prox μiηi (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Equivariant Proximal Network for SVMAR</head><p>By unfolding the iterative rules <ref type="bibr" target="#b4">(5)</ref> for K iterations, we can easily build the unrolling neural network. Specifically, at iteration k, the network structure is sequentially composed of: x to execute the proximal operators prox μ1η1 and prox μ2η2 , respectively. To build proxNet θ (k) s , we follow <ref type="bibr" target="#b23">[24]</ref> and choose a standard-CNN-based structure with four [Conv+BN+ReLU+Conv+BN+Skip Connection] residual blocks, which do not change image sizes. While for proxNet θ (k)</p><formula xml:id="formula_10">Sk = proxNet θ (k) s Sk-1 -η1 Ȳ Ȳ Sk-1 -PX k-1 +λ 1-T r∪D Ȳ Ȳ Sk-1 -Ysvma , X k = proxNet θ (k) x X k-1 -η2P T PX k-1 -Ȳ Sk ,<label>(6)</label></formula><p>x , we carefully investigate that during the CT scanning, the same body organ can be imaged at different rotation angles. However, the conventional CNN for modeling proxNet θ (k)</p><p>x in <ref type="bibr" target="#b23">[24]</ref> has only the translation equivariance property and it cannot preserve such an intrinsic rotation equivariance structure <ref type="bibr" target="#b2">[3]</ref>. Against this issue, we propose to replace the standard CNN in <ref type="bibr" target="#b23">[24]</ref> with a rotation equivariant CNN. Then we can embed more useful prior, such as rotation equivariance, to constrain the network, which would further boost the quality of reconstructed CT images (refer to Sect. 5.2).</p><p>Specifically, from Eq. ( <ref type="formula" target="#formula_0">1</ref>), for a rotation group G and any input feature map f , we expect to find a properly parameterized convolutional filter ψ which is group equivariant about G, satisfying</p><formula xml:id="formula_11">[T θ [f ]] ψ = T θ [f ψ] = f π θ [ψ], ∀θ ∈ G, (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where π θ is a rotation operator. Due to its solid theoretical foundation, the Fourier-series-expansion-based method <ref type="bibr" target="#b27">[28]</ref> is adopted to parameterize ψ as:</p><formula xml:id="formula_13">ψ(x) = p-1 m=0 p-1 n=0 a mn ϕ c mn x + b mn ϕ s mn x ,<label>(8)</label></formula><p>Fig. <ref type="figure">2</ref>. The framework of the proposed MEPNet where "prior-net" is designed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>where x = [x i , x j ] T is 2D spatial coordinates; a mn and b mn are learnable expansion coefficients; ϕ c mn x and ϕ s mn x are 2D fixed basis functions as designed in <ref type="bibr" target="#b27">[28]</ref>; p is chosen to be 5 in experiments. The action π θ on ψ in Eq. ( <ref type="formula" target="#formula_11">7</ref>) can be achieved by coordinate transformation as:</p><formula xml:id="formula_14">π θ [ψ] x = ψ U -1 θ x , where U θ = cosθ sinθ -sinθ cosθ , ∀θ ∈ G. (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>Based on the parameterized filter in Eq. ( <ref type="formula" target="#formula_13">8</ref>), we follow <ref type="bibr" target="#b27">[28]</ref> to implement the rotation-equivariant convolution for the discrete domain. Compared with other types, e.g., harmonics and partial-differential-operator-like bases <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, the basis in Eq. ( <ref type="formula" target="#formula_13">8</ref>) has higher representation accuracy, especially when being rotated. By implementing proxNet θ (k) s and proxNet θ (k)</p><p>x in Eq. ( <ref type="formula" target="#formula_10">6</ref>) with the standard CNN and the rotation-equivariant CNN with the p8 group,<ref type="foot" target="#foot_0">1</ref> respectively, we can then construct the model-driven equivariant proximal network, called MEP-Net, as shown in Fig. <ref type="figure">2</ref>. The expansion coefficients, {θ (k) s } K k=1 , θ prior for learning Ȳ <ref type="bibr" target="#b23">[24]</ref>, η 1 , η 2 , and λ, are all flexibly learned from training data end-to-end. Remark: Our MEPNet is indeed inspired by InDuDoNet <ref type="bibr" target="#b23">[24]</ref>. However, MEP-Net contains novel and challenging designs: 1) It is specifically constructed based on the physical imaging procedure for the SVMAR task, leading to a clear working mechanism; 2) It embeds more prior knowledge, e.g., rotation equivariance, via advanced filter parametrization method, which promotes better reconstruction; 3) It is desirable that the usage of more transformation symmetries would further decrease the number of model parameters and improve the generalization. These advantages are validated in the experiments below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Details Description</head><p>Datasets and Metrics. Consistent with <ref type="bibr" target="#b23">[24]</ref>, we synthesize the training set by randomly selecting 1000 clean CT images from the public DeepLesion <ref type="bibr" target="#b28">[29]</ref> and collecting 90 metals with various sizes and shapes from <ref type="bibr" target="#b32">[33]</ref>. Specifically, following the CT imaging procedure with fan-beam geometry in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>, all the CT images are resized as 416 × 416 pixels where pixel spacing is used for normalization, and 640 fully-sampled projection views are uniformly spaced in 360 • . To synthesize sparse-view metal-affected sinogram Y svma , similar to <ref type="bibr" target="#b35">[36]</ref>, we uniformly sample 80, 160, and 320 projection views to mimic 8, 4, and 2-fold radiation dose reduction. By executing the FBP process on Y svma , we can obtain the degraded CT image X svma .</p><p>The proposed method is tested on three datasets including DeepLesion-test (2000 pairs), Pancreas-test (50 pairs), and CLINIC-test (3397 pairs). Specifically, DeepLesion-test is generated by pairing another 200 clean CT images from DeepLesion <ref type="bibr" target="#b28">[29]</ref> with 10 extra testing metals from <ref type="bibr" target="#b32">[33]</ref>. Pancreas-test is formed by randomly choosing 5 patients with 50 slices from Pancreas CT <ref type="bibr" target="#b16">[17]</ref> and pairing each slice with one randomly-selected testing metal. CLINIC-test is synthesized by pairing 10 volumes with 3397 slices randomly chosen from CLINIC <ref type="bibr" target="#b12">[13]</ref> with one testing metal slice-by-slice. The 10 testing metals have different sizes as <ref type="bibr" target="#b34">[35]</ref> in pixels. For evaluation on different sizes of metals as listed in Table <ref type="table" target="#tab_1">2</ref> below, we merge the adjacent two sizes into one group. Following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, we adopt peak signal-to-noise ratio (PSNR) and structured similarity index (SSIM) for quantitative analysis. Implementation Details. Our MEPNet is trained end-to-end with a batch size of 1 for 100 epochs based on PyTorch <ref type="bibr" target="#b15">[16]</ref> on an NVIDIA Tesla V100-SMX2 GPU card. An Adam optimizer with parameters (β 1 , β 2 ) = (0.5, 0.999) is exploited. The initial learning rate is 2 × 10 -4 and it is decayed by 0.5 every 40 epochs. For a fair comparison, we adopt the same loss function as <ref type="bibr" target="#b23">[24]</ref> and also select the total number of iterations K as 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>Working Mechanism. Figure <ref type="figure" target="#fig_1">3</ref> presents the sinogram S k and CT image X k reconstructed by MEPNet at different stages. We can easily observe that S k and X k are indeed alternatively optimized in information restoration and artifact reduction, approaching the ground truth Y gt and X gt , respectively. This finely shows a clear working mechanism of our proposed MEPNet, which evolves in the right direction specified by Eq. ( <ref type="formula" target="#formula_8">5</ref>). Visual Comparison. Figure <ref type="figure">4</ref> shows the reconstructed results of different methods, including FBPConvNet <ref type="bibr" target="#b8">[9]</ref>, DuDoNet <ref type="bibr" target="#b11">[12]</ref>, InDuDoNet <ref type="bibr" target="#b23">[24]</ref>, and the proposed MEPNet, on three degraded images from DeepLesion-test with different sparse-view under-sampling rates and various sizes of metallic implants. <ref type="foot" target="#foot_1">2</ref> As seen, compared with these baselines, our proposed MEPNet can consistently produce cleaner outputs with stronger artifact removal and higher structural fidelity, especially around the metals, thus leading to higher PSNR/SSIM values.</p><p>Figure <ref type="figure">5</ref> presents the cross-domain results on Pancreas-test with ×4 undersampling rate where DL-based methods are trained on synthesized DeepLesion. As seen, DuDoNet produces over-smoothed output due to the lack of physical geometry constraint on the final result. In contrast, MEPNet achieves more efficient artifact suppression and sharper detail preservation. Such favorable generalization ability is mainly brought by the dual-domain joint regularization and the fine utilization of rotation symmetry via the equivariant network, which can reduce the model parameters from 5,095,703 (InDuDoNet) to 4,723,309 (MEP-Net). Besides, as observed from the bone marked by the green box, MEPNet alleviates the rotational-structure distortion generally existing in other baselines. This finely validates the effectiveness of embedding rotation equivariance. Quantitative Evaluation. Table <ref type="table" target="#tab_0">1</ref> lists the average PSNR/SSIM on three testing sets. It is easily concluded that with the increase of under-sampling rates, all these comparison methods present an obvious performance drop. Nevertheless, our MEPNet still maintains higher PSNR/SSIM scores on different testing sets, showing good superiority in generalization capability. In this paper, for the SVMAR task, we have constructed a dual-domain reconstruction model and built an unrolling model-driven equivariant network, called MEPNet, with a clear working mechanism and strong generalization ability. These merits have been substantiated by extensive experiments. Our proposed method can be easily extended to more applications, including limited-angle and low-dose reconstruction tasks. A potential limitation is that consistent with <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>, the data pairs are generated based on the commonly-adopted protocol, which would lead to a domain gap between simulation settings and clinical scenarios. In the future, we will try to collect clinical data captured in the sparseview metal-inserted scanning configuration to evaluate our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where proxNet θ (k) s and proxNet θ (k) x are proximal networks with parameters θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The sinogram S k and CT image X k reconstructed by our MEPNet (K = 10).</figDesc><graphic coords="7,79,47,54,47,294,04,85,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. DeepLesion-test: Artifact-reduced images (the corresponding PSNRs/SSIMs are shown below) of the comparing methods under different sparse-view under-sampling rates (a) ×8, (b) ×4, (c) ×2, and various sizes of metals marked by red pixels. (Color figure online)</figDesc><graphic coords="8,56,31,333,02,311,32,74,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Table2reports the results on the DeepLesion-test with different sizes of metals under the ×4 sparse-view under-sampling rate. We can observe that MEPNet almost outperforms others, especially for the large metal setting, showing good generality.3   Average PSNR (dB) and SSIM of different methods on three testing sets.</figDesc><table><row><cell>Methods</cell><cell cols="2">DeepLesion-test</cell><cell></cell><cell cols="2">Pancreas-test</cell><cell></cell><cell cols="2">CLINIC-test</cell><cell></cell></row><row><cell></cell><cell>×8</cell><cell>×4</cell><cell>×2</cell><cell>×8</cell><cell>×4</cell><cell>×2</cell><cell>×8</cell><cell>×4</cell><cell>×2</cell></row><row><cell>Input</cell><cell>12.65</cell><cell>13.63</cell><cell>16.55</cell><cell>12.37</cell><cell>13.29</cell><cell>16.04</cell><cell>13.95</cell><cell>14.99</cell><cell>18.04</cell></row><row><cell></cell><cell>0.3249</cell><cell>0.3953</cell><cell>0.5767</cell><cell>0.3298</cell><cell>0.3978</cell><cell>0.5645</cell><cell>0.3990</cell><cell>0.4604</cell><cell>0.6085</cell></row><row><cell cols="2">FBPConvNet [9] 25.91</cell><cell>27.38</cell><cell>29.11</cell><cell>24.24</cell><cell>25.44</cell><cell>26.85</cell><cell>27.92</cell><cell>29.62</cell><cell>31.56</cell></row><row><cell></cell><cell>0.8467</cell><cell>0.8851</cell><cell>0.9418</cell><cell>0.8261</cell><cell>0.8731</cell><cell>0.9317</cell><cell>0.8381</cell><cell>0.8766</cell><cell>0.9362</cell></row><row><cell>DuDoNet [12]</cell><cell>34.33</cell><cell>36.83</cell><cell>38.18</cell><cell>30.54</cell><cell>35.14</cell><cell>36.97</cell><cell>34.47</cell><cell>37.34</cell><cell>38.81</cell></row><row><cell></cell><cell>0.9479</cell><cell>0.9634</cell><cell>0.9685</cell><cell>0.9050</cell><cell>0.9527</cell><cell>0.9653</cell><cell>0.9157</cell><cell>0.9493</cell><cell>0.9598</cell></row><row><cell>InDuDoNet [24]</cell><cell>37.50</cell><cell>40.24</cell><cell>40.71</cell><cell>36.86</cell><cell>38.17</cell><cell>38.22</cell><cell>38.39</cell><cell>39.67</cell><cell>40.86</cell></row><row><cell></cell><cell>0.9664</cell><cell>0.9793</cell><cell>0.9890</cell><cell>0.9664</cell><cell>0.9734</cell><cell>0.9857</cell><cell>0.9572</cell><cell>0.9621</cell><cell>0.9811</cell></row><row><cell>MEPNet (Ours)</cell><cell>38.48</cell><cell>41.43</cell><cell>42.66</cell><cell>36.76</cell><cell>40.69</cell><cell>41.17</cell><cell>39.04</cell><cell>41.58</cell><cell>42.30</cell></row><row><cell></cell><cell cols="9">0.9767 0.9889 0.9910 0.9726 0.9872 0.9896 0.9654 0.9820 0.9857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average PSNR (dB)/SSIM of the comparing methods on DeepLesion-test with the ×4 under-sampling rate and different sizes of metallic implants.</figDesc><table><row><cell>Methods</cell><cell>Large Metal</cell><cell>-→</cell><cell>Small Metal</cell><cell></cell><cell></cell><cell>Average</cell></row><row><cell>Input</cell><cell>13.68/0.3438</cell><cell>13.63/0.3736</cell><cell>13.61/0.4046</cell><cell>13.61/0.4304</cell><cell>13.60/0.4240</cell><cell>13.63/0.3953</cell></row><row><cell cols="2">FBPConvNet [9] 26.15/0.7865</cell><cell>26.96/0.8689</cell><cell>27.77/0.9154</cell><cell>27.98/0.9216</cell><cell>28.03/0.9331</cell><cell>27.38/0.8851</cell></row><row><cell>DuDoNet [12]</cell><cell>31.73/0.9519</cell><cell>33.89/0.9599</cell><cell>37.81/0.9667</cell><cell>40.19/0.9688</cell><cell>40.54/0.9696</cell><cell>36.83/0.9634</cell></row><row><cell>InDuDoNet [24]</cell><cell>33.78/0.9540</cell><cell>38.15/0.9746</cell><cell>41.96/0.9873</cell><cell>43.48/0.9898</cell><cell cols="2">43.83/0.9910 40.24/0.9793</cell></row><row><cell>MEPNet (Ours)</cell><cell cols="6">37.51/0.9797 39.45/0.9879 42.78/0.9920 43.92/0.9924 43.51/0.9924 41.31/0.9889</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Considering performance and efficiency, we follow<ref type="bibr" target="#b27">[28]</ref> and chose p8 group for discretized equivariance convolution on CT images. The parameterized filters for eight different rotation orientations share a set of expansion coefficients, largely reducing the network parameters (validated in Sect. 5.2).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Here InDuDoNet is a particularly strong baseline and it is exactly an ablation study, which is the degenerated form of MEPNet with removing group equivariance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>More experimental results are included in supplementary material.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2020AAA0109500/2020AAA0109501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_F6hj3FD">
					<idno type="grant-number">2020AAA0109500/2020AAA0109501</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_11.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Equivariant neural networks for inverse problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Celledoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Etmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Owren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Prob</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">85006</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Equivariant imaging: learning beyond the range space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tachella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4379" to="4388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learnable multi-scale Fourier interpolation for sparse view CT image reconstruction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="286" to="295" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">KXNet: a model-driven deep neural network for blind super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19800-7_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19800-7_14" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision, ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13679</biblScope>
			<biblScope unit="page" from="235" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast enhanced CT metal artifact reduction using data domain deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imag</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale-equivariant unrolled neural networks for data-efficient accelerated MRI reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="737" to="747" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metal artifact reduction on cervical CT images by deep residual learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep-neural-network-based sinogram synthesis for sparse-view CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ADN: artifact disentanglement network for unsupervised metal artifact reduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="643" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DuDoNet: dual domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10512" to="10521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08721</idno>
		<title level="m">Deep learning to segment pelvic bones: large-scale CT datasets and baseline models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-light image enhancement by retinex-based algorithm unrolling and adjustment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proximal algorithms. Found. Trends Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in PyTorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepOrgan: multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="556" to="564" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NeRP: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PDO-eConvs: partial differential operator based equivariant convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8697" to="8706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szmaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11093</idno>
		<title level="m">Scale-equivariant steerable networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">DuDoTrans: dualdomain transformer provides more attention for sinogram restoration in sparseview CT reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10790</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DICDNet: deep interpretable convolutional dictionary network for metal artifact reduction in CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="869" to="880" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adaptive convolutional dictionary network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07471</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">InDuDoNet: an interpretable dual domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="107" to="118" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">InDuDoNet+: a deep unfolding dual domain network for metal artifact reduction in CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102729</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Orientation-shared convolution representation for CT metal artifact learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="665" to="675" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fourier series expansion based filter parametrization for equivariant convolutions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4537" to="4551" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep lesion graphs in the wild: relationship learning and organization of significant radiology image findings in a diverse large-scale lesion database</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9261" to="9270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ADMM-Net: a deep learning approach for compressive sensing MRI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06869</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sinogram completion with image prior for metal artifact reduction in CT images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MetaInv-Net: meta inversion network for sparse view CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="621" to="634" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural network based metal artifact reduction in X-ray computed tomography</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1370" to="1381" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sparse-view CT reconstruction method based on combination of DenseNet and deconvolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1407" to="1417" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TransCT: dual-path transformer for low dose computed tomography</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DuDoDR-Net: dual-domain data consistent recurrent network for simultaneous sparse view and metal artifact reduction in computed tomography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102289</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Limited view tomographic reconstruction using a cascaded residual dense spatial-channel attention network with projection data fidelity layer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1792" to="1804" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
