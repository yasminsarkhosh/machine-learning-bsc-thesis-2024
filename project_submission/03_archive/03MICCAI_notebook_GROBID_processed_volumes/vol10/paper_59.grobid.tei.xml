<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation</title>
				<funder ref="#_DtzQHSu">
					<orgName type="full">Liaoning Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_cbmJ8AB">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zuopeng</forename><surname>Tan</surname></persName>
							<email>zuopengtan@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hengyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Urology</orgName>
								<orgName type="institution">Affiliated Zhongshan Hospital of Dalian University</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Laboratory of Microenvironment Regulation and Immunotherapy of Urinary Tumors in Liaoning Province</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<email>zhanglihe@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weibing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Urology</orgName>
								<orgName type="institution">Affiliated Zhongshan Hospital of Dalian University</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Laboratory of Microenvironment Regulation and Immunotherapy of Urinary Tumors in Liaoning Province</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="623" to="633"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CCE43162B84F1CE20F2D2A42EFFDD476</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_59</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation-assisted registration models can leverage few available labels in exchange for large performance gains by their complementarity. Recent related works independently build the prediction branches of deformation field and segmentation label without any information interaction except for the joint supervision. They ignore underlying relationship between the two tasks, thereby failing to fully exploit their complementary nature. To this end, we propose a ProGressively Coupling Network (PGCNet) that relies on segmentation to regularize the correct projecting of registration. Our overall framework is a multitask learning paradigm in which features are extracted by one shared encoder and then separate prediction branches are built for segmentation and registration. In the prediction phase, we utilize the bidirectional deformation fields as bridges to warp the features of moving and fixed images to each other's segmentation branches, thereby progressively and interactively supplementing additional context information at multiple levels for their segmentation. By establishing the entangled correspondence, segmentation supervision can indirectly regularize registration stream to accurately project semantic layout for segmentation branches. In addition, we design the position correlation calculation for registration to easier capture the spatial correlation of the images from the shared features. Experimental results on public 3D brain MRI datasets show that our work performs favorably against the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deformable medical image registration has many applications in clinic, including but not limited to clinical case tracking, surgical navigation. In recent years, many advanced learning-based medical image registration methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref> have been proposed. Due to the limitations of accessible labels, it is more common to employ unsupervised ways for optimization. Although unsupervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">19]</ref> can guide the registration network to optimize by maximizing the image-wise similarity between warped and fixed images, the lack of guidance from regions of interest (ROIs) makes the registration performance fall into a bottleneck. Moreover, medical image labeling usually requires professional medical staff, and it's laborious to get large-scale training labels. Integrating segmentation into registration can considerably compensate for the lack of image labels via their complementarity <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">23]</ref>. Specifically, warping segmentation result of moving image to align to that of fixed image by deformation field can provide additional supervision of ROIs for registration task. Meanwhile, as a way of data augmentation, the unlabelled images can participate in the segmentation optimization to prevent the overfitting of segmentation in few-shot situation <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b27">26]</ref>.</p><p>Recently, several joint learning methods have been proposed. These methods can be generally divided into two categories in terms of model structure (see Fig. <ref type="figure" target="#fig_0">1</ref>), two-two model (i.e., two encoders and two decoders) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">23]</ref> and onetwo model (i.e., one encoder and two decoders) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">27]</ref>. The two-two models employ two completely independent models for segmentation and registration, and achieve mutual learning by joint loss. DeepAtlas <ref type="bibr" target="#b24">[23]</ref> alternately trains registration and segmentation networks to achieve mutual improvement in brain and knee images. He et al. <ref type="bibr" target="#b9">[10]</ref> feed the segmentation masks into the registration network to provide the internal texture information of the regions of interest (ROIs) so as to avoid incorrect deformation of internal areas.</p><p>In comparison, the one-two models are more in line with the general paradigm of multi-task learning, which can be regarded as a process of inductive bias <ref type="bibr" target="#b22">[21]</ref>, i.e., utilizing the shared encoder to induct the commonality of each task, and then using the respective decoder for preference prediction. They can effectively reduce the risk of overfitting and reduce the parameters of the network through the way of sharing parameters <ref type="bibr" target="#b2">[3]</ref>. However, existing one-two models all focus on loss design to depict joint learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">27]</ref>. There is not any explicit feature interaction between two task streams. It is well known that sensible interactions help model capture extra key features or regularize networks to optimize in desired directions <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b29">28]</ref>. In addition, the semantic information of moving and fixed images are interchangeable and mutually exploitable. As it happens, the deformation field in registration is able to interconvert the context information of moving and fixed images and the converted features can be employed as additional contextual information for their segmentation.</p><p>Based on the framework of one-two model, we propose a progressively coupling network (PGCNet) as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, progressive field estimators are designed to calculate multi-level deformation fields from coarse to fine. By warping the moving features and computing the correlation between warped and fixed features at each level, the deformation field is refined progressively. Different from common correlation calculation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, we introduce learnable absolute position coordinates to provide spatial information for registration, which can better measure coordinate correspondence of voxels. In order to closely couple the registration and segmentation branches, we infuse the warped contexts into the segmentation branches. In this way, segmentation supervision sets an extra attention focusing for the registration decoder, which drives the deformation field to project semantic layout from moving image to fixed one. This semantic projection helps improve the registration accuracy. Meanwhile, the registration branch provides more sufficient semantic information for segmentation branch.</p><p>The main contributions can be summarized as follows: 1) We propose a novel registration learning framework to establish the entangled relationship between registration and segmentation by progressively coupling the moving and fixed segmentation, which promotes both registration and segmentation performance in few-shot situation. 2) To effectively measure the coordinate correspondence of voxels, we design the position correlation calculation, which provides spatial coordinate information for field estimator while measuring feature similarity. 3) Experimental results on the general brain MRI datasets, OASIS and IXI, show that our method outperforms the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our model aims to learn registration and segmentation tasks simultaneously. Let M , F be moving and fixed images, respectively. We parameterize our model as a function with parameter θ, f θ (M, F ) = φ, S M , S F , where φ is the deformation field, S M and S F represent the segmentation results of M and F . Registration task completes the transformation from moving images to fixed ones, and segmentation task generates the segmentation maps of moving and fixed images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Network Structure</head><p>The overall pipeline of our method is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, which mainly consists of two parts, shared encoder and pyramid decoder. The parameters of the encoder (rose red block) and segmentation decoder (purple block) are shared. The shared encoder of our model is a 3D residual convolution network, which has 4 feature levels. Except for the input level, we use 3, 5, and 5 residual blocks at the other levels, respectively. A residual block contains two convolutional layers with preactivation <ref type="bibr" target="#b8">[9]</ref> structure and shortcut connection, to extract features, respectively. The numbers of channels at the four levels are 8, 16, 32, and 64, respectively, and each convolution layer is followed by Instance Normalization and Leaky-ReLU with a negative slope of 0.2 except for the last output. We first calculate the correlation of the deepest level features and then estimate the initial deformation field based on the correlation matrix. In the segmentation branch, we employ the commonly used skip connection structure, and concatenate the complementary features from the registration branch at each level, which are warped by bidirectional deformation fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Coupling Decoder</head><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, there are four inputs in our coupling decoder: Field (ϕ), Inv-Field (ϕ -), Moving Context (F M ) and Fixed Context (F F ). Among them, Field (ϕ) and Inv-Field (ϕ -) are up-sampled by trilinear interpolation with the factor of 2 from the front level. Moving Context (F M ) and Fixed Context (F F ) are the context information from the corresponding level. We use ϕ to warp F M and input the warped features (F M • ϕ) into the segmentation decoder of fixed image as additional complementary information (coupled features). In this way, the segmentation network obtains richer semantic information. The same operation is done in the segmentation branch of moving image as well. Relying on the projecting ability of the deformation field, we align and combine context information of moving and fixed images. Thus, registration and segmentation branches establish an entangled correspondence to constrain each other to learn a well generalized model. Generally speaking, if the fixed image is precisely segmented, the coupled features, which are projected from moving image to fixed one, usually are well matched. That is, the coupling process can constrain the deformation field to become accurate. Position Correlation Calculation for Registration. In order to measure the similarity between each voxel and its neighbors, the warped (F M • ϕ) and fixed (F F ) context features will conduct correlation calculation to obtain a cost volume. Note that the inverse correlation is calculated between the warped (F F • ϕ -) and moving (F M ) context features, which is not shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The spatial correlation between adjacent voxels in moving and fixed images is the key to determining the deformation field. According to the cost volume, we can judge the displacement between moving and fixed images at the current level. Different from existing works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, we embed the position information into feature volumes before correlation calculation. Segmentation and registration tasks have different semantic expression requirements. The position coding can reduce the semantic confusion brought about by the shared encoder and enforce the decoder to understand spatial relationships of voxels. Let E be position coding map and r be the displacement radius, the correlation calculation is formulated as:</p><formula xml:id="formula_0">Corr(F 1 , F 2 , E) = r i,j,k &lt; (F 1 + E) x,y,z • (F 2 + E) x+i,y+j,z+k &gt;, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where</p><formula xml:id="formula_2">F 1 = F M • ϕ, F 2 = F F . E is initialized as 0.</formula><p>x, y and z represent the coordinate indexes of feature map in three directions, respectively. We move feature map point by point along x, y and z directions with radius r and do dot product to generate a correlation map. The dimension of correlation matrix (cost volume) is (2r + 1) 3 , and we set r = 1 in this work. Then, Field and Inv-Field estimation modules utilize three residual blocks to compute the increment of forward and inverse displacement fields according to the correlation and inverse correlation matrices, respectively. The parameters of the two modules are shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>The loss functions of our framework comprise three parts, which regularize registration, segmentation, and joint optimization, respectively. The registration loss has similarity and smoothness penalty terms to align the moving and fixed images and ensure the smoothness of the deformation field. We use local normalized cross-correlation <ref type="bibr" target="#b1">[2]</ref> with window size ω (ω = 9) as similarity function. Let F and W represent the fixed and warped (M • φ) scans. L 2 loss of deformation field gradient is set as regularization function. The registration loss can be defined as:</p><formula xml:id="formula_3">L reg = LN CC ω (F, W ) + λ 1 φ 2 2 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where λ 1 is a balance hyperparameter and is set as 1 in the experiments.</p><p>For segmentation, we use the combination of weighted cross entropy and Dice loss, which is formulated as:</p><formula xml:id="formula_5">L wce (S, S * ) = - 1 NP N n=0 P p=0 w n S * n,p logS n,p ,<label>(3)</label></formula><formula xml:id="formula_6">L dice (S, S * ) = - 1 N N n=0 P p=0 S * n,p • S n,p P p=0 S * n,p + P p=0 S n,p ,<label>(4)</label></formula><p>where N represents the total number of semantic classes, n indicates the channel of the corresponding category, and P is the number of voxels in each channel. S is the segmentation result and S * is the voxel-wise manual segmentation label. In Eq. 3, w n indicates the inverse proportion of voxels in category n to all voxels. For these two segmentation loss functions, their weights are fixed to the same value which means L seg = L wce + L dice . The joint loss function is defined as L dice (S F , S M • φ). It regularizes registration network to provide additional training data for segmentation task. Meanwhile, registration network can pay more attention to the ROI regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets and Preprocessing. We validate our method by using 414 T1weighted brain MRI scans from OASIS <ref type="bibr" target="#b15">[16]</ref> dataset and 576 T1-weighted brain MRI scans from IXI 1 dataset. These scans are preprocessed, which includes skull dissection, spatial normalization. All MRI scans are resampled with the same isotropic voxels of 1 mm × 1 mm × 1 mm, and we center crop the scans from OASIS dataset to 160 × 192 × 144 and the scans from IXI dataset to 160 × 160 × 192, which crops excess background to reduce the memory of GPU.</p><p>1 IXI dataset is available in https://brain-development.org/ixi-dataset/.</p><p>The subcortical structures in OASIS and IXI are labeled as 35 and 44 categories by FreeSurfer for evaluation. We randomly select 5 scans as atlas from OASIS dataset, then the remaining scans are randomly divided into 255, 22, 132 for training, validation and testing. We do not use the labels of the training scans, only 5 labels of atlas are used to simulate a few-shot scenario. A total of 1,275, 110, 660 pairs of scans are used for training, validation and testing. Similarly, the IXI dataset is randomly split into 397, 58, 115, and 5 labeled scans are randomly taken as atlas for few-shot situation. The training, validation and testing include 1,985, 290, 575 pairs of scans, respectively.</p><p>Implementation. All trainings use Adam optimizer with learning rate 1e -4 and the batch size is set as 1. We first train the network by optimizing L reg + L seg for 5,000 iterations and then by optimizing L reg + L seg + L joint for 80,000 iterations. All experiments are performed on 1 Nvidia RTX 3090 GPU.</p><p>Baseline Methods. We compare our method with a series of registration models. SYN <ref type="bibr" target="#b0">[1]</ref> and LDDM <ref type="bibr" target="#b3">[4]</ref> are two traditional registration algorithms. Voxelmorph <ref type="bibr" target="#b1">[2]</ref> and transmorph <ref type="bibr" target="#b4">[5]</ref> are the learning-based methods, which directly predict the deformation field. While LapIRN <ref type="bibr" target="#b16">[17]</ref> and ULAE <ref type="bibr" target="#b19">[19]</ref> are two progressive methods. We also evaluate these two methods under the semi-supervised setting with auxiliary segmentation labels. The semi-supervised loss function is the same as ours. DeepAtlas <ref type="bibr" target="#b24">[23]</ref>, UResNet <ref type="bibr" target="#b7">[8]</ref>, and PC-Reg <ref type="bibr" target="#b9">[10]</ref> are joint learning methods, and we adopt the same training strategy as theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Registration Performance. As shown in Table <ref type="table">1</ref>, PGCNet achieves competitive registration accuracy, 87.72% DSC mean and 1.59 HD95 on OASIS dataset and 78.08% DSC mean and 3.19 HD95 on IXI dataset. For VoxelMorph and TransMorph, which directly estimate the deformation field, their DSC mean are relatively low. Coarse-to-fine registration methods, LapIRN and ULAE, can meet the complex transformation needs between images through multiple deformations, but they still have poor performance due to the lack of ROIs supervision information. By introducing additional ROIs supervision information, their performance is improved by 1.1% and 1.8%, respectively. Their improvement is weak because of the limited semi-supervised information. Our method outperforms UResNet, DeepAtlas, and PC-Reg by 12.7%, 5.7%, and 1.7%, respectively, in joint learning models. The poor performance of UResNet can be attributed to its approach of concatenating moving and fixed images to extract features, while only predicting the moving segmentation result. Figure <ref type="figure" target="#fig_3">4</ref> presents visual comparison of various methods. It can be observed that our method exhibits lowest registration error in the third row.</p><p>Ablation Study. We conduct a series of ablation experiments on OASIS dataset to verify the effectiveness of each module, as shown in Table <ref type="table" target="#tab_1">2</ref>. We take the Table <ref type="table">1</ref>. Quantitative results of different registration methods on OASIS and IXI datasets. Average Dice similarity coefficient (DSCmean) and 95% percentile of the Hausdorff distance (HD95) are used to measure the registration accuracy of the models, and the average proportion of folding voxels in the deformation fields (| J φ |&lt; 0) indicates the topological reversibility of the deformation field.   registration-only model as baseline, which has the same encoder and field estimation module as the final model, utilizing common correlation calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OASIS IXI</head><formula xml:id="formula_7">DSCmean ↑ HD95 ↓ | J φ |&lt; 0 ↓ DSCmean ↑ HD95 ↓ | J φ |&lt; 0 ↓ SYN [</formula><p>Joint training (JTrain) introduces semi-supervised loss for registration. And the coupling feature connection (Couple) improves the performance of registration by guiding the deformation field in advance to map semantic features. Position embedding (PEmbedding) assists the field estimator in understanding the correlation between voxels. In addition, we further analyse the factors that affect the performance of segmentation (see Table <ref type="table" target="#tab_2">3</ref>). Coupling feature connection provides more adequate semantic information, resulting in 3.3% improvement, while joint training provides additional trainable data, leading to 2.4% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a progressively coupling network (PGCNet), which employs the deformation fields to couple the registration and segmentation branches. This is a novel mode of regularization for registration and segmentation, which promotes their performance in few-shot situation. In addition, position embedding provides additional spatial coordinate information for registration branch. The experimental results indicate that our work is superior to existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two common joint registration and segmentation frameworks. (a) Two-two model: two encoders and two decoders. (b) One-two model: one shared encoder and two separate decoders.</figDesc><graphic coords="2,41,79,153,41,336,94,90,67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of progressively coupling network (PGCNet), which is an encoderdecoder structure. The red arrows denote the shared encoder path. The blue and purple arrows indicate the information flow of registration and segmentation, respectively, and the spatial transformation path is represented as brown. The structure of Coupling Decoder (the blue octagon) is shown in Fig. 3 (Color figure online).</figDesc><graphic coords="4,44,58,53,96,337,54,145,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Coupling decoder. Spatial transformer utilizes deformation field to warp context feature. Correlation calculation gets cost volume. Field estimation module predicts the increment of deformation field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization results of different methods. In error maps, the redder color indicates the larger registration error, while the bluer color represents the smaller error.</figDesc><graphic coords="8,41,79,400,01,340,33,131,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation for registration.</figDesc><table><row><cell cols="4">JTrain Couple PEmbedding RegDice</cell></row><row><cell>√ √ √</cell><cell>√ √</cell><cell>√</cell><cell>82.51 86.29 87.03 87.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation for segmentation.</figDesc><table><row><cell cols="3">Couple JTrain SegDice</cell></row><row><cell>√ √</cell><cell>√</cell><cell>81.45 84.17 86.15</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> #<rs type="grantNumber">62276046</rs>, and the <rs type="funder">Liaoning Natural Science Foundation</rs> #<rs type="grantNumber">2021-KF-12-10</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cbmJ8AB">
					<idno type="grant-number">62276046</idno>
				</org>
				<org type="funding" xml:id="_DtzQHSu">
					<idno type="grant-number">2021-KF-12-10</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="39" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computing large deformation metric mappings via geodesic flows</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Beg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trouve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TransMorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep discontinuity-preserving image registration network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_5" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="226" to="236" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">U-ReSNet: ultimate coupling of registration and segmentation with deep nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Estienne</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_35" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_38" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning for deformable medical image registration with perception-correspondence decoupling and reverse teaching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2021.3095409</idno>
		<ptr target="https://doi.org/10.1109/JBHI.2021.3095409" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1177" to="1187" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep complementary joint model for complex scene registration and few-shot segmentation on medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58523-5_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58523-5_45" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12363</biblScope>
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive decomposition network for deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5130" to="5141" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58536-5_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58536-5_33" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="557" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-stream pyramid registration network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102379</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint progressive and coarse-to-fine registration of brain MRI via deformation field integration and non-rigid feature fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3170879</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3170879" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2788" to="2802" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<idno>978-3-030-59716-0_21</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint learning of motion estimation and segmentation for cardiac MR image sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-2_53" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Medical image registration based on uncoupled learning and accumulative enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_1" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AtlasNet: multi-atlas non-linear deep networks for medical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_75" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3614" to="3633" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PAD-Net: multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepAtlas: joint semi-supervised learning of image registration and segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_47" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep network for joint registration and parcellation of cortical surfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_17" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="171" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint learning of salient object detection, depth estimation and contour extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7350" to="7362" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
