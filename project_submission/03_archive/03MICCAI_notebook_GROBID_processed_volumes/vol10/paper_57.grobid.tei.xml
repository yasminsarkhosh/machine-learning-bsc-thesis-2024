<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer</title>
				<funder ref="#_y4nh9UZ">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
				<funder ref="#_dzehn3D">
					<orgName type="full">Shanghai Municipal Science and Technology Committee of Shanghai Outstanding Academic Leaders Plan</orgName>
				</funder>
				<funder ref="#_Ymd8Wyk">
					<orgName type="full">Shanghai Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_hr9tYtC">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tai</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communications and Electronic Engineering</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinru</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communications and Electronic Engineering</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Communications and Electronic Engineering</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ying</forename><surname>Wen</surname></persName>
							<email>ywen@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Communications and Electronic Engineering</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<postCode>200241</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="602" to="612"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C9074EC91994FF69801F6C5D988F1123</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image registration</term>
					<term>convolutional neural networks</term>
					<term>image processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large deformation image registration is a challenging task in medical image registration. Iterative registration and pyramid registration are two common CNN-based methods for the task. However, these methods usually consume more parameters and time. Additionally, the existing CNN-based registration methods mainly focus on local feature extraction, limiting their ability to capture the long-distance correlation between image pairs. In this paper, we propose a fast and accurate learning-based algorithm, Pyramid-Iterative Vision Transformer (PIViT), for 3D large deformation medical image registration. Our method constructs a novel pyramid iterative composite structure to solve large deformation problem by using low-scale iterative registration with a Swin Transformer-based long-distance correlation decoder. Furthermore, we exploit pyramid structure to supplement the detailed information of the deformation field by using high-scale feature maps. Comprehensive experimental results implemented on brain MRI and liver CT datasets show that the proposed method is superior to the existing registration methods in terms of registration accuracy, training time and parameters, especially of a significant advantage in running time. Our code is available at https://github.com/Torbjorn1997/PIViT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deformable image registration is one of the fundamental tasks in computer vision and has been widely used in medical image processing. In recent years, deep learning methods based on convolutional neural networks are widely applied in deformable image registration. Balakrishnan et al. <ref type="bibr" target="#b2">[3]</ref> proposed VoxelMorph with a structure similar to Unet and further developed a diffeomorphism implementation of VoxelMorph <ref type="bibr" target="#b7">[8]</ref>. Mok et al. <ref type="bibr" target="#b20">[21]</ref> proposed SYMNet to achieve accurate diffeomorphic registration by exploiting the cycle consistency of registration. However, when there is a significant difference between the images, it is difficult to learn an accurate deformation field for alignment because large deformation image registration has a high degree of freedom in transformation. Typical registration methods utilize rigid or affine transformation with a low degree of freedom to provide initialized global transformation for large deformation, however, this requires the introduction of additional preprocessing to obtain the corresponding affine matrix <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b22">[23]</ref>. In order to solve the high degree of freedom of large deformation transformation, the end-to-end deformable image registration methods are mainly divided into two types: iterative registration (Fig. <ref type="figure" target="#fig_0">1 (a)</ref>) and pyramid registration (Fig. <ref type="figure" target="#fig_0">1 (b)</ref>). (a) Iterative registration achieves coarse-to-fine image registration by cascading several CNNs, which requires huge GPU memory during training. In addition, iterative registration methods learn separate image features in each iteration, which brings additional computational costs when repeatedly extracting features. Typical iterative registration methods include RCN <ref type="bibr" target="#b27">[28]</ref> and LapIRN <ref type="bibr" target="#b21">[22]</ref>. (b) Pyramid registration achieves coarse-to-fine registration within one iteration by warping feature maps. These methods successively learn feature maps and deformation fields from low to high resolution. Typical pyramid registration methods include Dual-PRNet <ref type="bibr" target="#b13">[14]</ref> and NICE-Net <ref type="bibr" target="#b19">[20]</ref>. However, current non-iterative registration methods still cannot well solve the image registration problem under the significant differences condition.</p><p>Inspired by the capabilities of Transformer in NLP, recent researchers have extended Transformer to computer vision tasks <ref type="bibr" target="#b10">[11]</ref> [19] and acquired results that surpass CNNs' in many tasks <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b26">[27]</ref>. Many Transformer-based registration methods have also been proposed for image registration tasks, such as Trans-Morph <ref type="bibr" target="#b6">[7]</ref>, Swin-VoxelMorph <ref type="bibr" target="#b29">[30]</ref> and XMorpher <ref type="bibr" target="#b25">[26]</ref>. Compared with CNNbased methods, Transformer-based methods have achieved better registration results, which illustrates that the global receptive field of Transformer is helpful for image registration.</p><p>In this paper, we propose a novel Pyramid-Iterative Vision Transformer (PIViT) by combining Swin Transformer-based long-range correlation decoder and the proposed pyramid-iterative registration framework shown in Fig. <ref type="figure" target="#fig_0">1 (c)</ref>. Our main contributions of this work are as: <ref type="bibr" target="#b0">(1)</ref> We establish a pyramiditerative registration framework to address large deformation image registration. The framework first extracts feature map pairs via a dual-stream weight-sharing encoder, then performs iterative registration on the low-scale feature space, and finally complements detail information and learns accurate deformation fields during pyramid decoding process. <ref type="bibr" target="#b1">(2)</ref> We propose a Swin Transformer-based long-range correlation decoder, which exploits the global receptive field of Swin Transformer on low-scale feature maps to learn high accuracy large deformation fields while maintaining low parameters. (3) Compared with other popular registration methods, the proposed unsupervised end-to-end network is more lightweight and suitable for time-sensitive tasks.</p><p>Extensive experiments on 3D brain MRI and liver CT registration tasks demonstrate that PIViT achieves state-of-the-art performance in terms of accuracy but consumes less time and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>In this section, we first propose a novel pyramid-iterative registration framework to solve large deformation image registration. The pyramid-iterative registration framework combines the advantages of iteration and pyramid registration framework to achieve fast and accurate registration. Then, we introduce a long-range correlation decoder based on Swin Transformer into the iterative registration stage of the proposed framework and utilize the global receptive field of the Swin Transformer to capture global correlations, thereby implementing high accurate and fast registration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pyramid-Iterative Registration Framework</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the proposed pyramid-iterative registration framework can be divided into three parts: dual-stream feature extraction, low-scale iterative registration and multi-scale pyramid registration.</p><p>Dual-Stream Feature Extraction: Similar to pyramid registration network, the proposed framework utilizes a weight-sharing feature encoder to construct feature pyramids for the fixed image I f and the moving image I m , respectively. At the i th step (i ∈ [1 • • • N ]), the feature maps of I f and I m are formulated as F i f and F i m , respectively. The weight-sharing feature encoder reuses the same network blocks to extract the feature maps F i f and F i m without adding parameters or complicating the training process while ensuring that F i f and F i m are in the same feature space.</p><p>Low-Scale Iterative Registration: The pyramid-iterative registration uses two different decoding modules at different scales. To capture large deformation, we adopt low-scale feature maps to obtain the coarse distribution of large deformation fields without considering the fine distribution in this paper. Therefore, at the last N th level of feature pyramid, deformation field is predicted from F N f and F N m multiple times through an iterative structure. Similar to iterativebased registration methods, F N m is warped by the predicted deformation field φ N t , where t is the number of iterations. The warped F N,t m and F N f are used for the next iteration. In the first iteration, the decoder obtains the initial deformation field φ N 1 , and in the subsequent iterations, the residual deformation field Δφ N t is obtained in each prediction and the updated overall deformation field φ N t is obtained. This procedure can be formulated as:</p><formula xml:id="formula_0">F N,t m = F N m • φ N t , φ N t = Δφ N t , t = 1, φ N t-1 + Δφ N t , t = 2, • • • , T, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where T is the upper limit of iteration, • denotes warping the feature map with deformation fields, and + denotes element-wise summation of deformation fields.</p><p>Compared with other iterative registration methods, the advantage of iterating only at the N th level is that there is no need to re-extract image features, thus the computational complexity and time consumption of our method can be greatly reduced. This can greatly accelerate the speed of model training and deformation field prediction, and better solve large deformation.</p><p>Multi-scale Pyramid Registration: After the implementation of low-scale iterative registration, the deformation field φ N T is rescaled by a factor of 2 and the rescaled flow φ N is obtained. The subsequent process is the same as that of the pyramid registration method. At each level, warped feature</p><formula xml:id="formula_2">F i m • φ i+1 and fixed feature F i f (i = N -1, • • • , 1</formula><p>) are concatenated and the residual deformation field Δφ i is predicted by 3D convolution. Δφ i is used to update φ i+1 so as to obtain the deformation field φ i corresponding to the i th layer. φ i is rescaled by a factor of 2 and warps moving feature F i-1 m . The purpose of introducing multi-scale pyramid registration is to supplement the lack of fine information caused by only using low-scale features in the iterative registration stage. This process is repeated at each level of the feature pyramid until the deformation field is rescaled to the original image resolution. Finally, the pyramid-iterative registration framework obtains the predicted global deformation field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long-Range Correlation Decoder</head><p>To capture large deformation at low-scale registration, the study of the decoder is very essential. Therefore, we propose a long-range correlation decoder (LCD) in the iterative registration phase. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the LCD consists of a Swin transformer-based block and two consecutive convolutions. The Swin transformer-based block models the long-range correlation between F N f and F N m • φ N t-1 using the self-attention mechanism of the transformer, and then the residual flow field Δφ N t is obtained by the convolution block. In order to enhance the information interaction between non-overlapping windows, we adopt the shifted local window attention strategy of the Swin Transformer. The structure of the Swin Transformer-based block is shown in the red frame area in Fig. <ref type="figure" target="#fig_2">3</ref>, which consists of shifted window-based self-attention modules (W-SA &amp; SW-SA), followed by a 2-layer MLP. A LayerNorm (LN) layer is applied before each SA and MLP module, and a residual connection is applied after each module.</p><p>Current Transformer-based registration methods usually directly migrate the Transformer structure to the 3D image registration task, which leads to a large number of parameters and a remarkably long inference time. In contrast, the proposed PIViT models long-range correlations in low-scale iterative registration with LCD to warp corresponding voxels between feature maps to spatial neighborhoods, thus it is not necessary to use the Transformer on large feature maps at high scales. In addition, LCD also removes position embedding, only uses single-head self-attention and reduces the number of channels. These operations accelerate the speed of PIViT and significantly reduce parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Loss Function</head><p>PIViT is an unsupervised end-to-end registration network. In this section, we design a loss function to train the proposed network. In the final stage of pyramid registration, PIViT obtains the deformation field φ between I m and I f and the warped image I w = I m • φ by using the differential operation based on the spatial transformer network <ref type="bibr" target="#b14">[15]</ref>. In order to minimize the difference, we use the normalized cross-correlation (NCC) as a measure of the difference between the warped image I w and fixed image I f .</p><p>In order to ensure the continuity and smoothness of the deformation field φ in space, a regular term on its spatial gradient is introduced. The complete loss function is:</p><formula xml:id="formula_3">L I f ,Im,φ = L sim + λL smooth = -NCC(I f , I w ) + λ p∈Ω ∇φ(p) 2 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where λ is the regularization hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Data and Pre-processing: We evaluate the performance of PIViT on brain MRI datasets and liver CT datasets. In the experiments, we compare the proposed method with commonly used 3D convolutional registration methods Voxelmorph <ref type="bibr" target="#b2">[3]</ref>, Dual-PRNet <ref type="bibr" target="#b13">[14]</ref>, RCN <ref type="bibr" target="#b27">[28]</ref>, LKU-Net <ref type="bibr" target="#b15">[16]</ref>, TransMorph <ref type="bibr" target="#b6">[7]</ref>, Swin-VoxelMorph <ref type="bibr" target="#b29">[30]</ref> and NICE-Net <ref type="bibr" target="#b19">[20]</ref>. The accuracy of image registration is measured by Dice score <ref type="bibr" target="#b9">[10]</ref>. We choose 2303 brain MRI scans from the ABIDE <ref type="bibr" target="#b8">[9]</ref>, ADHD <ref type="bibr" target="#b4">[5]</ref> and ADNI <ref type="bibr" target="#b23">[24]</ref> brain MRI datasets for training and LPBA <ref type="bibr" target="#b24">[25]</ref> for testing. LPBA dataset contains 40 brain MRI scans with segmentation ground truth of 56 anatomical structures. For liver CT datasets, 1025 scans from MSD <ref type="bibr" target="#b1">[2]</ref> and BFH <ref type="bibr" target="#b28">[29]</ref> are selected for training and SLIVER <ref type="bibr" target="#b12">[13]</ref>, LiTS <ref type="bibr" target="#b5">[6]</ref> and LSPIG <ref type="bibr" target="#b27">[28]</ref> for testing. The images are all resampled to the size of 128×128×128. In order to better verify the effect of each method on large deformation image registration, we do not perform affine pre-alignment process. Atlas-based and scan-to-scan registrations are performed on brain and liver scans, respectively.</p><p>Implementation: We set λ to 1 for PIViT to guarantee the smoothness of the deformation field. Algorithm runtimes are computed on an NVIDIA GeForce RTX 3090 GPU and an Intel(R) Xeon(R) Silver 4210R CPU. We implement the model using Keras with a Tensorflow <ref type="bibr" target="#b0">[1]</ref> backend and the ADAM <ref type="bibr" target="#b17">[18]</ref> optimizer with a learning rate of 1e -4 . The batch size is set as 1 and the networks are trained for 150,000 iterations.</p><p>Results:  As shown in Table <ref type="table" target="#tab_0">1</ref>, VoxelMorph, VoxelMorph-diff, TransMorph, Swin-VoxelMorph and LKU-Net all get low Dice scores, indicating that these singlestream registration methods are difficult to solve large deformation. However, the iterative registration method RCN and the pyramid registration method NICE-Net obtain relatively good Dice scores, indicating that both iterative and pyramid registration methods can be useful to deal with large deformation. However, the Dice score of the proposed PIViT combining their advantages surpasses that of VoxelMorph by 14.8%, and the improvements compared to RCN and NiceNet also reach 2.5% and 2.0% on LPBA, respectively. On 3 liver datasets, compared with VoxelMorph, PIViT achieves 17.9%, 19.6% and 15.7% improvements, while compared with NICE-Net, PIViT achieves 3.8%, 4.1% and 5.1% improvements, respectively. The experiments indicate that the proposed PIViT implements large deformation fine registration better than other methods.</p><p>In addition to the superior Dice score, another advantage of the proposed PIViT is its fast and lightweight registration. Table <ref type="table" target="#tab_0">1</ref> shows that the parameters, training and registration time of PIViT are close to that of VoxelMorph, far less than those of RCN and NICE-Net. These properties of PIViT make it easier to train and more suitable for time-sensitive tasks. Compared to other Transformerbased methods, the proposed PIViT has orders of magnitude optimization in parameters, which is because we only use the Transformer block at low scales, and LCD is tuned and optimized for 3D image registration tasks. What's more, although there is no additional constraint on the diffeomorphism of the deformation field, the deformation field obtained by PIViT has better diffeomorphism properties than those obtained by other methods except VoxelMorph-diff.</p><p>The visualization result of the experiment on the LPBA dataset is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Obviously, most of the methods produce severe misregistration in the yellow regions of Fig. <ref type="figure" target="#fig_3">4</ref>, due to the existence of large deformation. Compared with the registration results of RCN and NICE-Net, the proposed method achieves better alignment on the fine structure, which can be seen in the areas indicated by the red arrow in Fig. <ref type="figure" target="#fig_3">4</ref>. It can be seen, since PIViT focuses on lightweight and fast registration of large deformations, its effectiveness on fine registration tasks is still somewhat weak. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Iterations and Decoder Type:</head><p>In this section, we explore how the number of iterations and decoder type of block in the long-range correlation decoder affect the registration performance. We select three different blocks, i.e. LCD, CNN and GRU <ref type="bibr" target="#b3">[4]</ref>, to perform iterative decoding and predict low-scale deformation fields. In order to verify the effectiveness of iterative registration and how the number of iterations affects the registration effect, we performed 1 to 5 iterations for each decoder. The Dice scores corresponding to different decoders and number of iterations are shown in Table <ref type="table" target="#tab_1">2</ref>, t represents the time of iteration. Experiments are performed on LPBA and SLIVER datasets. Obviously, among the three decoders, LCD gets the best registration results in a limited number of iterations. When the number of iterations is 1, the proposed structure degenerates into pyramid registration, and when the number of iterations is greater than or equal to 2, the pyramid-iteration structure is used. Obviously, the registration accuracy is greatly improved compared with the pyramid structure when the number of iterations is 2. On this task, when the number of iterations reaches 3, the registration accuracy tends to be stable, which indicates that the large deformation has been basically captured. Compared with the GRU block commonly used in optical flow tasks, LCD requires fewer iterations to converge, which verifies that LCD can better capture long-distance correlation and learn accurate flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose an unsupervised pyramid-iterative vision Transformer (PIViT) for large deformation image registration. PIViT is an iterative and pyramid composite framework to achieve fine registration of large deformable images by iterative registration of low-scale feature maps and pyramid feature supplementation on high-scale feature maps. Furthermore, in the iterative decoding stage, a Swin Transformer-based long-range correlation decoder is introduced to capture the long-distance dependencies between feature maps, which further improves the ability to handle large deformation. Experiments on brain MRI scans and liver CT scans demonstrate that our method can accurately register 3D large deformation medical images. Furthermore, our method has significant advantages in terms of parameters and time, which can make it more suitable for time-sensitive tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Network architecture (a) iterative registration, (b) pyramid registration and (c) the proposed pyramid-iterative registration.</figDesc><graphic coords="2,69,96,484,16,312,13,71,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed PIViT. The number of pyramid levels N is set as 3 for illustration.</figDesc><graphic coords="3,42,30,383,60,339,97,158,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of the structure of the proposed long-range correlation decoder (LCD). The red box indicates the Swin Transformer-based block. (Color figure online)</figDesc><graphic coords="5,48,81,450,71,326,38,63,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of comparative registration results.</figDesc><graphic coords="7,47,79,234,86,328,36,142,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Table 1 shows the comparison of the proposed PIViT and other meth-Comparison among VoxelMorph, VoxelMorph-diff, DualPRNet, RCN, Trans-Morph, Swin-VoxelMorph, LKU-Net, NICE-Net and the proposed PIViT on the LPBA, SLIVER, LiTs and LSPIG datasets. * indicates that the t-test p-value between PIViT and all other methods is less than 0.05. ± 11.6 81.6 ± 5.3 0.10 s 1.99 s 45675.0K 0.86 s Swin-VoxelMorph (MICCAI 2022) 58.1 ± 5.2 15857.6 77.1 ± 6.1 69.2 ± 10.5 70.6 ± 6.5 0.12 s 6.74 s 26573.9K 1.18 s LKU-Net (MICCAI 2022) 58.9 ± 5.5 2215.7 81.4 ± 5.1 77.3 ± 7.4 73.9 ± 6.8 0.10 s 1.27 s 2037.4K 0.18 s</figDesc><table><row><cell>Method</cell><cell>LPBA</cell><cell></cell><cell>SLIVER</cell><cell>LiTs</cell><cell>LSPIG</cell><cell>GRT ↓ CRT ↓ Params ↓ TPI ↓</cell></row><row><cell></cell><cell>Dice ↑</cell><cell cols="2">|Js|≤0 ↓ Dice ↑</cell><cell>Dice ↑</cell><cell>Dice ↑</cell></row><row><cell>VoxelMorph (CVPR 2018)</cell><cell>55.3 ± 6.3</cell><cell cols="2">17353.1 73.0 ± 6.7</cell><cell>67.3 ± 8.3</cell><cell cols="2">69.0 ± 8.8 0.05 s 0.73 s 312.7K 0.14 s</cell></row><row><cell cols="2">VoxelMorph-diff (MICCAI 2019) 60.1 ± 4.9</cell><cell>0.0</cell><cell>82.8 ± 6.2</cell><cell>80.1 ± 7.1</cell><cell cols="2">76.8 ± 6.5 0.08 s 0.99 s 319.7K</cell><cell>0.22 s</cell></row><row><cell>DualPRNet (MICCAI 2019)</cell><cell>58.3 ± 4.9</cell><cell cols="2">5446.9 79.1 ± 5.6</cell><cell>77.0 ± 6.5</cell><cell cols="2">71.5 ± 7.5 0.11 s 1.59 s 581.7K</cell><cell>0.28 s</cell></row><row><cell>RCN (ICCV 2019)</cell><cell>67.6 ± 2.6</cell><cell cols="2">6559.3 80.0 ± 6.6</cell><cell>73.7 ± 8.5</cell><cell cols="2">68.8 ± 7.3 0.35 s 2.20 s 938.7K</cell><cell>0.36 s</cell></row><row><cell cols="5">TransMorph (MIA 2022) 78.7 NICE-Net (MICCAI 2022) 56.7 ± 5.6 24588.1 88.2 ± 4.1 68.1 ± 2.4 8061.6 87.1 ± 5.0 82.8 ± 6.8</cell><cell>79.6 ± 6.</cell></row></table><note><p>ods on 4 medical datasets. The Dice score, number of voxels with non-positive Jacobian determinants (|J s | ≤0 ), GPU registration time (GRT), CPU registration time (CRT), network parameters and training time per iteration (TPI) of each method are presented.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Dice score vs. different decoders and number of iterations.</figDesc><table><row><cell>t</cell><cell cols="2">Decoder Type</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell>GRU</cell><cell></cell><cell></cell><cell>LCD</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1(pyramid) 67.2</cell><cell>87.7</cell><cell>224.6K</cell><cell>0.12 s 68.3</cell><cell>88.4</cell><cell>413.7K</cell><cell>0.11 s 69.3</cell><cell>88.7</cell><cell>291.0K</cell><cell>0.12 s</cell></row><row><cell>2</cell><cell>68.2</cell><cell>88.7</cell><cell>278.7K</cell><cell>0.12 s 69.4</cell><cell>89.7</cell><cell>413.7K</cell><cell>0.12 s 69.8</cell><cell>90.4</cell><cell>355.4K</cell><cell>0.13 s</cell></row><row><cell>3</cell><cell>68.9</cell><cell>89.5</cell><cell>332.7K</cell><cell>0.12 s 69.5</cell><cell>90.2</cell><cell>413.7K</cell><cell>0.14 s 70.1</cell><cell>90.9</cell><cell>420.8K</cell><cell>0.14 s</cell></row><row><cell>4</cell><cell>68.8</cell><cell>90.3</cell><cell>386.7K</cell><cell>0.12 s 70.0</cell><cell>90.7</cell><cell>413.7K</cell><cell>0.14 s 70.0</cell><cell>90.8</cell><cell>486.1K</cell><cell>0.16 s</cell></row><row><cell>5</cell><cell>68.9</cell><cell>90.1</cell><cell>440.8K</cell><cell>0.13 s 69.9</cell><cell>91.2</cell><cell>413.7K</cell><cell>0.16 s 70.0</cell><cell>90.9</cell><cell>551.5K</cell><cell>0.19 s</cell></row></table><note><p>LPBA ↑ SLIVER ↑ Params ↓ TPI ↓ LPBA ↑ SLIVER ↑ Params ↓ TPI ↓ LPBA ↑ SLIVER ↑ Params ↓ TPI ↓</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="funder">National Nature Science Foundation of China</rs> (<rs type="grantNumber">62273150</rs>), <rs type="funder">Shanghai Natural Science Foundation</rs> (<rs type="grantNumber">22ZR1421000</rs>), <rs type="funder">Shanghai Municipal Science and Technology Committee of Shanghai Outstanding Academic Leaders Plan</rs> (<rs type="grantNumber">21XD1430600</rs>), the <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">14DZ2260800</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_y4nh9UZ">
					<idno type="grant-number">62273150</idno>
				</org>
				<org type="funding" xml:id="_Ymd8Wyk">
					<idno type="grant-number">22ZR1421000</idno>
				</org>
				<org type="funding" xml:id="_dzehn3D">
					<idno type="grant-number">21XD1430600</idno>
				</org>
				<org type="funding" xml:id="_hr9tYtC">
					<idno type="grant-number">14DZ2260800</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_57.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05735</idno>
		<title level="m">The medical segmentation decathlon</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The neuro bureau ADHD-200 preprocessed repository</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chouinard-Decorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benhajali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Margulies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Craddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="275" to="286" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (LITS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TransMorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="226" to="236" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The autism brain imaging data exchange: towards a largescale evaluation of the intrinsic brain architecture in autism</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="659" to="667" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Geometric visual similarity learning in 3D medical image self-supervised pre-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2303.00874</idno>
		<ptr target="https://arxiv.org/abs/2303.00874" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of methods for liver segmentation from CT datasets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1251" to="1265" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-stream pyramid registration network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_43" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="382" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-Net vs TransFormer: is U-Net outdated in medical image registration?</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21014-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21014-3_16" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2022</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13583</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to estimate hidden motions with global motion aggregation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9772" to="9781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-iterative coarse-to-fine registration based on single-pass deep cumulative learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_9" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast symmetric diffeomorphic image registration with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4644" to="4653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_21" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affine medical image registration with coarse-to-fine vision transformer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20835" to="20844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ways toward an early diagnosis in Alzheimer&apos;s disease: the Alzheimer&apos;s disease neuroimaging initiative (ADNI)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mueller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
	<note>Alzheimer&apos;s Dement</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Construction of a 3D probabilistic atlas of human cortical structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1064" to="1080" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">XMorpher: full transformer for deformable medical image registration via cross attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_21" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1346</biblScope>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GMFlow: learning optical flow via global matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8121" to="8130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive cascaded networks for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10600" to="10610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised 3D end-toend medical image registration with volume Tweening network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1394" to="1404" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin-VoxelMorph: a symmetric unsupervised learning model for deformable medical image registration using Swin transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
