<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising</title>
				<funder ref="#_SeURAFV">
					<orgName type="full">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
				</funder>
				<funder ref="#_amub7Eg">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_3Jr7DMT">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_wjS92Du">
					<orgName type="full">ZJLab, Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<idno type="ORCID">0000-0002-0604-3197</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University)</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai Center for Brain Science and Brain-Inspired Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="355" to="365"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B942DC29E6A24A811B923DEC576182BA</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CT denoising</term>
					<term>Deep learning</term>
					<term>Self-attention</term>
					<term>Contrastive learning</term>
					<term>Anatomical semantics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github. com/hao1635/ASCON.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the success of deep learning in the field of computer vision and image processing, many deep learning-based methods have been proposed and achieved promising results in low-dose CT (LDCT) denoising <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Typically, they employ a supervised learning setting, which involves a set of image pairs, LDCT images and their normal-dose CT (NDCT) counterparts. These methods typically use a pixel-level loss (e.g. mean squared error or MSE), which can cause over-smoothing problems.</p><p>To address this issue, a few studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> used a structural similarity (SSIM) loss or a perceptual loss <ref type="bibr" target="#b10">[11]</ref>. However, they all perform in a sample-to-sample manner and ignore the inherent anatomical semantics, which could blur details in areas with low noise levels. Previous studies have shown that the level of noise in CT images varies depending on the type of tissues <ref type="bibr" target="#b16">[17]</ref>; see an example in Fig. <ref type="figure" target="#fig_0">S1</ref> in Supplementary Materials. Therefore, it is crucial to characterize the anatomical semantics for effectively denoising diverse tissues.</p><p>In this paper, we focus on taking advantage of the inherent anatomical semantics in LDCT denoising from a contrastive learning perspective <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. To this end, we propose a novel Anatomy-aware Supervised CONtrastive learning framework (ASCON), which consists of two novel designs: an efficient selfattention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to ensure that MAC-Net can effectively extract anatomical information, diverse global-local contexts and a larger input size are necessary. However, operations on full-size CT images with self-attention are computationally unachievable due to potential GPU memory limitations <ref type="bibr" target="#b19">[20]</ref>. To address this limitation, we propose an ESAU-Net that utilizes a channel-wise self-attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> which can efficiently capture both local and global contexts by computing cross-covariance across feature channels.</p><p>Second, to exploit inherent anatomical semantics, we present the MAC-Net that employs a disentangled U-shaped architecture <ref type="bibr" target="#b24">[25]</ref> to produce global and local representations. Globally, a patch-wise non-contrastive module is designed to select neighboring patches with similar semantic context as positive samples and align the same patches selected in denoised CT and NDCT which share the same anatomical information, using an optimization method similar to the BYOL method <ref type="bibr" target="#b6">[7]</ref>. This is motivated by the prior knowledge that adjacent patches often share common semantic contexts <ref type="bibr" target="#b26">[27]</ref>. Locally, to further improve the anatomical consistency between denoised CT and NDCT, we introduce a pixel-wise contrastive module with a hard negative sampling strategy <ref type="bibr" target="#b20">[21]</ref>, which randomly selects negative samples from the pixels with high similarity around the positive sample within a certain distance. Then we use a local InfoNCE loss <ref type="bibr" target="#b17">[18]</ref> to pull the positive pairs and push the negative pairs.</p><p>Our contributions are summarized as follows. 1) We propose a novel ASCON framework to explore inherent anatomical information in LDCT denoising, which is important to provide interpretability for LDCT denoising. 2) To better explore anatomical semantics in MAC-Net, we design an ESAU-Net, which utilizes a channel-wise self-attention mechanism to capture both local and global contexts. 3) We propose a MAC-Net that employs a disentangled U-shaped architecture and incorporates both global non-contrastive and local contrastive modules. This enables the exploitation of inherent anatomical semantics at the patch level, as well as improving anatomical consistency at the pixel level. 4) Extensive experimental results demonstrate that our ASCON outperforms other state-ofthe-art methods, and provides anatomical interpretability for LDCT denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the Proposed ASCON</head><p>Figure <ref type="figure" target="#fig_0">1</ref> presents the overview of the proposed ASCON, which consists of two novel components: ESAU-Net and MAC-Net. First, given an LDCT image, X ∈ R 1×H×W , where H × W denotes the image size. X is passed through the ESAU-Net to capture both global and local contexts using a channel-wise self-attention mechanism and obtain a denoised CT image Y ∈ R 1×H×W .</p><p>Then, to explore inherent anatomical semantics and remain inherent anatomical consistency, the denoised CT Y and NDCT Y are passed to the MAC-Net to compute a global MSE loss L global in a patch-wise non-contrastive module and a local infoNCE loss L local in a pixel-wise contrastive module. During training, we use an alternate learning strategy to optimize ESAU-Net and MAC-Net separately, which is similar to GAN-based methods <ref type="bibr" target="#b9">[10]</ref>. Please refer to Algorithm S1 in Supplementary Materials for a detailed optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Self-attention-Based U-Net</head><p>To better leverage anatomical semantic information in MAC-Net and adapt to the high-resolution input, we design the ESAU-Net that can capture both local and global contexts during denoising. Different from previous works that only use self-attention in the coarsest level <ref type="bibr" target="#b19">[20]</ref>, we incorporate a channel-wise selfattention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref> at each up-sampling and down-sampling level in the U-Net <ref type="bibr" target="#b21">[22]</ref> and add an identity mapping in each level, as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>.</p><p>Specifically, in each level, given the feature map F l-1 as the input, we first apply a 1×1 convolution and a 3×3 depth-wise convolution to aggregate channelwise contents and generate query (Q), key (K), and value (V ) followed by a reshape operation, where Q ∈ R C×HW , K ∈ R C×HW , and V ∈ R C×HW (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>). Then, a channel-wise attention map A ∈ R C×C is generated through a dot-product operation by the reshaped query and key, which is more efficient than the regular attention map of size HW × HW <ref type="bibr" target="#b2">[3]</ref>, especially for high-resolution input. Overall, the process is defined as</p><formula xml:id="formula_0">Attention(F ) = w(V T A) = w(V T • Softmax(KQ T /α)),<label>(1)</label></formula><p>where w(•) first reshapes the matrix back to the original size C ×H ×W and then performs 1 × 1 convolution; α is a learnable parameter to scale the magnitude of the dot product of K and Q. We use multi-head attention similar to the standard multi-head self-attention mechanism <ref type="bibr" target="#b2">[3]</ref>. The output of the channelwise self-attention is represented as: F l-1 = Attention(F l-1 ) + F l-1 . Finally, the output F l of each level is defined as: F l = Conv(F l-1 ) + Iden(F l-1 ), where Conv(•) is a two-layer convolution and Iden(•) is an identity mapping using a 1 × 1 convolution; refer to Fig. <ref type="figure" target="#fig_3">S2</ref>(a) for the details of ESAU-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-scale Anatomical Contrastive Network</head><p>Overview of MAC-Net. The goal of our MAC-Net is to exploit anatomical semantics and maintain anatomical embedding consistency, First, a disentangled U-shaped architecture <ref type="bibr" target="#b21">[22]</ref> is utilized to learn global representation</p><formula xml:id="formula_1">F g ∈ R 512× H 16 × W</formula><p>16 after four down-sampling layers, and learn local representation F l ∈ R 64×H×W by removing the last output layer. And we cut the connection between the coarsest feature and its upper level to make F g and F l more independent <ref type="bibr" target="#b24">[25]</ref> (see Fig. <ref type="figure" target="#fig_3">S2(b)</ref>). The online network and the target network, both using the same architecture above, handle denoised CT Y and NDCT Y , respectively, with F g and F l generated by the online network, and F g and F l generated by the target network (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). The parameters of the target network are an exponential moving average of the parameters in the online network, following the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Next, a patch-wise non-contrastive module uses F g and F g to compute a global MSE loss L global , while a pixel-wise contrastive module uses F l and F l to compute a local infoNCE loss L local . Let us describe these two loss functions specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Wise Non-contrastive Module.</head><p>To better learn anatomical representations, we introduce a patch-wise non-contrastive module, also shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Specifically, for each pixel f (i) g ∈ R 512 in the F g where i ∈ {1, 2, . . . , HW  256 } is the index of the pixel location, it can be considered as a patch due to the expanded receptive field achieved through a sequence of convolutions and down-sampling operations <ref type="bibr" target="#b18">[19]</ref>. To identify positive patch indices, we adopt a neighboring positive matching strategy <ref type="bibr" target="#b26">[27]</ref>, assuming that a semantically similar patch f (j) g exists in the vicinity of the query patch f (i) g , as neighboring patches often share a semantic context with the query. We empirically consider a set of 8 neighboring patches. To sample patches with similar semantics around the query patch f (i) g , we measure the semantic closeness between the query patch f (i) g and its neighboring patches f (j) g using the cosine similarity, which is formulated as</p><formula xml:id="formula_2">s(i, j) = (f (i) g ) (f (j) g )/ f (i) g 2 f (j) g 2 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>We then select the top-4 positive patches {f</p><formula xml:id="formula_4">(j)</formula><p>g } j∈P (i) based on s(i, j), where P (i) is a set of selected patches (i.e., |P (i) | = 4). To obtain patch-level features g (i) ∈ R 512 for each patch f (i) g and its positive neighbors, we aggregate their features using global average pooling (GAP) in the patch dimension. For the local representation of f (i) g , we select positive patches as same as P (i) , i.e., {f (j) g } j∈P (i) . Formally,</p><formula xml:id="formula_5">g (i) := GAP(f (i) g , {f (j) g } j∈P (i) ), g (i) := GAP(f (i) g , {f (j) g } j∈P (i) ). (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>From the patch-level features, the online network outputs a projection z g (i) = p g (g (i) ) and a prediction q (z g (i) ) while target network outputs the target projection z g (i) = p g (g (i) ). The projection and prediction are both multilayer perceptron (MLP). Finally, we compute the global MSE loss between the normalized prediction and target projection <ref type="bibr" target="#b6">[7]</ref>,</p><formula xml:id="formula_7">L global = i∈N g pos q (z g (i) ) -z g (i) 2 2 = i∈N g pos 2 -2 • q (z g (i) ),zg (i) q (z g (i) ) 2• zg (i) 2 ,<label>(4)</label></formula><p>where N g pos is the indices set of positive samples in the patch-level embedding. Pixel-Wise Contrastive Module. In this module, we aim to improve anatomical consistency between the denoised CT and NDCT using a local InfoNCE loss <ref type="bibr" target="#b17">[18]</ref> (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). First, for a query f l (i) ∈ R 64 in the F l and its positive sample f (i) l ∈ R 64 in the F l (i ∈ {1, 2, . . . , HW } is the location index), we use a hard negative sampling strategy <ref type="bibr" target="#b20">[21]</ref> to select "diffcult" negative samples with high probability, which enforces the model to learn more from the fine-grained details. Specifically, candidate negative samples are randomly sampled from F l as long as their distance from f (i) l is less than m pixels (m = 7). We also use cosine similarity in Eq. ( <ref type="formula" target="#formula_2">2</ref>) to select a set of semantically closest pixels, i.e. {f </p><formula xml:id="formula_8">(i) l ∈ R (2+|N (i)</formula><p>neg |)×256 . The local InfoNCE loss in the pixel level is defined as</p><formula xml:id="formula_9">L local = - i∈N l pos log exp v (i) l •v (i) l /τ exp v (i) l •v (i) l /τ + |N (i) neg | j=1 exp v (i) l •v (j) l /τ , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where N l pos is the indices set of positive samples in the pixel level. v</p><formula xml:id="formula_11">(i) l , v l (i) ,</formula><p>and v (j) l ∈ R 256 are the query, positive, and negative sample in z (i) l , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Total Loss Function</head><p>The final loss is defined as L = L global + L local + λL pixel , where L pixel consists of two common supervised losses: MSE and SSIM, defined as L pixel = L MSE + L SSIM . λ is empirically set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>We use two publicly available low-dose CT datasets released by the NIH AAPM-Mayo Clinic Low-Dose CT Grand Challenge in 2016 <ref type="bibr" target="#b14">[15]</ref> and lately released in 2020 <ref type="bibr" target="#b15">[16]</ref>, denoted as Mayo-2016 and Mayo-2020, respectively. There is no overlap between the two datasets. neg | = 24 for Mayo-2020. We use a binary function to filter the background while selecting queries in MAC-Net. For the training strategy, we employ a window of <ref type="bibr">[-1000, 2000</ref>] HU. We train our network for 100 epochs on 2 NVIDIA GeForce RTX 3090, and use the AdamW optimizer <ref type="bibr" target="#b13">[14]</ref> with the momentum parameters β 1 = 0.9, β 2 = 0.99 and the weight decay of 1.0 × 10 -9 . We initialize the learning rate as 1.0×10 -4 , gradually reduced to 1.0×10 -6 with the cosine annealing <ref type="bibr" target="#b12">[13]</ref>. Since MAC-Net is only implemented during training, the testing time of ASCON is close to most of the compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparisons</head><p>Quantitative Evaluations. We use three widely-used metrics including peak signal-to-noise ratio (PSNR), root-mean-square error (RMSE), and SSIM. Table <ref type="table" target="#tab_0">1</ref> presents the testing results on Mayo-2016 and Mayo-2020 datasets. We compare our methods with 5 state-of-the-art methods, including RED-CNN <ref type="bibr" target="#b0">[1]</ref>, WGAN-VGG <ref type="bibr" target="#b25">[26]</ref>, EDCNN <ref type="bibr" target="#b11">[12]</ref>, DU-GAN <ref type="bibr" target="#b8">[9]</ref>, and CNCL <ref type="bibr" target="#b5">[6]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows that our ESAU-Net with MAC-Net achieves the best performance on both the Mayo-2016 and the Mayo-2020 datasets. Compared to the ESAU-Net, ASCON further improves the PSNR by up to 0.54 dB on Mayo-2020, which demonstrates the effectiveness of the proposed MAC-Net and the importance of the inherent anatomical semantics during CT denoising. We also compute the contrast-tonoise ratio (CNR) to assess the detectability of a selected area of low-contrast lesion and our ASCON achieves the best CNR in Fig. <ref type="figure" target="#fig_4">S3</ref>.    Visualization of Inherent Semantics. To demonstrate that our MAC-Net can exploit inherent anatomical semantics of CT images during denoising, we select the features before the last layer in ASCON without MAC-Net and ASCON from Mayo-2016. Then we cluster these two feature maps respectively using a K-means algorithm and visualize them in the original dimension, and finally visualize the clustering representations using t-SNE, as shown in Fig. <ref type="figure" target="#fig_4">3</ref>. Note that ASCON produces a result similar to organ semantic segmentation after clustering and the intra-class distribution is more compact, as well as the inter-class separation is more obvious. To the best of our knowledge, this is the first time that anatomical semantic information has been demonstrated in a CT denoising task, providing interpretability to the field of medical image reconstruction.</p><p>Ablation Studies. We start with a ESAU-Net using MSE loss and gradually insert some loss functions and our MAC-Net. Table <ref type="table" target="#tab_2">2</ref> presents the results of different loss functions. It shows that both the global non-contrastive module and local contrastive module are helpful in obtaining better metrics due to the capacity of exploiting inherent anatomical information and maintaining anatomical consistency. Then, we add our MAC-Net to two supervised models: RED-CNN <ref type="bibr" target="#b0">[1]</ref> and U-Net <ref type="bibr" target="#b21">[22]</ref> but it is less effective, which demonstrates the importance of our ESAU-Net that captures both local and global contexts during denoising in Table <ref type="table" target="#tab_0">S1</ref>. In addition, we evaluate the effectiveness of the training strategies including alternate learning, neighboring positive matching and hard negative sampling in Table <ref type="table" target="#tab_2">S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we explore the anatomical semantics in LDCT denoising and take advantage of it to improve the denoising performance. To this end, we propose an Anatomy-aware Supervised CONtrastive learning framework (ASCON), consisting of an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net), which can capture both local and global contexts during denoising and exploit inherent anatomical information. Extensive experimental results on Mayo-2016 and Mayo-2020 datasets demonstrate the superior performance of our method, and the effectiveness of our designs. We also validated that our method introduces interpretability to LDCT denoising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed ASCON. (a) Efficient self-attention-based U-Net (ESAU-Net); and (b) multi-scale anatomical contrastive network (MAC-Net).</figDesc><graphic coords="3,70,47,53,78,311,29,225,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(j) l } j∈N (i) neg . Then we concatenate f (i) l , f (i) l , and {f (j) l } j∈N (i) neg and map them to a K-dimensional vector (K=256) through a two-layer MLP, obtaining z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Mayo-2016 includes normal-dose abdominal CT images of 10 anonymous patients and corresponding simulated quarter-dose CT images. Mayo-2020 provides the abdominal CT image data of 100 patients with 25% of the normal dose, and we randomly choose 20 patients for our experiments. For the Mayo-2016, we choose 4800 pairs of 512 × 512 images from 8 patients for the training and 1136 pairs from the rest 2 patients as the test set. For the Mayo-2020, we employ 9600 image pairs with a size of 256 × 256 from randomly selected 16 patients for training and 580 pairs of 512 × 512 images from rest 4 patients for testing. The use of large-size training is to adapt our MAC-Net to exploit inherent semantic information. The default sampling hyper-parameters for Mayo-2016 are |N l pos | = 32, |N g pos | = 512, |N (i) neg | = 24, while |N l pos | = 16, |N g pos | = 256, |N (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Transverse CT images and corresponding difference images from the Mayo-2016 dataset: (a) NDCT; (b) LDCT; (c) RED-CNN [1]; (d) EDCNN [12]; (e) DU-GAN [9]; (f) ESAU-Net (ours); and (g) ASCON (ours). The display window is [-160, 240] HU.</figDesc><graphic coords="7,66,48,429,41,319,57,134,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of inherent semantics; (a) NDCT; (b) clustering and t-SNE results of ASCON w/o MAC-Net; and (c) clustering and t-SNE results of ASCON.</figDesc><graphic coords="8,57,81,194,60,308,98,52,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the Mayo-2016 and Mayo-2020 datasets in terms of PSNR [dB], RMSE [×10 -2 ], and SSIM [%]WGAN-VGG<ref type="bibr" target="#b25">[26]</ref> 42.49±1.28 0.76±0.12 96.16±1.30 46.88±1.81 0.46±0.10 98.15±0.20 ±1.26 0.61 ±0.09 97.47 ±0.87 48.31 ±1.87 0.40 ±0.12 99.30 ±0.<ref type="bibr" target="#b17">18</ref> </figDesc><table><row><cell>Methods</cell><cell>Mayo-2016</cell><cell></cell><cell></cell><cell>Mayo-2020</cell><cell></cell></row><row><cell></cell><cell>PSNR↑</cell><cell>RMSE↓</cell><cell>SSIM↑</cell><cell>PSNR↑</cell><cell>RMSE↓</cell><cell>SSIM↑</cell></row><row><cell>U-Net [22]</cell><cell cols="6">44.13±1.19 0.64±0.12 97.38±1.09 47.67±1.64 0.43±0.09 99.19±0.23</cell></row><row><cell>RED-CNN [1]</cell><cell cols="6">44.23±1.26 0.62±0.09 97.34±0.86 48.05±2.14 0.41±0.11 99.28±0.18</cell></row><row><cell>EDCNN [12]</cell><cell cols="6">43.14±1.27 0.70±0.11 96.45±1.36 47.90±1.27 0.41±0.08 99.14±0.17</cell></row><row><cell>DU-GAN [9]</cell><cell cols="6">43.06±1.22 0.71±0.10 96.34±1.12 47.21±1.52 0.44±0.10 99.00±0.21</cell></row><row><cell>CNCL [6]</cell><cell cols="6">43.06±1.07 0.71±0.10 96.68±1.11 45.63±1.34 0.53±0.11 98.92±0.59</cell></row></table><note><p>ESAU-Net (ours) 44.38</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>ASCON (ours) 44.48±1.32 0.60±0.10 97.49±0.86 48.84±1.68 0.37±0.11 99.32±0.18</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation results of Mayo-2020 on the different types of loss functions.</figDesc><table><row><cell>Loss</cell><cell>PSNR↑</cell><cell>RMSE↓</cell><cell>SSIM↑</cell></row><row><cell>LMSE</cell><cell cols="3">48.34±2.22 0.40±0.11 99.27±0.18</cell></row><row><cell>LMSE + L Perceptual</cell><cell cols="3">47.83±1.99 0.42±0.10 99.13±0.19</cell></row><row><cell>LMSE + LSSIM</cell><cell cols="3">48.31±1.87 0.40±0.12 99.30±0.18</cell></row><row><cell>LMSE + LSSIM + L global</cell><cell cols="3">48.58±2.12 0.39±0.10 99.31±0.17</cell></row><row><cell>LMSE + LSSIM + L local</cell><cell cols="3">48.48±2.37 0.38±0.11 99.31±0.18</cell></row><row><cell>LMSE + LSSIM+ L</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>local + L</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>global 48.84±1.68 0.37±0.11 99.32±0.18</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62101136</rs>), <rs type="programName">Shanghai Sailing Program</rs> (No. <rs type="grantNumber">21YF1402800</rs>), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (No. <rs type="grantNumber">2018SHZDZX01</rs>) and <rs type="funder">ZJLab, Shanghai Municipal of Science and Technology Project</rs> (No. <rs type="grantNumber">20JC1419500</rs>), and <rs type="funder">Shanghai Center for Brain Science and Brain-inspired Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_amub7Eg">
					<idno type="grant-number">62101136</idno>
					<orgName type="program" subtype="full">Shanghai Sailing Program</orgName>
				</org>
				<org type="funding" xml:id="_3Jr7DMT">
					<idno type="grant-number">21YF1402800</idno>
				</org>
				<org type="funding" xml:id="_wjS92Du">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_SeURAFV">
					<idno type="grant-number">20JC1419500</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 34.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">LIT-Former: Linking in-plane and throughplane transformers for simultaneous CT image denoising and deblurring</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10630</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">CoreDiff: Contextual errormodulated generalized diffusion model for low-dose CT denoising and generalization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01814</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CoCoDiff: a contextual conditional diffusion model for low-dose CT image denoising</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Developments in X-Ray Tomography XIV</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12242</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Content-noise complementary learning for medical image denoising</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="419" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DU-GAN: generative adversarial networks with dual-domain U-Net-based discriminators for low-dose CT denoising</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EDCNN: edge enhancement-based densely connected network with compound loss for low-dose CT denoising</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-dose CT for the detection and classification of metastatic liver lesions: results of the 2016 low dose CT grand challenge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="339" to="e352" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-dose CT image and projection dataset</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Moen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="902" to="911" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Organ-based tube current modulation in chest CT. A comparison of three vendors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Mussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiography</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-719" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net transformer: self and cross attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rambour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Themyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87589-328" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3-D convolutional encoder-decoder network for low-dose CT via transfer learning from a 2-D trained network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1522" to="1534" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SAM: self-supervised learning of pixel-wise anatomical embeddings in radiological images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2658" to="2669" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Low-dose CT image denoising using a generative adversarial network with Wasserstein distance and perceptual loss</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1348" to="1357" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Patch-level representation learning for selfsupervised vision transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8354" to="8363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5728" to="5739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
