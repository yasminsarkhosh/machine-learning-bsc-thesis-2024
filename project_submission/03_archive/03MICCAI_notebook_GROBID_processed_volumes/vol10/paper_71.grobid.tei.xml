<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration</title>
				<funder ref="#_4tmGhkX">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Meng</surname></persName>
							<idno type="ORCID">0000-0002-9562-1613</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Translational Medicine</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lei</forename><surname>Bi</surname></persName>
							<email>lei.bi@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0001-9759-0200</idno>
							<affiliation key="aff1">
								<orgName type="department">Institute of Translational Medicine</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Fulham</surname></persName>
							<idno type="ORCID">0000-0003-0602-6319</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Molecular Imaging</orgName>
								<orgName type="institution">Royal Prince Alfred Hospital</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dagan</forename><surname>Feng</surname></persName>
							<idno type="ORCID">0000-0002-3381-214X</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Med-X Research Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinman</forename><surname>Kim</surname></persName>
							<idno type="ORCID">0000-0001-5960-1060</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="750" to="760"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F3F6CB1BFED29A453CFD6DB13C7647C7</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Registration</term>
					<term>Coarse-to-fine Registration</term>
					<term>Transformer Supplementary Information The online version contains supplementary material available at</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address this limitation for their capabilities to capture long-range dependency, but the benefits of using transformers for NICE registration have not been explored. In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the first deep registration method that (i) performs joint affine and deformable coarse-to-fine registration within a single network, and (ii) embeds transformers into a NICE registration framework to model long-range relevance between images. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image registration is a fundamental requirement for medical image analysis and has been an active research focus for decades <ref type="bibr" target="#b0">[1]</ref>. It aims to find a spatial transformation between a pair of fixed and moving images, through which the moving image can be warped to spatially align with the fixed image. Similar to natural image registration <ref type="bibr" target="#b1">[2]</ref>, medical image registration usually requires affine registration to eliminate rigid misalignments and then performs additional deformable registration to address non-rigid deformations. Traditional methods usually formulate medical image registration as a time-consuming iterative optimization problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, deep registration methods based on deep learning have been widely adopted to perform end-to-end registration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Deep registration methods learn a mapping from image pairs to spatial transformations based on training data in an unsupervised manner, which have shown advantages in registration accuracy and computational efficiency <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>.</p><p>Many deep registration methods perform coarse-to-fine registration to improve registration accuracy, where the registration is decoupled into multiple coarse-to-fine registration steps that are iteratively performed by using multiple cascaded networks <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> or repeatedly running a single network for multiple iterations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">15]</ref>. Mok et al. <ref type="bibr" target="#b12">[13]</ref> proposed a Laplacian pyramid Image Registration Network (LapIRN), where multiple networks at different pyramid levels were cascaded. Shu et al. <ref type="bibr" target="#b13">[14]</ref> proposed to use a single network (ULAE-net) to perform coarse-to-fine registration with multiple iterations. These methods perform iterative coarse-to-fine registration and extract image features repeatedly in each iteration, which inevitably increases computational loads and prolongs the registration runtime. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration with a single network in a single iteration <ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. For example, we previously proposed a NICE registration network (NICE-Net) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>, where multiple coarse-to-fine registration steps are performed with a single network in a single iteration. These NICE registration methods show advantages in both registration accuracy and runtime on the benchmark task of intra-patient brain MRI registration. Nevertheless, we identified that existing NICE registration methods still have two main limitations.</p><p>Firstly, existing NICE registration methods merely focus on deformable coarseto-fine registration, while affine registration, a common prerequisite, is still reliant on traditional registration methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref> or extra affine registration networks <ref type="bibr" target="#b17">[17]</ref>. Using traditional registration methods incurs time-consuming iterative optimization, while cascading extra networks consumes additional computational resources (e.g., extra GPU memory and runtime). Secondly, existing NICE registration methods are based on Convolution Neural Networks (CNN) and thus are limited by the intrinsic locality (i.e., limited receptive field) of convolution operations. Transformers have been widely adopted in many medical applications for their capabilities to capture long-range dependency <ref type="bibr" target="#b20">[20]</ref>. Recently, transformers have also been shown to improve registration with conventional Voxelmorph <ref type="bibr" target="#b6">[7]</ref>-like architecture <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>. However, the benefits of using transformers for NICE registration have not been explored.</p><p>In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for joint affine and deformable registration. Our technical contributions are two folds: (i) We extend the existing NICE registration framework to affine registration, where multiple steps of both affine and deformable coarse-to-fine registration are performed with a single network in a single iteration. (ii) We explore the benefits of transformers for NICE registration, where Swin Transformer <ref type="bibr" target="#b24">[24]</ref> is embedded into the NICE-Trans to model long-range relevance between fixed and moving images. This is the first deep registration method that integrates previously separated affine and deformable coarse-to-fine registration into a single network, and this is also the first deep registration method that exploits transformers for NICE registration. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Image registration aims to find a spatial transformation φ that warps a moving image I m to a fixed image I f , so that the warped image I m•φ = I m • φ is spatially aligned with the I f . In this study, we assume the I m and I f are two single-channel, grayscale volumes defined in a 3D spatial domain ⊂ R 3 , which is consistent with common medical image registration studies <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. The φ is parameterized as a displacement field, and we parametrized the image registration problem as a function R θ (I f , I m ) = φ using NICE-Trans. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our NICE-Trans consists of an intra-image feature learning encoder and an inter-image relevance modeling decoder (refer to Sect. 2.1). Multiple steps of affine and deformable registration are performed within a single network iteration (refer to Sect. 2.2). The θ is a set of learnable parameters that are optimized through unsupervised learning (refer to Sect. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-iterative Coarse-to-fine Transformer Networks (NICE-Trans)</head><p>The architecture of the proposed NICE-Trans is presented in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of a dual-path encoder to learn image features from I m and I f separately and a single-path decoder to model the spatial relevance between I m and I f . Skip connections are used at multiple scales to propagate features from the encoder to the decoder. Here, we assume the NICE-Trans performs L a and L d steps of affine and deformable registration, resulting in a total of L = L a + L d steps of coarse-to-fine registration.</p><p>The encoder has two identical, weight-shared paths P m and P f that take I m and I f as input, respectively. Each path consists of L successive Conv modules with 2 × 2 × 2 max pooling applied between two adjacent modules, which produces two L-level feature pyramids</p><formula xml:id="formula_0">F m ∈ F m 1 , F m 2 , . . . , F m L and F f ∈ F f 1 , F f 2 , . . . , F f L ,</formula><p>where the F f i and F m i are the output of the i th Conv module in the P f and P m . Each Conv module consists of two 3 × 3 × 3 convolutional layers followed by LeakyReLU activation with parameter 0.2. This dual-path design can learn uncoupled image features of I m and I f , which enables the NICE-Trans to reuse the learned features at multiple registration steps, thereby discarding the requirement for repeated feature learning.</p><p>The decoder consists of L-1 SwinTrans modules and a Conv module, with a patch expanding layer <ref type="bibr" target="#b23">[23]</ref> applied between two adjacent modules to double the feature resolution and halve the feature dimension. Each SwinTrans module consists of one 1 × 1 × 1 convolutional layer for feature dimension reduction and four successive Swin Transformer blocks <ref type="bibr" target="#b24">[24]</ref> including layer normalization, Window/Shifted Window-based Multi-head Self-Attention (W/SW-MSA), Multilayer Perceptron (MLP), and residual connections. The output of each decoder module is fed into an affine or deformable registration head that maps the input features into a displacement field, which produces L displacement fields φ i ∈ {φ 1 , φ 2 , . . . , φ L } for L steps of coarse-to-fine registration (detailed in Sect. 2.2). The output of each patch expanding layer is concatenated with</p><formula xml:id="formula_1">F f i and F m i • φ i-1</formula><p>, which is then fed into its later decoder module. The decoder performs finer registration after each decoder module, where the φ L is the final output φ. Detailed architecture settings (e.g., feature dimensions, head numbers of self-attention) are presented in the supplementary materials.</p><p>Our NICE-Trans differs from the existing NICE-Net <ref type="bibr" target="#b18">[18]</ref> mainly in two aspects: (i) our NICE-Trans integrates affine and deformable registration into a unified network, and (ii) our NICE-Trans leverages Swin Transformer to model long-range spatial relevance between I m and I f . In addition, the existing NICE-Net extracts features from the intermediately warped image at each registration step, while our NICE-Trans directly warps the F m to avoid this process and achieves similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Affine and Deformable Registration</head><p>The output features of the first L a decoder modules are fed into L a affine registration heads, where the features are mapped to a 3 × 4 affine matrix through global average pooling and two fully-connected layers, which are then sampled as a dense displacement field. After the first L a steps of affine registration, the output features of the last L d decoder modules are fed into L d deformable registration heads, where the features are directly mapped to a dense displacement field via a 3 × 3 × 3 convolutional layer.</p><p>At the beginning of coarse-to-fine registration, the φ 1 is the output of the first registration head. Then, the φ 1 is upsampled (×2) and voxel-wisely added to the output of the second registration head to derive φ 2 . This process is repeated until the φ L is derived, which realizes joint affine and deformable coarse-to-fine registration. In our experiments, we set L a and L d as 1 and 4 (illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>) as this setting achieved the best validation results (refer to the supplementary materials). Figure <ref type="figure" target="#fig_1">2</ref> exemplifies a registration result of the NICE-Trans with five steps of coarse-to-fine registration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised Learning</head><p>The learnable parameters θ are optimized using an unsupervised loss L that does not require labels. The L is defined as L = L sim + σ L reg , where the L sim is an image similarity term that penalizes the differences between the warped image I m•φ and the fixed image I f , the L reg is a regularization term that encourages smooth and invertible transformations φ, and the σ is a regularization parameter.</p><p>We adopt negative local normalized cross-correlation (NCC) as the L sim , which is a widely used similarity metric in image registration methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. For the L reg , we impose a diffusion regularizer on the φ to encourage its smoothness and also adopt a Jacobian Determinant (JD) loss <ref type="bibr" target="#b25">[25]</ref> to enhance its invertibility. As the φ is not invertible at voxel p where the Jacobian determinant is negative (|J φ(p)| ≤ 0) <ref type="bibr" target="#b26">[26]</ref>, the JD loss explicitly penalizes the negative Jacobian determinants of φ. Finally, the L reg is defined as , where the λ is a regularization parameter balancing registration accuracy and transformation invertibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Preprocessing</head><p>We evaluated the proposed NICE-Trans on the task of inter-patient brain MRI registration, which is a common benchmark task in medical image registration studies <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. We followed the dataset settings in <ref type="bibr" target="#b18">[18]</ref>: 2,656 brain MRI images acquired from four public datasets (ADNI <ref type="bibr" target="#b27">[27]</ref>, ABIDE <ref type="bibr" target="#b28">[28]</ref>, ADHD <ref type="bibr" target="#b29">[29]</ref>, and IXI <ref type="bibr" target="#b30">[30]</ref>) were used for training; two public brain MRI datasets with anatomical segmentation (Mindboggle <ref type="bibr" target="#b31">[31]</ref> and Buckner <ref type="bibr" target="#b32">[32]</ref>) were used for validation and testing. The Mindboggle dataset contains 100 MRI images and were randomly split into 50/50 images for validation/testing. The Buckner dataset contains 40 MRI images and were used for testing only. In addition to the original settings of <ref type="bibr" target="#b18">[18]</ref>, we adopted an additional public brain MRI dataset (LPBA <ref type="bibr" target="#b33">[33]</ref>) for testing, which contains 40 MRI images.</p><p>We performed brain extraction and intensity normalization for each MRI image with FreeSurfer <ref type="bibr" target="#b32">[32]</ref>. Each image was placed at the same position via Center of Mass (CoM) initialization <ref type="bibr" target="#b34">[34]</ref>, and then was cropped into 144 × 192 × 160 voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implemented our NICE-Trans using PyTorch on a NVIDIA Titan V GPU with 12 GB memory. We used an ADAM optimizer with a learning rate of 0.0001 and a batch size of 1 to train the NICE-Trans for 100,000 iterations. At each iteration, two images were randomly picked from the training data as the fixed and moving images. A total of 100 image pairs, randomly picked from the validation data, were used to monitor the training process and to optimize hyper-parameters. We set σ as 1 to ensure that the L sim and σ L reg have close values, while the λ was set as 10 -4 to ensure that the percentage of voxels with negative Jacobian determinants is less than 0.05% (refer to the supplementary materials for detailed regularization analysis). Our code will be available in https://github.com/ MungoMeng/Registration-NICE-Trans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Methods</head><p>Our NICE-Trans was compared with nine image registration methods, including two traditional methods and seven deep registration methods. The compared traditional methods are SyN <ref type="bibr" target="#b2">[3]</ref> and NiftyReg <ref type="bibr" target="#b3">[4]</ref>. For these methods, we used cross-correlation as the similarity measure and adopted FLIRT <ref type="bibr" target="#b35">[35]</ref> for affine registration. The compared deep registration methods are VoxelMorph (VM) <ref type="bibr" target="#b6">[7]</ref>, Diffeomorphic VoxelMorph (DifVM) <ref type="bibr" target="#b7">[8]</ref>, TransMorph <ref type="bibr" target="#b21">[21]</ref>, Swin-VoxelMorph (Swin-VM) <ref type="bibr" target="#b22">[22]</ref>, LapIRN <ref type="bibr" target="#b12">[13]</ref>, ULAE-net <ref type="bibr" target="#b13">[14]</ref>, and NICE-Net <ref type="bibr" target="#b18">[18]</ref>. The VM and DifVM are two commonly benchmarked registration methods in the literature <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>. The TransMorph and Swin-VM are two state-of-the-art methods that embed Swin Transformer into VM-like architecture. The LapIRN, ULAE-net, and NICE-Net are three state-of-the-art coarse-to-fine registration methods. For the compared deep registration methods, we adopted NCC as the similarity loss and followed <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b36">36]</ref> to cascade a CNN-based registration network (AffineNet) for affine registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Settings</head><p>We compared the NICE-Net to the nine comparison methods for subject-to-subject registration. For testing, we randomly picked 100 image pairs from each of the Mindboggle, Buckner, and LPBA testing sets. We used standard evaluation metrics for medical image registration <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref>. The registration accuracy was evaluated using the Dice similarity coefficients (DSC) of segmentation labels, while the smoothness and invertibility of spatial transformations were evaluated using the percentage of Negative Jacobian Determinants (NJD). Generally, a higher DSC and a lower NJD indicate better registration performance. A two-sided P value less than 0.05 is considered to indicate a statistically significant difference between two DSCs.</p><p>We also performed an ablation study to explore the benefits of transformers. We built a baseline method that has the same architecture as the NICE-Trans but only uses Conv modules. After that, we embedded Swin Transformer into the baseline method, where SwinTrans modules replaced the Conv modules in the encoder (Trans-Encoder), decoder (Trans-Decoder), or both (Trans-All).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" target="#tab_0">1</ref> presents the registration performance of our NICE-Trans and all comparison methods. The registration accuracy of all methods degraded by 1-3% in DSC when affine registration was not performed, which demonstrates the importance of affine registration. However, using FLIRT or AffineNet for affine registration incurred extra computational loads and increased the registration runtime. Our NICE-Trans performed joint affine and deformable registration, which enabled it to realize affine registration with negligible additional runtime. Moreover, we suggest that integrating affine and deformable registration into a single network also brings convenience for network training. Training two separate affine and deformable registration networks will prolong the whole training time, while joint training will consume more GPU memory. As for registration accuracy, the TransMorph and Swin-VM achieved higher DSCs than the conventional VM and DifVM, but still cannot outperform the existing CNN-based coarse-to-fine registration methods (LapIRN, ULAE-net, and NICE-Net). Our NICE-Trans leverages Swin Transformer to perform coarse-to-fine registration, which enabled it to achieve the highest DSCs among all methods. This means that our NICE-Trans also has advantages on registration accuracy. We present a qualitative comparison in the supplementary materials, which shows that the registration result produced by our NICE-Trans is more consistent with the fixed image. In addition, there usually exists a trade-off between DSC and NJD as imposing constraints on the spatial transformations limits their flexibility, which results in degraded registration accuracy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">18]</ref>. For example, compared with VM, the DifVM with diffeomorphic constraints achieved better NJDs and worse DSCs. Nevertheless, our NICE-Trans achieved both the best DSCs and NJDs. We suggest that, if we set λ as 0 to maximize the registration accuracy with the cost of transformation invertibility, our NICE-Trans can achieve higher DSCs and outperform the comparison methods by a larger margin (refer to the regularization analysis in the supplementary materials).</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the results of our ablation study. Swin Transformer improved the registration performance when embedded into the decoder, but had limited benefits in the encoder. This suggests that Swin Transformer can benefit registration in modeling inter-image spatial relevance while having limited benefits in learning intra-image representations. This finding is intuitive as image registration aims to find spatial relevance between images, instead of finding the internal relevance within an image. Under this aim, embedding transformers in the decoder helps to capture long-range relevance between images and improves registration performance. We noticed that previous studies gained improvements by embedding Swin Transformer in the encoder <ref type="bibr" target="#b21">[21]</ref> or leveraging a full transformer network <ref type="bibr" target="#b22">[22]</ref>. This is attributed to the fact that they used a VM-like architecture that entangles image representation learning and spatial relevance modeling throughout the whole network. Our NICE-Trans decouples these two parts and provides further insight on using transformers for registration: leveraging transformers to learn intra-image relevance might not be beneficial but merely incurs extra computational loads. It should be acknowledged that there are a few limitations in our study. First, the experiment (Table <ref type="table" target="#tab_0">1</ref>) demonstrated that our NICE-Trans can well address the inherent misalignments among inter-patient brain MRI images, but the sensitivity of affine registration to different degrees of misalignments is still awaiting further exploration. Second, in this study, we evaluated the NICE-Trans on the benchmark task of inter-patient brain MRI registration, while we believe that our NICE-Trans also could apply to other image registration applications (e.g., brain tumor registration <ref type="bibr" target="#b37">[37]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have outlined a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for medical image registration. Unlike the existing image registration methods, our NICE-Trans performs joint affine and deformable coarse-to-fine registration with a single network in a single iteration. The experimental results show that our NICE-Trans can outperform the state-of-the-art coarse-to-fine or transformer-based deep registration methods on both registration accuracy and runtime. Our study also suggests that transformers benefit registration in modeling inter-image spatial relevance while having limited benefits in learning intra-image representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Architecture of our NICE-Trans. The affine and deformable registration steps, L a and L d , are set as 1 and 4 for illustration.</figDesc><graphic coords="4,55,98,56,69,340,21,219,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Registration results of the NICE-Trans with L a = 1 and L d = 4. From left to right are the moving image, the images warped by 5 registration steps, and the fixed image.</figDesc><graphic coords="5,41,79,195,32,340,15,73,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Registration performance of our NICE-Trans and all comparison methods.</figDesc><table><row><cell cols="2">Method</cell><cell cols="8">Mindboggle DSC NJD (%) DSC NJD (%) DSC NJD (%) CPU Buckner LPBA Runtime (s) GPU</cell></row><row><cell cols="2">Before registration</cell><cell>0.269 *</cell><cell>/</cell><cell>0 . 3 3 0 *</cell><cell>/</cell><cell>0 . 5 3 6 *</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">FLIRT (affine only)</cell><cell>0.347 *</cell><cell>/</cell><cell>0 . 4 0 6 *</cell><cell>/</cell><cell>0 . 6 2 6 *</cell><cell>/</cell><cell>5 8 . 2</cell><cell>/</cell></row><row><cell cols="2">SyN (no affine)</cell><cell>0.535 *</cell><cell>0.25</cell><cell>0.566 *</cell><cell>0.28</cell><cell>0.674 *</cell><cell>0.12</cell><cell>3688</cell><cell>/</cell></row><row><cell cols="2">NiftyReg (no affine)</cell><cell>0.558 *</cell><cell>0.32</cell><cell>0.601 *</cell><cell>0.35</cell><cell>0.690 *</cell><cell>0.15</cell><cell>165</cell><cell>/</cell></row><row><cell>FLIRT +</cell><cell>SyN NiftyReg</cell><cell>0.548 * 0.567 *</cell><cell>0.26 0.34</cell><cell>0.577 * 0.610 *</cell><cell>0.25 0.30</cell><cell>0.692 * 0.705 *</cell><cell>0.09 0.13</cell><cell>3746 223</cell><cell>/ /</cell></row><row><cell cols="2">AffineNet (affine only)</cell><cell>0.341 *</cell><cell>/</cell><cell>0 . 4 0 0 *</cell><cell>/</cell><cell>0 . 6 1 1 *</cell><cell>/</cell><cell>1.12</cell><cell>0.118</cell></row><row><cell cols="2">VM (no affine)</cell><cell>0.518 *</cell><cell>2.63</cell><cell>0.558 *</cell><cell>2.37</cell><cell>0.663 *</cell><cell>1.21</cell><cell>3.85</cell><cell>0.395</cell></row><row><cell cols="2">DifVM (no affine)</cell><cell cols="2">0.502 * 0.042</cell><cell cols="2">0.548 * 0.032</cell><cell cols="2">0.671 * 0.005</cell><cell>3.92</cell><cell>0.446</cell></row><row><cell cols="2">TransMorph (no affine)</cell><cell>0.545 *</cell><cell>2.25</cell><cell>0.585 *</cell><cell>2.15</cell><cell>0.682 *</cell><cell>1.27</cell><cell>3.90</cell><cell>0.432</cell></row><row><cell cols="2">Swin-VM (no affine)</cell><cell cols="2">0.542 * 0.022</cell><cell cols="2">0.589 * 0.017</cell><cell cols="2">0.684 * 0.004</cell><cell>5.82</cell><cell>0.550</cell></row><row><cell cols="2">LapIRN (no affine)</cell><cell cols="2">0.563 * 0.046</cell><cell cols="2">0.599 * 0.039</cell><cell cols="2">0.688 * 0.006</cell><cell>6.52</cell><cell>0.624</cell></row><row><cell cols="2">ULAE-net (no affine)</cell><cell>0.579 *</cell><cell>2.08</cell><cell>0.611 *</cell><cell>2.00</cell><cell>0.695 *</cell><cell>1.06</cell><cell>7.21</cell><cell>0.730</cell></row><row><cell cols="2">NICE-Net (no affine)</cell><cell cols="2">0.580 * 0.048</cell><cell cols="2">0.611 * 0.034</cell><cell cols="2">0.696 * 0.004</cell><cell>4.17</cell><cell>0.423</cell></row><row><cell></cell><cell>VM</cell><cell>0.548 *</cell><cell>2.54</cell><cell>0.580 *</cell><cell>2.24</cell><cell>0.682 *</cell><cell>1.17</cell><cell>4.97</cell><cell>0.513</cell></row><row><cell></cell><cell>DifVM</cell><cell cols="2">0.526 * 0.048</cell><cell cols="2">0.565 * 0.027</cell><cell cols="2">0.686 * 0.005</cell><cell>5.04</cell><cell>0.564</cell></row><row><cell></cell><cell cols="2">TransMorph 0.568 *</cell><cell>2.14</cell><cell>0.604 *</cell><cell>2.18</cell><cell>0.694 *</cell><cell>1.10</cell><cell>5.02</cell><cell>0.550</cell></row><row><cell>AffineNet +</cell><cell>Swin-VM</cell><cell cols="2">0.563 * 0.024</cell><cell cols="2">0.607 * 0.021</cell><cell cols="2">0.696 * 0.003</cell><cell>6.94</cell><cell>0.668</cell></row><row><cell></cell><cell>LapIRN</cell><cell cols="2">0.581 * 0.042</cell><cell cols="2">0.611 * 0.036</cell><cell cols="2">0.699 * 0.006</cell><cell>7.64</cell><cell>0.742</cell></row><row><cell></cell><cell>ULAE-net</cell><cell>0.595 *</cell><cell>2.12</cell><cell>0.625 *</cell><cell>1.92</cell><cell>0.705 *</cell><cell>0.97</cell><cell>8.33</cell><cell>0.848</cell></row><row><cell></cell><cell>NICE-Net</cell><cell cols="2">0.596 * 0.034</cell><cell cols="2">0.624 * 0.026</cell><cell cols="2">0.705 * 0.004</cell><cell>5.29</cell><cell>0.541</cell></row><row><cell cols="3">NICE-Trans (affine only) 0.353 *</cell><cell>/</cell><cell>0 . 4 1 0 *</cell><cell>/</cell><cell>0 . 6 1 8 *</cell><cell>/</cell><cell>1.04</cell><cell>0.105</cell></row><row><cell cols="2">NICE-Trans (no affine)</cell><cell cols="2">0.594 * 0.018</cell><cell cols="2">0.622 * 0.016</cell><cell cols="2">0.704 * 0.003</cell><cell>4.52</cell><cell>0.480</cell></row><row><cell cols="2">NICE-Trans (ours)</cell><cell>0.612</cell><cell>0.016</cell><cell>0.636</cell><cell>0.015</cell><cell>0.715</cell><cell>0.002</cell><cell>4.69</cell><cell>0.486</cell></row></table><note><p>Bold: the best DSC and NJD in each testing dataset and the shortest runtime of completing both affine and deformable registration. *: &lt;0.05, in comparison to NICE-Trans (ours).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of our ablation study.</figDesc><table><row><cell>Method</cell><cell cols="8">Mindboggle DSC NJD (%) DSC NJD (%) DSC NJD (%) CPU Buckner LPBA Runtime (s) GPU</cell></row><row><cell>Baseline</cell><cell>0.600</cell><cell>0.028</cell><cell>0.627</cell><cell>0.025</cell><cell>0.706</cell><cell>0.003</cell><cell>4.25</cell><cell>0.438</cell></row><row><cell>Trans-Encoder</cell><cell>0.600</cell><cell>0.024</cell><cell>0.625</cell><cell>0.018</cell><cell>0.705</cell><cell>0.003</cell><cell>4.82</cell><cell>0.488</cell></row><row><cell>Trans-Decoder (ours)</cell><cell>0.612</cell><cell>0.016</cell><cell>0.636</cell><cell>0.015</cell><cell>0.715</cell><cell>0.002</cell><cell>4.69</cell><cell>0.486</cell></row><row><cell>Trans-All</cell><cell>0.612</cell><cell>0.015</cell><cell>0.634</cell><cell>0.017</cell><cell>0.714</cell><cell>0.002</cell><cell>5.48</cell><cell>0.557</cell></row></table><note><p>Bold: the best DSC and NJD in each testing dataset.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">Australian Research Council (ARC)</rs> under Grant <rs type="grantNumber">DP200103748</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4tmGhkX">
					<idno type="grant-number">DP200103748</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deformable medical image registration: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1153" to="1190" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-quality panorama stitching based on asymmetric bidirectional optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence and Applications (ICCIA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="118" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast free-form deformation using graphics processing units</title>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="284" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A review of deep learning-based three-dimensional medical image registration methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4895" to="4916" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Voxelmorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="226" to="236" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing medical image registration via appearance adjustment networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page">119444</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep learning framework for unsupervised affine and deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="128" to="143" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">mlVIRNET: multilevel variational image registration network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heldmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="257" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive cascaded networks for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10600" to="10610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Medical image registration based on uncoupled learning and accumulative enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Debruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive decomposition network for deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5130" to="5141" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual-stream pyramid registration network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102379</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint progressive and coarse-to-fine registration of brain MRI via deformation field integration and non-rigid feature fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2788" to="2802" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-iterative coarse-to-fine registration based on singlepass deep cumulative learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07876</idno>
		<title level="m">Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transmorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin-voxelmorph: a symmetric unsupervised learning model for deformable medical image registration using swin transformer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xmorpher: full transformer for deformable medical image registration via cross attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faim-a convnet method for unsupervised 3d medical image registration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schmah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMI 2019</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Suk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11861</biblScope>
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fast diffeomorphic image registration algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="113" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ways toward an early diagnosis in Alzheimer&apos;s disease: the Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimers Dement</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="659" to="667" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">the ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience</title>
		<idno>ADHD-200 consortium.:</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Syst. Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<ptr target="https://brain-development.org/ixi-dataset/" />
		<title level="m">The Information eXtraction from Images (IXI) dataset</title>
		<imprint>
			<date type="published" when="2022-10">Oct 2022</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">101 labeled brain images and a consistent human cortical labeling protocol</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tourville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FreeSurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Construction of a 3D probabilistic atlas of human cortical structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1064" to="1080" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ITK: enabling reproducible research and open science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A global optimisation method for robust affine registration of brain images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised 3D end-to-end medical image registration with volume tweening network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1394" to="1404" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The brain tumor sequence registration challenge: establishing correspondence between pre-operative and follow-up MRI scans of diffuse glioma patients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.06979</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
