<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</title>
				<funder>
					<orgName type="full">Channel 7 Children&apos;s Research Foundation of South Australia Incorporated</orgName>
					<orgName type="abbreviated">CRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vu</forename><forename type="middle">Minh Hieu</forename><surname>Phan</surname></persName>
							<idno type="ORCID">0000-0003-3861-0296</idno>
						</author>
						<author>
							<persName><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
							<idno type="ORCID">0000-0001-9965-4511</idno>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johan</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
							<idno type="ORCID">0000-0002-8336-6774</idno>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minh-Son</forename><surname>To</surname></persName>
							<idno type="ORCID">0000-0002-8060-6218</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Flinders Health and Medical Research Institute</orgName>
								<orgName type="institution" key="instit2">Flinders University</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="56" to="65"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">33488BCBFB51CE0F45F5669DF59B00F5</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Unpaired CT synthesis • Structural consistency</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image synthesis is a challenging task due to the scarcity of paired data. Several methods have applied CycleGAN to leverage unpaired data, but they often generate inaccurate mappings that shift the anatomy. This problem is further exacerbated when the images from the source and target modalities are heavily misaligned. Recently, current methods have aimed to address this issue by incorporating a supplementary segmentation network. Unfortunately, this strategy requires costly and timeconsuming pixel-level annotations. To overcome this problem, this paper proposes MaskGAN, a novel and cost-effective framework that enforces structural consistency by utilizing automatically extracted coarse masks. Our approach employs a mask generator to outline anatomical structures and a content generator to synthesize CT contents that align with these structures. Extensive experiments demonstrate that MaskGAN outperforms state-of-the-art synthesis methods on a challenging pediatric dataset, where MR and CT scans are heavily misaligned due to rapid growth in children. Specifically, MaskGAN excels in preserving anatomical structures withouttheneedforexpertannotations.Thecodeforthispapercanbefound at https://github.com/HieuPhan33/MaskGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) and computed tomography (CT) are two commonly used cross-sectional medical imaging techniques. MRI and CT produce different tissue contrast and are often used in tandem to provide complementary information. While MRI is useful for visualizing soft tissues (e.g. muscle,  <ref type="bibr" target="#b20">[20]</ref> fails to preserve the smooth anatomy of the MRI. (c) AttentionGAN <ref type="bibr" target="#b11">[12]</ref> inflates the head area in the synthetic CT, which is inconsistent with the original MRI. Quantitative evaluations in MAE (lower is better) are shown in yellow.</p><p>fat), CT is superior for visualizing bony structures. Some medical procedures, such as radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically require both MRI and CT for planning. Unfortunately, CT imaging exposes patients to ionizing radiation, which can damage DNA and increase cancer risk <ref type="bibr" target="#b8">[9]</ref>, especially in children and adolescents. Given these issues, there are clear advantages for synthesizing anatomically accurate CT data from MRI.</p><p>Most synthesis methods adopt supervised learning paradigms and train generative models to synthesize CT <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">17]</ref>. Despite the superior performance, supervised methods require a large amount of paired data, which is prohibitively expensive to acquire. Several unsupervised MRI-to-CT synthesis methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>, leverage CycleGAN with cycle consistency supervision to eliminate the need for paired data. Unfortunately, the performance of unsupervised CT synthesis methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> is inferior to supervised counterparts. Due to the lack of direct constraints on the synthetic outputs, CycleGAN <ref type="bibr" target="#b20">[20]</ref> struggles to preserve the anatomical structure when synthesizing CT images, as shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>). The structural distortion in synthetic results exacerbates when data from the two modalities are heavily misaligned, which usually occurs in pediatric scanning due to the rapid growth in children.</p><p>Recent unsupervised methods impose structural constraints on the synthesized CT through pixel-wise or shape-wise consistency. Pixel-wise consistency methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> capture and align pixel-wise correlations between MRI and synthesized CT. However, enforcing pixel-wise consistency may introduce undesirable artifacts in the synthetic results. This problem is particularly relevant in brain scanning, where both the pixel-wise correlation and noise statistics in MR and CT images are different, as a direct consequence of the signal acquisition technique. The alternative shape-wise consistency methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">19]</ref> aim to preserve the shapes of major body parts in the synthetic image. Notably, shape-CycleGAN <ref type="bibr" target="#b3">[4]</ref> segments synthesized CT and enforces consistency with the ground-truth MRI segmentation. However, these methods rely on segmentation annotations, which are time-consuming, labor-intensive, and require expert radiological annotators. A recent natural image synthesis approach, called Attention-GAN <ref type="bibr" target="#b11">[12]</ref>, learns attention masks to identify discriminative structures. Atten-tionGAN implicitly learns prominent structures in the image without using the ground-truth shape. Unfortunately, the lack of explicit mask supervision can lead to imprecise attention masks and, in turn, produce inaccurate mappings of the anatomy, as shown in Fig. <ref type="figure" target="#fig_0">1(c)</ref>. In this paper, we propose MaskGAN, a novel unsupervised MRI-to-CT synthesis method, that preserves the anatomy under the explicit supervision of coarse masks without using costly manual annotations. Unlike segmentationbased methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">18]</ref>, MaskGAN bypasses the need for precise annotations, replacing them with standard (unsupervised) image processing techniques, which can produce coarse anatomical masks. Such masks, although imperfect, provide sufficient cues for MaskGAN to capture anatomical outlines and produce structurally consistent images. Table <ref type="table" target="#tab_0">1</ref> highlights our differences compared with previous shape-aware methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. Our major contributions are summarized as follows. 1) We introduce MaskGAN, a novel unsupervised MRI-to-CT synthesis method. MaskGAN is the first framework that maintains shape consistency without relying on human-annotated segmentation. 2) We present two new structural supervisions to enforce consistent extraction of anatomical structures across MRI and CT domains. 3) Extensive experiments show that our method outperforms state-of-the-art methods by using automatically extracted coarse masks to effectively enhance structural consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>In this section, we first introduce the MaskGAN architecture, shown in Fig. <ref type="figure" target="#fig_1">2</ref>, and then describe the three supervision losses we use for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MaskGAN Architecture</head><p>The network comprises two generators, each learning an MRI-CT and a CT-MRI translation. Our generator design has two branches, one for generating masks and the other for synthesizing the content in the masked regions. The mask branch learns N attention masks A i , where the first N -1 masks capture foreground (FG) structures and the last mask represents the background (BG). The content branch synthesizes N -1 outputs for the foreground structures, denoted as C. Each output, C i , represents the synthetic content for the corresponding foreground region that is masked by the attention mask A i .</p><p>Intuitively, each channel A i in the mask tensor A focuses on different anatomical structures in the medical image. For instance, one channel emphasizes on synthesizing the skull, while another focuses on the brain tissue. The last channel A N in A corresponds to the background and is applied to the original input to preserve the background contents. The final output is the sum of masked foreground contents and masked background input. Formally, the synthetic CT output generated from the input MRI x is defined as</p><formula xml:id="formula_0">O CT = G CT (x) = A N CT x + N -1 i=1 A i CT C i CT . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The synthetic MRI output from the CT scan y is defined similarly based on the attention masks and the contents from the MR generator. The proposed network is trained using three training objectives described in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CycleGAN Supervision</head><p>The two generators, G MR and G CT , map images from MRI domain (X) and CT domain (Y ), respectively. Two discriminators, D MR and D CT , are used to distinguish real from fake images in the MRI and CT domains. The adversarial loss for training the generators to produce synthetic CT images is defined as</p><formula xml:id="formula_2">L CT (G MR , D CT , x, y) = E y∼pdata(y) log D CT (y) + E x∼pdata(x) log(1 -D CT (G MR (x))) .</formula><p>(2) The adversarial loss L MR for generating MRI images is defined in a similar manner. For unsupervised training, CycleGAN imposes the cycle consistency loss, which is formulated as follows</p><formula xml:id="formula_3">L cycle = E x∼p data (x) x -G CT (G MR (x)) + E y∼p data (y) y -G MR (G CT (y)) .</formula><p>(3) The CycleGAN's objective L GAN is the combination of adversarial and cycle consistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mask and Cycle Shape Consistency Supervision</head><p>Mask Loss. To reduce spurious mappings in the background regions, MaskGAN explicitly guides the mask generator to differentiate the foreground objects from the background using mask supervision. We extract the coarse mask B using basic image processing operations. Specifically, we design a simple but robust algorithm that works on both MRI and CT scans, with a binarization stage followed by a refinement step. In the binarization stage, we normalize the intensity to the range [0, 1] and apply a binary threshold of 0.1, selected based on histogram inspection, to separate the foreground from the background. In the post-processing stage, we refine the binary image using morphological operations, specifically employing a binary opening operation to remove small artifacts. We perform connected component analysis <ref type="bibr" target="#b10">[11]</ref> and keep the largest component as the foreground. Column 6 in Fig. <ref type="figure" target="#fig_2">3</ref> shows examples of extracted masks.</p><p>We introduce a novel mask supervision loss that penalizes the difference between the background mask A N learned from the input image and the groundtruth background mask B in both MRI and CT domains. The mask loss for the attention generators is formulated as</p><formula xml:id="formula_4">L mask = E x∼p data (x) A MR N -B MR N 1 + E y∼p data (y) A CT N -B CT N 1 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Discussion. Previous shape-aware methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">18]</ref> use a pre-trained U-Net <ref type="bibr" target="#b9">[10]</ref> segmentation network to enforce shape consistency on the generator. U-Net is pre-trained in a separate stage and frozen when the generator is trained. Hence, any errors produced by the segmentation network cannot be corrected. In contrast, we jointly train the shape extractor, i.e., the mask generator, and the content generator end-to-end. Besides mask loss L mask , the mask generator also receives supervision from adversarial loss L GAN to adjust the extracted shape and optimize the final synthetic results. Moreover, in contrast to previous methods that train a separate shape extractor, our MaskGAN uses a shared encoder for mask and content generators, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Our design embeds the extracted shape knowledge into the content generator, thus improving the structural consistency of the synthetic contents.</p><p>Cycle Shape Consistency Loss. Spurious mappings can occur when the anatomy is shifted during translation. To preserve structural consistency across domains, we introduce the cycle shape consistency (CSC) loss as our secondary contribution. Our loss penalizes the discrepancy between the background attention mask A MR N learned from the input MRI image and the mask ÃCT N learned from synthetic CT. Enforcing consistency in both domains, we formulate the shape consistency loss as</p><formula xml:id="formula_6">L shape = E x∼p data (x) A MR N -ÃCT N 1 + E y∼p data (y) A CT N -ÃMR N 1 . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The final loss for MaskGAN is the sum of three loss objectives weighted by the corresponding loss coefficients: L = L GAN + λ mask L mask + λ shape L shape .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Data Collection. We collected 270 volumetric T1-weighted MRI and 267 thinslice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols<ref type="foot" target="#foot_0">1</ref> . We targeted the age group from 6-24 months since pediatric patients are more susceptible to ionizing radiation and experience a greater cancer risk (up to 24% increase) from radiation exposure <ref type="bibr" target="#b6">[7]</ref>. Furthermore, surgery for craniosynostosis, a birth defect in which the skull bones fuse too early, typically occurs during this age <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">16]</ref>. The scans were acquired by Ingenia 3.0T MRI scanners and Philips Brilliance 64 CT scanners. We then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm 3 . The dataset comprises brain MR and CT volumes from 262 subjects. 13 MRI-CT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms. The dataset is divided into 249, 1 and 12 subjects for training, validating and testing set. Following <ref type="bibr" target="#b12">[13]</ref>, we conducted experiments on sagittal slices. Each MR and CT volume consists of 180 to 200 slices, which are resized and padded to the size of 224 × 224. The intensity range of CT is clipped into <ref type="bibr">[-1000, 2000</ref>]. All models are trained using the Adam optimizer for 100 epochs, with a learning rate of 0.0002 which linearly decays to zero over the last 50 epochs. We use a batch size of 16 and train on two NVIDIA RTX 3090 GPUs.</p><p>Evaluation Metrics. To provide a quantitative evaluation of methods, we compute the same standard performance metrics as in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> including mean absolute error (MAE), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM) between ground-truth and synthesized CT. The scope of the paper centers on theoretical development; clinical evaluations such as dose calculation and treatment planning will be conducted in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussions</head><p>Comparisons with State-of-the-Art. We compare the performance of our proposed MaskGAN with existing state-of-the-art image synthesis methods, including CycleGAN <ref type="bibr" target="#b20">[20]</ref>, AttentionGAN <ref type="bibr" target="#b11">[12]</ref>, structure-constrained CycleGAN (sc-CycleGAN) <ref type="bibr" target="#b13">[14]</ref> and shape-CycleGAN <ref type="bibr" target="#b3">[4]</ref>. Shape-CycleGAN requires annotated segmentation to train a separate U-Net. For a fair comparison, we implement shape-CycleGAN using our extracted coarse masks based on the authors' official code. Note that CT-to-MRI synthesis is a secondary task supporting the primary MRI-to-CT synthesis task. As better MRI synthesis leads to improved CT synthesis, we also report the model's performance on MRI synthesis. Table <ref type="table">2</ref>. Quantitative comparison of different methods on the primary MRI-CT task and the secondary CT-MRI task. The results of an ablated version of our proposed MaskGAN are also reported. ± standard deviation is reported over five evaluations. The paired t-test is conducted between MaskGAN and a compared method at p = 0.05. The improvement of MaskGAN over all compared methods is statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Primary: MRI-to-CT Secondary:</p><formula xml:id="formula_8">CT-to-MRI MAE ↓ PSNR ↑ SSIM (%) ↑ MAE ↓ PSNR ↑ SSIM (%) ↑</formula><p>CycleGAN <ref type="bibr" target="#b20">[20]</ref> 32.12 ± 0.31 31.57 ± 0.12 46.17 ± 0.20 34.21 ± 0.33 29.88 ± 0.24 45.73 ± 0.17 AttentionGAN <ref type="bibr" target="#b11">[12]</ref> 28.25 ± 0.25 32.88 ± 0.09 53.57 ± 0.15 30.47 ± 0.22 30.15 ± 0.10 50.66 ± 0.14 sc-CycleGAN <ref type="bibr" target="#b13">[14]</ref> 24.55 ± 0.24 32.97 ± 0.07 57.08 ± 0.11 26.13 ± 0.15 31.22 ± 0.07 54.14 ± 0.10 shape-CycleGAN <ref type="bibr" target="#b3">[4]</ref> 24.30 ± 0.28 33. Table <ref type="table">2</ref> demonstrates that our proposed MaskGAN outperforms existing methods for statistical significance of p = 0.05 in both tasks. The method reduces the MAE of CycleGAN and AttentionGAN by 29.07% and 19.36%, respectively. Furthermore, MaskGAN outperforms shape-CycleGAN, reducing its MAE by 11.28%. Unlike shape-CycleGAN, which underperforms when trained with coarse segmentations, our method obtains consistently higher results. Figure <ref type="figure" target="#fig_2">3</ref> shows the visual results of different methods. sc-CycleGAN produces artifacts (e.g., the eye socket in the first sample and the nasal cavity in the second sample), as it preserves pixel-wise correlations. In contrast, our proposed MaskGAN preserves shape-wise consistency and produces the smoothest synthetic CT. Unlike adult datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, pediatric datasets are easily misaligned due to children's rapid growth between scans. Under this challenging setting, unpaired image synthesis can have non-optimal visual results and SSIM scores. Yet, our MaskGAN achieves the highest quality, indicating its suitability for pediatric image synthesis.</p><p>We perform an ablation study by removing the cycle shape consistency loss (w/o Shape). Compared with shape-CycleGAN, MaskGAN using only a mask loss significantly reduces MAE by 6.26%. The combination of both mask and cycle shape consistency losses results in the largest improvement, demonstrating the complementary contributions of our two losses.</p><p>Robustness to Error-Prone Coarse Masks. We compare the performance of our approach with shape-CycleGAN <ref type="bibr" target="#b3">[4]</ref> using deformed masks that simulate human errors during annotation. To alter object shapes, we employ random elastic deformation, a standard data augmentation technique <ref type="bibr" target="#b9">[10]</ref> that applies random displacement vectors to objects. The level of distortion is controlled by the standard deviation of the normal distribution from which the vectors are sampled. Figure <ref type="figure" target="#fig_3">4</ref> (Left) shows MAE of the two methods under increasing levels of distortion. MAE of shape-CycleGAN drastically increases as the masks become more distorted. Figure <ref type="figure" target="#fig_3">4</ref> (Right) shows that our MaskGAN (d) better preserves the anatomy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes MaskGAN -a novel automated framework that maintains the shape consistency of prominent anatomical structures without relying on expert annotated segmentations. Our method generates a coarse mask outlining the shape of the main anatomy and synthesizes the contents for the masked foreground region. Experimental results on a clinical dataset show that MaskGAN significantly outperforms existing methods and produces synthetic CT with more consistent mappings of anatomical structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visual results (Row 1) and the error map (Row 2) between the ground-truth and synthetic CT on pediatric dataset. (a) Input MRI and the paired CT. (b) Cycle-GAN [20] fails to preserve the smooth anatomy of the MRI. (c) AttentionGAN [12] inflates the head area in the synthetic CT, which is inconsistent with the original MRI. Quantitative evaluations in MAE (lower is better) are shown in yellow.</figDesc><graphic coords="2,83,46,54,44,285,49,142,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed MaskGAN. First, we automatically extract coarse masks from the input xMR. MaskGAN then learns via two new objectives in addition to the standard CycleGAN loss. The mask loss L mask minimizes the L1 distance between the extracted background mask and the coarse mask, ensuring accurate anatomical structure generation. The cycle shape consistency (CSC) loss LCSC minimizes the L1 distance between the masks learned by the MRI and CT generators, promoting consistent anatomy segmentation across domains.</figDesc><graphic coords="4,60,96,54,56,330,88,213,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of synthesized CT images by different methods on two samples. Column 1: Input MRI (Row 1 and 3) and the corresponding paired CT scan (Row 2 and 4). Column 2-5: Synthesized CT results (Row 1 and 3) and the corresponding error maps (Row 2 and 4). Column 6: Extracted coarse background (ground-truth) masks (Row 1 and 3) and attention masks learned by our MaskGAN (Row 2 and 4).</figDesc><graphic coords="8,56,97,54,59,338,08,207,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Left : MAE of the two shape-aware methods using deformed masks. Right: Qualitative results of shape-CycleGAN (c) and our MaskGAN (d) when training using coarse masks deformed by the standard deviation of 2.0.</figDesc><graphic coords="9,52,80,54,23,318,76,109,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of different shape-aware image synthesis.</figDesc><table><row><cell>Method</cell><cell>Mask</cell><cell>Human</cell><cell>Structural</cell></row><row><cell></cell><cell>Supervision</cell><cell>Annotation</cell><cell>Consistency</cell></row><row><cell cols="2">Shape-cycleGAN [4] Precise mask</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell cols="2">AttentionGAN [12] Not required</cell><cell>No</cell><cell>No</cell></row><row><cell>MaskGAN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>(Ours) Coarse mask No Yes</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>14 ± 0.05 57.73 ± 0.13 25.96 ± 0.19 31.69 ± 0.08 54.88 ± 0.09</figDesc><table><row><cell cols="2">MaskGAN (w/o Shape) 22.78 ± 0.19 34.02 ± 0.09 60.19 ± 0.06 23.58 ± 0.23 32.43 ± 0.07 57.35 ± 0.08</cell></row><row><cell>MaskGAN (Ours)</cell><cell>21.56 ± 0.18 34.75 ± 0.08 61.25 ± 0.10 22.77 ± 0.17 32.55 ± 0.06 58.32 ± 0.10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Ethics approval was granted by Southern Adelaide Clinical Human Research Ethics Committee.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This study was supported by <rs type="funder">Channel 7 Children's Research Foundation of South Australia Incorporated (CRF)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 6.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MedGAN: medical image translation using GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer. Med. Imag. Graphic</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ResViT: residual vision transformers for multimodal medical image synthesis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dalmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2598" to="2614" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SA-GAN: structure-aware GAN for organ-preserving synthetic CT generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Nejad-Davarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Glide-Hurst</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-146" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="471" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unpaired MR to CT synthesis with explicit structural constrained adversarial learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Governale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pediatr. Neurol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="394" to="401" />
			<date type="published" when="2015">2015</date>
			<publisher>Craniosynostosis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CT synthesis from MRI using multi-cycle GAN for head-and-neck radiation therapy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer. Med. Imag. Graphic</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">101953</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cancer risk in 680000 people exposed to computed tomography scans in childhood or adolescence: data linkage study of 11 million AUS-TRALIANS</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised-learning-based method for chest MRI-CT transformation using structure constrained unsupervised generative attention networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11090</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Risk of cancer from occupational exposure to ionising radiation: retrospective cohort study of workers in France, the united kingdom, and the united states (INWORKS)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient component labeling of images of arbitrary dimension represented by linear bintrees</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="586" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AttentionGAN: unpaired imageto-image translation using attention-guided generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neu. Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1972" to="1987" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep MR to CT synthesis using unpaired data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H F</forename><surname>Savenije</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Seevinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A T</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68127-6_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68127-62" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2017</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gooya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10557</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised MR-to-CT synthesis using structure-constrained CycleGAN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4249" to="4261" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unpaired brain MR-to-CT synthesis using a structureconstrained CycleGAN</title>
		<author>
			<persName><forename type="first">Heran</forename><surname>Yang</surname></persName>
		</author>
		<idno>DLMIA/ML-CDS -2018</idno>
		<editor>Stoyanov, Danail, et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_20</idno>
		<idno>978-3-030-00889-5 20</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="174" to="182" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surgical correction of craniosynostosis: a review of 100 cases</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Zakhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Woerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Notarianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cranio-Maxillofac. Surg</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1684" to="1691" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mapping in cycles: dualdomain PET-CT synthesis framework with cycle-consistent constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_72</idno>
		<idno>978-3-031-16446-0 72</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="758" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9242" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anatomyguided multimodal registration by learning segmentation without ground truth: application to intraprocedural CBCT/MR liver segmentation and registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Augenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102041</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
