<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication</title>
				<funder ref="#_9uJE3K9">
					<orgName type="full">Science and Technology Plan of Liaoning Province</orgName>
				</funder>
				<funder ref="#_VFQSMeh #_6j3BMwn">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jianning</forename><surname>Chi</surname></persName>
							<email>chijianning@mail.neu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianli</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaosheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengdong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Robot Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="98" to="108"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7D2FE74FE5532D99BFCB0F4845902C4A</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-dose computed tomography</term>
					<term>Image denoising</term>
					<term>Image super-resolution</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-dose computer tomography (LDCT) has been widely used in medical diagnosis yet suffered from spatial resolution loss and artifacts. Numerous methods have been proposed to deal with those issues, but there still exists drawbacks: (1) convolution without guidance causes essential information not highlighted; (2) features with fixed-resolution lose the attention to multi-scale information; (3) single super-resolution module fails to balance details reconstruction and noise removal. Therefore, we propose an LDCT image super-resolution network consisting of a dual-guidance feature distillation backbone for elaborate visual feature extraction, and a dual-path content communication head for artifacts-free and details-clear CT reconstruction. Specifically, the dual-guidance feature distillation backbone is composed of a dual-guidance fusion module (DGFM) and a sampling attention block (SAB). The DGFM guides the network to concentrate the feature representation of the 3D inter-slice information in the region of interest (ROI) by introducing the average CT image and segmentation mask as complements of the original LDCT input. Meanwhile, the elaborate SAB utilizes the essential multi-scale features to capture visual information more relative to edges. The dual-path reconstruction architecture introduces the denoising head before and after the super-resolution (SR) head in each path to suppress residual artifacts, respectively. Furthermore, the heads with the same function share the parameters so as to efficiently improve the reconstruction performance by reducing the amount of parameters. The experiments compared with 6 state-of-the-art methods on 2 public datasets prove the superiority of our method. The code is made available at https://github.com/neu-szy/dual-guidance_LDCT_SR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Following the "as low as reasonably achievable" (ALARA) principle <ref type="bibr" target="#b22">[22]</ref>, lowdose computer tomography (LDCT) has been widely used in various medical applications, for example, clinical diagnosis <ref type="bibr" target="#b18">[18]</ref> and cancer screening <ref type="bibr" target="#b28">[28]</ref>. To balance the high image quality and low radiation damage compared to normaldose CT (NDCT), numerous algorithms have been proposed for LDCT superresolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>In the past decades, image post-processing techniques attracted much attention from researchers because they did not rely on the vendor-specific parameters <ref type="bibr" target="#b1">[2]</ref> like iterative reconstruction algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">23]</ref> and could be easily applied to current CT workflows <ref type="bibr" target="#b29">[29]</ref>. Image post-processing super-resolution (SR) methods could be divided into 3 categories: interpolated-based methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b25">25]</ref>, modelbased methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b26">26]</ref> and learning-based methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b17">17]</ref>. Interpolatedbased methods could recover clear results in those flattened regions but failed to reconstruct detailed textures because they equally recover information with different frequencies. And model-based methods often involved time-consuming optimization processes and degraded quickly when image statistics were biased from the image prior <ref type="bibr" target="#b5">[6]</ref>.</p><p>With the development of deep learning (DL), various learning-based methods have been proposed, such as EDSR <ref type="bibr" target="#b20">[20]</ref>, RCAN <ref type="bibr" target="#b31">[31]</ref>, and SwinIR <ref type="bibr" target="#b19">[19]</ref>. Those methods optimized their trainable parameters by pre-degraded low-resolution (LR) and high-resolution (HR) pairs to build a robust model with generalization and finally reconstruct SR images. However, they were designed for known degradation (for example bicubic degradation) and failed to deal with more complex and unknown degradation processes (such as LDCT degradation). Facing more complex degradation processes, blind SR methods have attracted attention. Huang et al. <ref type="bibr" target="#b10">[11]</ref> introduced a deep alternating network (DAN) which estimated the degradation kernels and corrected those kernels iteratively and reconstructed results following the inverse process of the estimated degradation. More recently, aiming at improving the quality of medical images further, Huang et al. <ref type="bibr" target="#b11">[12]</ref> first composited degradation model proposed for radiographs and proposed attention denoising super-resolution generative adversarial network (AID-SRGAN) which could denoise and super-resolve radiographs simultaneously. To accurately reconstruct HR CT images from LR CT images, Hou et al. <ref type="bibr" target="#b9">[10]</ref> proposed a dual-channel joint learning framework which could process the denoising reconstruction and SR reconstruction in parallel.</p><p>The aforementioned methods still have drawbacks: (1) They treated the regions of interest (ROI) and regions of uninterest equally, resulting in the extra cost in computing source and inefficient use for hierarchical features. (2) Most of them extracted the features with a fixed resolution, failing to effectively leverage multi-scale features which are essential to image restoration task <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">32]</ref>.</p><p>(3) They connected the SR task and the LDCT denoising task stiffly, leading to smooth texture, residual artifacts and unclear edges.</p><p>To deal with those issues, as shown in Fig. <ref type="figure">1</ref>(a), we propose an LDCT image SR network with dual-guidance feature distillation and dual-path content com-Fig. <ref type="figure">1</ref>. Architecture of our proposed method. SAM is sampling attention module. CAM is channel attention module. AVG CT is the average image among adjacent CT slices of each patient. munication. Our contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> We design a dual-guidance fusion module (DGFM) which could fuse the 3D CT information and ROI guidance by mutual attention to make full use of CT features and reconstruct clearer textures and sharper edges. <ref type="bibr" target="#b1">(2)</ref> We propose a sampling attention block (SAB) which consists of sampling attention module (SAM), channel attention module (CAM) and elaborate multi-depth residual connection aiming at the essential multi-scale features by up-sampling and down-sampling to leverage the features in CT images. <ref type="bibr" target="#b2">(3)</ref> We design a multi-supervised mechanism based on shared task heads, which introducing the denoising head into SR task to concentrate on the connection between the SR task and the denoising task. Such design could suppress more artifacts while decreasing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>The pipeline of our proposed method is shown in Fig. <ref type="figure">1(a)</ref>. We first calculate the average CT image of adjacent CT slices of each patient to provide the 3D spatial structure information of CT volume. Meanwhile, the ROI mask is obtained by a pre-trained segmentation network to guide the network to concentrate on the focus area or tissue area. Then those guidance images and the input LDCT image are fed to the dual-guidance feature distillation backbone to extract the deep features. Finally, the proposed dual-path architecture consisting of parametershared SR heads and denoising heads leverages the deep visual features obtained by our backbone to build the connection between the SR task and the denoising task, resulting in noise-free and detail-clear reconstructed results.</p><p>Dual-Guidance Feature Distillation Backbone. To decrease the redundant computation and make full use of the above-mentioned extra information, we design a dual-guidance feature distillation backbone consisting of a dual-guidance fusion module (DGFM) and sampling attention block(SAB).</p><p>Firstly, we use a 3 × 3 convolutional layer to extract the shallow features of the three input images. Then, those features are fed into 10 DGFM-SAB blocks to obtain the deep visual features.</p><p>Especially, the DGFM-SAB block is composed of DGFM concatenated with SAB. Considering the indicative function of ROI, we calculate the correlation matrix between LDCT and its mask and then acquire the response matrix between the correlation matrix and the average CT image by multi-heads attention mechanism:</p><formula xml:id="formula_0">F i = Sof tmax[P rj(F SAB i ) T × P rj(F mask )] × P rj(F AV G )<label>(1)</label></formula><p>where, F SAB i are the output of i-th SAB. F mask and F AV G represent the shallow features of the input ROI mask and the average CT image respectively. Meanwhile, P rj(•) is the projection function, Sof tmax[•] means the softmax function and F i are the output features of the i-th DGFM. The DGFM helps the backbone to focus on the ROI and tiny structural information by continuously introducing additional guidance information.</p><p>Furthermore, to take advantage of the multi-scale information which is essential for obtaining the response matrix containing the connections between different levels of features, as shown in Fig. <ref type="figure">1</ref>(b), we design the sampling attention block (SAB) which introduces the resampling features into middle connection to fuse the multi-scale information. In the SAB, the input features are up-sampled and down-sampled simultaneously and then down-sampled and up-sampled to recover the spatial resolution, which can effectively extract multi-scale features. In addition, as shown in Fig. <ref type="figure">1(c</ref>), we introduce the channel attention module (CAM) to focus on those channels with high response values, leading to detailed features with high differentiation to different regions. Shared Heads Mechanism. Singly using the SR head that consists of Pixel Shuffle layer and convolution layer fails to suppress the residual artifacts because of its poor noise removal ability. To deal with this problem, we develop a dualpath architecture by introducing the shared denoising head into SR task where the parameters of SR heads and denoising heads in different paths are shared respectively. Two paths are designed to process the deep features extracted from our backbone: <ref type="bibr" target="#b0">(1)</ref> The SR path transfers the deep features to those with highfrequency information and reconstructs the SR result, and (2) the denoising path migrates the deep features to those without noise and recovers the clean result secondly. Especially, the parameters of those two paths are shared and optimized by multiple supervised strategy simultaneously. This process could be formulated as:</p><formula xml:id="formula_1">I f inal = I f 1 + I f 2 2 = DN [H 3×3 (I SR )] + SR[H 3×3 (I DN )] 2 = H 1×1 H 3×3 {P S[H 3×3 (F n )]} + P S H 3×3 {H 3×3 [H 1×1 (F n )]} 2<label>(2)</label></formula><p>where, </p><formula xml:id="formula_2">F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target Function</head><p>Following the multiple supervision strategy, the target function L total is calculated as:</p><formula xml:id="formula_3">L total = λ 1 L SR + λ 2 L DN + L f inal = λ 1 I gt -I SR 1 + λ 2 BI(I gt ) -I DN 1 + I gt -I f inal 1<label>(3)</label></formula><p>where, I gt is the ground truth, BI(•) means bicubic interpolation, • 1 represents the L1 norm and λ 1 , λ 2 are the weight parameters for adjusting the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Experiment Setup</head><p>Datasets. Two widely-used public CT image datasets, 3D-IRCADB <ref type="bibr" target="#b4">[5]</ref> and PANCREAS <ref type="bibr" target="#b4">[5]</ref> We augment the data by rotation and flipping first and then randomly crop them to 128 × 128 patches. Adam optimizer with β 1 = 0.9 and β 2 = 0.99 is used to minimize the target function. λ 1 and λ 2 of our target function are set as 0.2. The batch size is set to 16 and the learning rate is set to 10 -4 which decreases to 5×10 -5 at 200K iterations. Peak signal-to-noise (PSNR) and structural similarity (SSIM) are used as the quantitative indexes to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>Table <ref type="table" target="#tab_1">1a</ref> shows the experimental result of the dual-guidance ablation study.</p><p>Introducing the average CT image guidance alone degrades performance compared with the model without guidance for both the scale factor of 2 and 4. And introducing mask guidance alone could improve the reconstruction effect. When the average CT image guidance and the mask guidance are both embedded, the performance will be promoted further. Table <ref type="table" target="#tab_1">1b</ref> presents the result of the shared heads mechanism ablation study. The experimental result proves that introducing the proposed dual-path architecture could promote the reconstruction performance and the model with shared heads is superior than that without them in both reconstruction ability and parameter amount. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Methods</head><p>We compare the performance of our proposed method with other state-of-theart methods, including Bicubic interpolation <ref type="bibr" target="#b16">[16]</ref>, DAN <ref type="bibr" target="#b10">[11]</ref>, RealSR <ref type="bibr" target="#b15">[15]</ref>, SPSR <ref type="bibr" target="#b21">[21]</ref>, AID-SRGAN <ref type="bibr" target="#b11">[12]</ref> and JDNSR <ref type="bibr" target="#b9">[10]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the qualitative comparison results on the 3D-IRCADB dataset with the scale factor of 2. All methods enhance the image quality to different extents compared with bicubic interpolation. However, for the calcifications within the liver which are indicated by the blue arrows, our method recovers the clearest edges. The results of DAN, SPSR and AID-SRGAN suffers from the artifacts. JDNSR blurs the issue structural information, e.g. the edges of liver and bone. For the inferior vena cava, portal vein, and gallbladder within the kidney, RealSR restores blurred details and textures though it could recover clear edges of calcifications. Figure <ref type="figure" target="#fig_1">3</ref> shows the qualitative comparison results on the PANCREAS dataset with the scale factor of 4. Figure <ref type="figure" target="#fig_1">3</ref> has similar observation as Fig. <ref type="figure" target="#fig_0">2</ref>, that is, our method could suppress more artifacts than other methods, especially at the edges of the pancreas and the texture and structure of the issues with in the kidney. Therefore, our method reconstructs more detailed results than other methods.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the quantitative comparison results of different state-of-theart methods with two scale factors on two datasets. For the 3D-IRCADB and PANCREAS datasets, our method outperforms the second-best methods 1.6896/0.0157 and 1.7325/0.0187 on PSNR/SSIM with the scale factor of 2 respectively. Similarly, our method outperforms the second-best methods  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose an LDCT image SR network with dual-guidance feature distillation and dual-path content communication. Facing the existing problem that reconstructed results suffer from residual artifacts, we design a dualguidance feature distillation backbone which consists of DGFM and SAB to extract deep visual information. Especially, the DGFM could fuse the average CT image to take the advantage of the 3D spatial information of CT volume and the segmentation mask to focus on the ROI, which provides pixel-wise shallow information and deep semantic features for our backbone. The SAB leverages the essential multi-scale features to enhance the ability for feature extraction. Then, our shared heads mechanism reconstructs the deep features obtained by our backbone to satisfactory results. The experiments compared with 6 state-ofthe-art methods on 2 public datasets demonstrate the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results on the 3D-IRCADB dataset with the scale factor of 2. (a) is the HR image and its red rectangle region displays the liver and its lateral issues. (b) to (h) are the reconstruction results by different methods. (Color figure online)</figDesc><graphic coords="7,41,79,151,67,340,24,80,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results on PANCREAS dataset with the scale factor of 4. (a) is the HR image and its red rectangle region shows the pancreas and kidney. (b) to (h) are the reconstruction results by different methods. (Color figure online)</figDesc><graphic coords="8,55,98,151,70,340,24,80,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>n is the output of our backbone, H k×k means k × k convolutional layer, SR(•) represents SR head, DN (•) represents denoising head, P S(•) expresses Pixel Shuffle layer, I SR is the result of SR head, I DN is the result of denoising head and I f inal is the final reconstructed result.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>, are used for both training and testing. The 3D-IRCADB dataset is used for liver and its lesion detection which consists of 2823 512 × 512 CT files from 20 patients. We choose 1663 CT images from 16 patients for training, 226 CT images from 2 patients for validation and 185 CT images from 2 patients for testing. Similarly, the PANCREAS dataset is used for pancreas segmentation which consists of 19328 512 × 512 CT files from 82 patients. We choose 5638 CT images from 65 patients for training, 668 CT images from 8 patients for validation and 753 CT images from 9 patients for testing. All HU values are set as[-135, 215]. Following Zeng et al.<ref type="bibr" target="#b30">[30]</ref>, we set the blank flux as 0.5 × 10 5 to simulate the effect of low dose noise. And we use bicubic interpolation to degrade the HR images to 256 × 256 LR images and 128 × 128 LR images. Ablation experiments on PANCREAS dataset with the scale factor of 2 and 4</figDesc><table><row><cell cols="6">(a) Ablation experiments for dual-guidance on the PANCREAS dataset with the scale factor of</cell></row><row><cell>2 and 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AVG CT Mask</cell><cell>×2 PSNR</cell><cell>SSIM</cell><cell>×4 PSNR</cell><cell>SSIM</cell></row><row><cell>×</cell><cell>×</cell><cell cols="4">30.0282 ± 2.9426 0.8948 ± 0.0431 28.5120 ± 2.2875 0.8643 ± 0.0508</cell></row><row><cell></cell><cell>×</cell><cell cols="4">29.9600 ± 3.2378 0.8950 ± 0.0419 28.1490 ± 2.3284 0.8592 ± 0.0543</cell></row><row><cell>×</cell><cell></cell><cell cols="4">30.2991 ± 3.1391 0.8960 ± 0.0413 28.6589 ± 2.2497 0.8639 ± 0.0522</cell></row><row><cell></cell><cell></cell><cell cols="4">30.4047 ± 3.1558 0.8974 ± 0.0383 28.7542 ± 2.2728 0.8672 ± 0.0412</cell></row><row><cell cols="6">The best quantitative performance is shown in bold and the second-best in underlined.</cell></row><row><cell cols="6">(b) Ablation experiments for shared heads mechanism on the PANCREAS dataset with the scale</cell></row><row><cell cols="2">factor of 2 and 4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Heads</cell><cell cols="2">Param (M) ×2 ×2/×4 PSNR</cell><cell>SSIM</cell><cell>×4 PSNR</cell><cell>SSIM</cell></row><row><cell cols="6">SR Only 5.748/5.896 30.2904 ± 3.0620 0.8948 ± 0.0431 28.4422 ± 2.3707 0.8628 ± 0.0523</cell></row><row><cell cols="6">Unshared 6.009/6.304 30.3257 ± 3.2504 0.8940 ± 0.0442 28.5675 ± 2.2540 0.8645 ± 0.0529</cell></row><row><cell>Shared</cell><cell cols="5">5.795/5.934 30.4047 ± 3.1558 0.8974 ± 0.0383 28.7542 ± 2.2728 0.8672 ± 0.0412</cell></row></table><note><p>Experiment Setup. All experiments are implemented on Ubuntu 16.04.12 with an NVIDIA RTX 3090 24G GPU using Pytorch 1.8.0 and CUDA 11.1.74.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison on the 3D-IRCADB and PANCREAS datasets with other state-of-the-art methods ± 0.5969 0.7948 ± 0.0544 25.5834 ± 2.3017 0.8105 ± 0.0675 AID-SRGAN 4 27.5824 ± 1.1192 0.8312 ± 0.0506 26.0488 ± 1.9642 0.8193 ± 0.0585 JDNSR 4 26.6793 ± 0.6690 0.8194 ± 0.0438 24.7504 ± 1.4858 0.7857 ± 0.0515 Proposed 4 30.0436 ± 1.7803 0.8811 ± 0.462 28.7542 ± 2.2728 0.8672 ± 0.0412 0.6047/0.0157 and 1.1813/0.0281 on PSNR/SSIM with the scale factor of 4 respectively. Those quantitative superiorities confirm our qualitative observations.</figDesc><table><row><cell>Method</cell><cell cols="2">Scale 3D-IRCADB</cell><cell></cell><cell>PANCREAS</cell></row><row><cell></cell><cell></cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>PSNR↑</cell><cell>SSIM↑</cell></row><row><cell>Bicubic</cell><cell>2</cell><cell cols="3">28.2978 ± 1.0071 0.8613 ± 0.0400 26.9997 ± 1.7192 0.8362 ± 0.0452</cell></row><row><cell>DAN</cell><cell>2</cell><cell cols="3">29.4768 ± 1.7852 0.8636 ± 0.0465 28.1050 ± 2.8794 0.8602 ± 0.0510</cell></row><row><cell>RealSR</cell><cell>2</cell><cell cols="3">30.0746 ± 1.6303 0.8981 ± 0.0390 28.6712 ± 2.8363 0.8784 ± 0.4810</cell></row><row><cell>SPSR</cell><cell>2</cell><cell cols="3">29.8686 ± 1.8197 0.8809 ± 0.0417 28.3321 ± 2.9872 0.8755 ± 0.0508</cell></row><row><cell cols="2">AID-SRGAN 2</cell><cell cols="3">29.6212 ± 1.5741 0.8784 ± 0.0423 27.5311 ± 2.7042 0.8579 ± 0.0521</cell></row><row><cell>JDNSR</cell><cell>2</cell><cell cols="3">27.7927 ± 0.9000 0.8579 ± 0.0402 25.7842 ± 1.7106 0.8322 ± 0.0424</cell></row><row><cell>Proposed</cell><cell>2</cell><cell cols="3">31.7642 ± 2.5292 0.9138 ± 0.0362 30.4047 ± 3.1558 0.8974 ± 0.0383</cell></row><row><cell>Bicubic</cell><cell>4</cell><cell cols="3">25.9209 ± 0.5327 0.8071 ± 0.0452 23.7091 ± 1.4903 0.7700 ± 0.5860</cell></row><row><cell>DAN</cell><cell>4</cell><cell cols="3">29.4389 ± 1.6800 0.8703 ± 0.0471 27.5829 ± 2.4628 0.8491 ± 0.0534</cell></row><row><cell>RealSR</cell><cell>4</cell><cell cols="3">27.5951 ± 0.9666 0.8272 ± 0.0508 25.9374 ± 2.0109 0.8214 ± 0.0621</cell></row><row><cell>SPSR</cell><cell>4</cell><cell>25.8575</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61901098</rs>, <rs type="grantNumber">61971118</rs>, <rs type="funder">Science and Technology Plan of Liaoning Province</rs> <rs type="grantNumber">2021JH1/10400051</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VFQSMeh">
					<idno type="grant-number">61901098</idno>
				</org>
				<org type="funding" xml:id="_6j3BMwn">
					<idno type="grant-number">61971118</idno>
				</org>
				<org type="funding" xml:id="_9uJE3K9">
					<idno type="grant-number">2021JH1/10400051</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distance-driven projection and backprojection in three dimensions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Samit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2463" to="2475" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Double paths network with residual information distillation for improving lung CT image super resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Sig. Process. Control</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">103412</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CT image super-resolution reconstruction based on global hybrid attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page">106112</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10593-2_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10593-2_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_25" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ct image quality enhancement via a dualchannel neural network with jointing denoising and super-resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">492</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unfolding the alternating optimization for blind super resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5632" to="5643" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking degradation: radiograph superresolution via AID-SRGAN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13583</biblScope>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>MLMI 2022</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21014-3_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21014-3_5" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Super resolution from image sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Pattern Recognition</title>
		<meeting>the 10th International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Models Image Process. (CVGIP)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-world super-resolution via kernel estimation and noise injection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="466" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Sig. Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diagnostic value and key features of computed tomography in coronavirus disease</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emerg. Microbes Infect</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="787" to="793" />
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SwinIR: image restoration using Swin transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure-preserving image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7898" to="7911" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Radiation protection in humans: extending the concept of as low as reasonably achievable (ALARA) from dose to biological damage</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">914</biblScope>
			<biblScope unit="page" from="97" to="99" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A splitting-based iterative algorithm for accelerated statistical X-ray CT reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extraction of high-resolution frames from video sequences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="996" to="1011" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilinear interpolation of digital images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultramicroscopy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="204" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-resolution image recovery from image-plane arrays, using convex projections</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oskoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1715" to="1726" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recommendations for implementing lung cancer screening with low-dose computed tomography in Europe</title>
		<author>
			<persName><forename type="first">G</forename><surname>Veronesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1672</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain progressive 3D residual convolution network to improve low-dose CT imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2903" to="2913" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple low-dose X-ray CT simulation from high-dose scan</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nucl. Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2226" to="2233" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_18" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2480" to="2495" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
