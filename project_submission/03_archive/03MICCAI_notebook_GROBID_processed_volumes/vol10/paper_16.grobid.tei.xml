<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huidong</forename><surname>Xie</surname></persName>
							<email>huidong.xie@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiongchao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephanie</forename><surname>Thorn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi-Hwa</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Sinusas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
							<email>chi.liu@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06511</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="163" to="172"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">713AEDBBEC5EC1107FAB6B7B9B8BB466</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cardiac SPECT</term>
					<term>Few-view Imaging</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a fewview imaging problem. In this work, we propose a novel 3D transformerbased dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projectionto-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The GE Discovery NM Alcyone 530c/570c <ref type="bibr" target="#b0">[1]</ref> are dedicated cardiac SPECT systems with 19 cadmium zinc telluride (CZT) detector modules designed for stationary imaging. Limited amount of angular sampling on scanners of this type could affect image quality. Due to the unique geometry of Alcyone scanners, the centers of FOV vary at different angular positions. Hence, unlike CT scanners, there is no straightforward method to combine projections at different positions on Alcyone scanners. Xie et al. <ref type="bibr" target="#b12">[12]</ref> proposed to incorporate the displacements between centers of FOV and the rotation angles into the iterative method for multi-angle reconstructions with registration steps within each iteration. Despite its promising results, acquiring multi-angle projections on this scanner is time-consuming and inconvenient in reality. Rotating the detectors also limits the capability of dynamic imaging. Thus, it is desirable to obtain high-quality rotation-based reconstruction directly from the stationary SPECT projection data. This is essentially a few-view reconstruction problem.</p><p>Previous works have attempted to address this problem by using deeplearning-based image-to-image networks. Xie et al. proposed a 3D U-net-like network to directly synthesize dense-view images from few-view counterparts <ref type="bibr" target="#b12">[12]</ref>. Since convolutional networks have limited receptive fields due to small kernel size, Xie et al. further improved their method with a transformer-based image-to-image network for SPECT reconstruction <ref type="bibr" target="#b13">[13]</ref>. Despite their promising reconstruction results, these methods use MLEM (maximum likelihood expectation maximization) reconstruction from one-angle few-view acquisition data as network input. The initial MLEM reconstruction may contain severe image artifacts with important image features lost during the iterative reconstruction process, thus would be challenging to be recovered with image-based methods. Learning to reconstruct images directly from the projection data could lead to improved quality.</p><p>There are a few previous studies proposed to learn the mapping between raw data and images. AUTOMAP <ref type="bibr" target="#b14">[14]</ref> utilized fully-connected layers to learn the inverse Fourier transform between k-space data and the corresponding MRI images. While such a technique could be theoretically adapted to other imaging modalities, using a similar approach would require a significant amount of trainable parameters, and thus is infeasible for 3D data. Würfl et al. <ref type="bibr" target="#b10">[10]</ref> proposed a back-projection operator to link projections and images to reduce memory burden for CT. However, their back-projection process is not learnable. There are also recent works <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b11">11]</ref> that tried to incorporate the embedded imaging physics and geometry of CT scanners into the neural networks to reduce redundant trainable parameters for domain mapping. However, these networks are unable to be generalized to other imaging modalities due to different physical properties. Moreover, these methods are hard to be extended to 3D cases because of the geometric complexity and additional memory/computational burden.</p><p>Here, we propose a novel 3D Transformer-based Dual-domain (projection &amp; image) Network (TIP-Net) to address these challenges. The proposed method reconstructs high-quality few-view cardiac SPECT using a two-stage process. First, we develop a 3D projection-to-image transformer reconstruction network that directly reconstructs 3D images from the projection data. In the second stage, this intermediate reconstruction is combined with the original few-view reconstruction for further refinement, using an image-domain reconstruction network. Validated on physical phantoms, porcine, and human studies acquired on GE Alcyone 570c SPECT/CT scanners, TIP-Net demonstrated superior performance than previous baseline methods. Validated by cardiac catheterization (Cath) images, diagnostic results from nuclear cardiologists, and cardiac defect quantified by an FDA-510(k)-cleared software, we also show that TIP-Net produced images with higher resolution and cardiac defect contrast on human studies, as compared to previous baselines. Our method could be a clinically useful tool to improve cardiac SPECT imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Following the acquisition protocol described by Xie et al. <ref type="bibr" target="#b12">[12]</ref>, we acquired a total of eight porcine and two physical phantom studies prospectively with four angles of projections. The training target in this work is four-angle data with 76 (19×4) projections and the corresponding input is one-angle data with 19 projections. Twenty clinical anonymized 99m Tc-tetrofosmin SPECT human studies were retrospectively included for evaluation. Since only one-angle data was available for the human studies, Cath images and clinical interpretations from nuclear cardiologists were used to determine the presence/absence of true cardiac defects and to assess the images reconstructed by different methods. The use of animal and human studies was approved by the Institutional Animal Care &amp; Use Committee (IACUC) of Yale University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Structure</head><p>The overall network structure is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. TIP-Net is divided into two parts. The transformer-based <ref type="bibr" target="#b1">[2]</ref> projection-net (P-net) aims at reconstructing 3D SPECT volumes directly from 3D projections obtained from the scanner. Information from the system matrix (IMG bp ) is also incorporated in P-net as prior knowledge for image reconstruction. IMG bp is obtained by multiplying the system matrix S ∈ R 19,456×245,000 with the projection data. The outputs from P-net (IMG p ) serve as an input for image-net (I-net).</p><p>To reconstruct IMG p in P-net, we may simply project the output from the Transformer block to the size of 3D reconstructed volume (i.e., 70 × 70 × 50). However, such implementation requires a significant amount of GPU memory. To alleviate the memory burden, we proposed to learn the 3D reconstruction in a slice-by-slice manner. The output from the Transformer block is projected to a single slice (70 × 70), and the whole Transformer network (red rectangular in Fig. <ref type="figure" target="#fig_0">1</ref>) is looped 50 times to produce a whole 3D volume (70 × 70 × 50). Since different slice has different detector sensitivity, all the 50 loops use different trainable parameters and the i th loop aims to reconstruct the i th slice in the 3D volume. Within each loop, the Transformer block takes the entire 3D projections as input to reconstruct the i th slice in the SPECT volume. With the self-attention mechanism, all 50 loops can observe the entire 3D projections for reconstruction. The 70×70 slice is then combined with IMG bp (only at i th slice), and the resized 3D projection data. The resulting feature maps (70 × 70 × 21) are fed into a shallow 2D CNN to produce a reconstructed slice at the i th loop (70 × 70 × 1), which is expected to be the i th slice in the SPECT volume. I-net takes images reconstructed by P-net (IMG p ), prior knowledge from the system matrix (IMG bp ), and images reconstructed by MLEM (IMG mlem ) using one-angle data for final reconstructions. With such a design, the network can combine information from both domains for potentially better reconstructions compared with methods that only consider single-domain data.</p><p>Both 3D-CNN 1 and 3D-CNN 2 use the same network structure as the network proposed in <ref type="bibr" target="#b12">[12]</ref>, but with different trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimization, Training, and Testing</head><p>In this work, the TIP-Net was trained using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty <ref type="bibr" target="#b3">[4]</ref>. A typical WGAN contains 2 separate networks, one generator (G) and a discriminator (D). In this work, the network G is the TIP-Net depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Formulated below, the overall objective function for network G includes SSIM, MAE, and Adversarial loss. where I one and I four denote images reconstructed by one-angle data and fourangle data respectively using MLEM. θ G represents trainable parameters of network G. P net (I one ) represents the output of the P-net (IMG p ). The function is formulated as below:</p><formula xml:id="formula_0">(X, Y ) = MAE (X, Y ) + λ c SSIM (X, Y ) + λ d MAE (SO(X), SO(Y )),<label>(2)</label></formula><p>where X and Y represent two image volumes used for calculations. MAE and SSIM represent MAE and SSIM loss functions, respectively. The Sobel operator (SO) was used to obtain edge images, and the MAE between them was included as the loss function. λ a = 0.1, λ b = 0.005, λ c = 0.8, and λ d = 0.1 were fine-tuned experimentally. Network D shares the same structure as that proposed in <ref type="bibr" target="#b12">[12]</ref>. The Adam method <ref type="bibr" target="#b8">[8]</ref> was used for optimizations. The parameters were initialized using the Xavier method <ref type="bibr" target="#b2">[3]</ref>. 250 volumes of simulated 4D extended cardiac-torso (XCAT) phantoms <ref type="bibr" target="#b9">[9]</ref> were used for network pre-training. Leaveone-out testing process was used to obtain testing results for all the studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluations</head><p>Reconstructed images were quantitatively evaluated using SSIM, RMSE, and PSNR. Myocardium-to-blood pool (MBP) ratios were also included for evaluation. For myocardial perfusion images, higher ratios are favorable and typically represent higher image resolution. Two ablated networks were trained and used for comparison. One network, denoted as 3D-CNN, shared the same structure as either 3D-CNN 1 or 3D-CNN 2 but only with IMG mlem as input. The other network, denoted as Dual-3D-CNN, used the same structure as the TIP-Net outlined in Fig. <ref type="figure" target="#fig_0">1</ref> but without any projection-related inputs. Compared with these two ablated networks, we could demonstrate the effectiveness of the proposed projection-to-image module in the TIP-Net. We also compared the TIP-Net with another transformer-based network (SSTrans-3D) <ref type="bibr" target="#b13">[13]</ref>. Since SSTrans-3D only considers image-domain data, comparing TIP-Net with SSTrans-3D could demonstrate the effectiveness of the transformer-based network for processing projection-domain information.</p><p>To compare the performance of cardiac defect quantifications, we used the FDA 510(k)-cleared Wackers-Liu Circumferential Quantification (WLCQ) software <ref type="bibr" target="#b7">[7]</ref> to calculate the myocardial perfusion defect size (DS). For studies without cardiac defect, we should expect lower measured defect size as the uniformity of myocardium improves. For studies with cardiac defects, we should expect higher measured defect size as defect contrast improves.</p><p>Cath images and cardiac polar maps are also presented. Cath is an invasive imaging technique used to determine the presence/absence of obstructive lesions that results in cardiac defects. We consider Cath as the gold standard for the defect information in human studies. The polar map is a 2D representation of the 3D volume of the left ventricle. All the metrics were calculated based on the entire 3D volumes. All the clinical descriptions of cardiac defects in this paper were confirmed by nuclear cardiologists based on SPECT images, polar maps, WLCQ quantification, and Cath images (if available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Porcine Results</head><p>Results from one sample porcine study are presented in Fig. <ref type="figure" target="#fig_0">S1</ref>. This pig had a large post-occlusion defect created by inflating an angioplasty balloon in the left anterior descending artery. As pointed out by the green arrows in Fig. <ref type="figure" target="#fig_0">S1</ref>, all neural networks improved the defect contrast with higher MBP ratios compared with the one-angle images. The TIP-Net results were better than other network results in terms of defect contrast based on defect size measured by WLCQ. TIP-Net also produced images with clearer right ventricles, as demonstrated by the Full-width at Half Maximum (FHMW) values presented in Fig. <ref type="figure" target="#fig_0">S1</ref>.</p><p>Quantitative results for 8 porcine and 2 physical phantom studies are included in Table <ref type="table" target="#tab_0">1</ref>. Based on paired t-tests, all the network results are statistically better than one-angle results (p &lt; 0.001), and the TIP-Net results are statistically better than all the other three network results (p &lt; 0.05). The average MBP ratios shown in Table <ref type="table" target="#tab_0">1</ref> indicate that the proposed TIP-Net produced images with higher resolution compared with image-based networks.</p><p>Average defect size measurements for porcine and physical phantom studies with cardiac defects are 35.9%, 42.5%, 42.3%, 43.6%, 47.0%, 46.2% for one-angle, 3D-CNN, Dual-3D CNN, TIP-Net, and four-angle image volumes, respectively. Images reconstructed by TIP-Net present overall higher defect contrast and the measured defect size is closest to the four-angle images.</p><p>For normal porcine and physical phantom studies without cardiac defect, these numbers are 16.0%, 12.0%, 14.7%, 11.2%, 11.5%, and 10.1%. All neural networks showed improved uniformity in the myocardium with lower measured defect size. Both transformer-based methods, TIP-Net and SSTrans-3D, showed better defect quantification than other methods on these normal subjects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human Results</head><p>Chosen by a nuclear cardiologist, results from one representative human study with multiple cardiac defects are presented in Fig. <ref type="figure" target="#fig_2">2</ref>. Note that there was no four-angle data for human studies. Based on clinical interpretations from the nuclear cardiologist and Cath results, this patient had multiple perfusion defects in the apical (blue arrows in Fig. <ref type="figure" target="#fig_2">2</ref>) and basal (green and yellow arrows in Fig. <ref type="figure" target="#fig_2">2</ref>) regions. As presented in Fig. <ref type="figure" target="#fig_2">2</ref>, all the networks produced images with better defect contrasts. The proposed TIP-Net further improved the defect contrast, which is favorable and could make the clinical interpretations more confident. Another human study was selected and presented in Fig. <ref type="figure" target="#fig_3">3</ref>. This patient had stenosis in the distal left anterior descending coronary artery, leading to a medium-sized defect in the entire apical region (light-green arrows). As validated by the Cath images, polar map, and interpretation from a nuclear cardiologist, TIP-Net produced images with higher apical defect contrast compared with other methods, potentially leading to a more confident diagnostic decision. The average MBP ratios on human studies for one-angle, 3D-CNN, Dual-3D-CNN, SSTrans-3D, and TIP-Net images are 3.13 ± 0.62, 4.08 ± 0.83, 4.31 ± 1.07, 4.17 ± 0.99, and 4.62 ± 1.29, respectively (MEAN ± STD). The higher ratios in images produced by TIP-Net typically indicate higher image resolution.</p><p>14 patients in the testing data have cardiac defects, according to diagnostic results. The average defect size measurements for these patients are 16.8%, 22.6%, 21.0%, 22.4%, and 23.6% for one-angle, 3D-CNN, Dual-3D-CNN, SSTrans-3D, and TIP-Net results. The higher measured defect size of the TIP-Net indicates that the proposed TIP-Net produced images with higher defect contrast.</p><p>For the other 6 patients without cardiac defects, these numbers are 11.5%, 12.8%, 13.8%, 12.4%, and 11.8%. These numbers show that TIP-Net did not introduce undesirable noise in the myocardium, maintaining the overall uniformity for normal patients. However, other deep learning methods tended to introduce non-uniformity in these normal patients and increased the defect size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intermediate Network Output</head><p>To further show the effectiveness of P-net in the overall TIP-Net design, outputs from P-net (images reconstructed by the network directly from projections) are presented in Fig. <ref type="figure" target="#fig_2">S2</ref>. The presented results demonstrate that the network can learn the mapping between 3D projections to 3D image volumes directly without the iterative reconstruction process. In the porcine study, limited angular sampling introduced streak artifacts in the MLEM-reconstructed one-angle images (yellow arrows in Fig. <ref type="figure" target="#fig_2">S2</ref>). P-net produced images with fewer few-view artifacts and higher image resolution.</p><p>The human study presented in Fig. <ref type="figure" target="#fig_2">S2</ref> has an apical defect according to the diagnostic results (blue arrows in Fig. <ref type="figure" target="#fig_2">S2</ref>). However, this apical defect is barely visible in the one-angle image. P-net produced images with higher resolution and improved defect contrast. Combining both outputs (one-angle images and IMG p ), TIP-Net further enhanced the defect contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>We proposed a novel TIP-Net for 3D cardiac SPECT reconstruction. To the best of our knowledge, this work is the first attempt to learn the mapping from 3D realistic projections to 3D image volumes. Previous works in this direction <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b14">14]</ref> were 2D and only considered simulated projections with ideal conditions. 3D realistic projection data have more complex geometry are also affected by other physical factors that may not exist in simulated projections.</p><p>The proposed method was tested for myocardial perfusion SPECT imaging. Validated by nuclear cardiologists, diagnostic results, Cath images, and defect size measured by WLCQ, the proposed TIP-Net produced images with higher resolution and higher defect contrast for patients with perfusion defects. For normal patients without perfusion defects, TIP-Net maintained overall uniformity in the myocardium with higher image resolution. Similar performance was observed in porcine and physical phantom studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall TIP-Net structure. The TIP-Net is divided into P-net and I-net. The back-projected image volume was generated by multiplying the 3D projections with the system matrix. FC layer: fully-connected layer. Both 3D-CNN1 and 3D-CNN2 share the same structure but with different trainable parameters.</figDesc><graphic coords="4,79,89,124,85,264,28,216,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>min θG L = (G(I one ), I four ) + λ a (P net (I one ), I four ) -λ b E Ione [D(G(I one ))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results from a sample stress human study. Red arrows point to different coronary vessels. Non-red arrows with the same color point to the same defect/stenosis in the heart. Numbers in the parentheses are the MBP ratios and the defect size (%LV). (Color figure online)</figDesc><graphic coords="7,188,94,426,14,106,39,98,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results from a sample stress human study. Red arrows point to different coronary vessels. Light-green arrows point to the same defect/stenosis in the heart. Numbers in the parentheses are the MBP ratios and the defect size (%LV). (Color figure online)</figDesc><graphic coords="8,279,90,152,87,87,11,87,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative values for porcine and phantom results obtained using different methods (MEAN ± STD). Images reconstructed using four-angle data were used as the reference. Best values are marked in bold. p &lt; 0.05 observed between all groups.</figDesc><table><row><cell cols="2">Evaluation One-angle</cell><cell>3D-CNN</cell><cell cols="2">Dual-3D-CNN SSTrans-3D</cell><cell>TIP-Net</cell><cell>Four-angle</cell></row><row><cell>SSIM↑</cell><cell cols="5">0.945 ± 0.009 0.951 ± 0.008 0.950 ± 0.008 0.947 ± 0.009 0.953 ± 0.008 0.953 ± 0.008 0.953 ± 0.008</cell><cell></cell></row><row><cell>PSNR↑</cell><cell cols="5">35.764 ± 3.546 36.763 ± 3.300 36.787 ± 3.208 36.613 ± 3.509 38.114 ± 2.876 38.114 ± 2.876 38.114 ± 2.876</cell><cell></cell></row><row><cell>RMSE↓</cell><cell cols="5">0.020 ± 0.005 0.0181 ± 0.005 0.0185 ± 0.005 0.0194 ± 0.005 0.0179 ± 0.005 0.0179 ± 0.005 0.0179 ± 0.005</cell><cell></cell></row><row><cell>MBP↑</cell><cell>5.16 ± 1.83</cell><cell>10.73 ± 2.98</cell><cell>10.21 ± 3.17</cell><cell>11.10 ± 2.78</cell><cell>12.73 ± 3.73 12.73 ± 3.73 12.73 ± 3.73</cell><cell>13.63 ± 3.91</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 16.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast cardiac gamma camera with dynamic SPECT capabilities: design, system validation and future potential</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Blevis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tsukerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shrem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kovalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Volokh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00259-010-1488-z</idno>
		<ptr target="https://doi.org/10.1007/s00259-010-1488-z" />
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1887" to="1902" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<ptr target="http://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. Machine Learning Research<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010. 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Radon inversion via deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.2964266</idno>
		<ptr target="https://doi.org/10.1109/TMI.2020.2964266" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2076" to="2087" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to reconstruct computed tomography (CT) images directly from sinogram data under a variety of data acquisition conditions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2019.2910760</idno>
		<ptr target="https://doi.org/10.1109/TMI.2019.2910760" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2469" to="2481" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantification of SPECT myocardial perfusion images: methodology and validation of the Yale-CQ method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinusas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wackers</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1071-3581(99)90080-6</idno>
		<ptr target="https://doi.org/10.1016/s1071-3581" />
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Cardiol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90080" to="90086" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015, Conference Track Proceedings</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09">7-9 May 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">4D XCAT phantom for multimodality imaging research</title>
		<author>
			<persName><forename type="first">W</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sturgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tsui</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.3480985</idno>
		<ptr target="https://doi.org/10.1118/1.3480985.https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.3480985" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4902" to="4915" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning computed tomography: learning projection-domain weights from image domain in limited angle problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Würfl</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2833499</idno>
		<ptr target="https://doi.org/10.1109/TMI.2018.2833499" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1454" to="1463" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep efficient end-to-end reconstruction (DEER) network for fewview breast CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3033795</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3033795" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020">196633-196646 (2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Increasing angular sampling through deep learning for stationary cardiac SPECT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12350-022-02972-z</idno>
		<ptr target="https://doi.org/10.1007/s12350-022-02972-z" />
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Cardiol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning based few-angle cardiac SPECT reconstruction using transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TRPMS.2022.3187595</idno>
		<ptr target="https://doi.org/10.1109/TRPMS.2022.3187595" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image reconstruction by domain-transform manifold learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature25988</idno>
		<ptr target="https://doi.org/10.1038/nature25988" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="issue">7697</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
