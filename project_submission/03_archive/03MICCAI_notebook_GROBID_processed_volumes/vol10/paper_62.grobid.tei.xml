<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images</title>
				<funder ref="#_XeVXd5J">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_u9Mv6Xr #_K3W3aZT">
					<orgName type="full">Foshan HKUST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="655" to="665"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2A0F359ABD261334E3936B2EEBEB8A6E</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deformable Image Registration • Score-based Generative Model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised deformable image registration is one of the challenging tasks in medical imaging. Obtaining a high-quality deformation field while preserving deformation topology remains demanding amid a series of deep-learning-based solutions. Meanwhile, the diffusion model's latent feature space shows potential in modeling the deformation semantics. To fully exploit the diffusion model's ability to guide the registration task, we present two modules: Feature-wise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG). Specifically, FDG uses the diffusion model's multi-scale semantic features to guide the generation of the deformation field. SDG uses the diffusion score to guide the optimization process for preserving deformation topology with barely any additional computation. Experiment results on the 3D medical cardiac image registration task validate our model's ability to provide refined deformation fields with preserved topology effectively.</p><p>Code is available at: https://github.com/xmed-lab/FSDiffReg.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deformable image registration is the process of accurately estimating non-rigid voxel correspondences, such as the deformation field, between the same anatomical structure of a moving and fixed image pair. Fast, accurate, and realistic image registration algorithms are essential to improving the efficiency and accuracy of clinical practices. By observing dynamic changes, such as lesions, physicians can more comprehensively design treatment plans for patients <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. When images during surgery align with preoperative ones, surgeons can locate instruments better and improve surgical prognosis <ref type="bibr" target="#b0">[1]</ref>. As reported in <ref type="bibr" target="#b11">[12]</ref>, cardiac image registration is especially vital in improving heart chamber analysis accuracy, correcting cardiac imaging errors, and guiding cardiac surgeries. Thus, several studies have explored classical <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> and deep-learning-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> registration methods over the years.</p><p>Classical registration methods <ref type="bibr" target="#b15">[16]</ref> used hand-crafted features to align images by solving computational-expensive optimization problems. Recently, researchers explored the deep-learning-based unsupervised deformable image registration <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> to address the computational burden while reducing the need for accurate ground truth in the registration task. VoxelMorph <ref type="bibr" target="#b2">[3]</ref>, as the baseline, took moving and fixed image pairs as the input and maximized image pair similarity to train a registration network. To achieve higher accuracy, most unsupervised methods adopted a cascaded network with several sub-networks or an iterative refinement strategy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>. These strategies made the training procedure complicated and computational resources demanding. Meanwhile, to obtain smoother and more realistic deformation fields, i.e., topology preservation, many existing works introduced explicit diffeomorphic constraints <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> or additional calculations on cycle consistency <ref type="bibr" target="#b13">[14]</ref>. For example, CycleMorph <ref type="bibr" target="#b13">[14]</ref> utilized the bidirectional registration consistency to preserve the topology during training. VoxelMorph-Diff <ref type="bibr" target="#b6">[7]</ref> adopted velocity field-based deformation field and new diffeomorphic estimation. SYMNet <ref type="bibr" target="#b18">[19]</ref> used symmetric deformation field estimation to achieve the goal. However, these schemes did not fully exploit the inherent network features, thereby overlooking these features' ability for better topology preservation. f -m intuitively indicates where significant deformation occurs, as the redboxed area shows. We calculated the per voxel energy score in the diffusion model's latent obtained in <ref type="bibr" target="#b12">[13]</ref> as <ref type="bibr" target="#b9">[10]</ref> suggests to identify the areas where complex deformation is likely to happen. The result indicates the same area, which was not explicitly utilized in prior work <ref type="bibr" target="#b12">[13]</ref>. (Color figure online)</p><p>Recently, Kim et al. <ref type="bibr" target="#b12">[13]</ref> first proposed a diffusion model <ref type="bibr" target="#b8">[9]</ref>, which is simpler to train than other generative models yet rich in semantics, for the registration task. They used the latent feature from the diffusion model's score function, i.e., the gradient field of a distribution's log-likelihood function <ref type="bibr" target="#b21">[22]</ref>, as one of the registration network's inputs for a better registration result. However, this method only used the final diffusion score as an image level guidance, which ignored diffusion model's rich task-specific semantics in the feature levels, as proven in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. This resulted in the latent semantics of the diffusion model not being able to directly guide the features learned at the hidden layers of the registration network. As a result, the informativeness of these features for image registration was reduced. Moreover, this method only preserved deformation topology by simply using the diffusion score as the input, thereby ignoring the informative details about areas where significant deformations occur ; see Fig. <ref type="figure">1d</ref> for unexploited informative semantics. Therefore, the registration network was unable to explicitly prioritize hard-to-register areas, thereby limiting its effectiveness in preserving the deformation topology.</p><p>To address these issues, we present two novel modules, namely Featurewise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG) in the registration network. FDG introduces a direct feature-wise diffusion guidance technique for generating deformation fields by utilizing crossattention to integrate the intermediate features of the diffusion model into the hidden layer of the registration network's decoder. Furthermore, we embed the feature-wise guidance into multiple layers of the registration network and produce the feature-level deformation fields in multiple scales. Finally, after obtaining deformation fields at multiple scales, we upsample and average them to generate the full-resolution deformation field for registration. Our SDG introduces explicit score-wise diffusion guidance for deformation topology preservation by reweighing the similarity-based unsupervised registration loss based on the diffusion score. Through this reweighing scheme, direct attention is given during the optimization process to ensure the preservation of the deformation topology. Our main contribution can be summarized as follows:</p><p>-We propose a novel feature-wise diffusion-guided module (FDG), which utilizes multi-scale intermediate features from the diffusion model to effectively guide the registration network in generating deformation fields. -We also propose a score-wise diffusion-guided module (SDG), which leverages the diffusion model's score function to guide deformation topology preservation during the optimization process without incurring any additional computational burden. -Experimental results on the cardiac dataset validated the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline Registration Model</head><p>Figure <ref type="figure" target="#fig_2">2a</ref> shows the overview of our proposed method. We first sample a perturbed noisy image x t from the fixed target image f following the same scheme in <ref type="bibr" target="#b8">[9]</ref>, which can be formulated as Eq. 1:</p><formula xml:id="formula_0">x t = √ α t f + √ 1 -α t ,</formula><p>where</p><formula xml:id="formula_1">α t = t s=1 (1 -β s ), ∼ N (0, I )<label>(1)</label></formula><p>where 0 &lt; β s &lt; 1 is the variance of the noise, t is the noise level. Then we perform the registration training task. Given an input x in consisting of a fixed reference image f , a moving unaligned image m, and the perturbed noisy image x t , we feed this input x in = {f, m, x t } into the registration network's shared encoder  </p><formula xml:id="formula_2">F i = {(F i G , F i R )}, i = 1, .</formula><p>.., N from the i-th layer of the decoder. Of note, we generate the registration decoder's feature map by incorporating the guidance from the diffusion decoder, which can be formulated as Eq. 2:</p><formula xml:id="formula_3">F i R = r i (concat(F i-1 R , F i E , F i G )), where i = 1, ..., N<label>(2)</label></formula><p>where r i is the i-th layer of the registration decoder, and F i E is the skip connection of features from the shared encoder layer at the same depth.</p><p>After obtaining the feature map pairs, our FDG module estimates the i-th feature level deformation field φ i from the feature map pair (F i G , F i R ) using linear cross attention <ref type="bibr" target="#b20">[21]</ref>, which can be defined as Eq. 3:</p><formula xml:id="formula_4">φ i = Conv(softmax(F i R (GroupNorm(F i G ) T • F i G )) + F i R )<label>(3)</label></formula><p>After obtaining all feature-level deformation fields from the shallowest layer to the deepest layer, we generate the final deformation field φ by enlarging and averaging all feature-level deformation fields. This is a commonly adopted method for multi-scale deformation field merging so as to merge features which attend different scales and granularity. The final φ is then fed into the spatial transformation layer with the moving image m to generate the registered image m(φ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Score-Wise Diffusion-Guided Module</head><p>Given the representation z encoded by the shared encoder z = E β (x in ), the diffusion decoder G σ outputs a diffusion score estimation S = G σ (z). Then, the Score-wise Diffusion-Guided Module (SDG) uses this score to reweigh the similarity-based normalized cross-correlation loss function, formulated as Eq. 4:</p><formula xml:id="formula_5">L scoreN CC (m, f, S) = ( 1 1 + e -S ) γ -(m(φ) ⊗ f )<label>(4)</label></formula><p>where m(φ) is the warped moving image, defines the Hadamard product, and ⊗ defines the local normalized cross-correlation function. γ is a hyperparameter to amplify the reweighing effect. By this means, SDG utilizes the diffusion score to explicitly indicate the hardto-register areas, i.e., areas where deformation topology is hard to preserve, then assigning higher weights in the loss function for greater attention, and vice versa for easier-to-register areas. Therefore, the information on deformation topology is effectively incorporated into the optimization process without additional constraints by the SDG module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overall Training and Inference</head><p>Loss Function. Our network predicts the deformable fields at the feature level and then outputs the registered image. The total loss function of our method is defined as Eq. 5:</p><formula xml:id="formula_6">L total = L dif f usion (x in , t) + λL scoreN CC (m, f, S) + λ φ ||∇ φ || 2<label>(5)</label></formula><formula xml:id="formula_7">L dif f usion (x in , t) = E z ||G σ (E β (x in , t)) -|| 2 2</formula><p>,where ∼ N (0, I )</p><p>where L dif f usion is the auxiliary loss function for training the diffusion decoder G σ (Eq. 6), and t is the noise level of x t , following the method in <ref type="bibr" target="#b8">[9]</ref>. Our proposed L scoreN CC encourages maximizing the similarity between the registered and reference images while preserving the deformation topology.</p><p>||∇ φ || 2 is the conventional smoothness penalty on the deformation field. λ and λ φ are hyperparameters, and we empirically set them to 20 in our experiments.</p><p>Inference. In the inference stage, we perform image registration in the same style as <ref type="bibr" target="#b12">[13]</ref>. Instead of the perturbed image x t , we input the original reference image f into the network, and the total network input becomes x in = {f, m, f }. Given this network input x in , our network first generates the deformation field φ between the moving image m and the reference image f and produces the registered moving image m(φ) by feeding the moving image m and the deformation field φ into the spatial transformation layer (STL). The registered moving image is the final output of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset and Preprocessing. Following the previous work <ref type="bibr" target="#b12">[13]</ref>, we used the publicly available 3D cardiac MR dataset ACDC <ref type="bibr" target="#b4">[5]</ref> for experiments. The dataset includes 100 4D temporal cardiac MRI data with corresponding segmentation maps. We selected the 3D image at the end of the diastolic stage as the fixed image and the image at the end of the systolic stage as the moving image. We resampled all scans to the voxel spacing of 1.5×1.5×3.15 mm, then cropped them to the voxel size of 128 × 128 × 32. We normalized the intensity of all images to [-1, 1]. The training set contains 90 image pairs, while the remaining 10 pairs form the test set. The abovementioned preprocessing steps were performed in accordance with the approach described in prior work <ref type="bibr" target="#b12">[13]</ref> to ensure a fair comparison. Implementation Details. The proposed framework was implemented using the PyTorch library, version 1.12.0. Following <ref type="bibr" target="#b12">[13]</ref>, we used DDPM UNet's 3D encoder as our shared encoder and DDPM UNet's 3D decoder as our diffusion decoder. For the registration part, instead of a complete 3D UNet in <ref type="bibr" target="#b12">[13]</ref>, we only used DDPM UNet's 3D decoder as our registration decoder to generate the deformation field. During the diffusion task, we gradually increased the noise schedule from 10 -6 to 10 -2 over 2000 timesteps. We utilized an Nvidia RTX3090 GPU and the Adam optimization algorithm <ref type="bibr" target="#b14">[15]</ref> to train the model with λ = 20, λ φ = 20, γ = 1, batch size B = 1, a learning rate of 2 × 10 -4 , and a maximum of 700 epochs.</p><p>Evaluation Metrics. We employed three evaluation metrics, i.e., DICE, |J| ≤ 0(%), and SD(|J|) to measure the image registration performance, following existing registration methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. DICE measures the spatial overlap of anatomical segmentation maps between the warped moving image and the fixed reference image. A higher Dice score indicates better alignment between the warped moving image and the fixed reference image, thus reflecting an improved registration quality. |J| ≤ 0(%) indicates the percentage of non-positive values in the Jacobian determinant of the registration field. This metric indicates the percentage of voxels that lacks a one-to-one registration mapping relation, causing unrealistic deformations and roughness. SD(|J|) refers to the standard deviation of the Jacobian determinant of the registration field. A lower standard deviation indicates that the registration field is relatively smooth and consistent across the image.</p><p>Compare with the State-of-the-Art Methods. Table <ref type="table">1</ref> shows the comparison of our method with existing state-of-the-art methods including Voxel-Morph <ref type="bibr" target="#b2">[3]</ref>, VoxelMorph-Diff <ref type="bibr" target="#b6">[7]</ref>, and DiffuseMorph <ref type="bibr" target="#b12">[13]</ref> on the same training and testing dataset. We produced baseline results using the recommended hyperparameters in their paper. The result shows that our proposed method outperforms existing baseline methods by a substantial margin (Wilcoxon signed-rank test, p &lt; 0.005) (Also see Fig. <ref type="figure" target="#fig_3">3</ref>). Furthermore, our method aligned better in areas where larger deformation happened, such as myocardium (myo).</p><p>Table <ref type="table">1</ref>. Image registration results with standard deviation in parenthesis on the 3D cardiac dataset. "LV", "Myo", "RV" refers to Left Ventricle, Myocardium, and Right Ventricle, respectively. "Overall" refers to the averaged registration result of the left blood pool, myocardium, left ventricle, right ventricle, and these total region, following <ref type="bibr" target="#b12">[13]</ref>. ↑: the higher, the better results. ↓: the lower, the better results.   Ablation Study. To validate the effectiveness of our proposed learning strategies, including the Feature-wise Diffusion-Guided module(FDG) and Score-wise Diffusion-Guided module(SDG), we conducted ablative experiments, as shown in Table <ref type="table" target="#tab_2">2</ref>. The network without FDG also uses the denoising diffusion decoder but generates the deformation field from the encoded feature directly, and without SDG means that we optimize the network using the vanilla NCC loss. By integrating multi-scale intermediate latent diffusion features into generating deformation fields, we can see that the network's performance increased by 1%. By deploying the reweighing loss, the Jacobian metric decreased by 60.5%. The result achieved a balance when all components were deployed. These results demonstrated that our proposed components could effectively guide the deformation field generation by using multi-scale diffusion features (Also see Fig. <ref type="figure" target="#fig_4">4</ref>).</p><p>Optimization guided by diffusion score led to better preservation of deformation topology. It is worth noticing that the results without FDG or SDG showed only marginal improvement over baseline results, indicating the importance of feature-level deformation field generation and the reweighing scheme. The ablative study of hyperparameter λ is illustrated in Supp. Fig. <ref type="figure">1</ref>.  Analysis of γ. Furthermore, to validate SDG's effectiveness on topology preservation, we conducted another ablative study on SDG's hyperparameter γ, as Table <ref type="table" target="#tab_3">3</ref> shows. Increased γ indicates a more substantial reweighing effect. The results showed that by adding stronger reweighing influence, we could obtain deformation fields with better topology preservation almost without compromising accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work proposes two novel modules for unsupervised deformable image registration: the Feature-wise Diffusion-Guided module (FDG) and the Score-wise Diffusion-Guided module (SDG). Among these modules, FDG can effectively guide the deformation field generation by utilizing the multi-scale intermediate diffusion features. SDG demonstrates its ability to guide the optimization process for better deformation topology preservation using the diffusion score. Extensive experiments show that the proposed framework brings impressive improvements over all baselines. The proposed work models the non-linear deformation semantics using the diffusion model. Therefore, it is sound to generalize to other registration tasks and images, which may be one of the future research directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig.1. f -m intuitively indicates where significant deformation occurs, as the redboxed area shows. We calculated the per voxel energy score in the diffusion model's latent obtained in<ref type="bibr" target="#b12">[13]</ref> as<ref type="bibr" target="#b9">[10]</ref> suggests to identify the areas where complex deformation is likely to happen. The result indicates the same area, which was not explicitly utilized in prior work<ref type="bibr" target="#b12">[13]</ref>. (Color figure online)</figDesc><graphic coords="2,67,80,339,80,288,16,75,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. a) The workflow of FSDiffReg. b) The illustration of the guidance process of the Feature-wise Diffusion-Guided Module.</figDesc><graphic coords="4,41,79,196,70,340,33,268,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2. 2</head><label>2</label><figDesc>Feature-Wise Diffusion-Guided ModuleThe main component of the Feature-wise Diffusion-Guided module (FDG) is an auxiliary denoising diffusion decoder G σ . The workflow of FDG is shown in Fig.2b. Given the input x in = {f, m, x t }, the UNet shared encoder E β extracts the representation z. z is then fed into the diffusion decoder G and the registration decoder R to get intermediate feature map pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The visualization of registration results. As the red-boxed area shows, our deformation field and the corresponding image are more refined in the area where larger deformation happens. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The visualization example of the effectiveness of FDG. The deformation field shows more details and is more distinct when guided by the FDG, thus improving registration accuracy, as the area in the red box shows. (Color figure online)</figDesc><graphic coords="8,41,79,208,73,340,27,111,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on FDG and SDG.</figDesc><table><row><cell cols="2">FDG SDG</cell><cell>DICE↑</cell><cell>|J| ≤ 0(%) ↓</cell></row><row><cell>√ √</cell><cell cols="3">0.811(0.098) 0.114(0.076) 0.818(0.102) 0.062(0.038) √ 0.810(0.188) 0.045(0.025) √ 0.823(0.096) 0.054(0.026)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study on hyperparameter γ in SDG. |J| ≤ 0(%) ↓ 0.069(0.029) 0.054(0.026) 0.042(0.023)</figDesc><table><row><cell>γ</cell><cell>0.5</cell><cell>1</cell><cell>2</cell></row><row><cell>DICE↑</cell><cell cols="3">0.817(0.100) 0.823(0.096) 0.816(0.104)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> under Project <rs type="grantNumber">ITS/030/21 &amp; PRP/041/22FX</rs>, as well as by <rs type="funder">Foshan HKUST</rs> Projects under Grants <rs type="grantNumber">FSUST21-HKUST10E</rs> and <rs type="grantNumber">FSUST21-HKUST11E</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XeVXd5J">
					<idno type="grant-number">ITS/030/21 &amp; PRP/041/22FX</idno>
				</org>
				<org type="funding" xml:id="_u9Mv6Xr">
					<idno type="grant-number">FSUST21-HKUST10E</idno>
				</org>
				<org type="funding" xml:id="_K3W3aZT">
					<idno type="grant-number">FSUST21-HKUST11E</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 62.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Medical image registration in image guided surgery: issues, challenges and research opportunities</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybern. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labelefficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SlxSY2UZQT" />
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AMNet: adaptive multi-level network for deformable registration of 3D brain MR images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102740</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="226" to="236" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Breast image analysis for risk assessment, detection, diagnosis, and treatment of cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="327" to="357" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Difficulty-aware hierarchical convolutional neural networks for deformable registration of brain MR images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101817</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amalgamation of machine learning and slice-by-slice registration of MRI for early prognosis of cognitive decline</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gambhir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Comput. Sci. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An overview on image registration techniques for cardiac diagnosis and treatment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cardiol. Res. Pract</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DiffuseMorph: unsupervised deformable image registration using diffusion model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19821-220" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13691</biblScope>
			<biblScope unit="page" from="347" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CycleMorph: cycle consistent unsupervised deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102036</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Elastix: a toolbox for intensity-based medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised probabilistic deformation modeling for robust diffeomorphic registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-512" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS 2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion models already have a semantic latent space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=pd1P2eUBVfq" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast symmetric diffeomorphic image registration with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4644" to="4653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<idno>978-3-030-59716-0 21</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient attention: attention with linear complexities</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3531" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Scorebased generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Plug-and-play diffusion features for text-driven image-to-image translation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tumanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12572</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
