<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning</title>
				<funder ref="#_hQgKVRw">
					<orgName type="full">Korea government</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Health Technology R&amp;D Project</orgName>
				</funder>
				<funder ref="#_eesK8Yb">
					<orgName type="full">Ministry of Health &amp; Welfare, Republic of Korea</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Health Industry Development Institute</orgName>
					<orgName type="abbreviated">KHIDI</orgName>
				</funder>
				<funder ref="#_xPWyjbb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changyong</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution" key="instit1">AMIST</orgName>
								<orgName type="institution" key="instit2">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Convergence Medicine</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiheon</forename><surname>Jeong</surname></persName>
							<idno type="ORCID">0000-0002-8238-6642</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution" key="instit1">AMIST</orgName>
								<orgName type="institution" key="instit2">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Convergence Medicine</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sangyoon</forename><forename type="middle">Min</forename><surname>Lee</surname></persName>
							<idno type="ORCID">0009-0005-8825-5175</idno>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Convergence Medicine</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Min</forename><surname>Lee</surname></persName>
							<idno type="ORCID">0000-0002-2173-2193</idno>
						</author>
						<author role="corresp">
							<persName><forename type="first">Namkug</forename><surname>Kim</surname></persName>
							<email>namkugkim@gmail.com</email>
							<idno type="ORCID">0000-0002-3438-2217</idno>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Convergence Medicine</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Asan Medical Center</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="344" to="354"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7F5DADBAFDFF8DA637D1ADB25047ADBE</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CT</term>
					<term>Kernel conversion</term>
					<term>Image-to-image translation</term>
					<term>Contrastive learning</term>
					<term>Style transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computed tomography (CT) image can be reconstructed by various types of kernels depending on what anatomical structure is evaluated. Also, even if the same anatomical structure is analyzed, the kernel being used differs depending on whether it is qualitative or quantitative evaluation. Thus, CT images reconstructed with different kernels would be necessary for accurate diagnosis. However, once CT image is reconstructed with a specific kernel, the CT raw data, sinogram is usually removed because of its large capacity and limited storage. To solve this problem, many methods have been proposed by using deep learning approach using generative adversarial networks in image-to-image translation for kernel conversion. Nevertheless, it is still challenging task that translated image should maintain the anatomical structure of source image in medical domain. In this study, we propose CT kernel conversion method using multi-domain imageto-image translation with generator-guided contrastive learning. Our proposed method maintains the anatomical structure of the source image accurately and can be easily utilized into other multi-domain image-to-image translation methods with only changing the discriminator architecture and without adding any additional networks. Experimental results show that our proposed method can translate CT images from sharp into soft kernels and from soft into sharp kernels compared to other image-to-image translation methods. Our code is available at https://git hub.com/cychoi97/GGCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computed tomography (CT) image is reconstructed from sinogram, which is tomographic raw data collected from detectors. According to kernels being used for CT image reconstruction, there is a trade-off between spatial resolution and noise, and it affects intensity and texture quantitative values <ref type="bibr" target="#b0">[1]</ref>. When CT image is reconstructed with sharp kernel, spatial resolution and noise increase, and abnormality can be easily detected in bones or lung. In contrast, with soft kernel, spatial resolution and noise reduce, and abnormality can be easily detected in soft tissues or mediastinum. In other words, CT image is reconstructed depending on what anatomical structure is evaluated. Also, even if the same anatomical structure is analyzed, the kernel being used differs depending on whether it is qualitative or quantitative evaluation. For example, CT images reconstructed with soft kernel is required to evaluate quantitative results for lung instead of sharp kernel. Thus, CT images reconstructed with different kernels would be necessary for accurate diagnosis.</p><p>However, once CT image is reconstructed with a specific kernel, sinogram is usually removed because of its large capacity and limited storage. Therefore, clinicians have difficulty to analyze qualitative or quantitative results without CT image reconstructed with different kernels, and this limitation reveals on retrospective or longitudinal studies that cannot control technical parameters, particularly <ref type="bibr" target="#b1">[2]</ref>. Besides, there is another problem that patients should be scanned again and exposed to radiation.</p><p>Recently, many studies have achieved improvement in kernel conversion <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> using image-to-image translation methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> based on deep learning, especially generative adversarial networks (GANs) <ref type="bibr" target="#b13">[14]</ref>. Nevertheless, it remains challenging that translated image should maintain its anatomical structure of source image in medical domain <ref type="bibr" target="#b8">[9]</ref>. It is important for quantitative evaluation as well as qualitative evaluation. To solve this problem, we focus on improving maintenance of structure when the source image is translated.</p><p>Our contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> we propose multi-domain image-to-image translation with generator-guided contrastive learning (GGCL) for CT kernel conversion, which maintains the anatomical structure of the source image accurately; (2) Our proposed GGCL can be easily utilized into other multi-domain image-to-image translation with only changing the discriminator architecture and without adding any additional networks; (3) Experimental results showed that our method can translate CT images from sharp into soft kernels and from soft into sharp kernels compared to other image-to-image translation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>In deep learning methods for CT kernel conversion, there were proposed methods using convolutional neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, but they were trained in a supervised manner. Recently, Yang et al. <ref type="bibr" target="#b4">[5]</ref> proposed a new method using the adaptive instance normalization (AdaIN) <ref type="bibr" target="#b14">[15]</ref> in an unsupervised manner and it showed significant performance, however, this method still has limitations that the target image for the test phase and additional architecture for AdaIN are needed.</p><p>Generator-guided discriminator regularization (GGDR) <ref type="bibr" target="#b15">[16]</ref> is discriminator regularization method that intermediate feature map in the generator supervises semantic representations by matching with semantic label map in the discriminator for unconditional image generation. It has advantages that we don't need any ground-truth semantic segmentation masks and can improve fidelity as much as conditional GANs <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, it has been shown that dense contrastive learning can have a positive effect on learning dense semantic labels. In dense prediction tasks such as object detection and semantic segmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, both global and local contrastive learning have been proposed to embed semantic information. Furthermore, it has been demonstrated that patch-wise contrastive learning performs well in style transfer for unsupervised image-to-image translation <ref type="bibr" target="#b11">[12]</ref>. This motivated our experiments as it demonstrates that intermediate features can be learned through contrastive learning when learning dense semantic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generator-Guided Contrastive Learning</head><p>GGDR <ref type="bibr" target="#b15">[16]</ref> uses cosine distance loss between the feature map and the semantic label map for unconditional image generation. However, unlike image generation, the generator has a structure with an encoder and a decoder in image-to-image translation <ref type="bibr" target="#b10">[11]</ref>, and this is quite important to maintain the structure of source image while translating the style of target image. Thus, it might be helpful for discriminator to inform more fine detail semantic representations by comparing similarity using patch-based contrastive learning <ref type="bibr" target="#b11">[12]</ref> (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Domain Image-To-Image Translation.</head><p>We apply generator-guided contrastive learning (GGCL) to StarGAN <ref type="bibr" target="#b5">[6]</ref> as base architecture which is one of the multi-domain image-to-image translation model to translate kernels into all directions at once and show stability of GGCL. Basically, StarGAN uses adversarial loss, domain classification loss and cycle consistency loss <ref type="bibr" target="#b12">[13]</ref> as follows:</p><formula xml:id="formula_0">L D = -L adv + λ cls L r cls ,<label>(1)</label></formula><formula xml:id="formula_1">L G = L adv + λ cls L f cls + λ cyc L cyc ,<label>(2)</label></formula><p>where L D and L G are the discriminator and generator losses, respectively. They both have L adv , which is the adversarial loss. L r cls and L f cls are the domain classification losses for a real and fake image, respectively. L cyc , which is the cycle consistency loss, has an importance for the translated image to maintain the structure of source image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Based Contrastive Learning.</head><p>Our method is to add PatchNCE loss <ref type="bibr" target="#b11">[12]</ref> between "positive" and "negative" patches from the feature map of the decoder in the generator and "query" patch from the semantic label map in the discriminator. The query patch is the same location with positive patch and different locations with N negative patches. So, the positive patch is learned to associate to the query patch more than the N negative patches. GGCL loss is the same as PatchNCE loss, which is the cross-entropy loss calculated for an (N + 1)-way classification, and it follows as: where v, v + and v - n are the vectors which are mapped from query, positive and n-th negative patches, respectively. τ = 0.07 is the same configuration as CUT <ref type="bibr" target="#b11">[12]</ref>. Since we use the features from the generator and the discriminator themselves, this requires no additional auxiliary networks and no feature encoding process.</p><formula xml:id="formula_2">L ggcl = E v -log exp v • v + /τ exp v • v + /τ + N n=1 exp v • v - n /τ ,<label>(3)</label></formula><p>Total Objective. GGCL follows the concept of GGDR, which the generator supervises the semantic representations to the discriminator, so it is a kind of the discriminator regularization. Discriminator conducts real/fake classification, domain classification and semantic label map segmentation, so it can be also a kind of the multi-task learning <ref type="bibr" target="#b21">[22]</ref>. Our total objective functions for the discriminator and generator are written, respectively, as:</p><formula xml:id="formula_3">L D = -L adv + λ cls L r cls + λ ggcl L ggcl , (<label>4</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">L G = L adv + λ cls L f cls + λ cyc L cyc , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where λ cls , λ cyc and λ ggcl are hyper-parameters that weight the importance of domain classification loss, cycle consistency loss and GGCL loss, respectively. We used λ cls = 1, λ cyc = 10 and λ ggcl = 2 in our experiments.</p><p>3 Experiments and Results Implementation Details. We maintained the original resolution 512 × 512 of CT images and normalized their Hounsfield unit (HU) range from [-1024HU ~3071HU] to [-1 ~1] for pre-processing. For training, the generator and the discriminator were optimized by Adam <ref type="bibr" target="#b22">[23]</ref> with β 1 = 0.5, β 2 = 0.999, learning rate 1e-4 and the batch size is 2. We used WGAN-GP <ref type="bibr" target="#b23">[24]</ref> and set n critic = 5, where n critic is the number of discriminator updates per each generator update. The feature map and the semantic label map were extracted in 256 × 256 size and resized 128 × 128 using averaging pooling. The number of patches for contrastive learning is 64. All experiments were conducted using single NVIDIA GeForce RTX 3090 24GB GPU for 400,000 iterations. We used peak signal-to-noise ratio (PSNR) <ref type="bibr" target="#b24">[25]</ref> and structural similarity index measure (SSIM) <ref type="bibr" target="#b25">[26]</ref> for quantitative assessment.</p><p>Architecture Improvements. Instead of using original StarGAN <ref type="bibr" target="#b5">[6]</ref> architecture, we implemented architecture ablations to sample the best quality results, empirically. In generator, original StarGAN runs 4 × 4 transposed convolutional layers for upsampling. However, it causes degradation of visual quality of the translated image because of checkerboard artifact <ref type="bibr" target="#b26">[27]</ref>. By using 3 × 3 convolutional layers and 2 × 2 pixelshuffle <ref type="bibr" target="#b27">[28]</ref>, we could prevent the artifact. In discriminator, we changed the discriminator to U-Net architecture <ref type="bibr" target="#b28">[29]</ref> with skip connection, which consists of seven encoder layers for playing the role of patchGAN <ref type="bibr" target="#b7">[8]</ref> and six decoder layers for extracting semantic label map, to utilize GGCL. For each decoder layer, we concatenated the feature from the encoder and the decoder layer with the same size, and ran 1 × 1 convolutional layer, then ran 2 × 2 pixelshuffle for upsampling. At the end of the decoder, it extracts semantic label map to compare with the feature map from the decoder layer of the generator. Lastly, we added spectral normalization <ref type="bibr" target="#b29">[30]</ref> and leakyReLU activation function in all layers of the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Other Image-to-Image Translation Methods</head><p>We compared GGCL with two-domain image-to-image translation methods such as CycleGAN <ref type="bibr" target="#b12">[13]</ref>, CUT <ref type="bibr" target="#b11">[12]</ref>, UNIT <ref type="bibr" target="#b9">[10]</ref> and multi-domain image-to-image translation methods such as AttGAN <ref type="bibr" target="#b6">[7]</ref>, StarGAN and StarGAN with GGDR <ref type="bibr" target="#b15">[16]</ref> to show the effectiveness of GGCL. In this section, qualitative and quantitative results were evaluated for the translation into B30f, B50f and B70f kernels, respectively.</p><p>Qualitative Results. We showed the qualitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into Siemens (see Fig. <ref type="figure" target="#fig_1">2</ref>). For visualization, window width and window level were set 1500 and -700, respectively. While UNIT could not maintain the global structure of the source image and translate the kernel style of the target image, the other methods showed plausible results. However, they could not maintain the fine details like airway wall and vessel in specific kernel conversion, e.g., B50f to B30f, B30f to B50f and B50f to B70f. It could be observed through difference map between the target image and the translated image (see Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>). GGCL showed stability of translation for kernel conversion with any directions and maintained the fine details including airway wall, vessel and even noise pattern as well as the global structure of the source image.</p><p>Quantitative Results. We showed the quantitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into the Siemens (see Table <ref type="table" target="#tab_2">2</ref>). In case of two-domain image-to-image translation methods, they showed high PSNR and SSIM performance in translation from B70f into B30f and from B30f into B70f, and UNIT showed the best performance in translation from B30f into B70f. However, they showed low performance in translation into the other kernels, especially soft into sharp, and it indicates that two-domain methods are unstable and cannot maintain the structure of the source image well. In case of multidomain image-to-image translation methods, their performance still seemed unstable, however, when applying GGDR to StarGAN, it showed quite stable and improved the performance in translation into sharp kernels. Furthermore, when applying GGCL, it outperformed GGDR in translation into many kernels, especially from B30f into B70f and from B50f into B70f. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We implemented ablation studies about the number of patches, size of pooling and loss weight for GGCL to find out the best performance. We evaluated our method while preserving the network architecture. Ablation studies were also evaluated by PSNR and SSIM. All ablation studies were to change one factor and the rest of them were fixed with their best configurations. The results about the number of patches showed improvement when the number of patches was 64 (see Table <ref type="table" target="#tab_3">3</ref>). The size of pooling also affected the performance improvement, and 2 was appropriate (see Supplementary Table <ref type="table" target="#tab_1">1</ref>). Lastly, the results of the loss weight for GGCL showed that 2 was the best performance (see Supplementary Table <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we proposed CT kernel conversion method using multi-domain imageto-image translation with generator-guided contrastive learning (GGCL). In medical domain image-to-image translation, it is important to maintain anatomical structure of the source image while translating style of the target image. However, GAN based generation has limitation that the training process may be unstable, and the results may be inaccurate so that some fake details may be generated. Especially in unsupervised manner, the anatomical structure of the translated image relies on cycle consistency mainly. If trained unstably, as the translated image to the target domain would be inaccurate, the reversed translated image to the original domain would be inaccurate as well. Then, the cycle consistency would fail to lead the images to maintain the anatomical structure. CycleGAN <ref type="bibr" target="#b12">[13]</ref>, CUT <ref type="bibr" target="#b11">[12]</ref> and UNIT <ref type="bibr" target="#b9">[10]</ref> showed this limitation (see Fig. <ref type="figure" target="#fig_1">2</ref> and Table <ref type="table" target="#tab_2">2</ref>), but GGCL solved this problem without any additional networks. The benefit of GGCL was revealed at the translation from soft into sharp kernels. It is a more difficult task than the translation from sharp into soft kernels because spatial resolution should be increased and noise patterns should be clear, so this benefit can be meaningful. Nevertheless, the improvements from GGCL were quite slight compared to GGDR <ref type="bibr" target="#b15">[16]</ref> (see Table <ref type="table" target="#tab_2">2</ref>) and inconsistent according to the number of patches (see Table <ref type="table" target="#tab_3">3</ref>). Besides, we did not show results about the different kernels from the external manufacturer. In future work, we will collect different types of kernels from the external manufacturer, and conduct experiments to show better improvements and stability of GGCL. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of generator-guided contrastive learning (GGCL) framework. The proposed method is to add patch-based contrastive learning between the intermediate feature map from the generator and the semantic label map from the discriminator to solve (N + 1)-way classification. G E , G R and G D are the encoder, residual and decoder blocks of the generator, respectively. D E and D D are the encoder and decoder blocks of the discriminator, respectively. This method can be applied to any multi-domain image-to-image translation methods.</figDesc><graphic coords="4,89,97,57,32,273,04,284,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The qualitative results of image-to-image translation methods including GGDR and our method for kernel conversion from Siemens into Siemens.</figDesc><graphic coords="7,41,79,56,54,340,15,228,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3.1 Datasets and Implementation Datasets.</head><label></label><figDesc>For train dataset, chest CT images were reconstructed with B30f, B50f and B70f kernels, from soft to sharp, in Siemens Healthineers. We collected chest CT images from 102 (63 men and 39 women; mean age, 62.1 ± 12.8 years), 100 (47 men and 53 women; mean age, 64.9 ± 13.7 years) and 104 (64 men and 40 women; mean age, 62.2 ± 12.9 years) patients for B30f, B50f and B70f kernels from Siemens (see Table1).For test dataset, we collected chest CT images from paired 20 (15 men and 5 women; mean age, 67.1 ± 7.4 years) patients for each kernel in Siemens for quantitative and qualitative evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>CT acquisition parameters of dataset according to kernels.</figDesc><table><row><cell></cell><cell cols="3">Kernel Patients Slices</cell><cell>Age (year)</cell><cell cols="3">Sex (M:F) Slice Thickness kVp</cell></row><row><cell cols="2">Test B30f</cell><cell>102</cell><cell cols="3">36199 62.1 ± 12.8 63:39</cell><cell>1.0</cell><cell>120</cell></row><row><cell></cell><cell>B50f</cell><cell>100</cell><cell cols="3">32795 64.9 ± 13.7 47:53</cell><cell>1.0</cell><cell>120</cell></row><row><cell></cell><cell>B70f</cell><cell>104</cell><cell cols="3">36818 62.2 ± 12.9 64:40</cell><cell>1.0</cell><cell>120</cell></row><row><cell></cell><cell>Kernel</cell><cell>Patients</cell><cell>Slices</cell><cell>Age (year)</cell><cell>Sex (M:F)</cell><cell>Slice Thickness</cell><cell>kVp</cell></row><row><cell>Test</cell><cell>B30f</cell><cell>20</cell><cell>6897</cell><cell>67.1 ± 7.4</cell><cell>15:5</cell><cell>1.0</cell><cell>120</cell></row><row><cell></cell><cell>B50f</cell><cell>20</cell><cell>6897</cell><cell>67.1 ± 7.4</cell><cell>15:5</cell><cell>1.0</cell><cell>120</cell></row><row><cell></cell><cell>B70f</cell><cell>20</cell><cell>6897</cell><cell>67.1 ± 7.4</cell><cell>15:5</cell><cell>1.0</cell><cell>120</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The quantitative results of image-to-image translation methods including GGDR and our method from Siemens to Siemens.</figDesc><table><row><cell>Method</cell><cell cols="2">Sharp to Soft</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">B50f → B30f</cell><cell cols="2">B70f → B30f</cell><cell cols="2">B70f → B50f</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>CycleGAN</cell><cell>35.672</cell><cell>0.950</cell><cell>44.741</cell><cell>0.974</cell><cell>32.553</cell><cell>0.905</cell></row><row><cell>CUT</cell><cell>37.261</cell><cell>0.956</cell><cell>40.394</cell><cell>0.961</cell><cell>34.595</cell><cell>0.910</cell></row><row><cell>UNIT</cell><cell>24.545</cell><cell>0.672</cell><cell>44.964</cell><cell>0.979</cell><cell>22.868</cell><cell>0.540</cell></row><row><cell>AttGAN</cell><cell>38.685</cell><cell>0.927</cell><cell>37.435</cell><cell>0.900</cell><cell>32.596</cell><cell>0.733</cell></row><row><cell>StarGAN</cell><cell>37.262</cell><cell>0.930</cell><cell>36.024</cell><cell>0.903</cell><cell>31.660</cell><cell>0.799</cell></row><row><cell>w/ GGDR</cell><cell>47.659</cell><cell>0.987</cell><cell>45.213</cell><cell>0.979</cell><cell>41.391</cell><cell>0.950</cell></row><row><cell>w/ GGCL (ours)</cell><cell>47.831</cell><cell>0.989</cell><cell>44.943</cell><cell>0.981</cell><cell>40.332</cell><cell>0.944</cell></row><row><cell>Method</cell><cell cols="2">Soft to Sharp</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">B30f → B50f</cell><cell cols="2">B30f → B70f</cell><cell cols="2">B50f → B70f</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>CycleGAN</cell><cell>28.536</cell><cell>0.830</cell><cell>31.544</cell><cell>0.754</cell><cell>30.719</cell><cell>0.758</cell></row><row><cell>CUT</cell><cell>35.320</cell><cell>0.902</cell><cell>29.659</cell><cell>0.660</cell><cell>31.402</cell><cell>0.834</cell></row><row><cell>UNIT</cell><cell>22.994</cell><cell>0.511</cell><cell>34.733</cell><cell>0.869</cell><cell>23.288</cell><cell>0.563</cell></row><row><cell>AttGAN</cell><cell>32.604</cell><cell>0.753</cell><cell>28.293</cell><cell>0.556</cell><cell>28.662</cell><cell>0.564</cell></row><row><cell>StarGAN</cell><cell>31.738</cell><cell>0.836</cell><cell>28.531</cell><cell>0.601</cell><cell>28.527</cell><cell>0.601</cell></row><row><cell>w/ GGDR</cell><cell>41.606</cell><cell>0.961</cell><cell>31.062</cell><cell>0.757</cell><cell>34.547</cell><cell>0.869</cell></row><row><cell>w/ GGCL (ours)</cell><cell>41.279</cell><cell>0.958</cell><cell>32.584</cell><cell>0.818</cell><cell>34.857</cell><cell>0.872</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies about the number of patches.</figDesc><table><row><cell>Patch Num</cell><cell>Sharp to Soft</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">B50f → B30f</cell><cell cols="2">B70f → B30f</cell><cell cols="2">B70f → B50f</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>64</cell><cell>47.831</cell><cell>0.989</cell><cell>44.943</cell><cell>0.981</cell><cell>40.332</cell><cell>0.944</cell></row><row><cell>128</cell><cell>47.788</cell><cell>0.989</cell><cell>45.318</cell><cell>0.981</cell><cell>38.484</cell><cell>0.913</cell></row><row><cell>256</cell><cell>47.513</cell><cell>0.989</cell><cell>45.282</cell><cell>0.983</cell><cell>40.190</cell><cell>0.937</cell></row><row><cell>Patch Num</cell><cell>Soft to Sharp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">B30f → B50f</cell><cell cols="2">B30f → B70f</cell><cell cols="2">B50f → B70f</cell></row><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>64</cell><cell>41.279</cell><cell>0.958</cell><cell>32.584</cell><cell>0.818</cell><cell>34.857</cell><cell>0.872</cell></row><row><cell>128</cell><cell>40.294</cell><cell>0.948</cell><cell>31.812</cell><cell>0.781</cell><cell>31.676</cell><cell>0.764</cell></row><row><cell>256</cell><cell>41.375</cell><cell>0.958</cell><cell>33.208</cell><cell>0.830</cell><cell>34.710</cell><cell>0.867</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by a grant of the <rs type="funder">Korea Health Technology R&amp;D Project</rs> through the <rs type="funder">Korea Health Industry Development Institute (KHIDI)</rs>, funded by the <rs type="funder">Ministry of Health &amp; Welfare, Republic of Korea</rs> (<rs type="grantNumber">HI18C0022</rs>) and by <rs type="institution">Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP)</rs> grant funded by the <rs type="funder">Korea government</rs> (<rs type="grantNumber">1711134538</rs>, <rs type="grantNumber">20210003930012002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eesK8Yb">
					<idno type="grant-number">HI18C0022</idno>
				</org>
				<org type="funding" xml:id="_hQgKVRw">
					<idno type="grant-number">1711134538</idno>
				</org>
				<org type="funding" xml:id="_xPWyjbb">
					<idno type="grant-number">20210003930012002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching and homogenizing convolution kernels for quantitative studies in computed tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">288</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CT image conversion among different reconstruction kernels without a sinogram by using a convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Korean J. Radiol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CT kernel conversions using convolutional neural net for super-resolution with simplified squeeze-and-excitation blocks and progressive learning among smooth and sharp kernels</title>
		<author>
			<persName><forename type="first">D.-I</forename><surname>Eun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page">105615</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gravina</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-06427-2_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-06427-2_9" />
		<title level="m">Image Analysis and Processing -ICIAP 2022: 21st International Conference</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Distante</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Leo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</editor>
		<meeting><address><addrLine>Lecce, Italy; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">May 23-27, 2022. 2022</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous conversion of CT kernel using switchable CycleGAN with AdaIN</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3015" to="3029" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attgan: facial attribute editing by only changing what you want</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5464" to="5478" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Breaking the dilemma of medical image-to-image translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="1964">1964-1978 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation: methods and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3859" to="3881" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<idno type="DOI">10.1007/978-3-030-58545-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-7_19" />
		<title level="m">Computer Vision -ECCV 2020: 16th European Conference, Glasgow</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</editor>
		<meeting><address><addrLine>UK; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IX</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generator knows what discriminator should learn in unconditional GANs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19790-1_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19790-1_25" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022: 17th European Conference</title>
		<editor>
			<persName><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Shai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="406" to="422" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVII</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">You only need adversarial supervision for semantic image synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sushko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04781</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Propagate yourself: exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5586" to="5609" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A formal evaluation of PSNR as quality measurement parameter for image segmentation algorithms</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Fardo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07116</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-10-05">2015. October 5-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Proceedings, Part III 18</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
