<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</title>
				<funder ref="#_4AsEJyn #_DGeVwak">
					<orgName type="full">MRC</orgName>
				</funder>
				<funder ref="#_ectsGeE #_5TcMPmh #_vFzMZQe">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Che</forename><surname>Liu</surname></persName>
							<email>che.liu21@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Earth Science and Engineering</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sibo</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengyun</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Brain Sciences</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weitong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Shah</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Infectious Disease Epidemiology</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Royal Brompton and Harefield Hospitals</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjia</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Brain Sciences</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rossella</forename><surname>Arcucci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Earth Science and Engineering</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="637" to="647"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">585A4F820B034FFB37D4781C5D55F73B</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision-language model</term>
					<term>Vision-language pre-training</term>
					<term>Self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78%. Notably, M-FLAG achieves outstanding performance on the segmentation task while using only 1% of the RSNA dataset, even outperforming ImageNet pre-trained models that have been fine-tuned using 100% of the data. The code can be found in https://github.com/cheliu-computation/M-FLAG-MICCAI2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has made significant progress in medical computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> but requires large annotated datasets, which are often difficult to obtain. Selfsupervised learning (SSL) offers a solution by utilizing large unannotated medical image sets. It also enables vision-language pre-training (VLP), which learns representations for both imaging and text data and their relationships <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>. Several recent medical VLP approaches such as ConVIRT <ref type="bibr" target="#b31">[32]</ref>, GLoRIA <ref type="bibr" target="#b10">[11]</ref>, and MGCA <ref type="bibr" target="#b26">[27]</ref> have shown the effectiveness of model pre-training with medical images and radiology reports together, which outperformed the conventionally pre-trained models using image only in downstream tasks <ref type="bibr" target="#b31">[32]</ref>. However, training such models is not an easy task as they require extensive resources for training both vision and language models. In particular, most VLP approaches are based on pre-trained BERT <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, whose parameters are 5 times larger than a standard ResNet50 <ref type="bibr" target="#b9">[10]</ref>. This indicates high computational cost, as well as training complexity and potential instability in joint training <ref type="bibr" target="#b12">[13]</ref>. On the other hand, previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> suggest a training strategy that forces image latent space to match language latent space, which can be sub-optimal with latent space collapse problem <ref type="bibr" target="#b13">[14]</ref>, reducing its performance for downstream tasks <ref type="bibr" target="#b33">[34]</ref>. In this work, we would like to answer the following two questions: <ref type="bibr" target="#b0">(1)</ref> Is it necessary to tune pre-trained language models for medical VLP? <ref type="bibr" target="#b1">(2)</ref> How to regularize the latent space in pre-training?</p><p>We propose a novel VLP framework named Medical vision-language pretraining with Frozen language models and Latent spAce Geometry optimization method (M-FLAG). Different from most existing VLP approaches, M-FLAG is computationally efficient as it only requires training the vision model, while keeping the language model frozen. To harmonize the latent spaces in vision and language models, we relax the visual-language alignment objective with a orthogonality loss to alleviate the latent space collapse problem. The main contributions of this work include: (1) To the best of our knowledge, this is the first work to explore the collapsed latent space problem in medical VLP. (2) A novel and effective VLP framework is proposed to alleviate the collapsed latent space problem by explicitly optimizing the latent geometry towards orthogonal using our orthogonality loss in addition to the visual-language alignment loss, encouraging the in-dependency between latent variables and maximizing its informativeness for downstream tasks. (3) M-FLAG consistently outperforms existing medical VLP methods on three downstream tasks: medical image classification, segmentation, and object detection, while reducing 78% trainable parameters due to the frozen language model strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works:</head><p>To connect vision and language modalities, the idea of VLP was proposed in CLIP <ref type="bibr" target="#b20">[21]</ref>, which involves learning mutual knowledge from two modalities by maximizing their feature similarity. CLIP <ref type="bibr" target="#b20">[21]</ref> and later FLIP <ref type="bibr" target="#b18">[19]</ref> focus on learning cross-representation in natural language and images. However, there is a significant lack of research in the medical domain due to the complexity of the medical text and the limited availability of large-scale paired medical image-text datasets. Recently, ConVIRT <ref type="bibr" target="#b31">[32]</ref>, GLoRIA <ref type="bibr" target="#b10">[11]</ref>, and MGCA <ref type="bibr" target="#b26">[27]</ref> have made notable progress in aligning medical text and images. These methods require significant computational resources and are sometimes limited by the collapse issue of the latent space.</p><p>It has been suggested that optimal vision and language latent spaces should be of different geometry <ref type="bibr" target="#b7">[8]</ref> and latent space uniformity is considered an essential indicator to evaluate the success of learning <ref type="bibr" target="#b27">[28]</ref>. Yet, most existing VLP approaches rigorously align the vision latent space to the language space without considering the latent space geometry, which may lead to latent space collapse. As pointed out by <ref type="bibr" target="#b13">[14]</ref>, latent space collapse indicates significant information loss, which can crucially affect the robustness of the pre-trained model on downstream tasks when transferring the model to unseen domains <ref type="bibr" target="#b2">[3]</ref>. To solve this problem, contrastive learning-based approaches can be used to spread visual features over the unit sphere with good uniformity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. However, it requires a large number of negative samples in each training batch, which inevitably increases computational costs. Differently, here we address this problem by employing a orthogonality loss, which directly aligns the geometry of the latent space towards a uniform hypersphere to tackle the collapse problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The proposed M-FLAG is a simple and light VLP framework that aims to learn visual and text representations by leveraging both medical images and radiology Fig. <ref type="figure">1</ref>. M-FLAG overview. M-FLAG consists of a vision encoder EV for learning vision latent zv, a frozen language model ET for extracting medical text latent zt, and a projector p(•) to map zv to za for alignment with zt. M-FLAG employs an alignment loss L align for vision-text latent space alignment between za and zt and a orthogonality loss L orth to encourage the orthogonality of zv (Sect. 2.2). Visualization of the first 3 dominant dimensions of latent space zv via PCA <ref type="bibr" target="#b29">[30]</ref> shows the M-FLAG alleviates the dimensional collapse in the latent space, while MGCA <ref type="bibr" target="#b26">[27]</ref> and GLoRIA <ref type="bibr" target="#b10">[11]</ref> suffer the problem to different extents.</p><p>reports. We employ a freeze strategy for the text encoder E T to mitigate ambiguity in vision-text latent space alignment. Additionally, we explicitly optimize the latent space geometry using a orthogonality loss. By doing so, we encourage the visual latent space to keep a stable geometry and reduce the risk of collapse. Figure <ref type="figure">1</ref> illustrates the workflow of M-FLAG and the learned latent space geometry compared to two recent medical VLP approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Encoder and Frozen Text Encoder:</head><p>The paired medical image and text are denoted as x v , x t , respectively. As illustrated in Fig. <ref type="figure">1</ref>, we obtain the image embedding z v via the vision encoder E V and the text embedding z t via the frozen text encoder E T .</p><p>Vision Embedding: E V , the vision embedding z v ∈ R B×N is extracted from the last pooling layer of E V . N denotes the dimension of the latent space and B represents the batch size.</p><p>Text Embedding: A text encoder E T extracts text embedding of word tokens from a medical report. Similar to BERT <ref type="bibr" target="#b5">[6]</ref>, a special token [cls] is added, which aggregates the representations of all word tokens into one embedding. The embedding of [cls] is used as the text report representation and denoted as z t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frozen Language Model</head><p>In this work, we use a frozen text encoder E T , which can be obtained from any general language model. The latent space of z v is thus stable without the risk of latent space perturbation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref> due to the joint training of two encoders. Naturally, the computational cost is considerably reduced since the proposed approach only requires the training of a light vision encoder E V and a projector p(•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Alignment and Uniformity</head><p>As illustrated in Fig. <ref type="figure">1</ref>, after obtaining the visual embedding z v = E V (x v ) and text embedding z t = E T (x t ) using corresponding encoders, the vision embedding z v is projected to z a by a linear projector z a = p(z v ), so that z a is of the same dimension as z t and alignment can be performed.</p><p>We compute a composite loss L total to train the vision encoder E V and the projector p(•), which consists of two parts, alignment loss L align and orthogonality loss L orth :</p><formula xml:id="formula_0">L total = L align + L orth (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">L align = || za -zt || 2 2 = 2 -2 za T , zt<label>(2)</label></formula><formula xml:id="formula_3">L orth = i=1 1 -( zv T • zv ) ii 2 + i =j (z T v • zv ) 2 ij ,<label>(3)</label></formula><p>where {i, j} ∈ {1, ..., dim(z v )} 2 . We implement 2 -normalization on z a , z t , z v to obtain za , zt , zv . L align minimizes the discrepancy between za and zt , while L orth maximizes the independence among latent features in zv , forcing its empirical correlation matrix to be an identity matrix. In other words, we expect different latent feature dimensions to be independent. The objective of the first term on the right side in Eq. ( <ref type="formula" target="#formula_3">3</ref>) aims to optimize the diagonal elements of the empirical correlation matrix to 1, while the second term on the right side aims to reduce all non-diagonal elements to 0. Here, (•) T denotes the matrix transpose operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset for Pre-training</head><p>M-FLAG is pre-trained on the MIMIC-CXR (MIMIC) dataset <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, which contains 227,827 image-text pairs with chest X-ray (CXR) images and radiology reports. Following the preprocessing procedure of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>, 213,384 image-text pairs are used. We use ResNet50 <ref type="bibr" target="#b9">[10]</ref> as E V and frozen CXR-BERT <ref type="bibr" target="#b0">[1]</ref> as E T .</p><p>Pre-training takes 100 epochs on 8 A100 GPUs, with a batch size of 128 for each GPU and a learning rate of 0.001 using the LARS <ref type="bibr" target="#b30">[31]</ref> optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets for Downstream Tasks</head><p>The pre-trained model is evaluated on 3 downstream tasks across 5 datasets:</p><p>Medical image classification is implemented on MIMIC, CheXpert (CXP), and NIH <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref> datasets, each consisting of images from 14 disease categories.</p><p>To reduce sampling bias and maintain consistency, we follow the dataset split in CheXclusion <ref type="bibr" target="#b23">[24]</ref> and evaluate the macro AUC scores.</p><p>Image segmentation is evaluated on two datasets, RSNA <ref type="bibr" target="#b24">[25]</ref> (pneumonia segmentation) and SIIM <ref type="bibr" target="#b16">[17]</ref> (pneumothorax segmentation). Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>, we use U-Net <ref type="bibr" target="#b22">[23]</ref> as the segmentation backbone. The pre-trained model is used as the frozen encoder of the U-net <ref type="bibr" target="#b22">[23]</ref> and we only update the decoder of the Unet during fine-tuning. We evaluate segmentation performance using Dice scores.</p><p>Object detection is implemented on the RSNA <ref type="bibr" target="#b24">[25]</ref> dataset for pneumonia detection, using the preprocessing techniques outlined in <ref type="bibr" target="#b26">[27]</ref>. Following <ref type="bibr" target="#b26">[27]</ref>, we use YOLOv3 <ref type="bibr" target="#b21">[22]</ref> as the detection framework. We employ the pre-trained vision encoder of M-FLAG as the backbone and only fine-tune the detection head. The detection task is evaluated using mean average precision (mAP) with the intersection of union (IoU) thresholds ranging from 0.4 to 0.75.</p><p>Table <ref type="table" target="#tab_0">1</ref> reports the data split details. For all downstream tasks, we fine-tune using 1%, 10%, 100% of the train data on a single A100 GPU. Classification MIMIC <ref type="bibr" target="#b15">[16]</ref> [24] 215,187 5,000 23,137 CXP <ref type="bibr" target="#b11">[12]</ref> [24] 167,185 5,000 19,027 NIH <ref type="bibr" target="#b28">[29]</ref> [24] 100,747 5,000 6,373</p><p>Segmentation RSNA <ref type="bibr">[25] [11, 27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Image Classification:</head><p>The AUC scores on MIMIC, CXP, and CXR14 are reported in Table <ref type="table" target="#tab_2">2</ref>. It shows that M-FLAG consistently outperforms all baseline methods across almost all datasets and data fractions. Notably, M-FLAG achieves superior performance while using only 22% of the trainable parameters compared to other methods. While MGCA <ref type="bibr" target="#b26">[27]</ref> slightly outperforms our method only when fine-tuning on 10% of the CXP dataset, it requires more than five times parameters than M-FLAG.</p><p>Segmentation and Object Detection: Table <ref type="table" target="#tab_3">3</ref> shows that M-FLAG outperforms all SOTA methods across all datasets and data fractions in segmentation and detection tasks. In the segmentation task, M-FLAG achieves the highest Dice score across all fractions of both the SIIM and RSNA datasets. Interestingly, even when fine-tuned with only 1% of the data in RSNA, M-FLAG outperforms the ImageNet pre-trained model fine-tuned with 100% of the data. Similarly, in the object detection task, M-FLAG achieves the highest mean average precision (mAP) across all data fractions of the RSNA dataset. When fine-tuned with only 10% of the data, M-FLAG still outperforms the ImageNet pre-trained model with 100% fine-tuning. These results indicate the advantages of using a frozen language model and introducing orthogonality loss during pre-training, which may yield more informative latent representations that are better suited for downstream tasks. Overall, the improvements achieved by M-FLAG across diverse downstream tasks demonstrate its effectiveness and versatility in medical image analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dimensional Collapse Analysis</head><p>Recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> have highlighted that latent space learned via selfsupervised learning can suffer from issues such as complete collapse or dimensional collapse, which would lead to poor performance for downstream tasks. Figure <ref type="figure">1</ref> bottom right panel shows that both MGCA and GLoRIA <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> suffer from dimensional collapse. Figure <ref type="figure" target="#fig_0">2</ref> shows that if the last n layers of the language model in M-FLAG are not frozen, the latent space geometry would also exhibit varying degrees of collapse. This indicates the importance of using a frozen language model. Quantitative results in Tables 2, 3, 4 and 5 and qualitative visualization in Figs. <ref type="figure">1</ref> and<ref type="figure" target="#fig_0">2</ref> further demonstrate that a collapsed latent space can impair the performance for various downstream tasks, especially for segmentation and detection. These findings highlight the usefulness of a frozen language model in preventing latent space collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>Ablation Study: Table <ref type="table" target="#tab_5">4</ref> presents the results of an ablation study to evaluate the impact of L orth and L align on model performance. Across all experiments, the proposed version of M-FLAG achieves the highest performance, with a clear advantage over implementations that only use L orth or L align in pre-training.</p><p>The performance of the model pre-trained with only L align drops dramatically in segmentation and detection tasks, although less severe in the classification tasks.</p><p>On the other hand, the model pre-trained with only L orth does not suffer severe performance drop across the three tasks, indicating that the uniform latent space could have a considerable contribution to the performance of M-FLAG. Overall, these results underscore the importance of both loss functions in M-FLAG and highlight their complementary contributions.  Comparing M-FLAG with Frozen vs. Unfrozen Language Models: We conducted further experiments to evaluate the performance of M-FLAG while unfreezing the last few layers of the language model. This not only increases the number of trainable parameters but also influences the model performance.</p><p>Table <ref type="table" target="#tab_6">5</ref> shows that when the language model is unfrozen, the performance slightly drops, compared to M-FLAG with the frozen language model (proposed). M-FLAG achieves a better performance with an average improvement of 2.18% than its Unfreeze 1-6 variants on the NIH dataset and an average improvement of 4.32% on the SIIM dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Simple architecture means low computational cost and stable training. In this work, we propose a simple and efficient VLP framework that includes a frozen language model and a latent space orthogonality loss function. Extensive experiments show that M-FLAG outperforms SOTA medical VLP methods with 78% fewer parameters. M-FLAG also demonstrates its robustness by achieving the highest performance when transferred to unseen test sets and diverse downstream tasks for medical image classification, segmentation, and detection. This indicates the benefits of freezing the language model and regularizing the latent space. The results exhibit promising potential for improving the pre-training of vision-language models in the medical domain. In addition, the latent space geometry explored in this work provides useful insight for future work in VLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the first 3 dominant PCA dimensions of latent space on NIH dataset. M-FLAG (green) is compared to its variants (red) when the last n layers of the language model are not frozen. (Color figure online)</figDesc><graphic coords="8,43,29,364,28,337,48,61,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,63,48,353,96,325,84,159,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets are split following<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Split</cell><cell>Train</cell><cell>Valid Test</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>AUC scores (%) of image classification tasks on MIMIC, CXP, NIH datasets with 1%, 10%, 100% labeled data.</figDesc><table><row><cell>Method</cell><cell>Trainable</cell><cell>MIMIC</cell><cell>CXP</cell><cell>NIH</cell></row><row><cell></cell><cell cols="4">parameters(M) 1% 10% 100% 1% 10% 100% 1% 10% 100%</cell></row><row><cell>Random</cell><cell>38.3</cell><cell cols="3">53.6 66.5 78.2 62.6 69.0 76.9 56.4 67.1 76.9</cell></row><row><cell>ImageNet</cell><cell>38.3</cell><cell cols="3">67.8 70.5 79.3 63.7 70.7 77.7 59.7 68.9 78.1</cell></row><row><cell cols="2">ConVIRT [32] 110.3</cell><cell cols="3">67.8 73.4 80.1 63.2 71.3 77.7 60.0 69.0 76.6</cell></row><row><cell cols="2">GLoRIA [11] 113.1</cell><cell cols="3">67.5 72.6 80.1 62.9 69.0 77.8 60.1 71.2 77.7</cell></row><row><cell>MGCA [27]</cell><cell>113.4</cell><cell cols="3">68.4 74.4 80.2 63.4 72.1 78.1 61.1 67.8 77.3</cell></row><row><cell>M-FLAG</cell><cell>25.6</cell><cell cols="3">69.5 74.8 80.2 64.4 71.4 78.1 62.2 71.6 78.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Dice (%) of segmentation tasks on SIIM, RSNA datasets. mAP (%) of detection task on RSNA dataset. All tasks are fine-tuned with 1%, 10%, 100% labeled data.</figDesc><table><row><cell>Method</cell><cell cols="2">Segmentation</cell><cell></cell><cell></cell><cell cols="2">Object Detection</cell></row><row><cell></cell><cell cols="2">SIIM (Dice%)</cell><cell cols="2">RSNA (Dice%)</cell><cell cols="2">RSNA (mAP%)</cell></row><row><cell></cell><cell cols="6">1% 10% 100% 1% 10% 100% 1% 10% 100%</cell></row><row><cell>Random</cell><cell>9.0</cell><cell cols="2">28.6 54.3 6.9</cell><cell cols="2">10.6 18.5 1.0</cell><cell>4.0</cell><cell>8.9</cell></row><row><cell>ImageNet</cell><cell cols="5">10.2 35.5 63.5 34.8 39.9 64.0 3.6</cell><cell>8.0</cell><cell>15.7</cell></row><row><cell cols="6">ConVIRT [32] 25.0 43.2 59.9 55.0 67.4 67.5 8.2</cell><cell>15.6 17.9</cell></row><row><cell cols="7">GLoRIA [11] 37.4 57.1 64.0 60.3 68.7 68.3 11.6 16.1 24.8</cell></row><row><cell>MGCA [27]</cell><cell cols="6">49.7 59.3 64.2 63.0 68.3 69.8 12.9 16.8 24.9</cell></row><row><cell>M-FLAG</cell><cell>52.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>61.2 64.8 64.6 69.7 70.5 13.7 17.5 25.4</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance for ablation study of M-FLAG. "only L orth /L align " indicates that M-FLAG is pre-trained only with L orth /L align .</figDesc><table><row><cell>Method</cell><cell>MIMIC</cell><cell>CXP</cell><cell>NIH</cell><cell>SIIM</cell><cell>RSNA</cell></row><row><cell></cell><cell cols="5">AUC (%) AUC (%) AUC (%) Dice (%) mAP (%)</cell></row><row><cell></cell><cell>1%</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell></row><row><cell cols="2">only L align 69.3</cell><cell>62.6</cell><cell>61.4</cell><cell>45.7</cell><cell>12.1</cell></row><row><cell cols="2">only L orth 68.6</cell><cell>61.5</cell><cell>61.2</cell><cell>50.5</cell><cell>13.2</cell></row><row><cell>M-FLAG</cell><cell>69.5</cell><cell>64.4</cell><cell>62.2</cell><cell>52.5</cell><cell>13.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Performance of M-FLAG compared to its unfrozen variants. Unfreezen indicates that the last n layers of the language model are unfrozen.</figDesc><table><row><cell>Method</cell><cell>Trainable</cell><cell>MIMIC</cell><cell>CXP</cell><cell>NIH</cell><cell>SIIM</cell><cell>RSNA</cell></row><row><cell></cell><cell cols="6">Parameters(M) AUC (%) AUC (%) AUC (%) Dice (%) mAP (%)</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell><cell>1%</cell></row><row><cell cols="2">Unfreeze1 32.6</cell><cell>67.8</cell><cell>63.1</cell><cell>59.7</cell><cell>47.2</cell><cell>12.5</cell></row><row><cell cols="2">Unfreeze2 39.7</cell><cell>68.7</cell><cell>63.3</cell><cell>60.6</cell><cell>48.9</cell><cell>12.3</cell></row><row><cell cols="2">Unfreeze3 46.8</cell><cell>68.8</cell><cell>63.7</cell><cell>60.7</cell><cell>45.8</cell><cell>10.7</cell></row><row><cell cols="2">Unfreeze4 53.9</cell><cell>68.7</cell><cell>62.6</cell><cell>60.1</cell><cell>50.3</cell><cell>11.4</cell></row><row><cell cols="2">Unfreeze5 60.9</cell><cell>68.2</cell><cell>64.1</cell><cell>59.2</cell><cell>46.8</cell><cell>11.8</cell></row><row><cell cols="2">Unfreeze6 68.1</cell><cell>68.2</cell><cell>63.7</cell><cell>59.9</cell><cell>50.1</cell><cell>11.5</cell></row><row><cell cols="2">M-FLAG 25.6</cell><cell>69.5</cell><cell>64.4</cell><cell>62.2</cell><cell>52.5</cell><cell>13.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. <rs type="person">C. Liu</rs> and <rs type="person">R. Arcucci</rs> were supported in part by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">EP/T003189/1</rs> <rs type="projectName">Health assessment across biological length scales for personal pollution exposure and its mitigation (INHALE</rs>), <rs type="funder">EPSRC</rs> <rs type="grantName">Programme Grant PREMIERE</rs> (<rs type="grantNumber">EP/T000414/1</rs>). <rs type="person">W. Bai</rs> and <rs type="person">M. Qiao</rs> were supported by <rs type="funder">EPSRC</rs> <rs type="grantName">Project Grant DeepGeM</rs> (<rs type="grantNumber">EP/W01842X/1</rs>). A. Shah was supported by a <rs type="grantName">MRC Clinical Academic Research Partnership award</rs> (<rs type="grantNumber">MR/TOO5572/1</rs>) and by an <rs type="funder">MRC</rs> centre grant <rs type="funder">MRC</rs> (<rs type="grantNumber">MR/R015600/1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ectsGeE">
					<idno type="grant-number">EP/T003189/1</idno>
					<orgName type="project" subtype="full">Health assessment across biological length scales for personal pollution exposure and its mitigation (INHALE</orgName>
				</org>
				<org type="funding" xml:id="_5TcMPmh">
					<idno type="grant-number">EP/T000414/1</idno>
					<orgName type="grant-name">Programme Grant PREMIERE</orgName>
				</org>
				<org type="funding" xml:id="_vFzMZQe">
					<idno type="grant-number">EP/W01842X/1</idno>
					<orgName type="grant-name">Project Grant DeepGeM</orgName>
				</org>
				<org type="funding" xml:id="_4AsEJyn">
					<idno type="grant-number">MR/TOO5572/1</idno>
					<orgName type="grant-name">MRC Clinical Academic Research Partnership award</orgName>
				</org>
				<org type="funding" xml:id="_DGeVwak">
					<idno type="grant-number">MR/R015600/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Learning -Transfer Learning</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-51" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning in computer vision: a critical review of emerging techniques and application scenarios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">100134</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perfectly balanced: improving transfer and robustness of supervised contrastive learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generative textguided 3D vision-language pretraining for unified medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04811</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning-enabled medical computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01512</idno>
		<title level="m">Latent topology induction for understanding contextualized representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GLoRIA: a multimodal globallocal representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">How to train BERT with an academic budget</title>
		<author>
			<persName><forename type="first">P</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berchansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07705</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding dimensional collapse in contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
		<title level="m">SIIM-ACR Pneumothorax Segmentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12311</idno>
		<title level="m">Frozen language model helps ECG zero-shot learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling language-image pretraining via masking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00794</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep feature correlation learning for multi-modal remote sensing image registration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: an incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CheXclusion: fairness gaps in deep chest X-ray classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocomputing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible Pneumonia</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Halabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Prevedello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180041</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Med-UniC: unifying cross-lingual medical vision-language pretraining by diminishing bias</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19894</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-granularity crossmodal alignment for generalized medical visual representation learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="33536" to="33549" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemom. Intell. Lab. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A review: deep learning for medical image segmentation using multi-modality fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Array</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100004</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
