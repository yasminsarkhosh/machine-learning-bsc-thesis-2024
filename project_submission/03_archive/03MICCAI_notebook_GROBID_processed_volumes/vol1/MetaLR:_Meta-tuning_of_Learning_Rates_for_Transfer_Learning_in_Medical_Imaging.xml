<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging</title>
				<funder ref="#_TXh8JmQ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_4fRrsqv">
					<orgName type="full">GuangDong Basic and Applied Basic Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixiong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingxian</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Software School</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD7E0C996273F9428375515BE05EB533</idno>
					<idno type="DOI">10.1007/978-3-031-43907-067.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image analysis</term>
					<term>Meta-learning</term>
					<term>Transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common finetuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based learning rate (LR) tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various medical applications show that MetaLR outperforms previous state-of-the-art (SOTA) fine-tuning strategies. Codes are released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transfer learning has become a standard practice in medical image analysis as collecting and annotating data in clinical scenarios can be costly. The pre-trained parameters endow better generalization to DNNs than the models trained from scratch <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>. A popular approach to enhancing model transferability is by pretraining on domains similar to the targets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b29">[29]</ref>. However, utilizing specialized pre-training for all medical applications becomes impractical due to the Fig. <ref type="figure">1</ref>. The motivation of MetaLR. Previous works fix transferable layers in pre-trained models to prevent them from catastrophic forgetting. It is inflexible and labor-expensive for this method to find the optimal scheme. MetaLR uses meta-learning to automatically optimize layer-wise LR for fine-tuning.</p><p>diversity between domains and tasks and privacy concerns related to pre-training data. Consequently, recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> has focused on improving the generalization capabilities of existing pre-trained DNN backbones through fine-tuning techniques.</p><p>Previous studies have shown that the transferability of lower layers is often higher than higher layers that are near the model output <ref type="bibr" target="#b25">[26]</ref>. Layer-wise finetuning <ref type="bibr" target="#b22">[23]</ref>, was thus introduced to preserve the transferable low-level knowledge by fixing lower layers. But recent studies <ref type="bibr" target="#b6">[7]</ref> revealed that the lower layers may also be sensitive to small domains like medical images. Given the two issues, transferability for medical tasks becomes more complicated <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. It can even be irregular among layers for medical domains far from pre-training data <ref type="bibr" target="#b6">[7]</ref>. Given the diverse medical domains and model architectures, there is currently no universal guideline to follow to determine whether a particular layer should be retrained for a given target domain.</p><p>To search for optimal layer combinations for fine-tuning, manually selecting transferable layers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> can be a solution, but it requires a significant amount of human labor and computational cost. In order to address this issue and improve the flexibility of fine-tuning strategies, we propose controlling the fine-tuning process with layer-wise learning rates (LRs), rather than simply manually fixing or updating the layers (see Fig. <ref type="figure">1</ref>). Our proposed algorithm, Meta Learning Rate (MetaLR), is based on meta-learning <ref type="bibr" target="#b12">[13]</ref> and adaptively adjusts LRs for each layer according to transfer feedback. It treats the layer-wise LRs as metaknowledge and optimizes them to improve the model generalization. Larger LRs indicate less transferability of corresponding layers and require more updating, while smaller LRs preserve transferable knowledge in the layers. Inspired by <ref type="bibr" target="#b19">[20]</ref>, we use an online adaptation strategy of LRs with a time complexity of O(n), instead of the computationally-expensive bi-level O(n 2 ) meta-learning. We also enhance the algorithm's performance and stability with a proportional hyper-LR (LR for LR) and a validation scheme on training data batches.</p><p>In summary, this work makes the following three contributions. 1) We introduce MetaLR, a meta-learning-based LR tuner that can adaptively adjust layerwise LRs based on transfer learning feedback from various medical domains.</p><p>2) We enhance MetaLR with a proportional hyper-LR and a validation scheme using batched training data to improve the algorithm's stability and efficacy. 3) Extensive experiments on both lesion detection and tumor segmentation tasks were conducted to demonstrate the superior efficiency and performance of Met-aLR compared to current SOTA medical fine-tuning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This section provides a detailed description of the proposed MetaLR. It is a meta-learning-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> approach that determines the appropriate LR for each layer based on its transfer feedback. It is important to note that fixing transferable layers is a special case of this method, where fixed layers always have zero LRs. First, we present the theoretical formulation of MetaLR. Next, we discuss online adaptation for efficiently determining optimal LRs. Finally, we demonstrate the use of a proportional hyper-LR and a validation scheme with batched training data to enhance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formulation of Meta Learning Rate</head><p>Let (x, y) denote a sample-label pair, and {(x i , y i ) | i = 1, ..., N } be the training data. The validation dataset {(x v i , y v i ) | i = 1, ..., M } is assumed to be independent and identically distributed as the training dataset. Let ŷ = Φ(x, θ) be the prediction for sample x from deep model Φ with parameters θ. In standard training of DNNs, the aim is to minimize the expected risk for the training set:  </p><formula xml:id="formula_0">M i=1 L(ŷ v i , y v i ).</formula><p>Based on the generalization, one can tune the hyper-parameters of the training process to improve the model. The key idea of MetaLR is considering the layer-wise LRs as self-adaptive hyper-parameters during the training and automatically adjusting them to achieve better model generalization. We denote the LR and model parameters for the layer j at the iteration t as α t j and θ t j . The LR scheduling scheme α = {α t j | j = 1, ..., d; t = 1, ..., T } is what MetaLR wants to learn, affecting which local optimal θ * (α) the model parameters θ t = {θ t j | j = 1, ..., d} will converge to. The optimal parameters θ * (α) are given by optimization on the training data. At the same time, the best LR tuning scheme α * can be optimized based on the feedback for θ * (α) from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Online Meta Learning Rate Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Training data D, validation data D v , initial model parameter {θ 0 1 , ..., θ 0 d }, LRs {α 0 1 , ..., α 0 d }, batch size n, max iteration T; Output:</p><p>Final model parameter</p><formula xml:id="formula_1">θ T = {θ T 1 , ..., θ T d }; 1: for t = 0 : T -1 do 2: {(xi, yi) | i = 1, ..., n} ← TrainDataLoader(D, n) ; 3: {(x v i , y v i ) | i = 1, ..., n} ← ValidDataLoader(D v , n) ; 4:</formula><p>Step forward for one step to get { θt 1 (α  <ref type="formula" target="#formula_7">4</ref>); 7: end for the validation loss. This problem can be formulated as the following bi-level optimization problem:</p><formula xml:id="formula_2">min α 1 M M i=1 L(Φ(x v i , θ * (α)), y v i ), s.t. θ * (α) = arg min θ 1 N N i=1 L(Φ(x i , θ), y i ).<label>(1)</label></formula><p>MetaLR aims to use the validation set to optimize α through an automatic process rather than a manual one. The optimal scheme α * can be found by a nested optimization <ref type="bibr" target="#b12">[13]</ref>, but it is too computationally expensive in practice. A faster and more lightweight method is needed to make it practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Online Learning Rate Adaptation</head><p>Inspired by the online approximation <ref type="bibr" target="#b19">[20]</ref>, we propose efficiently adapting the LRs and model parameters online. The motivation of the online LR adaptation is updating the model parameters θ t and LRs {α t j | j = 1, 2, ..., d} within the same loop. We first inspect the descent direction of parameters θ t j on the training loss landscape and adjust the α t j based on the transfer feedback. Positive feedback (lower validation loss) means the LRs are encouraged to increase.</p><p>We adopt Stochastic Gradient Descent (SGD) as the optimizer to conduct the meta-learning. The whole training process is summarized in Algorithm 1. At the iteration t of training, a training data batch {(x i , y i ) | i = 1, ..., n} and a validation data batch {(x v i , y v i ) | i = 1, ..., n} are sampled, where n is the size of the batches. First, the parameters of each layer are updated once with the current LR according to the descent direction on training batch.</p><formula xml:id="formula_3">θt j (α t j ) = θ t j -α t j ∇ θj ( 1 n n i=1 L(Φ(x i , θ t j ), y i )), j = 1, ..., d. (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>This step of updating aims to get feedback for LR of each layer. After taking derivative of the validation loss w.r.t. α t j , we can utilize the gradient to know how the LR for each layer should be adjusted. So the second step of MetaLR is to move the LRs along the meta-objective gradient on the validation data:</p><formula xml:id="formula_5">α t+1 j = α t j -η∇ αj ( 1 n n i=1 L(Φ(x v i , θt j (α t j )), y v i )), (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where η is the hyper-LR. Finally, the updated LRs can be employed to optimize the model parameters through gradient descent truly.</p><formula xml:id="formula_7">θ t+1 j = θ t j -α t+1 j ∇ θj ( 1 n n i=1 L(Φ(x i , θ t j ), y i )). (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>For practical use, we constrain the LR for each layer to be α t j ∈ [10 -6 , 10 -2 ]. Online MetaLR optimizes the layer-wise LRs as well as the training objective on a single task, which differentiates it from traditional meta-learning algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> that train models on multiple small tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proportional Hyper Learning Rate</head><p>In practice, LRs are often tuned in an exponential style (e.g., 1e-3, 3e-3, 1e-2) and are always positive values. However, if a constant hyper-LR is used, it will linearly update its corresponding LR regardless of numerical constraints. This can lead to fluctuations in the LR or even the risk of the LR becoming smaller than 0 and being truncated. To address this issue, we propose using a proportional hyper-LR η = β × α t j , where β is a pre-defined hyper-parameter. This allows us to rewrite Eq. (3) as:</p><formula xml:id="formula_9">α t+1 j = α t j (1 -β∇ αj ( 1 n n i=1 L(Φ(x v i , θt j (α t j )), y v i ))).<label>(5)</label></formula><p>The exponential update of α t j guarantees its numerical stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Generalizability Validation on Training Data Batch</head><p>One limitation of MetaLR is that the LRs are updated using separate validation data, which reduces the amount of data available for the training process. This can be particularly problematic for medical transfer learning, where the amount of downstream data has already been limited. In Eq. 2 and Eq. 3, the update of model parameter θ t j and LR α t j is performed using different datasets to ensure that the updated θ t j can be evaluated for generalization without being influenced by the seen data. As an alternative, but weaker, approach, we explore using another batch of training data for Eq. 3 to evaluate generalization. Since this batch was not used in the update of Eq. 2, it may still perform well for validation in meta-learning. The effect of this approach is verified in Sect. 3.2, and the differences between the two methods are analyzed in Sect. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We extensively evaluate MetaLR on four transfer learning tasks (as shown in Table <ref type="table" target="#tab_1">1</ref>). To ensure the reproducibility of the results, all pre-trained models (USCL <ref type="bibr" target="#b8">[9]</ref>, ImageNet <ref type="bibr" target="#b10">[11]</ref>, C2L <ref type="bibr" target="#b27">[28]</ref>, Models Genesis <ref type="bibr" target="#b29">[29]</ref>) and target datasets (POCUS <ref type="bibr" target="#b4">[5]</ref>, BUSI <ref type="bibr" target="#b0">[1]</ref>, Chest X-ray <ref type="bibr" target="#b16">[17]</ref>, LiTS <ref type="bibr" target="#b3">[4]</ref>) are publicly available. In our work, we consider models pre-trained on both natural and medical image datasets, with three target modalities and three target organs, which makes our experimental results more credible. For the lesion detection tasks, we used ResNet-18 <ref type="bibr" target="#b14">[15]</ref> with the Adam optimizer. The initial learning rate (LR) and hyper-LR coefficient β are set to 10 -3 and 0.1, respectively. In addition, we use 25% of the training set as the validation set for meta-learning. For the segmentation task, we use 3D U-Net <ref type="bibr" target="#b9">[10]</ref> with the SGD optimizer. The initial LR and hyper-LR coefficient β are set to 10 -2 and 3 × 10 -3 , respectively. The validation set for the LiTS segmentation dataset comprises 23 samples from the training set of size 111. All experiments are implemented using PyTorch 1.10 on an Nvidia RTX A6000 GPU. We report the mean values and standard deviations for each experiment with five different random seeds. For more detailed information on the models and hyper-parameters, please refer to our supplementary material. ImageNet <ref type="bibr" target="#b10">[11]</ref> supervised BUSI <ref type="bibr" target="#b0">[1]</ref> Breast US Tumor detection 780 images MIMIC-CXR <ref type="bibr" target="#b15">[16]</ref> C2L <ref type="bibr" target="#b27">[28]</ref> Chest X-ray <ref type="bibr" target="#b16">[17]</ref> Lung X-ray Pneumonia detection 5856 images LIDC-IDRI <ref type="bibr" target="#b2">[3]</ref> Models Genesis <ref type="bibr" target="#b29">[29]</ref> LiTS <ref type="bibr" target="#b3">[4]</ref> Liver CT Liver segmentation 131 volumes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>In order to evaluate the effectiveness of our proposed method, we conduct an ablation study w.r.t. the basic MetaLR algorithm, the proportional hyper-LR, and batched-training-data validation (as shown in Table <ref type="table" target="#tab_2">2</ref>). When applying only the basic MetaLR, we observe only marginal performance improvements for the four downstream tasks. We conjecture that this is due to two reasons: Firstly, the constant hyper-LR makes the training procedures less stable than direct training, which is evident from the larger standard deviation of performance. Secondly, part of the training data are split for validation, which can be detrimental to the performance. After applying the proportional hyper-LR, significant improvements are in both the performance and its stability. Moreover, although the generalization validation on the training data batch may introduce bias, providing sufficient training data ultimately benefits the performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparative Experiments</head><p>In our study, we compare MetaLR with several other fine-tuning schemes, including tuning only the last layer / all layers with constant LRs, layer-wise finetuning <ref type="bibr" target="#b22">[23]</ref>, bi-directional fine-tuning <ref type="bibr" target="#b6">[7]</ref>, and AutoLR <ref type="bibr" target="#b21">[22]</ref>. The U-Net finetuning scheme proposed by Amiri et al. <ref type="bibr" target="#b1">[2]</ref> was also evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Lesion Detection Tasks.</head><p>MetaLR consistently shows the best performance on all downstream tasks (Table <ref type="table" target="#tab_3">3</ref>). It shows 1%-2.3% accuracy improvements compared to direct training (i.e., tuning all layers) because it takes into account the different transferabilities of different layers. While manual picking methods, such as layer-wise and bi-directional fine-tuning, also achieve higher performance, they require much more training time (5×-50×) for searching the best tuning scheme. On the other hand, AutoLR is efficient, but its strong hypothesis harms its performance sometimes. In contrast, MetaLR makes no hypothesis about transferability and learns appropriate layer-wise LRs on different domains. Moreover, its performance improvements are gained with only 1.5×-2.5× training time compared with direct training.</p><p>Results on Segmentation Task. MetaLR achieves the best Dice performance on the LiTS segmentation task (Table <ref type="table" target="#tab_4">4</ref>). Unlike ResNet for lesion detection, the U-Net family has a more complex network topology. With skip connections, there are two interpretations <ref type="bibr" target="#b1">[2]</ref> of depths for layers: 1) the left-most layers are the shallowest, and 2) the top layers of the "U" are the shallowest. This makes the handpicking methods even more computationally expensive. However, MetaLR  updates the LR for each layer according to their validation gradients, and its training efficiency is not affected by the complex model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion and Findings</head><p>The LRs Learned with MetaLR. For ResNet-18 (Fig. <ref type="figure" target="#fig_2">2</ref> (a)), the layer-wise LRs fluctuate drastically during the first 100 iterations. However, after iteration 100, all layers except the first layer "Conv1" become stable at different levels.</p><p>The first layer has a decreasing LR (from 2.8 × 10 -3 to 3 × 10 -4 ) throughout the process, reflecting its higher transferability. For 3D U-Net (Fig. <ref type="figure" target="#fig_2">2 (b</ref>)), the middle layers of the encoder "Down-128" and "Down-256" are the most transferable and have the lowest LRs, which is difficult for previous fine-tuning schemes to discover. As expected, the randomly initialized "FC" and "Out" layers have the largest LRs since they are not transferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effectiveness of Proportional Hyper-LR and Training Batches</head><p>Validation. We illustrate the LR curves with a constant hyper-LR instead of a proportional one. The LR curves of "Block 3-1" and "Block 4-2" become much more fluctuated (Fig. <ref type="figure" target="#fig_2">2 (c</ref>)). This instability may be the key reason for the instability of performance when using a constant hyper-LR. Furthermore, we surprisingly find that the learned LRs are similar to the curves learned when validated on the training set when using a separate validation set Fig. <ref type="figure" target="#fig_2">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a new fine-tuning scheme, MetaLR, for medical transfer learning. It achieves significantly superior performance to the previous SOTA fine-tuning algorithms. MetaLR alternatively optimizes model parameters and layer-wise LRs in an online meta-learning fashion with a proportional hyper-LR. It learns to assign lower LRs for the layers with higher transferability and higher LRs for the less transferable layers. The proposed algorithm is easy to implement and shows the potential to replace manual layer-wise fine-tuning schemes. Future works include adapting MetaLR to a wider variety of clinical tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ŷ i , y i ) with fixed training hyper-parameters, where L(ŷ, y) is the loss function for the current task. The model generalization can be evaluated by the validation loss1   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>M</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The LR curves for MetaLR on POCUS detection (a), on LiTS segmentation (b), with constant hyper-LR (c), and with a separate validation set (d).</figDesc><graphic coords="8,191,76,212,57,203,44,157,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Pre-training data, algorithms, and target tasks.</figDesc><table><row><cell>Source</cell><cell>Pre-train Method</cell><cell>Target</cell><cell cols="2">Organ Modality Task</cell><cell>Size</cell></row><row><cell>US-4 [9]</cell><cell>USCL [9]</cell><cell>POCUS [5]</cell><cell>Lung US</cell><cell cols="2">COVID-19 detection 2116 images</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for MetaLR, hyper-LR, and validation data. The baseline is the direct tuning of all layers with constant LRs. The default setting for MetaLR is a constant hyper-LR of 10 -3 and a separate validation set.</figDesc><table><row><cell>MetaLR Prop. hyper-LR Val. on trainset POCUS</cell><cell>BUSI</cell><cell cols="2">Chest X-ray LiTS</cell></row><row><cell cols="3">91.6 ± 0.8 84.4 ± 0.7 94.8 ± 0.3</cell><cell>93.1 ± 0.4</cell></row><row><cell cols="3">91.9 ± 0.6 84.9 ± 1.3 95.0 ± 0.4</cell><cell>93.2 ± 0.8</cell></row><row><cell cols="3">93.6 ± 0.4 85.2 ± 0.8 95.3 ± 0.2</cell><cell>93.3 ± 0.6</cell></row><row><cell cols="3">93.0 ± 0.3 86.3 ± 0.7 95.5 ± 0.2</cell><cell>93.9 ± 0.5</cell></row><row><cell cols="3">93.9 ± 0.4 86.7 ± 0.7 95.8 ± 0.3</cell><cell>94.2 ± 0.5</cell></row><row><cell cols="3">• Final MetaLR outperforms baseline with p-values of 0.0014, 0.0016, 0.0013, 0.0054.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparative experiments on lesion detection. We report sensitivities (%) of the abnormalities, overall accuracy (%), and training time on each task.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">POCUS</cell><cell></cell><cell></cell><cell>BUSI</cell><cell></cell><cell>Chest X-ray</cell></row><row><cell></cell><cell>COVID</cell><cell>Pneu.</cell><cell>Acc</cell><cell cols="2">Time Benign</cell><cell cols="2">Malignant Acc</cell><cell>Time Pneu.</cell><cell>Acc</cell><cell>Time</cell></row><row><cell cols="7">Last Layer 77.9 ± 2.1 84.0 ± 1.3 84.1 ± 0.2 15.8 m 83.5 ± 0.4 47.6 ± 4.4</cell><cell cols="2">66.8 ± 0.5 4.4 m 99.7 ± 1.3 87.8 ± 0.6 12.7 m</cell></row><row><cell cols="7">All Layers 85.8 ± 1.7 90.0 ± 1.9 91.6 ± 0.8 16.0 m 90.4 ± 1.5 77.8 ± 3.5</cell><cell cols="2">84.4 ± 0.7 4.3 m 98.8 ± 0.2 94.8 ± 0.3 12.9 m</cell></row><row><cell cols="5">Layer-wise 87.5 ± 1.0 92.3 ± 1.3 92.1 ± 0.3 2.4 h</cell><cell cols="2">90.8 ± 1.2 75.7 ± 2.6</cell><cell cols="2">85.6 ± 0.4 39.0 m 97.9 ± 0.3 95.2 ± 0.2 1.9 h</cell></row><row><cell>Bi-direc</cell><cell cols="6">90.1 ± 1.2 92.5 ± 1.5 93.6 ± 0.2 12.0 h 92.2 ± 1.0 77.1 ± 3.5</cell><cell cols="2">86.5 ± 0.5 3.2 h</cell><cell>98.4 ± 0.3 95.4 ± 0.1 9.7 h</cell></row><row><cell>AutoLR</cell><cell cols="6">89.8 ± 1.6 89.7 ± 1.5 90.4 ± 0.8 17.5 m 90.4 ± 1.8 76.2 ± 3.2</cell><cell cols="2">84.9 ± 0.8 4.9 m 95.4 ± 0.5 93.0 ± 0.8 13.3 m</cell></row><row><cell>MetaLR</cell><cell cols="6">94.8 ± 1.2 93.1 ± 1.5 93.9 ± 0.4 24.8 m 92.2 ± 0.7 75.6 ± 3.6</cell><cell cols="2">86.7 ± 0.7 6.0 m 97.4 ± 0.4 95.8 ± 0.3 26.3 m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparative experiments on LiTS liver segmentation task. ± 5.5 71.5 ± 4.2 33.5 ± 3.4 2.5 h All Layers 94.0 ± 0.6 93.1 ± 0.7 93.1 ± 0.4 2.6 h Layer-wise 92.1 ± 1.3 96.4 ± 0.4</figDesc><table><row><cell>Method</cell><cell>PPV</cell><cell cols="2">Sensitivity Dice</cell><cell>Time</cell></row><row><cell cols="5">Last Layer 26.1 93.7 ± 0.3 41.6 h</cell></row><row><cell>Bi-direc</cell><cell cols="2">92.4 ± 1.1 96.1 ± 0.2</cell><cell cols="2">93.8 ± 0.1 171.2 h</cell></row><row><cell cols="3">Mina et al. 92.7 ± 1.2 93.2 ± 0.5</cell><cell cols="2">92.4 ± 0.5 2.6 h</cell></row><row><cell>MetaLR</cell><cell cols="2">94.4 ± 0.9 93.6 ± 0.4</cell><cell cols="2">94.2 ± 0.5 5.8 h</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>(d)). With similar learned LR curves and more training data, it is reasonable that batched training set validation can be an effective alternative to the basic MetaLR.Limitations of MetaLR.Although MetaLR improves fine-tuning for medical image analysis, it has several limitations. First, the gradient descent of Eq. (3) takes more memory than the usual fine-tuning strategy, it may restrict the batch size available during training. Second, MetaLR sometimes does not get converged LRs after the parameters converge, which may harm its performance in some cases. Third, MetaLR is designed for medical fine-tuning instead of general cases, what problem it may encounter in other scenarios remains unknown.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62101351</rs>) and the <rs type="funder">GuangDong Basic and Applied Basic Research Foundation</rs> (No.<rs type="grantNumber">2020A1515110376</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TXh8JmQ">
					<idno type="grant-number">62101351</idno>
				</org>
				<org type="funding" xml:id="_4fRrsqv">
					<idno type="grant-number">2020A1515110376</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-tuning u-net for ultrasound image segmentation: different layers, different outcomes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ultrason. Ferroelectr. Freq. Control</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2510" to="2518" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bidaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (LITS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating detection of lung pathologies with explainable ultrasound image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cossio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">672</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved fine-tuning of in-domain transformer model for inferring COVID-19 presence in multi-institutional radiology reports</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking two consensuses of the transferability in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00399</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating and weighting semantically consistent sample pairs for ultrasound contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">USCL: pretraining deep ultrasound image diagnosis model through video contrastive representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_60</idno>
		<idno>978-3-030-87237-3 60</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D U-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1568" to="1577" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spottune: transfer learning through adaptive fine-tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4805" to="4814" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large dataset of labeled optical coherence tomography (OCT) and chest x-ray images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mendeley Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="10" to="17632" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Meta-SGD: learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">On first-order meta-learning algorithms</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-tuning and training of densenet for histopathology image representation using TCGA diagnostic slides</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riasatian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">102032</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autolr: layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2486" to="2494" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: full training or fine tuning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer learning with adaptive fine-tuning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vrbančič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Podgorelec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="196197" to="196211" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive medical image segmentation using deep learning with image-specific fine tuning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1562" to="1573" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HiCo: hierarchical contrastive learning for ultrasound video model pretraining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-26351-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-26351-41" />
	</analytic>
	<monogr>
		<title level="m">ACCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Chin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13846</biblScope>
			<biblScope unit="page" from="229" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparing to learn: surpassing imagenet pretraining on radiographs by comparing image representations</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-839" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Models genesis: generic autodidactic models for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-942" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
