<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images</title>
				<funder ref="#_6hnpgvX">
					<orgName type="full">Guangdong Province Key Field Research and Development Plan Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianbao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zefeiyun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">Nanfang Hospital</orgName>
								<orgName type="institution" key="instit2">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yusi</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">Nanfang Hospital</orgName>
								<orgName type="institution" key="instit2">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Guangzhou SiDe MedTech Company Ltd</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Guangzhou SiDe MedTech Company Ltd</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijie</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">Nanfang Hospital</orgName>
								<orgName type="institution" key="instit2">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiyi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Guangzhou SiDe MedTech Company Ltd</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Side</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">Nanfang Hospital</orgName>
								<orgName type="institution" key="instit2">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Guangzhou SiDe MedTech Company Ltd</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<email>weiyanggm@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="518" to="527"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A5468456741D8109AC16C158A9C3E12B</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wireless capsule endoscopy</term>
					<term>Super-resolution</term>
					<term>Domain adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super-resolution (SR) of wireless capsule endoscopy (WCE) images is challenging because paired high-resolution (HR) images are not available. An intuitive solution is to simulate paired low-resolution (LR) WCE images from HR electronic endoscopy images for supervised learning. However, the SR model obtained by this method cannot be well adapted to real WCE images due to the large domain gap between electronic endoscopy images and WCE images. To address this issue, we propose a Multi-level Domain Adaptation SR model (MDA-SR) in an unsupervised manner using arbitrary set of WCE images and HR electronic endoscopy images. Our approach implements domain adaptation at the image level and latent level during the degradation and SR processes, respectively. To the best of our knowledge, this is the first work to explore an unsupervised SR approach for WCE images. Furthermore, we design an Endoscopy Image Quality Evaluator (EIQE) based on the reference-free image evaluation metric NIQE, which is more suitable for evaluating WCE image quality. Extensive experiments demonstrate that our MDA-SR method outperforms state-of-the-art SR methods both quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Wireless capsule endoscopy (WCE) is an emerging examination technique that offers several advantages over traditional electronic endoscopy, including noninvasiveness, safety, and non-cross-infection. It enables the examination of the T. Liu and Z. Chen-contributed equally to this work. entire human gastrointestinal tract and is widely used in clinical practice <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b23">23]</ref>. Despite its successful clinical applications, the limited volume and data transmission bandwidth of WCE result in image quality drawbacks such as low resolution and poor quality <ref type="bibr" target="#b4">[4]</ref>. Recent research has shown that high definition colonoscopy increases the identification of any polyps by 3.8% <ref type="bibr" target="#b19">[19]</ref>, and a 3-center prospective randomized trial has further proven the value of high resolution in invasive endoscopy <ref type="bibr" target="#b17">[17]</ref>. Hence it is desirable to restore the image quality of WCE images via super-resolution techniques.</p><p>Most super-resolution methods based on deep learning are supervised by paired low-resolution (LR) and high-resolution (HR) images <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26]</ref>. However, the corresponding HR WCE image is currently unavailable due to the hardware limitations in capsule size and transmission bandwidth. Alternatively, researchers have adopted predefined simple linear degradation assumptions (e.g., bicubic downsampling, gaussian downsampling) to feasibly synthesize corresponding LR samples from ground-truth HR images. Similarly, Almalioglu et al. <ref type="bibr" target="#b0">[1]</ref> adopted this assumption to synthesis paired LR electronic endoscopy images from the HR ones for supervised super-resolution learning. However, this method is difficult to generalize to real WCE images due to the domain gap between WCE images and electronic endoscopy images.</p><p>What causes this domain gap? It might seem reasonable to adopt the simple linear degradation assumption by simply analogizing the domain gap between a mobile camera and a professional camera to a WCE and an electronic endoscope. However, what cannot be ignored is the different examination environment, where the WCE requires filling the stomach with water, while the electronic endoscopy inflates the stomach, which directly leads to the difference between the two image domains in terms of villi pose and speckle reflection, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This domain gap cannot be described by a simple linear degradation matrix.</p><p>Recently, many studies have utilized the CycleGAN <ref type="bibr" target="#b28">[28]</ref> to combine explicit domain adaptation into SR. The basic idea is to generate LR versions of HR V Fig. <ref type="figure">2</ref>. Overview of the proposed MDA-SR, which consists of two parts: adaptive degradation and domian adaptation SR.</p><p>images with degenerate distributions similar to the real LR images, and then train the SR model on the generated LR-HR paired dataset <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref>. The challenge is that the large domain gap between WCE and electronic endoscopic images makes the generator sensitive to learning shallow differences in content or style and unable to effectively bridge the domain gap.</p><p>In this work, we propose a Multi-level Domain Adaptation Super-Resolution (MDA-SR) for WCE images to bridge the domain gap between electronic endoscopy images and WCE images. MDA-SR leverages prior knowledge of HR electronic endoscopy images to guide the SR process of WCE images, as illustrated in Fig. <ref type="figure">2</ref>. First, we train the adaptive degradation at the image level, employing a generative adversarial network to learn the complex and variable degradation distribution in WCE images, while incorporating an adaptive data loss <ref type="bibr" target="#b18">[18]</ref> as the fidelity term of the image content. In contrast to previous methods that assume generated LR images are free from domain shift <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b25">25]</ref>, we propose to minimize the domain gap during the SR process by aligning the latent feature distribution of electronic endoscopy images and WCE images. We further proposed EIQE for improving the reference-free image evaluation metric NIQE to be more suitable for endoscopy images. Through extensive experiments on real WCE images, we demonstrate the superiority of our method over other state-of-the-art SR methods, and its efficacy in reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of the Proposed Method</head><p>Given a set of WCE images and HR electronic endoscopy images I LR cap , I HR ele , we aim to learn a SR function R (•) that maps an observed I LR cap to its HR version according to the distribution defined by I HR ele in testing. As shown in Fig. <ref type="figure">2</ref>, the proposed scheme consists of two major parts: an adaptive degradation that generates the LR version of I HR ele , denoted as I LR gen , and a domain adaptation SR that aligns the latent features of the WCE and electronic endoscopy datasets during the SR process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Degradation</head><p>The purpose of the adaptive degradation is to obtain I LR gen with a degradation distribution similar to I LR cap . To achieve this, we employ the architecture of GAN <ref type="bibr" target="#b5">[5]</ref>, where the degradation generator G maps the generated I LR gen to I LR cap and the image-level discriminator F image learns to distinguish between generated samples I LR gen and realistic samples I LR cap with adversarial loss:</p><formula xml:id="formula_0">L adv (G, F image ) = F image G I HR ele 2 + 1 -F image I LR cap 2<label>(1)</label></formula><p>G is optimized by maximizing the loss in Eq. ( <ref type="formula" target="#formula_0">1</ref>) against an adversarial F image that tries to minimize the loss. A critical requirement is that the LR image generated by G should be consistent with the low-frequency information of the HR image. To enforce this constraint, we incorporate data loss as an additional supervision information.</p><p>The process of degradation from HR images to LR images is unknown. We adopt an adaptive downsampling kernel k <ref type="bibr" target="#b18">[18]</ref> to approximate the unknown degradation process, since a widely-used approach uses predefined downsampling assumptions, such as bicubic downsampling or s × s average pooling <ref type="bibr" target="#b2">[3]</ref>. It has been observed in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b12">12]</ref> that an appropriate downsampling function consists of low-filtering and decimation, we linearize the degradation generator G to a corresponding 2D kernel k:</p><formula xml:id="formula_1">k = arg min k N i=1 I HR ele,i * k ↓s -G I HR ele,i<label>2 2 (2)</label></formula><p>where I HR ele,i denotes an i-th example to estimate the kernel, N is the total number of samples that have been used and ↓ s represents downsampling operation with scale factor s. Finally, the data fidelity term L data is defined as follows:</p><formula xml:id="formula_2">L data (G) = I HR ele * k ↓s -G I HR ele 1<label>(3)</label></formula><p>Given the definitions of adversarial and data losses above, the training loss of our adaptive degradation is defined as:</p><formula xml:id="formula_3">L LR (G, F image ) = L adv (G, F image ) + λL data (G)<label>(4)</label></formula><p>where λ is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Adaptation SR</head><p>The SR function R (•) can then be supervised by the aligned image pair set I LR gen , I HR ele obtained from adaptive degradation. As shown in Fig. <ref type="figure">2</ref>, we use the pixel-wise content loss on the SR results R I LR gen , which ensures the accuracy of HR image composition:</p><formula xml:id="formula_4">L content = R I LR gen -I HR ele 1<label>(5)</label></formula><p>To further improve the performance of the WCE SR, it is crucial to bridge the domain gap between WCE images and electronic endoscopy images, even though the adaptive degradation generator G already learns the degradation distribution of WCE images through domain adaptation at the image level. To achieve this, we improve the domain adaptation at the latent level during the SR process.</p><p>A straightforward way is to adopt a GAN <ref type="bibr" target="#b5">[5]</ref> structure to reduce the distribution shift. As shown in Fig. <ref type="figure">2</ref>, the SR function R (•) consists of an encoder E and a decoder D. We use two encoders E with shared weights to generate the electronic endoscopy latent feature z ele as well as the WCE latent feature z cap . We introduce latent-level discriminator F latent to distinguish the domain for each latent feature, while the encoder E is trained to deceive F latent . The optimization of E and F latent is achieved via the adversarial way, we use LSGAN <ref type="bibr" target="#b11">[11]</ref> here:</p><formula xml:id="formula_5">L align (E) = F latent E I LR cap -0.5 2 + F latent E I LR gen -0.5 2<label>(6)</label></formula><formula xml:id="formula_6">L align (F latent ) = F latent E I LR cap -1 2 + F latent E I LR gen -0 2<label>(7)</label></formula><p>As a result, the discriminator F latent is trained with its corresponding loss in Eq. <ref type="bibr" target="#b7">(7)</ref>. The SR function R (•) is trained with the following loss function:</p><formula xml:id="formula_7">L SR = L content + μL align (E)<label>(8)</label></formula><p>where μ is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Settings</head><p>Datasets. Our proposed model is trained on WCE image dataset and electronic endoscopy image dataset, and tested on WCE images. The WCE image dataset contains 14090 images, and the electronic endoscopy image dataset contains 2033 images, including 1302 images from the public Kvasir dataset <ref type="bibr" target="#b16">[16]</ref> and 731 images from the local hospital. We perform a strict quality selection and remove the problematic images such as blurry, low-resolution and poor quality images. Evaluation Metrics. In WCE SR problem, there is no corresponding groundtruth image used to calculate the evaluation metric. To address this issue, we design a no-reference Endoscopy Image Quality Evaluator (EIQE), which derives the quality-aware features from the Endoscopy Scene Statistic (ESS) model, inspired by the reference-free Natural Image Quality Evaluator (NIQE) <ref type="bibr" target="#b14">[14]</ref>.</p><p>The quality of a given test image is then expressed as the distance between a multivariate gaussian (MVG) fit of the ESS features extracted from the test image and a MVG model of the quality-aware features extracted from the corpus of HR electronic endoscopy images. Additionally, we use the no-reference metric BRISQUE <ref type="bibr" target="#b13">[13]</ref> for evaluation purposes.</p><p>To better illustrate the subjective quality, we conduct a mean opinion score (MOS) test for comparison with other methods. We randomly select 100 different WCE images from the test set to subjectively evaluate the quality of the 2x and 4x WCE SR images. Four gastroenterology clinicians rate the visual perceptual qualities by assigning scores. Scores from 0 to 5 are used to indicate the qualities from low to high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>Throughout the framework, the discriminators F image and F latent use the patchbased discriminator <ref type="bibr" target="#b7">[7]</ref> with the instance normalization <ref type="bibr" target="#b22">[22]</ref>. The generator G, encoder E and decoder D in the model follow the residual block structure from the EDSR <ref type="bibr" target="#b8">[8]</ref>. The parameters in Eq. ( <ref type="formula" target="#formula_3">4</ref>) and Eq. ( <ref type="formula" target="#formula_7">8</ref>) are set to be λ = 1 and μ = 0.001, respectively. During training, we use the Adam optimizer and set the batch size and learning rate as 10 and 1 × 10 -4 , respectively. To streamline the model training and reduce its complexity, we have divided the training process into two stages. In the first stage, we focus on training the adaptive degradation, which stabilizes the quality of generated LR images after 50 epochs. Following this, we incorporate the domain adaptation SR into the training process by training another 50 epochs. The experiments are implemented with Pytorch platform on NVIDIA GeForce RTX 2080 Ti.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussions</head><p>Comparison with Previous Methods. To validate the effectiveness of our proposed method, we compare it with existing state-of-the-art conventional SR methods without domain adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">8]</ref> and SR methods with domain adaptation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref>. Quantitative results are shown in Table <ref type="table" target="#tab_0">1</ref>, while visualization results are provided in Fig. <ref type="figure" target="#fig_1">3</ref>. As EIQE is the most important metric in Table <ref type="table" target="#tab_0">1</ref> and BRISQUE <ref type="bibr" target="#b13">[13]</ref> is provided for reference since BRISQUE is based on natural scene statistic. As can be seen in Table <ref type="table" target="#tab_0">1</ref>, approaches that incorporated domain adaptation generally outperformed those that did not, thus highlighting the value of domain adaptation for the WCE SR problem. For the WCE image dataset, our MDA-SR method achieves the top EIQE performance.Note that in Fig. <ref type="figure" target="#fig_1">3</ref>, the proposed method produces results containing clean and natural textures, while the result of ADM <ref type="bibr" target="#b18">[18]</ref> are overly sharpened, producing unreal artifacts. The MOS results are shown in Fig. <ref type="figure" target="#fig_2">4</ref>, indicating that our MDA-SR model produced the highest scores on average and with relatively smaller variance.  Ablation Study. We use the t-SNE <ref type="bibr" target="#b10">[10]</ref> for the visual representation of the image latent features distribution during SR process. It can be observed from Fig. <ref type="figure" target="#fig_3">5</ref>(a) that there is a significant domain gap between WCE images and electronic endoscopy images, and from Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a multi-level domain adaptation SR for real WCE images. Our method first utilizes adaptive degradation to simulate the degradation distribution of WCE and generate LR electronic endoscopy images. We then employ implicit domain adaptation at the latent level during the SR process to further bridge the domain gap between WCE images and electronic endoscopy images. Through extensive experiments on real WCE images, we demonstrate the superiority of our method over other state-of-the-art SR methods, and its efficacy in reality. Further evaluation for downstream tasks such as disease classification, region segmentation, or depth and pose estimation from the generated SR WCE images is warranted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The visual presentation of WCE images and electronic endoscopy images. The WCE images on the left are dim and blurred, while the electronic endoscopy images on the right are bright and clear.</figDesc><graphic coords="2,56,97,54,47,338,68,112,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The visual comparisons for 2x and 4x SR on the WCE images. Obviously, our result contains more natural details and textures suffering from less blur and artifacts.</figDesc><graphic coords="6,57,48,53,78,337,36,92,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Mean opinion of the subjective evaluation for different SR methods.</figDesc><graphic coords="7,76,80,54,29,271,00,105,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual representation of electronic endoscopy latent feature z ele and WCE latent feature zcap via t-SNE. zcap in both (a) and (b) is obtained by encoding the I LR cap . z ele in (a) is obtained by encoding the bicubic downsampled electronic endoscopy image I LR ele , while the z ele in (b) is obtained by encoding the adaptively degraded electronic endoscopy image I LR gen</figDesc><graphic coords="8,101,46,54,50,249,85,116,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results of the proposed MDA-SR model and other state-of-theart methods on WCE image dataset, where DNSR, USR-DA and ADM incorporate domain adaptation but EDSR and EndoL2H do not. The best values are highlighted in bold font.</figDesc><table><row><cell>Method</cell><cell>2x</cell><cell></cell><cell>4x</cell><cell></cell></row><row><cell></cell><cell cols="4">EIQE↓ BRISQUE↓ EIQE↓ BRISQUE↓</cell></row><row><cell>Bicubic</cell><cell>5.65</cell><cell>58.03</cell><cell>7.36</cell><cell>76.75</cell></row><row><cell>EDSR [8]</cell><cell>6.06</cell><cell>59.06</cell><cell>6.81</cell><cell>75.89</cell></row><row><cell>EndoL2H [1]</cell><cell>6.63</cell><cell>57.42</cell><cell>6.66</cell><cell>69.92</cell></row><row><cell>DNSR [27]</cell><cell>5.50</cell><cell>58.16</cell><cell>6.71</cell><cell>74.72</cell></row><row><cell>USR-DA [24]</cell><cell>5.23</cell><cell>51.82</cell><cell>6.42</cell><cell>75.30</cell></row><row><cell>ADM [18]</cell><cell>5.19</cell><cell>52.62</cell><cell>6.21</cell><cell>59.49</cell></row><row><cell cols="2">MDA-SR(ours) 5.14</cell><cell>50.70</cell><cell>6.16</cell><cell>64.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on our proposed method. We report four different levels of domain adaptation for the MDA-SR affect the SR results on WCE images.</figDesc><table><row><cell cols="2">Image Level Latent Level EIQE↓</cell></row><row><cell>2x</cell><cell>4x</cell></row><row><cell cols="2">5.96 6.85</cell></row><row><cell cols="2">5.76 6.35</cell></row><row><cell cols="2">5.48 6.26</cell></row><row><cell cols="2">5.14 6.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>5(b)  show that our MDA-SR effectively bridges the domain gap. To better verify the effectiveness of our proposed model, we perform ablation experiments on WCE dataset. We obtain four different frameworks by removing different levels of domain adaptation structures. As shown in Table2, domain adaptation at both the image and latent levels is effective. The worst framework is to train SR model on electronic endoscopy dataset and test it on WCE dataset. Our proposed MDA-SR model has the best SR effect on WCE images.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research is supported by the <rs type="funder">Guangdong Province Key Field Research and Development Plan Project</rs> (<rs type="grantNumber">2022B0303020003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6hnpgvX">
					<idno type="grant-number">2022B0303020003</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EndoL2H: deep super-resolution for capsule endoscopy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Almalioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4297" to="4309" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind super-resolution kernel estimation using an internal-GAN</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell-Kligler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a GAN to learn how to do image degradation first</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11210</biblScope>
			<biblScope unit="page" from="187" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01231-1_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01231-1_12" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An ingenious application-specific quality assessment methods for compressed wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Fante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abdurahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Gemeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Environ. Electr. Eng</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning for real-world super-resolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3408" to="3416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blind/referenceless image spatial quality evaluator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Conference Record of the Forty Fifth Asilomar Conference on SignAls, Systems And Computers (ASILOMAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="723" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision-based personalized wireless capsule endoscopy for smart healthcare: taxonomy, literature review, opportunities and challenges</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Futur. Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="266" to="280" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">KVASIR: a multi-class image dataset for computer aided gastrointestinal disease detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM on Multimedia Systems Conference</title>
		<meeting>the 8th ACM on Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-definition colonoscopy versus Endocuff versus Endorings versus Full-spectrum Endoscopy for adenoma detection at colonoscopy: a multicenter randomized trial</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Rex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward real-world superresolution via adaptive downsampling models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8657" to="8670" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High definition colonoscopy vs. standard video endoscopy for the detection of colonic polyps: a meta-analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mannath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ragunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="499" to="505" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to zoom-in via learning to zoom-out: real-world super-resolution by generating and adapting degradation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2947" to="2962" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4799" to="4807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Instance normalization: the missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wireless capsule endoscopy</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="805" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised real-world superresolution: a domain adaptation perspective</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4318" to="4327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised degradation learning for single image super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04240</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
