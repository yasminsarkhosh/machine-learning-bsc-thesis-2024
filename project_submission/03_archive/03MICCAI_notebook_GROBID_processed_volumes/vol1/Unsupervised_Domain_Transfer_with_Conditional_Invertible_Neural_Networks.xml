<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Domain Transfer with Conditional Invertible Neural Networks</title>
				<funder ref="#_wRHqNhw">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Surgical Oncology Program of the National Center for Tumor Diseases</orgName>
					<orgName type="abbreviated">NCT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kris</forename><forename type="middle">K</forename><surname>Dreher</surname></persName>
							<email>k.dreher@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Physics and Astronomy</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonardo</forename><surname>Ayala</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Melanie</forename><surname>Schellenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Hübner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan-Hinrich</forename><surname>Nölke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><forename type="middle">J</forename><surname>Adler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silvia</forename><surname>Seidlitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">National Center for Tumor Diseases (NCT) Heidelberg a Partnership Between DKFZ</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Sellner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Studier-Fischer</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of General, Visceral, and Transplantation Surgery</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Janek</forename><surname>Gröhl</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Cancer Research</orgName>
								<orgName type="institution">Cambridge Institute</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Nickel</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of General, Visceral, and Transplantation Surgery</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ullrich</forename><surname>Köthe</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Seitel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">National Center for Tumor Diseases (NCT) Heidelberg a Partnership Between DKFZ</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lena</forename><surname>Maier-Hein</surname></persName>
							<email>l.maier-hein@dkfz-heidelberg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Intelligent Medical Systems</orgName>
								<orgName type="department" key="dep2">German Cancer Research Center (DKFZ)</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">National Center for Tumor Diseases (NCT) Heidelberg a Partnership Between DKFZ</orgName>
								<orgName type="institution">Heidelberg University Hospital</orgName>
								<address>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Domain Transfer with Conditional Invertible Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D96455396E34B4718EBF74C2232E853</idno>
					<idno type="DOI">10.1007/978-3-031-43907-073.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthetic medical image generation has evolved as a key technique for neural network training and validation. A core challenge, however, remains in the domain gap between simulations and real data. While deep learning-based domain transfer using Cycle Generative Adversarial Networks and similar architectures has led to substantial progress in the field, there are use cases in which state-of-the-art approaches still fail to generate training images that produce convincing results on relevant downstream tasks. Here, we address this issue with a domain transfer approach based on conditional invertible neural networks (cINNs). As a particular advantage, our method inherently guarantees cycle consistency through its invertible architecture, and network training can efficiently be conducted with maximum likelihood training. To showcase our method's generic applicability, we apply it to two spectral imaging modalities at different scales, namely hyperspectral imaging (pixel-level) and photoacoustic tomography (image-level</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of supervised learning methods in the medical domain led to countless breakthroughs that might be translated into clinical routine and have the potential to revolutionize healthcare <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. For many applications, however, labeled reference data (ground truth) may not be available for training and validating a neural network in a supervised manner. One such application is spectral imaging which comprises various non-interventional, non-ionizing imaging techniques that can resolve functional tissue properties such as blood oxygenation in real time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23]</ref>. While simulations have the potential to overcome the lack of ground truth, synthetic data is not yet sufficiently realistic <ref type="bibr" target="#b8">[9]</ref>. Cycle Generative Adversarial Networks (GAN)-based architectures are widely used for domain transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> but may suffer from issues such as unstable training, hallucinations, or mode collapse <ref type="bibr" target="#b14">[15]</ref>. Furthermore, they have predominantly been used for conventional RGB imaging and one-channel cross-modality domain adaptation, and may not be suitable for other imaging modalities with more channels. We address these challenges with the following contributions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Transfer Method:</head><p>We present an entirely new sim-to-real transfer approach based on conditional invertible neural networks (cINNs) (cf. Fig. <ref type="figure" target="#fig_0">1</ref>) specifically designed for data with many spectral channels. This approach inherently addresses weaknesses of the state of the art with respect to the preservation of spectral consistency and, importantly, does not require paired images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instantiation to Spectral Imaging:</head><p>We show that our method can generically be applied to two complementary modalities: photoacoustic tomography (PAT; image-level) and hyperspectral imaging (HSI; pixel-level).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comprehensive Validation:</head><p>In comprehensive validation studies based on more than 2,000 PAT images (real: ∼1,000) and more than 6 million spectra for HSI (real: ∼6 million) we investigate and subsequently confirm our two main hypotheses: (H1) Our cINN-based models can close the domain gap between simulated and real spectral data better than current state-of-the-art methods regarding spectral plausibility. (H2) Training models on data transferred by our cINN-based approach can improve their performance on the corresponding (clinical) downstream task without them having seen labeled real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Transfer with Conditional Invertible Neural Networks</head><p>Concept Overview. Our domain transfer approach (cf. Fig. <ref type="figure" target="#fig_1">2</ref>) is based on the assumption that data samples from both domains carry domain-invariant information (e.g. on optical tissue properties) and domain-variant information (e.g. modality-specific artifacts). The invertible architecture, which inherently guarantees cycle consistency, transfers both simulated and real data into a shared latent space. While the domain-invariant features are captured in the latent space, the domain-variant features can either be filtered (during encoding) or added (during decoding) by utilizing a domain label D. To achieve spectral consistency, we leverage the fact that different tissue types feature characteristic spectral signatures and condition the model on the tissue label Y if available. For unlabeled (real) data, we use randomly generated proxy labels instead. To achieve high visual quality beyond spectral consistency, we include two discriminators Dis sim and Dis real for their respective domains. Finally, as a key theoretical advantage, we avoid mode collapse with maximum likelihood optimization. Implementation details are provided in the following. cINN Model Design. The core of our architecture is a cINN <ref type="bibr" target="#b1">[2]</ref> (cf. Fig. <ref type="figure" target="#fig_1">2</ref>), comprising multiple (i) scales of N i -chained affine conditional coupling (CC) blocks <ref type="bibr" target="#b6">[7]</ref>. These scales are necessary in order to increase the receptive field of the network and are achieved by Haar wavelet downsampling <ref type="bibr" target="#b10">[11]</ref>. A CC block consists of subnetworks that can be freely chosen depending on the data dimensionality (e.g. fully connected or convolutional networks) as they are only evaluated in the forward direction. The CC blocks receive a condition consisting of two parts: domain label and tissue label, which are then concatenated to the input along the channel dimension. In the case of PAT, the tissue label is a full semantic and random segmentation map for the simulated and real data, respectively. In the case of HSI, the tissue label is a one-hot encoded vector for organ labels.</p><p>Model Training. In the following, the proposed cINN with its parameters θ will be referred to as f (x, DY, θ) and its inverse as f -1 for any input x ∼ p D from domain D ∈ {D sim , D real } with prior density p D and its corresponding latent space variable z. The condition DY is the combination of domain label D as well as the tissue label Y ∈ {Y sim , Y real }. Then the maximum likelihood loss ML for a training sample x i is described by</p><formula xml:id="formula_0">ML D = E i ||f (x i , DY, θ)|| 2 2 2 -log|J i | with J i = det ∂f ∂x xi . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For the adversarial training, we employ the least squares training scheme <ref type="bibr" target="#b17">[18]</ref> for generator Gen D = f -1 D • f D and discriminator Dis D for each domain with x D as input from the source domain and x D as input from the target domain:</p><formula xml:id="formula_2">L GenD = E x D ∼p D (Dis D (Gen D (x D ) -1)) 2<label>(2)</label></formula><formula xml:id="formula_3">L DisD = E xD∼pD (Dis D (x D ) -1) 2 + E x D ∼p D (Dis D (Gen D (x D ))) 2 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Finally, the full loss for the proposed model comprises the following:</p><formula xml:id="formula_5">L T otalGen = ML real + ML sim + L Gen real + L Gensim and L T otalDis = L Dis real + L Dissim . (4)</formula><p>Model Inference. The domain transfer is done in two steps: 1) A simulated image is encoded in the latent space with conditions D sim and Y sim to its latent representation z, 2) z is decoded to the real domain via D real with the simulated tissue label Y sim :</p><formula xml:id="formula_6">x sim→real = f -1 (•, D real Y sim , θ) • f (•, D sim Y sim , θ)(x sim ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral Imaging Data</head><p>Photoacoustic Tomography Data. PAT is a non-ionizing imaging modality that enables the imaging of functional tissue properties such as tissue oxygenation <ref type="bibr" target="#b21">[22]</ref>. The real PAT data (cf. Fig. <ref type="figure" target="#fig_2">3</ref>) used in this work are images of human forearms that were recorded from 30 healthy volunteers using the MSOT Acuity Echo (iThera Medical GmbH, Munich, Germany) (all regulations followed under study ID: S-451/2020, and the study is registered with the German Clinical Trials Register under reference number DRKS00023205). In this study, 16 wavelengths from 700 nm to 850 nm in steps of 10 nm were recorded for each image. The resulting 180 images were semantically segmented into the structures shown in Fig. <ref type="figure" target="#fig_2">3</ref> according to the annotation protocol provided in <ref type="bibr" target="#b19">[20]</ref>. Additionally, a full sweep of each forearm was performed to generate more unlabeled images, thus amounting to a total of 955 real images. The simulated PAT data (cf. Fig. <ref type="figure" target="#fig_2">3</ref>) used in this work comprises 1,572 simulated images of human forearms. They were generated with the toolkit for Simulation and Image Processing for Photonics and Acoustics (SIMPA) <ref type="bibr" target="#b7">[8]</ref> based on a forearm literature model <ref type="bibr" target="#b20">[21]</ref> and with a digital device twin of the MSOT Acuity Echo.</p><p>Hyperspectral Imaging Data. HSI is an emerging modality with high potential for surgery <ref type="bibr" target="#b3">[4]</ref>. In this work, we performed pixel-wise analysis of HSI images. The real HSI data was acquired with the Tivita R Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany) camera, featuring a spectral resolution of approximately 5 nm in the spectral range between 500 nm and 1000 nm nm. In total, 458 images, corresponding to 20 different pigs, were acquired (all regulations followed under study IDs: 35-9185.81/G-161/18 and 35-9185.81/G-262/19) and annotated with ten structures: bladder, colon, fat, liver, omentum, peritoneum, skin, small bowel, spleen, and stomach (cf. Fig. <ref type="figure" target="#fig_2">3</ref>). This amounts to 6,410,983 real spectra in total. The simulated HSI data was generated with a Monte Carlo method (cf. algorithm provided in the supplementary material). This procedure resulted in 213,541 simulated spectra with annotated organ labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>The purpose of the experiments was to investigate hypotheses H1 and H2 (cf. Sect. 1). As comparison methods, a CycleGAN <ref type="bibr" target="#b23">[24]</ref> and an unsupervised imageto-image translation (UNIT) network <ref type="bibr" target="#b15">[16]</ref> were implemented fully convolutionally for PAT and in an adapted version for the one-dimensional HSI data. To make the comparison fair, the tissue label conditions were concatenated with the input, and we put significant effort into optimizing the UNIT on our data.</p><p>Realism of Synthetic Data (H1) : According to qualitative analyses (Fig. <ref type="figure" target="#fig_3">4</ref>) our domain transfer approach improves simulated PAT images with respect to key properties, including the realism of skin, background, and sharpness of vessels.  The PCA plots in a) represent a kernel density estimation of the first and second components of a PCA embedding of the real data, which represent about 67% and 6% of the variance in the real data, respectively. The distributions on top and on the right of the PCA plot correspond to the marginal distributions of each dataset's first two components. b) Violin plots show that the cINN yields spectra that feature a smaller difference to the real data compared to the simulations and the UNIT-generated data.</p><p>The dashed lines represent the mean difference value, and each dot represents the difference for one wavelength.</p><p>A principal component analysis (PCA) performed on all artery and vein spectra of the real and synthetic datasets demonstrates that the distribution of the synthetic data is much closer to the real data after applying our domain transfer approach (cf. Fig. <ref type="figure" target="#fig_4">5a</ref>)). The same holds for the absolute difference, as shown in Fig. <ref type="figure" target="#fig_4">5b</ref>). Slightly better performance was achieved with the cINN compared to the UNIT. Similarly, our approach improves the realism of HSI spectra, as illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>, for spectra of five exemplary organs (colon, stomach, omentum, spleen, and fat). The cINN-transferred spectra generally match the real data very closely. Failure cases where the real data has a high variance (translucent band) are also shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefit of Domain-Transferred Data for Downstream Tasks (H2):</head><p>We examined two classification tasks for which reference data generation was feasible: classification of veins/arteries in PAT and organ classification in HSI. For both modalities, we used the completely untouched real test sets, comprising 162 images in the case of PAT and ∼ 920,000 spectra in the case of HSI. For both tasks, a calibrated random forest classifier (sklearn <ref type="bibr" target="#b18">[19]</ref> with default parameters) was trained on the simulated, the domain-transferred (by UNIT and cINN), and real spectra. As metrics, the balanced accuracy (BA), area under receiver operating characteristic (AUROC) curve, and F1-score were selected based on <ref type="bibr" target="#b16">[17]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, our domain transfer approach dramatically increases the classification performance for both downstream tasks. Compared to physicsbased simulation, the cINN obtained a relative improvement of 37% (BA), 25% (AUROC), and 22% (F1 Score) for PAT whereas the UNIT only achieved a  relative improvement in the range of 20%-27% (depending on the metric). For HSI, the cINN achieved a relative improvement of 21% (BA), 1% (AUROC), and 33% (F1 Score) and it scored better in all metrics except for the F1 Score than the UNIT. For all metrics, training on real data still yields better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>With this paper, we presented the first domain transfer approach that combines the benefits of cINNs (exact maximum likelihood estimation) with those of GANs (high image quality). A comprehensive validation involving qualitative and quantitative measures for the remaining domain gap and downstream tasks suggests that the approach is well-suited for sim-to-real transfer in spectral imaging. For both PAT and HSI, the domain gap between simulations and real data could be substantially reduced, and a dramatic increase in downstream task performance was obtained -also when compared to the popular UNIT approach. The only similar work on domain transfer in PAT has used a cycle GANbased architecture on a single wavelength with only photon propagation as PAT image simulator instead of full acoustic wave simulation and image reconstruction <ref type="bibr" target="#b13">[14]</ref>. This potentially leads to spectral inconsistency in the sense that the spectral information either is lost during translation or remains unchanged from the source domain instead of adapting to the target domain. Outside the spectral/medical imaging community, Liu et al. <ref type="bibr" target="#b15">[16]</ref> and Grover et al. <ref type="bibr" target="#b9">[10]</ref> tasked variational autoencoders and invertible neural networks for each domain, respectively, to create the shared encoding. They both combined this approach with adversarial training to achieve high-quality image generation. Das et al. <ref type="bibr" target="#b4">[5]</ref> built upon this approach by using labels from the source domain to condition the domain transfer task. In contrast to previous work, which used en-/decoders for each domain, we train a single network as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. with a two-fold condition consisting of a domain label (D) and a tissue label (Y ) from the source domain, which has the advantage of explicitly aiding the spectral domain transfer.</p><p>The main limitation of our approach is the high dimensionality of the parameter space of the cINN as dimensionality reduction of data is not possible due to the information and volume-preserving property of INNs. This implies that the method is not suitable for arbitrarily high dimensions. Future work will comprise the rigorous validation of our method with tissue-mimicking phantoms for which reference data are available.</p><p>In conclusion, our proposed approach of cINN-based domain transfer enables the generation of realistic spectral data. As it is not limited to spectral data, it could develop into a powerful method for domain transfer in the absence of labeled real data for a wide range of image modalities in the medical domain and beyond.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Pipeline for data-driven spectral image analysis in the absence of labeled reference data. A physics-based simulation framework generates simulated spectral images with corresponding reference labels (e.g. tissue type or oxygenation (sO2)). Our domain transfer method based on cINNs leverages unlabeled real data to increase their realism. The domain-transferred data can then be used for supervised training of a downstream task (e.g. classification).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Proposed architecture based on cINNs. The invertible architecture transfers both simulated and real data into a shared latent space (right). By conditioning on the domain D (bottom), a latent vector can be transferred to either the simulated or the real domain (left) for which the discriminator Dissim and Dis real calculate the losses for adversarial training.</figDesc><graphic coords="4,56,70,63,47,339,46,101,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training data used for the validation experiments.For PAT, 960 real images from 30 volunteers were acquired. For HSI, more than six million spectra corresponding to 460 images and 20 individuals were used. The tissue labels PAT correspond to 2D semantic segmentations, whereas the tissue labels for HSI represent 10 different organs. For PAT, ∼1600 images were simulated, whereas around 210,000 spectra were simulated for HSI.</figDesc><graphic coords="5,64,80,350,93,294,28,171,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative results. In comparison to simulated PAT images (left), images generated by the cINN (middle) resemble real PAT images (right) more closely. All images show a human forearm at 800 nm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Our domain transfer approach yields realistic spectra (here: of veins).The PCA plots in a) represent a kernel density estimation of the first and second components of a PCA embedding of the real data, which represent about 67% and 6% of the variance in the real data, respectively. The distributions on top and on the right of the PCA plot correspond to the marginal distributions of each dataset's first two components. b) Violin plots show that the cINN yields spectra that feature a smaller difference to the real data compared to the simulations and the UNIT-generated data. The dashed lines represent the mean difference value, and each dot represents the difference for one wavelength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The cINN-transferred spectra are in closer agreement with the real spectra than the simulations and the UNIT-transferred spectra. Spectra for five exemplary organs are shown from 500 nm to 1000 nm. For each subplot, a zoom-in for the near-infrared region (&gt;900 nm) is shown. The translucent bands represent the standard deviation across spectra for each organ.</figDesc><graphic coords="8,69,81,54,38,322,30,125,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Classification scores for different training data.</head><label>1</label><figDesc>The training data refers to real data, physics-based simulated data, data generated by a CycleGAN, by a UNIT without and with tissue labels (UNITY), and by a cINN without (cINND) and with (proposed cINNDY) tissue labels as condition. Additionally, cINN DY without GAN refers to a cINNDY without the adversarial training. The best-performing methods, except if trained on real data, are printed in bold.</figDesc><table><row><cell cols="2">Classifier training data PAT</cell><cell></cell><cell>HSI</cell><cell></cell></row><row><cell></cell><cell cols="4">BA AUROC F1-Score BA AUROC F1-Score</cell></row><row><cell>Real</cell><cell>0.75 0.84</cell><cell>0.82</cell><cell>0.40 0.81</cell><cell>0.44</cell></row><row><cell>Simulated</cell><cell>0.52 0.64</cell><cell>0.64</cell><cell>0.24 0.75</cell><cell>0.18</cell></row><row><cell>CycleGAN</cell><cell>0.39 0.20</cell><cell>0.16</cell><cell>0.11 0.57</cell><cell>0.06</cell></row><row><cell>UNIT</cell><cell>0.50 0.44</cell><cell>0.65</cell><cell>0.20 0.72</cell><cell>0.20</cell></row><row><cell>UNIT Y</cell><cell>0.64 0.81</cell><cell>0.77</cell><cell>0.24 0.74</cell><cell>0.25</cell></row><row><cell>cINN D</cell><cell>0.66 0.73</cell><cell>0.72</cell><cell>0.25 0.72</cell><cell>0.20</cell></row><row><cell cols="2">cINN DY without GAN 0.65 0.78</cell><cell>0.76</cell><cell>0.28 0.75</cell><cell>0.26</cell></row><row><cell cols="2">cINN DY (proposed) 0.71 0.80</cell><cell>0.78</cell><cell>0.29 0.76</cell><cell>0.24</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This project was supported by the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme (NEURAL SPICING</rs>, <rs type="grantNumber">101002198</rs>) and the <rs type="funder">Surgical Oncology Program of the National Center for Tumor Diseases (NCT) Heidelberg</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wRHqNhw">
					<idno type="grant-number">101002198</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme (NEURAL SPICING</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uncertainty-aware performance assessment of optical imaging modalities with invertible neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Adler</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-01939-9</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01939-9" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="997" to="1007" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lüth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<title level="m">Conditional invertible neural networks for guided image generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral imaging enables contrast agent-free real-time ischemia monitoring in laparoscopic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ayala</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.add6778</idno>
		<ptr target="https://doi.org/10.1126/sciadv.add6778" />
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surgical spectral imaging</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101699</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cdcgen: cross-domain conditional generation via normalizing flows and adversarial training</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Spanos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11368</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simpa: an open-source toolkit for simulation and image processing for photonics and acoustics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gröhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">83010</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for biomedical photoacoustic imaging: a review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gröhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schellenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dreher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photoacoustics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100241</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alignflow: cycle consistent learning from multiple domains via normalizing flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4028" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zur theorie der orthogonalen funktionensysteme</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="53" />
			<date type="published" when="1911">1911</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cycada: cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Tz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">nnu-net a selfconfiguring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning-based quantitative optoacoustic tomography of deep tissues in the absence of labeled experimental data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optica</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for fewshot learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13470" to="13479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Metrics reloaded: pitfalls and recommendations for image analysis validation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Godau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Tizabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Büttner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christodoulou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2206.01653</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2206.01653" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic segmentation of multispectral photoacoustic images using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schellenberg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pacs.2022.100341</idno>
		<ptr target="https://doi.org/10.1016/j.pacs.2022.100341" />
	</analytic>
	<monogr>
		<title level="j">Photoacoustics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">100341</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Photoacoustic image synthesis with generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schellenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photoacoustics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">100402</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noninvasive imaging of hemoglobin concentration and oxygenation in the rat brain using high-resolution photoacoustic tomography</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24015</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Physiological parameter estimation from multispectral images unleashed</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wirkert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-716" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
