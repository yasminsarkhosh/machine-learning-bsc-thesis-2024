<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs</title>
				<funder ref="#_gHpPGPh">
					<orgName type="full">Department of Engineering &quot;Enzo Ferrari&quot; of the University of Modena</orgName>
				</funder>
				<funder>
					<orgName type="full">DECIDER</orgName>
				</funder>
				<funder ref="#_8U53A4e">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_CB8QENs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gianpaolo</forename><surname>Bontempo</surname></persName>
							<email>gianpaolo.bontempo@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
							<email>angelo.porrello@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Bolelli</surname></persName>
							<email>federico.bolelli@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
							<email>simone.calderara@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Ficarra</surname></persName>
							<email>elisa.ficarra@unimore.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
								<address>
									<settlement>Modena</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="248" to="258"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">89A9DD34A02E9E150DD83B1339F97F94</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole-slide Images</term>
					<term>Multi-instance Learning</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The adoption of Multi-Instance Learning (MIL) for classifying Whole-Slide Images (WSIs) has increased in recent years. Indeed, pixel-level annotation of gigapixel WSI is mostly unfeasible and timeconsuming in practice. For this reason, MIL approaches have been profitably integrated with the most recent deep-learning solutions for WSI classification to support clinical practice and diagnosis. Nevertheless, the majority of such approaches overlook the multi-scale nature of the WSIs; the few existing hierarchical MIL proposals simply flatten the multiscale representations by concatenation or summation of features vectors, neglecting the spatial structure of the WSI. Our work aims to unleash the full potential of pyramidal structured WSI; to do so, we propose a graphbased multi-scale MIL approach, termed DAS-MIL, that exploits message passing to let information flows across multiple scales. By means of a knowledge distillation schema, the alignment between the latent space representation at different resolutions is encouraged while preserving the diversity in the informative content. The effectiveness of the proposed framework is demonstrated on two well-known datasets, where we outperform SOTA on WSI classification, gaining a +1.9% AUC and +3.3% accuracy on the popular Camelyon16 benchmark. The source code is available at https://github.com/aimagelab/mil4wsi.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern microscopes allow the digitalization of conventional glass slides into gigapixel Whole-Slide Images (WSIs) <ref type="bibr" target="#b17">[18]</ref>, facilitating their preservation and Fig. <ref type="figure">1</ref>. Overview of our proposed framework, DAS-MIL. The features extracted at different scales are connected (8-connectivity) by means of different graphs. The nodes of both graphs are later fused into a third one, respecting the rule "part of". The contextualized features are then passed to distinct attention-based MIL modules that extract bag labels. Furthermore, a knowledge distillation mechanism encourages the agreement between the predictions delivered by different scales.</p><p>retrieval, but also introducing multiple challenges. On the one hand, annotating WSIs requires strong medical expertise, is expensive, time-consuming, and labels are usually provided at the slide or patient level. On the other hand, feeding modern neural networks with the entire gigapixel image is not a feasible approach, forcing to crop data into small patches and use them for training. This process is usually performed considering a single resolution/scale among those provided by the WSI image.</p><p>Recently, Multi-Instance Learning (MIL) emerged to cope with these limitations. MIL approaches consider the image slide as a bag composed of many patches, called instances; afterwards, to provide a classification score for the entire bag, they weigh the instances through attention mechanisms and aggregate them into a single representation. It is noted that these approaches are intrinsically flat and disregard the pyramidal information provided by the WSI <ref type="bibr" target="#b14">[15]</ref>, which have been proven to be more effective than single-resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. However, to the best of our knowledge, none of the existing proposals leverage the full potential of the WSI pyramidal structure. Indeed, the flat concatenation of features <ref type="bibr" target="#b18">[19]</ref> extracted at different resolutions does not consider the substantial difference in the informative content they provide. A proficient learning approach should instead consider the heterogeneity between global structures and local cellular regions, thus allowing the information to flow effectively across the image scales.</p><p>To profit from the multi-resolution structure of WSI, we propose a pyramidal Graph Neural Network (GNN) framework combined with (self) Knowledge Distillation (KD), called DAS-MIL (Distilling Across Scales). A visual representation of the proposed approach is depicted in Fig. <ref type="figure">1</ref>. Distinct GNNs provide contextualized features, which are fed to distinct attention-based MIL modules that compute bag-level predictions. Through knowledge distillation, we encour-age agreement across the predictions delivered at different resolutions, while individual scale features are learned in isolation to preserve the diversity in terms of information content. By transferring knowledge across scales, we observe that the classifier self-improves as information flows during training. Our proposal has proven its effectiveness on two well-known histological datasets, Camelyon16 and TCGA lung cancer, obtaining state-of-the-art results on WSI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>MIL Approaches for WSI Classification. We herein summarize the most recent approaches; we refer the reader to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref> for a comprehensive overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Scale.</head><p>A classical approach is represented by AB-MIL <ref type="bibr" target="#b15">[16]</ref>, which employs a side-branch network to calculate the attention scores. In <ref type="bibr" target="#b27">[28]</ref>, a similar attention mechanism is employed to support a double-tier feature distillation approach, which distills features from pseudo-bags to the original slide. Differently, DS-MIL <ref type="bibr" target="#b18">[19]</ref> applies non-local attention aggregation by considering the distance with the most relevant patch. The authors of <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b24">[25]</ref> propose variations of AB-MIL, which introduce clustering losses and transformers, respectively. In addition, SETMIL <ref type="bibr" target="#b30">[31]</ref> makes use of spatial-encoding transformer layers to update the representation. The authors of <ref type="bibr" target="#b6">[7]</ref> leverage DINO <ref type="bibr" target="#b4">[5]</ref> as feature extractor, highlighting its effectiveness for medical image analysis. Beyond classical attention mechanisms, there are also algorithms based on Recurrent Neural Networks (RNN) <ref type="bibr" target="#b3">[4]</ref>, and Graphs Neural Networks (GNN) <ref type="bibr" target="#b31">[32]</ref>.</p><p>Multi-Scale. Recently, different authors focused on multi-resolution approaches. DSMIL-LC <ref type="bibr" target="#b18">[19]</ref> merges representations from different resolutions, i.e., low instance representations are concatenated with the ones obtained at a higher resolution. MS-RNNMIL <ref type="bibr" target="#b3">[4]</ref>, instead, fed an RNN with instances extracted at different scales. In <ref type="bibr" target="#b5">[6]</ref>, a self-supervised hierarchical transformer is applied at each scale. In MS-DA-MIL <ref type="bibr" target="#b12">[13]</ref>, multi-scale features are included in the same attention algorithm. <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b14">[15]</ref> exploit multi-resolution through GNN architectures.</p><p>Knowledge Distillation. Distilling knowledge from a more extensive network (teacher ) to a smaller one (student) has been widely investigated in recent years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> and applied to different fields, ranging from model compression <ref type="bibr" target="#b2">[3]</ref> to WSI analysis <ref type="bibr" target="#b16">[17]</ref>. Typically, a tailored learning objective encourages the student to mimic the behaviour of its teacher. Recently, self-supervised representation learning approaches have also employed such a schema: as an example, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> exploit KD to obtain an agreement between networks fed with different views of the same image. In <ref type="bibr" target="#b27">[28]</ref>, KD is used to transfer the knowledge between MIL tiers applied on different subsamples bags. Taking inspiration from <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b29">[30]</ref>, our model applies (self) knowledge distillation between WSI scale resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach aims to promote the information flow through the different employed resolutions. While existing works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> take into account interscales interactions by mostly leveraging trivial operations (such as concatenation of related feature representations), we instead provide a novel technique that builds upon: i) a GNN module based on message passing, which propagates patches' representation according to the natural structure of multi-resolutions WSI; ii) a regulation term based on (self) knowledge distillation, which pins the most effective resolution to further guide the training of the other one(s). In the following, we are delving into the details of our architecture.</p><p>Feature Extraction. Our work exploits DINO, the self-supervised learning approach proposed in <ref type="bibr" target="#b4">[5]</ref>, to provide a relevant representation of each patch. Differently from other proposals <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, it focuses solely on aligning positive pairs during optimization (and hence avoids negative pairs), which has shown to require a lower memory footprint during training. We hence devise an initial stage with multiple self-supervised feature extractors f (•; θ 1 ), . . . , f M (•; θ M ), one dedicated to each resolution: this way, we expect to promote feature diversity across scales. After training, we freeze the weights of these networks and use them as patch-level feature extractors. Although we focus only on two resolutions at time (i.e., M = 2) the approach can be extended to more scales.</p><p>Architecture. The representations yield by DINO provide a detailed description of the local patterns in each patch; however, they retain poor knowledge of the surrounding context. To grasp a global guess about the entire slide, we allow patches to exchange local information. We achieve it through a Pyramidal Graph Neural Network (PGNN) in which each node represents an individual WSI patch seen at different scales. Each node is connected to its neighbors (8-connectivity) in the euclidean space and between scales following the relation "part of"<ref type="foot" target="#foot_0">1</ref> . To perform message passing, we adopt Graph ATtention layers (GAT) <ref type="bibr" target="#b26">[27]</ref>.</p><p>In general terms, such a module takes as input multi-scale patch-level representations X = [X 1 X 2 ], where X 1 ∈ R N1×F and X 2 ∈ R N2×F are respectively the representations of the lower and higher scale. The input undergoes two graph layers: while the former treats the two scales as independent subgraphs A 1 ∈ R N1×N1 and A 2 ∈ R N2×N2 , the latter process them jointly by considering the entire graph A (see Fig. <ref type="figure">1</ref>, left). In formal terms:</p><formula xml:id="formula_0">H = PGNN(X ; A, A 1 , A 2 , θ PGNN ) = GAT([GAT(X 1 ; A 1 , θ 1 ) GAT(X 2 ; A 2 , θ 2 )]; A, θ 3 ),</formula><p>where H ≡ [H 1 H 2 ] stands for the output of the PGNN obtained by concatenating the two scales. These new contextualized patch representations are then fed to the attention-based MIL module proposed in <ref type="bibr" target="#b18">[19]</ref>, which produces bag-level scores y BAG 1 , y BAG 2 ∈ R 1×C where C equals the number of classes. Notably, such a module provides additional importance scores z 1 ∈ R N1 and z 2 ∈ R N2 , which quantifies the importance of each original patch to the overall prediction.</p><p>Aligning Scales with (Self ) Knowledge Distillation. We have hence obtained two distinct sets of predictions for the two resolutions: namely, a bag-level score (e.g., a tumor is either present or not) and a patch-level one (e.g., which instances contribute the most to the target class). However, as these learned metrics are inferred from different WSI zooms, a disagreement may emerge: indeed, we have observed (see Table <ref type="table" target="#tab_2">4</ref>) that the higher resolutions generally yield better classification performance. In this work, we exploit such a disparity to introduce two additional optimization objectives, which pin the predictions out of the higher scale as teaching signal for the lower one. Further than improving the results of the lowest scale only, we expect its benefits to propagate also to the shared message-passing module, and so to the higher resolution.</p><p>Formally, the first term seeks to align bag predictions from the two scales through (self) knowledge distillation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>:</p><formula xml:id="formula_1">L KD = τ 2 KL(softmax( y BAG 1 τ ) softmax( y BAG 2 τ )),<label>(1)</label></formula><p>where KL stands for the Kullback-Leibler divergence and τ is a temperature that lets secondary information emerge from the teaching signal.</p><p>The second aligning term regards the instance scores. It encourages the two resolutions to assign criticality scores in a consistent manner: intuitively, if a lowresolution patch has been considered critical, then the average score attributed to its children patches should be likewise high. We encourage such a constraint by minimizing the Euclidean distance between the low-resolution criticality grid map z 1 and its subsampled counterpart computed by the high-resolution branch:</p><formula xml:id="formula_2">L CRIT = z 1 -GraphPooling(z 2 ) 2 2 .</formula><p>(2)</p><p>In the equation above, GraphPooling identifies a pooling layer applied over the higher scale: to do so, it considers the relation "part of" between scales and then averages the child nodes, hence allowing the comparison at the instance level.</p><p>Overall Objective. To sum up, the overall optimization problem is formulated as a mixture of two objectives: the one requiring higher conditional likelihood w.r.t. ground truth labels y and carried out through the Cross-Entropy loss L CE (•; y); the other one based on knowledge distillation:</p><formula xml:id="formula_3">min θ (1 -λ)L CE (y BAG 2 ) + L CE (y BAG 1 ) + λL KD + βL CRIT , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where λ is a hyperparameter weighting the tradeoff between the teaching signals provided by labels and the higher resolution, while β balances the contributions of the consistency regularization introduced in Eq. ( <ref type="formula">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>WSIs Pre-processing. We remove background patches through an approach similar to the one presented in the CLAM framework <ref type="bibr" target="#b19">[20]</ref>: after an initial segmentation process based on Otsu <ref type="bibr" target="#b21">[22]</ref> and Connected Component Analysis <ref type="bibr" target="#b1">[2]</ref>, non-overlapped patches within the foreground regions are considered.</p><p>Optimization. We use Adam as optimizer, with a learning rate of 2 × 10 -4 and a cosine annealing scheduler (10 -5 decay w/o warm restart). We set τ = 1.5, β = 1, and λ = 1. The DINO feature extractor has been trained with two RTX5000 GPUs: differently, all subsequent experiments have been performed with a single RTX2080 GPU using Pytorch-Geometric <ref type="bibr" target="#b11">[12]</ref>. To asses the performance of our approach, we adhere to the protocol of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> and use the accuracy and AUC metrics. Moreover, the classifier on the higher scale has been used to make the final overall prediction. Regarding the KD loss, we apply the temperature term to both student and teacher outputs for numerical stability.  <ref type="bibr" target="#b3">[4]</ref> 0.806 0.806 0.862 0.911 ABMIL <ref type="bibr" target="#b15">[16]</ref> 0.845 0.865 0.900 0.949 CLAM-SB <ref type="bibr" target="#b19">[20]</ref> 0.865 0.885 0.875 0.944 CLAM-MB <ref type="bibr" target="#b19">[20]</ref> 0.850 0.894 0.878 0.949 Trans-MIL † <ref type="bibr" target="#b24">[25]</ref> 0.883 0.942 0.881 0.948 DTFD (AFS) <ref type="bibr" target="#b27">[28]</ref> 0.908 0.946 0.891 0.951 DTFD (MaxMinS) <ref type="bibr" target="#b27">[28]</ref> 0.899 0.941 0.894 0.961 DSMIL † <ref type="bibr" target="#b18">[19]</ref> 0.915 0.952 0.888 0.951 Multi Scale MS-DA-MIL <ref type="bibr" target="#b12">[13]</ref> 0.876 0.887 0.900 0.955 MS-MILRNN <ref type="bibr" target="#b3">[4]</ref> 0.814 0.837 0.891 0.921 HIPT † <ref type="bibr" target="#b5">[6]</ref> 0.890 0.951 0.890 0.950 DSMIL-LC † <ref type="bibr" target="#b18">[19]</ref> 0.909 0.955 0.913 0.964 H 2 -MIL † <ref type="bibr" target="#b14">[15]</ref> 0.859 0.912 0.823 0.917 DAS-MIL (ours) 0.945 0.973 0.925 0.965</p><p>Camelyon16. <ref type="bibr" target="#b0">[1]</ref> We adhere to the official training/test sets. To produce the fairest comparison with the single-scale state-of-the-art solution, the 270 remaining WSIs are split into training and validation in the proportion 9:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TCGA Lung Dataset.</head><p>It is available on the GDC Data Transfer Portal and comprises two subsets of cancer: Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC), counting 541 and 513 WSIs, respectively. The aim is to classify LUAD vs LUSC; we follow the split proposed by DSMIL <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with the State-of-the-art</head><p>Table <ref type="table" target="#tab_0">1</ref> compares our DAS-MIL approach with the state-of-the-art, including both single-and multi-scale architectures. As can be observed: i) the joint exploitation of multiple resolutions is generally more efficient; ii) our DAS-MIL yields robust and compelling results, especially on Camelyon16, where it provides 0.945 of accuracy and 0.973 AUC (i.e., an improvement of +3.3% accuracy and +1.9% AUC with respect to the SOTA). Finally, we remark that most of the methods in the literature resort to different feature extractors; however, the next subsections prove the consistency of DAS-MIL benefits across various backbones.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Analysis</head><p>On the Impact of Knowledge Distillation. To assess its merits, we conducted several experiments varying the values of the corresponding balancing coefficients (see Table <ref type="table" target="#tab_1">2</ref>). As can be observed, lowering their values (even reaching λ = 0, i.e., no distillation is performed) negatively affects the performance. Such a statement holds not only for the lower resolution (as one could expect), but also for the higher one, thus corroborating the claims we made in Sect. 3 on the bidirectional benefits of knowledge distillation in our multi-scale architecture. We have also performed an assessment on the temperature τ , which controls the smoothing factor applied to teacher's predictions (Table <ref type="table">3</ref>). We found that the lowest the temperature, the better the results, suggesting that the teacher scale is naturally not overconfident about its predictions, but rather well-calibrated.</p><p>Single-Scale vs Multi-Scale.  The Impact of the Feature Extractors and GNNs. Table <ref type="table" target="#tab_4">5</ref> proposes an investigation of these aspects, which considers both SimCRL <ref type="bibr" target="#b7">[8]</ref> and DINO, as well as the recently proposed graph mechanism H 2 -MIL <ref type="bibr" target="#b14">[15]</ref>. In doing so, we fix the input resolutions to 5× and 20×. We draw the following conclusions: i) when our DAS-MIL feature propagation layer is used, the selection of the optimal feature extractor (i.e., SimCLR vs Dino) has less impact on performance, as the message-passing can compensate for possible lacks in the initial representation; ii) DAS-MIL appears a better features propagator w.r.t. H 2 -MIL.</p><p>H 2 -MIL exploits a global pooling layer (IHPool) that fulfils only the spatial structure of patches: as a consequence, if non-tumor patches surround a tumor patch, its contribution to the final prediction is likely to be outweighed by the IHPool module of H 2 -MIL. Differently, our approach is not restricted in such a way, as it can dynamically route the information across the hierarchical structure (also based on the connections with the critical instance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel way to exploit multiple resolutions in the domain of histological WSI. We conceived a novel graph-based architecture that learns spatial correlation at different WSI resolutions. Specifically, a GNN cascade architecture is used to extract context-aware and instance-level features considering the spatial relationship between scales. During the training process, this connection is further amplified by a distillation loss, asking for an agreement between the lower and higher scales. Extensive experiments show the effectiveness of the proposed distillation approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>λ λ λ Table 3 .</head><label>λ3</label><figDesc>20× × × 10× × × β β β 20× × × 10× × × Impact (Camelyon16) of KD temperature (Eq. 1), α = β = 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art solutions. Results marked with " †" have been calculated on our premises as the original papers lack the specific settings; all the other numbers are taken from<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Camelyon16</cell><cell cols="2">TCGA Lung</cell></row><row><cell></cell><cell cols="4">Accuracy AUC Accuracy AUC</cell></row><row><cell>Single Scale Mean-pooling  †</cell><cell>0.723</cell><cell cols="2">0.672 0.823</cell><cell>0.905</cell></row><row><cell>Max-pooling  †</cell><cell>0.893</cell><cell cols="2">0.899 0.851</cell><cell>0.909</cell></row><row><cell>MILRNN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Impact (AUC, Came-lyon16) of Eq. 3 hyperparameters.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison between scales. The target column indicates the features passed to the two MIL layers: the " " symbol indicates that they have been previously concatenated.</figDesc><table><row><cell cols="2">Input Scale MIL Target(s)</cell><cell cols="2">Accuracy AUC</cell></row><row><cell>10×</cell><cell>10×</cell><cell>0.818</cell><cell>0.816</cell></row><row><cell>20×</cell><cell>20×</cell><cell>0.891</cell><cell>0.931</cell></row><row><cell>5×, 20×</cell><cell>5×, 20×</cell><cell>0.891</cell><cell>0.938</cell></row><row><cell>5×, 20×</cell><cell>5×, [5× 20×]</cell><cell>0.898</cell><cell>0.941</cell></row><row><cell cols="2">10×, 20× 10×, 20×</cell><cell>0.945</cell><cell>0.973</cell></row><row><cell cols="3">10×, 20× 10×, [10× 20×] 0.922</cell><cell>0.953</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 4 demonstrates the contribution of hierarchical representations. For singlescale experiments, the model is fed only with patches extracted at a single reference scale. For what concerns multi-scale results, representations can be combined in different ways. Overall, the best results are obtained with 10× and 20× input resolutions; the table also highlights that 5× magnitude is less effective and presents a worst discriminative capability. We ascribe it to the specimenlevel pixel size relevant for cancer diagnosis task; different datasets/tasks may benefit from different scale combinations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison between DAS-MIL with and w/o (✗) the graph contextualization mechanism, and the most recent graph-based multi-scale approach H 2 -MIL, when using different resolutions as input (5× and 20×).</figDesc><table><row><cell cols="3">Feature Extractor Graph Mechanism Camelyon16 TCGA Lung</cell></row><row><cell></cell><cell></cell><cell>Acc. AUC Acc. AUC</cell></row><row><cell>SimCLR</cell><cell>✗</cell><cell>0.859 0.869 0.864 0.932</cell></row><row><cell>SimCLR</cell><cell>DAS-MIL</cell><cell>0.906 0.928 0.883 0.9489</cell></row><row><cell>SimCLR</cell><cell>H 2 -MIL</cell><cell>0.836 0.857 0.826 0.916</cell></row><row><cell>DINO</cell><cell>✗</cell><cell>0.852 0.905 0.906 0.956</cell></row><row><cell>DINO</cell><cell>DAS-MIL</cell><cell>0.891 0.938 0.925 0.965</cell></row><row><cell>DINO</cell><cell>H 2 -MIL</cell><cell>0.859 0.912 0.823 0.917</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The relation "part of" connects a parent WSI patch (lying in the lower resolution) with its children, i.e., the higher-scale patches it contains.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This project has received funding from <rs type="funder">DECIDER</rs>, the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under GA No. <rs type="grantNumber">965193</rs>, and from the <rs type="funder">Department of Engineering "Enzo Ferrari" of the University of Modena</rs> through the <rs type="programName">FARD-2022 (Fondo di Ateneo per la Ricerca 2022</rs>). We also acknowledge the <rs type="grantName">CINECA award</rs> under the <rs type="grantName">ISCRA initiative</rs>, for the availability of high performance computing resources and support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8U53A4e">
					<idno type="grant-number">965193</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_gHpPGPh">
					<orgName type="grant-name">CINECA award</orgName>
					<orgName type="program" subtype="full">FARD-2022 (Fondo di Ateneo per la Ricerca 2022</orgName>
				</org>
				<org type="funding" xml:id="_CB8QENs">
					<orgName type="grant-name">ISCRA initiative</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 24.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One DAG to rule them all</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bolelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Allegretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3647" to="3658" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning Meaningful Representations of Life</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diagnose like a pathologist: weakly-supervised pathologist-tree network for slide-level immunohistochemical scoring</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for whole slide image analysis: an overview</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Caie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">264</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale domain-adversarial multiple-instance CNN for cancer subtype classification with unannotated histopathological images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3852" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">H2-MIL: exploring hierarchical representation with heterogeneous multiple instance learning for whole slide image analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TSFD-Net: tissue specific feature distillation network for nuclei segmentation and classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Mannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Whole slide imaging (WSI) in pathology: current perspectives and future directions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1034" to="1040" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How many observations are enough? Knowledge distillation for trajectory forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6543" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust Re-Identification by Multiple Views Knowledge Distillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="93" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust re-identification by multiple views knowledge distillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58607-2_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58607-26" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12355</biblScope>
			<biblScope unit="page" from="93" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transmil: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst. (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DTFD-MIL: double-tier feature distillation multiple instance learning for histopathology whole slide image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18802" to="18812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-distillation: towards efficient and compact neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4388" to="4403" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Be your own teacher: improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SETMIL: spatial encoding transformer-based multiple instance learning for pathological image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-77" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting lymph node metastasis using histopathological images based on multiple instance learning with deep graph convolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4837" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
