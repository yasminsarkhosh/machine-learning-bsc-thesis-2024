<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenxi</forename><surname>Zhang</surname></persName>
							<email>zxzhang_5@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>2 South Taibai Road</addrLine>
									<settlement>Xi&apos;an, Shanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Ran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Cancer Center</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunna</forename><surname>Tian</surname></persName>
							<email>chnatian@xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>2 South Taibai Road</addrLine>
									<settlement>Xi&apos;an, Shanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>2 South Taibai Road</addrLine>
									<settlement>Xi&apos;an, Shanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">AIQ</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">AIQ</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Jiao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Diagnostic Imaging</orgName>
								<orgName type="institution">Warren Alpert Medical School of Brown University</orgName>
								<address>
									<settlement>Providence</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="192" to="201"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">91F8F1DEDD951907A6A18F4E26CD9EDE</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prototypical learning</term>
					<term>Consistency learning</term>
					<term>Semi-supervised segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consistency learning plays a crucial role in semi-supervised medical image segmentation as it enables the effective utilization of limited annotated data while leveraging the abundance of unannotated data. The effectiveness and efficiency of consistency learning are challenged by prediction diversity and training stability, which are often overlooked by existing studies. Meanwhile, the limited quantity of labeled data for training often proves inadequate for formulating intra-class compactness and inter-class discrepancy of pseudo labels. To address these issues, we propose a self-aware and cross-sample prototypical learning method (SCP-Net) to enhance the diversity of prediction in consistency learning by utilizing a broader range of semantic information derived from multiple inputs. Furthermore, we introduce a self-aware consistency learning method which exploits unlabeled data to improve the compactness of pseudo labels within each class. Moreover, a dual loss re-weighting method is integrated into the cross-sample prototypical consistency learning method to improve the reliability and stability of our model. Extensive experiments on ACDC dataset and PROMISE12 dataset validate that SCP-Net outperforms other stateof-the-art semi-supervised segmentation methods and achieves significant performance gains compared to the limited supervised training. Code is available at https://github.com/Medsemiseg/SCP-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing demand for accurate and efficient medical image analysis, Semi-supervised segmentation methods offer a viable solution to tackle the problems associated with scarce labeled data and mitigate the reliance on manual expert annotation. It is often not feasible to annotate all images in a dataset. By exploring the information contained in the unlabeled data, semi-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> can help to improve segmentation performance compared to using only a small set of annotated examples.</p><p>Consistency constraint is a widely-used solution in semi-supervised segmentation to improve performance by making the prediction and/or intermediate features remain consistent under different perturbations. However, it's challenging to obtain universal and appropriate perturbations (e.g., augmentation <ref type="bibr" target="#b2">[3]</ref>, contexts <ref type="bibr" target="#b3">[4]</ref>, and decoders <ref type="bibr" target="#b4">[5]</ref>) across different tasks. In addition, the efficacy of the consistency loss utilized in semi-supervised segmentation models could be weakened by minor perturbations that have no discernible effect on the predicted results. Conversely, unsuitable perturbations or unclear boundaries between structures could introduce inaccurate supervisory signals, causing a build-up of errors and leading to sub-optimal performance of the model. Recently, some unsupervised prototypical learning methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref> apply the feature matching operation based on the category prototypes to generate the pseudo labels in the semi-supervised segmentation task. Then, the consistency constraint is enforced between the model's prediction and the corresponding prototypical prediction to enhance the model's performance. For example, Xu, et al. <ref type="bibr" target="#b5">[6]</ref> propose a cyclic prototype consistency learning framework which involves a two-way flow of information between labeled and unlabeled data. Wu, et al. <ref type="bibr" target="#b6">[7]</ref> suggest to facilitate the convergence of class-specific features towards their corresponding high-quality prototypes by promoting their alignment. Zhang, et al. <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10]</ref> exploit the feature distances from prototypes to facilitate online correction of the pseudo label in the training course. Limited by the quantity of prototypes and insufficient feature relation learning, the only one global category prototype used in <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b8">[8]</ref> for feature matching might omit diversity and impair the representation capability.</p><p>To put it briefly, prior research has not fully addressed the robustness and variability of prediction results in response to perturbations. To address this, unlike the global prototypes in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we propose a novel prototype generation method, namely self-aware and cross-sample class prototypes, which generates two distinct prototype predictions to enhance semantic information interaction and ensure disagreement in consistency training. We also propose to use prediction uncertainty between self-aware prototype prediction and multiple predictions to re-weight the consistency constraint loss of cross-sample prototypes. By doing so, we can reduce the adverse effects of label noise in challenging areas such as low-contrast regions or adhesive edges, resulting in a more stable consistency constraint training process. This, in turn, would lead to significantly improved model performance and accuracy. Lastly, we present SCP-Net, a parameter-free semi-supervised segmentation framework (Fig. <ref type="figure" target="#fig_0">1</ref>) that incorporates both types of prototypical consistency constraints.</p><p>The main contributions of this paper can be summarized as: <ref type="bibr" target="#b0">(1)</ref> We conduct an in-depth study on prototype-based semi-supervised segmentation methods and propose self-aware prototype prediction and cross-sample prototype predic-tion to ensure appropriate prediction diversity in consistency learning. <ref type="bibr" target="#b1">(2)</ref> To enhance the intra-class compactness of pseudo labels, we propose a self-aware prototypical consistency learning method. (3) To boost the stability and reliability of cross-sample prototypical consistency learning, we design a dual loss re-weighting method which helps to reduce the negative effect of noisy pseudo labels. (4) Extensive experiments on ACDC and PROMISE12 datasets have demonstrated that SCP-Net effectively utilizes the unlabeled data and improves semi-supervised segmentation performance with a low annotation ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In the semi-supervised segmentation task, the training set is divided into the labeled set</p><formula xml:id="formula_0">D l = {(x k , y k )} N l k=1 and the unlabeled set D u = {x k } N l +Nu k=N l +1 , where N u N l . Each labeled image x k ∈ R H×W has its ground-truth mask y k ∈ {0, 1}</formula><p>C×H×W , where H, W , and C are the height, width, and class number, respectively. Our objective is to enhance the segmentation performance of the model by extracting additional knowledge from the unlabeled dataset D u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-cross Prototypical Prediction</head><p>The prototype in segmentation refers to the aggregated representation that captures the common characteristics of some pixel-wise features from a particular object or class. Let p c k (i) denote the probability of pixel i belonging to class c, f k ∈ R D×H×W represent the feature map of sample k. The class-wise prototypes q c k is defined as follows:</p><formula xml:id="formula_1">q c k = i p c k (i) • f k (i) i p c k (i)<label>(1)</label></formula><p>Let B denote the batch size. In the iterative training process, one mini-batch contains B × C prototypes for sample k = 1 and other samples with index j = 2, 3, • • • , B. Then, feature similarity is calculated according to the selfaware prototype q c k or cross-sample prototypes q c j to form multiple segmentation probability matrices. Specifically, ŝc kk is the self-aware prototypical similarity map via calculating the cosine similarity between the feature map f k and the prototype vector q c k as Eq. 2:</p><formula xml:id="formula_2">ŝc kk = f k • q c k f k • q c k (2)</formula><p>Then, sof tmax function is applied to generate the self-aware probability prediction pkk ∈ R C×H×W based on ŝkk ∈ R C×H×W . Since q c k is aggregated in sample k itself, which can align f k with more homologous features, ensuring the intra-class consistency of prediction. Similarly, we can obtain B -1 cross-sample prototypical similarity maps ŝc kj following Eq. 3:</p><formula xml:id="formula_3">ŝc kj = f k • q c j f k • q c j (3)</formula><p>This step ensures that features are associated and that information is exchanged in a cross-image manner. To enhance the reliability of prediction, we take the multiple similarity estimations ŝkj ∈ R C×H×W into consideration and integrate them to get the cross-sample probability prediction pko ∈ R C×H×W : </p><formula xml:id="formula_4">pc ko = B j=2 e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prototypical Prediction Uncertainty</head><p>To effectively evaluate the predication consistency and training stability in semisupervised settings, we propose a prototypical prediction uncertainty estimation method based on the similarity matrices ŝkk and ŝkj . First, we generate B binary represented mask mkn ∈ R C×H×W via argmax operation and one-hot encoding operation, where n = 1, 2, • • • , B. Then, we sum all masks mkn and dividing it by B to get a normalized probability pnorm as:</p><formula xml:id="formula_5">pc norm = B n=1 mc kn B (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>And a normalized entropy is estimated from pnorm , denoted as e k ∈ R H×W :</p><formula xml:id="formula_7">e k = - 1 log(C) C c=1 pc norm log pc norm (6)</formula><p>where e k serves as the overall confidence of multiple prototypical predictions, and a higher entropy equals more prediction uncertainty. Then, we use e k to adjust the pixel-wise weight of labeled and unlabeled samples, which will be elaborated in next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised Prototypical Consistency Constraint</head><p>To enhance the prediction diversity and training effectiveness in consistency learning and mitigate the negative effect of noisy predictions in pkk and pkj , we propose two unsupervised prototypical consistency constraints (PCC) in SPC-Net benefiting from the self-aware prototypical prediction pkk , cross-sample prototypical prediction pkj , and the corresponding uncertainty estimation e k .</p><p>Self-aware Prototypical Consistency Constraint (SPCC). To boost the intra-class compactness of segmentation prediction, we propose a SPCC method which applies pkk as pseudo-label supervision. Therefore, the loss function of SPCC is formulated as:</p><formula xml:id="formula_8">L spcc = 1 C × H × W H×W i=1 C c=1 pc kk (i) -p c k (i) 2<label>(7)</label></formula><p>Cross-sample Prototypical Consistency Constraint (CPCC). To derive dependable knowledge from other training samples, we propose a dual-weighting method for CPCC. First, we take the uncertainty estimation e k into account, which reflects the prediction stability. A higher value of e k indicates that pseudo labels with greater uncertainty may be more susceptible to errors. However, these regions provide valuable information for segmentation performance. To reduce the influence of the suspicious pseudo labels and adjust the contribution of these crucial supervisory signals during training, we incorporate e k in CPCC by setting a weight w 1ki = 1-e ki . Second, we introduce the self-aware probability prediction pkk into the CPCC module. Specifically, we calculate the maximum value of pkk along class c, termed as the self-aware confidence weight w 2ki :</p><formula xml:id="formula_9">w 2ki = max c pc kk (i)<label>(8)</label></formula><p>w 2k can further enhance the reliability of CPCC. Therefore, the optimized function of CPCC is calculated between cross-sample prototypical prediction pko and pk :</p><formula xml:id="formula_10">L cpcc = 1 C × H × W H×W i=1 C c=1 w 1ki • w 2ki • pc ko (i) -p c k (i) 2<label>(9)</label></formula><p>Loss Function of SCP-Net We use the combination of cross-entropy loss L ce and Dice loss L Dice to supervise the training process of labeled set <ref type="bibr" target="#b11">[11]</ref>, which is defined as:</p><formula xml:id="formula_11">L seg = L ce (p k , y k ) + L Dice (p k , y k )<label>(10)</label></formula><p>For both labeled data and unlabeled data, we leverage L spcc and L cpcc to provide unsupervised consistency constraints for network training and explore the valuable unlabeled knowledge. To sum it up, the overall loss function of SCPNet is the combination of the supervised loss and the unsupervised consistency loss, which is formulated as:</p><formula xml:id="formula_12">L total = N l k=1 L seg (p k , y k ) + λ N l +Nu k=1 (L spcc (p k , pkk ) + L cpcc (p k , pko ))<label>(11)</label></formula><p>λ (t) = 0.1 • e -5(<ref type="foot" target="#foot_0">1</ref>-t/tmax)<ref type="foot" target="#foot_1">2</ref> is a weight using a time-dependent Gaussian warming up function <ref type="bibr" target="#b12">[12]</ref> to balance the supervised loss and unsupervised loss. t represents the current training iteration, and t max is the total iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset and Evaluation Metric. We validate the effectiveness of our method on two public benchmarks, namely the Automated Cardiac Diagnosis Challenge 1 (ACDC) dataset <ref type="bibr" target="#b13">[13]</ref> and the Prostate MR Image Segmentation challenge 2 (PROMISE12) dataset <ref type="bibr" target="#b14">[14]</ref>. Implementation Details. Our method adopts U-Net <ref type="bibr" target="#b16">[16]</ref> as the baseline. We use the stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.1, and apply the "poly" learning rate policy to update the learning rate during training. The batch size is set to 24. Each batch includes 12 labeled slices and 12 unlabeled slices. To alleviate overfitting, we employ random flipping and random rotation to augment data. All comparison experiments and ablation experiments follow the same setup for a fair comparison, we use the same experimental setup for all comparison and ablation experiments. All frameworks are implemented with PyTorch and conducted on a computer with a 3.0 GHz CPU, 128 GB RAM, and four NVIDIA GeForce RTX 3090 GPUs.  Comparison with Other Methods. To demonstrate the effectiveness of SCP-Net, we compare it with 7 state-of-the-art methods for semi-supervised segmentation and fully-supervised (100% labeled ratio) limited supervised (20% labeled ratio) baseline. The quantitative analysis results of ACDC dataset are shown in Table <ref type="table" target="#tab_1">1</ref>. SCP-Net significantly outperforms the limited supervised baseline by 7.02%, 6.13%, and 6.32% on DSC for RV, Myo, and LV, respectively. SCP-Net achieves comparable DSC and ASSD to the fully supervised baseline. (89.69% vs 91.78 and 0.73 vs 0.52). Compared with other methods, SCP-Net achieves the best DSC and ASSD, which is 1.58% and 0.24 higher than the second-best metric, respectively. Moreover, we visualize several segmentation examples of ACDC dataset in Fig. <ref type="figure" target="#fig_1">2</ref>. SCP-Net yields consistent and accurate segmentation results for the RV, Myo, and LV classes according to ground truth (GT), proving that the unsupervised prototypical consistency constraints effectively extract valuable unlabeled information for segmentation performance improvement. Table <ref type="table">3</ref> in supplementary material reports the quantitative result for prostate segmentation. We also perform the limited supervised and fully supervised training with 10% labeled ratio and 100% labeled ratio, respectively. SCP-Net surpasses the limited supervised baseline by 16.18% on DSC, and 10.35 on ASSD. In addition, SCP-Net gains the highest DSC of 77.06%, which is 5.63% higher than the second-best CCT. All improvements suggest that SPCC and CPCC are beneficial for exploiting unlabeled information. We also visualize some prostate segmentation examples in the last two rows of Fig. <ref type="figure" target="#fig_1">2</ref>. We can observe that SCP-Net generates anatomically-plausible results for prostate segmentation.  Ablation Study. To demonstrate the effectiveness of the key design of SCP-Net, we perform ablation study on PROMISE12 dataset by gradually adding loss components. Table <ref type="table" target="#tab_2">2</ref> reports the results of ablation results. It can be observed that both the design of SPCC and CPCC promote the semi-supervised segmentation performance according to the first three rows, which demonstrates that PCC extracts valuable information from the image itself and other images, making them well-suited for semi-supervised segmentation. We also visualize the prototypical prediction pkk and pko for different structures in Fig. <ref type="figure" target="#fig_2">3</ref>. These predictions are consistent with the ground truths and show intra-class compactness and inter-class discrepancy, which validates that PCC provides effective supervision for semi-supervised segmentation. In the last three rows, the gradually improving performance verifies that the integration of prediction uncertainty w 1 and self-aware confidence w 2 in CPCC improves the reliability and stability of consistency training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To summarize, our proposed SCP-Net, which leverages self-aware and crosssample prototypical consistency learning, has successfully tackled the challenges of prediction diversity and training effectiveness in semi-supervised consistency learning. The intra-class compactness of pseudo label is boosted by SPCC.</p><p>The dual loss re-weighting method of CPCC enhances the model's reliability. The superior segmentation performance demonstrates that SCP-Net effectively exploits the useful unlabeled information to improve segmentation performance given limited annotated data. Moving forward, our focus will be on investigating the feasibility of learning an adaptable number of prototypes that can effectively handle varying levels of category complexity. By doing so, we expect to enhance the quality of prototypical predictions and improve the overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall flowchart of SCP-Net, which consists of three parts: the supervised training with Lseg, SPCC module with Lspcc, CPCC module with Lcpcc.</figDesc><graphic coords="4,61,98,175,25,328,54,232,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualized segmentation results of different methods on ACDC and PROMISE12. SCP-Net better preserves anatomical morphology compared to others.</figDesc><graphic coords="7,44,79,238,61,334,57,134,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualized results for prototypical probability predictions for RV, Myo, LV, and prostate class: (a) Ground truth, (b) Self-aware probability prediction, pkk , (c) Cross-sample probability prediction, pko .</figDesc><graphic coords="8,70,47,370,97,311,26,86,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparision with other methods on the ACDC test set. DSC (%) and ASSD (mm) are reported with 28 labeled scans and 112 unlabeled scans for semi-supervised training. The bold font represents the best performance.</figDesc><table><row><cell>Method</cell><cell>Scans Used</cell><cell></cell><cell>RV</cell><cell>Myo</cell><cell>LV</cell><cell>Avg</cell></row><row><cell></cell><cell>Labeled</cell><cell cols="5">Unlabeled DSC ↑ ASSD ↓ DSC↑ ASSD↓ DSC↑ ASSD↓ DSC↑ ASSD↓</cell></row><row><cell>U-Net</cell><cell>28 (20%)</cell><cell>0</cell><cell>82.24 2.18</cell><cell>80.98 2.21</cell><cell>86.89 1.75</cell><cell>83.37 1.60</cell></row><row><cell>U-Net</cell><cell cols="2">140 (100%) 0</cell><cell>91.48 0.47</cell><cell>89.22 0.54</cell><cell>94.64 0.55</cell><cell>91.78 0.52</cell></row><row><cell>MT [12]</cell><cell>28</cell><cell>112</cell><cell>87.47 0.42</cell><cell>86.19 1.11</cell><cell>90.23 2.56</cell><cell>87.97 1.37</cell></row><row><cell>UAMT [17]</cell><cell>28</cell><cell>112</cell><cell>87.69 0.43</cell><cell>85.97 0.76</cell><cell>90.67 2.18</cell><cell>88.11 1.52</cell></row><row><cell>CCT [18]</cell><cell>28</cell><cell>112</cell><cell>87.97 0.45</cell><cell>86.07 1.30</cell><cell>89.60 3.38</cell><cell>87.88 1.71</cell></row><row><cell>URPC [19]</cell><cell>28</cell><cell>112</cell><cell>80.55 0.39</cell><cell>84.09 1.82</cell><cell>88.76 3.74</cell><cell>84.47 1.98</cell></row><row><cell>SSNet [7]</cell><cell>28</cell><cell>112</cell><cell>87.21 0.45</cell><cell>86.00 1.68</cell><cell>90.91 1.67</cell><cell>88.04 0.97</cell></row><row><cell>MC-Net [5]</cell><cell>28</cell><cell>112</cell><cell>82.69 0.96</cell><cell>84.15 1.66</cell><cell>88.86 3.66</cell><cell>85.24 2.09</cell></row><row><cell>SLC-Net [15]</cell><cell>28</cell><cell>112</cell><cell>82.19 1.93</cell><cell>82.57 1.21</cell><cell>88.97 1.25</cell><cell>84.58 1.47</cell></row><row><cell cols="2">SCP-Net (Ours) 28</cell><cell>112</cell><cell>89.26 0.77</cell><cell>87.11 0.51</cell><cell>92.70 0.92</cell><cell>89.69 0.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Abaliton study of the key design of SCP-Net. w means with and w/o means without.</figDesc><table><row><cell>Loss Function</cell><cell cols="2">Scans Used</cell><cell cols="2">Weight</cell><cell>DSC↑ ASSD ↓</cell></row><row><cell></cell><cell cols="4">Labeled Unlabeled w1 w2</cell><cell></cell></row><row><cell>Lseg</cell><cell>7</cell><cell>0</cell><cell cols="3">w/o w/o 60.88 13.87</cell></row><row><cell>Lseg + Lspcc</cell><cell>7</cell><cell>28</cell><cell cols="3">w/o w/o 73.48 5.06</cell></row><row><cell>Lseg + Lcpcc</cell><cell>7</cell><cell>28</cell><cell>w</cell><cell>w</cell><cell>73.52 4.98</cell></row></table><note><p>Lseg + Lcpcc + Lspcc 7 28 w/o w/o 74.99 4.05 Lseg + Lcpcc + Lspcc 7 28 w w/o 76.12 3.78 Lseg + Lcpcc + Lspcc 7 28 w w 77.06 3.52</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://promise12.grand-challenge.org</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_18.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty guided semi-supervised segmentation of retinal layers in OCT images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sedai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7_32" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="282" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transformationconsistent self-ensembling model for semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with directional contextaware consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1205" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">All-around real label supervision: cyclic prototype consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3174" to="3184" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring smoothness and class-separation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_4" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Model-driven self-aware self-training framework for label noisetolerant medical image segmentation. Signal Process</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">109177</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic prototypical feature representation learning framework for semi-supervised skin lesion segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">507</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of prostate segmentation algorithms for MRI: the promise12 challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation using cross-model pseudo-supervision with shape awareness and local context constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VIII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_67" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_30" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
