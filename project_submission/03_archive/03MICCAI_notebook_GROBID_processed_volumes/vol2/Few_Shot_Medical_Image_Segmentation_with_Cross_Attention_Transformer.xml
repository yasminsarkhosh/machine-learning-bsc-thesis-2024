<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few Shot Medical Image Segmentation with Cross Attention Transformer</title>
				<funder ref="#_9vpcRQ6">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_ZNrEerX">
					<orgName type="full">Shenzhen Science and Technology Innovation Committee Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yufan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few Shot Medical Image Segmentation with Cross Attention Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="233" to="243"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6AFD0A7AA547BB67D8942EA74E24CB16</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few Shot</term>
					<term>Cross Attention</term>
					<term>Iterative Refinement Y. Lin and Y. Chen-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation has made significant progress in recent years. Deep learning-based methods are recognized as datahungry techniques, requiring large amounts of data with manual annotations. However, manual annotation is expensive in the field of medical image analysis, which requires domain-specific expertise. To address this challenge, few-shot learning has the potential to learn new classes from only a few examples. In this work, we propose a novel framework for few-shot medical image segme ntation, termed CAT-Net, based on cross masked attention Transformer. Our proposed network mines the correlations between the support image and query image, limiting them to focus only on useful foreground information and boosting the representation capacity of both the support prototype and query features. We further design an iterative refinement framework that refines the query image segmentation iteratively and promotes the support feature in turn. We validated the proposed method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental results demonstrate the superior performance of our method compared to state-of-the-art methods and the effectiveness of each component. Source code: https://github. com/hust-linyi/CAT-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic segmentation of medical images is a fundamental step for a variety of medical image analysis tasks, such as diagnosis, treatment planning, and disease monitoring <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The emergence of deep learning (DL) has enabled the development of many medical image segmentation methods, which have achieved remarkable success <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref>. Most of the existing methods follow a fullysupervised learning paradigm, which requires a considerable amount of labeled data for training. However, the manual annotation of medical images is timeconsuming and labor-intensive, limiting the application of DL in medical image segmentation. Specifically for the 3D volumetric medical images (e.g., CT, MRI), the manual annotation is even more challenging which requires the annotators to go through hundreds of 2D slices for each 3D scan.</p><p>To address the challenge of manual annotation, various label-efficient techniques have been explored, such as self-supervised learning <ref type="bibr" target="#b14">[15]</ref>, semi-supervised learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, and weakly-supervised learning <ref type="bibr" target="#b10">[11]</ref>. Despite leveraging information from unlabeled or weakly-labeled data, these techniques still require a substantial amount of training data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>, which may not be practical for novel classes with limited examples in the medical domain. This limitation encourages the few-shot learning paradigm <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> to be applied to medical image segmentation. Specifically, the few-shot learning paradigm aims to learn a model from a small number of labeled data (denoted as support) and then apply it to a new task (denoted as query) with only a few labeled data without any retraining. Considering the hundreds of organs and countless diseases in the human body, FSL brings great potential to the various medical image segmentation tasks where a new task can be easily investigated in a data-efficient manner.</p><p>Most few-shot segmentation methods follow the learning-to-learn paradigm, which aims to learn a meta-learner to predict the segmentation of query images based on the knowledge of support images and their respective segmentation labels. The success of this paradigm depends on how effectively the knowledge can be transferred from the support prototype to the query images. Existing fewshot segmentation methods mainly focus on the following two aspects: (1) how to learn the meta-learner <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>; and (2) how to better transfer the knowledge from the support images to the query images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Despite prototype-based methods having shown success, they typically ignore the interaction between support and query features during training. In this paper, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we propose CAT-Net, a Cross Attention Transformer network for few-shot medical image segmentation, which aims to fully capture intrinsic classes details while eliminating useless pixel information and learn an interdependence between the support and query features. Different from the existing FSS methods that only focus on the single direction of knowledge transfer (i.e., from the support features to the query features), the proposed CAT-Net can boost the mutual interactions between the support and query features, benefiting the segmentation performance of both the support and query images. Additionally, we propose an iterative training framework that feed the prior query segmentation into the attention transformer to effectively enhance and refine the features as well as the segmentation. Three publicly available datasets are adopted to evaluate our CAT-Net, i.e., Abd-CT <ref type="bibr" target="#b8">[9]</ref>, Abd-MRI <ref type="bibr" target="#b7">[8]</ref>, and Card-CT <ref type="bibr" target="#b32">[33]</ref>. Extensive experiments validate the effectiveness of each component in our CAT-Net, and demonstrate its state-of-the-art performance.  To obtain the segmentation model for FSS, the commonly used episode training approach is employed <ref type="bibr" target="#b28">[29]</ref>. Each trainig/testing episode (S i , Q i ) instantiates a N -way K-shot segmentation learning task. Specifically, the support set S i contains K samples of N classes, while the query set Q i contains one sample from the same class. The FSS model is trained with episodes to predict the novel class for the query image, guided by the support set. During inference, the model is evaluated directly on D test without any re-training. In this paper, we follow the established practice in medical FSS <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> that consider the 1-way 1-shot task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Overview</head><p>The Overview of our CAT-Net is illustrated in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. It consists of three main components: 1) a mask incorporated feature extraction (MIFE) sub-net that extracts initial query and support features as well as query mask; 2) a cross masked attention Transformer (CMAT) module in which the query and support features boost each other and thus refined the query prediction; and 3) an iterative refinement framework that sequentially applies the CMAT modules to continually promote the segmentation performance. The whole framework can be trained in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mask Incorporated Feature Extraction</head><p>The Mask Incorporate Feature Extraction (MIFE) sub-net takes query and support images as input and generates their respective features, integrated with the support mask. A simple classifier is then used to predict the segmentation for the query image. Specifically, we first employ a feature extractor network (i.e., ResNet-50) to map the query and support image pair I q and I s into the feature space, producing multi-level feature maps F q and F s for query and support image, respectively. Next, the support mask is pooled with F s and then expanded and concatenated with both F q and F s . Additionally, the segmentation mask of query image in MIFE is further concatenated with the query feature to strengthen the correlation between query and support features via a pixel-wise similarly map. Finally, the query feature is processed by a simple classifier to get the query mask. Further details of the MIFE architecture can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cross Masked Attention Transformer</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, the cross masked attention Transformer (CMAT) module comprises three main components: 1) a self-attention module for extracting global information from query and support features; 2) a cross masked attention module for transferring foreground information between query and support features while eliminating redundant background information, and 3) a prototypical segmentation module for generating the final prediction of the query image.</p><p>Self-Attention Module. To capture the global context information of every pixel in the query feature F q 0 and support features F s 0 , the initial features are first flattened into 1D sequences and fed into two identical self-attention modules. Each self-attention module consists of a multi-head attention (MHA) layer and a multi-perceptron (MLP) layer. Given an input sequence S, the MHA layer first projects the sequence into three sequences K, Q, and V with different weights. The attention matrix A is then calculated as:</p><formula xml:id="formula_0">A(Q, K) = QK T √ d (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where d is the dimension of the input sequence. The attention matrix is then normalized by a softmax function and multiplied by the value sequence V to get the output sequence O:</p><formula xml:id="formula_2">O = softmax(A)V (2)</formula><p>The MLP layer is a simple 1×1 convolution layer that maps the output sequence O to the same dimension as the input sequence S. Finally, the output sequence O is added to the input sequence S and normalized using layer normalization (LN) to obtain the final output sequence X. The output feature sequence of the selfattention alignment encoder is represented by X q ∈ R HW ×D and X s ∈ R HW ×D for query and support features, respectively.</p><p>Cross Masked Attention Module. We utilize cross masked attention to incorporate query features and support features with respect to their foreground information by constraining the attention region in attention matrix with support and query masks. Specifically, given the query feature X q and support features X s from the aforementioned self-attention module, we first project the input sequence into three sequences K, Q, and V using different weights, resulting in K q , Q q , V q , and K s , Q s , V s , respectively. Taking the support features as an example, the cross attention matrix is calculated by:</p><formula xml:id="formula_3">A(K q , Q s ) = (K q ) T Q s √ d (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>We expand and flatten the binary query mask M q to limit the foreground region in attention map. The masked cross attention (MCA) map is computed as:</p><formula xml:id="formula_5">MCA(K q , Q s , V q , M s ) = M s • V q (softmax(A(K q , Q s )))<label>(4)</label></formula><p>Similar to self-attention, the support feature is processed by MLP and LN layer to get the final enhanced query features F s 1 . Similarly, the enhanced query feature F q</p><p>1 is obtained with foreground information from the query feature.</p><p>Prototypical Segmentation Module. Once the enhanced query and support features are obtained, the prototypical segmentation is used to obtain the final prediction. First, a prototype of class c is built by masked average pooling of the support feature F s 1 as follows:</p><formula xml:id="formula_6">p c = 1 K K k=1 k,x,y F s i,(k,x,y) m s (k,x,y,c) x,y m s (k,x,y,c)<label>(5)</label></formula><p>where K is the number of support images, and m s (k,x,y,c) is a binary mask that indicates whether pixel at the location (x, y) in support feature k belongs to class c. Next, we use the non-parametirc metric learning method to perform segmentation. The prototype network calculates the distance between the query feature vector and the prototype P = {P c |c ∈ C}. Softmax function is applied to produce probabilistic outputs for all classes, generating the query segmentation:</p><p>M q i,(x,y) = softmax αcos(F q i,(x,y) , p c ) • softmax(αcos(F q i,(x,y) , p c ))</p><p>where cos(•) denotes cosine distance, α is a scaling factor that helps gradients to back-propagate in training. In our work, α is set to 20, same as in <ref type="bibr" target="#b28">[29]</ref>. Additionally, we design a double threshold strategy to obtain query segmentation. Specifically, we set the first threshold τ to 0.5 to obtain the binary query mask M q , which is used to calculate the Dice loss and update the model. Then, the second threshold τ is set to 0.4 to obtain the dilated query mask M q , which is used to generate the enhanced query feature F q 2 in the next iteration. The second threshold τ is set lower than the first threshold τ to prevent some foreground pixels from being mistakenly discarded. The query segmentation mask M q and dilated mask M q are represented by:</p><formula xml:id="formula_8">M q i =</formula><p>1, M q i,(x,y) &gt; τ 0, M q i,(x,y) &lt; τ</p><formula xml:id="formula_9">M q i = 1, M q i,(x,y) &gt; τ 0, M q i,(x,y) &lt; τ<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Iterative Refinement Framework</head><p>As explained above, the CMAT module is designed to refine the query and support features, as well as the query segmentation mask. Thus, it's natural to iteratively apply this sub-net to get the enhanced features and refine the mask, resulting in a boosted segmentation result. The result after the i-th iteration is represented by:</p><formula xml:id="formula_10">(F s i , F q i , M q i , M q i ) = CMAT(F s i-1 , F q i-1 , M q i-1 , M s )<label>(8)</label></formula><p>The subdivision of each step can be specifically expressed as:</p><formula xml:id="formula_11">(F s i , F q i ) = CMA(F s i-1 , F q i-1 , M q i-1 , M s ) (9) (M q i , M q i ) = Proto(F s i , F q i , M s , τ, τ )<label>(10)</label></formula><p>where CMA(•) indicates the self-attention and cross masked attention module, and Proto(•) represents the prototypical segmentation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>We evaluate the proposed method on three public datasets, i.e., Abd-CT <ref type="bibr" target="#b8">[9]</ref>, Abd-MRI <ref type="bibr" target="#b7">[8]</ref>, and Card-MRI <ref type="bibr" target="#b32">[33]</ref>. Abd-CT contains 30 abdominal CT scans with annotations of left and right kidney (LK and RK), spleen (Spl), liver (Liv). Abd-MRI contains 20 abdominal MRI scans with annotations of the same organs as Abd-CT. Card-MRI includes 35 cardiac MRI scans with annotations of left ventricular blood pool (LV-B), left ventricular myocardium (LV-M), and right ventricle (RV). We use the Dice score as the evaluation metric following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>To ensure a fair comparison, all the experiments are conducted under the 1-way 1-shot scenario using 5-fold cross-validation. We follow <ref type="bibr" target="#b14">[15]</ref> to remove all slices containing test classes during training to ensure that the test classes are all unseen during validation. In each fold, we follow <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> that takes the last patient as the support image and the remaining patients as the query (setting I). We further propose a new validation setting (setting II) that takes every image in each fold as a support image alternately and the other images as the query. The averaged result of each fold is reported. It could evaluate the generalization ability of the model by reducing the affect of support image selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The proposed method is implemented using PyTorch. Each 3D scan is sliced into 2D slices and reshaped into 256×256 pixels. Common 3D image pre-processing techniques, such as intensity normalization and resampling, are applied to the training data. We apply episode training with 20k iterations. SGD optimizer is adopted with a learning rate of 0.001 and a batch size of 1. Each episode training takes approximately 4 h using a single NVIDIA RTX 3090 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Methods</head><p>We compare the proposed CAT-Net with state-of-the-art (SOTA) methods, including SE-Net <ref type="bibr" target="#b18">[19]</ref>, PANet <ref type="bibr" target="#b28">[29]</ref>, ALP-Net <ref type="bibr" target="#b14">[15]</ref>, and AD-Net <ref type="bibr" target="#b6">[7]</ref>, and Q-Net <ref type="bibr" target="#b19">[20]</ref>. PANet <ref type="bibr" target="#b28">[29]</ref> are the typical prototypical FSS method in the natural image domain, SE-Net <ref type="bibr" target="#b18">[19]</ref>, ALP-Net <ref type="bibr" target="#b14">[15]</ref>, AD-Net <ref type="bibr" target="#b6">[7]</ref>, and Q-Net <ref type="bibr" target="#b19">[20]</ref> are the most representative work in medical FSS task. Experiment results presented in Table <ref type="table" target="#tab_0">1</ref> demonstrate that the proposed method outperforms SOTAs on all three datasets under both setting I and setting II. is able to generate more accurate and detailed segmentation results compared to SOTAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We conduct an ablation study to investigate the effectiveness of each component in CAT-Net. All ablation studies are conducted on Abd-MRI under setting II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of CMAT Block:</head><p>To demonstrate the importance of our proposed CAT-Net in narrowing the information gap between the query and supporting images and obtaining enhanced features, we conducted an ablation study. Specifically, we compared the results of learning foreground information only from the support (S →Q) or query image (Q→S ) and obtaining a single enhanced feature instead of two (S ↔Q). It can be observed that using the enhanced query feature (S →Q) achieves 66.72% in Dice, outperforming only using the enhanced support feature (Q→S ) by 0.74%. With our CMAT block, the mutual boosted support and query feature (S ↔Q) could improve the Dice by 1.90%. Moreover, the iteration refinement framework consistently promotes the above three variations by 0.96%, 0.56%, and 2.26% in Dice, respectively (Table <ref type="table" target="#tab_1">2</ref>).  Influence of Iterative Mask Refinement Block: To determine the optimal number of iterative refinement CMAT block, we experiment with different numbers of blocks. In Fig. <ref type="figure" target="#fig_3">3</ref>, we observe that increasing the number of blocks results in improved performance, with a maximum improvement of 2.26% in Dice when using 5 blocks. Considering the performance gain between using 4 and 5 CMAT blocks was insignificant, we hence opt to use four CMAT blocks in our final model to strike a balance between efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose CAT-Net, Cross Attention Transformer network for few-shot medical image segmentation. Our CAT-Net enables mutual interaction between the query and support features by the cross masked attention module, enhancing the representation abilities for both of them. Additionally, the proposed CMAT module can be iteratively applied to continually boost the segmentation performance. Experimental results demonstrated the effectiveness of each module and the superior performance of our model to the SOTA methods.</p><p>In the future, we plan to extend our CAT-Net from 2D to 3D networks, explore the application of our model to other medical image segmentation tasks, as well as the extension of our model to other clinical applications, such as rare diseases and malformed organs, where data and annotations are scarce and costly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Overview of the CAT-NET; (b) The architecture of CMAT module.</figDesc><graphic coords="3,160,02,58,73,237,64,139,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Few-shot segmentation (FSS) aims to segment novel classes by just a few samples with densely-annotated samples. In FSS, the dataset is divided into the training set D train , containing the base classes C train , and the test set D test , containing the novel classes C test , where C train ∩ C test = ∅.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results of our method on Abd-CT and Abd-MRI.</figDesc><graphic coords="7,58,44,447,32,339,34,132,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The influence of different numbers of iteration CMAT modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods in Dice coefficient (%) on Abd-CT and Abd-MRI, and Card-MRI datasets under setting I &amp; II.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Abd-CT [8]</cell><cell></cell><cell cols="3">Abd-MRI [9]</cell><cell>Card-MRI [33]</cell></row><row><cell></cell><cell>Methods</cell><cell>LK</cell><cell>RK</cell><cell>Spl.</cell><cell>Liv. Avg.</cell><cell>LK</cell><cell>RK</cell><cell>Spl.</cell><cell cols="2">Liv. Avg. LV-B LV-M RV</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="4">SE-Net [19] 32.83 14.84 0.23</cell><cell cols="6">0.27 11.91 62.11 61.32 51.80 27.43 50.66 58.04 25.18 12.86 32.03</cell></row><row><cell>Setting I</cell><cell cols="10">PA-Net [29] 37.58 34.69 43.73 61.71 44.42 47.71 47.95 58.73 64.99 54.85 70.43 46.79 69.52 62.25 ALP-Net [15] 63.34 54.82 60.25 73.65 63.02 73.63 78.39 67.02 73.05 73.02 61.89 87.54 76.71 75.38 AD-Net [7] 63.84 56.98 61.84 73.95 64.15 71.89 76.02 65.84 76.03 72.70 65.47 88.36 78.35 77.39</cell></row><row><cell></cell><cell>Q-Net [20]</cell><cell cols="9">63.26 58.37 63.36 74.36 64.83 74.05 77.52 67.43 78.71 74.43 66.87 89.63 79.25 78.58</cell></row><row><cell></cell><cell>Ours</cell><cell cols="9">63.36 60.05 67.65 75.31 66.59 74.01 78.90 68.83 78.98 75.18 66.85 90.54 79.71 79.03</cell></row><row><cell>Setting II</cell><cell cols="10">ALP-Net [15] 65.99 59.49 65.02 73.50 66.05 70.17 77.05 67.71 72.45 71.85 61.61 87.13 77.35 75.36 AD-Net [7] 67.35 59.88 64.35 76.78 67.09 72.26 76.57 67.89 73.96 72.67 65.08 86.26 76.50 75.95 Q-Net [20] 66.25 62.36 67.35 77.33 68.32 73.96 81.07 65.39 72.36 73.20 66.35 88.40 79.37 78.04 Ours 68.82 64.56 66.02 80.51 70.88 75.31 83.23 67.31 75.02 75.22 67.21 90.54 80.34 79.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Effectiveness of each component. S →Q and Q→S denote one branch CAT-Net to enhance support or query feature, respectively. S ↔Q indicates applying cross attention to both S and Q.</figDesc><table><row><cell cols="2">S →Q Q→S S↔Q Iter Dice Improve</cell></row><row><cell>66.72</cell><cell>-</cell></row><row><cell cols="2">65.98 -0.74</cell></row><row><cell cols="2">68.62 +1.90</cell></row><row><cell cols="2">67.68 +0.96</cell></row><row><cell cols="2">66.54 +0.56</cell></row><row><cell cols="2">70.88 +2.26</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">Shenzhen Science and Technology Innovation Committee Fund</rs> (Project No. <rs type="grantNumber">SGDX20210823103201011</rs>) and <rs type="funder">Hong Kong Innovation and Technology Fund</rs> (Project No. <rs type="grantNumber">ITS/028/21FP</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZNrEerX">
					<idno type="grant-number">SGDX20210823103201011</idno>
				</org>
				<org type="funding" xml:id="_9vpcRQ6">
					<idno type="grant-number">ITS/028/21FP</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_22.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image quality-aware diagnosis via meta-knowledge co-embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19819" to="19829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04378</idno>
		<title level="m">Towards generalizable diabetic retinopathy grading in unseen domains</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning robust representation for joint grading of ophthalmic diseases via adaptive curriculum and feature disentanglement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_50" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TransUNet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-support few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19800-7_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19800-7_41" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022. ECCV 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13679</biblScope>
			<biblScope unit="page" from="701" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detectioninspired few-shot medical image segmentation through self-supervision with supervoxels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102385</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MICCAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seg4Reg+: consistency learning between spine segmentation and cobb angle regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_47" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="490" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Label propagation for annotation-efficient nuclei segmentation from pathology images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08195</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking boundary detection in deep learning models for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-34048-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-34048-2_56" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging. IPMI 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wassermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13939</biblScope>
			<biblScope unit="page" from="730" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intermediate prototype mining transformer for few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">PFENet++: boosting few-shot semantic segmentation with the noise-filtered context-aware prior mask</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13788</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised learning for few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1837" to="1848" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-Machine Interactive Tissue Prototype Learning for Label-Efficient Histopathology Image Segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-34048-2_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-34048-2_5" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging. IPMI 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wassermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13939</biblScope>
			<biblScope unit="page" from="679" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarially robust prototypical few-shot segmentation with neural-ODEs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chasmai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lall</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_8" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical dense correlation distillation for few-shot segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23641" to="23651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">&apos;squeeze &amp; excite&apos; guided few-shot segmentation of volumetric images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101587</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Q-Net: query-informed few-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.11451</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AMP: adaptive masked proxies for fewshot segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5249" to="5258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation using a global correlation network with discriminative embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105067</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent mask refinement for fewshot medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3918" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized few-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PANet: few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">All-around real label supervision: cyclic prototype consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3174" to="3184" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bi-modality medical image synthesis using semi-supervised sequential generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="855" to="865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep learning for medical image segmentation: tricks, challenges and future directions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10307</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multivariate mixture model for myocardial segmentation combining multi-source images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2933" to="2946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
