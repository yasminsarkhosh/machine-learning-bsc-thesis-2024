<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Category-Independent Visual Explanation for Medical Deep Network Understanding</title>
				<funder ref="#_Bufz7qq">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_NDRBHAc">
					<orgName type="full">RIE2020 Health and Biomedical Sciences (HBMS) Industry Alignment Fund Pre-Positioning</orgName>
					<orgName type="abbreviated">PP</orgName>
				</funder>
				<funder>
					<orgName type="full">A*STAR Central Research Fund &quot;A Secure and Privacy Preserving AI Platform for Digital Health</orgName>
				</funder>
				<funder ref="#_4wrgqZN">
					<orgName type="full">Agency for Science, Technology and Research (A*STAR)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Meetyou AI Lab</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsheng</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ocular Epidemiology and Data Sciences</orgName>
								<orgName type="institution">Eye Research Institute</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Duke-NUS Medical School</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yih</forename><forename type="middle">Chung</forename><surname>Tham</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ocular Epidemiology and Data Sciences</orgName>
								<orgName type="institution">Eye Research Institute</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Centre for Innovation and Precision Eye Health</orgName>
								<orgName type="department" key="dep2">Yong Loo Ling School of Medicine</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Yong Loo Ling School of Medicine</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Duke-NUS Medical School</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chingyu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ocular Epidemiology and Data Sciences</orgName>
								<orgName type="institution">Eye Research Institute</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Centre for Innovation and Precision Eye Health</orgName>
								<orgName type="department" key="dep2">Yong Loo Ling School of Medicine</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Yong Loo Ling School of Medicine</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Duke-NUS Medical School</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rick</forename><surname>Siow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mong</forename><surname>Goh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way</addrLine>
									<postCode>16-16, 138632</postCode>
									<settlement>Connexis</settlement>
									<country>Singapore, Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Category-Independent Visual Explanation for Medical Deep Network Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78CD57620B1ACBBFA3F324E9E609CADF</idno>
					<idno type="DOI">10.1007/978-3-031-43895-017.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual explanations have the potential to improve our understanding of deep learning models and their decision-making process, which is critical for building transparent, reliable, and trustworthy AI systems. However, existing visualization methods have limitations, including their reliance on categorical labels to identify regions of interest, which may be inaccessible during model deployment and lead to incorrect diagnoses if an incorrect label is provided. To address this issue, we propose a novel category-independent visual explanation method called Hessian-CIAM. Our algorithm uses the Hessian matrix, which is the second-order derivative of the activation function, to weigh the activation weight in the last convolutional layer and generate a region of interest heatmap at inference time. We then apply an SVD-based post-process to create a smoothed version of the heatmap. By doing so, our algorithm eliminates the need for categorical labels and modifications to the deep learning model. To evaluate the effectiveness of our proposed method, we compared it to seven state-of-the-art algorithms using the Chestx-ray8 dataset. Our approach achieved a 55% higher IoU measurement than classical GradCAM and a 17% higher IoU measurement than EigenCAM. Moreover, our algorithm obtained a Judd AUC score of 0.70 on the glaucoma retinal image database, demonstrating its potential applicability in various medical applications. In summary, our category-independent visual explanation method, Hessian-CIAM, generates high-quality region of interest heatmaps that are not dependent on</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>categorical labels, making it a promising tool for improving our understanding of deep learning models and their decision-making process, particularly in medical applications. Three different categorical labels lead GradCAM to generate different distinguishable heatmaps. By contrast, our Hessian-CIAM generates a stable ROI without the categorical label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical application is a field that has high requirements of model reliability, trustworthiness, and interpretability. According to the act proposed by the European Commission on AI system regulation <ref type="bibr" target="#b3">[4]</ref>, medical AI systems are categorized as high-risk systems. Five sets of requirements are listed: <ref type="bibr" target="#b0">(1)</ref> high quality of data, (2) traceability, (3) transparency, (4) human oversight, (5) robustness, accuracy, and cybersecurity. These requirements impose a potential challenge for deep learning models where such a model is often used as a black-box system. To increase a model's explainability, many visualization methods are proposed to generate the region of interest (ROI) heatmap based on the output of the deep learning model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. This ROI heatmap highlights the region that deep learning algorithms focus on. This region often contains cues for researchers to investigate the algorithm's decision making process which would help doctors to gain confidence in the AI assisted products. For example, when doctors see a model make a correct prediction and at the same time highlight the right ROI, then it would help this model to gain more trust from doctors.</p><p>The state-of-art algorithms mostly focus on providing visualization during training where the categorical label is available. It becomes problematic at product deployment stage when no label is available. Without supplying the ground truth categorical labels, the false categorical labels would mislead the visualization algorithm to highlight wrong regions for cues. A sample is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. GradCAM <ref type="bibr" target="#b22">[23]</ref> visualization is used widely on a deep learning network that classifies multiple diseases. Three different categorical labels are supplied (Fig. <ref type="figure" target="#fig_0">1 (bd</ref>)) which leads GradCAM to generate three distinguishable ROI heatmaps. To address this issue, we propose a method called Hessian-Category Independent Activation Maps (Hessian-CIAM), which utilizes the Hessian matrix as an activation weighting function to eliminate the need for categorical labels to compute the ROI heatmap. Then a polarity checking process is added to the post process which corrects the polarity error from the SVD based smoothing function. Figure <ref type="figure" target="#fig_0">1</ref> (e) shows the visualization from our category-independent method. We benchmark our algorithm against seven state-of-art algorithms on the Chestx-ray8 dataset which demonstrated the superior performance of our algorithm. Additionally, we demonstrate a clinical use case in glaucoma detection from retinal images which shows the flexibility of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The visual explanation for deep networks is an essential task for researchers to interpret and debug deep networks where an ROI heat map is one of the most popular tools. This field is pioneered by Oquab et al. <ref type="bibr" target="#b17">[18]</ref> which additional Global Max Pooling (GMP) layers are added to extract the attention region from a trained convolutional network. It is later improved by CAM <ref type="bibr" target="#b26">[27]</ref> by attaching a Global Average Pooling (GAP) layer to the existing model. The GAP identifies the extent of the object while GMP only finds one discriminative part. One drawback of Oquab's method and CAM is the requirement of modifying the original network to output visualizations. This requirement is eliminated by a gradientbased approach GradCam <ref type="bibr" target="#b22">[23]</ref>. In this algorithm, the activation weights from the last convolutional layer of the deep network are extracted and weighed by a gradient from the back-propagation to generate the ROI heat map. This method is later improved by GradCAM++ <ref type="bibr" target="#b2">[3]</ref> and LayerCAM <ref type="bibr" target="#b11">[12]</ref>. An alternative way to generate an ROI heatmap is perturbation-based methods. It removes the requirement of the gradient calculation by iteratively perturbating different parts of the activations weight <ref type="bibr" target="#b21">[22]</ref> or image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref> to identify the region on the image that has the highest impact on the prediction result. One major drawback of such an approach is its speed as it requires iteratively running the deep learning model. The gradient-based and perturbation-based methods deliver high-quality ROI heatmap when a categorical label is supplied. It is a useful visualization tool to help researchers interpret the deep network during the development stage. It becomes a different story when it comes to deployment. During the deployment, there is no such luxury of having a ground truth categorical label that is supplied to the visualization algorithm. One solution to relax this problem is using the prediction result as a target label, but this solution often generates a wrong visualization as when the deep learning algorithm outputs incorrect prediction. Muhammad <ref type="bibr" target="#b16">[17]</ref> proposed a method to eliminate the dependence on the ground truth categorical label by directly applying SVD on the 2D activations and using its first principle component as the ROI heat map. The first principle component's polarity is bi-directional which could potentially highlight the non-interest region instead. Visualization techniques such as slot attention <ref type="bibr" target="#b14">[15]</ref>, SCOUTER <ref type="bibr" target="#b12">[13]</ref>, and SHAP <ref type="bibr" target="#b15">[16]</ref> require modification on the original network and training to generate an ROI heatmap. It is not the main scope of our paper and we will not further discuss it here.</p><p>Medical applications have high requirements for model reliability, trustworthiness, and interpretability. The visualization tools such as GradCAM and Fig. <ref type="figure">2</ref>. Overview of our algorithm, the Hessian matrix, and activation weight from the last convolutional layer is used to create an ROI heatmap followed by a post process.</p><p>GradCAM++ are widely applied to medical applications such as retina imaging <ref type="bibr" target="#b20">[21]</ref>, X-ray <ref type="bibr" target="#b9">[10]</ref>, CT <ref type="bibr" target="#b5">[6]</ref>, MRI <ref type="bibr" target="#b25">[26]</ref>, and ultrasound <ref type="bibr" target="#b10">[11]</ref>. However, those visualization algorithms require categorical labels to generate visual explanations. This requirement limits the usage of algorithms to the training stage where the ground truth category label is available. Generating high quality visual explanations without relying on the category label at the deployment stage remains a challenge. In this work, we propose a category-independent visual explanation method to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our algorithm generates an ROI heatmap to indicate the region on the image that the deep learning algorithms focus on when making classification decisions. Our method does not require any modification or additional training on target deep networks. The overview flow of our algorithm is illustrated in Fig. <ref type="figure">2</ref>. Input images feed into the deep network where the activation weights from the last convolution layer are weighted by the Hessian matrix followed by a post-process to output a clean ROI heatmap.</p><p>It is well known that the Hessian matrix appears in the expansion of gradient about a point in parameter space <ref type="bibr" target="#b18">[19]</ref>, as:</p><formula xml:id="formula_0">ω (ω + Δω) = ω (ω) + HΔω + O( Δω 2 ),<label>(1)</label></formula><p>where ω is a point in parameter space, Δω is a perturbation of ω, Δω is the gradient and H is the hessian matrix. In order to approximate the Hessian matrix H, we let Δω = rv, where v is the identity matrix, and r is a small number which leads the O(r) term to become insignificant. So we can further simplify the equation into:</p><formula xml:id="formula_1">Hv = ω (ω + rv) -ω (ω) r + O(r) = ω (ω + rv) -ω (ω) r . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Our goal is to apply the Hessian matrix as a weighting function to indicate the significance of each activation function output in the CNN. So we applied an L2 normalization on the Hv, here v is an identity matrix, so we can get the normalized Hessian matrix Ĥ = |Hv| Hv 2</p><p>. In the CNN we denote A k as the feature activation map from the kth convolution layer. Ĥk denotes the normalized Hessian matrix in the kth layer. We calculate the Hadamard product between Ĥk and A k , then apply ReLU to obtain the new activation map. n is the depth of the activation map. The ROI heatmap L H = n k=1 ReLu( Ĥk A k ). The ROI heatmap L H can be noisy, we follow Muhammad's approach <ref type="bibr" target="#b16">[17]</ref> to smooth out the L H which applies SVD on A k H = ReLu( Ĥk A k ) = UΣV T where U denotes a M × M matrix. Σ denotes a diagonal matrix with size of M × N . V denotes a N × N matrix. The column of U and V are the left singular vectors. The V 1 denotes the first component in V which is a weight function to create a smoothed ROI heatmap</p><formula xml:id="formula_3">L HS = A k H V 1 .</formula><p>One drawback of Muhammad's approach <ref type="bibr" target="#b16">[17]</ref> is the polarity of V 1 is not considered as the Eigenvectors from SVD are bidirectional. It could lead the algorithm to output non-ROI regions. To solve this problem, we revise the algorithm to calculate the correlation between the smoothed version L HS and the original ROI heatmap L H . If the correlation appears negative, we will reverse the ROI heatmap, as:</p><formula xml:id="formula_4">L HS = ReLu(A k H V 1 ), if corr(A k H V 1 , L H ) &gt; 0, ReLu(-A k H V 1 ), otherwise.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We conduct experiments on lung disease classification Chestx-ray8 <ref type="bibr" target="#b24">[25]</ref> to evaluate the performance of our algorithm. The Chestx-ray8 dataset contains 100,000</p><p>x-ray images with 19 disease labels. It is a significantly imbalanced dataset with some categories having as few as 7 images. To demonstrate our visualization techniques, we simplified the dataset by selecting 6 diseases with a higher number of images. After the selection, our training set contains images from atelectasis (3135 images), effusion (2875 images), infiltration (6941 images), mass (1665 images), nodule (2036 images), and pneumothorax (1485 images). Additionally, we randomly selected 7000 images from healthy people. 20% of images in the training set were set aside as validation sets for parameter tuning. This dataset contains 881 test images with bounding boxes that indicate the location of the diseases which 644 images were in the 6 diseases we selected. We utilize the pre-trained ResNet50 <ref type="bibr" target="#b7">[8]</ref> as the backbone. The cross-entropy loss is used as a loss function; the learning rate is set to 0.00001; the batch size is 64. The training cycle is set to 100 epochs. Our workstation is equipped with 2 Nvidia 3090 GPU (24 GB RAM), Intel Xeon CPU (3.30 GHz), and 128 GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>The algorithm is evaluated following the method proposed by Cao et al. <ref type="bibr" target="#b1">[2]</ref>. The union of intersection (IoU) between the bounding box and ROI is measured. The  foreground of ROI is extracted based on applying thresholds to find the area that covers 95%, 90%, 85%, 80%, and 75% of energy from the heatmap. The gradient and perturbation-based methods require ground truth labels to generate an ROI heatmap but, at the inference time, the ground truth label is not available.</p><p>To simulate the deployment scenario, we conduct two sets of evaluations. In the first evaluation, the prediction results (our ResNet model delivers 42.6% prediction accuracy) from the deep learning model are used as a label feed into the visualization methods. One drawback of this approach is the prediction result is not always reliable and the incorrect prediction could mislead the algorithm to output the wrong ROI. As a comparison, in the second evaluation, we supply ground truth labels to visualization methods. The quantitative evaluation of different visualization methods is shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Three groups of visualization algorithms are evaluated in our experiment. The gradient based group contains the algorithm that relies on the gradient from the label to generate the ROI. In this group, GradCAM achieved the highest at 0.175 IoU at the 75% threshold. The pertubation based group makes small perturbations in the input image or activation weight to find the ROI that has the highest impact. In this group, the ScoreCAM achieved the highest 0.168 IoU at the 75% threshold. The category independent group contains algorithms that do not require a label to generate ROI. Our method scored the highest IoU at 0.271 IoU at the 75% threshold. When ground truth labels are supplied, the IoU for gradient based methods is improved in the range of 5% to 20%. For perturbation based methods, supplying ground truth data reduced the performance of AblationCAM and had minimal impact on RISE.</p><p>Next, we split the test set into two categories which are samples with wrong and correct predictions (shown in Fig. <ref type="figure" target="#fig_1">3</ref> (left)). The 75% threshold is used to calculate IoU. The samples with correct prediction consistently scored higher IoU across all visualization methods. Our method shows the highest performance in both wrong and correct prediction categories. The perturbation based methods consistently scored lower than other methods indicating this group is not suitable for X-ray image classification applications.</p><p>To further investigate the efficiency of our algorithm, we extract the IoU on each disease (Fig. <ref type="figure" target="#fig_1">3</ref> (right)) where a 75% threshold is applied to calculate the mean IoU. The evaluation shows our algorithm is positively correlated with the size of the ground truth bounding box. It indicates the disease with a larger infection area is easier to visualize by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>Sample images comparing our algorithm with five state-of-art algorithms are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Our algorithm has a cleaner heatmap. The gradient methods generate a heatmap that contains a higher level of noise that covers a large area of the non-lung regions such as the shoulder. The perturbation based methods deliver the worst visualization in our evaluation. The AlbationCAM and ScoreCAM are only able to highlight the whole lung area but it does not provide any clinical value to pinpoint the disease locations. The RISE <ref type="bibr" target="#b19">[20]</ref> method delivers multiple clusters of highlight regions that are not feasible to provide human-readable information. The last row of Fig. <ref type="figure" target="#fig_2">4</ref> shows the worst case in our evaluation which is a representative case to illustrate the failure mode of our algorithm. The deep learning algorithm may fail to detect the small size lesions which leads to the wrong ROI for visualization methods. More comparison is available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Clinical Application</head><p>Our algorithm has the potential to apply to many clinical applications. We conducted an additional experiment on the glaucoma retinal image database <ref type="bibr" target="#b13">[14]</ref>   with 3,144 negative and 1,712 positive glaucoma samples<ref type="foot" target="#foot_0">1</ref> Each sample contains a saliency map annotated by ophthalmologists by using mouse clicks to simulate the human visual attention process. Since our goal is to evaluate the explainability of our visualization algorithm, we decided to use all images to train the glaucoma classification model. We follow the work from Bylinskii et al. <ref type="bibr" target="#b0">[1]</ref> to apply similarity (histogram intersection), cross-correlation, and Judd AUC to measure the performance of our algorithm. The 95% energy of the ROI heatmap was used as a threshold to clean our heatmap. Our algorithm achieved 0.618 ± 0.0024 in similarity, 0.755 ± 0.0033 in cross-correlation, and 0.703 ± 0.0013 in Judd AUC. The complete evaluation is available in the supplementary material (Fig. <ref type="figure" target="#fig_3">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we propose a novel category-independent deep learning visualization algorithm that does not rely on categorical labels to generate visualizations. Our evaluation demonstrates that our algorithm outperforms seven state-of-theart algorithms by a significant margin on a multi-disease classification task using X-ray images. This indicates that our algorithm has the potential to enhance model explainability and facilitate its deployment in medical applications. Additionally, we demonstrate the flexibility of our algorithm by showing a clinical use case on retinal image glaucoma detection. Overall, our proposed Hessian-CIAM algorithm represents a promising tool for improving our understanding of deep learning models and enhancing their interpretability, particularly in medical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of GradCAM (b-d) supplied different labels vs. our method (e). Three different categorical labels lead GradCAM to generate different distinguishable heatmaps. By contrast, our Hessian-CIAM generates a stable ROI without the categorical label.</figDesc><graphic coords="2,56,31,109,46,311,92,76,51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (left) IoU on samples with a wrong and correct prediction on methods. (right) IoU for our method on different diseases in the bar chart (left y-axis) and the ground truth bounding box size (dashed line, right y-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of visualization methods on Chestx-ray8 dataset. The ground truth bounding box drawn by clinicians overlays on the heatmaps.</figDesc><graphic coords="8,43,29,53,81,337,45,178,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Five samples of the original image (row 1), ground truth saliency map (row 2), and heatmap from our method (row 3) are shown.</figDesc><graphic coords="8,72,30,278,30,279,10,155,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of visualization methods on Chestx-ray8 dataset. The IoU using prediction as the label is shown here. The value in the bracket is the IoU that uses ground truth as the label.</figDesc><table><row><cell></cell><cell cols="2">IoU on different thresholds</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell>0.90</cell><cell>0.85</cell><cell>0.80</cell><cell>0.75</cell></row><row><cell cols="2">Gradient based approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GradCAM [23]</cell><cell cols="5">0.127 (0.136) 0.139 (0.151) 0.150 (0.163) 0.159 (0.175) 0.175 (0.185)</cell></row><row><cell cols="6">GradCAM++ [3] 0.103 (0.108) 0.117 (0.124) 0.131 (0.139) 0.144 (0.154) 0.155 (0.167)</cell></row><row><cell>HiResCAM [5]</cell><cell cols="5">0.104 (0.120) 0.112 (0.133) 0.118 (0.143) 0.124 (0.153) 0.129 (0.162)</cell></row><row><cell cols="2">Perturbation based approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">AblationCAM [22] 0.092 (0.090) 0.097 (0.094) 0.102 (0.098) 0.107 (0.103) 0.113 (0.109)</cell></row><row><cell>ScoreCAM [24]</cell><cell cols="5">0.134 (N/A) 0.141 (N/A) 0.149 (N/A) 0.158 (N/A) 0.168 (N/A)</cell></row><row><cell>RISE [20]</cell><cell cols="5">0.095 (0.097) 0.096 (0.096) 0.097 (0.097) 0.098 (0.098) 0.099 (0.099)</cell></row><row><cell cols="2">Category-independent approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EigenCAM [17]</cell><cell>0.213</cell><cell>0.222</cell><cell>0.227</cell><cell>0.231</cell><cell>0.232</cell></row><row><cell>Ours</cell><cell>0.240</cell><cell>0.253</cell><cell>0.262</cell><cell>0.267</cell><cell>0.271</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>the dataset is obtained from https://github.com/smilell/AG-CNN.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">Agency for Science, Technology and Research (A*STAR)</rs> under its <rs type="funder">RIE2020 Health and Biomedical Sciences (HBMS) Industry Alignment Fund Pre-Positioning (IAF-PP)</rs> Grant No. <rs type="grantNumber">H20c6a0031</rs>, the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), the <rs type="funder">Agency for Science, Technology and Research (A*STAR)</rs> through its <rs type="programName">AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, <rs type="funder">A*STAR Central Research Fund "A Secure and Privacy Preserving AI Platform for Digital Health</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NDRBHAc">
					<idno type="grant-number">H20c6a0031</idno>
				</org>
				<org type="funding" xml:id="_Bufz7qq">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_4wrgqZN">
					<orgName type="program" subtype="full">AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03605</idno>
		<title level="m">What do different evaluation metrics tell us about saliency models? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Look and think twice: capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2956" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grad-CAM++: generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Proposal for a regulation of the European parliament and of the council</title>
		<author>
			<persName><forename type="first">E</forename><surname>Commission</surname></persName>
		</author>
		<ptr target="https://artificialintelligenceact.eu/the-act/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">HiResCAM: faithful location representation in visual attention for explainable 3D medical image classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Draelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explainable multiple abnormality classification of chest CT volumes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Draelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">102372</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="27" to="48" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>recent Developments on Deep Big Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: an interrogative survey for the next frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2843369</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2018.2843369" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Detecting a fetus in ultrasound images using Grad-CAM and locating the fetus in the uterus</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iwata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
		<respStmt>
			<orgName>ICPRAM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Layercam: exploring hierarchical class activation maps for localization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5875" to="5888" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SCOUTER: slot attention-based classifier for explainable image recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kawasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1046" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention based glaucoma detection: a large-scale database and CNN model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11525" to="11538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eigen-CAM: class activation map using principal components</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Is object localization for free? weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the hessian</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">RISE: randomized input sampling for explanation of black-box models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07421</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="158" to="164" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ablation-CAM: visual explanations for deep convolutional network via gradient-free localization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="983" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Score-CAM: score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual explanations from deep 3D convolutional neural networks for Alzheimer&apos;s disease classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">1571</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
