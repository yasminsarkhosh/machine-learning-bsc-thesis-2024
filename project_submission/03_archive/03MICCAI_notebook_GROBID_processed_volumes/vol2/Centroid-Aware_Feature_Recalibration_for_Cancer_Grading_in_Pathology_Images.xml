<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images</title>
				<funder ref="#_sCuYE4K #_yaRpzs3">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jaeung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keunho</forename><surname>Byeon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jin</forename><forename type="middle">Tae</forename><surname>Kwak</surname></persName>
							<email>jkwak@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="212" to="221"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">EFBB1673E628F598423F44146E8DBB05</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cancer grading</term>
					<term>attention</term>
					<term>feature calibration</term>
					<term>pathology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cancer grading is an essential task in pathology. The recent developments of artificial neural networks in computational pathology have shown that these methods hold great potential for improving the accuracy and quality of cancer diagnosis. However, the issues with the robustness and reliability of such methods have not been fully resolved yet. Herein, we propose a centroid-aware feature recalibration network that can conduct cancer grading in an accurate and robust manner. The proposed network maps an input pathology image into an embedding space and adjusts it by using centroids embedding vectors of different cancer grades via attention mechanism. Equipped with the recalibrated embedding vector, the proposed network classifiers the input pathology image into a pertinent class label, i.e., cancer grade. We evaluate the proposed network using colorectal cancer datasets that were collected under different environments. The experimental results confirm that the proposed network is able to conduct cancer grading in pathology images with high accuracy regardless of the environmental changes in the datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Globally, cancer is a leading cause of death and the burden of cancer incidence and mortality is rapidly growing <ref type="bibr" target="#b0">[1]</ref>. In cancer diagnosis, treatment, and management, pathologydriven information plays a pivotal role. Cancer grade is, in particular, one of the major factors that determine the treatment options and life expectancy. However, the current pathology workflow is sub-optimal and low-throughput since it is, by and large, manually conducted, and the large volume of workloads can result in dysfunction or errors in cancer grading, which have an adversarial effect on patient care and safety <ref type="bibr" target="#b1">[2]</ref>. Therefore, there is a high demand to automate and expedite the current pathology workflow and to improve the overall accuracy and robustness of cancer grading.</p><p>Recently, many computational tools have shown to be effective in analyzing pathology images <ref type="bibr" target="#b2">[3]</ref>. These are mainly built based upon deep convolutional neural networks (DCNNs). For instance, <ref type="bibr" target="#b3">[4]</ref> used DCCNs for prostate cancer detection and grading, <ref type="bibr" target="#b4">[5]</ref> classified gliomas into three different cancer grades, and <ref type="bibr" target="#b5">[6]</ref> utilized an ensemble of DCNNs for breast cancer classification. To further improve the efficiency and effectiveness of DCNNs in pathology image analysis, advanced methods that are tailored to pathology images have been proposed. For example, <ref type="bibr" target="#b6">[7]</ref> proposed to incorporate both local and global contexts through the aggregation learning of multiple context blocks for colorectal cancer classification; <ref type="bibr" target="#b7">[8]</ref> extracted and utilized multi-scale patterns for cancer grading in prostate and colorectal tissues; <ref type="bibr" target="#b8">[9]</ref> proposed to re-formulate cancer classification in pathology images as both categorical and ordinal classification problems. Built based upon a shared feature extractor, a categorical classification branch, and an ordinal classification branch, it simultaneously conducts both categorical and ordinal learning for colorectal and prostate cancer grading; a hybrid method that combines DCCNs with hand-crafted features was developed for mitosis detection in breast cancer <ref type="bibr" target="#b9">[10]</ref>. Moreover, attention mechanisms have been utilized for an improved pathology image analysis. For instance, <ref type="bibr" target="#b10">[11]</ref> proposed a two-step framework for glioma sub-type classification in the brain, which consists of a contrastive learning framework for robust feature extractor training and a sparse-attention block for meaningful multiple instance feature aggregation. Such attention mechanisms have been usually utilized in a multiple instance learning framework or as self-attention for feature representations. To the best of our knowledge, attention mechanisms have not been used for feature representations of class centroids.</p><p>In this study, we propose a centroid-aware feature recalibration network (CaFeNet) for accurate and robust cancer grading in pathology images. CaFeNet is built based upon three major components: 1) a feature extractor, 2) a centroid update (Cup) module, and 3) a centroid-aware feature recalibration (CaFe) module. The feature extractor is utilized to obtain the feature representation of pathology images. Cup module obtains and updates the centroids of class labels, i.e., cancer grades. CaFe module adjusts the input embedding vectors with respect to the class centroids (i.e., training data distribution). Assuming that the classes are well separated in the feature space, the centroid embedding vectors can serve as reference points to represent the data distribution of the training data. This indicates that the centroid embedding vectors can be used to recalibrate the input embedding vectors of pathology images. During inference, we fix the centroid embedding vectors so that the recalibrated embedding vectors do not vary much compared to the input embedding vectors even though the data distribution substantially changes, leading to improved stability and robustness of the feature representation. In this manner, the feature representations of the input pathology images are re-calibrated and stabilized for a reliable cancer classification. The experimental results demonstrate that CaFeNet achieves the state-of-the-art cancer grading performance in colorectal cancer grading datasets. The source code of CaFeNet is available at https://github.com/col in19950703/CaFeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overview of the proposed CaFeNet is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. CaFeNet employs a deep convolutional neural network as a feature extractor and an attention mechanism to produce robust feature representations of pathology images and conducts cancer grading with high accuracy. Algorithm 1 depicts the detailed algorithm of CaFeNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Centroid-Aware Feature Recalibration</head><p>Let {x i , y i } N i=1 be a set of pairs of pathology images and ground truth labels where N is the number of pathology image-ground truth label pairs, x i ∈ R h×w×c is the i th pathology image, y i ∈ {C 1 , . . . , C M } represents the corresponding ground truth label. h, w, and c denote the height, width, and the number of channels, respectively. M is the cardinality of the class labels. Given x i , a deep neural network f maps x i into an embedding space, producing an embedding vector e i ∈ R d . The embedding vector e i is fed into 1) a centroid update (Cup) module and 2) a centroid-aware feature recalibration (CaFe) module. Cup module obtains and updates the centroid of the class label in the embedding space E C ∈ R M ×D . CaFe module adjusts the embedding vector e i in regard to the embedding vectors of the class centroids and produces a recalibrated embedding vector e R i . e i and e R i are concatenated together and is fed into a classification layer to conduct cancer grading. </p><formula xml:id="formula_0">Q E ∈ R N ×D from E and keys K C ∈ R M ×D and values V C ∈ R M ×D</formula><p>from the centroid embedding vectors E C by using a linear layer. Then, attention scores are computed via a dot product between Q E and K C followed by a softmax operation. Multiplying the attention scores by V C , we obtain the recalibrated feature representation E R for the input embedding vectors E. The process can be formulated as follows:</p><formula xml:id="formula_1">E R = softmax Q E K CT V C . (1)</formula><p>Finally, CaFe concatenates E and E R and produces them as the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>We employ EfficientNet-B0 <ref type="bibr" target="#b11">[12]</ref> as a backbone network. EfficientNet is designed to achieve the state-of-the-art accuracy on computer vision tasks while minimizing computational costs through a compound scaling method. EfficientNet-B0 is composed of one convolution layer and 16 stages of mobile inverted bottleneck blocks, of which each with a different number of layers and channels. Each mobile inverted bottleneck block comprises one pointwise convolution (1 × 1 convolution for the channel expansion), one depth-wise separable convolution with a kernel size of 3 or 5, and one project pointwise convolution (1 × 1 convolution for the channel reduction). 3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Two publicly available colorectal cancer datasets <ref type="bibr" target="#b8">[9]</ref> were employed to evaluate the effectiveness of the proposed CaFeNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparative Experiments</head><p>We conducted a series of comparative experiments to evaluate the effectiveness of CaFeNet for cancer grading, in comparison to several existing methods: 1) three DCNNbased models: ResNet <ref type="bibr" target="#b12">[13]</ref>, DenseNet <ref type="bibr" target="#b13">[14]</ref>, EfficientNet <ref type="bibr" target="#b11">[12]</ref>, 2) two metric learningbased models: triplet loss (Triplet) <ref type="bibr" target="#b14">[15]</ref> and supervised contrastive loss (SC) <ref type="bibr" target="#b15">[16]</ref>, 3) two transformer-based models: vision transformer (ViT) <ref type="bibr" target="#b16">[17]</ref> and swin transformer (Swin) <ref type="bibr" target="#b17">[18]</ref>, and 4) one (pathology) domain-specific model (M MAE-CE o ) <ref type="bibr" target="#b8">[9]</ref>, which demonstrates the state-of-the-art performance on the two colorectal cancer datasets under consideration. For Triplet and SC, EfficientNet was used as a backbone network. We trained CaFeNet and other competing networks on C Train and selected the best model usingC Validation . Then, the chosen model of each network was separately applied to C TestI andC TestII . The results of M MAE-CE o were obtained from the original literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>We initialized all models using the pre-trained weights on the ImageNet dataset, and then trained them using the Adam optimizer with default parameter values (β 1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs. We employed cosine anneal warm restart schedule with initial learning rates of 1.0 e -3 , η min = 1.0 e -3 , and T 0 = 20. After data augmentation, all patches, except for those used in ViT <ref type="bibr" target="#b16">[17]</ref> and Swin <ref type="bibr" target="#b17">[18]</ref> models, were resized to 512 × 512 pixels. For ViT and Swin, the patches were resized to 384 × 384 pixels. We implemented all models using the PyTorch platform and trained on a workstation equipped with two RTX 3090 GPUs. To increase the variability of the dataset during the training phase, we applied several data augmentation techniques, including affine transformation, random horizontal and vertical flip, image blurring, random Gaussian noise, dropout, random color saturation and contrast conversion, and random contrast transformations. All these techniques were implemented using the Aleju library (https:// github.com/aleju/imgaug). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Result and Discussions</head><p>We evaluated the performance of colorectal cancer grading by the proposed CaFeNet and other competing models using five evaluation metrics, including accuracy (Acc), precision, recall, F1-score (F1), and quadratic weighted kappa (κ w ). Table <ref type="table" target="#tab_2">2</ref> demonstrates the quantitative experimental results on C TestI . The results show that CaFeNet was one of the best performing models along with ResNet, Swin, and M MAE-CE o .</p><p>Were the best performing models. Among DCNN-based models, ResNet was superior to other DCNN-based models. Metric learning was able to improve the classification performance. EffcientNet was the worst model among them, but with the help of triplet loss (Triplet) or supervised contrastive loss (SC), the overall performance increased by ≥2.8% Acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 F1, and ≥0.047 κ w . Among the transformer-based models, Swin was one of the best performing models, but ViT showed much lower performance in all evaluation metrics. Moreover, we applied the same models to C TestII to test the generalizability of the models. We note that C TestI originated from the same set with C Train and C Validation and C TestII was obtained from different time periods and using a different slide scanner. Table <ref type="table" target="#tab_3">3</ref> depicts the quantitative classification results on C TestII . CaFeNet outperformed other competing models in all evaluation metrics except Triplet for recall. In a headto-head comparison of the classification results between C TestI and C TestII , there was a consistent performance drop in the proposed CaFeNet and other competing models. This is ascribable to the difference between the test datasets (C TestI and C TestII ) and the training and validation datasets (C Train and C Validation ). In regard to such differences, it is striking that the proposed CaFeNet achieved the best performance on C TestII . CaFeNet, ResNet, Swin, and M MAE-CE o were the four best performing models on C TestI . However, ResNet, Swin, and M MAE-CE o showed a higher performance drop in all evaluation metrics. CaFeNet had a minimal performance drop except EfficientNet. EfficientNet, however, obtained poorer performance on both C TestI and C TestII . These results suggest that CaFeNet has the better generalizability so as to well adapt to unseen histopathology image data.</p><p>We conducted ablation experiments to investigate the effect of the CaFe module on cancer classification. The results are presented in Table <ref type="table" target="#tab_4">4</ref>. The exclusion of the CaFe module, i.e., EfficientNet, resulted in much worse performance than CaFeNet. Using only the recalibrated embedding vectors E R , a substantial drop in performance was observed. These two results indicate that the recalibrated embedding vectors complement to the input embedding vectors E. Moreover, we examined the effect of the method that merges the two embedding vectors. Using addition, instead of concatenation, there was a consistent performance drop, indicating that concatenation is the superior approach for combining the two embedding vectors together. In addition, we compared the model complexity of the proposed CaFeNet and other competing models. Table <ref type="table" target="#tab_5">5</ref> demonstrates the number of parameters, floating point operations per second (FLOPs), and training and inference time (in milliseconds). The proposed CaFeNet was one of the models that require a relatively small number of parameters and FLOPs and a short amount of time during training and inference. DenseNet, EfficientNet, Triplet, SC, and M MAE-CE o contain the smaller number of parameters than that of CaFeNet, but these models show either the higher number of FLOPs or longer time during training and/or inference. Similar observations were made for ResNet, ViT, and Swin. These models require much larger number of parameters and FLOPs and longer training time. These results confirm that the proposed CaFeNet is computational efficient and it does not achieve its superior learning capability and generalizability at the expense of the model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Herein, we propose an attention mechanism-based deep neural network, called CaFeNet, for cancer classification in pathology images. The proposed approach proposes to improve the feature representation of deep neural networks by re-calibrating input embedding vectors via an attention mechanism in regard to the centroids of cancer grades. In the experiments on colorectal cancer datasets against several competing models, the proposed network demonstrated that it has a better learning capability as well as a generalizability in classifying pathology images into different cancer grades. However, the experiments were only conducted on two public colorectal cancer datasets from a single institute. Additional experiments need to be conducted to further verify the findings of our study. Therefore, future work will focus on validating the effectiveness of the proposed network for other types of cancers and tissues in pathology images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of CaFeNet. CaFeNet consists of a feature extractor, a CaFe module, a Cup module, and a classification layer.</figDesc><graphic coords="3,41,79,56,69,340,18,164,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Given a batch of input embedding vectors E = {e i |i = 0, . . . , N -1}, Cup module computes and updates the centroid embedding vector of each class label per epoch. Specifically, Cup module adds up the embedding vectors of different class labels over the iterations per epoch, computes the average embedding vectors, and updates the centroid embedding vectors E C = e C j |j = 0, . . . , M -1 . CaFe module receives a batch of embedding vectors E = {e i |i = 0, . . . , N -1} and the ground truth labels Y = {y i |i = 0, . . . , N -1} and a set of centroid embedding vectors E C = e C j |j = 0, . . . , M -1 and outputs a batch of recalibrated embedding vectors E R = e R i |i = 0, . . . , N -1 via an attention mechanism (Fig. 1). It first produces queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of colorectal cancer datasets</figDesc><table><row><cell>Class</cell><cell>C Train</cell><cell>C Validation</cell><cell>C TestI</cell><cell>C TestII</cell></row><row><cell>Benign</cell><cell>773</cell><cell>374</cell><cell>453</cell><cell>27896</cell></row><row><cell>WD</cell><cell>1866</cell><cell>264</cell><cell>192</cell><cell>8394</cell></row><row><cell>MD</cell><cell>2997</cell><cell>370</cell><cell>738</cell><cell>61985</cell></row><row><cell>PD</cell><cell>1391</cell><cell>234</cell><cell>205</cell><cell>11895</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The first dataset includes 1600 BN, 2322 WD, 4105 MD, and 1830 PD image patches that were collected between 2006 and 2008 using an Aperio digital slide scanner (Leica Biosystems) at 40x magnification. Each image patch has a spatial size of 1024 × 1024 pixels. This dataset is divided into a training dataset (C Train ), validation dataset (C Validation ), and a test dataset (C TestI ). The second dataset, designated as C TestII , contains 27986 BN, 8394 WD, 61985 MD, and 11985 PD image patches of size 1144 × 1144 pixels.</figDesc><table><row><cell>These were</cell></row></table><note><p>shows the details of the datasets. Both datasets provide colorectal pathology images with ground truth labels for cancer grading. The ground labels are benign (BN), well-differentiated (WD) cancer, moderatelydifferentiated (MD) cancer, and poorly-differentiated (PD) cancer.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Result of colorectal cancer grading on C TestI .</figDesc><table><row><cell>Model</cell><cell>Acc (%)</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>κ w</cell></row><row><cell>ResNet [13]</cell><cell>87.1</cell><cell>0.834</cell><cell>0.843</cell><cell>0.838</cell><cell>0.938</cell></row><row><cell>DenseNet [14]</cell><cell>86.2</cell><cell>0.823</cell><cell>0.839</cell><cell>0.829</cell><cell>0.929</cell></row><row><cell>EfficientNet[12]</cell><cell>82.2</cell><cell>0.794</cell><cell>0.811</cell><cell>0.802</cell><cell>0.873</cell></row><row><cell>Triplet [15]</cell><cell>86.6</cell><cell>0.832</cell><cell>0.824</cell><cell>0.827</cell><cell>0.937</cell></row><row><cell>SC [16]</cell><cell>85.0</cell><cell>0.817</cell><cell>0.812</cell><cell>0.812</cell><cell>0.920</cell></row><row><cell>ViT [17]</cell><cell>85.8</cell><cell>0.818</cell><cell>0.813</cell><cell>0.815</cell><cell>0.934</cell></row><row><cell>Swin [18]</cell><cell>87.4</cell><cell>0.847</cell><cell>0.820</cell><cell>0.832</cell><cell>0.941</cell></row><row><cell>M MAE-CE o [9]</cell><cell>87.7</cell><cell>-</cell><cell>-</cell><cell>0.843</cell><cell>0.940</cell></row><row><cell>CaFeNet (Ours)</cell><cell>87.5</cell><cell>0.853</cell><cell>0.816</cell><cell>0.832</cell><cell>0.940</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Result of colorectal cancer grading on C TestII .</figDesc><table><row><cell>Model</cell><cell>Acc (%)</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>κ w</cell></row><row><cell>ResNet [13]</cell><cell>77.2</cell><cell>0.691</cell><cell>0.800</cell><cell>0.713</cell><cell>0.869</cell></row><row><cell>DenseNet [14]</cell><cell>78.8</cell><cell>0.698</cell><cell>0.792</cell><cell>0.722</cell><cell>0.866</cell></row><row><cell>EfficientNet [12]</cell><cell>79.3</cell><cell>0.701</cell><cell>0.802</cell><cell>0.727</cell><cell>0.870</cell></row><row><cell>Triplet [15]</cell><cell>79.1</cell><cell>0.702</cell><cell>0.815</cell><cell>0.730</cell><cell>0.886</cell></row><row><cell>SC [16]</cell><cell>79.7</cell><cell>0.718</cell><cell>0.809</cell><cell>0.739</cell><cell>0.876</cell></row><row><cell>ViT [17]</cell><cell>80.7</cell><cell>0.706</cell><cell>0.797</cell><cell>0.733</cell><cell>0.889</cell></row><row><cell>Swin [18]</cell><cell>78.6</cell><cell>0.690</cell><cell>0.785</cell><cell>0.712</cell><cell>0.873</cell></row><row><cell>M MAE-CE o [9]</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>0.744</cell><cell>0.891</cell></row><row><cell>CaFeNet (Ours)</cell><cell>82.7</cell><cell>0.728</cell><cell>0.810</cell><cell>0.756</cell><cell>0.901</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on CaFeNet.</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell>Acc</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>κ w</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C TestI</cell><cell>Backbone (EfficientNet)</cell><cell>82.2</cell><cell>0.794</cell><cell>0.811</cell><cell>0.802</cell><cell>0.873</cell></row><row><cell></cell><cell>E R only</cell><cell>77.8</cell><cell>0.572</cell><cell>0.634</cell><cell>0.600</cell><cell>0.835</cell></row><row><cell></cell><cell>ADD E, E R</cell><cell>82.9</cell><cell>0.781</cell><cell>0.803</cell><cell>0.788</cell><cell>0.846</cell></row><row><cell></cell><cell>CONCAT E, E R (Ours)</cell><cell>87.5</cell><cell>0.853</cell><cell>0.816</cell><cell>0.832</cell><cell>0.940</cell></row><row><cell>C TestII</cell><cell>Backbone (EfficientNet)</cell><cell>79.3</cell><cell>0.701</cell><cell>0.802</cell><cell>0.727</cell><cell>0.870</cell></row><row><cell></cell><cell>E R only</cell><cell>56.2</cell><cell>0.399</cell><cell>0.428</cell><cell>0.296</cell><cell>-0.114</cell></row><row><cell></cell><cell>ADD E, E R</cell><cell>75.2</cell><cell>0.674</cell><cell>0.775</cell><cell>0.688</cell><cell>0.789</cell></row><row><cell></cell><cell>CONCAT E, E R (Ours)</cell><cell>82.7</cell><cell>0.728</cell><cell>0.810</cell><cell>0.756</cell><cell>0.901</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Model complexity of CaFeNet and competing models.</figDesc><table><row><cell>Model</cell><cell># Params (M)</cell><cell># FLOPs</cell><cell>Training</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>(M)</cell><cell>(ms/batch)</cell><cell>(ms/batch)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (No. <rs type="grantNumber">2021R1A2C2014557</rs> and No. <rs type="grantNumber">2021R1A4A1031864</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sCuYE4K">
					<idno type="grant-number">2021R1A2C2014557</idno>
				</org>
				<org type="funding" xml:id="_yaRpzs3">
					<idno type="grant-number">2021R1A4A1031864</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: Can</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pathologists&apos; workload and patient safety</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagn. Histopathol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="283" to="287" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated Gleason grading of prostate cancer tissue microarrays via deep learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arvaniti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12054</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated grading of gliomas using deep learning in digital pathology images: a modular approach with ensemble of convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Ertosun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">1899</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image classification using an ensemble of deep learning models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Garcia-Zapirain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Javier Aguirre</surname></persName>
		</author>
		<author>
			<persName><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vanegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">4373</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-aware convolutional neural network for grading of colorectal cancer histology images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2395" to="2405" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale binary pattern encoding network for cancer classification in pathology images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1152" to="1163" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint categorical and ordinal learning for cancer grading in pathology images</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102206</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">34003</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smile: sparse-attention based multiple instance contrastive learning for glioma sub-type classification using pathological images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">MICCAI Workshop on Computational Pathology</title>
		<imprint>
			<biblScope unit="page" from="159" to="169" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficientnet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FaceNet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Umap: uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
