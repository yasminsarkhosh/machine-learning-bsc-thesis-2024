<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases</title>
				<funder ref="#_23Kaasw">
					<orgName type="full">Major Key Project of PCL</orgName>
				</funder>
				<funder ref="#_P3FpDrT #_KWgPUhK">
					<orgName type="full">Guangdong Provincial Natural Science Fund</orgName>
				</funder>
				<funder ref="#_ZhdfyHX #_DMesnna">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="institution">MOE</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="institution">MOE</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Key Laboratory of Computational Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="institution">MOE</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruixuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="institution">MOE</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="68" to="78"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">929E0885D930166394FDBB943706F2EE</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Continual learning</term>
					<term>Adapter</term>
					<term>Disease diagnosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently intelligent diagnosis systems lack the ability of continually learning to diagnose new diseases once deployed, under the condition of preserving old disease knowledge. In particular, updating an intelligent diagnosis system with training data of new diseases would cause catastrophic forgetting of old disease knowledge. To address the catastrophic forgetting issue, an Adapter-based Continual Learning framework called ACL is proposed to help effectively learn a set of new diseases at each round (or task) of continual learning, without changing the shared feature extractor. The learnable lightweight task-specific adapter(s) can be flexibly designed (e.g., two convolutional layers) and then added to the pretrained and fixed feature extractor. Together with a specially designed task-specific head which absorbs all previously learned old diseases as a single 'out-of-distribution' category, task-specific adapter(s) can help the pretrained feature extractor more effectively extract discriminative features between diseases. In addition, a simple yet effective fine-tuning is applied to collaboratively fine-tune multiple task-specific heads such that outputs from different heads are comparable and consequently the appropriate classifier head can be more accurately selected during model inference. Extensive empirical evaluations on three image datasets demonstrate the superior performance of ACL in continual learning of new diseases. The source code is available at https:// github.com/GiantJun/CL Pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shown expert-level performance in various disease diagnoses <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref>. In practice, a deep neural network is often limited to the diagnosis of only a few diseases, partly because it is challenging to collect enough training data of all diseases even for a specific body tissue or organ. One possible solution is to enable a deployed intelligent diagnosis system to continually learn new diseases with collected new training data later. However, if old data are not accessible due to certain reasons (e.g., challenge in data sharing), current intelligent systems will suffer from catastrophic forgetting of old knowledge when learning new diseases <ref type="bibr" target="#b17">[17]</ref>.</p><p>Multiple approaches have been proposed to alleviate the catastrophic forgetting issue. One approach aims to determine part of the model parameters which are crucial to old knowledge and tries to keep these parameters unchanged during learning new knowledge <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18]</ref>. Another approach aims to preserve old knowledge by making the updated model imitate the behaviour (e.g., output at certain layer) of the old model particularly with the help of knowledge distillation technique <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b34">34]</ref>. Storing a small amount of old data or synthesizing old data relevant to old knowledge and using them together with training data of new knowledge can often help significantly alleviate forgetting of old knowledge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22]</ref>. Although the above approaches can help the updated model keep old knowledge to some extent, they often fall into the dilemma of model plasticity (for new knowledge learning) and stability (for old knowledge preservation). In order to resolve this dilemma, new model components (e.g., neurons or layers in neural networks) can be added specifically for learning new knowledge, while old parameters are largely kept unchanged for old knowledge <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b30">30]</ref>. While this approach has shown state-of-the-art continual learning performance, it faces the problem of rapid model expansion and effective fusion of new model components into the existing ones. To alleviate the model expansion issue and meanwhile well preserve old knowledge, researchers have started to explore the usage of a pretrained and fixed feature extractor for the whole process of continual learning <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref>, where the challenge is to discriminate between different classes of knowledge with limited learnable parameters.</p><p>In this study, inspired by recent advances in transfer learning in natural language processing <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b12">12]</ref>, we propose adding a light-weight learnable module called adapter to a pretrained and fixed convolutional neural network (CNN) for effective continual learning of new knowledge. For each round of continual learning, the CNN model will be updated to learn a set of new classes (hereinafter also called learning a new task). The learnable task-specific adapters are added between consecutive convolutional stages to help the pretrained CNN feature extractor more effectively extract discriminative features of new diseases. To the best of our knowledge, it is the first time to apply the idea of CNN adapter in the continual learning field. In addition, to keep extracted features discriminative between different tasks, a special task-specific classifier head is added when learning each new task, in which all previously learned old classes are considered as the 'out-of-distribution' (OOD) class and correspond to an additional output neuron in each task-specific classifier head. A simple yet effective finetuning strategy is applied to calibrate outputs between multiple task-specific heads. Extensive empirical evaluations on three image datasets show that the proposed method outperforms existing continual learning methods by a large margin, consistently supporting the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This study aims to improve continual learning performance of an intelligent diagnosis system. At each learning round, following previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3]</ref> which show that rehearsal with old samples can significantly improve continual learning performance, the system will be updated based on the training data of new diseases and preserved small subset for each previously learned disease. During inference, the system is expected to accurately diagnose all learned diseases, without knowing which round (i.e., task) the class of any test input is from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Framework</head><p>We propose an Adapter-based Continual Learning framework called ACL with a multi-head training strategy. With the motivation to make full use of readily available pretrained CNN models and slow down the speed of model expansion that appears in some state-of-the-art continual learning methods (e.g., DER <ref type="bibr" target="#b30">[30]</ref>), and inspired by a recently developed transfer learning strategy Delta tuning for downstream tasks in natural language process <ref type="bibr" target="#b7">[7]</ref>, we propose adding a learnable light-weight adapter between consecutive convolutional stages in a pretrained and fixed CNN model when learning new classes of diseases at each learning round (Fig. <ref type="figure" target="#fig_0">1</ref>). Each round of continual learning as a unique task is associated with task-specific adapters and a task-specific classifier head. Model update at each round of continual learning is to find optimal parameters in the newly added task-specific adapters and classifier head. During inference, since multiple classifier heads exist, the correct head containing the class of a given input is expected to be selected. In order to establish the potential connections between tasks and further boost the continual learning performance, a two-stage multi-head learning strategy was proposed by including the idea of out-of-distribution (OOD) detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task-Specific Adapters</head><p>State-of-the-art continual learning methods try to preserve old knowledge by either combining old fixed feature extractors with the newly learned feature extractor <ref type="bibr" target="#b30">[30]</ref>, or by using a shared and fixed pretrained feature extractor <ref type="bibr" target="#b32">[32]</ref>. However, simply combining feature extractors over rounds of continual learning would rapidly expand the model, while using a shared and fixed feature extractor could largely limit model ability of learning new knowledge because only the model head can be tuned to discriminate between classes. To resolve this dilemma, we propose adding a light-weight task-specific module called adapter into a single pretrained and fixed CNN feature extractor (e.g., with the ResNet backbone), such that the model expands very slowly and the adapter-tuned feature extractor for each old task is fixed when learning a new task. In this way, old knowledge largely stored in the feature extractor and associated taskspecific adapters will be well preserved when the model learns new knowledge at subsequent rounds of continual learning. Formally, suppose the pretrained CNN feature extractor contains K stages of convolutional layers (e.g., 5 stages in ResNet), and the output feature maps from the k-th stage is denoted by z k , k ∈ {1, . . . , K -1}. Then, when the model learns a new task at the t-th round of continual learning, a task-specific adapter A t,k is added between the k-th and (k + 1)-th stages as follows,</p><formula xml:id="formula_0">ẑt,k = A t,k (z k ) + z k , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where the adapter-tuned output ẑt,k will be used as input to the (k +1)-th stage.</p><p>The light-weight adapter can be flexibly designed. In this study, a simple twolayer convolution module followed by a global scaling is designed as the adapter (Fig. <ref type="figure" target="#fig_0">1</ref>, Right). The input feature maps to the adapter are spatially downsampled by the first convolutional layer and then upsampled by the second layer. The global scaling factor α is learned together with the two convolutional layers. The proposed task-specific adapter for continual learning is inspired by Delta tuning <ref type="bibr" target="#b7">[7]</ref> which adds learnable 2-layer perceptron(s) into a pretrained and fixed</p><p>Transformer model in natural language processing. Different from Delta tuning which is used as a transfer learning strategy to adapt a pretrained model for any individual downstream task, the proposed task-specific adapter is used as a continual learning strategy to help a model continually learn new knowledge over multiple rounds (i.e., multiple tasks) in image processing, with each round corresponding to a specific set of adapters. Also note that the proposed adapter differs from existing adapters in CLIP-Adapter (CA) [9] and Tip-Adapter (TA) <ref type="bibr" target="#b33">[33]</ref>. First, in structure, CA and TA use 2-layer MLP or cache model, while ours uses a 2-layer convnet with a global scaling factor. Second, the number and locations of adapters in model are different. CA and TA use adapter only at output of the last layer, while ours appears between each two consecutive CNN stages. Third, the roles of adapters are different. Existing adapters are for few-shot classification, while ours is for continual learning. It is also different from current prompt tuning. Prompts appear as part of input to the first or/and intermediate layer(s) of model, often in the form of learnable tokens for Transformer or image regions for CNNs. In contrast, our adapter appears as an embedded neural module for each two consecutive CNN stages, in the form of sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task-Specific Head</head><p>Task-specific head is proposed to alleviate the potential feature fusion issue in current state-of-the-art methods <ref type="bibr" target="#b30">[30]</ref> which combine task-specific features by a unified classifier head. In particular, if feature outputs from multiple taskspecific feature extractors are simply fused by concatenating or averaging followed by a unified 1-or 2-layer percepton (as in <ref type="bibr" target="#b30">[30]</ref>), discriminative feature information appearing only in those classes of a specific task could become less salient after fusion with multiple (possibly less discriminative) features from other task-specific feature extractors. As a result, current state-of-the-art methods often require storing relatively more old data to help train a discriminative unified classifier head between different classes. To avoid the possible reduction in feature discriminability, we propose not fusing features from multiple feature extractors, but a task-specific classifier head for each task. Each task-specific head consists of one fully connected layer followed by a softmax operator. Specially, for a task containing C new classes, one additional class absorbing all previously learned old classes ('others' output neuron in Fig. <ref type="figure" target="#fig_0">1</ref>) is also included, and therefore the number of output elements from the softmax will be C +1. The 'others' output is used to predict the probability of the input image being from certain class of any other task rather than from the current task. In other words, each task-specific head has the ability of out-of-distribution (OOD) ability with the help of the 'others' output neuron. At the t-round of continual learning (i.e., for the t-th task learning), the task-specific adapters and the task-specific classifier head can be directly optimized, e.g., by cross-entropy loss, with the C new classes of training data and the 'others' class of all preserved old data.</p><p>However, training the task-specific classifier head without considering its relationship with existing classifier heads of previously learned tasks may cause the head selection issue during model inference. For example, a previously learned old classifier head may consider an input of latterly learned class as one of the old classes (correspondingly the 'others' output from the old head will be low). In other words, the 'others' outputs from multiple classifier heads cannot not be reliably compared (i.e., not calibrated) with one another if each classifier head is trained individually. In this case, if all classifier heads consider a test input as 'others' class with high confidence or multiple classifier heads consider a test input as one of their classes, it would become difficult to choose an appropriate classifier head for final prediction. To resolve the head selection issue, after initial training of the current task's adapters and classifier head, all the tasks' heads are fine-tuned together such that all 'others' outputs from the multiple heads are comparable. In short, at the t-th round of continual learning, the t task-specific classifier heads can be fine-tuned by minimizing the loss function L,</p><formula xml:id="formula_2">L = 1 t t s=1 L c s , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where L c s is the cross-entropy loss for the s-th classifier head. Following the finetuning step in previous continual learning studies <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b30">30]</ref>, training data of the current t-th task are sub-sampled such that training data in the fine-tuning step are balanced across all learned classes so far. Note that for each input image, multiple runs of feature extraction are performed, with each run adding adapters of a different task to the original feature extractor and extracting the feature vector for the corresponding task-specific head. Also note that in the fine-tuning step, adapters of all tasks are fixed and not tuned. Compared to training the adapters of each task with all training data of the corresponding task, fine-tuning these adapters would likely cause over-fitting of the adapters to the sub-sampled data and therefore is avoided in the fine-tuning step.</p><p>Once the multi-head classifier is fine-tuned at the t-th round of continual learning, the classifier can be applied to predict any test data as one of all the learned classes so far. First, the task head with the smallest 'others' output probability (among all t 'others' outputs) is selected, and then the class with the highest output from the selected task head is selected as the final prediction result. Although unlikely selected, the 'others' class in the selected task head is excluded for the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Four datasets were used to evaluate the proposed ACL (Table <ref type="table" target="#tab_0">1</ref>). Among them, Skin8 is imbalanced across classes and from the public challenge organized by the International Skin Imaging Collaboration (ISIC) <ref type="bibr" target="#b24">[24]</ref>. Path16 is a subset of publicly released histopathology images collated from multiple publicly available datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35]</ref>, including eleven diseases and five normal classes (see Supplementary Material for more details about dataset generation). These data In all experiments, publicly released CNN models which are pretrained on the Imagenet-1K dataset were used for the fixed feature extractor. During continual learning, the stochastic gradient descent optimizer was used for task-specific adapter learning, with batch size 32, weight decay 0.0005, and momentum 0.9. The initial learning rate was 0.01 and decayed by a factor of 10 at the 70th, 100th and 130th epoch, respectively. The adapters were trained for up to 200 epochs with consistently observed convergence. For fine-tuning classifier heads, the Adam optimizer was adopted, with initial learning rate 0.001 which decayed by a factor of 10 at the 55th, and 80th, respectively. The classifier heads were finetuned for 100 epochs with convergence observed. Unless otherwise mentioned, ResNet18 was used as the backbone, the size of memory for storing old images was 40 on Skin8, 80 on Path16, 2000 on CIFAR100 and 200 on MedMNIST.</p><p>In continual learning, the classifier sequentially learned multiple tasks, with each task a small number of new classes (e.g., 2, 10, 20). After learning each task, the mean class recall (MCR) over all classes learned so far is used to measure the classifier's performance. Note that MCR is equivalent to classification accuracy for class-balanced test set. For each experiment, the order of classes is fixed, and all methods were executed three times with different initialization. The mean and standard deviation of MCRs over three runs were reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Result Analysis</head><p>Effectiveness Evaluation: In this section, we compare ACL against stateof-the-art baselines, including iCaRL <ref type="bibr" target="#b19">[19]</ref>, DynaER <ref type="bibr" target="#b30">[30]</ref>, DER++ <ref type="bibr" target="#b3">[3]</ref>, WA <ref type="bibr" target="#b34">[34]</ref>, PODNet <ref type="bibr" target="#b8">[8]</ref>, and UCIR <ref type="bibr" target="#b11">[11]</ref>. In addition, an upper-bound result (from a classifier which was trained with all classes of training data) is also reported. Similar amount of effort was taken in tuning each baseline method. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our method outperforms all strong baselines in almost all settings, no matter whether the classifier learn continually 2 classes each time on Skin8 (Fig. <ref type="figure" target="#fig_1">2</ref>, first column), in two different task orders on Path16 (Fig. <ref type="figure" target="#fig_1">2</ref>, second column), in 10 or  20 classes each time on CIFAR100 (Fig. <ref type="figure" target="#fig_1">2</ref>, last column), or in 4 different domains on MedMNIST <ref type="bibr" target="#b31">[31]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref> in Supplementary Material). Note that performance of most methods does not decrease (or even increase) at the last two or three learning rounds on Path16, probably because most methods perform much better on these tasks than on previous rounds of tasks.</p><p>Ablation Study: An ablation study was performed to evaluate the performance gain of each proposed component in ACL. Table <ref type="table" target="#tab_1">2</ref> (first four rows) shows that the continual learning performance is gradually improved while more components are included, confirming the effectiveness of each proposed component. In addition, when fusing all the task features with a unified classifier head (Table <ref type="table" target="#tab_1">2</ref>, last row), the continual learning performance is clearly decreased compared to that from the proposed method (fourth row), confirming the effectiveness of task-specific classifier heads for class-incremental learning. Generalizability Study: The pretrained feature extractor with different CNN backbones were used to evaluate the generalization of ACL. As shown in Table <ref type="table" target="#tab_2">3</ref>, ACL consistently outperforms representative strong baselines with each CNN backbone (ResNet18 <ref type="bibr" target="#b10">[10]</ref>, EfficientNet-B0 <ref type="bibr" target="#b23">[23]</ref> and MobileNetV2 <ref type="bibr" target="#b21">[21]</ref>) on Skin8, supporting the generalizability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Here we propose a new adapter-based strategy for class-incremental learning of new diseases. The learnable light-weight and task-specific adapters, together with the pretrained and fixed feature extractor, can effectively learn new knowledge of diseases and meanwhile keep old knowledge from catastrophic forgetting. The task-specific heads with the special 'out-of-distribution' output neuron within each head helps keep extracted features discriminative between different tasks. Empirical evaluations on multiple medical image datasets confirm the efficacy of the proposed method. We expect such adapter-based strategy can be extended to other continual learning tasks including lesion detection and segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed framework for continual learning of new diseases. Left: taskspecific adapters ({At,1, At,1, . . . , At,K-1} in orange) between consecutive convolutional stages are added and learned for a new task. After learning the new task-specific adapters, all tasks' classifier heads (orange rectangles) are fine-tuned with balanced training data. The pretrained feature extractor is fixed during the continual learning process. Right: the structure of each adapter, with α representing the global scaling. (Color figure online)</figDesc><graphic coords="3,44,31,54,47,335,11,168,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance of continual learning on the Skin8, Path16 and CIFAR100 dataset, respectively. First column: 2 new classes each time on Skin8 respectively with memory size 16 and 40. Second column: continual learning on Path16 in different task orders. Last column: respectively learning 10 and 20 new classes each time on CIFAR100.</figDesc><graphic coords="8,70,98,54,62,310,93,132,07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of three datasets. '[600, 1024]': the range of image width and height.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Classes Train set Test set Number of tasks Size</cell></row><row><cell>Skin8 [24]</cell><cell>8</cell><cell>3,555</cell><cell>705</cell><cell>4</cell><cell>[600, 1024]</cell></row><row><cell>Path16</cell><cell>16</cell><cell>12,808</cell><cell>1,607</cell><cell>7</cell><cell>224 × 224</cell></row><row><cell cols="2">CIFAR100 [16] 100</cell><cell>50,000</cell><cell cols="2">10,000 5, 10</cell><cell>32 × 32</cell></row><row><cell cols="2">MedMNIST [31] 36</cell><cell cols="3">302,002 75,659 4</cell><cell>28 × 28</cell></row><row><cell cols="6">are divided into seven tasks based on source of images, including Oral cavity</cell></row><row><cell cols="6">(OR, 2 classes), Lymph node (LY, 2 classes), Breast (BR, 2 classes), Colon (CO,</cell></row><row><cell cols="6">2 classes), Lung (LU, 2 classes), Stomach (ST, 4 classes), and Colorectal polyp</cell></row><row><cell cols="6">(CP, 2 classes). In training, each image is randomly rotated and then resized to</cell></row><row><cell>224 × 224 pixels.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of ACL on Skin8 (with 2 new class per time) and on CIFAR100 (with 10 new classes per time). 'T.S.H.': inclusion of task-specific heads; 'Others': inclusion of the 'others' output neuron in each head. 'Avg': average of MCRs over all rounds of continual learning; 'Last': MCR at the last round.</figDesc><table><row><cell>Components</cell><cell>Skin8</cell><cell></cell><cell>CIFAR100</cell></row><row><cell cols="2">T.S.H Adapter Others Fine-tune Avg</cell><cell>Last</cell><cell>Avg</cell><cell>Last</cell></row><row><cell></cell><cell cols="4">50.91±0.18 27.47±0.32 41.68±0.04 18.64±0.14</cell></row><row><cell></cell><cell cols="2">60.89±0.56 35.4±1.20</cell><cell cols="2">47.22±0.09 21.20±0.13</cell></row><row><cell></cell><cell cols="4">60.90±1.97 42.18±2.65 58.72±0.07 46.44±0.42</cell></row><row><cell></cell><cell cols="4">66.44±0.90 50.38±0.31 82.50±0.39 73.02±0.47</cell></row><row><cell></cell><cell cols="4">64.80±0.87 46.77±1.58 81.67±0.39 70.72±0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Continual learning performance with different CNN backbones. Two new classes and ten new classes were learned each time on Skin 8 and CIFAR100, respectively. The range of standard deviation is [0.06, 3.57].</figDesc><table><row><cell>Backbones</cell><cell>ResNet18</cell><cell></cell><cell>EfficientNet-B0</cell><cell></cell><cell>MobileNetV2</cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">iCaRL DynaER ACL(ours) iCaRL DynaER ACL(ours) iCaRL DynaER ACL(ours)</cell></row><row><cell>Skin8</cell><cell>Avg 62.16 60.24</cell><cell>66.44</cell><cell>61.17 60.52</cell><cell>66.86</cell><cell>64.58 62.16</cell><cell>66.08</cell></row><row><cell></cell><cell>Last 41.94 39.47</cell><cell>50.38</cell><cell>42.60 40.17</cell><cell>48.50</cell><cell>42.52 41.49</cell><cell>48.83</cell></row><row><cell cols="2">CIFAR100 Avg 75.74 78.59</cell><cell>82.50</cell><cell>73.98 81.98</cell><cell>84.56</cell><cell>73.47 77.04</cell><cell>81.04</cell></row><row><cell></cell><cell>Last 57.75 64.97</cell><cell>73.02</cell><cell>53.40 70.13</cell><cell>75.55</cell><cell>55.00 64.30</cell><cell>70.88</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported in part by the <rs type="funder">Major Key Project of PCL</rs> (grant No. <rs type="grantNumber">PCL2023AS7-1</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (grant No. <rs type="grantNumber">62071502</rs> &amp; No. <rs type="grantNumber">12071496</rs>), <rs type="programName">Guangdong Excellent Youth Team Program</rs> (grant No. <rs type="grantNumber">2023B1515040025</rs>), and the <rs type="funder">Guangdong Provincial Natural Science Fund</rs> (grant No. <rs type="grantNumber">2023A1515012097</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_23Kaasw">
					<idno type="grant-number">PCL2023AS7-1</idno>
				</org>
				<org type="funding" xml:id="_ZhdfyHX">
					<idno type="grant-number">62071502</idno>
				</org>
				<org type="funding" xml:id="_DMesnna">
					<idno type="grant-number">12071496</idno>
					<orgName type="program" subtype="full">Guangdong Excellent Youth Team Program</orgName>
				</org>
				<org type="funding" xml:id="_P3FpDrT">
					<idno type="grant-number">2023B1515040025</idno>
				</org>
				<org type="funding" xml:id="_KWgPUhK">
					<idno type="grant-number">2023A1515012097</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Borkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Deland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12142</idno>
		<title level="m">Mastorides, S.M.: Lung and colon cancer histopathological image dataset (LC25000)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer without forgetting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boschini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13683</biblScope>
			<biblScope unit="page" from="692" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20050-2_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20050-240" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dark experience for general continual learning: a strong, simple baseline</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00420</idno>
		<title level="m">Efficient lifelong learning with a-gem</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<publisher>Digital Pathology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Delta tuning: a comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PODNet: pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58565-5_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58565-56" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lifelong learning via progressive distillation and retrospection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Oral cancer dataset</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Kebede</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/ashenafifasilkebede/dataset" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A multi-head model for continual learning via out-ofdistribution replay</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Lifelong Learning Agents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">iCaRL: incremental classifier and representation learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mobilenetv 2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficientnet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rotation equivariant CNNs for digital pathology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-224" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient feature transformations for discriminative and generative continual learning</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dualprompt: complementary prompting for rehearsal-free continual learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19809-0_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19809-036" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13686</biblScope>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to prompt for continual learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A petri dish for histopathology image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Der: Dynamically expandable representation for class incremental learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Medmnist classification decathlon: a lightweight automl benchmark for medical image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continual learning with Bayesian model based on a fixed pre-trained feature extractor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-338" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tip-adapter: training-free adaption of clip for few-shot classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19833-5_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19833-529" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13695</biblScope>
			<biblScope unit="page" from="493" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Maintaining discrimination and fairness in class incremental learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A deep learning model and human-machine fusion for prediction of EBV-associated gastric cancer from histopathology</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2790</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ensembled deep learning model outperforms human experts in diagnosing biliary atresia from sonographic gallbladder images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1259</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
