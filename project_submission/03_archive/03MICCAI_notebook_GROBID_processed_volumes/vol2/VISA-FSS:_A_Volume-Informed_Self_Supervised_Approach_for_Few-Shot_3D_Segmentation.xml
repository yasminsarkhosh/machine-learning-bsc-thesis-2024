<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Mozafari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adeleh</forename><surname>Bitarafan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Farid</forename><surname>Azampour</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Azade</forename><surname>Farshad</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mahdieh</forename><surname>Soleymani Baghshah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="112" to="122"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E44CC5BE57BF02000B47C3C42901F77A</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image segmentation</term>
					<term>Few-shot learning</term>
					<term>Few-shot semantic segmentation</term>
					<term>Self-supervised learning</term>
					<term>Supervoxels M. Mozafari and A. Bitarafan-Equal Contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot segmentation (FSS) models have gained popularity in medical imaging analysis due to their ability to generalize well to unseen classes with only a small amount of annotated data. A key requirement for the success of FSS models is a diverse set of annotated classes as the base training tasks. This is a difficult condition to meet in the medical domain due to the lack of annotations, especially in volumetric images. To tackle this problem, self-supervised FSS methods for 3D images have been introduced. However, existing methods often ignore intra-volume information in 3D image segmentation, which can limit their performance. To address this issue, we propose a novel selfsupervised volume-aware FSS framework for 3D medical images, termed VISA-FSS. In general, VISA-FSS aims to learn continuous shape changes that exist among consecutive slices within a volumetric image to improve the performance of 3D medical segmentation. To achieve this goal, we introduce a volume-aware task generation method that utilizes consecutive slices within a 3D image to construct more varied and realistic self-supervised FSS tasks during training. In addition, to provide pseudolabels for consecutive slices, a novel strategy is proposed that propagates pseudo-labels of a slice to its adjacent slices using flow field vectors to preserve anatomical shape continuity. In the inference time, we then introduce a volumetric segmentation strategy to fully exploit the interslice information within volumetric images. Comprehensive experiments on two common medical benchmarks, including abdomen CT and MRI, demonstrate the effectiveness of our model over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated image segmentation is a fundamental task in many medical imaging applications, such as diagnosis <ref type="bibr" target="#b23">[24]</ref>, treatment planning <ref type="bibr" target="#b5">[6]</ref>, radiation therapy planning, and tumor resection surgeries <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. In the current literature, numerous fully-supervised deep learning (DL) methods have become dominant in the medical image segmentation task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. They can achieve their full potential when trained on large amounts of fully annotated data, which is often unavailable in the medical domain. Medical data annotation requires expert knowledge, and exhaustive labor, especially for volumetric images <ref type="bibr" target="#b16">[17]</ref>. Moreover, supervised DL-based methods are not sufficiently generalizable to previously unseen classes.</p><p>To address these limitations, few-shot segmentation (FSS) methods have been proposed <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b24">25]</ref>, that segment an unseen class based on just a few annotated samples. The main FSS approaches use the idea of meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> and apply supervised learning to train a few-shot model. However, to avoid overfitting and improve the generalization capability of FSS models, they rely on a large number of related tasks or classes. This can be challenging as it may require a large amount of annotated data, which may not always be available. Although some works on FSS techniques focus on training with fewer data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>, they require re-training before applying to unseen classes. To eliminate the need for annotated data during training and re-training on unseen classes, some recent works have proposed self-supervised FSS methods for 3D medical images which use superpixel-based pseudo-labels as supervision during training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. These methods design their self-supervised tasks (support-query pairs) by applying a predefined transformation (e.g., geometric and intensity transformation) on a support image (i.e., a random slice of a volume) to synthetically form a query one. Thus, these methods do not take into account intra-volume information and context that may be important for the accurate segmentation of volumetric images during inference.</p><p>We propose a novel volume-informed self-supervised approach for Few-Shot 3D Segmentation (VISA-FSS). Generally, VISA-FSS aims to exploit information beyond 2D image slices by learning inter-slice information and continuous shape changes that intrinsically exists among consecutive slices within a 3D image. To this end, we introduce a novel type of self-supervised tasks (see Sect. 2.2) that builds more varied and realistic self-supervised FSS tasks during training. Besides of generating synthetic queries (like <ref type="bibr" target="#b18">[19]</ref> by applying geometric or intensity transformation on the support images), we also utilize consecutive slices within a 3D volume as support and query images. This novel type of task generation (in addition to diversifying the tasks) allows us to present a 2.5D loss function that enforces mask continuity between the prediction of adjacent queries. In addition, to provide pseudo-labels for consecutive slices, we propose the superpixel propagation strategy (SPPS). It propagates the superpixel of a support slice into query ones by using flow field vectors that exist between adjacent slices within a 3D image. We then introduce a novel strategy for volumetric segmentation during inference that also exploits inter-slice information within query volumes. It propagates a segmentation mask among consecutive slices using the few-shot segmenter trained by VISA-FSS. Comprehensive experiments demonstrate the superiority of our method against state-of-the-art FSS approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we introduce our proposed VISA-FSS for 3D medical image segmentation. Our method goes beyond 2D image slices and exploits intra-volume information during training. To this end, VISA-FSS designs more varied and realistic self-supervised FSS tasks (support-query pairs) based on two types of transformations: 1) applying a predefined transformation (e.g., geometric and intensity transformation as used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>) on a random slice as support image to synthetically make a query one, 2) taking consecutive slices in a 3D volume as support and query images to learn continuous shape transformation that exists intrinsically between consecutive slices within a volumetric image (see Sect. 2.2). Moreover, the volumetric view of task generation in the second type of tasks allows us to go beyond 2D loss functions. Thus, in Sect. 2.2, we present a 2.5D loss function that enforces mask continuity between the prediction of adjacent queries during training the few-shot segmenter. In this way, the trained few-shot segmenter is able to effectively segment a new class in a query slice given a support slice, regardless of whether it is in a different volume (due to learning the first type of tasks) or in the same query volume (due to learning the second type of tasks). Finally, we propose a volumetric segmentation strategy for inference time which is elaborated upon in Sect. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setup</head><p>In FSS, a training dataset D tr = {(x i , y i (l))} Ntr i=1 , l ∈ L tr , and a testing dataset D te = {(x i , y i (l))} Nte i=1 , l ∈ L te are available, where (x i , y i (l)) denotes an imagemask pair of the binary class l. L tr and L te are the training and testing classes, respectively, and L tr ∩ L te = ∅. The objective is to train a segmentation model on D tr that is directly applicable to segment an unseen class l ∈ L te in a query image, x q ∈ D te , given a few support set {(x j s , y j s (l))} p j=1 ⊂ D te . Here, q and s indicate that an image or mask is from a query or support set. To simplify notation afterwards, we assume p = 1, which indicates the number of support images. During training, a few-shot segmenter takes a support-query pair (S, Q) as the input data, where Q = {(x i q , y i q (l))} ⊂ D tr , and S = {(x j s , y j s (l))} ⊂ D tr . Then, the model is trained according to the cross-entropy loss on each support-query pair as follows: L(θ) = -log p θ (y q |x q , S). In this work, we model p θ (y q |x q , S) using the prototypical network introduced in <ref type="bibr" target="#b18">[19]</ref>, called ALPNet. However, the network architecture is not the main focus of this paper, since our VISA-FSS framework can be applied to any FSS network. The main idea of VISA-FSS is to learn a few-shot segmenter according to novel tasks designed in Sect. 2.2 to be effectively applicable for volumetric segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Task Generation</head><p>There is a large level of information in a 3D medical image over its 2D image slices, while prior FSS methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> ignore intra-volume information for creating their self-supervised tasks during training, although they are finally applied to segment volumetric images during inference. Previous approaches employ a predefined transformation (e.g., geometric and intensity) to form support-query pairs. We call these predefined transformations as synthetic transformations. On the other hand, there is continuous shape transformation that intrinsically exists between consecutive slices within a volume (we name them realistic transformation). VISA-FSS aims to, besides synthetic transformations, exploit realistic ones to learn more varied and realistic tasks. Figure <ref type="figure" target="#fig_0">1</ref> outlines a graphical overview of the proposed VISA-FSS framework, which involves the use of two types of selfsupervised FSS tasks to train the few-shot segmenter. The two types of tasks are synthetic tasks and realistic tasks: Synthetic Tasks. In the first type, tasks are formed the same as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. For each slice x i s , its superpixels are extracted by the unsupervised algorithm <ref type="bibr" target="#b9">[10]</ref>, and its pseudo-mask is generated by randomly selecting one of its superpixels as a pseudo-organ. Thus, the support is formed as S = (x i s , y i s (l)) ⊂ D tr , where l denotes the chosen superpixel. Then, after applying a random synthetic transformation T on S, the synthetic query will be prepared, i.e., Q s = (x i q , y i q (l)) = (T (x i s ), T (y i s (l))). In this way, the (S, Q s ) pair is taken as the input data of the few-shot segmenter, presenting a 1-way 1-shot segmentation problem. A schematic view of a representative (S, Q s ) pair is given in the blue block of Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Realistic Tasks. To make the second type of task, we take 2m adjacent slices of the support image x i s , as our query images {x j q } j∈N (i) , where N (i) = {i-m, ..., i-1, i+ 1, i+ m}. These query images can be considered as real deformations of the support image. This encourages the few-shot segmenter to learn intra-volume information contrary to the first type of task. Importantly, pseudo-label generation for consecutive slices is the main challenge. To solve this problem, we introduce a novel strategy called SPPS that propagates the pseudo-label of the support image into query ones. Specifically, we consecutively apply flow field vectors that exist between adjacent image slices on y i s (l) to generate pseudo-label y j q (l) as follows:</p><formula xml:id="formula_0">y j q (l) = y i s (l)•φ(x i s , x i+1 q )•φ(x i+1 q , x i+2 q )•...•φ(x i+m-1 q , x i+m q</formula><p>) for j &gt; m, and</p><formula xml:id="formula_1">y j q (l) = y i s (l) • φ(x i s , x i-1 q ) • φ(x i-1 q , x i-2 q ) • ... • φ(x i-m+1 q , x i-m q</formula><p>) for j &lt; m, where φ(x i , x j ) is the flow field vector between x i and x j , which can be computed by deformably registering the two images using VoxelMorph <ref type="bibr" target="#b1">[2]</ref> or Vol2Flow <ref type="bibr" target="#b2">[3]</ref>. A schematic illustration of pseudo-label generation using SPPS is depicted in the Supplementary Materials. The yellow block in Fig. <ref type="figure" target="#fig_0">1</ref> demonstrates a representative (S, Q r ) pair formed using realistic tasks, where Q r = {(x j q , y j q (l))} j∈N (i) .</p><p>Loss Function. The network is trained end-to-end in two stages. In the first stage, we train the few-shot segmenter on both types of synthetic and realistic tasks using the segmentation loss employed in <ref type="bibr" target="#b18">[19]</ref> and regularization loss defined in <ref type="bibr" target="#b24">[25]</ref>, which are based on the standard cross-entropy loss. Specifically, in each iteration, the segmentation loss L seg can be followed as:</p><formula xml:id="formula_2">L seg = -1 HW H h=1 W w=1 y i q (l)(h, w) log(ŷ i q (l)(h, w)) + (1 -y i q (l)(h, w)) log(1 - ŷi q (l)(h, w))</formula><p>, which is applied on a random query x i q (formed by synthetic or realistic transformations) to predict the segmentation mask ŷi q (l), where l ∈ L tr . The regularization loss L reg is defined to segment the class l in its corresponding support image x i s , as follows:</p><formula xml:id="formula_3">L reg = -1 HW H h=1 W w=1 y i s (l)(h, w) log(ŷ i s (l)(h, w)) + (1 -y i s (l)(h, w)) log(1 -ŷi s (l)(h, w)).</formula><p>Overall, in each iteration, the loss function during the first-stage training is</p><formula xml:id="formula_4">L 1 = L seg + L reg .</formula><p>In the second stage of training, we aim to exploit information beyond 2D image slices in a volumetric image by employing realistic tasks.</p><p>To this end, we define the 2.5D loss function, L 2.5D , which enforces mask continuity among the prediction of adjacent queries. The proposed L 2.5D profits the Dice loss <ref type="bibr" target="#b17">[18]</ref> to measure the similarity between the predicted mask of 2m adjacent slices of the support image x i s as follows:</p><formula xml:id="formula_5">L 2.5D = 1 2m -1 j∈N (i) (1 -Dice(ŷ j q (l), ŷj+1 q (l))).<label>(1)</label></formula><p>Specifically, the loss function compares the predicted mask of a query slice with the predicted mask of its adjacent slice and penalizes any discontinuities between them. This helps ensure that the model produces consistent and coherent segmentation masks across multiple slices, improving the overall quality and accuracy of the segmentation. Hence, in the second-stage training, we train the network only on realistic tasks using the loss function:</p><formula xml:id="formula_6">L 2 = L seg + L reg + λ 1 L 2.5D</formula><p>, where λ 1 is linearly increased from 0 to 0.5 every 1000th iteration during training. Finally, after self-supervised learning, the few-shot segmenter can be directly utilized for inference on unseen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Volumetric Segmentation Strategy</head><p>During inference, the goal is to segment query volumes based on a support volume with only a sparse set of human-annotated slices, while the few-shot segmenter is trained with 2D images. To evaluate 2D segmentation on 3D volumetric images, we take inspiration from <ref type="bibr" target="#b20">[21]</ref> and propose the volumetric segmentation propagation strategy (VSPS). Assume, X s = {x 1 s , x 2 s , ..., x ns s } and X q = {x 1 q , x 2 q , ..., x nq q } denote support and query volumes, comprising of n s and n q consecutive slices, respectively. We follow the same setting as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> in which slices containing semantic class l are divided into K equally-spaced groups, including [X 1 s , X 2 s , ..., X K s ] in the support, and [X 1 q , X 2 q , ..., X K q ] in the query volume, where X k indicates the set of slices in the k th group. Suppose, in each of the k groups in the support volume, the manual annotation of the middle slice [(x c s ) 1 , (x c s ) 2 , ..., (x c s ) K ] are available as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. For volumetric segmentation, previous methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>, for each group k ∈ {1, ..., K}, pair the annotated center slice in the support volume with all the unannotated slices of the corresponding group in the query volume. More precisely, <ref type="figure">((x c</ref> s ) k , (y c s ) k ) is considered as the support for all slices in X k q , where (y c s ) k is annotation of the center slice (x c s ) k . Finally, they use the 2D few-shot segmenter to find the mask of each of the query slices individually and therefore segment the whole query volume accordingly. In this work, we exploit the VSPS algorithm, which is based on two steps. In the first step, an inter-volume task is constructed to segment the center slice of each group in the query volume. More precisely, the center slice of each query group, (x c q ) k , is segmented using ((x c s ) k , (y c s ) k ) as the support. Then, by employing the volumetric view even in the inference time, we construct intra-volume tasks to segment other slices of each group. Formally, VSPS consecutively segments each (x j q ) k ∈ X k q , starting (x c q ) k , with respect to the image-mask pair of its previous slice, i.e., ((x j-1 q ) k , (ŷ j-1 q ) k ).</p><p>In fact, we first find the pseudo-mask of (x c q ) k using the 2D few-shot segmenter and consequently consider this pseudo-annotated slice as the support for all other slices in X k q . It is worth mentioning that our task generation strategy discussed in Sect. 2.2 is capable of handling such intra-volume tasks. Further details of the VSPS algorithm are brought in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>To unify experiment results, we follow the evaluation protocol established by <ref type="bibr" target="#b18">[19]</ref>, such as Hyper-parameters, data preprocessing techniques, evaluation metric (i.e., Dice score), and compared methods. The architecture and implementation of the network are exactly the same as developed in <ref type="bibr" target="#b18">[19]</ref>. Moreover, during inference, a support volume with 3 annotated slices (i.e., K = 3) is used as a reference to segment each query volume, the same as in <ref type="bibr" target="#b18">[19]</ref>. Also, we set m = 3, taking 3 adjacent slices of the support image as consecutive query images. However, the effect of this hyper-parameter is investigated in the Supplementary Materials.</p><p>Dataset. Following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, we perform experiments on two common medical benchmarks, including abdominal CT image scans from MICCAI 2015 Multi-Atlas Abdomen Labeling challenge <ref type="bibr" target="#b14">[15]</ref> and abdominal MRI image scans from ISBI 2019 Combined Healthy Abdominal Organ Segmentation Challenge <ref type="bibr" target="#b13">[14]</ref>. In addition, in all experiments, average results are reported according to 5-fold cross-validation on four anatomical structures the same as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, including left kidney (LK), right kidney (RK), spleen, and liver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Comparison with Existing Approaches. Table <ref type="table" target="#tab_0">1</ref> compares VISA-FSS with state-of-the-art FSS methods in terms of Dice, including: Vanilla PANet <ref type="bibr" target="#b24">[25]</ref>, SE-Net <ref type="bibr" target="#b20">[21]</ref>, SSL-RPNet <ref type="bibr" target="#b22">[23]</ref>, SSL-ALPNet <ref type="bibr" target="#b18">[19]</ref>, and CRAPNet <ref type="bibr" target="#b7">[8]</ref>. Vanilla PANet and SE-Net are baselines on natural and medical images, respectively, which utilize manual annotations for training. SSL-RPNet, SSL-ALPNet, and CRAP-Net are self-supervised methods that construct their FSS tasks using synthetic transformations (e.g., geometric and intensity) in the same way, and are only different in the network architecture. As demonstrated, VISA-FSS outperforms vanilla PANet and SE-Net without using any manual annotation in its training phase. Moreover, the performance gains of VISA-FSS compared with SSL-RPNet, SSL-ALPNet, and CRAPNet highlight the benefit of learning continuous shape transformation among consecutive slices within a 3D image for volumetric segmentation. Also, the performance of VISA-FSS was evaluated using Hausedorff Distance and Surface Dice metrics on CT and MRI datasets. On the CT dataset, VISA-FSS reduced SSLALPNet's Hausedorff Distance from 30.07 to <ref type="bibr">23.62</ref>  Effect of Task Generation. To investigate the effect of realistic tasks in selfsupervised FSS models, we perform an ablation study on the absence of this type of task. The experiment results are given in rows (a) and (b) of Table <ref type="table" target="#tab_1">2</ref>. As expected, performance gains can be observed when both synthetic and realistic tasks are employed during training. This can highlight that the use of more and diverse tasks improves the performance of FSS models. Of note, to generate pseudo-label for consecutive slices, instead of SPPS, we can also employ supervoxel generation strategies like the popular SLIC algorithm <ref type="bibr" target="#b0">[1]</ref>. However, we observed that by doing so the performance is 66.83 in the term of mean Dice score, under-performing SPPS (row (b) in Table <ref type="table" target="#tab_1">2</ref>) by about 8%. It can be inferred that contrary to SLIC, SPPS implicitly takes pseudolabel shape continuity into account due to its propagation process, which can help construct effective realistic tasks. To intuitively illustrate this issue, visual comparison of some pseudo-labels generated by SLIC and SPPS is depicted in the Supplementary Materials. In addition, to demonstrate the importance of the 2.5D loss function defined in Eq. 1 during training, we report the performance with and without L 2.5D in Table <ref type="table" target="#tab_1">2</ref> (see row (d) and (e)). We observe over 1% increase in the average Dice due to applying the 2.5D loss function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work introduces a novel framework called VISA-FSS, which aims to perform few-shot 3D segmentation without requiring any manual annotations during training. VISA-FSS leverages inter-slice information and continuous shape changes that exist across consecutive slices within a 3D image. During training, it uses consecutive slices within a 3D volume as support and query images, as well as support-query pairs generated by applying geometric and intensity transformations. This allows us to exploit intra-volume information and introduce a 2.5D loss function that penalizes the model for making predictions that are discontinuous among adjacent slices. Finally, during inference, a novel strategy for volumetric segmentation is introduced to employ the volumetric view even during the testing time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed VISA-FSS framework during training, where four adjacent slices of the support image, x i s , are taken as query images (i.e. m = 2). SPPS is a pseudo-label generation module for consecutive slices.</figDesc><graphic coords="3,44,79,54,53,334,51,130,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>and improved Surface Dice from 89.31% to 90.68%. On the MRI dataset, it decreased Hausedorff Distance from 26.49 to 22.14 and improved Surface Dice from 90.16% to 91.08%. Further experiments and qualitative results are given in the Supplementary Materials, demonstrating satisfactory results on different abdominal organs. Comparison results of different methods (in Dice score) on abdominal images.</figDesc><table><row><cell>Method</cell><cell cols="2">Abdominal-CT</cell><cell></cell><cell cols="3">Mean Abdominal-MRI</cell><cell>Mean</cell></row><row><cell></cell><cell>RK</cell><cell>LK</cell><cell>Spleen Liver</cell><cell>RK</cell><cell>LK</cell><cell>Spleen Liver</cell></row><row><cell cols="7">Vanilla PANet [25] 21.19 20.67 36.04 49.55 31.86 32.19 30.99 40.58 50.40 38.53</cell></row><row><cell>SE-Net [21]</cell><cell cols="6">12.51 24.42 43.66 35.42 29.00 47.96 45.78 47.30 29.02 42.51</cell></row><row><cell>SSL-RPNet [23]</cell><cell cols="6">66.73 65.14 64.01 72.99 67.22 81.96 71.46 73.55 75.99 75.74</cell></row><row><cell>CRAPNet [8]</cell><cell cols="6">74.18 74.69 70.37 75.41 73.66 86.42 81.95 74.32 76.46 79.79</cell></row><row><cell cols="7">SSL-ALPNet [19] 71.81 72.36 70.96 78.29 73.35 85.18 81.92 72.18 76.10 78.84</cell></row><row><cell cols="7">VISA-FSS (Ours) 76.17 77.05 76.51 78.70 77.11 89.55 87.90 78.05 77.00 83.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on task generation and different types of volumetric segmentation strategies on abdominal CT dataset (Results are based on Dice score).Importance of the Volumetric Segmentation Strategy. To verify the influence of our proposed volumetric segmentation strategy, we compare VSPS against two different strategies: VSS and RPS. VSS (volumetric segmentation strategy) is exactly the same protocol established by<ref type="bibr" target="#b20">[21]</ref> (explained in detail in Sect. 2.3). In addition, RPS (registration-based propagation strategy) is a ablated version of VSPS which propagates the annotation of the center slice in each query volume group into unannotated slices in the same group using registration-based models like<ref type="bibr" target="#b2">[3]</ref> instead of using the trained few-shot segmenter. Comparison results are given in rows (b) to (d) of Table2, demonstrating the superiority of VSPS compared with other strategies. In fact, due to learning synthetic transformations (e.g., geometric and intensity transformation) during training, VSPS, during inference, can successfully segment a new class in a query slice given a support slice from a different volume. Also, due to learning realistic transformations (e.g., intra-volume transformations), each query slice can be effectively segmented with respect to its neighbour slice.</figDesc><table><row><cell>Training</cell><cell></cell><cell>Inference</cell><cell>Organs</cell><cell>Mean</cell></row><row><cell>Tasks</cell><cell>Loss</cell><cell cols="2">Vol. Seg. Str RK LK</cell><cell>Spleen Liver</cell></row><row><cell>(a) Syn</cell><cell cols="2">w.o. L2.5D VSS [21]</cell><cell cols="2">71.81 72.36 70.96 78.29 73.35</cell></row><row><cell cols="3">(b) Syn. + Re w.o. L2.5D VSS [21]</cell><cell cols="2">71.59 72.02 73.85 78.57 74.01</cell></row><row><cell cols="3">(c) Syn. + Re w.o. L2.5D RPS</cell><cell cols="2">71.13 71.72 72.68 77.69 73.31</cell></row><row><cell cols="3">(d) Syn. + Re w.o. L2.5D VSPS</cell><cell cols="2">74.67 75.14 75.00 78.74 75.88</cell></row><row><cell cols="2">(e) Syn. + Re w. L2.5D</cell><cell>VSPS</cell><cell cols="2">76.17 77.05 76.51 78.70 77.11</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_11.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Slic superpixels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vol2flow: segment 3d volumes using a sequence of registration flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bitarafan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Azampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bakhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani Baghshah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_58" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference, Proceedings, Part IV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="609" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d image segmentation with sparse annotation by self-training and internal registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bitarafan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Baghshah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2665" to="2672" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep learning-based auto-segmentation system for organs-atrisk on whole-body computed tomography images for radiation therapy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiother. Oncol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatio-temporal learning from longitudinal data for multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Denner</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72084-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72084-1_11" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12658</biblScope>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation with cycle-resemblance attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2488" to="2497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metamedseg: volumetric meta-learning for few-shot organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makarevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16852-9_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16852-9_5" />
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation and Representation Transfer 2022</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta-learning in neural networks: a survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5149" to="5169" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Miccai multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An integrated iterative annotation technique for easing neural network training in medical image analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lutnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="119" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervision with superpixels: training few-shot medical image segmentation without annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58526-6_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58526-6_45" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12374</biblScope>
			<biblScope unit="page" from="762" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data efficient unsupervised domain adaptation for cross-modality image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_74</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_74" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze &amp; excite&apos;guided few-shot segmentation of volumetric images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101587</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent mask refinement for fewshot medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3918" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating segmentation information into CNN for breast cancer diagnosis of mammographic masses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tsochatzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koutla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Costaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page">105913</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Panet: few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
