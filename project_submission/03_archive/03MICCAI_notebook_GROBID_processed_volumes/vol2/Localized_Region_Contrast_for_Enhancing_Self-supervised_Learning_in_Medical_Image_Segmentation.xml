<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangyi</forename><surname>Yan</surname></persName>
							<email>xiangyy4@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junayed</forename><surname>Naushad</surname></persName>
							<email>jnaushad@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<email>chenyu.you@yale.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<email>htang6@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanlin</forename><surname>Sun</surname></persName>
							<email>shanlins@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
							<email>haoyum3@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
							<email>james.duncan@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="468" to="478"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">63C9C67F24EF7FFBB32B79810C6A1D25</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-supervised Learning</term>
					<term>Contrastive Learning</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in self-supervised learning have demonstrated that effective visual representations can be learned from unlabeled images. This has led to increased interest in applying selfsupervised learning to the medical domain, where unlabeled images are abundant and labeled images are difficult to obtain. However, most selfsupervised learning approaches are modeled as image level discriminative or generative proxy tasks, which may not capture the finer level representations necessary for dense prediction tasks like multi-organ segmentation. In this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation. Our approach involves identifying Super-pixels by Felzenszwalb's algorithm and performing local contrastive learning using a novel contrastive sampling loss. Through extensive experiments on three multi-organ segmentation datasets, we demonstrate that integrating LRC to an existing self-supervised method in a limited annotation setting significantly improves segmentation performance. Moreover, we show that LRC can also be applied to fully-supervised pre-training methods to further boost performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-organ segmentation is a crucial step in medical image analysis that enables physicians to perform diagnosis, prognosis, and treatment planning. However, manual segmentation of large volume computed tomography (CT) and magnetic resonance (MR) images is time-consuming and prone to high inter-rater variability <ref type="bibr" target="#b29">[30]</ref>. In recent years, deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on a wide range of segmentation tasks for natural images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. However, in the medical domain, there is often a lack of labeled examples to optimally train a deep neural network from scratch. Since unlabeled medical images are comparatively easier to obtain in larger quantities, an alternative strategy is to perform self-supervised learning and generate pre-trained models from unlabeled datasets. Self-supervised learning involves automatically generating a supervisory signal from the data itself and learning a representation by solving a pretext task.</p><p>In computer vision, current self-supervised learning methods can be broadly divided into discriminative modeling and generative modeling. In earlier times, discriminative self-supervised pretext tasks are designed as rotation prediction <ref type="bibr" target="#b14">[15]</ref>, jigsaw solving <ref type="bibr" target="#b17">[18]</ref>, and relative patch location prediction <ref type="bibr" target="#b5">[6]</ref>, etc. Recently, contrastive learning achieves great success, whose core idea is to attract different augmented views of the same image and repulse augmented views of different images. Based on this, MoCo <ref type="bibr" target="#b10">[11]</ref> is proposed, which greatly shrink the gap between self-supervised learning and fully-supervised learning. More advanced techniques have emerged recently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Contrastive learning frameworks have also shown promising results in the medical domain, achieving good performance with few labeled examples <ref type="bibr" target="#b0">[1]</ref>. Generative modeling also provides a feasible way for self-supervised pre-training <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, <ref type="bibr">He et al.</ref> propose MAE <ref type="bibr" target="#b27">[28]</ref> and yield a nontrivial and meaningful generative self-supervisory task, by masking a high proportion of the input image. Transfer learning performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior. In medical image domain, Model Genesis <ref type="bibr" target="#b36">[36]</ref> uses a "painting" operation to generate a new image by modifying the input image. Several selfsupervised learning approaches have also achieved state-of-the-art performance in the medical domain on both classification and segmentation tasks while significantly reducing annotation cost <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> However, most self-supervised pre-training strategies are image <ref type="bibr" target="#b10">[11]</ref> or patch <ref type="bibr" target="#b0">[1]</ref> level, which are not capable of capturing the detailed feature representations required for accurate medical segmentation. To address this issue, in this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation.</p><p>Our proposed framework leverages Felzenszwalb's algorithm <ref type="bibr" target="#b7">[8]</ref> to formulate local regions and defines a novel contrastive sampling loss to perform localized contrastive learning. Our main contributions include -We propose a standalone localized contrastive learning module that can be integrated into most existing pre-training strategy to boost multi-organ segmentation performance by learning localized feature representations. -We introduce a novel localized contrastive sampling loss for dense selfsupervised pre-training on local regions.</p><p>-We conduct extensive experiments on three multi-organ segmentation benchmarks and demonstrate that our method consistently outperforms current supervised and unsupervised pre-training approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates our complete framework, which comprises two stages: the contrastive pre-training stage and the fine-tuning stage. Although LRC can be integrated with most of the current popular pre-training strategies, for the purpose of illustration, in this section, we demonstrate how to integrate our LRC module with the classical global (image-level) contrast pre-training strategy MoCo <ref type="bibr" target="#b10">[11]</ref>, using both its original global contrast and our localized contrastive losses during the contrastive pre-training stage. During the fine-tuning stage, we simply concatenate the local and global contrast models and fine-tune the resulting model on a small target dataset. Further details about each stage are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pre-training Stage</head><p>In the pre-training stage, for each batch an image x q is randomly chosen from B images as a query sample, and the rest of the images x n ∈ {x 1 , x 2 , ..., x B } are considered as negative key samples, where n = q. To formulate a positive key sample x p , elastic transforms are performed on the query sample x q .</p><p>Global Contrast. To explore global contextual information, we train a latent encoder E g following the contrastive protocol in <ref type="bibr" target="#b10">[11]</ref>. Three sets of latent embeddings z q , z p , z n are extracted by E g from x q , x p , x n respectively. Using dot product as a measure of similarity, a form of a contrastive loss function called InfoNCE <ref type="bibr" target="#b18">[19]</ref> is considered:</p><formula xml:id="formula_0">L g = -log exp(z q •z p /τg) B i=1 exp(z q •z i /τg)</formula><p>, where τ g is the global temperature hyper-parameter per <ref type="bibr" target="#b26">[27]</ref>. Note that in the global contrast branch, we only pre-train E g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Region</head><p>Contrast. Unlike global contrast, positive and negative pairs for local contrast are only generated from input image x q and its transform x p . We differentiate local regions and formulate the positive and negative pairs by using Felzenszwalb's algorithm. For an input image x, Felzenszwalb's algorithm provides K local regions R = {r 1 , r 2 , .., r K }, where r k is the k-th local region cluster for image x. We then perform elastic transform for both the query image x q and its local regions R q so that we have the augmented image x p = T e (x q ) and its local regions R p = {r 1 p , r 2 p , .., r Kp p }, where r k p = T e (r k q ). Note that K q = K p always holds since R p is a one-to-one mapping from R q . Following the widely used U-Net <ref type="bibr" target="#b21">[22]</ref> model design, the query image x q and augmented image x p are then forwarded to a randomly initialized U-Net variant, which includes a convolutional encoder E l and a convolutional decoder D l . We get corresponding feature maps f q and f p with the same spatial dimensions as x q and x p and D channels from the last convolutional layer of D l . Afterwards, we sample N vectors with dimension D from the local region r k q in f q , and formulate the sample mean</p><formula xml:id="formula_1">f k q = 1 N N n=1 f k,n q</formula><p>, where f k,n q is the n-th vector sampled from feature map f q within the k-th local region r k q . Our sampling strategy is straightforward: we sample random points with replacement following a uniform distribution. We simply refer to this as "random sampling". Similarly, for feature map f p , its sample mean f k p can be provided following the same random sampling process. Each local region pair of f k q and f k p is considered a positive pair. For the negative pairs, we sample both f q and f p from the rest of the local regions {r 1 , r 2 , ..., r k-1 , r k+1 , ..., r K }. The local contrastive loss can be defined as follows:</p><formula xml:id="formula_2">L l = - K k1=1 log exp f k1 q • f k1 p /τ l K k2=1 exp f k1 q • f k2 q /τ l + exp f k1 q • f k2 p /τ l</formula><p>where τ l is the local temperature hyper-parameter. Compared to the global contrast branch, in local contrastive learning, we pre-train both E l and D l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-Tuning Stage</head><p>In the former pre-training stages, E g , E l , and D l are pre-trained with global and local contrast strategy accordingly, with a large number of unlabelled images. In the fine-tuning stage, we fine-tune the model with a limited number of labelled images x f ∈ {x 1 , x 2 , ..., x F }, where F is the size of the fine-tuning dataset.</p><p>Besides the two pre-trained encoders and one decoder, a randomly initialized decoder D g is appended to the pre-trained E g to ensure that the embeddings have the same dimensions prior to concatenation. We combine local and global contrast models by concatenating the output of D g and D l 's last convolutional layer, and fine-tune on the target dataset in an end-to-end fashion. Different levels of feature maps from encoders are concatenated with corresponding layers of decoders through skip connections to provide alternative paths for the gradient. Dice loss is applied as in usual multi-organ segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Dataset</head><p>During both global and local pre-training stages, we pre-train the encoders on the Abdomen-1K <ref type="bibr" target="#b16">[17]</ref> dataset. It contains over one thousand CT images which equates to roughly 240,000 2D slices. The CT images have been curated from 12 medical centers and include multi-phase, multi-vendor, and multi-disease cases.</p><p>Although segmentation masks for liver, kidney, spleen, and pancreas are provided in this dataset, we ignore these labels during pre-training since we are following the self-supervised protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-Tuning Datasets</head><p>During the fine-tuning stage, we perform extensive experiments on three datasets with respect to different regions of the human body. ABD-110 is an abdomen dataset from <ref type="bibr" target="#b24">[25]</ref> that contains 110 CT images from patients with various abdominal tumors and these CT images were taken during the treatment planning stage. We report the average DSC on 11 abdominal organs (large bowel, duodenum, spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney, stomach and gallbladder).</p><p>Thorax-85 is a thorax dataset from <ref type="bibr" target="#b4">[5]</ref> that contains 85 thoracic CT images. We report the average DSC on 6 thoracic organs (esophagus, trachea, spinal cord, left lung, right lung, and heart).</p><p>HaN is from <ref type="bibr" target="#b23">[24]</ref> and contains 120 CT images covering the head and neck region. We report the average DSC on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right optical nerve, left parotid, right parotid, left submandibular gland, and right submandibular gland).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>All images are re-sampled to have spacing of 2.5 mm × 1.0 mm × 1.0 mm, with respect to the depth, height, and width of the 3D volume. In the pre-training stage, we apply elastic transform to formulate positive samples. In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> (for MAE <ref type="bibr" target="#b9">[10]</ref>, we use ViT-base <ref type="bibr" target="#b6">[7]</ref>.) encoder E g for 200 epochs. In the local contrast branch, we use the Adam optimizer to pre-train both encoder E l and decoder D l for 30 epochs. The dimension of sampled vectors D is 64 since f q and f p have 64 channels. In the fine-tuning stage, we use the Adam optimizer to train the whole framework in an end-to-end fashion. All optimizers in both pre-training and fine-tuning stages are set to have momentum of 0.9 and weight decay of 10 -4 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Quantitative Results</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we select 9 self-supervised pre-trained with 1 ImageNet supervised pre-trained networks and combine with our proposed localized region contrast (LRC). Through extensive experiments on 3 different datasets, we demonstrate LRC is capable of enhancing these pre-training algorithms in a consistent way. We use Sørensen-Dice coefficient (DSC) to measure our experimental results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Qualitative Results</head><p>In Fig. <ref type="figure" target="#fig_2">2</ref>, we show segmentation results on ABD-110, Thorax-85, and HaN datasets respectively. All the results are provided by models trained with target dataset size |X T | = 10. By comparing (c) with (g) and (d) with (h), our method shows significant improvement, particularly on the challenging HaN dataset. However, due to limited space, we are only capable of demonstrating selected global pre-training methods.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Visualization of Localized Regions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Ablation Study</head><p>Effect of Additional Parameters. Additional parameters do bring performance enhancement in machine learning. However, in Number of Samples N . In Table <ref type="table" target="#tab_3">3</ref>, we explore the effect of different number of samples N to the contrastive sampling loss. When the sample mean f k is only averaged from a small number of vectors, the capability of representing certain region level can be limited. In the opposite, when the number of samples N is large, the sampling bias can be high, since the number of pixels can be smaller than N . Therefore, we need a proper choice of N . With N = 50, our method demonstrates the best DSC score of 0.732. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a contrastive learning framework, which integrates a novel localized contrastive sampling loss and enables the learning of finegrained representations that are crucial for accurate segmentation of complex structures. Through extensive experiments on three multi-organ segmentation datasets, we demonstrated that our approach consistently boosts current supervised and unsupervised pre-training methods. LRC provides a promising direction for improving the accuracy of medical image segmentation, which is a crucial step in various clinical applications. Overall, we believe that our approach can significantly benefit the medical image analysis community and pave the way for future developments in self-supervised learning for medical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our self-supervised framework with pre-training stage, consisting of global (with an example of MoCo) and our localized contrastive loss, and fine-tuning stage.</figDesc><graphic coords="4,57,48,54,02,337,78,257,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For ABD-110, LRC enhances 9 and 10 out of 10 pre-training approaches, with |X T | = 10 and 60 respectively. For thorax-85, LRC enhances 7 and 9 out of 10 pre-training approaches, with |X T | = 10 and 60 respectively. For HaN, LRC enhances 10 and 6 out of 10 pre-training approaches, with |X T | = 10 and 60 respectively. The experiments consistently show LRC boosts the multiorgan segmentation performance of most global contrast model across all three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ground truth segmentation masks and predictions on a slice from each dataset. Due to limited space, we only demonstrate selected global pre-training methods. By comparing (c) with (g) and (d) with (h), our method shows significant improvement, particularly on the challenging HaN dataset.</figDesc><graphic coords="7,68,79,134,27,286,75,351,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3 presents three pairs of localized region visualizations generated by Felzenszwalb's algorithm (with a black background) and the corresponding feature representations extracted from LRC. We use K-Means clustering to formulate these feature representations into K clusters, which are shown in purple in the figure. Our results demonstrate that LRC learns informative semantic feature representations that can be effectively clustered using a simple K-Means algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Three pairs of examples comparing the local regions generated by Felzenszwalb's algorithm on the left and K-Means clustering of the embeddings from the local contrast model on the right.</figDesc><graphic coords="8,55,98,410,21,340,12,50,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our proposed pre-training strategy combining local contrast with different pre-training methods. Models are fine-tuned on three datasets where |XT | is the number of labeled CT images, and the evaluation metric is Dice score. Bold numbers indicate corresponding global pre-training method is enhanced by LRC.</figDesc><table><row><cell>Global Pre-</cell><cell>ABD-110</cell><cell>Thorax-85</cell><cell>HaN</cell></row><row><cell>training Method</cell><cell cols="3">|XT |=10 |XT |=60 |XT |=10 |XT |=60 |XT |=10 |XT |=60</cell></row><row><cell></cell><cell cols="2">Supervised Pre-training on Natural Images</cell><cell></cell></row><row><cell>ImageNet[23]</cell><cell cols="3">70.9 72.7 77.6 78.6 87.2 88.1 89.4 90.3 67.5 72.6 77.0 77.9</cell></row><row><cell></cell><cell cols="2">Discriminative Self-supervised Pre-training</cell><cell></cell></row><row><cell>Relative Loc[6]</cell><cell cols="3">69.1 72.6 76.4 78.0 86.1 86.0 89.4 89.6 55.2 60.3 77.9 77.9</cell></row><row><cell cols="4">Rotation Pred[15] 69.1 70.2 77.0 78.1 86.3 84.6 89.2 89.7 54.7 59.6 76.8 77.3</cell></row><row><cell>MoCo v1[11]</cell><cell cols="3">72.1 75.3 77.7 79.0 86.6 88.6 89.3 90.1 52.3 70.6 76.0 77.2</cell></row><row><cell>MoCo v2[3]</cell><cell cols="3">72.2 75.2 77.9 79.6 86.6 87.4 89.7 90.0 55.8 68.2 76.7 77.5</cell></row><row><cell>BYOL[9]</cell><cell cols="3">71.6 74.8 78.0 79.0 87.3 88.2 89.2 89.5 53.1 61.1 76.3 76.6</cell></row><row><cell>DenseCL[26]</cell><cell cols="3">71.6 72.0 77.2 78.5 84.3 85.1 87.5 87.8 59.5 62.2 76.7 76.7</cell></row><row><cell>SimSiam[4]</cell><cell cols="3">73.4 76.0 79.2 79.5 88.2 87.0 88.6 89.9 57.2 63.2 78.9 77.2</cell></row><row><cell></cell><cell cols="2">Generative Self-supervised Pre-training</cell><cell></cell></row><row><cell cols="4">Models Genesis[36] 72.9 73.2 80.2 80.6 88.2 88.4 90.1 91.3 64.0 67.2 74.2 73.2</cell></row><row><cell>MAE[10]</cell><cell cols="3">71.5 71.2 79.5 79.7 86.2 86.5 89.3 89.0 52.8 55.2 77.3 77.5</cell></row></table><note><p>w/ or wo/ LRC w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ Random init 68.8 70.9 76.0 78.0 85.9 87.8 89 89.4 50.9 52.6 77.8 78.0</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of integrating LRC vs MoCo with different pre-training methods. We show LRC enhances global pre-training approaches by integrating localized features rather than simply adding additional parameters.</figDesc><table><row><cell cols="7">Global Pre-training Method ImageNet Relative Rotation MAE BYOL SimSiam</cell></row><row><cell></cell><cell>[23]</cell><cell cols="3">Loc [6] Pred [15] [10]</cell><cell>[9]</cell><cell>[4]</cell></row><row><cell>w/ MoCo [11]</cell><cell>78.4</cell><cell>77.1</cell><cell>77.3</cell><cell cols="2">79.5 78.2</cell><cell>79.5</cell></row><row><cell>w/ LRC</cell><cell>78.6</cell><cell>78.0</cell><cell>78.1</cell><cell cols="2">79.7 79.0</cell><cell>79.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,</head><label>2</label><figDesc>we show our proposed LRC enhances the performance of general global pre-training approaches by integrating localized features rather than simply adding additional parameters. We prove this argument by adding the same amount of MoCo pre-trained network parameters, to the above global pre-trained methods. As a result, LRC outperforms MoCo under every setting. In this experiment, we use ABD-110 as fine-tuning dataset and set |X T | = 60.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Different number of samples N largely influences the fine-tuning results. Results are provided by LRC + MoCo fine-tuned on ABD-110 with |XT | =10.</figDesc><table><row><cell>N</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell cols="4">DSC 0.695 0.732 0.717</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrastive learning of global and local features for medical image segmentation with limited annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep learning-based auto-segmentation system for organs-atrisk on whole-body computed tomography images for radiation therapy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiother. Oncol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised contrastive learning for labelefficient medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abdomenct-1k: is abdominal organ segmentation a solved problem?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="6695" to="6714" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-45" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-supervision with superpixels: training few-shot medical image segmentation without annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context encoders: feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning framework for organs at risk delineation in CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent mask refinement for fewshot medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3918" to="3928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Early convolutions help transformers see better</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Representation recovering for self-supervised pre-training on medical images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="2685" to="2695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">AFTer-UNet: axial fusion transformer U-Net for medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking semi-supervised medical image segmentation: a variancereduction perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01735</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bootstrapping semi-supervised medical image segmentation with anatomical-aware contrastive distillation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-34048-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-34048-249" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wassermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13939</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Momentum contrastive voxel-wise representation learning for semi-supervised volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-861" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SimCVD: simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transa. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2228" to="2237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-940" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101840</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101840" />
		<title level="m">Models genesis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
