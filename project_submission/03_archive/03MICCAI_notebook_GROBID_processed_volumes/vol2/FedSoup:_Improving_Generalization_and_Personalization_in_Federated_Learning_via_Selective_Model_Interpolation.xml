<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</title>
				<funder>
					<orgName type="full">Public Safety Canada</orgName>
				</funder>
				<funder ref="#_6CCh35z">
					<orgName type="full">Compute Canada and National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minghui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meirui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zehua</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
							<email>xiaoxiao.li@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="318" to="328"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7F1280C8AE49C9A9DD3A0F13D9A67E28</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Federated learning (FL) has emerged as a promising methodology for harnessing the power of private medical data without necessitating centralized data governance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. However, recent study <ref type="bibr" target="#b27">[28]</ref> has identified a significant issue in current FL algorithms, namely, the trade-off between local and global performance when encountering distribution shifts. This issue is particularly prevalent in medical scenarios <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, where medical images may undergo shifts of varying degrees due to differences in imaging device vendors, parameter settings, and the patient demographics. Personalized FL (PFL) techniques are typically utilized to address the data heterogeneity problem by weighing more on in-distribution (ID) data of each client. For instance, FedRep <ref type="bibr" target="#b4">[5]</ref> learns the entire network during local updates and keeps part of the network from global synchronization. However, they have a risk of overfitting to local data <ref type="bibr" target="#b22">[23]</ref>, especially when client local data is limited, and have poor generalizability on outof-distribution (OOD) data. Another line of work has studied the heterogeneity issue by regularizing the updates of local model. For instance, FedProx <ref type="bibr" target="#b14">[15]</ref> constraints local updates to be closer to the global model. An effective way to evaluate FL's generalizability is to investigate its performance on the joint global distribution following <ref type="bibr" target="#b27">[28]</ref>, which refers to testing the FL models on {D i }, where D i indicates client i's distribution <ref type="foot" target="#foot_0">1</ref> . Unfortunately, the existing works have not found the sweet spot between personalized (local) and consensus (global) models.</p><p>In this regard, we study a practical problem of enhancing personalization and generalization jointly in cross-silo FL for medical image classification when faced data heterogeneity. To this end, we aim to address the following two questions in FL: What could be the causes that result in local and global trade-off ? and How to achieve better local and global trade-off ? First, we provide a new angle to understand the trade-off. We reveal that over-personalization in FL can cause overfitting on local data and trap the model into a sharp valley of loss landscape (highly sensitive to parameter perturbation, see detailed definition in Sec. 2.2), thus limiting its generalizability. An effective strategy for avoiding sharp valleys in the loss landscape is to enforce models to obtain flat minima. In the centralized domain, weight interpolation has been explored as a means of seeking flat minima as its solution is moved closer to the centroid of the high-performing models, which corresponds to a flatter minimum <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>. However, research on these interpolation methods has been overlooked in FL.</p><p>With the above basis, we propose to track both local and global models during the federated training and perform model interpolation to seek the optimal balance. Our insight is drawn from the model soup method <ref type="bibr" target="#b26">[27]</ref>, which shows that averaging weights of multiple trained models with same initial parameters can enhance model generalization. However, the original model soup method requires training substantial models with varying hyper-parameters, which can be prohibitively time-consuming and costly in terms of communication during FL. Given the high communication cost and the inability to restart training from scratch in FL, we leverage global models at different time points within a single training session as the ingredients to adapt the model soup method <ref type="bibr" target="#b26">[27]</ref> to FL.</p><p>In this paper, we propose a novel federated model soup method (FedSoup) to produce an ensembled model from local and global models that achieve better local-global trade-off. We refer the 'soup' as a combo of different federated models. Our proposed FedSoup includes two key modules. The first one is temporal model selection, which aims to select suitable models to be combined into one. The second module is Federated model patching <ref type="bibr" target="#b9">[10]</ref>, which refers to a finetuning technique that aims to enhance personalization without compromising the already satisfactory global performance. For the first module, temporal model selection, we utilize a greedy model selection strategy based on the local validation performance. This avoids incorporating models that could be located in a different error landscape basin than the local loss landscape (shown in Fig. <ref type="figure" target="#fig_0">1</ref>). Consequently, each client possesses their personalized global model soups, consisting of a subset of historical global models that are selected based on their local validation sets. As for the second module In summary, our key contributions are as follows: (i) A novel FL method called Federated Model Soups (FedSoup) is proposed to improve generalization and personalization by promoting smoothness and seeking flat minima. (ii) A new temporal model selection mechanism is designed for FL, which maintains a client-specific model soups with temporal history global model to meet personalization requirements while not incurring additional training costs. (iii) An innovative federated model patching method between local and global models is introduced in federated client training to alleviate overfitting of local limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setup</head><p>Consider a cross-silo FL setting with N clients. Let D := {D i } N i=1 be a set of N training domain, each of which is a distribution over the input space X . For each client, we have access to n training data points in the form of (x i j , y i j ) n j=1 ∼ D i , where y i j denotes the target label for input x i j . We also define a set of unseen target domains T := {T i } N i in a similar manner, where N is the number of target domains and is typically set to one. The goal of personalization (local performance) is to find a model f ( </p><formula xml:id="formula_0">•; θ) via minimizing an empirical risk E Di (θ) := 1 n n j=1 (f (x i ; θ), y i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generalization and Flat Minima</head><p>In practice, ERM in deep neural networks, i.e., arg min θ E D (θ), can yield multiple solutions that offer comparable training loss, but vastly different levels of generalizability <ref type="bibr" target="#b2">[3]</ref>. However, without proper regularization, models are prone to overfit the training data and the training model will fall into a sharp valley of the loss surface, which is less generalizable <ref type="bibr" target="#b3">[4]</ref>.</p><p>One common reason for failures in ERM is the presence of variations in the data distribution (D i = D), which can cause a shift in the loss landscape. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the sharper the optimized minima, the more sensitive it is to shifts in the loss landscape. This results in an increase in generalization error. In cross-silo FL, each client may overfit their local training data, leading to poor global performance. This is due to the distribution shift problem, which creates conflicting objectives among the local models <ref type="bibr" target="#b22">[23]</ref>. Therefore, when the local model converges to a sharp minima, the higher the degree of personalization (local performance) of the model, the more likely it is to have poor generalization ability (global performance).</p><p>From the domain generalization formalization in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, the test loss E T (θ) can be bounded by the robust empirical loss E D (θ) as follows:</p><formula xml:id="formula_1">E T (θ) &lt; E D (θ) + 1 N N i=1 sup A∈A |P Di (A) -P T (A)| + ξ, (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where the sup A |P Di (A) -P T (A)| is a divergence between domain D i and T , A is the set of measurable subsets under D i and T , and ξ is the confidence bound term related to the radius and the number of the training samples.</p><p>From the Equation (1), we can infer that minimizing sharpness and seeking flat minima is directly related with the generalization performance on the unseen target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our Solution: FedSoup</head><p>After analyzing the aforementioned relationship between sharpness and generalization, we expound on the distinctive challenges of seeking flat minima and mitigating the trade-off between local and global performance in FL. Consequently, we introduce two refined modules as the ingredient of our proposed FedSoup solution. FedSoup only needs to modify the training method of the client, and the algorithm implementation is shown in Algorithm 1.</p><p>Temporal Model Selection. Stochastic Weight Averaging (SWA) <ref type="bibr" target="#b10">[11]</ref> and Sharpness-Aware Minimization (SAM) <ref type="bibr" target="#b7">[8]</ref> are two commonly used flat minima optimizers, which seek to find parameters in wide low-loss basins. In contrast to SAM, which incurs extra computational cost to identify the worst parameter perturbation, SWA is a more succinct and effective approach for implicitly favoring the flat minima by averaging weights. The SWA algorithm is motivated by the observation that SGD often finds high-performing models in the weight space but rarely reaches the central points of the optimal set. By averaging the parameter values over iterations, the SWA algorithm moves the solution closer to the centroid of this space of points. Nevertheless, when it comes to cross-silo FL training, the discrepancy between clients is significant, and models might lie in different basins. Merging all these models haphazardly is not effective and might hinder generalization. Recently, a selective weight averaging method called model soups <ref type="bibr" target="#b26">[27]</ref> was introduced to enhance the generalization of fine-tuned models. The original model soups is not applicable to the FL setting, requiring high communication costs and training compute. We adapt the idea to a new approach by leveraging global models trained at different time points in one pass of FL training. Additionally, considering the heterogeneity of data distribution in cross-silo FL and the requirement of personalization, we propose a model selection strategy where each client utilizes the performance of its local validation set as a monitoring indicator. We called this module temporal model selection (see Algorithm 1 Line 7-8) Federated Model Patching. According to previous analysis on the loss landscape, there is a loss landscape offset between different FL clients due to their domain discrepancy. Thus, simply integrating a global model can damage the model's personalization. To address this, we introduce the use of the model patching <ref type="bibr" target="#b9">[10]</ref>  without a large barrier of linear connectivity. <ref type="bibr" target="#b18">[19]</ref>. We called this module federated model patching. In summary, the update rule of FedSoup is implemented as follows:</p><formula xml:id="formula_3">θ F edSoup ← θ 1 g + • • • + θ k g + θ l k + 1 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where θ g is global model, θ l is local model, k is the number of selected global models. This update rule corresponds to Algorithm 1 Line 9.</p><p>Algorithm 1. FedSoup It is important to note that our proposed FedSoup algorithm requires only one carefully tuned hyper-parameter, namely the interpolation start epoch. To mitigate the risk of having empty global model soups when the start epoch is too late and to prevent potential performance degradation when the start epoch is too early, we have set the default interpolation start epoch to be 75% of the total training epochs, aligning with the default setting of SWA. Furthermore, it is worth mentioning that the modified model soup and model patching modules in our proposed FedSoup framework are interdependent. Model patching, which is a technique based on our modified model soup algorithm, provides an abundance of models to explore flatter minima and enhance performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset. We validate the effectiveness of our proposed method, FedSoup, on two medical image classification tasks. The first task involved the classification of pathology images from five different sources using Camelyon17 dataset <ref type="bibr" target="#b0">[1]</ref>, and each source is viewed as a client. The pathology experiment consists of a total of 4, 600 images 2 , each with a resolution of 96 × 96. The second task involved retinal fundus images from four different institutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, and each institute is viewed as a client. The retinal fundus experiment consists of a total of 1, 264 images, each with a resolution of 128 × 128. The objective of both datasets is to identify abnormal images from normal ones. We also maintained an equal number of samples from each client to prevent clients with more data from having a disproportionate influence on the global performance evaluation. Evaluation Setup. For each client, we take 75% of the data as the training set.</p><p>To assess the generalization ability and personalization of our model, we have constructed both local and global testing sets. Following <ref type="bibr" target="#b27">[28]</ref>, in our experimental setting, we first create a held-out global testing set by randomly sampling an equal number of images per source/institute, so its distribution is different from either of the client. The local testing dataset for each FL client is the remaining sample from the same source as its training set. The number of local testing set per client is approximately the same as that of the held-out global testing set. For the pathology dataset, as each subject can have multiple samples, we have ensured that data from the same subject only appeared in either the training or testing set. To align with the cross-validation setting for subsequent out-of-domain evaluations, we conducted a five-fold leave-one-client-data crossvalidation, with three repetitions using different random seeds in each fold. The results of the repeated experiments without hold-out client data are provided in the appendix. For PFL methods, we report the performance by averaging the results of each personalized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Training</head><p>Hyper-Paramters. We employ the ResNet-18 architecture as the backbone model. Our approach initiates local-global interpolation at the 75% training phase, consistent with the default hyper-parameter setting of SWA. We utilize the Adam optimizer with learning rate of 1e-3, momentum coefficients of 0.9 and 0.99 and set the batch size to 16. We set the local training epoch to 1 and perform a total of 1, 000 communication rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Art Methods</head><p>We compare our method with seven common FL and state-of-the-art PFL methods. Results in Table <ref type="table" target="#tab_1">1</ref> demonstrate that our FedSoup method achieves competitive performance on local evaluation sets while significantly improving generalization with respect to global performance. Furthermore, FedSoup exhibits greater stability with lower variance of performance across multiple experiments. Comparing the performance improvement of FedSoup across different datasets, we observed that FedSoup had a more substantial effect on the smaller retinal fundus dataset compared to the larger pathology dataset. In terms of performance gap compared to the second-best methods (FedBABU on Retina and FedProx on Pathology), our approach demonstrates a larger advantage on the Retina dataset. This observation suggests that our proposed method can mitigate the negative impact of local overfitting caused by small local datasets, thus improving the model's generalization ability. Sharpness Quantification. The sharpness measure used in this study calculates the median of the dominating Hessian eigenvalue across all training set batches using the Power Iteration algorithm <ref type="bibr" target="#b28">[29]</ref>. This metric indicates the maximum curvature of the loss landscape, which is often used in the literature on flat minima <ref type="bibr" target="#b12">[13]</ref> to reflect the sharpness. The median of the dominating Hessian eigenvalue of all clients in the retinal fundus dataset was measured in this part. Based on the Fig. <ref type="figure" target="#fig_4">2</ref>(a) presented, it is evident that the proposed method leads to flatter minima as compared to the other methods.</p><p>Trade-off at Different Personalized Levels. Following the evaluation in <ref type="bibr" target="#b27">[28]</ref>, we conducted an experiment comparing the local and globalperformance of different models at various levels of personalization using the retinal fundus datasets. We control the personalization level for the PFL methods by varying the number of iterations that the model undergoes fine-tuning using only the local training set after federated training. As we increase the number of finetuning iterations, we consider the level of personalization to be higher. We choose to show model performance after fine-tuning iterations 1, 7, and 15. The results in Fig. <ref type="figure" target="#fig_4">2</ref>(b) indicated that existing FL methods often have a trade-off between local and global performance. As the number of fine-tuning iterations increases, local performance typically improves but global performance decreases. Compared to other methods, our appraoch maintains high local performance while also preventing a significant drop in global performance, which remains much higher than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unseen Domain Generalization</head><p>We show the additional benefits of FedSoup on unseen domain generalization.</p><p>Setup. To evaluate the generalization of our method beyond the participating domains, we utilize one domain that did not take part in the distributed training and used its data as the evaluation set for unseen domain generalization. To this end, we perform leave-one-out cross-validation by having one client as the tobe-evaluated unseen set each time. To ensure a reliable results of unseen domain generalization, we conducted experiments on the Camelyon17 dataset, which has a larger number of samples.</p><p>Results. Overall, our proposed method demonstrates an advantage in terms of unseen domain generalization capabilities (see Fig. <ref type="figure" target="#fig_4">2(c</ref>)). In comparison to FedAvg, Our approach resulted in a 2.87-point increase in the AUC index for generalization to unseen domains on the pathology dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we demonstrate the trade-off between personalization and generalization in the current FL methods for medical image classification. To optimize this trade-off and achieve flat minima, we propose the novel FedSoup method. By maintaining personalized global model pools in each client and interpolating weights between local and global models, our proposed method enhances both generalization and personalization. FedSoup outperforms other PFL methods in terms of both generalization and personalization, without incurring any additional inference or memory costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of our method (FedSoup) compared with common PFL methods. PFL methods typically minimize local loss but suffering high global loss. While our federated model soup method balances both local and global loss by seeking flat minima. The black dots in the figure represent ellipsis and indicate multiple rounds of model upload and model training in between. Compared to previous pFL methods, our approach introduces global model selection modules and local model interpolation with the global model (referred to as model patching).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, federated model patching, it introduces model patching in local client training by interpolating the local model and the global model soups into a new local model, bridging the gap between local and global domains. It promotes the personalization of the model for ID testing and also maintains good global performance for OOD generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>over a local client training set D i , where (•, •) is a loss function. On the other hands, the objective of generalization (global performance) is to minimize both population loss E D (θ) and E T (θ) over multiple domains by Empirical Risk Minimization (ERM) E D (θ) := 1 Nn N i=1 n j=1 (f (x i ; θ), y i )) over all training clients' training sets D. In this work, we evaluate the local performance on local testing samples from D i , and evaluate the global performance on testing samples from the joint global distribution D := {D i } N i=1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(i.e. local and global model interpolation) during client-side local training in FL, aiming to improve model personalization and maintain the good global performance. Specifically, model patching approach forces local client not to distort global model severely and seek low-loss interpolated model between local and global, encouraging the local and global model lie in the same basin</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Analysis of our approach: (a) sharpness quantification on the retina dataset, (b) local and global trade-off under different personalized levels (fine-tuning epochs), (c) unseen domain generalization on the pathology dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: global model θg, local model θ l , last epoch local model θ i l , number of clients k, number of epochs n, interpolation start epoch E.</figDesc><table><row><cell cols="2">2: soup ← {}</cell><cell></cell></row><row><cell cols="2">3: for i = 0 to n do</cell><cell></cell></row><row><cell>4:</cell><cell>θg ← Aggregation( θ 1 l , . . . , θ k l )</cell><cell></cell></row><row><cell>5:</cell><cell>θ l ← ClientUpdate(θg)</cell><cell></cell></row><row><cell>6:</cell><cell>if i ≥ E then</cell><cell></cell></row><row><cell>7:</cell><cell cols="2">if ValAcc(average(soup ∪ {θ l } ∪ {θg})) ≥ ValAcc(average(soup ∪ {θ l })) then</cell></row><row><cell>8:</cell><cell>soup ← {θg}</cell><cell>{ Module 1: Temporal Model Selection}</cell></row><row><cell>9:</cell><cell>θ</cell><cell></cell></row></table><note><p><p>1: l ← average(soup ∪ {θ l })</p>{Module 2: Federated Model Patching} 10: return θ l</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Local and global performance results comparison with SOTA PFL methods on two medical image classification tasks. 90.44 (0.42) 70.18 (1.25) 78.74 (1.31) 89.81 (1.17) 95.24 (0.73) 73.27 (3.97) 81.02 (4.57) FedProx [15] 86.34 (0.52) 92.78 (0.36) 67.42 (1.32) 77.18 (1.32) 90.14 (0.20) 95.47 (0.63) 70.46 (2.36) 76.84 (2.93) MOON [14] 85.71 (0.42) 91.98 (0.34) 70.61 (1.54) 79.27 (1.43) 89.14 (0.77) 94.91 (0.98) 77.49 (3.15) 84.89 (2.70) FedBN [17] 82.32 (0.87) 90.07 (0.77) 65.16 (0.69) 71.61 (0.97) 89.70 (1.45) 95.49 (0.56) 75.11 (0.52) 82.70 (1.01) FedFomo [30] 80.99 (1.07) 86.51 (0.99) 61.00 (0.59) 61.69 (0.78) 89.70 (0.00) 94.88 (0.45) 59.90 (2.00) 63.82 (1.14) FedRep [5] 82.50 (0.53) 89.77 (0.43) 66.87 (0.60) 72.34 (0.75) 89.38 (0.89) 95.16 (0.83) 71.81 (1.79) 79.09 (2.44) FedBABU [20] 85.18 (0.33) 92.39 (0.29) 69.56 (1.40) 77.26 (1.48) 90.25 (1.39) 95.10 (0.33) 77.65 (0.24) 85.09 (0.21)</figDesc><table><row><cell>Method</cell><cell>Pathology</cell><cell></cell><cell>Retinal Fundus</cell></row><row><cell></cell><cell>Local Performance</cell><cell>Global Performance</cell><cell>Local Performance</cell><cell>Global Performance</cell></row><row><cell></cell><cell>Accuracy ↑ AUC ↑</cell><cell>Accuracy ↑ AUC ↑</cell><cell>Accuracy ↑ AUC ↑</cell><cell>Accuracy ↑ AUC ↑</cell></row><row><cell>FedAvg [18]</cell><cell>82.41 (0.61)</cell><cell></cell><cell></cell></row></table><note><p>FedSoup 85.71 (0.37) 92.47 (0.31) 72.87 (1.35) 81.45 (1.40) 90.92 (0.50) 96.00 (0.43) 78.64 (0.90) 86.24 (0.86)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the heterogeneous setting (Di = Dj), Dj is viewed as the OOD data for client i.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We take a random subset from the original Camelyon17 dataset to match the small data settings in FL<ref type="bibr" target="#b17">[18]</ref>.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs>, <rs type="funder">Public Safety Canada</rs>, <rs type="funder">Compute Canada and National Natural Science Foundation of China</rs> (Project No. <rs type="grantNumber">62201485</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6CCh35z">
					<idno type="grant-number">62201485</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 30.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From detection of individual metastases to classification of lymph node status at the patient level: the CAMELYON17 challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bándi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SWAD: domain generalization by seeking flat minima</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="22405" to="22418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entropy-SGD: biasing gradient descent into wide valleys</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting shared representations for personalized federated learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakkottai</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2089" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Federated learning for predicting clinical outcomes in patients with covid-19</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1735" to="1743" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Federated deep learning for detecting covid-19 lung abnormalities in CT: a privacypreserving multinational validation study</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">RIM-ONE: an open retinal image database for optic nerve evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fumero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alayón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sigut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>González-Hernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
		<respStmt>
			<orgName>CBMS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Patching open-vocabulary models by interpolating weights</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<idno>CoRR abs/2208.05592</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AUAI Press</publisher>
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic bank learning for semi-supervised federated image diagnosis with class imbalance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-819" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="196" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Questions for flat-minima optimization of modern neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<idno>CoRR abs/2202.00661</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Model-contrastive federated learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Computer Vision Foundation/IEEE</publisher>
			<biblScope unit="page" from="10713" to="10722" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSys. mlsys.org</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-site FMRI analysis using privacy-preserving federated learning and domain adaptation: abide results</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ventola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101765</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FedBN: federated learning on non-IID features via local batch normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">AISTATS. Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linear mode connectivity in multitask and continual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Görür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fedbabu: Towards enhanced representation for federated image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<idno>CoRR abs/2106.06042</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">REFUGE challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Federated learning enables big data for rare cancer boundary detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7346</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized federated learning via sharpness aware minimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="18250" to="18280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recycling diverse models for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>CoRR abs/2212.10445</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The future of digital health with federated learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tabish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSM Biomed. Imaging Data Papers</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1004</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML. Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="23965" to="23998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Motley: benchmarking heterogeneity and personalization in federated learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR abs/2206.09262</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PyHessian: neural networks through the lens of the hessian</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalized federated learning with first order model optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
