<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation</title>
				<funder>
					<orgName type="full">Intel R&amp;D UK</orgName>
				</funder>
				<funder ref="#_sqqsnN6">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder ref="#_RFd2fGc">
					<orgName type="full">Royal Society</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alfie</forename><surname>Roddan</surname></persName>
							<email>a.roddan21@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Surgery and Cancer</orgName>
								<orgName type="institution">The Hamlyn Centre for Robotic Surgery</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Xu</surname></persName>
							<email>chi.xu20@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Surgery and Cancer</orgName>
								<orgName type="institution">The Hamlyn Centre for Robotic Surgery</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serine</forename><surname>Ajlouni</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">University Witten Herdecke</orgName>
								<address>
									<settlement>Witten</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irini</forename><surname>Kakaletri</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Medical Faculty</orgName>
								<orgName type="department" key="dep2">Rheinische Friedrich Wilhelms</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patra</forename><surname>Charalampaki</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Medical Faculty</orgName>
								<orgName type="institution">University Witten Herdecke</orgName>
								<address>
									<settlement>Witten</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Cologne Medical Center</orgName>
								<address>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stamatia</forename><surname>Giannarou</surname></persName>
							<email>stamatia.giannarou@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Surgery and Cancer</orgName>
								<orgName type="institution">The Hamlyn Centre for Robotic Surgery</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="575" to="585"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">18612F60432BEB11624838D3420F1BBD</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainability</term>
					<term>Uncertainty</term>
					<term>MC Dropout</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For the surgeon to trust the model, explainability of the generated predictions needs to be provided. For image classification models, pixel attribution (PA) and risk estimation are popular methods to infer explainability. However, the former method lacks trustworthiness while the latter can not provide visual explanation of the model's attention. In this paper, we propose the first approach which incorporates risk estimation into a PA method for improved and more trustworthy image classification explainability. The proposed method iteratively applies a classification model with a PA method to create a volume of PA maps. We introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data verifies that our improved explainability method outperforms the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When using a Machine Learning (ML) model during intraoperative tissue characterisation, it is vital that the surgeon is able to assess how reliable a model's prediction is <ref type="bibr" target="#b7">[8]</ref>. For the surgeon to trust the output predictions of the model, the model must be able to explain itself reliably in a clinical scenario <ref type="bibr" target="#b1">[2]</ref>. To assess an explainability method we consider five metrics of performance: speed, usability, generalisability, trustworthiness and ability to localise semantic features. The explanation of a model's predictions is trustworthy if small perturbations in the input or model parameters, results in a similar output explanation. One form of explainability in the image classification domain is pixel attribution (PA) mapping. PA maps aim to highlight the "most important" pixels to the classification. PA maps can be used to visually highlight whether a model is poorly extracting semantic features <ref type="bibr" target="#b31">[32]</ref> and/or that the model is misinformed due to spurious correlations within the data that it was trained on <ref type="bibr" target="#b15">[16]</ref>. To efficiently process image data, these methods mainly rely on Convolutional Neural Networks (CNNs) and achieve state-of-the-art (SOTA) performance. One of the first PA methods proposed for CNNs was class activation maps (CAM) <ref type="bibr" target="#b32">[33]</ref>. CAM uses one forward pass of the model to find the channels in the last convolutional layer that contributed most to the prediction. One of CAM's limitations is its reliance on global average pooling (GAP) <ref type="bibr" target="#b20">[21]</ref> after the last convolutional layer as it dramatically reduces the number of architectures that can use CAM. To improve on this, Grad-CAM <ref type="bibr" target="#b29">[30]</ref> generalises to all CNN architectures which are differentiable from the output logit layer to the chosen convolutional layer. However, Grad-CAM often lacks sharpness in object localisation, as noted and improved on in Grad-CAM++ <ref type="bibr" target="#b5">[6]</ref> and SmoothGrad-CAM++ <ref type="bibr" target="#b23">[24]</ref>. These extensions of Grad-CAM have good semantic feature localisation but they are unable to be deployed for use in surgery <ref type="bibr" target="#b4">[5]</ref>. Both Score-CAM <ref type="bibr" target="#b30">[31]</ref> and Recipro-CAM <ref type="bibr" target="#b4">[5]</ref> also generalise to all CNN architectures but are deployable. Score-CAM improves on object localisation within the visual PA map without losing the class specific capabilities of Grad-CAM by masking out regions of the image and measuring the change in the output score. This is similar to perturbation methods like RISE <ref type="bibr" target="#b25">[26]</ref>, LIME <ref type="bibr" target="#b27">[28]</ref> and other perturbation techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. On the other hand, Recipro-CAM focuses on the speed of PA map computation whilst maintaining comparable SOTA performance. By utilising the CNN's receptive field, Recipro-CAM generates a number of spatial masks and then measures the effect on the output score much like Score-CAM.</p><p>Despite being speedy, easy to deploy and able to localise semantic features, the above methods lack trustworthiness due to the training strategy of their underlying model. Deep learning (DL) models trained with empirical risk minimisation (ERM) are overconfident in prediction <ref type="bibr" target="#b11">[12]</ref> and vulnerable to adversarial attacks <ref type="bibr" target="#b12">[13]</ref>. Bayesian Neural Networks (BNNs) <ref type="bibr" target="#b22">[23]</ref> bring improved regularisation and output uncertainty estimates. Unfortunately, the non-linearity and number of variables within NNs make Bayesian inference a computationally intensive task. For this reason, variational methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> are used to approximate Bayesian inference. More recently, the variational method Bayes by Backprop <ref type="bibr" target="#b3">[4]</ref> used Dropout <ref type="bibr" target="#b18">[19]</ref> to approximate Bayesian inference. Dropout is a regularisation technique which has also been noted to improve salient feature extraction. Although Bayes by Backprop is trustworthy, it often fails to scale to the complex architectures of SOTA models. To improve on this lack of generalisability, another variational method called Monte Carlo (MC) Dropout <ref type="bibr" target="#b11">[12]</ref>  proposes that a model trained with Dropout is equivalent to a probabilistic deep Gaussian process <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. With this assumption, an estimated output distribution is computed after a number of forward passes with Dropout have been applied. This output distribution is used in practice to indicate risk in the model's predictions. Surgeons in practice can use this risk during diagnosis to trust the model for decision making <ref type="bibr" target="#b13">[14]</ref>. Using Dropout to perturb a model is a computationally cheap method of model averaging <ref type="bibr" target="#b18">[19]</ref>. It is worth noting though that this method's validity as a Bayesian Inference approximation was later questioned <ref type="bibr" target="#b9">[10]</ref>. However, this does not affect the use of this method for risk estimation. So far, model explainability and risk estimation have mostly been used separately to assess models' suitability for surgical applications. DistDeepSHAP <ref type="bibr" target="#b19">[20]</ref> computed the uncertainty of Shapley values to show uncertainty in explainability maps. However, DistDeepSHAP is a model-agnostic interpretability method that shows the global effect of perturbing inputs, instead of providing an insight to the model's learned representations. The aim of this paper is to show that the fusion of MC Dropout and PA methods leads to improved explainability.</p><p>In this paper, we propose the first approach which incorporates risk estimation into a PA method. A classification model is trained with Dropout and a PA method is used to generate a PA map. At test time, the classification model is employed with the Dropout enabled. In this work, we propose to repeat this process for a number of iterations creating a volume of PA maps. This volume is used to generate a pixel-wise distribution of PA values from which we can infer risk. More specifically, we introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. This provides an improved explanation of the model's prediction by clearly presenting to the surgeon which salient areas to trust in the model's enhanced PA map. In this work, we focus on the explainability of the classification of brain tumours using probe-based Confocal Laser Endomicroscopy (pCLE) data. Performance evaluation on pCLE data shows that our improved explainability method outperforms the SOTA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The aim of the proposed method is to produce an improved PA map of a classification model, while providing risk estimation of the model's explainability to enhance trustworthiness in decision making during intraoperative tissue characterisation.</p><p>In our method, any CNN classification model trained with Dropout can be used. Let Ŷ be the output logits of the CNN model, where Dropout is enabled at test time, with input image X ∈ R height×width×channels . Any PA method can be used to generate a PA map using the output logits S = f s ( Ŷ ) ∈ R height×width where f s (.) is the PA method. We propose to repeat the above process for T iterations to create a volume of PA maps S = {S 1 , ..., S T } ∈ R height×width×T . A visual representation of how the volume is generated is show in Fig. <ref type="figure" target="#fig_1">2</ref>. The aim is to use this volume to generate a pixel-wise distribution of PA values from which we can infer risk. To achieve this, we compute the expectation and variance values of the volume along the third dimension as:</p><formula xml:id="formula_0">E(S i,j ) ≈ 1 T T t=1 f s ( Ŷt ) i,j V ar(S i,j ) ≈ 1 T T t=1 f s ( Ŷt ) T i,j f s ( Ŷt ) i,j -E(S i,j ) T E(S i,j ),<label>(1)</label></formula><p>where, i, j represent the pixel's row and column coordinates, respectively. The expectation E(S i,j ) of each pixel (i, j) is used to generate an enhanced PA map of size height × width. The intuition is that the above distribution of PA values can produce less noisy and risky estimations of a pixel's contribution to the final explainability map compared to a single estimate.</p><p>As well as advancing SOTA PA methods, our method also estimates the trustworthiness of the enhanced PA map generated above. For risk estimation, it is important to consider that different pixels in the PA map correspond to different semantic features which contribute differently to the output logits. This makes the pixel-wise distributions have different scales. For this reason, the coefficient of variation (CV) is used to estimate pixel-wise risk, as it allows us to compare pixel-wise variances despite their different scales. This is mathematically defined as:</p><formula xml:id="formula_1">S cv i,j = V ar(S i,j ) E(S i,j ) = std(S i,j ) E(S i,j ) . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Our proposed method improves trustworthiness of explainability as it allows visualisation of both the explainability of the classification model (provided by the enhanced PA map) together with the pixel-wise risk of this map (provided by the CV map). For instance, salient areas on the PA map should not be trusted unless the CV values are low. An example of the enhanced PA and risk maps generated with the proposed method are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. This shows that the proposed method not only improves explainability but also provides associated risk information which improves trustworthiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Analysis</head><p>Dataset. The developed explainability framework has been validated on an in vivo and ex vivo pCLE dataset of meningioma, glioblastoma and metastases of an invasive ductal carcinoma (IDC). All studies on human subjects were performed according to the requirements of the local ethic committee and in agreement with the Declaration of Helsinki (No. CLE-001 Nr: 2014480). The Cellvizio c by Mauna Kea Technologies, Paris, France has been used in combination with the mini laser probe CystoFlex c UHD-R. The distinguishing characteristic of the meningioma is the psammoma body with concentric circles that show various degrees of calcification. Regarding glioblastomas, the pCLE images allow for the visualization of the characteristic hypercellularity, evidence of irregular nuclei with mitotic activities or multinuclear appearance with irregular cell shape. When examining metastases of an IDC, the tumor presents as egg-shaped cells with uniform evenly spaced nuclei. Our dataset includes 38 meningioma videos, 24 glioblastoma and 6 IDC. Each pCLE video represents one tumour type and corresponds to a different patient. The data has been curated to remove noisy images and similar frames. This resulted in a training dataset of 2500 frames per class (7500 frames in total) and a testing dataset of the same size. The dataset is split into a training and testing subset, with the division done on the patient level.</p><p>Implementation. To implement the DL models we use the open-source framework PyTorch <ref type="bibr" target="#b24">[25]</ref> and a NVIDIA Geforce RTX 3090 graphics card for parallel computation. To show our method generalises we trained two lightweight models: ResNet-18 <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 0.01 and MobileNetV2 <ref type="bibr" target="#b28">[29]</ref> with a learning rate of 0.001. Both were trained using the Adam-W <ref type="bibr" target="#b21">[22]</ref> optimiser with a weight decay of 0.01 and Dropout probability 0.1. We report the model's Top-1 accuracy for Resnet18 as 94.0% and for MobileNet as 86.6%. At test time, we set T = 100 to create a fair distribution of PA maps. PA methods were implemented with the help of TorchCAM <ref type="bibr" target="#b8">[9]</ref> and ReciproCAM was implemented using the authors' source code.</p><p>Evaluation Metrics. Evaluating a PA method is not a trivial task because a PA map may not need to be inline with what a human deems "reasonable" <ref type="bibr" target="#b0">[1]</ref>. Segmentation scores like intersection over union (IoU) may be used with caution to compare thresholded PA maps to ground truth maps with annotated salient regions. By doing so, we can measure how informed the model is about a particular class. To quantify how misinformed a model is, we can estimate at its average drop <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_3">AverageDrop(f s , Ŷ , X) = max(0, Ŷ (X) -Ŷ ( X)) Ŷ (X) ,<label>(3)</label></formula><p>where, X = X f s ( Ŷ (X). The above equation measures the effect on the output score of the classification model if we only include the pixels which the PA method scored highly. A minimum average drop is desired.</p><p>As average drop was found to not be sufficient on its own, the unified method ADCC <ref type="bibr" target="#b26">[27]</ref> was introduced which is the harmonic mean of average drop, coherency and complexity, defined as:</p><formula xml:id="formula_4">ADCC(f s ( Ŷ )) =3(<label>1</label></formula><formula xml:id="formula_5">Coherency(f s ( Ŷ )) + 1 1 -Complexity(f s ( Ŷ )) + 1 1 -AverageDrop(f s , Ŷ , X) ) -1 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Coherency is the Pearson Correlation Coefficient which ensures that the remaining pixels after dropping are still important, defined as:</p><formula xml:id="formula_7">Coherency(f s ( Ŷ )) = Cov(f s ( Ŷ ( X)), f s ( Ŷ )) σ(f s ( Ŷ ( X))σ(f s ( Ŷ )) ,<label>(5)</label></formula><p>where, Cov(., .) is the covariance and σ is the standard deviation. A higher coherency is better. Complexity is the L1 norm of the output PA map.</p><formula xml:id="formula_8">Complexity(f s ( Ŷ ))) = ||f s ( Ŷ ))|| 1 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Complexity is used to measure how cluttered a PA map is. For a good PA map, complexity should be a minimum. As it has been shown in the literature, the metrics in Eqs. ( <ref type="formula" target="#formula_3">3</ref>), ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula" target="#formula_8">6</ref>), can not be used individually to evaluate a PA method <ref type="bibr" target="#b26">[27]</ref>. ADCC combined with computation time gives us a reliable overall metric of how a PA method is performing. Performance Evaluation. The proposed method has been compared to combinations of ResNet18 and MobileNetV2 with SOTA PA methods. At test time, Dropout is not enabled for these standard methods, it is only enabled for our method. In Table <ref type="table" target="#tab_0">1</ref>, we show that our method outperforms all the compared CNN-PA method combinations on ADCC. The Dropout version of ScoreCAM is too computationally expensive and therefore is not included in our comparison. We believe that the better performance of our method is because of the random dropping of features taking place during Dropout at test time which helps to suppress noise in the estimated enhanced PA map. The combination of Recipro-CAM with our proposed method improves performance (increases ADCC) at the expense of increasing the computational complexity. We believe that this could be reduced using a batched implementation of Recipro-CAM. We attribute slow down in SmoothGradCAM++ when Dropout is applied during test time to the perturbations it adds on top of the PA method. Our validation study shows that Grad-CAM, Grad-CAM++ and Recipro-CAM are often leading in terms of speed as expected from the literature. In Fig. <ref type="figure" target="#fig_0">1</ref>, we can see our proposed method reduces noise in the PA map around the salient region. The distinguishing characteristic of the meningioma is the psammoma body which is highlighted by all the PA methods. Risk estimations from Eq. ( <ref type="formula" target="#formula_1">2</ref>) are also displayed and provide an added visualisation for a surgeon to trust the model. As it can be seen, areas of low CV match the areas of high PA values which verifies the trustworthiness of our method. We believe that the proposed explainability method could be used to support the surgeon intraoperatively in diagnosis and decision making during tumour resection. The enhanced PA map extracted with our method highlights the areas which were the most important to the model's prediction. When these areas correlate with clinically relevant areas, it shows that the model has learned to robustly classify the different tissue classes. Hence, it can be trusted by the surgeon for diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work we have introduced the first combination of risk in an explainability method. Using our proposed framework we not only improve on all the tested SOTA PA method's ADCC performances but also produce an estimation of risk on the output PA values. The proposed method can clearly present to the surgeon areas of the explainability map that are more trustworthy. From this work we hope to encourage trust between the surgeon and DL models. For future work, we plan to reducing the computation time of our method and deploy the proposed framework for use in surgery.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. PA maps generated using ResNet18 on meningioma pCLE data. a) Score-CAM PA map b) Grad-CAM PA map c) Grad-CAM PA map with our method applied d) Risk map (CV values) of c) e) Meningioma with the salient region highlighted with red bounding box f) Recipro-CAM PA map g) Recipro-CAM PA map with our method applied h) Risk map (CV values) of f). Yellow represents the highest PA value and black the lowest. (Color figure online)</figDesc><graphic coords="3,60,96,74,57,330,13,202,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Outline of the proposed method. A PA volume is generated using T forward passes of a CNN model with Dropout applied.</figDesc><graphic coords="4,49,80,278,63,321,04,110,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. PA maps generated using ResNet18 on pCLE data. a) Metastasised IDC b) Grad-CAM++ PA map on a) c) Grad-CAM++ PA map with our method applied on a) d) Risk map (CV values) of c) e) Glioblastoma f) SmoothGrad-CAM++ PA map on e) g) SmoothGrad-CAM++ PA map using our proposed method on e) h) Risk map (CV values) of g). Yellow represents the highest PA value and black the lowest. (Color figure online)</figDesc><graphic coords="6,43,80,72,53,336,55,168,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance evaluation study based on the ADCC and time metrics. Coh is Coherence, Comp is Complexity, AD is average drop and each of these are reported for completeness. Time(s) is the average time to compute one PA map using a batch size of one. All metrics are run over the validation set and averaged. ScoreCAM with the proposed method takes &gt;5 s per batch so was omitted due to resource constraints.</figDesc><table><row><cell cols="2">Architecture PA method</cell><cell cols="5">Coh ↑ Comp ↓ AD ↓ ADCC ↑ Time(s) ↓</cell></row><row><cell>ResNet18</cell><cell cols="2">Standard -single iteration</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Grad-CAM</cell><cell>90.1</cell><cell>32.7</cell><cell cols="2">10.1 76.6</cell><cell>0.006</cell></row><row><cell></cell><cell>Grad-CAM++</cell><cell>90.6</cell><cell>33.1</cell><cell cols="2">10.6 76.2</cell><cell>0.006</cell></row><row><cell></cell><cell cols="2">SmoothGradCAM++ 88.3</cell><cell>27.6</cell><cell cols="2">14.3 74.8</cell><cell>0.065</cell></row><row><cell></cell><cell>Score-CAM</cell><cell>90.0</cell><cell>32.3</cell><cell>5.9</cell><cell>80.5</cell><cell>0.124</cell></row><row><cell></cell><cell>Recipro-CAM</cell><cell cols="2">91.0 41.2</cell><cell cols="2">10.0 72.8</cell><cell>0.007</cell></row><row><cell></cell><cell>Proposed method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Grad-CAM</cell><cell>97.0</cell><cell>34.2</cell><cell cols="2">11.8 77.7</cell><cell>0.079</cell></row><row><cell></cell><cell>Grad-CAM++</cell><cell>93.4</cell><cell>32.6</cell><cell cols="2">12.5 78.2</cell><cell>0.081</cell></row><row><cell></cell><cell cols="2">SmoothGradCAM++ 92.4</cell><cell>30.7</cell><cell cols="2">17.0 75.8</cell><cell>0.463</cell></row><row><cell></cell><cell>Score-CAM</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Recipro-CAM</cell><cell>92.2</cell><cell>37.9</cell><cell cols="2">11.3 76.1</cell><cell>0.420</cell></row><row><cell cols="3">MoblieNetV2 Standard -single iteration</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Grad-CAM</cell><cell>82.9</cell><cell>21.3</cell><cell cols="2">73.8 29.3</cell><cell>0.010</cell></row><row><cell></cell><cell>Grad-CAM++</cell><cell>86.2</cell><cell>30.0</cell><cell cols="2">66.9 37.8</cell><cell>0.010</cell></row><row><cell></cell><cell cols="2">SmoothGradCAM++ 77.7</cell><cell>18.1</cell><cell cols="2">76.2 24.5</cell><cell>0.072</cell></row><row><cell></cell><cell>Score-CAM</cell><cell>62.5</cell><cell>33.9</cell><cell cols="2">56.3 43.9</cell><cell>0.324</cell></row><row><cell></cell><cell>Recipro-CAM</cell><cell>85.8</cell><cell>32.3</cell><cell cols="2">67.1 35.8</cell><cell>0.008</cell></row><row><cell></cell><cell></cell><cell cols="3">Proposed method</cell><cell></cell></row><row><cell></cell><cell>Grad-CAM</cell><cell>90.0</cell><cell>27.0</cell><cell cols="2">59.4 48.0</cell><cell>0.103</cell></row><row><cell></cell><cell>Grad-CAM++</cell><cell>91.4</cell><cell>35.9</cell><cell cols="2">41.5 59.7</cell><cell>0.105</cell></row><row><cell></cell><cell cols="2">SmoothGradCAM++ 89.8</cell><cell>22.1</cell><cell cols="2">71.0 37.5</cell><cell>0.322</cell></row><row><cell></cell><cell>Score-CAM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Recipro-CAM</cell><cell>90.6</cell><cell>33.7</cell><cell cols="2">48.3 55.9</cell><cell>0.674</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">Engineering and Physical Sciences Research Council</rs> (<rs type="grantNumber">EP/T51780X/1</rs>) and <rs type="funder">Intel R&amp;D UK</rs>. <rs type="person">Dr Giannarou</rs> is supported by the <rs type="funder">Royal Society</rs> (<rs type="grantNumber">URF\R\201014</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sqqsnN6">
					<idno type="grant-number">EP/T51780X/1</idno>
				</org>
				<org type="funding" xml:id="_RFd2fGc">
					<idno type="grant-number">URF\R\201014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<title level="m">Sanity Checks for Saliency Maps</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explainability for artificial intelligence in healthcare: a multidisciplinary perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Amann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blasimme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vayena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Madai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards better understanding of gradient-based attribution methods for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Weight Uncertainty in Neural Networks</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recipro-CAM: gradient-free reciprocal class activation map</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-09">Sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Grad-CAM++: improved Visual Explanations for Deep Convolutional Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Deep Gaussian Processes</title>
		<imprint>
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Physician understanding, explainability, and trust in a hypothetical machine learning risk calculator</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Diprose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Buist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Thurier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Association</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Fernandez</surname></persName>
		</author>
		<title level="m">TorchCAM: class activation explorer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Is MC Dropout Bayesian?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10">Oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dropout as a Bayesian Approximation: Appendix</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for safe intraoperative decision support</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grantcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Surg</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1064" to="1065" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Practical Variational Inference for Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Identifying Spurious Correlations and Correcting them with an Explanation-based Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Hagos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mac Namee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-11">Nov 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Keeping neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ventola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Efficient Shapley Explanation for Features Importance Estimation Under Uncertainty</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network In Network</title>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<title level="m">Bayesian Learning for Neural Networks</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Smooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Omeiza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weldermariam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">RISE: Randomized Input Sampling for Explanation of Black-box Models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Revisiting The Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Why Should I Trust You?Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-02">Feb 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018-01">Jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-11">Nov 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Learning Deep Features for Discriminative Localization</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
