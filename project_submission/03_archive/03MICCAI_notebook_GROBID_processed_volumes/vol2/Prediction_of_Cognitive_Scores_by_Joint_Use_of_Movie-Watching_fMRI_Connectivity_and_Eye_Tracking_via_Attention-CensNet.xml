<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet</title>
				<funder ref="#_F8p8RaV">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_HEadjzH #_PP3QNrr #_XejPaf3 #_j8QE2gR #_xcpek2V #_f96pYKX #_hZ5BRH6 #_5fAzGz3">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_qAhzAKV">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ksd4F3Y">
					<orgName type="full">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaxing</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Cortical Architecture Imaging and Discovery Lab</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Bioimaging Research Center</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyang</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhibin</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaonai</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianming</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Cortical Architecture Imaging and Discovery Lab</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Bioimaging Research Center</orgName>
								<orgName type="institution">The University of Georgia</orgName>
								<address>
									<settlement>Athens</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tuo</forename><surname>Zhang</surname></persName>
							<email>tuozhang@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="287" to="296"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6E1B92491F8042BAE5ADC1EF7C9B6099</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Functional Connectivity</term>
					<term>Naturalistic Stimulus</term>
					<term>Eye Movement</term>
					<term>CensNet</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Brain functional connectivity under the naturalistic paradigm has been demonstrated to be better at predicting individual behaviors than other brain states, such as rest and task. Nevertheless, the state-of-the-art methods are difficult to achieve desirable results from movie-watching paradigm fMRI(mfMRI) induced brain functional connectivity, especially when the datasets are small, because it is difficult to quantify how much useful dynamic information can be extracted from a single mfMRI modality to describe the state of the brain. Eye tracking, becoming popular due to its portability and less expense, can provide abundant behavioral features related to the output of human's cognition, and thus might supplement the mfMRI in observing subjects' subconscious behaviors. However, there are very few works on how to effectively integrate the multimodal information to strengthen the performance by unified framework. To this end, an effective fusion approach with mfMRI and eye tracking, based on Convolution with Edge-Node Switching in Graph Neural Networks (CensNet), is proposed in this article, with subjects taken as nodes, mfMRI derived functional connectivity as node feature, different eye tracking features used to compute similarity between subjects to construct heterogeneous graph edges. By taking multiple graphs as different channels, we introduce squeeze-and-excitation attention module to CensNet (A-CensNet) to integrate graph embeddings from multiple channels into one. The experiments demonstrate the proposed model outperforms the one using single modality, single channel and state-of-the-art methods. The results suggest that brain functional activities and eye behaviors might complement each other in interpreting trait-like phenotypes. Our code will make public later.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a growing interest in leveraging brain imaging data to predict non-brain-imaging phenotypes in individual participants, since brain functional activity could intrinsically serve as an "objective" observer of a subject given that the emergence of behavior and cognition were widely attributed to the orchestration of local and remote cortical areas by means of a densely connected brain network <ref type="bibr" target="#b0">[1]</ref>. In this context, the movie-watching paradigm has been widely demonstrated to provide richer life-like scenarios <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, better subject compliance in contrast to rest and task states, and were suggested to be better at predicting emotional/cognitive reaction <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> that could be more easily and saliently evoked by naturalistic input load, making movie watching the upper bound of current paradigms.</p><p>However, in recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, the individual trait prediction from moviewatching paradigm fMRI (mfMRI) induced brain activity/functional connectivity can only achieve an accuracy around 0.40 (Pearson/Spearman's r). The accuracy drops even dramatically when dataset size is small. Regarding the reality that it is a challenging task to increase mfMRI data size, at least two strategies can be intuitively proposed to improve the performance on the basis of limited number of subjects: 1) Incorporation of other data modalities, such as behavior. Conceptually, a joint use of what a subject "thinks" and what the subject "reacts" to a stimulus could help to increase the accuracy of prediction of "who" he/she is. Eye movement behaviors <ref type="bibr" target="#b11">[12]</ref>, as one example, have been related to subjects' cognitive and phenotypical measures <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, and might supplement the fMRI derived brain activities in monitoring subjects' attention and task compliance and observing subjects' subconscious traits <ref type="bibr" target="#b14">[15]</ref>; 2) Increase the number of different video clips watched by the same group of subjects.</p><p>Integration of multimodal data has been realized by using a graph to present the relation between subjects, where subjects are defined as nodes, node feature is fMRI connectivity, and edge is estimated by thresholding the similarity of behaviors, including eye movement <ref type="bibr" target="#b15">[16]</ref>. This graph convolution networks could realize an embedding of a cohort of subjects' brain activity features according to their behavior similarity, and estimate a mapping of these embedded features to cognitive scores, and further propagate the mapping to other nodes as a prediction of their scores. However, behavior in this model only provides the topology of the graph but are not fully involved in the process of embeddings. Also, different definitions of edges, such as eye trajectory and pupil size in this work, yield a set of graphs with different topologies on the same set of nodes. Therefore, we aim to solve the following two technique problems: 1) how to integrate the different edge features to node ones and learn graph embedding for both node and edge; and 2) how to integrate the embeddings of heterogeneous graphs with the same set of nodes to fulfill classification or regression. Based on Convolution with Edge-Node Switching graph neural network (CensNet) <ref type="bibr" target="#b16">[17]</ref>, we proposed Attention-CensNet (A-CensNet for short), where subjects are nodes, with the mfMRI derived functional connectivity as nodal features. Eye tracking derived gaze trajectory and temporal pupil size variation were respectively used to measure the similarity between subjects and to construct a set of heterogenous edges. Each of these heterogenous graphs was taken as an independent channel, where CensNet was used to alternatively learn both node embeddings and edge embeddings. Then, Squeeze-and-Excitation attention module (SENet) <ref type="bibr" target="#b17">[18]</ref> was used to integrate the node-edge embeddings from multiple channels into one hybrid graph on which the final round of node embedding was performed. Note that mfMRI and eye tracking data from the same cohort exposed to different movies inputs also yield additional channels in this work.</p><p>In the following sections, we firstly introduce the dataset and preprocessing steps. Basics of CensNet and SENet, and proposal of A-CensNet are introduced followed by its application to our task. Comparative and ablation studies regarding to prediction accuracy (AUC) were presented in the Results to demonstrate the effectiveness of multiple channel integration strategy, and the better performance of A-CensNet than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>In the Human Connectome Project (HCP) 7T release <ref type="bibr" target="#b18">[19]</ref>, movie-watching fMRI and resting-state fMRI data were acquired on a 7 T Siemens Magnetom scanner <ref type="bibr" target="#b19">[20]</ref>. Among the four scan sessions (MOVIE 1~4), MOVIE 2&amp;3 were used as a testbed in this work. Important imaging parameters are as follows: TR = 1000 ms, TE = 22.2 ms, flip angle = 45 deg, FOV = 208 × 208 mm, matrix = 130 × 130, spatial resolution = 1.6mm 3 , number of slices = 85, multiband factor = 5. During MOVIE 2 runs, 4 video clips (818 time points, TRs), separated by five 20s rest sessions (100 time points in total), were presented to subjects. In MOVIE 3 runs, 5 video clips (789 time points, TRs), separated by five 20s rest sessions (120 time points in total), were presented to subjects. Eye tracking data were acquired during MOVIE runs using an EyeLink S1000 system with a sampling rate of 1000 Hz. HCP provides many phenotypic measures from a variety of domains. As suggested by <ref type="bibr" target="#b7">[8]</ref>, we focus on measures in the cognition domain in this work. After a quality control of data modalities, 81 subjects are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>FMRI data have been preprocessed by the minimal preprocessing pipeline for the Human Connectome Project <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. The signals were mapped to the grayordinate system, which includes 64k vertices on the reconstructed cortical surface plus 30k subcortical voxels for an individual. The within-subject cross-modal registration and cross-subject registration are adopted to warp the grayordinate vertices and voxels to the same space, such that the associated fMRI signals have cross-subject correspondence.</p><p>In this study, subcortical regions are not our major interest and not included. Destrieux atlas <ref type="bibr" target="#b21">[22]</ref> is applied to cortical surfaces to yield 75 cortical areas on each hemisphere. We compute the mean fMRI signal averaged over vertices within a cortical area, and construct a 150-by-150 functional connectivity matrix, by means of Pearson correlation between these average signals (blue panel in Fig. <ref type="figure" target="#fig_0">1(a)</ref>). We zero the negative correlation and 90% of the lowest positive correlation. The upper triangular matrix is converted to a vector and used as the functional feature.</p><p>For eye tracking data, time stamps are used to extract the effective data points and synchronize the eye behavior features across subjects. Blink session is not considered. Since many phenotypic measures within a domain could be correlated with one another, we perform principal components analysis (PCA) to measures classified in "cognition" domains <ref type="bibr" target="#b7">[8]</ref>. The first principal component is used to classify the subjects to four groups by their scores. Subject number balance among groups is considered. Each group is assigned a label l ∈ L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification of Population via Attention-CensNet</head><p>Basics of CensNet. Supposing we have a dataset of M subjects, our objective is to assign each subject a cognitive group label l. We construct a graph G = {V, E, A} to represent the entire cohort as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, where v ∈ V is a node of the graph, the subjects in this work. Edges E s as well as the adjacent matrix A encode the similarity between subjects. The implementation of CensNet was detailed elsewhere <ref type="bibr" target="#b16">[17]</ref>, and we provide a summarized version as follows:</p><p>For spectral graph convolution, normalized graph Laplacian of a graph G = {V, E, A} is computed: L = I N -D -1/2 AD -1/2 where I N is the identity matrix and D is the diagonal degree matrix. One of the important steps is the layer-wise propagation rule based on an approximated graph spectral kernel as follows:</p><formula xml:id="formula_0">H l+1 = σ ( D-1/2 Ã D-1/2 H l W l ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Ã = A + I N and D is the degree matrix, H l and W l are the hidden feature matrix and learnable weight of the l th layer. On this basis, the CensNet is proposed to have both node and edge convolution layers. For convenience, the graph (white box) in Fig. <ref type="figure" target="#fig_0">1</ref> (a) consists of a node-center version (yellow box) and an edge-center one (green box). In the node convolution layer, the embedding of nodes in the white box is updated, while the edge adjacency matrix and edge features in the green box are intact. Then, a similar update is implemented in the edge layer. Such a node-edge switching is implemented by the following equations:</p><formula xml:id="formula_2">H l+1 v = σ (T (H l e P e )T T Ãv H l v W l v ) (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">H l+1 e = σ (T T (H l v P v )T Ãe H l e W l e )<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">Ãv = D-1 2 v (A v + I Nv ) D-1 2 v , Ãe = D-1 2 e (A e + I Ne ) D-1 2</formula><p>e , T ∈ R Nv×Ne is a binary matrix that indicates whether an edge connects a node. P e is a learnable weight vector, denotes the diagonalization operation. denotes the element-wise product. The loss function is defined as:</p><formula xml:id="formula_6">L( ) = - l∈Y L F f =1 Y lf logM lf (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where Y L is the subset of nodes with labels, M is the softmax results of the last node layer where node feature map has F dimensions.</p><p>Squeeze-and-Excitation Attention Block. The SENet <ref type="bibr" target="#b17">[18]</ref> is introduced to integrate multiple graphs that share the same nodes but have different node features and edges. Note that a typical CensNet includes 1) the node-and-edge switching embedding plus 2) an additional round of node embedding (for node classification). The SENet is inserted between 1) and 2) (lower panel in Fig. <ref type="figure" target="#fig_0">1(a)</ref>). The process (node as same as edge) is expressed by:</p><formula xml:id="formula_8">z node = F sq (F node ) = 1 M ×H n M i=1 H n j=1 F node (i, j) (<label>5</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">s node = F ex (z node , W ) = σ (g(z node , W )) = σ (W 2 δ(W 1 z node ))<label>(6)</label></formula><p>It is important to note that previous works <ref type="bibr" target="#b8">[9]</ref> adopted regression scheme to predict cognitive scores, and the prediction accuracy was quantified by Pearson/Spearman correlation. However, the small dataset size significantly reduces the prediction accuracy <ref type="bibr" target="#b8">[9]</ref>, especially when sample size is below 100. We followed the suggestion in <ref type="bibr" target="#b15">[16]</ref> to use the classification strategy instead and the accuracy of class label prediction was quantified by the AUC. Accordingly, the regression layer of some of state-of-the-art methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref> are replaced by classification one. We set the dimension of the final output to 4 (the number of classes) and pass it through logsoftmax function. The loss function and the AUC calculation is the same as that used in A-CensNet, while keeping the other layers by their default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>It was demonstrated in He et al., 2020 that the behavior score prediction accuracy via regression drops dramatically when subject number is below 100 (no more than r = 0.1 via Spearman correlation). Since we only have 81 subjects, a low regression accuracy cannot be a trustworthy to be used to compare with state-of-the-arts. As a compromise solution, we adopted classification scheme to demonstrate the effectiveness of our proposed framework.</p><p>In our application, subjects are divided to four cognitive groups (around 20 subjects in each one, a total of 81 subjects). Then, we randomly split the dataset to training, validation and testing sets, respectively. We randomly selected 10 subjects from each group, a total of 40 subjects (about 50% of the total) to form the training set. Among the remaining 41 people, we randomly selected 20 people to be the verification group and 21 people to be the testing group (each accounting for 25% of the total). Such a random division of training/validation/testing subsets was repeated 100 times, independently. The results (AUCs) were the average of 100 independent replicates.</p><p>We experiment on preserving {10%, 15%, 20%} top graph and edges by their weights, and find that preserving 10% node feature and 10% edges yields the best prediction performance. We try different settings of learning rate from {0.05, 0.01, 0.005, 0.001}, dropout {0.2, 0.3, 0.4, 0.5}, hidden {16, 32, 64, 128, 512, 1024}, and found that the best performance is yielded by learning rate to 0.005, dropout to 0.2, hidden to 1024. We implement the A-CensNet structure by adding the Attention mechanism based on CensNet, which empirically produce the best performance. AUC is adopted for each independent replication experiment to evaluate the prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>The prediction accuracy measured by AUC of 100 repeated experiments are reported in Table <ref type="table" target="#tab_0">1</ref>. As a comparison, attention module is removed, as shown in the "Attention-No" rows in Table <ref type="table" target="#tab_0">1</ref>. In our method, the attention block is added after the node-edge updates in each channel (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>). The results are reported in "Attention-Middle" section. We also move the attention module ahead of CensNet. The results are reported in "Attention-Before" section. It is noted that data from different datasets are taken as different channels for attention module. To evaluate whether multiple stimulus loads to the same subjects enhance the performance, we only use two channels (trajectory and pupil variation as two sets of edges) within a single movie. The results are in the first row in "Attention-Top" and "Attention-Middle" sections. We use the format {Node, Edge} to describe the graph structure.</p><p>In the non-attention algorithms, CensNet on movie2 dataset ({mfMRI2, Pupil2}) yields the best performance. Concatenation of node feature or edge feature from two movie datasets even decreases the accuracy. When attention module is added (Attention-Middle), two-channel models (gray rows) do not significantly increase the accuracy. Within one movie dataset, when node feature is fixed and eye trajectory and pupil size variation are taken as two channels, the performance is not better than that on single-channel model on movie3 dataset. When two movie datasets are considered (car-neose&amp;green rows), they are integrated by means of channels when models with attention are used. In contrast, in non-attention models, the two movie datasets are integrated by means of feature concatenation. It is seen that the channel integration outperforms feature concatenation when pupil size is used as edges (green). But this does not hold when eye trajectory was used as edges (carneose). When both trajectory and pupil size are both considered as two channels, but features from two dataset are concatenated (white rows), the prediction performance is worst (46.21 ± 0.55) in all attention models. Finally, when both trajectory and pupil size from two dataset are used as four channels (blue row) in our algorithm, the best performance was yielded.</p><p>These comparisons suggest that integration of edge features by channel attention does not always improves the performance than single use of an edge (gray rows), while integration of datasets by channel attention significantly outperforms the integration by feature concatenation. This observation still holds for "Attention-Before" models. But their accuracy is not as high as that via "Attention-Middle" models (see the crosscomparison between blue rows and gray rows), suggesting that a node-edge embedding could yield latent features more sensitive to individual variations, such that a channel attention works better at this deep feature space than being applied immediately after the original shallow features ("Attention-Before"). Finally, in addition to the 4-group classification, we evenly divided the subjects into 6 and 8 groups, respectively. AUCs via our model are 56.34 ± 0.65 and 56.95 ± 0.48, demonstrating the robustness of the algorithm to class numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Arts</head><p>We compare our results with the ones by state-of-the-art methods listed in Table <ref type="table" target="#tab_1">2</ref>. The results via linear model and GCN were applied on Movie2 and have been reported in <ref type="bibr" target="#b15">[16]</ref>, and is thus listed for reference. Note that the linear, FNN and BrainNetCNN are not GNN-related methods. They have no edges but only rely on mfMRI features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Data AUC Edge Node Linear [8]  -mfM RI2 41.94±0.81 GCN [16]  Trj2 mfM RI2 48.51±0.94 CensNet [17]  mfM RI2 49.75±0.65</p><p>CensNet [17]  Trj2 mfM RI2 50.36±0.71 Ppl2 52.91±0.73 FNN [9]  - All nonlinear deep neural networks yield significant improvement in contrast to the linear method (41.94 ± 0.81). A concatenation of mfMRI features (mfMRI2 +3 rows) does not improve the prediction accuracy in contrast to that on a single dataset, suggesting the importance of the strategy selection for concatenating features from multiple datasets. Within a single movie dataset (unshaded rows), models integrating mfMRI and eye tracking outperform the models (FNN and BrainNetCNN) that used single modality (mfMRI). After integrating mfMRI and eye behavior from multiple datasets, our results outperform all the-state-of-arts. These results demonstrate the effectiveness of integration of brain activity and eye behavior to one framework in cognition prediction. Also, given the limited subjects, multiple loads of stimuli integrated via attention modules could significantly improve the prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose A-CensNet to predict subjects' cognitive scores, with subjects taken as nodes, mfMRI derived functional connectivity as node feature, different eye tracking features are used to compute similarity between subjects to construct heterogeneous graph edges. These graphs from different dataset are all taken as different channels. The proposed model integrates graph embeddings from multiple channels into one. This model outperforms the one using single modality, single channel and state-of-the-art methods. Our results suggest that the brain functional activity patterns and the behavior patterns might complement each other in interpreting trait-like phenotypes, and might provide new clues to studies of diseases with cognitive abnormality <ref type="bibr" target="#b23">[24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The preprocessings and flowchart of A-CensNet. Data preprocessings are in the shallower blue panel. Construction of the graph is in the darker blue panel. Node feature generation is highlighted by red arrows and edge generation blue arrows. The graph (white box) is presented by its node-center version (yellow box) and edge-center version (green box) and was fed to CensNet in each channel. Gray frame highlights a typical CensNet flow. Attention module (SENet) is highlighted by carneose panel. Note that SENet was inserted in the middle of a CensNet procedure, before the last round of node convolution. (Color figure online)</figDesc><graphic coords="4,57,30,57,26,309,55,282,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation Studies. The prediction accuracy is measured by AUC. Graph structure is represented by {Node, Edge}. "+" denotes a temporal concatenation of two features. C: channel. Trj: eye movement trajectory. Ppl: pupil size variation. The index after mfMRI, Trj and Ppl indicates which movie dataset it comes from. Red: the highest AUC. Blue: the second-highest one.</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="2">Graph Structure</cell></row><row><cell cols="2">{mfMRI2, Trj2}</cell><cell>{mfMRI2, Ppl2}</cell><cell>{mfMRI3, Trj3}</cell><cell>{mfMRI3, Ppl3}</cell></row><row><cell>50.36±0.71</cell><cell></cell><cell>52.91±0.73</cell><cell>51.49±0.75</cell><cell>51.83±0.85</cell></row><row><cell>Attention No</cell><cell></cell><cell cols="2">{mfMRI2+mfMRI3, Trj2+Trj3}</cell></row><row><cell>(CensNet)</cell><cell></cell><cell cols="2">51.94±0.81</cell></row><row><cell></cell><cell></cell><cell cols="2">{mfMRI2+mfMRI3, Ppl2+Ppl3}</cell></row><row><cell></cell><cell></cell><cell cols="2">45.45±0.43</cell></row><row><cell cols="3">C1: {mfMRI2, Trj2}</cell><cell cols="2">C1: {mfMRI3, Trj3}</cell></row><row><cell cols="3">C2: {mfMRI2, Ppl2}</cell><cell cols="2">C2: {mfMRI3, Ppl3}</cell></row><row><cell></cell><cell cols="2">50.66±0.67</cell><cell cols="2">51.79±0.67</cell></row><row><cell></cell><cell></cell><cell cols="2">C1: {mfMRI2, Trj2} C2: {mfMRI3, Trj3}</cell></row><row><cell></cell><cell></cell><cell cols="2">49.35±0.69</cell></row><row><cell>Attention Middle</cell><cell></cell><cell cols="2">C1: {mfMRI2, Ppl2} C2: {mfMRI3, Ppl3} 50.38±0.62 C1: {mfMRI2+mfMRI2, Trj2+Trj3}</cell></row><row><cell></cell><cell></cell><cell cols="2">C2: {mfMRI2+mfMRI2, Ppl2+Ppl3}</cell></row><row><cell></cell><cell></cell><cell cols="2">46.21 0.55</cell></row><row><cell></cell><cell></cell><cell cols="3">C1: {mfMRI2, Trj2} C2: {mfMRI2, Ppl2}</cell></row><row><cell></cell><cell></cell><cell cols="3">C3: {mfMRI3, Trj3} C4: {mfMRI3, Ppl3}</cell></row><row><cell></cell><cell></cell><cell cols="2">54.63±0.65</cell></row><row><cell cols="3">C1: {mfMRI2, Trj2}</cell><cell cols="2">C1: {mfMRI3, Trj3}</cell></row><row><cell cols="3">C2: {mfMRI2, Ppl2}</cell><cell cols="2">C2: {mfMRI3, Ppl3}</cell></row><row><cell>Attention</cell><cell cols="2">49.00±0.52</cell><cell cols="2">48.49±0.78</cell></row><row><cell>Before</cell><cell></cell><cell cols="3">C1: {mfMRI2, Trj2} C2: {mfMRI2, Ppl2}</cell></row><row><cell></cell><cell></cell><cell cols="3">C3: {mfMRI3, Trj3} C4: {mfMRI3, Ppl3}</cell></row><row><cell></cell><cell></cell><cell cols="2">50.24±0.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-arts. Red font highlights the highest AUC and blue font highlights the second-highest one. "+" denotes a temporal concatenation of two features.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">31971288</rs>, <rs type="grantNumber">U1801265</rs>, <rs type="grantNumber">61936007</rs>, <rs type="grantNumber">62276050</rs>, <rs type="grantNumber">61976045</rs>, <rs type="grantNumber">U20B2065</rs>, <rs type="grantNumber">U1801265</rs> and <rs type="grantNumber">61936007</rs>); the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2020AAA0105701</rs>; Highlevel researcher start-up projects (Grant No. <rs type="grantNumber">06100-22GH0202178</rs>); <rs type="funder">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</rs> <rs type="grantNumber">CX2022052</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HEadjzH">
					<idno type="grant-number">31971288</idno>
				</org>
				<org type="funding" xml:id="_PP3QNrr">
					<idno type="grant-number">U1801265</idno>
				</org>
				<org type="funding" xml:id="_XejPaf3">
					<idno type="grant-number">61936007</idno>
				</org>
				<org type="funding" xml:id="_j8QE2gR">
					<idno type="grant-number">62276050</idno>
				</org>
				<org type="funding" xml:id="_xcpek2V">
					<idno type="grant-number">61976045</idno>
				</org>
				<org type="funding" xml:id="_f96pYKX">
					<idno type="grant-number">U20B2065</idno>
				</org>
				<org type="funding" xml:id="_hZ5BRH6">
					<idno type="grant-number">U1801265</idno>
				</org>
				<org type="funding" xml:id="_5fAzGz3">
					<idno type="grant-number">61936007</idno>
				</org>
				<org type="funding" xml:id="_F8p8RaV">
					<idno type="grant-number">2020AAA0105701</idno>
				</org>
				<org type="funding" xml:id="_ksd4F3Y">
					<idno type="grant-number">06100-22GH0202178</idno>
				</org>
				<org type="funding" xml:id="_qAhzAKV">
					<idno type="grant-number">CX2022052</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The emergent properties of the connected brain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thiebaut De Schotten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Forkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6619</biblScope>
			<biblScope unit="page" from="505" to="510" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global signal regression strengthens association between resting-state functional connectivity and behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="126" to="141" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Less head motion during MRI under task than resting-state conditions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huijbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R A</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Boenniger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stirnberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breteler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Function in the human connectome: task-fMRI and individual differences in behavior</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Barch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="169" to="189" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Naturalistic stimuli in neuroscience: critically acclaimed</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sonkusare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="699" to="714" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intersubject synchronization of cortical activity during natural vision</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">5664</biblScope>
			<biblScope unit="page" from="1634" to="1640" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can brain state be manipulated to emphasize individual differences in functional con-nectivity?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scheinost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Constable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="140" to="151" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Movie-watching outperforms rest for functional connectivitybased prediction of behavior</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page">117963</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behavior and de-mographics</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">116276</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Act natural: functional connectivity from naturalistic stimuli fMRI outperforms resting-state in predicting brain activity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Coldham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein-Eliav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta-matching as a simple framework to translate phenotypic predictive models from big to small data</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotion recognition using eye-tracking: taxonomy, review and current challenges</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mountstephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2384</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pupil size as related to interest value of visual stimuli</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Polt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3423</biblScope>
			<biblScope unit="page" from="349" to="350" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of two process tracing methods for choice tasks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Lohse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organ. Behav. Hum. Decis. Process</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating fMRI-based estimation of eye gaze during naturalistic viewing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1171" to="1184" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prediction of cognitive scores by movie-watching FMRI connectivity and eye movement via spectral graph convolutions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CensNet: convolution with edge-node switching in graph neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2656" to="2662" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Elam</surname></persName>
		</author>
		<ptr target="https://www.humanconnectome.org/study/hcp-young-adult/article/first-release-of-7t-mr-image-data" />
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ICA-based artefact removal and accelerated fMRI ac-quisition for improved resting state network imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Griffanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="232" to="247" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wu-Minn HCP Consortium: The minimal preprocessing pipelines for the human connectome project</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Glasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="105" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</title>
		<author>
			<persName><forename type="first">C</forename><surname>Destrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Halgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BrainNetCNN: Convolutional neural networks for brain networks; towards predicting neurodevelopment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1038" to="1049" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neurophysiological responses to faces and gaze direction differentiate children with ASD, ADHD and ASD + ADHD</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dev. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
