<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Pinetz</surname></persName>
							<email>pinetz@iam.uni-bonn.de</email>
							<idno type="ORCID">0000-0002-6100-2136</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Applied Mathematics</orgName>
								<orgName type="institution">Rheinische Friedrich-Wilhelms-Universität Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erich</forename><surname>Kobler</surname></persName>
							<idno type="ORCID">0000-0001-5167-4804</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">University Medical Center Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Haase</surname></persName>
							<idno type="ORCID">0000-0003-2311-8004</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">University Medical Center Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katerina</forename><surname>Deike-Hofmann</surname></persName>
							<idno type="ORCID">0000-0002-4319-0428</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">University Medical Center Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">German Center for Neurodegenerative Diseases (DZNE)</orgName>
								<orgName type="institution">Helmholtz Association of German Research Centers</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Radbruch</surname></persName>
							<idno type="ORCID">0000-0001-6238-6525</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">University Medical Center Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">German Center for Neurodegenerative Diseases (DZNE)</orgName>
								<orgName type="institution">Helmholtz Association of German Research Centers</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Effland</surname></persName>
							<email>effland@iam.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Applied Mathematics</orgName>
								<orgName type="institution">Rheinische Friedrich-Wilhelms-Universität Bonn</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="607" to="617"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1532FA1A30D5EF7217BF1EA7F506EB72</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MRI</term>
					<term>GANs</term>
					<term>Optimal Transport</term>
					<term>Noise Modelling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today Gadolinium-based contrast agents (GBCA) are indispensable in Magnetic Resonance Imaging (MRI) for diagnosing various diseases. However, GBCAs are expensive and may accumulate in patients with potential side effects, thus dose-reduction is recommended. Still, it is unclear to which extent the GBCA dose can be reduced while preserving the diagnostic value -especially in pathological regions. To address this issue, we collected brain MRI scans at numerous non-standard GBCA dosages and developed a conditional GAN model for synthesizing corresponding images at fractional dose levels. Along with the adversarial loss, we advocate a novel content loss function based on the Wasserstein distance of locally paired patch statistics for the faithful preservation of noise. Our numerical experiments show that conditional GANs are suitable for generating images at different GBCA dose levels and can be used to augment datasets for virtual contrast models. Moreover, our model can be transferred to openly available datasets such as BraTS, where non-standard GBCA dosage images do not exist.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic Resonance Imaging (MRI) of the brain is an essential imaging modality to accurately diagnose various neurological diseases ranging from inflammatory T. Pinetz and A. Effland-are funded the German Research Foundation under Germany's Excellence Strategy -EXC-2047/1 -390685813 and -EXC2151 -390873048 and R. Haase is funded by a research grant (BONFOR; O-194.0002.1). T. Pinetz and E. Kobler-contributed equally to this work. lesions to brain tumors and metastases. For accurate depictions of said pathologies, gadolinium-based contrast agents (GBCA) are injected intravenously to highlight brain-blood barrier dysfunctions. However, these contrast agents are expensive and may cause nephrogenic systemic fibrosis in patients with severely reduced kidney function <ref type="bibr" target="#b30">[31]</ref>. Moreover, <ref type="bibr" target="#b16">[17]</ref> reported that Gadolinium accumulates inside patients with unclear health consequences, especially after repeated application. The American College of Radiology recommends administering the lowest GBCA dose to obtain the needed clinical information <ref type="bibr" target="#b0">[1]</ref>.</p><p>Driven by this recommendation, several research groups have recently published dose-reduction techniques focusing on maintaining image quality. Complementary to the development of higher relaxivity contrast agents <ref type="bibr" target="#b27">[28]</ref>, virtual contrast <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> -replacing a large fraction of the GBCA dose by deep learninghas been proposed. These approaches typically acquire a contrast-enhanced (CE) scan with a lower GBCA dose along with non-CE scans, e.g., T1w, T2w, FLAIR, or ADC. These input images are then processed by a deep neural network (DNN) to replicate the corresponding standard-dose scan. While promising, virtual contrast techniques have not been integrated into clinical practice yet due to falsepositive signals or missed small lesions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. As with all deep learning-based approaches, the availability of large datasets is essential, which is problematic in the considered case since the additional CE low-dose scan is not acquired in clinical routine exams. Hence, there are no public datasets to easily benchmark and compare different algorithms or evaluate their performance. In general, the enhancement behavior of pathological tissues at various GBCA dosages has barely been researched due to a lack of data <ref type="bibr" target="#b11">[12]</ref>.</p><p>In recent years, generative models have been used to overcome data scarcity in the computer vision and medical imaging community. Frequently, generative adversarial networks (GANs) <ref type="bibr" target="#b8">[9]</ref> are applied as state-of-the-art in image generation <ref type="bibr" target="#b29">[30]</ref> or semantic translation/interpolation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. In a nutshell, the GAN framework trains two competing DNNs -the generator and the discriminator. The generator learns a non-linear transformation of a predefined noise distribution to fit the distribution of a target dataset, while the discriminator provides feedback by simultaneously approximating a distance or divergence between the generated and the target distribution. The choice of this distance leads to the well-known different GAN algorithms, e.g., Wasserstein GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, Least Squares GANs <ref type="bibr" target="#b23">[24]</ref>, or Non-saturating GANs <ref type="bibr" target="#b8">[9]</ref>. However, Lucic et al. <ref type="bibr" target="#b21">[22]</ref> showed that this choice has only a minor impact on the performance.</p><p>Learning conditional distributions between images can be accomplished by additionally feeding a condition (additional scans, dose level, etc.) into both the generator and discriminator. In particular, for image-to-image translation tasks, these conditional GANs have been successfully applied using paired <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> and unpaired training data <ref type="bibr" target="#b34">[35]</ref>. Within these methods, an additional content (cycle) loss typically penalizes pixel-wise deviations (e.g., 1 ) from a corresponding reference to enforce structural similarity, whereas a local adversarial loss (discriminator with local receptive field) controls textural similarity. In addition, embeddings have been used to inject metadata <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. To study the GBCA accumulation behavior, we collected 453 CE scans with non-standard GBCA doses in the set of {10%, 20%, 33%} along with the corresponding standard-dose (0.1 mmol/kg) scan after applying the remaining contrast agent. Using this dataset, we aim at the semantic interpolation of the GBCA signal at various fractional dose levels. To this end, we use GANs to learn the contrast enhancement behavior from the dataset collective and thereby enable the synthesis of contrast signals at various dose levels for individual cases. Further, to minimize the smoothing effect <ref type="bibr" target="#b18">[19]</ref> of typical content losses (e.g., 1 or perceptual <ref type="bibr" target="#b15">[16]</ref>), we develop a noise-preserving content loss function based on the Wasserstein distance between paired image patches calculated using a Sinkhornstyle algorithm. This novel loss enables a faithful generation of noise, which is important for the identification of enhancing pathologies and their usability as additional training data.</p><p>With this in mind, the contributions of this work are as follows:</p><p>-synthesis of GBCA behavior at various doses using conditional GANs, -loss enabling interpolation of dose levels present in training data, -noise-preserving content loss function to generate realistic synthetic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Given a native image x NA (i.e. without any contrast agent injection) and a CE standard-dose image x SD , our conditional GAN approach synthesizes CE lowdose images xLD for selected dose levels d ∈ D ⊂ [0, 1] from a uniform noise image z ∼ N (0, Id), see Fig. <ref type="figure" target="#fig_0">1</ref>. To focus the generation on the contrast agent signal, our model predicts residual images ŷLD ; the corresponding low-dose can be obtained by xLD = x NA + ŷLD .</p><p>For training and evaluation, we consider samples (x NA , x SD , y LD , d, B) of a dataset DS, where y LD = x LD -x NA is the residual image of a real CE low-dose scan x LD with dose level d ∈ D and B ∈ {1.5, 3} is the field-strength in Tesla of the used scanner. To simplify learning of the contrast accumulation behavior, we adapt the preprocessing pipeline of BraTS <ref type="bibr" target="#b5">[6]</ref>. Further details of the dataset and the preprocessing are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conditional GANs for Contrast Signal Synthesis</head><p>Our approach is built on the insight that contrast enhancement is an inherently local phenomenon and the necessary information for the synthesis task can be extracted from a local neighborhood within an image. Therefore, we use as generator g θ a convolutional neural network (CNN) that is based on the U-Net <ref type="bibr" target="#b28">[29]</ref> along with a local attention mechanism. The architecture design and the implementation details can be found in the supplementary material. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the generator uses a 3D noise sample z ∼ N(0, Id) along with the native and SD images (x NA , x SD ) as input. The synthesis is guided by the metadata ( d B) , containing the artificial dose level d ∈ D as well as the field strength of the corresponding scanner B ∈ {1.5, 3}. In particular, the metadata is injected into every residual block of the generator using an embedding, motivated by the recent success of diffusion-based models <ref type="bibr" target="#b12">[13]</ref>.</p><p>To learn this generator, a convolutional discriminator f φ is used, which is in turn trained to distinguish the generated residual images ŷLD with random dose level d from the real residual images y LD with the associated real dose level d. To make this a non-trivial task, label smoothing on the metadata is used, i.e., the real dose is augmented by zero-mean additive Gaussian noise with standard deviation 0.05. The discriminator architecture essentially implements the encoding side of the generator, however, no local attention layers are used as suggested by <ref type="bibr" target="#b19">[20]</ref>. Like the generator, the discriminator is conditioned on the metadata using an embedding, which is not shared between both networks.</p><p>For training of the generator θ and discriminator φ, we consider the loss</p><formula xml:id="formula_0">min θ max φ {L GAN (θ, φ) + λ GP L GP (φ) + λ C L C (θ)} , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>which consists of a Wasserstein GAN loss L GAN , a gradient penalty loss L GP , and a content loss L C that are relatively weighted by scalar non-negative factors λ GP and λ C . In detail, the Wasserstein GAN loss reads as  </p><formula xml:id="formula_2">L GAN (θ, φ) := E (xNA,</formula><formula xml:id="formula_3">( ∇f φ (h(α, y LD , ŷLD ), h(α, c, ĉ)) 2 -1) 2 using h(α, y, ŷ) = αŷ + (1 -α)y. A penalty term is introduced, if f φ is not</formula><p>Lipschitz continuous with factor 1 in its arguments as required by Wasserstein GANs <ref type="bibr" target="#b9">[10]</ref>. Here, ŷLD = g θ (z, ĉ) and the Lipschitz penalty is evaluated at convex combinations of real and synthetic images and condition tuples (essentially dose levels). Finally, using a distance C , the content loss L C (θ) := E (xNA,xSD,yLD,d,B)∼U (DS),z∼N (0,Id) C g θ (z, c) , y LD guides the generator g θ towards residual images in the dataset. Thus, it teaches the generator the principles of contrast enhancement. Typically, the 1 -norm is used as a distance function, which leads to smooth results since it also penalizes deviations from the noise in y LD .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noise-Preserving Content Loss</head><p>To generate realistic CE images, it is also important to retain the original noise characteristics. Therefore, we introduce a novel loss that accounts for deviations in local statistics using optimal transport between empirical distributions of paired patches, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Let x, x ∈ R n 3 be patches of size n × n × n extracted from the same location of a real and synthetic image, respectively. The Wasserstein distance of the associated empirical distributions using a transport plan T ∈ R n 3 ×n 3</p><formula xml:id="formula_4">+ and cost matrix C ∈ R n 3 ×n 3 + given by (C ij = |x i -x j |) is defined as W(x, x) = min T ∈R n 3 ×n 3 + C, T F s.t. T 1 = 1 1 n 3 , T 1 = 1 1 n 3 , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where 1 is the vector of ones of size n 3 . In contrast to the element-wise difference penalization of the 1 -distance, the Wasserstein distance accounts for mismatches between distributions. To illustrate this, let us, for instance, assume that both patches are Gaussian distributed (x ∼ N (μ, σ), x ∼ N (μ, σ)), which is a coarse simplification of real MRI noise <ref type="bibr" target="#b1">[2]</ref>. In this case, the Wasserstein distance reduces to second-order momentum matching, i.e., W 2 (x, x) = (μ -μ) 2 +(σ -σ) 2 . Thus, the Wasserstein distance generalizes this distributional loss to any distribution within paired patches.</p><p>To efficiently solve problem (2), we use the inexact proximal point algorithm <ref type="bibr" target="#b33">[34]</ref>. This algorithm is parallelized and applied to all non-overlapping patch pairs, to obtain our noise-preserving content loss</p><formula xml:id="formula_6">NP (ŷ, y) = E o∼U (O) p∈P W(P p+o ŷ, P p+o y) ,</formula><p>where P p extracts a local n × n × n patch at location p ∈ P = {0, n, 2n, . . .} 3 using periodic boundary conditions. Note that we compute the expectation over offsets o ∈ O = {0, 1, . . . , n 2 } 3 to avoid patching artifacts. In the numerical implementation, only a single offset is sampled for time and memory constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Numerical Results</head><p>In this section, we evaluate the proposed conditional GAN approach with a particular focus on different content loss distance functions. All synthesis models were trained on 250 samples acquired on 1.5T and 3T Philips Achieva scanners and evaluated on 193 test cases, all collected at site 1 . Further details of the dataset, model and training can be found in the supplementary. In our experiments, we observed that the choice of the content loss distance function C (ŷ, y) strongly influences the performance. Thus, we consider the different cases:</p><formula xml:id="formula_7">1 : ŷ -y 1 VGG: h(ŷ) -h(y) 1 NP: NP (ŷ, y)</formula><p>Following Johnsen et al. <ref type="bibr" target="#b15">[16]</ref>, h(x) is the VGG-16 model <ref type="bibr" target="#b31">[32]</ref> up to relu3_3.</p><p>A qualitative comparison of the different distance functions C is visualized in Fig. <ref type="figure">3</ref>. The first column depicts synthesized images using the 1 -norm as the distance function. These images depict a plausible contrast signal, however, suffer from unrealistic smooth homogeneous regions. An improvement thereof is shown by the perceptual content loss (VGG). The NP-loss leads to a further improvement not only in the contrast signal behavior but also in the realism of the noise texture, cf. zoom regions in the lower corners.</p><p>To highlight the generalization capabilities, we depict in the bottom row of Fig. <ref type="figure">3</ref> a sample from site 2 , which was acquired using a Philips Ingenia scanner. Moreover, the GBCA gadoterate was used, while our training data only consists of scans using the GBCA gadobutrol. Nevertheless, all models present realistically synthesized LD images. Comparing the zooms of the LD images, we observe that our NP-loss leads to a better synthesis of noise and thereby to Fig. <ref type="figure">3</ref>. Qualitative comparison of synthesized images using different loss functions to the corresponding reference xLD. While the 1 loss yields smooth low-dose images, the noise pattern is preserved to some extent using the VGG loss; our loss helps to further retain the noise characteristics. more realistic LD images. In the 1 and VGG columns, the noise is not faithfully synthesized, thus it is visually easy to spot the enhancing pathological regions.</p><p>For completeness, a quantitative ablation of the considered distance functions on the test images of site 1 is shown in Table <ref type="table">1</ref>. Although neither maximizing PSNR nor SSIM <ref type="bibr" target="#b32">[33]</ref> is our objective, we observe on-par performances of the perceptual (VGG) and our proposed content loss (NP) with the standard 1 distance function. Using the SD image, we define CE pixels as those pixels at which the intensity increases by at least 10% compared to the native scan. An example of these CE regions is illustrated in the supplementary. Thus, the mean absolute error for CE pixels (MAE CE ) quantifies the enhancement behavior. Further, we estimate the standard deviation of the non-CE pixels and report the MAE to the ground truth standard deviation (MAE σ ). As shown in Table <ref type="table">1</ref>, our loss outperforms the other content losses to a large extent on both metrics, proving its effectiveness for faithful contrast enhancement and noise generation. Further statistical analyses are presented in the supplementary.</p><p>Table <ref type="table">1</ref>. Quantitative comparison of the low-dose synthesis methods. The central columns present metrics evaluated on the synthesized low-dose images, whereas the right columns evaluate the effect of purely synthesized data for training the standarddose prediction model <ref type="bibr" target="#b25">[26]</ref>. Note, that the PSNR/SSIM of the standard dose prediction model was always evaluated on real LD images. The definitions of the mean absolute error on the contrast enhancement (MAECE) and on the noise standard deviation (MAEσ) are in Sect. Next, we evaluate the effect of synthesized LD images on the performance of a virtual contrast model (VCM). In particular, we consider the state-of-the-art 2.5D U-Net model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, which predicts an SD image given a corresponding native and LD image, see supplementary for further details. The columns on the right of Table <ref type="table">1</ref> list the average PSNR and SSIM score on the real 33% LD subset of our test data from site 1 . The bottom row depicts the performance if just real 33% LD images are used for training the VCM as an upper bound. In contrast, the other entries on the right list the performance if only synthesized LD images are used for training. Both metrics show that the samples synthesized using our NP-loss model are superior to both 1 and VGG.</p><p>To determine the effectiveness of the LD synthesis models at different settings, we acquired 160 data samples from 1.5T and 3T Philips Ingenia scanners at site 2 . This site used the GBCA gadoterate, which has a lower relaxivity compared to gadobutrol used at site 1 <ref type="bibr" target="#b14">[15]</ref>. For 80 samples real LD images were acquired, which are used for testing. Using the VCM solely trained on the real 33% LD data of site 1 yields an average PSNR and MAE CE on the test samples of site 2 of 40.04 and 0.092, respectively. Extending the training data for the VCM by synthesized LD images from our model with NP-loss, we get a significantly improvemed (p &lt; 0.001) PSNR score of 40.37 and MAE CE of 0.075.</p><p>Finally, Fig. <ref type="figure" target="#fig_3">4</ref> visualizes synthesized LD images on the BraTS dataset <ref type="bibr" target="#b5">[6]</ref> along with the associated VCM outputs. Comparing the predicted SD images xSD using 10% and 33% synthesized LD images xLD , we observe that the weakly enhancing tumor at the bottom zoom is not preserved in the case of 10%, enabling evaluation of dose reduction methods on known pathological regions.  <ref type="bibr" target="#b5">[6]</ref> along with the native (left) and real SD image (right). We also included non-fractional dosage levels (17% and 47%) to showcase the wide applicability of our algorithm. Top: the tumor is well contrasted in all xSD even for 10%. Bottom: the subtle enhancement of the tumor cannot be recovered from the 10% LD image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we used conditional GANs to synthesize contrast-enhanced images using non-standard GBCA doses. To this end, we introduced a novel noisepreserving content loss motivated by optimal transport theory. Numerous numerical experiments showed that our content loss improves the faithful synthesis of low-dose images. Further, the performance of virtual contrast models increases if training data is extended by synthesized images from our GAN model trained by the noise-preserving content loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Low-dose synthesis using a conditional GAN. The generator predicts a residual low-dose image ŷLD from a noise sample z conditioned on the native xNA and standarddose xSD images as well as the field strength B and the artificial dose d. Along with the discriminator, a novel noise-preserving loss -penalizing the Wasserstein distance of paired patches -is used for training. At inference, the generated residual ŷLD is added to the native image xNA to yield the corresponding synthetic low-dose xLD.</figDesc><graphic coords="3,73,08,58,22,279,01,156,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>xSD,yLD,d,B)∼U (DS) f φ (y LD , c) -E z∼N (0,Id), d∼U (D) {f φ (g θ (z, ĉ) , ĉ)} using condition tuples c = (x NA , x SD , (d B) ) and ĉ = (x NA , x SD , ( d B) ) to simplify notation. U(S) denotes a uniform distribution over a set S. We highlight that the artificial dose levels d for the generated images are uniformly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our patch-wise noise-preserving content loss. For each patch pair ((x, x), (x, x), (x, x)) extracted at the same position, the loss accounts for the Wasserstein distance W of the associated empirical distributions. In the center, the corresponding cost matrices C (pixel-wise absolute difference) along with the optimal transport maps T are shown, which are obtained by solving (2). The final loss is the sum of the element-wise multiplication of all C and T for every non-overlapping patch. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of synthesized LD images xLD and corresponding predicted SD images xSD for different dose levels on BraTS<ref type="bibr" target="#b5">[6]</ref> along with the native (left) and real SD image (right). We also included non-fractional dosage levels (17% and 47%) to showcase the wide applicability of our algorithm. Top: the tumor is well contrasted in all xSD even for 10%. Bottom: the subtle enhancement of the tumor cannot be recovered from the 10% LD image.</figDesc><graphic coords="9,63,96,60,77,324,22,81,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,79,41,60,86,290,56,270,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3. A * denotes if a Wilcoxon signed rank test between VGG and NP(our) row is significant.</figDesc><table><row><cell></cell><cell>low-dose synthesis</cell><cell></cell><cell cols="2">standard-dose prediction</cell></row><row><cell></cell><cell cols="4">PSNR SSIM MAECE MAEσ PSNR SSIM</cell></row><row><cell>1</cell><cell>38.34 0.978 0.022</cell><cell>0.012</cell><cell>33.83</cell><cell>0.922</cell></row><row><cell>VGG</cell><cell>37.84 0.976 0.019</cell><cell cols="3">0.009 36.33 0.958</cell></row><row><cell cols="5">NP(our) 38.05  *  0.976 0.011  *  0.004  *  37.15  *  0.960  *</cell></row><row><cell>xLD(real)</cell><cell></cell><cell></cell><cell>39.07</cell><cell>0.974</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_57.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACR Manual on Contrast Media. American College of Radiology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical Analysis of Noise in MRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aja-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vegas-Sánchez-Ferrero</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-39934-8</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-39934-8" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can deep learning replace gadolinium in neuro-oncology?: A reader study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ammari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="107" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MedGAN: medical image translation using GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">StarGAN v2: diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning enables reduced gadolinium dose for contrast-enhanced brain MRI</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="330" to="340" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<editor>Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reduction of gadolinium-based contrast agents in MRI using convolutional neural networks and different input protocols: limited interchangeability of synthesized sequences with original full-dose images despite excellent quantitative performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="420" to="430" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artificial contrast: Deep learning for reducing gadolinium-based contrast agents in neuroradiology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="539" to="547" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High relaxivity MRI contrast agents part 2: optimization of inner-and secondsphere relaxivity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Troughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caravan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">613</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High signal intensity in the dentate nucleus and globus pallidus on unenhanced t1-weighted MR images: relationship with increasing cumulative dose of a gadolinium-based contrast material</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Takenaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="834" to="841" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ViTGAN: training GANs with vision transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">One model to synthesize them all: Multi-contrast multi-scale transformer for missing data imputation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13738</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are GANs created equal? A large-scale study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning-based methods may minimize GBCA dosage in brain MRI</title>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6419" to="6428" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Medical image synthesis with context-aware generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A generic deep learning model for reduced gadolinium dose in contrast-enhanced brain MRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1687" to="1700" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep-learning-based synthesis of post-contrast t1-weighted MRI for tumour response assessment in neuro-oncology: a multicentre, retrospective cohort study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Preetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="784" to="e794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Physicochemical and pharmacokinetic profiles of Gadopiclenol: a new macrocyclic gadolinium chelate with high t1 relaxivity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">StyleGAN-XL: Scaling styleGAN to large diverse datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM SIGGRAPH</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gadolinium-based contrast agents in kidney disease: a comprehensive review and clinical practice guideline issued by the Canadian association of radiologists</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schieda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Kidney Health Dis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="136" to="150" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A fast proximal point method for computing exact wasserstein distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="433" to="453" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
