<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation</title>
				<funder ref="#_JNtnaNN">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_PVMGXhK">
					<orgName type="full">Wellcome/EPSRC</orgName>
				</funder>
				<funder ref="#_kWKMjhu #_988JmaJ #_7mzWM9u">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Muhammad</forename><surname>Asad</surname></persName>
							<email>muhammad.asad@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helena</forename><surname>Williams</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Development and Regeneration</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Indrajeet</forename><surname>Mandal</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">Oxford University Hospitals NHS Foundation Trust</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarim</forename><surname>Ather</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Radiology Department</orgName>
								<orgName type="institution">Oxford University Hospitals NHS Foundation Trust</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Deprest</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Development and Regeneration</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>D’hooge</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="564" to="574"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C26D1653C987CDB6C32406FA91B7AD1E</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing interactive segmentation methods leverage automatic segmentation and user interactions for label refinement, significantly reducing the annotation workload compared to manual annotation. However, these methods lack quick adaptability to ambiguous and noisy data, which is a challenge in CT volumes containing lung lesions from COVID-19 patients. In this work, we propose an adaptive multi-scale online likelihood network (MONet) that adaptively learns in a data-efficient online setting from both an initial automatic segmentation and user interactions providing corrections. We achieve adaptive learning by proposing an adaptive loss that extends the influence of user-provided interaction to neighboring regions with similar features. In addition, we propose a data-efficient probability-guided pruning method that discards uncertain and redundant labels in the initial segmentation to enable efficient online training and inference. Our proposed method was evaluated by an expert in a blinded comparative study on COVID-19 lung lesion annotation task in CT. Our approach achieved 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score than the state-of-the-art. Source code is available at: https://github.com/masadcv/MONet-MONAILabel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods for automatic lung lesion segmentation from CT volumes have the potential to alleviate the burden on clinicians in assessing lung damage and disease progression in COVID-19 patients <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. However, these methods require large amounts of manually labeled data to achieve the level of robustness required for their clinical application <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Manual labeling of CT volumes is time-consuming and may increase the workload of clinicians. Additionally, applying deep learning-based segmentation models to data from new unseen sources can result in suboptimal lesion segmentation due to unseen acquisition devices/parameters, variations in patient pathology, or future coronavirus variants resulting in new appearance characteristics or new lesion pathologies <ref type="bibr" target="#b15">[16]</ref>. To address this challenge, interactive segmentation methods that can quickly adapt to such changing settings are needed. These can be used either by end-users or algorithm developers to quickly expand existing annotated datasets and enable agile retraining of automatic segmentation models <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work:</head><p>Interactive segmentation methods for Artificial Intelligence (AI) assisted annotation have shown promising applications in the existing literature <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. BIFSeg <ref type="bibr" target="#b25">[26]</ref> utilizes a bounding box and scribbles with convolutional neural network (CNN) image-specific fine-tuning to segment potentially unseen objects of interest. MIDeepSeg <ref type="bibr" target="#b13">[14]</ref> incorporates user-clicks with the input image using exponential geodesic distance. However, BIFSeg, MIDeepSeg, and similar deep learning-based methods exploit large networks that do not adapt rapidly to new data examples in an online setting due to the elevated computational requirements.</p><p>Due to their quick adaptability and efficiency, a number of existing online likelihood methods have been applied as interactive segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>. DybaORF <ref type="bibr" target="#b23">[24]</ref> utilizes hand-crafted features with dynamically changing weights based on interactive labels' distribution to train a Random Forest classifier. ECONet <ref type="bibr" target="#b1">[2]</ref> improves online learning with a shallow CNN that jointly learns both features and classifier to outperform previous online likelihood inference methods. While ECONet is, to the best of our knowledge, the only online learning method that addresses COVID-19 lung lesion segmentation, it is limited to learning from user scribbles only. This means that it requires a significant amount of user interaction to achieve expert-level accuracy. Additionally, the model uses a single convolution for feature extraction, limiting its accuracy to a specific scale of pathologies. For each CT volume, the model is trained from scratch, resulting in lack of prior knowledge about lesions. Contributions: To overcome limitations of existing techniques, we propose adaptive multi-scale online likelihood network (MONet) for AI-assisted interactive segmentation of lung lesions in CT volumes from COVID-19 patients. Our contributions are three-fold, we propose: i. Multi-scale online likelihood network (MONet), consisting of a multi-scale feature extractor, which enables relevant features extraction at different scales for improved accuracy; ii. Adaptive online loss that uses weights from a scaled negative exponential geodesic distance from user-scribbles, enabling adaptive learning from both initial segmentation and user-provided corrections (Fig. <ref type="figure" target="#fig_0">1</ref>); iii. Probability-guided pruning approach where uncertainty from initial segmentation model is used for pruning ambiguous online training data.</p><p>MONet enables human-in-the-loop online learning to perform AI-assisted annotations and should not be mistaken for an end-to-end segmentation model. We perform expert evaluation which shows that adaptively learned MONet outperforms existing state-of-the-art, achieving 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given an input image volume, I, a pre-trained CNN segmentation model generates an automatic segmentation C with associated probabilities P . When using data from a new domain, the automated network may fail to properly segment foreground/background objects. To improve this, the user provides scribblesbased interaction indicating corrected class labels for a subset of voxels in the image I. Let S = S f ∪ S b represent these set of scribbles, where S f and S b denote the foreground and background scribbles, respectively, and S f ∩ S b = ∅. Figure <ref type="figure" target="#fig_1">2</ref> (a) shows scribbles S, along with the initial segmentation C and probabilities P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-scale Online Likelihood Network</head><p>Our proposed multi-scale online likelihood network (MONet), shown in Fig. <ref type="figure" target="#fig_1">2</ref> (b), uses a multi-scale feature extractor that applies a 3D convolution at various kernel sizes to capture spatial information at different scales. The output of each scale is concatenated and fed to a fully-connected classifier, which infers the likelihood for background/foreground classification of the central voxel in the input patch. Each layer in MONet is followed by batch normalization and ReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Loss for Online Learning</head><p>The scribbles S only provide sparse information for online learning. However, these corrections are likely also applicable to neighboring voxels with similar appearance features, thereby providing an extended source of training information. Concurrently, the initial automated segmentation C will often provide reliable results away from the scribbles. To extend the influence of the scribbles S while preserving the quality of the initial segmentation C, we propose a spatially-varying adaptive online loss:</p><formula xml:id="formula_0">L = - i (1 -W i )L C i + W i L S i , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where i is a voxel index, L C and L S are individual loss terms for learning from the automated segmentation C and the user-provided correction scribbles S respectively. W are spatially-varying interaction-based weights defined using the geodesic distance D between voxel i and the scribbles S:</p><formula xml:id="formula_2">W i = exp -D(i, S, I) τ ,<label>(2)</label></formula><p>where the temperature term τ controls the influence of W in I. The geodesic distance to the scribbles is defined as D(i, S, I) = min j∈S d(i, j, I) where d(i, j, I) = min p∈Pi,j 1 0 ∇I(p(x)) • u(x) dx and P i,j is the set of all possible differentiable paths in I between voxels i and j. A feasible path p is parameterized by x ∈ [0, 1]. We denote u(x) = p (x)/ p (x) the unit vector tangent to the direction of the path p. We further let D = ∞ for S = ∅. Dynamic Label-Balanced Cross-Entropy Loss: User-scribbles for online interactive segmentation suffer from dynamically changing class imbalance <ref type="bibr" target="#b1">[2]</ref>. Moreover, lung lesions in CT volumes usually occupy a small subset of all voxels, introducing additional label imbalance and hence reducing their impact on imbalanced online training. To address these challenges, we utilize a label-balanced cross-entropy loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, with dynamically changing class weights from segmentations and scribbles distribution. Given an online model with parameters θ, the foreground likelihood from this model is p i = P (s i = 1|I, θ). Then, the segmentations-balanced and scribbles-balanced cross-entropy terms are:</p><formula xml:id="formula_3">L C i = α f y C i log p i + α b (1 -y C i ) log(1 -p i ),<label>(3)</label></formula><formula xml:id="formula_4">L S i = β f y S i log p i + β b (1 -y S i ) log(1 -p i ),<label>(4)</label></formula><p>where α and β are class weights for labels C and scribbles S that are defined by labels and scribbles distributions during online interaction as: The patch-based training approach from <ref type="bibr" target="#b1">[2]</ref> is used to first extract K×K×K patches from I centered around each voxel in S and C and train MONet using Eq. ( <ref type="formula" target="#formula_0">1</ref>). Once learned, efficient online inference from MONet is achieved by applying it to the whole input CT volumes as a fully convolutional network <ref type="bibr" target="#b11">[12]</ref>.</p><formula xml:id="formula_5">α f = |T |/ C f , α b = |T |/ C b , β f = |T |/ S f , β b = |T |/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improving Efficiency with Probability-Guided Pruning</head><p>MONet is applied as an online likelihood learning method, where the online training happens with an expert human annotator in the loop, which makes online training efficiency critical. We observe that the automatic segmentation models provide dense labels C which may significantly impact online training and inference performance. C may contain ambiguous predictions for new data, and a number of voxels in C may provide redundant labels. To improve online efficiency while preserving accuracy during training, we prune labels as C * = M C where: M i is set to 1 if P i ≥ ζ and U i ≥ η and 0 otherwise. ζ ∈ [0, 1] is the minimum confidence to preserve a label, U i ∈ [0, 1] is a uniformly distributed random variable, and η ∈ [0, 1] is the fraction of samples to prune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Validation</head><p>Table <ref type="table" target="#tab_0">1</ref> outlines the different state-of-the-art interactive segmentation methods and their extended variants that we introduce for fair comparison. We compare our proposed MONet with ECONet <ref type="bibr" target="#b1">[2]</ref> and MIDeepSeg <ref type="bibr" target="#b13">[14]</ref>. As our proposed Eq. ( <ref type="formula" target="#formula_2">2</ref>) is inspired by the exponential geodesic distance from MIDeepSeg <ref type="bibr" target="#b13">[14]</ref>, we introduce MIDeepSegTuned, which utilizes our proposed addition of a temperature term τ . Moreover, to show the importance of multi-scale features, we include MONet-NoMS which uses features from a single 3D convolution layer. We utilize MONAI Label to implement all online likelihood methods <ref type="bibr" target="#b6">[7]</ref>. For methods requiring an initial segmentation, we train a 3D UNet <ref type="bibr" target="#b5">[6]</ref> using MONAI <ref type="bibr" target="#b16">[17]</ref> with features <ref type="bibr">[32,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">32]</ref>. Output from each method is regularized using GraphCut optimization. We also compare against a baseline interactive Graph-Cut (IntGraphCut) implementation, that updates UNet output with scribbles based on <ref type="bibr" target="#b2">[3]</ref> and then performs GraphCut optimization. The proposed method is targeted for online training and inference, where quick adaptability with minimal  set using cosine annealing scheduler <ref type="bibr" target="#b12">[13]</ref>. Dropout of 0.3 was used for all fullyconnected layers in online models. Each layer size in ECONet and MONet-NoMS was selected by repeating line search experiments from <ref type="bibr" target="#b1">[2]</ref>: (i) input patch/3D  convolution kernel size of K = 9, (ii) 128 input 3D convolution filters and (iii) fully-connected sizes of 32×16×2. For MONet, we utilize four input 3D convolution with multi-scale kernel sizes K = [1, 3, 5, 9] with each containing 32 filters (i.e., a total of 128 filters, same as (ii)). We utilize the same fully-connected sizes as in (iii) above. Parameters ζ = 0.8 and η = 0.98 are selected empirically.</p><p>We utilize τ = 0.3 for MONet, MIDeepSegTuned and MONet-NoMS. We use GraphCut regularization, where λ = 2.5 and σ = 0.15 <ref type="bibr" target="#b2">[3]</ref>. Search experiments used for selecting τ , λ, σ are shown in Fig. <ref type="figure" target="#fig_0">1</ref> and 2 in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative Comparison Using Synthetic Scribbler</head><p>We employ the synthetic scribbler method from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> where mis-segmented regions in the inferred segmentations are identified by comparison to the ground truth segmentations. Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_4">3</ref> present quantitative comparison of methods using synthetic scribbler. They show that MONet outperforms all existing state-of-the-art in terms of accuracy with the least number of synthetic scribbled voxels. In particular, MONet outperforms both MIDeepSeg <ref type="bibr" target="#b13">[14]</ref> and MIDeepSeg-Tuned, where adaptive online learning enables it to quickly adapt and refine segmentations. In terms of efficiency, online training and inference of the proposed MONet takes around 6.18 s combined, which is 22.4% faster as compared to 7.97 s for MIDeepSeg. However, it is slower than ECONet and ISeg. MIDeepSeg performs the worst as it is unable to adapt to large variations and ambiguity within lung lesions from COVID-19 patients, whereas by utilizing our proposed Eq. ( <ref type="formula" target="#formula_2">2</ref>) in MIDeepSegTuned, we improve its accuracy. When comparing to online learning methods, MONet outperforms MONet-NoMS, where the accuracy is improved due to MONet's ability to extract multi-scale features. Existing stateof-the-art online method ECONet <ref type="bibr" target="#b1">[2]</ref> requires significantly more scribbled voxels as it only relies on user-scribbles for online learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance and Workload Validation by Expert User</head><p>This experiment aims to compare the performance and perceived subjective workload of the proposed MONet with the best performing comparison method MIDeepSegTuned based on <ref type="bibr" target="#b13">[14]</ref>. We asked an expert, with 2 years of experience in lung lesion CT from Radiology Department, Oxford University Hospitals NHS Foundation Trust, to utilize each method for labelling the following pathologies as lung lesions in 10 CT volumes from UESTC-COVID-19 expert set <ref type="bibr" target="#b26">[27]</ref>: ground glass opacity, consolidation, crazy-paving, linear opacities. One CT volume is used by the expert to practice usage of our tool. The remaining 9 CT volumes were presented in a random order, where the perceived workload was evaluated by the expert at half way (after 5 segmentations) and at the end. We use the National Aeronautics and Space Administration Task Load Index (NASA-TLX) <ref type="bibr" target="#b8">[9]</ref> as per previous interactive segmentation studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. The NASA-TLX asks the expert to rate the task based on six factors, being performance, frustration, effort, mental, physical and temporal demand. The weighted NASA-TLX score is then recorded as the expert answers 15 pair-wise questions rating factors based on importance. In addition, we also recorded accuracy metrics (Dice and ASSD) against ground truth labels in <ref type="bibr" target="#b26">[27]</ref>, time taken to complete annotation and whether the expert was able to successfully complete their task within 10 min allocated for each volume. Table <ref type="table" target="#tab_2">3</ref> presents an overview for this experiment, where using the proposed MONet, the expert was able to complete 100% of the labelling task, whereas using MIDeepSegTuned they only completed 33.33% within the allocated time. In addition, MONet achieves better accuracy with lower time for complete annotation and less overall perceived workload with NASA-TLX of 52.33% as compared to 77.00% for MIDeepSegTuned. Table <ref type="table" target="#tab_3">4</ref> shows the individual scores that contribute to overall perceived workload. It shows that using the proposed MONet, the expert perceived reduced workload in all sub-scale scores except temporal demand. We believe this is due to the additional online train- ing/inference overhead for MONet application. Figure <ref type="figure" target="#fig_5">4</ref> visually compares these results where MONet results in more accurate segmentation as compared to MIDeepSegTuned. We also note that MONet's ability to apply learned knowledge on the whole volume enables it to also infer small isolated lesions, which MIDeepSegTuned fails to identify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a multi-scale online likelihood network (MONet) for scribblesbased AI-assisted interactive segmentation of lung lesions in CT volumes from COVID-19 patients. MONet consisted of a multi-scale feature extractor that enabled extraction of relevant features at different scales for improved accuracy. We proposed an adaptive online loss that utilized adaptive weights based on user-provided scribbles that enabled adaptive learning from both an initial automated segmentation and user-provided label corrections. Additionally, we proposed a dynamic label-balanced cross-entropy loss that addressed dynamic class imbalance, an inherent challenge for online interactive segmentation methods. Experimental validation showed that the proposed MONet outperformed the existing state-of-the-art on the task of annotating lung lesions in COVID-19 patients. Validation by an expert showed that the proposed MONet achieved on average 5.86% higher Dice while achieving 24.67% less perceived NASA-TLX workload score than the MIDeepSegTuned method <ref type="bibr" target="#b13">[14]</ref>.</p><p>This project utilized scribbles-based interactive segmentation tools from opensource project MONAI Label (https://github.com/Project-MONAI/MONAILabel) <ref type="bibr" target="#b6">[7]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Adaptive online training weights: (a) input, (b) foreground / background scribbles, (c) foreground and (d) background weights using τ = 0.2 in Eq. (2). (Color figure online)</figDesc><graphic coords="2,88,47,54,65,275,47,77,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Adaptive learning for interactive segmentation: (a) training and inference of MONet using adaptive loss and probability-guided pruning; (b) architecture of our multi-scale online likelihood network (MONet).</figDesc><graphic coords="3,44,61,57,80,193,51,92,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S b and |T | = |C| + |S|. y C i and y S i represent labels in C and S, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Training Parameters:</head><label></label><figDesc>Training of 3D UNet utilized a learning rate (lr) of 1e -4 for 1000 epochs and MONet/MONet-NoMS offline pre-training used 50 epochs, and lr = 1e -3 dropped by 0.1 at the 35th and 45th epoch. Online training for MONet, MONet-NoMS and ECONet [2] used 200 epochs with lr = 1e -2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Validation accuracy using synthetic scribbles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of interactive segmentation results from Sect. 3.2. Segmentations are shown with contours on axial plane slices from different cases.</figDesc><graphic coords="9,41,79,53,81,340,21,165,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>State-of-the-art evaluated comparison methods, showing improvement in accuracy (Dice and ASSD) when using different features. Features in blue text are proposed in this paper. Key: OL -online learning, PP -post-processing. This dataset contains 120 CT volumes with lesion labels, from which 50 are by expert annotators and 70 are by nonexpert annotators. To compare robustness of the proposed method against expert annotators, we only use the 50 expert labelled CT volumes.</figDesc><table><row><cell>Method</cell><cell cols="6">Technique Initial Multi Adaptive Temp. Dice ASSD</cell></row><row><cell></cell><cell></cell><cell>Seg.</cell><cell cols="2">Scale Loss</cell><cell>(τ )</cell><cell>(%)</cell></row><row><cell>MONet (proposed)</cell><cell>OL</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>77.77 11.82</cell></row><row><cell>MONet-NoMS</cell><cell>OL</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>77.06 13.01</cell></row><row><cell>ECONet [2]</cell><cell>OL</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>77.02 20.19</cell></row><row><cell cols="2">MIDeepSegTuned [14] PP</cell><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>76.00 20.16</cell></row><row><cell>MIDeepSeg [14]</cell><cell>PP</cell><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>56.85 33.25</cell></row><row><cell>IntGraphCut</cell><cell>PP</cell><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>68.58 28.64</cell></row><row><cell cols="7">latency is required. Note that incorporating more advanced deep learning meth-</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>ods in this context would result in a considerable decrease in online efficiency, rendering the method impractical for online applications</p><ref type="bibr" target="#b1">[2]</ref></p>. We utilize a GPUbased implementation of geodesic distance transform</p><ref type="bibr" target="#b0">[1]</ref> </p>in Eq. (</p>2</p>), whereas MIDeepSeg uses a CPU-based implementation. We use NVIDIA Tesla V100 GPU with 32 GB memory for all our experiments. Comparison of accuracy for each method is made using Dice similarity (Dice) and average symmetric surface distance (ASSD) metrics against ground truth annotations</p><ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref></p>. Moreover, we compare performance using execution time (Time), including online training and inference time, average full annotation time (FA-Time), and number of voxels with scribbles (S) needed for a given accuracy.</p>Data:</p>To simulate a scenario where the automatic segmentation model is trained on data from a different source than it is tested on, we utilize two different COVID-19 CT datasets. The dataset from the COVID-19 CT lesion segmentation challenge</p><ref type="bibr" target="#b20">[21]</ref> </p>is used for training and validation of 3D UNet for automatic segmentation task and patch-based pre-training of MONet/MONet-NoMS/ECONet. This dataset contains binary lung lesions segmentation labels for 199 CT volumes (160 training, 39 validation). We use UESTC-COVID-19</p><ref type="bibr" target="#b26">[27]</ref></p>, a dataset from a different source, for the experimental evaluation of interactive segmentation methods (test set).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of interactive segmentation methods using synthetic scribbler shows mean and standard deviation of Dice, ASSD, Time and Synthetic Scribbles Voxels.</figDesc><table><row><cell>Method</cell><cell>Dice (%)</cell><cell>ASSD</cell><cell>Time (s) Scribbles</cell></row><row><cell>MONet (proposed)</cell><cell cols="3">77.77 ± 6.84 11.82 ± 12.83 6.18 ± 2.42 20 ± 24</cell></row><row><cell>MONet-NoMS</cell><cell cols="3">77.06 ± 7.27 13.01 ± 15.29 7.76 ± 8.16 20 ± 24</cell></row><row><cell>ECONet [2]</cell><cell cols="3">77.02 ± 6.94 20.19 ± 14.71 1.46 ± 1.22 2283 ± 2709</cell></row><row><cell cols="4">MIDeepSegTuned [14] 76.00 ± 7.37 20.16 ± 22.57 7.97 ± 2.47 23 ± 17</cell></row><row><cell>MIDeepSeg [14]</cell><cell cols="3">56.85 ± 14.25 33.25 ± 25.26 6.26 ± 1.46 436 ± 332</cell></row><row><cell>IntGraphCut</cell><cell cols="3">68.58 ± 9.09 28.64 ± 27.36 0.11 ± 0.04 480 ± 359</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Workload validation by expert user, shows Dice (%), ASSD, full annotation time, FA-Time (s), overall NASA-TLX perceived workload score and the % of data successfully annotated by expert.</figDesc><table><row><cell></cell><cell>MONet</cell><cell>MIDeepSeg-</cell></row><row><cell></cell><cell>(proposed)</cell><cell>Tuned</cell></row><row><cell></cell><cell></cell><cell>[14]</cell></row><row><cell>Finished</cell><cell>100%</cell><cell>33.33%</cell></row><row><cell cols="2">NASA-TLX 52.33</cell><cell>77.00</cell></row><row><cell>Dice (%)</cell><cell cols="2">88.53 ± 2.27 82.67 ± 12.36</cell></row><row><cell>ASSD</cell><cell>2.91 ± 1.58</cell><cell>11.30 ± 20.09</cell></row><row><cell cols="2">FA-Time (s) 507.11</cell><cell>567.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>NASA-TLX perceived workload by expert user, shows total workload and individual sub-scale scores. The method with low score requires less effort, frustration, mental, temporal and physical demands with high perceived performance.</figDesc><table><row><cell>NASA-TLX</cell><cell>MONet</cell><cell>MIDeepSeg-</cell></row><row><cell>weighted scores</cell><cell>(proposed)</cell><cell>Tuned</cell></row><row><cell></cell><cell></cell><cell>[14]</cell></row><row><cell>Effort</cell><cell>14.67</cell><cell>21.33</cell></row><row><cell>Frustration</cell><cell>7.67</cell><cell>16.67</cell></row><row><cell cols="2">Mental Demand 13.33</cell><cell>15.00</cell></row><row><cell>Performance</cell><cell>10.00</cell><cell>17.00</cell></row><row><cell>Physical</cell><cell>4.67</cell><cell>7.00</cell></row><row><cell>Demand</cell><cell></cell><cell></cell></row><row><cell>Temporal</cell><cell>2.00</cell><cell>0.00</cell></row><row><cell>Demand</cell><cell></cell><cell></cell></row><row><cell>Total workload</cell><cell>52.33</cell><cell>77.00</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No <rs type="grantNumber">101016131</rs> (icovid project). This work was also supported by core and project funding from the <rs type="funder">Wellcome/EPSRC</rs> [<rs type="grantNumber">WT203148/Z/16/Z</rs>; <rs type="grantNumber">NS/A000049/1</rs>; <rs type="grantNumber">WT101957</rs>; <rs type="grantNumber">NS/A000027/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JNtnaNN">
					<idno type="grant-number">101016131</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_PVMGXhK">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_kWKMjhu">
					<idno type="grant-number">NS/A000049/1</idno>
				</org>
				<org type="funding" xml:id="_988JmaJ">
					<idno type="grant-number">WT101957</idno>
				</org>
				<org type="funding" xml:id="_7mzWM9u">
					<idno type="grant-number">NS/A000027/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 53.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fastgeodis: Fast generalised geodesic distance transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.00001</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ECONet: Efficient convolutional online likelihood network for scribble-based interactive segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in ND images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on active learning and human-in the-loop deep learning for medical image analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Budd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102062</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">AI-Driven CT-based quantification, staging and short-term outcome prediction of COVID-19 pneumonia</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chassagnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12852</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Pinto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12362</idno>
		<title level="m">Monai label: A framework for AI-assisted interactive labeling of 3D medical images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting when pre-trained nnU-net models fail silently for Covid-19 lung lesion segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gotkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kaltenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-2" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NASA-task load index (NASA-TLX); 20 years later</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors And Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors And Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="904" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The real-world-weight cross-entropy loss function: modeling the costs of mislabeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wookey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="4806" to="4813" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cost-sensitive learning with neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="8" to="94" />
		</imprint>
		<respStmt>
			<orgName>ECAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision And Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MIDeepSeg: minimally interactive segmentation of unseen objects from medical images using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102102</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Manual segmentation versus semi-automated segmentation for quantifying vestibular schwannoma volume on MRI</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mcgrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1445" to="1455" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The bullseye sign: a variant of the reverse halo sign in COVID-19 pneumonia</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gruden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Imaging</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="191" to="196" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MONAI: Medical Open Network for AI</title>
		<author>
			<orgName type="collaboration">MONAI Consortium</orgName>
		</author>
		<ptr target="https://github.com/Project-MONAI/MONAI" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using GOMS and NASA-TLX to to evaluate humancomputer interaction process in interactive segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Human-Computer Interact</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="134" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Study of thoracic CT in COVID-19: the STOIC project</title>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Revel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="E370" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rapid Artificial Intelligence Solutions in a Pandemic-The COVID-19-20</title>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lung CT Lesion Segmentation Challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of chest imaging in patient management during the COVID-19 pandemic: a multinational consensus statement from the Fleischner Society</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Comparative study of deep learning methods for the automatic segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tilborghs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15546</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamically balanced online random forests for interactive scribble based segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="35" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepIGeoS: a deep interactive geodesic framework for medical image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="155" to="1572" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive medical image segmentation using deep learning with imag-specific fine tuning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1562" to="1573" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive segmentation via deep learning and b-spline explicit active surfaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-230" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="315" to="325" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
