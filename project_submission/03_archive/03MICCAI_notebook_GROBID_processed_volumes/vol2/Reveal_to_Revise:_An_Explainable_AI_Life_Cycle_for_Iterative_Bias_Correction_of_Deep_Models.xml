<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models</title>
				<funder ref="#_jzNBJw5">
					<orgName type="full">German Research Foundation [DFG</orgName>
				</funder>
				<funder ref="#_TFCuksY #_xzEBXjj">
					<orgName type="full">BIFOLD</orgName>
				</funder>
				<funder ref="#_67E8Qv9 #_PdYQNvb">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_SvF4ZmH">
					<orgName type="full">Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Frederik</forename><surname>Pahde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Heinrich-Hertz-Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Dreyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Heinrich-Hertz-Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
							<email>wojciech.samek@hhi.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Heinrich-Hertz-Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Technische Universit√§t Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Foundations of Learning and Data</orgName>
								<orgName type="institution">BIFOLD -Berlin Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
							<email>sebastian.lapuschkin@hhi.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Heinrich-Hertz-Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="596" to="606"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">68F1D82341B8A1A282E277653099C891</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>XAI Life Cycle</term>
					<term>Bias Identification</term>
					<term>Model Correction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the first step (1), R2R reveals model weaknesses by finding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model's performance and remaining sensitivity towards the artifact. Using two medical benchmark datasets for Melanoma detection and bone age estimation, we apply our R2R framework to VGG, ResNet and EfficientNet architectures and thereby reveal and correct real dataset-intrinsic artifacts, as well as synthetic variants in a controlled setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations to mitigate different biases. Code is available on https:// github.com/maxdreyer/Reveal2Revise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Networks (DNNs) have successfully been applied in research and industry for a multitude of complex tasks. This includes various medical F. Pahde and M. Dreyer-Contributed equally. Fig. <ref type="figure">1</ref>. Our R2R life cycle for revealing and revising spurious behavior of any pretrained DNN. Firstly, we identify model weaknesses by finding either outliers in explanations using SpRAy (1a) or suspicious concepts using zoomed-in CRP concept visualizations (1b). Secondly <ref type="bibr" target="#b1">(2)</ref>, SpRAy clusters or collecting the top reference samples allows us to label artifactual samples and to compute an artifact CAV, which we use to model and localize the artifact in latent and input space, respectively. At this point, the artifact localization can be leveraged for (3) model correction, and (4) to evaluate the model's performance on a poisoned test set and measure its remaining attention on the artifact.</p><p>applications for which DNNs have even shown to be superior to medical experts, such as with Melanoma detection <ref type="bibr" target="#b4">[5]</ref>. However, the reasoning of these highly complex and non-linear models is generally not transparent <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, and as such, their decisions may be biased towards unintended or undesired features, potentially caused by shortcut learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. Particularly in high-stake decision processes, such as medical applications, unreliable or poorly understood model behavior may pose severe security risks.</p><p>The field of XAI brings light into the black boxes of DNNs and provides a better understanding of their decision processes. As such, local XAI methods reveal (input) features that are most relevant to a model, which, for image data, can be presented as heatmaps. In contrast, global XAI methods (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>) reveal general prediction strategies employed or features encoded by a model, which is necessary for the identification and understanding of systematic (mis-)behavior. Acting on the insights from explanations, various methods have been introduced to correct for undesired model behavior <ref type="bibr" target="#b30">[31]</ref>. While multiple approaches exist for either revealing or revising model biases, only few combine both steps, to be applicable as a framework. Such frameworks, however, either rely heavily on human feedback <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, are limited to specific bias types <ref type="bibr" target="#b1">[2]</ref>, or require laborintensive annotations for both model evaluation and correction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>To that end, we propose Reveal to Revise (R2R), an iterative XAI life cycle requiring low amounts of human interaction that consists of four phases, illustrated in Fig. <ref type="figure">1</ref>. Specifically, R2R allows to first (1) identify spurious model behavior and secondly, to (2) label and localize artifacts in an automated fashion. The generated annotations are then leveraged to (3) correct and (4) (re-)evaluate the model, followed by a repetition of the entire life cycle if required. For revealing model bias, we propose two orthogonal XAI approaches: While Spectral Relevance Analysis (SpRAy) <ref type="bibr" target="#b13">[14]</ref> automatically finds outliers in model explanations (potentially caused by the use of spurious features), Concept Relevance Propagation (CRP) <ref type="bibr" target="#b0">[1]</ref> precisely communicates the globally learned concepts of a DNN. For model revision, we apply and compare the methods of Class Artifact Compensation (ClArC) <ref type="bibr" target="#b1">[2]</ref>, Contextual Decomposition Explanation Penalization (CDEP) <ref type="bibr" target="#b19">[20]</ref> and Right for the Right Reason (RRR) <ref type="bibr" target="#b21">[22]</ref>, penalizing attention on artifacts via ground truth masks automatically generated in step <ref type="bibr" target="#b1">(2)</ref>. The artifact masks are further used for evaluation on a poisoned test set and to measure the remaining attention on the bias. We demonstrate the applicability and high automation of R2R on two medical tasks, including Melanoma detection and bone age estimation, using the VGG-16, ResNet-18 and EfficientNet-B0 DNN architectures. In our experiments, we correct model behavior w.r.t. dataset-intrinsic, as well as synthetic artifacts in a controlled setting. Lastly, we showcase the R2R life cycle through multiple iterations, unveiling and unlearning different biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Among other methods, e.g., leveraging auxiliary information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>, or training on de-biased representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>, shortcut unlearning is often approached with XAI. The majority of related works introduce methods to either identify spurious behavior <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, or to align the model behavior with pre-defined priors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, with only a few combining both, such as the eXplanatory Interactive Learning (XIL) framework <ref type="bibr" target="#b28">[29]</ref> or the approach introduced by Anders et al. <ref type="bibr" target="#b1">[2]</ref>. The former is based on presenting individual local explanations to a human, who, if necessary, provides feedback used for model correction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>. However, studying individual predictions is slow and labor-extensive, limiting its practicability. In contrast, the authors of <ref type="bibr" target="#b1">[2]</ref> use SpRAy <ref type="bibr" target="#b13">[14]</ref> for the detection of spurious model behavior and labeling of artifactual samples. In addition to SpRAy, we suggest to study latent features of the model via CRP concept visualizations <ref type="bibr" target="#b0">[1]</ref> as a tool for more fine-grained model inspection, catching systematic misbehavior which would not be visible through SpRAy clusters.</p><p>Most model correction methods require dense annotations, such as labels for artifactual samples or artifact localization masks, which are either crafted heuristically or by hand <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. In our R2R framework, we automate the annotation by following <ref type="bibr" target="#b1">[2]</ref> for data labeling through SpRAy outlier clusters, or by collecting the most representative samples of bias concepts according to CRP. The spatial artifact localization is further automated by computing artifact heatmaps as outlined in Sect. 3.1, thereby considerably easing the step from bias identification to correction.</p><p>Existing works for model correction measure the performance on the original or clean test set, with corrected models often showing an improved generalization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. A more targeted approach for measuring the artifact's influence is the evaluation on poisoned data <ref type="bibr" target="#b24">[25]</ref>, for which R2R is well suited by using its localization scheme to first extract artifacts and to then poison clean test samples. By precisely localizing artifacts, R2R further allows to measure the model's attention on an artifact through attribution heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reveal to Revise Framework</head><p>Our Reveal to Revise (R2R) framework comprises the entire XAI life cycle, including methods for (1) the identification of model bias, (2) artifact labeling and localization, (3) the correction of detected misbehavior, and (4) the evaluation of the improved model. To that end, we now describe the methods used for R2R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Artifact Identification and Localization</head><p>The identification of spurious data artifacts using CRP concept visualizations or SpRAy clusters is firstly described, followed by our artifact localization approach. CRP Concept Visualizations. CRP <ref type="bibr" target="#b0">[1]</ref> combines global concept visualization techniques with local feature attribution methods. This provides an understanding of the relevance of latent concepts for a prediction and their localization in the input. In this work, we use Layer-wise Relevance Propagation (LRP) <ref type="bibr" target="#b2">[3]</ref> for feature attribution under CRP and for heatmaps in general, however, other local XAI methods can be used as well. Jointly with Relevance Maximization <ref type="bibr" target="#b0">[1]</ref>, CRP is well suited for the identification of spurious concepts by precisely narrowing down the input parts that have been most relevant for model inference, as shown in Fig. <ref type="figure">1</ref> (bottom left) for band-aid concepts, where irrelevant background is overlaid with black semi-transparent color. The collection of top-ranked reference samples for spurious concepts allows us to label artifactual data.</p><p>Explanation Outliers Through SpRAy. Alternatively, SpRAy <ref type="bibr" target="#b13">[14]</ref> is a strategy to find outliers in local explanations, which are likely to stem from spurious model behavior, such as the use of a Clever Hans features, i.e., features correlating with a certain class that are unrelated to the actual task. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>, we apply SpRAy by clustering latent attributions computed through LRP. The SpRAy clusters then naturally allow us to label data containing the bias.</p><p>Artifact Localization. We automate artifact localization by training a Concept Activation Vector (CAV) h l to model the artifact in latent space of a layer l, representing the direction from artifactual to non-artifactual samples obtained from a linear classifier. The artifact localization is given by a modified backward pass on the biased model with LRP for an artifact sample x, where we initialize the relevances R l (x) at layer l as</p><formula xml:id="formula_0">R l (x) = a l (x) ‚Ä¢ h l (1)</formula><p>with activations a l and element-wise multiplication operator ‚Ä¢. This is equivalent to explaining the output from the linear classifier given as a l (x) ‚Ä¢ h l . Using a threshold, the resulting CAV heatmap can be further processed into a binary mask to crop out the artifact from any corrupted sample, as illustrated in Fig. <ref type="figure">1</ref> (bottom center ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methods for Model Correction</head><p>In the following, we present the methods used for mitigating model biases.</p><p>ClArC for Latent Space Correction. Methods from the ClArC framework correct model (mis-)behavior w.r.t. an artifact by modeling its direction h in latent space using CAVs <ref type="bibr" target="#b11">[12]</ref>. The framework consists of two methods, namely Augmentive ClArC (a-ClArC) and Projective ClArC (p-ClArC). While a-ClArC adds h l to the activations a l of layer l for all samples in a fine-tuning phase, hence teaching the model to be invariant towards that direction, p-ClArC suppresses the artifact direction during the test phase and does not require any fine-tuning. More precisely, the perturbed activations a l are given by</p><formula xml:id="formula_1">a l (x) = a l (x) + Œ≥(x)h l (2)</formula><p>with perturbation strength Œ≥(x) dependent on input x. Parameter Œ≥(x) is chosen such that the activation in direction of the CAV is as high as the average value over non-artifactual or artifactual samples for p-ClArC or a-ClArC, respectively.</p><p>RRR and CDEP for Correction Through Prior Knowledge. Model correction using RRR <ref type="bibr" target="#b21">[22]</ref> or CDEP <ref type="bibr" target="#b19">[20]</ref> is based on an additional Œª-weighted loss term (besides the cross-entropy loss L CE ) for neural network training that aligns the use of features by the model f Œ∏ , described by an explanation exp Œ∏ , to a defined prior explanation exp prior . The authors of RRR propose to penalize the model's attention on unfavorable artifacts using the input gradient w.r.t. the cross-entropy loss, leading to</p><formula xml:id="formula_2">L RRR exp Œ∏ (x), exp prior (x) = ‚àá x L CE (f Œ∏ (x), y true ) ‚Ä¢ M prior (x) 2 2<label>(3)</label></formula><p>with a binary mask M prior (x) localizing an artifact and class label y true . Alternatively, CDEP <ref type="bibr" target="#b19">[20]</ref> proposes to use CD <ref type="bibr" target="#b16">[17]</ref> importance scores Œ≤(x s ) for a feature subset x s based on the forward pass instead of gradient to align the model's attention. Penalizing artifact features via masked input x M results in</p><formula xml:id="formula_3">L CDEP exp Œ∏ (x), exp prior (x) = e Œ≤ (xM ) e Œ≤ (xM ) + e Œ≤ (x-xM ) 1 .</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The experimental section is divided into the two parts of (1) identification, mitigation and evaluation of spurious model behavior with various correction methods and (2) showcasing the whole R2R framework in an iterative fashion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We train VGG-16 <ref type="bibr" target="#b25">[26]</ref>, ResNet-18 <ref type="bibr" target="#b10">[11]</ref> and EfficientNet-B0 <ref type="bibr" target="#b27">[28]</ref> models on the ISIC 2019 dataset <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref> for skin lesion classification and Pediatric Bone Age dataset <ref type="bibr" target="#b9">[10]</ref> for bone age estimation based on hand radiographs. Besides evaluating our methodology on data-intrinsic artifacts occurring in these datasets, we artificially insert an artifact into data samples in a controlled setting. Specifically, we insert a "Clever Hans" text (shown in Fig. <ref type="figure" target="#fig_0">2</ref>) into a subset of training samples of one specific class. See Appendix A.1 for additional experiment details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Revealing and Revising Spurious Model Behavior</head><p>Revealing Bias: In the first step of the R2R life cycle, we can reveal the use of several artifacts by the examined models, including the well-known band-aid, ruler and skin marker <ref type="bibr" target="#b5">[6]</ref> and our synthetic Clever Hans for the ISIC dataset, as shown in Fig. <ref type="figure" target="#fig_0">2</ref> for VGG-16. Here, we show concept visualizations and cropped out artifacts based on our automatic artifact localization scheme described in Sect. 3.1. The "band-aid" use can be further identified via SpRAy, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref> (right). Besides the synthetic Clever Hans for bone age classification, we encountered the use of "L" markings, resulting from physical lead markers placed by radiologist to specify the anatomical side. Interestingly, the "L" markings are larger for hands of younger children, as all hands are scaled to similar size <ref type="bibr" target="#b9">[10]</ref>, offering the model to learn a shortcut by estimating the bone age based on the relative size of the "L" markings, instead of valid features. While we revealed the "L" marking bias using CRP, we did not find corresponding SpRAy clusters, underlining the importance of both approaches for model investigation.</p><p>Revising Model Behavior: Having revealed spurious behavior, we now revise the models, beginning with model correction. Specifically, we correct for the band-aid, "L" markings as well as synthetic artifacts. The skin marker and ruler  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Iterative Model Correction with R2R</head><p>Showcasing the full R2R life cycle (as shown in Fig. <ref type="figure">1</ref>), we now perform multiple R2R iterations, revealing and revising undesired model behavior step by step. Specifically, we successively correct the VGG-16 model w.r.t. the skin marker, band-aid, and ruler artifacts discovered in Sect. 4.2 using RRR. In order to prevent the model from re-learning previously unlearned artifacts, we keep the previous artifact-specific RRR losses intact. Thus, we are able to correct for all artifacts, with evaluation results given in Appendix A.2, applying the same metrics as in Sect. 4.2. In Fig. <ref type="figure" target="#fig_1">3</ref>, we show exemplary attribution heatmaps for all artifacts after each iteration. While there are large amounts of relevance on all artifacts initially, it can successfully be reduced in the according iterations to correct the model behavior w.r.t. skin marker (SM), band-aids (BA), and rulers (R). It is to note, that correcting for the skin marker also (slightly) improved the model w.r.t. other artifacts, which might result from corresponding latent features that are not independent, as shown by CRP visualizations in Fig. <ref type="figure" target="#fig_0">2</ref> for skin marker. Moreover, we show the SpRAy embedding of training samples after the first iteration in Fig. <ref type="figure" target="#fig_1">3</ref> (right), revealing an isolated cluster with samples containing the band-aid artifact, which dissipates after the correction step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present R2R, an XAI life cycle to reveal and revise spurious model behavior requiring minimal human interaction via high automation. To reveal model bias, R2R relies on CRP and SpRAy. Whereas SpRAy automatically points out Clever Hans behavior by analyzing large sets of attribution data, CRP allows for a finegrained investigation of spurious concepts learned by a model. Moreover, CRP is ideal for large datasets, as the concept space dimension remains constant. By automatically localizing artifacts, we successfully perform model revision, thereby reducing attention on the artifact and leading to improved performance on corrupted data. When applying R2R iteratively, we did not find the emergence of new biases, which, however, might happen if larger parts of the model are finetuned or retrained to correct strong biases. Future research directions include the application to non-localizable artifacts, and addressing fairness issues in DNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Overview of artifacts with CRP visualizations of corresponding concepts zoomed-in using receptive field information (top), input samples (middle), and cropped out artifacts (bottom) using our artifact localization method. Shown are band-aid, ruler, skin marker, and synthetic artifacts for the ISIC dataset, as well as "L"-marker and synthetic artifacts for the Bone Age dataset.</figDesc><graphic coords="6,63,90,71,93,332,56,82,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The effect of iterative model correction on relevances attributed to artifacts for each iteration (left) and the band-aid artifact cluster from SpRAy, which dissipates after its correction step (right). See Appendix A.2 for quantitative results.</figDesc><graphic coords="8,67,02,62,24,318,28,138,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>applying a-ClArC barely decreases the relevance attributed to artifacts in input space. This might result from ClArC methods not directly penalizing the use of artifacts, but instead encouraging the model to develop alternative prediction strategies. Overall, RRR yields the most consistent results, constantly reducing the artifact relevance while increasing the model performance on poisoned test sets. Both observations are underlined by heatmaps for revised models in Fig. A.1 (Appendix A.2), where RRR and CDEP visibly reduce the model attention on the artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,55,98,54,17,340,27,185,53" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>CDEP is not applied to EfficientNets, as existing implementations are incompatible.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by the <rs type="funder">Federal Ministry of Education and Research (BMBF)</rs> as grants [SyReal (<rs type="grantNumber">01IS21069B</rs>), <rs type="funder">BIFOLD</rs> (<rs type="grantNumber">01IS18025A</rs>, <rs type="grantNumber">01IS18037I</rs>)]; the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme (EU Horizon 2020</rs>) as grant [iToBoS (965221)]; the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2022 research and innovation programme (EU Horizon Europe</rs>) as grant [TEMA (101093003)]; the state of Berlin within the <rs type="programName">innovation support program ProFIT (IBB)</rs> as grant [BerDiBa (10174498)]; and the <rs type="funder">German Research Foundation [DFG</rs> <rs type="grantNumber">KI-FOR 5363</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SvF4ZmH">
					<idno type="grant-number">01IS21069B</idno>
				</org>
				<org type="funding" xml:id="_TFCuksY">
					<idno type="grant-number">01IS18025A</idno>
				</org>
				<org type="funding" xml:id="_xzEBXjj">
					<idno type="grant-number">01IS18037I</idno>
				</org>
				<org type="funding" xml:id="_67E8Qv9">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme (EU Horizon 2020</orgName>
				</org>
				<org type="funding" xml:id="_PdYQNvb">
					<orgName type="program" subtype="full">Horizon 2022 research and innovation programme (EU Horizon Europe</orgName>
				</org>
				<org type="funding" xml:id="_jzNBJw5">
					<idno type="grant-number">KI-FOR 5363</idno>
					<orgName type="program" subtype="full">innovation support program ProFIT (IBB)</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 56. We evaluate the effectiveness of model corrections based on two metrics: the attributed fraction of relevance to artifacts and prediction performance on both the original and a poisoned test set (in terms of F1-score and accuracy). Whereas in the synthetic case, we simply insert the artifact into all samples to poison the test set, data-intrinsic artifacts are cropped from random artifactual samples using our artifact localization strategy. Note that artifacts might overlap clinically informative features in poisoned samples, limiting the comparability of poisoned and original test performance. As shown in Tab. 1 (ISIC 2019) and Appendix A.2 (Bone Age), we are generally able to improve model behavior with all methods. The only exception is the synthetic artifact for VGG-16, where only RRR mitigates the bias to a certain extent, indicating that the artifact signal is too strong for the model. Here, fine-tuning only the last layer is not sufficient to learn alternative prediction strategies. Interestingly, despite successfully decreasing the models' output sensitivity towards artifacts,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Achtibat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03208</idno>
		<title level="m">From &quot;where&quot; to &quot;what&quot;: towards human-understandable explanations through concept relevance propagation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding and removing clever hans: using explanation methods to debug and improve deep models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="261" to="295" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="528" to="539" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image classification task</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="47" to="54" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of the ISIC image datasets: usage, benchmarks and recommendations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brodzicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jaworek-Korjakowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102305</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">BCN20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The RSNA pediatric bone age machine learning challenge</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Halabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="503" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning not to learn: training deep neural networks with biased data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9012" to="9020" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unmasking clever hans predictors and assessing what machines really learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>W√§ldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1096</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Causally motivated shortcut removal using auxiliary labels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Makar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'amour</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="739" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05453</idno>
		<title level="m">Beyond word importance: contextual decomposition to extract interactions from lstms</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nauta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization in the presence of nuisance-induced spurious correlations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Puli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Oermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00520</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretations are useful: penalizing explanations to align neural networks with prior knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8116" to="8126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Can contrastive learning avoid shortcut solutions?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4974" to="4986" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Right for the right reasons: training differentiable models by constraining their explanations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining deep neural networks and beyond: a review of methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="278" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making deep neural networks right for the right scientific reasons by interacting with their explanations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schramowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="476" to="486" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convnets and imagenet beyond accuracy: understanding mistakes and uncovering biases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="498" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficientnet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explanatory interactive machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="239" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond explaining: opportunities and challenges of XAI-based model improvement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
