<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation</title>
				<funder ref="#_9cqeG9P">
					<orgName type="full">Central Research Fund</orgName>
					<orgName type="abbreviated">CRF</orgName>
				</funder>
				<funder ref="#_vmSRtvv #_Ske5Bnj">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_7JaNzSW">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_qkVapfb">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_3J89UbN">
					<orgName type="full">Guangdong Provincial Department of Education</orgName>
				</funder>
				<funder ref="#_rCbKE8F">
					<orgName type="full">Shenzhen Natural Science Fund</orgName>
				</funder>
				<funder ref="#_kVAjyHS">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_F2cTqVQ">
					<orgName type="full">Agency for Science, Technology and Research (A*STAR) Advanced Manufacturing and Engineering</orgName>
					<orgName type="abbreviated">AME</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Research Institute of Trustworthy Autonomous Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Medical Intelligence and Innovation Academy</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haojin</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<email>zhaow3@sustech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Research Institute of Trustworthy Autonomous Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Medical Intelligence and Innovation Academy</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Southern University of Science and Technology Hospital</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiuyun</forename><surname>Su</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Medical Intelligence and Innovation Academy</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Southern University of Science and Technology Hospital</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<email>liuj@sustech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Research Institute of Trustworthy Autonomous Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Medical Intelligence and Innovation Academy</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation</orgName>
								<orgName type="institution">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="127" to="136"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7FB13B4DDDD2231CEE7E6E7C1CA00CCB</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image segmentation</term>
					<term>single-source domain generalization</term>
					<term>domain augmentation</term>
					<term>frequency spectrum</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The annotation scarcity of medical image segmentation poses challenges in collecting sufficient training data for deep learning models. Specifically, models trained on limited data may not generalize well to other unseen data domains, resulting in a domain shift issue. Consequently, domain generalization (DG) is developed to boost the performance of segmentation models on unseen domains. However, the DG setup requires multiple source domains, which impedes the efficient deployment of segmentation algorithms in clinical scenarios. To address this challenge and improve the segmentation model's generalizability, we propose a novel approach called the Frequency-mixed Single-source Domain Generalization method (FreeSDG). By analyzing the frequency's effect on domain discrepancy, FreeSDG leverages a mixed frequency spectrum to augment the single-source domain. Additionally, self-supervision is constructed in the domain augmentation to learn robust context-aware representations for the segmentation task. Experimental results on five datasets of three modalities demonstrate the effectiveness of the proposed algorithm. FreeSDG outperforms state-of-the-art methods and significantly improves the segmentation model's generalizability. Therefore, FreeSDG provides a promising solution for enhancing the generalization of medical image segmentation models, especially when annotated data is scarce. The code is available at https://github.com/liamheng/Non-IID Medical Image Segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the superiority in image representation, tremendous success has been achieved in medical image segmentation through recent advancements of deep learning <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, sufficient labeled training data is necessary for deep learning to learn state-of-the-art segmentation networks, resulting in the burden of costly and labor-intensive pixel-accurate annotations <ref type="bibr" target="#b1">[2]</ref>. Consequently, annotation scarcity has become a pervasive bottleneck for clinically deploying deep networks, and existing similar datasets have been resorted to alleviate the annotation burden. However, networks trained on a single-source dataset may suffer performance dropping when applied to clinical datasets, since neural networks are sensitive to domain shifts.</p><p>Consequently, domain adaptation (DA) and DG <ref type="bibr" target="#b13">[14]</ref> have been leveraged to mitigate the impact of domain shifts between source and target domains/datasets. Unfortunately, DA relies on a strong assumption that source and target data are simultaneously accessible <ref type="bibr" target="#b3">[4]</ref>, which does not always hold in practice. Thereby, DG has been introduced to overcome the absence of target data, which learns a robust model from distinct source domains to generalize to any target domain. To efficiently transfer domain knowledge across various source domains, FACT <ref type="bibr" target="#b11">[12]</ref> has been designed to adapt the domains by swapping the low-frequency spectrum of one with the other. Considering privacy protection in medical scenarios, federated learning and continuous frequency space interpolation were combined to achieve DG on medical image segmentation <ref type="bibr" target="#b4">[5]</ref>. More recently, single-source domain generalization (SDG) <ref type="bibr" target="#b8">[9]</ref> has been proposed to implement DG without accessing multi-source domains. Based on global intensity non-linear augmentation (GIN) and interventional pseudocorrelation augmentation (IPA), a causality-inspired SDG was designed in <ref type="bibr" target="#b6">[7]</ref>. Although DG has boosted the clinical practice of deep neural networks, troublesome challenges still remain in clinical deployment. 1) Data from multi-source domains are commonly required to implement DG, which is costly and even impractical to collect in clinics. 2) Medical data sharing is highly concerned, accessing multi-source domains exacerbates the risk of data breaching. 3) Additional generative networks may constrain algorithms' efficiency and versatility, negatively impacting clinical deployment.</p><p>To circumvent the above challenges, a frequency-mixed single-source domain generalization strategy, called FreeSDG, is proposed in this paper to learn generalizable segmentation models from a single-source domain. Specifically, the impact of frequency on domain discrepancy is first explored to test our hypotheses on domain augmentation. Then based on the hypotheses, diverse frequency views are extracted from medical images and mixed to augment the single-source domain. Simultaneously, a self-supervised task is posed from frequency views to learn robust context-aware representations. Such that the representations are injected into the vanilla segmentation task to train segmentation networks for out-of-domain inference. Our main contributions are summarised as follows: -We design an efficient SDG algorithm named FreeSDG for medical image segmentation by exploring the impact of frequency on domain discrepancy and mixing frequency views for domain augmentation. -Through identifying the frequency factor for domain discrepancy, a frequencymixed domain augmentation (FMAug) is proposed to extend the margin of the single-source domain. -A self-supervised task is tailored with FMAug to learn robust context-aware representations, which are injected into the segmentation task. -Experiments on various medical image modalities demonstrate the effectiveness of the proposed approach, by which data dependency is alleviated and superior performance is presented when compared with state-of-the-art DG algorithms in medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Aiming to robustly counter clinical data from unknown domains, an SDG algorithm for medical image segmentation is proposed, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A generalizable segmentation network is attempted to be produced from a single-source domain (x, m) ∼ D(x, m), where m ∈ R H×W is the segmentation mask for the image x ∈ R H×W ×3 . By mixing frequency spectrums, FMAug is executed to augment the single-source domain, and self-supervision is simultaneously acquired to learn context-aware representations. Thus a medical image segmentation network capable of out-of-domain generalization is implemented from a single-source domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frequency-controlled Domain Discrepancy</head><p>Generalizable algorithms have been developed using out-of-domain knowledge to circumvent the clinical performance dropping caused by domain shifts. Nevertheless, extra data dependency is often inevitable in developing the generalizable algorithms, limiting their clinical deployment. To alleviate the data dependency, a single source generalization strategy is designed inspired by the Fourier domain adaption <ref type="bibr" target="#b12">[13]</ref> and generalization <ref type="bibr" target="#b11">[12]</ref>.</p><p>According to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, the domain shifts between the source and target could be reduced by swapping/integrating the low-frequency spectrum (LFS) of one with the other. Thus we post two hypotheses:</p><p>1) uniformly removing the LFS reduces inter-and inner-domain shifts; 2) discriminatively removing the LFS from a single domain increases innerdomain discrepancy.</p><p>Various frequency views are thus extracted from medical images with changing parameters to verify the above hypotheses. Denote the frequency filter with parameters θ n as F n (•), where n ∈ R N +1 refers to the index of parameters. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, a frequency view acquired with θ n from an image x is given by xn = F n (x) = x -x * g(r n , σ n ), where g(r n , σ n ) denotes a Gaussian filter with radius r n ∈ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">50]</ref> and spatial constant σ n ∈ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">22]</ref>. Then the frequency views are converted to vectors by a pre-trained ResNet-18 and t-SNE is employed to demonstrate the domain discrepancy controlled by the low-frequency spectrum.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, compared to the raw images, the distribution of various datasets is more clustered after the uniform LFS removement, which indicates domain shift reduction. While the domain discrepancy in DRIVE is increased by discriminatively removing the LFS. Accordingly, these hypotheses can be leveraged to implement SDG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Frequency-mixed Domain Augmentation</head><p>Motivated by the hypotheses, domain augmentation is implemented by F n (•) with perturbed parameters. Moreover, the local-frequency-mix is executed to further extend the domain margin, as shown in Fig. <ref type="figure" target="#fig_1">2 (d)</ref>. As exhibited in the blue block of Fig. <ref type="figure" target="#fig_0">1</ref>, random patches are cut from a frequency view and mixed with diverse ones to conduct FMAug, which is given by</p><formula xml:id="formula_0">xk = M (x i , xj ) = M xi + (1 -M ) xj , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where M ∈ 0, 1 W ×H is a binary mask controlling where to drop out and fill in from two images, and is element-wise multiplication. k = (i -1) × N + (j -1) denotes the index of the augmentation outcomes, where i, j ∈ R N , i = j. Notably, self-supervision is simultaneously acquired from FMAug, where only patches from N frequency views xn , n ∈ R N are mixed, and the rest one x0 is cast as a specific view to be reconstructed from the mixed ones, where (r n , σ n ) = (27, 9). Under the self-supervision, an objective function for learning contextaware representations from view reconstruction is defined as</p><formula xml:id="formula_2">L sel = E K k=1 x0 -xk 1 .</formula><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>where xk refers to the view reconstructed from xk , K = N × (N -1). Consequently, FMAug not only extends the domain discrepancy and margin, but also poses a self-supervised pretext task to learn generalizable context-aware representations from view reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Coupled Segmentation Network</head><p>As the FMAug promises domain-augmented training data and generalizable context-aware representations, a segmentation model capable of out-of-domain inference is waiting to be learned. To inject the context-aware representations into the segmentation model seamlessly, a coupled network is designed with attention mechanisms (shown in the purple block of Fig. <ref type="figure" target="#fig_0">1</ref>), which utilize the most relevant parts of representation in a flexible manner.</p><p>Concretely, the network comprises an encoder E and two decoders D sel , D seg , where skip connection bridges E and D sel while D sel marries D seg using attention mechanisms. For the above pretext task, E and D sel compose a U-Net architecture to reconstruct x0 from xk with the objective function given in Eq. 2. On the other hand, the segmentation task shares E with the pretext task, and introduces representations from D sel to D seg . The features outcomes from the l-th layer of D seg are given by</p><formula xml:id="formula_4">f l seg = D l seg ([f l-1 seg , f l-1 sel ]), l = 1, 2, ..., L,<label>(3)</label></formula><p>where f l sel refers to the features from the l-th layer of D sel . Additionally, attention modules are implemented to properly couple the features from D sel and D seg . D l seg imports and concatenates f l-<ref type="foot" target="#foot_0">1</ref> seg and f l-1 sel as a tensor. Subsequently, the efficient channel and spatial attention modules proposed by <ref type="bibr" target="#b10">[11]</ref> are executed to couple the representations learned from the pretext and segmentation task.</p><p>Then convolutional layers are used to generate the final outcome f l seg . Accordingly, denote the segmentation result from xk as mk , the objective function for segmentation task is given by</p><formula xml:id="formula_5">L seg = E K k=1 [-m log mk -(1 -m) log (1 -mk )] . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where m denotes the ground-truth segmentation mask corresponding to the original source sample x. Therefore, the overall objective function for the network is defined as</p><formula xml:id="formula_7">L total = L sel (E, D sel ) + αL seg (E, D sel , D seg ), (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where α is the hyper-parameter to balance L sel and L seg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Implementation: Five image datasets of three modalities were collected to conduct segmentation experiments on fundus vessels and articular cartilage. For fundus vessels, training was based on 1) DRIVE The image data were resized to 512 × 512, the training batch size was 2, and Adam optimizer was used. The model was trained according to an earlystop mechanism, which means the optimal parameter on the validation set was selected in the total 200 epochs, where the learning rate is 0.001 in the first 80 epochs and decreases linearly to 0 in the last 120 epochs. The encoder and two decoders are constructed based on the U-net architecture with 8 layers. The comparisons were conducted with the same setting and were quantified by DICE and Matthews's correlation coefficient (Mcc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison and Ablation Study:</head><p>The effectiveness of the proposed algorithm is demonstrated in comparison with state-of-the-art methods and an ablation study. The Fourior-based DG methods FACT <ref type="bibr" target="#b11">[12]</ref>, FedDG <ref type="bibr" target="#b4">[5]</ref>, and the whitening-based DG method SAN-SAW <ref type="bibr" target="#b7">[8]</ref>, as well as the SGD method GIN-IPA <ref type="bibr" target="#b6">[7]</ref> were compared, where CE-Net <ref type="bibr" target="#b0">[1]</ref> and CS-Net <ref type="bibr" target="#b5">[6]</ref> were served as the base models cooperated with FACT <ref type="bibr" target="#b11">[12]</ref>. Then in the ablation study, FMAug, selfsupervised learning (SSL), and attention mechanisms (ATT) were respectively removed from the proposed algorithm.</p><p>(1) Comparison. Quantified comparison of our algorithm with the competing methods is summarized in Table <ref type="table" target="#tab_1">1</ref>, where segmentation results in three modalities and data dependency are exhibited. Due to the domain shifts between DRIVE and LES-AV as well as IOSTAR, interior performance are presented by CE-Net <ref type="bibr" target="#b0">[1]</ref> and CS-Net <ref type="bibr" target="#b5">[6]</ref>, which are only learned from DRIVE without DG. Due to the substantial domain discrepancy, EyePACS were treated as multiple source domains to implement DG. FACT <ref type="bibr" target="#b11">[12]</ref> boosts the generalization by transferring LFS across the multi-source domains, and efficiently promotes the performance of CE-Net <ref type="bibr" target="#b0">[1]</ref> and CS-Net <ref type="bibr" target="#b5">[6]</ref>. FedDG <ref type="bibr" target="#b4">[5]</ref> were then respectively trained using DRIVE perturbed by EyePACS. As SAN-SAW <ref type="bibr" target="#b7">[8]</ref> was designed for region structure segmentation, it appears redundant in the vessel structure task. Thanks to coupling federated learning and contrastive learning, reasonable performance are provided by FedDG <ref type="bibr" target="#b4">[5]</ref>. GIN-IPA <ref type="bibr" target="#b6">[7]</ref> and our FreeSDG were learned based on the single source domain of DRIVE. Through augmenting the source domain with intensity variance and consistency constraint, GIN-IPA <ref type="bibr" target="#b6">[7]</ref> performs decently on out-of-domain inference. The proposed FreeSDG allows for learning efficient segmentation models only from DRIVE. Therefore, our FreeSDG outperforms the state-of-the-art methods without extra data dependency. Additionally, an iden- tical situation is observed from the results of ultrasound data, further validating the effectiveness of our algorithm.</p><p>Visualized comparison is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Uneven brightness in LES-AV impacts the segmentation performance, vessels in the highlight box are ignored by most algorithms. Cooperating with FACT <ref type="bibr" target="#b11">[12]</ref>, CE-Net <ref type="bibr" target="#b0">[1]</ref> achieves impressive performance. The remarkable performance of GIN-IPA <ref type="bibr" target="#b6">[7]</ref> indicates that SDG is a promising paradigm for generalizable segmentation. In the cross-modality segmentation in IOSTAR, CE-Net <ref type="bibr" target="#b0">[1]</ref> married with FACT <ref type="bibr" target="#b11">[12]</ref> and GIN-IPA <ref type="bibr" target="#b6">[7]</ref> still performs outstandingly. In addition, decent segmentation is also observed from FedDG <ref type="bibr" target="#b4">[5]</ref> via DG with multi-source domains. FreeSDG efficiently recognizes the variational vessels in LES-AV and IOSTAR, indicating its robustness and generalizability in the quantitative comparison. Furthermore, FreeSDG outperforms the competing methods in accurately segmenting low-contrast cartilage of ultrasound images. In nutshell, our SDG strategy promises FreeSDG prominent performance without extra data dependency.</p><p>(2) Ablation Study. According to Table <ref type="table" target="#tab_1">1</ref>, the ablation study also validates the effectiveness of the three designed modules. Through FMAug, an augmented source domain with adequate discrepancy is constructed for training generalizable models. Robust context-aware representations are extracted from self-supervised learning, boosting the downstream segmentation task. Attention mechanisms seamlessly inject the context-aware representations into segmentation, further improving the proposed algorithm. Therefore, a promising segmentation model for medical images is learned from a single-source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Pixel-accurate annotations have long been a common bottleneck for developing medical image segmentation networks. Segmentation models learned from a single-source dataset always suffer performance dropping on out-of-domain data. Leveraging DG solutions bring extra data dependency, limiting the deployment of segmentation models. In this paper, we proposed a novel SDG strategy called FreeSDG that leverages a frequency-based domain augmentation technique to extend the single-source domain discrepancy and injects robust representations learned from self-supervision into the network to boost segmentation performance. Our experimental results demonstrated that the proposed algorithm outperforms state-of-the-art methods without requiring extra data dependencies, providing a promising solution for developing accurate and generalizable medical image segmentation models. Overall, our approach enables the development of accurate and generalizable segmentation models from a single-source dataset, presenting the potential to be deployed in real-world clinical scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of FreeSDG, which learns a generalizable segmentation network from a single-source domain. FMAug extends the domain margin by mixing patches (orange boxes) from diverse frequency views, and poses a self-supervised task to learn contextaware representations. The representations are injected into segmentation using attention mechanisms in the coupled network to achieve a generalizable model. (Color figure online)</figDesc><graphic coords="3,55,98,53,69,340,15,152,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data distribution visualized by t-SNE. Uniformly removing LFS reduces shifts between DRIVE, IOSTAR, and LES-AV. Discriminatively removing the LFS increases the discrepancy in DRIVE. FMAug extends the margin of DRIVE.</figDesc><graphic coords="4,41,79,54,32,340,18,110,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation comparison in three medical image modalities.</figDesc><graphic coords="7,70,47,54,23,311,35,206,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons and ablation study</figDesc><table><row><cell>Algorithms</cell><cell>Dependency*</cell><cell>LES-AV</cell><cell>IOSTAR</cell><cell>Ultrasound</cell></row><row><cell></cell><cell>IID MSD</cell><cell>DICE Mcc</cell><cell>DICE Mcc</cell><cell>DICE Mcc</cell></row><row><cell>CE-Net</cell><cell></cell><cell cols="3">0.636 0.618 0.505 0.514 0.788 0.796</cell></row><row><cell>CS-Net</cell><cell></cell><cell cols="3">0.593 0.559 0.520 0.521 0.699 0.721</cell></row><row><cell>CE-Net+FACT</cell><cell></cell><cell cols="3">0.730 0.711 0.728 0.705 0.846 0.846</cell></row><row><cell>CS-Net+FACT</cell><cell></cell><cell cols="3">0.725 0.705 0.580 0.572 0.829 0.827</cell></row><row><cell>SAN-SAW</cell><cell></cell><cell cols="3">0.629 0.599 0.617 0.585 0.819 0.822</cell></row><row><cell>Feddg</cell><cell></cell><cell cols="3">0.745 0.725 0.720 0.697 0.872 0.871</cell></row><row><cell>GIN-IPA</cell><cell></cell><cell cols="3">0.683 0.665 0.641 0.650 0.827 0.824</cell></row><row><cell>FreeSDG(ours)</cell><cell></cell><cell cols="3">0.795 0.778 0.736 0.716 0.913 0.912</cell></row><row><cell cols="5">FreeSDG w/o FMAug, SSL, ATT 0.720 0.705 0.687 0.665 0.875 0.873</cell></row><row><cell cols="2">FreeSDG w/o SSL, ATT</cell><cell cols="3">0.751 0.734 0.724 0.701 0.881 0.881</cell></row><row><cell cols="2">FreeSDG w/o ATT</cell><cell cols="3">0.777 0.760 0.731 0.709 0.898 0.897</cell></row><row><cell cols="5">* Independent and identically distributed data (IID) and multi-source domains</cell></row><row><cell>(MSD).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.isi.uu.nl/Research/Databases/DRIVE/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.kaggle.com/c/diabetic-retinopathy-detection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://figshare.com/articles/dataset/LES-AV dataset/11857698/1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.retinacheck.org/datasets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2020A1515110286</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">82102189</rs>, <rs type="grantNumber">82272086</rs>), <rs type="funder">Guangdong Provincial Department of Education</rs> (<rs type="grantNumber">2020ZDZX3043</rs>), <rs type="institution">Guangdong Provincial Key Laboratory</rs> (<rs type="grantNumber">2020B121201001</rs>), <rs type="funder">Shenzhen Natural Science Fund</rs> (<rs type="grantNumber">JCYJ20200109140820699</rs>, <rs type="grantNumber">20200925174052004</rs>), <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">SGDX202111 23114204007</rs>), <rs type="funder">Agency for Science, Technology and Research (A*STAR) Advanced Manufacturing and Engineering (AME) Programmatic Fund</rs> (<rs type="grantNumber">A20H4b0141</rs>) and <rs type="funder">Central Research Fund (CRF)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qkVapfb">
					<idno type="grant-number">2020A1515110286</idno>
				</org>
				<org type="funding" xml:id="_vmSRtvv">
					<idno type="grant-number">82102189</idno>
				</org>
				<org type="funding" xml:id="_Ske5Bnj">
					<idno type="grant-number">82272086</idno>
				</org>
				<org type="funding" xml:id="_3J89UbN">
					<idno type="grant-number">2020ZDZX3043</idno>
				</org>
				<org type="funding" xml:id="_rCbKE8F">
					<idno type="grant-number">2020B121201001</idno>
				</org>
				<org type="funding" xml:id="_kVAjyHS">
					<idno type="grant-number">JCYJ20200109140820699</idno>
				</org>
				<org type="funding" xml:id="_7JaNzSW">
					<idno type="grant-number">20200925174052004</idno>
				</org>
				<org type="funding" xml:id="_F2cTqVQ">
					<idno type="grant-number">SGDX202111 23114204007</idno>
				</org>
				<org type="funding" xml:id="_9cqeG9P">
					<idno type="grant-number">A20H4b0141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ce-net: context encoder network for 2d medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2281" to="2292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-learner based deep meta-learning for few-shot medical image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-consistent restoration network for cataract fundus image enhancement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-747" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MIC-CAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An annotation-free restoration network for cataractous fundus images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1699" to="1710" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CS-net: channel and spatial attention network for curvilinear structure segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_80</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-780" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causality-inspired single-source domain generalization for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1095" to="1106" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic-aware domain generalized segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2594" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out-of-domain generalization from a single source: an uncertainty quantification approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FGAM: a pluggable light-weight attention module for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">105628</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer vision (ECCV)</title>
		<meeting>the European Conference on Computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fourier-based framework for domain generalization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14383" to="14392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain generalization: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4396" to="4415" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
