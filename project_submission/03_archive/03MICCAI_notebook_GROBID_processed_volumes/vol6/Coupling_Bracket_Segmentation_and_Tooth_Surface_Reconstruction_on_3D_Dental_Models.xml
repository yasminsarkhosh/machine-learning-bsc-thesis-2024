<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models</title>
				<funder ref="#_Xwa3GAA">
					<orgName type="full">MoE Key Lab of Image Processing and Intelligent Control</orgName>
				</funder>
				<funder ref="#_RWqeUWR">
					<orgName type="full">Natural Science Fund of Hubei Province</orgName>
				</funder>
				<funder ref="#_rdHXWEc">
					<orgName type="full">Sichuan Univ. Interdisciplinary Innovation Res. Fund</orgName>
				</funder>
				<funder ref="#_yW9z86r">
					<orgName type="full">HUST Independent Inno. Res. Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuwen</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">Key Lab of Image Processing and Intelligent Control</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">Key Lab of Image Processing and Intelligent Control</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">Key Lab of Image Processing and Intelligent Control</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyi</forename><surname>Jing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Artificial Intelligence and Automation</orgName>
								<orgName type="laboratory">Key Lab of Image Processing and Intelligent Control</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiyang</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">National Clinical Research Center for Oral Diseases</orgName>
								<orgName type="department" key="dep2">Department of Orthodontics</orgName>
								<orgName type="laboratory">State Key Lab of Oral Diseases</orgName>
								<orgName type="institution" key="instit1">West China Hospital of Stomatology</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoran</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">National Clinical Research Center for Oral Diseases</orgName>
								<orgName type="department" key="dep2">Department of Orthodontics</orgName>
								<orgName type="laboratory">State Key Lab of Oral Diseases</orgName>
								<orgName type="institution" key="instit1">West China Hospital of Stomatology</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">National Clinical Research Center for Oral Diseases</orgName>
								<orgName type="department" key="dep2">Department of Orthodontics</orgName>
								<orgName type="laboratory">State Key Lab of Oral Diseases</orgName>
								<orgName type="institution" key="instit1">West China Hospital of Stomatology</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FBAF7483D7FE5475CE838DBAF52508A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D dental surface</term>
					<term>bracket segmentation</term>
					<term>surface reconstruction</term>
					<term>deep learning</term>
					<term>orthodontic treatment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Delineating and removing brackets on 3D dental models and then reconstructing the tooth surface can enable orthodontists to premake retainers for patients. It eliminates the waiting time and avoids the change of tooth position. However, it is time-consuming and laborintensive to process 3D dental models manually. To automate the entire process, accurate bracket segmentation and tooth surface reconstruction algorithms are of high need. In this paper, we propose a graph-based network named BSegNet for bracket segmentation on 3D dental models. The dynamic dilated neighborhood construction and residual connection in the graph network promote the bracket segmentation performance. Then, we propose a simple yet effective projection-based method to reconstruct the tooth surface. We project the vertices of the hole boundary on the tooth surface onto a 2D plane and then triangulate the projected polygon. We evaluate the performance of BSegNet on the bracket segmentation dataset and the results show the superiority of our method. The framework integrating the segmentation and reconstruction achieves a low reconstruction error and can be used as an effective tool to assist orthodontists in orthodontic treatment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of 3D scanning devices, the application of computeraided diagnosis in orthodontics has gradually developed. In orthodontic treatment, an essential step for patients is to wear retainers. Orthodontists need to remove the brackets, utilize the intra-oral scanners to acquire the digital models, and then print models to fabricate retainers. However, there is a long waiting time for patients and the position of teeth would change, which affects the effectiveness of orthodontic treatment. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, removing the brackets on 3D dental models and reconstructing the tooth surface can enable orthodontists to pre-make retainers. Patients can get retainers immediately after removing brackets which reduces the waiting time. When the retainer is lost or broken, a new one can be easily fabricated by using the archived digital model. The whole process helps to maintain the long-term stability of orthodontic treatment. However, it takes approximately 20 min for an orthodontist to precisely delineate brackets on a digital model. Besides that, the bracket segmentation requires a very high level of precision otherwise it will affect the reconstruction step. Therefore, an efficient and precise bracket segmentation method is crucial in orthodontic treatment. Orthodontists usually use CAD software (i.e., Geomagic Studio) to reconstruct the tooth surface after removing the brackets due to the clinical applicability of its reconstructed results. However, it requires interactive manual operation and automatic segmentation algorithms cannot be deployed to the software. To automate the entire process, a 3D mesh reconstruction algorithm is also needed. Integrating segmentation and reconstruction into a unified framework can serve as an effective tool to assist orthodontic treatment.</p><p>Since 3D dental models can be transformed into point clouds, methods proposed for point cloud segmentation would provide guidance. Point cloud segmentation methods can be mainly summarized as MLP-based <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13]</ref>, graphbased <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14]</ref>, convolution-based <ref type="bibr" target="#b16">[16]</ref>, and attention-based <ref type="bibr" target="#b3">[4]</ref>. Besides that, several methods have been proposed for computer-aided orthodontic processes such as tooth segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17]</ref>, landmark localization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b15">15]</ref>, and tooth completion <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19]</ref> on 3D dental models. A two-stream graph-based network TSGCNet <ref type="bibr" target="#b17">[17]</ref> is proposed for tooth segmentation. ToothCR <ref type="bibr" target="#b19">[19]</ref> is proposed to recover the missing tooth which consists of point completion and surface reconstruction. The holes formed by the removal of the brackets are simple and regular polygons located on each tooth surface. An effective and fast method should be proposed to fill these characteristic holes and reconstruct the tooth surface. Different from the network <ref type="bibr" target="#b8">[8]</ref> proposed for single-tooth bracket separation, our work focuses on the more challenging task of segmenting entire dental models. To the best of our knowledge, there exists no work proposed to integrate the segmentation and reconstruction of 3D dental models to automate the overall process.</p><p>As graph-based network <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref> shows its superiority on various tasks on 3D dental models, we analyze the performance of different local operations and modules in the graph network. Based on these analyses, we propose a network named BSegNet for bracket segmentation. After segmenting the brackets, we adopt a simple yet effective projection-based method to reconstruct the tooth surface, which converts 3D holes into a 2D plane and triangulates the projected polygons on the 2D plane. We then estimate the z coordinate of the new vertices through the neighbor information and transform the vertices to the original space. The proposed method can better recover the surface of teeth.</p><p>The contributions of this paper are as follows: 1) We propose a graph-based network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists; 2) An effective method is proposed to reconstruct the tooth surface where brackets are removed; 3) The low reconstruction error of the automatically processed models suggests that the framework integrating the segmentation and reconstruction can be used as a powerful tool to assist orthodontists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We propose a network named BSegNet for bracket segmentation on 3D dental models. BSegNet regards each mesh cell as a graph node and updates the nodewise feature via several local modules. For dental models with brackets removed, a simple yet effective reconstruction method is adopted to reconstruct the tooth surface. We will describe the network architecture of BSegNet (Fig. <ref type="figure" target="#fig_1">2</ref>) and the process of tooth surface reconstruction in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bracket Segmentation Network</head><p>Compared to point clouds, more geometric spatial information can be obtained from 3D mesh data. Different input features will have different effects on the subsequent network training. In this paper, we use a 24-dimension vector as the initial input. The 24-dimensional input vector corresponds to the coordinates of three vertices, the normal vectors of three vertices, the normal vectors of the mesh cell, and the coordinates of the mesh cell centroid.</p><p>We use a MLP to map the input feature</p><formula xml:id="formula_0">F 0 ∈ R 24 into F 1 ∈ R 64 .</formula><p>Then a transform net is adopted to transform the feature F 1 into a canonical space to improve the robustness. The transformed feature is fed to several local modules which are designed to encode the local features of each mesh cell in semantic space. In each local module, the first step is to construct the graph G(V, E) where V = {c 1 , c 2 , ..., c N } denotes the set of mesh cells and E represents the </p><formula xml:id="formula_1">N k i = {c 1 , c 1+d , c 1+2×d , ....c 1+(k-1)×d } (1)</formula><p>After building the graph, we construct the local feature of each cell and update the feature through the graph convolution layers. Let the f l i ∈ R 64 denote the feature vector of the mesh cell c i in the l-th layer and</p><formula xml:id="formula_2">f ij = {f i l -f j l }| j∈N k i</formula><p>denotes the edge feature which is used to capture the geometric relationship between each cell and its neighbors. Then the local feature f local = ( f l ij ⊕ f l i ) goes through two Conv2D layers to further encode the feature. The update process of the mesh cell features (graph node) is defined as</p><formula xml:id="formula_3">f l+1 i = R(h ϑ ( f l ij ⊕ f l i ))| j∈N k i + f l i (2)</formula><p>where h ϑ denotes the Conv2D layers and R(.) stands for the feature aggregation function. To avoid the over-smoothing problem in the graph network, we use the residual connection in each local module the same as DeepGCN <ref type="bibr" target="#b7">[7]</ref>. We concatenate the features of each local module and use an MLP layer to form a global high-dimension feature. Then the high-dimension global feature is concatenated with local features, forming the node-wise feature. Finally, several projection layers and one classifier layer are used to predict an N ×2 probability matrix.</p><p>As the prediction results of the network may have some isolated labeled mesh cells, we use the graph-cut method to refine the results. The post-processing stage minimizes an energy function by combining the probability term and the smoothness term. The energy function to be optimized is defined as</p><formula xml:id="formula_4">E = N i=1 -log(max(p i (l i ), )) + λ N i j∈Ni S(p i , p j , l i , l j )<label>(3)</label></formula><p>where p i (l i ) denotes the probability belongs to the l i , is the minimal probability threshold, and λ denotes the smooth parameter. The local smoothness term is defined as</p><formula xml:id="formula_5">S(p i , p j , l i , l j ) = ⎧ ⎨ ⎩ 0, l i = l j -log( θ ij π )d ij , l i = l j (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where θ ij denotes the dihedral angle of two adjacent facets and d ij denotes the distance between the centroids of two adjacent facets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tooth Surface Reconstruction</head><p>First, we identify all the holes and extract their boundaries. Then we project the vertices of the boundary into the 2D plane. We triangulate the projected polygon without inserting new vertices. If the line segment inside the polygon exceeds the preset length, it should be n-equally divided. To get more uniform triangles, we use the optimal delaunay triangulation algorithm <ref type="bibr" target="#b0">[1]</ref> to iteratively optimize the position of vertices. The optimization process of vertices is as follows</p><formula xml:id="formula_7">p * = 1 |T j | Tj ∈Ω(p) |T j |c j (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where p is the vertex needs to be optimized, Ω(p) denotes the set of first-ring neighborhood mesh cells of p, |T j | denotes the area of mesh cell T j , c j is the circumcentre of T j . After optimizing the positions of the vertices, the triangles have almost the same angles and the distribution is closer to the original models. After the polygon triangulation, the x-y coordinates are determined and a layer-by-layer procedure is employed to estimate the corresponding z-values. To avoid the vertices at the gingiva, we only use the information of the first-ring neighborhoods of the boundary vertices. We first compute the slopes of the boundary vertices which are defined as</p><formula xml:id="formula_9">k b i = 1 |N 1 (i)| j∈N 1 (i) z ij x 2 ij + y 2 ij (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where N 1 (i) denotes the set of first-ring neighborhood vertices of the boundary point b i . Then the calculation of the z-coordinate value is denoted as</p><formula xml:id="formula_11">z i new = 1 N b (i) j∈N b (i) (z b j + k b j x 2 ij + y 2 ij )<label>(7)</label></formula><p>where N b (i) denotes the set of adjacent boundary points and k b j denotes the slope of boundary vertex b j . Then we regard the added points as new boundary vertices and repeat the above process until the z-values of all vertices are calculated.</p><p>Before transforming back to the original space, the extreme z values are removed by median filtering. Then we use a rotation matrix to obtain the coordinates in the original space. Finally, we employ Laplacian smoothing to enhance the smoothness of the reconstructed surface. To make the reconstructed surface blend better with the boundary, we need to reduce the effect of smoothing on the first-ring neighborhood of the boundary. The calculation equation is as follows</p><formula xml:id="formula_12">p new = up b + (1 -u)p s , u = 1 1 + exp(-d h d b )<label>(8)</label></formula><p>where p b and p s denote the vertex before and after smoothing, d b is the average distance between the vertex and the adjacent boundary vertices, and d h is the average length of the hole boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>We collect 80 dental mesh models in STL format from different patients. The number of mesh cells in each dental model is approximately 100,000 and all the dental models are down-sampled to nearly 24,000 mesh cells. The ground truth segmentations are annotated by professional orthodontists on the downsampled dental models. We divide the dataset into a training, validation, and test set which consists of 45, 14, and 21 subjects, respectively. The performance of bracket segmentation is evaluated by mean Intersection-over-Union (mIoU) and Overall Accuracy (OA). The performance of reconstruction is evaluated by the Mean Distance (MD) and Standard Deviation (SD) of the distance between the models reconstructed by our method and by Geomagic Studio. We also evaluate the reconstruction error of models processed by the automatic framework and manually by orthodontists. We train all the networks by minimizing the crossentropy loss for 400 epochs except for MeshSegNet which minimizes the dice loss. We use the Adam optimizer and set the mini-batch as 5. The initial learning rate is 0.001, and we anneal the learning rate using the cosine functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Evaluation</head><p>Comparison Results. We compare our method with SOTA point cloud segmentation methods and teeth segmentation methods. All the results are shown in Table <ref type="table" target="#tab_0">1</ref> and BSegNet achieves the best performance. Since PointNet lacks the feature encoding of local regions, it performs worst among all the methods. The performance of PointNet++ and PointConv are close and outperform PointNet by a large margin. Although PointMLP has the best performance among all the compared methods, the mIoU of BSegNet is higher (94.60 vs. 93.15). We also compare our method with the attention-based method PCT and the mIoU of our method is higher than PCT (94.60 vs. 92.99). The tooth segmentation network on 3D dental models performs worse than several point segmentation methods in the bracket segmentation task, especially for TSGCNet which uses a two-stream network to encode the coordinates and normal vectors respectively. MeshSegNet performs better than TSGCNet but the proposed graph-constrained learning modules cause high computation complexity. Our method is based on the DGCNN but the mIoU of our method is much higher (94.60 vs. 92.42).</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, the reconstruction error of the models predicted by BSegNet is significantly lower than PointMLP. When reconstructing the models processed by doctors, our reconstruction method achieves low values of SD and MD which reveals it can replace the interactive reconstruction operation to some extent. We also compare our method with another reconstruction method Meshfix and our method has a lower value of SD (0.032 vs. 0.049). Compared to the manual process by doctors, the reconstruction error of our automatic framework is clinically acceptable and it can assist in orthodontic treatment. Figure <ref type="figure" target="#fig_2">3</ref> displays the segmentation and tooth surface reconstruction results.</p><p>Ablation Study and Analysis. In this section, we analyze different operations and modules in the BSegNet. As shown in Table <ref type="table" target="#tab_2">3</ref>, the performance of the 24-dimension input is better than the 15-dimension input which suggests the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists. BSegNet is a graph-based network that employs dynamic dilated neighborhood construction and residual connections to improve segmentation results. With label optimization, the segmentation results can be further refined. Experimental results on a clinical dataset demonstrate our method significantly outperforms related stateof-the-art methods. We also propose a simple yet effective method to reconstruct the tooth surface which can better recover the feature of the teeth. The whole framework achieves a low reconstruction error and can be used as a powerful tool to assist doctors in orthodontic diagnosis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of bracket segmentation and tooth surface reconstruction. Integrating those two steps into a unified framework automates the whole process.</figDesc><graphic coords="2,50,31,54,11,323,92,117,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of the bracket segmentation network (BSegNet).</figDesc><graphic coords="4,46,80,54,23,330,25,137,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) The bracket segmentation results after post-processing; (b) Dental models with brackets removed; (c) Models after tooth surface reconstruction; (d) Color-coded error map of automatically processed models compared with ground truth.</figDesc><graphic coords="8,52,29,53,81,319,93,129,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The segmentation results of the test dataset in terms of both OA and mIoU. (w/p) denotes the segmentation results after label optimization. Bold is the best.</figDesc><table><row><cell>Method</cell><cell>Input OA</cell><cell cols="3">mIoU Bracket Other OA(w/p) mIoU(w/p)</cell></row><row><cell>PointNet [12]</cell><cell cols="2">4p,4n 90.21 81.78 78.80</cell><cell>84.75 91.83</cell><cell>84.52</cell></row><row><cell cols="3">PointNet++ [13] 4p,4n 94.44 89.26 87.78</cell><cell>90.74 95.87</cell><cell>91.86</cell></row><row><cell>DGCNN [14]</cell><cell cols="2">4p,4n 96.14 92.42 91.42</cell><cell>93.41 97.47</cell><cell>94.96</cell></row><row><cell>PointConv [16]</cell><cell cols="2">4p,4n 95.30 90.83 89.65</cell><cell>92.00 96.81</cell><cell>93.66</cell></row><row><cell>PCT [4]</cell><cell cols="2">4p,4n 96.44 92.99 92.13</cell><cell>93.85 97.56</cell><cell>95.13</cell></row><row><cell>PointMLP [10]</cell><cell cols="2">4p,4n 96.51 93.15 92.29</cell><cell>94.00 97.60</cell><cell>95.23</cell></row><row><cell cols="3">MeshSegNet [9] 4p,1n 95.62 91.46 90.39</cell><cell>92.52 96.89</cell><cell>93.84</cell></row><row><cell>TSGCNet [17]</cell><cell cols="2">4p,4n 93.00 86.68 84.96</cell><cell>88.41 95.00</cell><cell>90.30</cell></row><row><cell>Ours</cell><cell cols="3">4p,4n 97.28 94.60 93.95 95.26 98.13</cell><cell>96.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The tooth surface reconstruction results compared to the ground truth in terms of MD(mm) and SD(mm). vectors of the vertices help the subsequent network training. However, curvature information which is helpful for the teeth segmentation task does not improve and even hurts the bracket segmentation performance. An essential step in the graph-based network is to construct the local features of the center node. Many methods have been proposed to construct complex local features. However, our results show that using the edge feature with the central feature is sufficient for effectively representing local regions. We also analyze the results of different aggregation operations, and the results indicate that Max pooling is the most effective method for aggregation. As shown in Table4, adopting the dynamic graph which updates the neighbor information in each local module, the mIoU has an improvement (+0.88) compared to the fixed graph. Under the same network structure, using the residual connection would further improve model performance by (+0.70). The use of dilated knn only brings slight performance improvement but does not require extra computation costs. With the post-processing, the predicted results can be further refined. Overall, using all the operations mentioned above acquires the best performance with 94.60 of mIoU and 96.25 after post-processing.</figDesc><table><row><cell cols="2">Method Seg.</cell><cell>Recon.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Semi.</cell><cell cols="2">PointMLP [10] Geo</cell><cell>0.060</cell><cell>0.014</cell><cell>0.005</cell><cell>0.0095</cell></row><row><cell>Semi.</cell><cell>BSegNet</cell><cell>Geo</cell><cell>0.026</cell><cell>0.004</cell><cell>0.005</cell><cell>0.0045</cell></row><row><cell>Semi.</cell><cell>Doctors</cell><cell>Ours</cell><cell>0.015</cell><cell>0.001</cell><cell>0.004</cell><cell>0.0025</cell></row><row><cell>Auto.</cell><cell>BSegNet</cell><cell cols="2">Meshfix [2] 0.049</cell><cell>0.001</cell><cell>0.018</cell><cell>0.0095</cell></row><row><cell>Auto.</cell><cell>BSegNet</cell><cell>Ours</cell><cell>0.032</cell><cell>0.002</cell><cell>0.009</cell><cell>0.0055</cell></row></table><note><p>SD(mm) MD+(mm) MD-(mm) Aver.(mm) normal</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Analysis experiments of different inputs, feature aggregation functions, and local feature constructions. Sum 96.72 93.52 { fij, fi, fj} 96.56 93.20 4p,1n,3c 95.50 91.26 Mean 96.72 93.51 { pij, fij, fi, fj} 96.75 93.57</figDesc><table><row><cell>Input</cell><cell>OA</cell><cell cols="2">mIoU R(.) OA</cell><cell>mIoU Feature</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>1p,1n</cell><cell cols="2">95.93 92.02 Att</cell><cell cols="2">95.40 91.03 { pij, fj}</cell><cell cols="2">95.02 90.31</cell></row><row><cell>4p,1n</cell><cell cols="4">96.74 93.55 Max 97.12 94.29 { fij, fi}</cell><cell cols="2">97.12 94.29</cell></row><row><cell>4p,4n</cell><cell cols="2">97.12 94.29</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of different operators in the BSegNet.</figDesc><table><row><cell>Dynamic Residual Con. Dilated-knn OA</cell><cell cols="2">mIoU OA(w/p) mIoU(w/p)</cell></row><row><cell cols="2">96.28 92.71 97.55</cell><cell>95.14</cell></row><row><cell cols="2">96.76 93.59 97.79</cell><cell>95.58</cell></row><row><cell cols="2">97.12 94.29 97.99</cell><cell>95.97</cell></row><row><cell cols="2">97.28 94.60 98.13</cell><cell>96.25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14225, pp. 411-420, 2023. https://doi.org/10.1007/978-3-031-43987-2_40</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was supported by <rs type="funder">Sichuan Univ. Interdisciplinary Innovation Res. Fund</rs> (<rs type="grantNumber">RD-03-202108</rs>), <rs type="funder">Natural Science Fund of Hubei Province</rs> (<rs type="grantNumber">2022CFB823</rs>), <rs type="funder">HUST Independent Inno. Res. Fund</rs> (<rs type="grantNumber">2021XXJS096</rs>), <rs type="programName">Alibaba Innovation Research (AIR) program</rs> (<rs type="grantNumber">CRAQ7WHZ11220001-20978282</rs>), and grants from <rs type="funder">MoE Key Lab of Image Processing and Intelligent Control</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rdHXWEc">
					<idno type="grant-number">RD-03-202108</idno>
				</org>
				<org type="funding" xml:id="_RWqeUWR">
					<idno type="grant-number">2022CFB823</idno>
				</org>
				<org type="funding" xml:id="_yW9z86r">
					<idno type="grant-number">2021XXJS096</idno>
					<orgName type="program" subtype="full">Alibaba Innovation Research (AIR) program</orgName>
				</org>
				<org type="funding" xml:id="_Xwa3GAA">
					<idno type="grant-number">CRAQ7WHZ11220001-20978282</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 40.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational tetrahedral meshing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yvinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Desbrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="617" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A lightweight approach to repairing digitized polygon meshes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Attene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1393" to="1406" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fully automatic AI system for tooth and alveolar bone segmentation from cone-beam CT images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2096</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PCT: point cloud transformer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DentalPointNet: landmark localization on high-resolution 3D digital dental models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="444" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-743" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DLLNet: an attention-based deep learning method for dental landmark localization on high-resolution 3D digital dental models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-146" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepGCNs: can GCNs go as deep as CNNs?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning for separation and feature extraction of bonded teeth: tool establishment and application</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep multi-scale mesh feature learning for automated labeling of raw dental surfaces from 3D intraoral scanners</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2440" to="2450" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07123</idno>
		<title level="m">Rethinking network design and local geometry in point cloud: a simple residual MLP framework</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-attention implicit function networks for 3D dental data completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Geom. Des</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">102026</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two-stage mesh deep learning for automated tooth segmentation and landmark localization on 3D intraoral scans</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3158" to="3166" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PointConv: deep convolutional networks on 3D point clouds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TSGCNet: discriminative geometric feature learning with twostream graph convolutional network for 3D dental model segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6699" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TeethGNN: semantic 3D teeth segmentation with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3158" to="3168" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ToothCR: a two-stage completion and reconstruction approach on 3D dental model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining. PAKDD 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Teng</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13282</biblScope>
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-05981-0_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-05981-013" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
