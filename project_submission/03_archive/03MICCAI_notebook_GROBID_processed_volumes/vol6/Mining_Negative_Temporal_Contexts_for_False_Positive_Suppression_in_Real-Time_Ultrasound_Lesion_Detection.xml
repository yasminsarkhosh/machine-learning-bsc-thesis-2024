<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection</title>
				<funder ref="#_B39BnAZ">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_kCCtxYy">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haojun</forename><surname>Yu</surname></persName>
							<email>haojunyu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youcheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanlin</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center of Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
								<address>
									<settlement>Huangpu, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center of Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dengbo</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Yizhun Medical AI Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wanglw@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="laboratory">National Key Laboratory of General Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Machine Learning Research</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="3" to="13"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BD860C4C7C8EB3C256F31313885D2D86</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound Video</term>
					<term>Real-time Lesion Detection</term>
					<term>Negative Temporal Context</term>
					<term>False Positive Suppression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During ultrasonic scanning processes, real-time lesion detection can assist radiologists in accurate cancer diagnosis. However, this essential task remains challenging and underexplored. General-purpose real-time object detection models can mistakenly report obvious false positives (FPs) when applied to ultrasound videos, potentially misleading junior radiologists. One key issue is their failure to utilize negative symptoms in previous frames, denoted as negative temporal contexts (NTC) <ref type="bibr" target="#b14">[15]</ref>. To address this issue, we propose to extract contexts from previous frames, including NTC, with the guidance of inverse optical flow. By aggregating extracted contexts, we endow the model with the ability to suppress FPs by leveraging NTC. We call the resulting model UltraDet. The proposed UltraDet demonstrates significant improvement over previous state-of-the-arts and achieves real-time inference speed. We release the code, checkpoints, and high-quality labels of the CVA-BUS dataset <ref type="bibr" target="#b8">[9]</ref> in https://github.com/HaojunYu1998/UltraDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ultrasound is a widely-used imaging modality for clinical cancer screening. Deep Learning has recently emerged as a promising approach for ultrasound lesion detection. While previous works focused on lesion detection in still images <ref type="bibr" target="#b24">[25]</ref> and offline videos <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, this paper explores real-time ultrasound video lesion detection. Real-time lesion prompts can assist radiologists during scanning, thus being more helpful to improve the accuracy of diagnosis. This task requires the model to infer faster than 30 frames per second (FPS) <ref type="bibr" target="#b18">[19]</ref> and only previous frames are available for current frame processing.</p><p>Previous general-purpose detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> report simple and obvious FPs when applied to ultrasound videos, e.g. the red box in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. These FPs, attributable to non-lesion anatomies, can mislead junior readers. These anatomies appear like lesions in certain frames, but typically show negative symptoms in adjacent frames when scanned from different positions. So experienced radiologists will refer to corresponding regions in previous frames, denoted as temporal contexts (TC), to help restrain FPs. If TC of a lesion-like region exhibit negative symptoms, denoted as negative temporal contexts (NTC), radiologists are less likely to report it as a lesion <ref type="bibr" target="#b14">[15]</ref>. Although important, the utilization of NTC remains unexplored. In natural videos, as transitions from non-objects to objects are implausible, previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref> only consider inter-object relationships. As shown in Sect. 4.4, the inability to utilize NTC is a key issue leading to the FPs reported by general-purpose detectors.</p><p>To address this issue, we propose a novel UltraDet model to leverage NTC. For each Region of Interest (RoI) R proposed by a basic detector, we extract temporal contexts from previous frames. To compensate for inter-frame motion, we generate deformed grids by applying inverse optical flow to the original regular RoI grids, illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Then we extract the RoI features from the deformed grids in previous frames and aggregate them into R. We call the overall process Negative Temporal Context Aggregation (NTCA). The NTCA module leverages RoI-level NTC which are crucial for radiologists but ignored in previous works, thereby effectively improving the detection performance in a reliable and interpretable way. We plug the NTCA module into a basic real-time detector to form UltraDet. Experiments on CVA-BUS dataset <ref type="bibr" target="#b8">[9]</ref> demonstrate that Ultra-Det, with real-time inference speed, significantly outperforms previous works, reducing about 50% FPs at a recall rate of 0.90.</p><p>Our contributions are four-fold. <ref type="bibr" target="#b0">(1)</ref> We identify that the failure of generalpurpose detectors on ultrasound videos derives from their incapability of utilizing negative temporal contexts. <ref type="bibr" target="#b1">(2)</ref> We propose a novel UltraDet model, incorpo-rating an NTCA module that effectively leverages NTC for FP suppression. <ref type="bibr" target="#b2">(3)</ref> We conduct extensive experiments to demonstrate the proposed UltraDet significantly outperforms the previous state-of-the-arts. (4) We release high-quality labels of the CVA-BUS dataset <ref type="bibr" target="#b8">[9]</ref> to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Real-Time Video Object Detection is typically achieved by single-frame detectors, often with temporal information aggregation modules. One-stage detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> use only intra-frame information, DETR-based detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> and Faster R-CNN-based detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref> are also widely utilized in video object detection. They aggregate temporal information by mining inter-object relationships without considering NTC.</p><p>Ultrasound Lesion Detection <ref type="bibr" target="#b9">[10]</ref> can assist radiologists in clinical practice. Previous works have explored lesion detection in still images <ref type="bibr" target="#b24">[25]</ref> and offline videos <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. Real-time video lesion detection is underexplored. In previous works, YOLO series <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref> and knowledge distillation <ref type="bibr" target="#b18">[19]</ref> are used to speed up inference. However, these works use single-frame detectors or post-process methods while learnable inter-frame aggregation modules are not adopted. Thus their performances are far from satisfactory.</p><p>Optical Flow <ref type="bibr" target="#b2">[3]</ref> is used to guide ultrasound segmentation <ref type="bibr" target="#b11">[12]</ref>, motion estimation <ref type="bibr" target="#b3">[4]</ref> and elastography <ref type="bibr" target="#b12">[13]</ref>. For the first time, we use inverse optical flow to guide temporal context information extraction. In real-time video lesion detection, given the current frame I t and a sequence of T previous frames as {I τ } t-1 τ =t-T , the goal is to detect lesions in I t by exploiting the temporal information in previous frames as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Real-Time Detector</head><p>The basic real-time detector comprises three main components: a lightweight backbone (e.g. ResNet34 <ref type="bibr" target="#b5">[6]</ref>), a Region Proposal Network (RPN) <ref type="bibr" target="#b13">[14]</ref>, and a Temporal Relation head <ref type="bibr" target="#b1">[2]</ref>. The backbone is responsible for extracting feature map F τ of frame I τ . The RPN generates proposals consisting of boxes B τ and proposal features Q τ using RoI Align and average pooling:</p><formula xml:id="formula_0">Q τ = AvgPool (RoIAlign(F τ , B τ ))<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">τ = t -T, • • • , t -1, t.</formula><p>To aggregate temporal information, proposals from all T + 1 frames are fed into the Temporal Relation head and updated with inter-lesion information extracted via a relation operation <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_2">Q l = Q l-1 + Relation(Q l-1 , B)<label>(2)</label></formula><p>where l = 1, • • • , L represent layer indices, B and Q are the concatenation of all B τ and Q τ , and Q 0 = Q. We call this basic real-time detector BasicDet. The BasicDet is conceptually similar to RDN <ref type="bibr" target="#b1">[2]</ref> but does not incorporate relation distillation since the number of lesions and proposals in this study is much smaller than in natural videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Negative Temporal Context Aggregation</head><p>In this section, we present the Negative Temporal Context Aggregation (NTCA) module. We sample T ctxt context frames from T previous frames, then extract temporal contexts (TC) from context frames and aggregate them into proposals. We illustrate the NTCA module in Fig. <ref type="figure" target="#fig_2">3</ref> and elaborate on details as follows. Inverse Optical Flow Align. We propose the Inverse Optical Flow Align (IOF Align) to extract TC features. For the current frame I t and a sampled context frame I τ with τ &lt; t, we extract TC features from the context feature map F τ with the corresponding regions. We use inverse optical flow</p><formula xml:id="formula_3">O t→τ ∈ R H×W ×2</formula><p>to transform the RoIs from frame t to τ : O t→τ = FlowNet(I t , I τ ) where H, W represent height and width of feature maps. The FlowNet(I t , I τ ) is a fixed network <ref type="bibr" target="#b2">[3]</ref> to predict optical flow from I t to I τ . We refer to O t→τ as inverse optical flow because it represents the optical flow in inverse chronological order from t to τ . We conduct IOF Align and average pooling to extract C t,τ :</p><formula xml:id="formula_4">C t,τ = AvgPool (IOFAlign(F τ , B t , O t→τ ))<label>(3)</label></formula><p>where IOFAlign(F τ , B t , O t→τ ) extracts context features in F τ from deformed grids generated by applying offsets O t→τ to the original regular grids in B t , which is illustrated in the Fig. <ref type="figure" target="#fig_0">1(b)</ref>.</p><p>Temporal Aggregation. We concatenate C t,τ in all T ctxt context frames to form C t and enhance proposal features by fusing C t into Q t :</p><formula xml:id="formula_5">Q l ctxt,t = Q l-1 ctxt,t + Attention(Q l-1 ctxt,t , C t , C t )<label>(4)</label></formula><p>where <ref type="bibr" target="#b17">[18]</ref>. We refer to the concatenation of all TC-enhanced proposal features in T + 1 frames as Q ctxt . To extract consistent TC, the context frames of T previous frames are shared with the current frame.</p><formula xml:id="formula_6">l = 1, • • • , L represent layer indices, Q 0 ctxt,t = Q t , and Attention(Q, K, V ) is Multi-head Attention</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">UltraDet for Real-Time Lesion Detection</head><p>We integrate the NTCA module into the BasicDet introduced in Sect. 3.1 to form the UltraDet model, which is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The head of UltraDet consists of stacked NTCA and relation modules:</p><formula xml:id="formula_7">Q l = Q l ctxt + Relation(Q l ctxt , B). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>During training, we apply regression and classification losses L = L reg + L cls to the current frame. To improve training efficiency, we apply auxiliary losses L aux = L to all previous T frames. During inference, the UltraDet model uses the current frame and T previous frames as inputs and generates predictions only for the current frame. This design endows the UltraDet with the ability to perform real-time lesion detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dateset</head><p>CVA-BUS Dateset. We use the open source CVA-BUS dataset that consists of 186 valid videos, which is proposed in CVA-Net <ref type="bibr" target="#b8">[9]</ref>. We split the dataset into train-val (154 videos) and test (32 videos) sets. In the train-val split, there are 21423 frames with 170 lesions. In the test split, there are 3849 frames with 32 lesions. We focus on the lesion detection task and do not utilize the benign/malignant classification labels provided in the original dataset.</p><p>High-Quality Labels. The bounding box labels provided in the original CVA-BUS dataset are unsteady and sometimes inaccurate, leading to jiggling and inaccurate model predictions. We provide a new version of high-quality labels that are re-annotated by experienced radiologists. We reproduce all baselines using our high-quality labels to ensure a fair comparison. Visual comparisons of two versions of labels are available in supplementary materials. To facilitate future research, we will release these high-quality labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Pr80, Pr90. In clinical applications, it is important for detection models to be sensitive. So we provide frame-level precision values with high recall rates of 0.80 and 0.90, which we denote as Pr80 and Pr90, respectively. FP80, FP90. We further report lesion-level FP rates as critical metrics. Framelevel FPs are linked by IoU scores to form FP sequences <ref type="bibr" target="#b23">[24]</ref>. The number of FP sequences per minute at recall rates of 0.80 and 0.90 are reported as FP80 and FP90, respectively. The unit of lesion-level FP rates is seq/min. AP50. We provide AP50 instead of mAP or AP75 because the IoU threshold of 0.50 is sufficient for lesion localization in clinical practice. Higher thresholds like 0.75 or 0.90 are impractical due to the presence of blurred lesion edges.</p><p>R@16. To evaluate the highest achievable sensitivity, we report the frame-level average recall rates of Top-16 proposals, denoted as R@16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>UltraDet Settings. We use FlowNetS <ref type="bibr" target="#b2">[3]</ref> as the fixed FlowNet in IOF Align and share the same finding with previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] that the FlowNet trained on natural datasets generalizes well on ultrasound datasets. We set the pooling stride in the FlowNet to 4, the number of UltraDet head layers L = 2, the number of previous frames T = 15 and T ctxt = 2, and the number of proposals is 16. We cached intermediate results of previous frames and reuse them to speed up inference. Other hyper-parameters are listed in supplementary materials.</p><p>Shared Settings. All models are built in PyTorch framework and trained using eight NVIDIA GeForce RTX 3090 GPUs. We use ResNet34 <ref type="bibr" target="#b5">[6]</ref> as backbones and set the number of training iterations to 10,000. We set the feature dimensions of detection heads to 256 and baselines are re-implemented to utilize only previous frames. We refer to our code for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Quantitative Results. We compare performances of real-time detectors with the UltraDet in Table <ref type="table" target="#tab_0">1</ref>. We perform 4-fold cross-validation and report the mean values and standard errors on the test set to mitigate fluctuations. The UltraDet outperforms all previous state-of-the-art in terms of precision and FP rates.</p><p>Especially, the Pr90 of UltraDet achieves 90.8%, representing a 5.4% absolute improvement over the best competitor, PTSEFormer <ref type="bibr" target="#b19">[20]</ref>. Moreover, the FP90 of UltraDet is 5.7 seq/min, reducing about 50% FPs of the best competitor, PTSEFormer. Although CVA-Net <ref type="bibr" target="#b8">[9]</ref> achieve comparable AP50 with our method, we significantly improve precision and FP rates over the CVA-Net <ref type="bibr" target="#b8">[9]</ref>. Importance of NTC. In Fig. <ref type="figure" target="#fig_3">4</ref>(a), we illustrate the FP ratios that can be suppressed by using NTC. The determination of whether FPs can be inhibited by NTC is based on manual judgments of experienced radiologists. We find that about 50%-70% FPs of previous methods are suppressible. However, by utilizing NTC in our UltraDet, we are able to effectively prevent this type of FPs.</p><p>Inference Speed. We run inference using one NVIDIA GeForce RTX 3090 GPU and report the inference speed in Table <ref type="table" target="#tab_0">1</ref>. The UltraDet achieves an inference speed of 30.4 FPS and already meets the 30 FPS requirement. Using Ten-sorRT, we further optimize the speed to 35.2 FPS, which is sufficient for clinical applications <ref type="bibr" target="#b18">[19]</ref>.</p><p>Qualitative Results.  Effectiveness of Each Sub-module. We ablate the effectiveness of each submodule of the NTCA module in Table <ref type="table" target="#tab_1">2</ref>. Specifically, we replace the IOF Align with an RoI Align and the Temporal Aggregation with a simple average pooling in the temporal dimension. The results demonstrate that both IOF Align and Temporal Aggregation are crucial, as removing either of them leads to a noticeable drop in performance. Design of the NTCA Module. Besides RoI-level TC aggregation in UltraDet, feature-level aggregation is also feasible. We plug the optical flow feature warping proposed in FGFA <ref type="bibr" target="#b27">[28]</ref> into the BasicDet and report the results in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We find RoI-level aggregation is more effective than feature-level, and bothlevel aggregation provides no performance gains. This conclusion agrees with radiologists' skills to focus more on local regions instead of global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we address the clinical challenge of real-time ultrasound lesion detection. We propose a novel Negative Temporal Context Aggregation (NTCA) module, imitating radiologists' diagnosis processes to suppress FPs. The NTCA module leverages negative temporal contexts that are essential for FP suppression but ignored in previous works, thereby being more effective in suppressing FPs. We plug the NTCA module into a BasicDet to form the UltraDet model, which significantly improves the precision and FP rates over previous state-ofthe-arts while achieving real-time inference speed. The UltraDet has the potential to become a real-time lesion detection application and assist radiologists in more accurate cancer diagnosis in clinical practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of Negative Temporal Context Aggregation (NTCA) module. (a) Our motivation: mining negative temporal contexts for FP suppression. (b) The NTCA module leverages temporal contexts to suppress the FP. (Color figure online)</figDesc><graphic coords="2,65,25,54,92,170,05,88,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of UltraDet model. The yellow and green frames are sampled as context frames, and their feature maps are inputs of the NTCA module. (Color figure online)</figDesc><graphic coords="3,57,96,405,41,329,23,85,54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the Negative Temporal Context Aggregation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Ratios of FPs that are suppressible by leveraging NTC. (b) Visual comparisons of BasicDet and UltraDet prediction results at recall 0.90. Blue boxes are true positives and red boxes are FPs. (Color figure online)</figDesc><graphic coords="7,244,59,389,00,148,84,97,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 (</head><label>4</label><figDesc>b) visually compares BasicDet and UltraDet. The BasicDet reports FPs at t = 30 and 40 as it fails to leverage NTC when t = 20, while the UltraDet successfully suppresses FPs with the NTCA module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of real-time lesion detection on CVA-BUS<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell>Model</cell><cell>Type Pr80 Pr90 FP80 FP90 AP50 R@16 FPS</cell></row><row><cell></cell><cell>One-Stage Detectors</cell></row><row><cell>YOLOX [5]</cell><cell>Image 69.73.7 43.47.7 23.84.8 87.624.5 80.41.6 97.50.5 59.8</cell></row><row><cell>RetinaNet [8]</cell><cell>Image 75.72.5 57.22.9 9.32.0 32.86.5 84.51.0 95.10.6 53.6</cell></row><row><cell>FCOS [16]</cell><cell>Image 87.22.2 72.25.1 11.02.4 23.03.7 89.51.4 98.80.3 56.1</cell></row><row><cell>DeFCN [21]</cell><cell>Image 81.51.8 67.52.3 21.13.2 33.44.3 86.41.3 99.30.3 51.2</cell></row><row><cell cols="2">Track-YOLO [24] Video 75.12.7 47.03.1 18.11.9 74.214.7 80.11.0 94.70.9 46.0</cell></row><row><cell></cell><cell>DETR-Based Detectors</cell></row><row><cell cols="2">DeformDETR [27] Image 90.13.2 72.710.6 5.62.2 37.820.9 90.52.0 98.70.3 33.8</cell></row><row><cell>TransVOD [26]</cell><cell>V i d e o 92.52.2 77.57.2 3.11.3 23.711.5 90.11.8 98.40.4 24.2</cell></row><row><cell>CVA-Net [9]</cell><cell>V i d e o92.32.6 80.26.1 4.72.6 19.65.6 91.61.9 98.60.8 23.1</cell></row><row><cell cols="2">PTSEFormer [20] Video 93.31.9 85.46.0 2.81.1 12.59.8 91.51.6 97.91.2 9.1</cell></row><row><cell></cell><cell>FasterRCNN-Based Detectors</cell></row><row><cell cols="2">FasterRCNN [14] Image 91.30.9 75.23.6 6.91.4 34.46.7 88.01.4 92.41.0 49.2</cell></row><row><cell>RelationNet [7]</cell><cell>Image 91.41.3 79.22.9 6.22.0 24.45.6 87.61.7 92.40.9 42.7</cell></row><row><cell>FGFA [28]</cell><cell>V i d e o92.91.5 82.24.1 4.41.6 13.33.7 90.51.1 93.60.9 33.8</cell></row><row><cell>SELSA [23]</cell><cell>V i d e o91.61.7 80.22.5 7.51.5 23.35.5 89.21.1 92.60.8 43.8</cell></row><row><cell>MEGA [1]</cell><cell>V i d e o93.91.5 86.92.3 3.11.7 11.73.0 90.91.0 93.60.7 40.2</cell></row><row><cell cols="2">BasicDet (RDN) [2] Video 92.41.0 83.62.2 3.81.2 13.43.2 88.71.4 92.70.6 42.2</cell></row><row><cell>UltraDet (Ours)</cell><cell>Video 95.71.2 90.81.4 1.90.4 5.71.6 91.61.6 93.81.3 30.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of each NTCA sub-module.</figDesc><table><row><cell cols="3">IOFAlign TempAgg Pr80</cell><cell>Pr90 FP80 FP90 AP50 R@16 FPS</cell></row><row><cell>-</cell><cell>-</cell><cell cols="2">92.41.0 83.62.2 3.81.2 13.43.2 88.71.4 92.70.6 42.2</cell></row><row><cell>-</cell><cell></cell><cell cols="2">93.71.8 84.31.4 3.41.0 12.50.8 90.01.9 93.01.3 37.2</cell></row><row><cell></cell><cell>-</cell><cell cols="2">94.52.3 88.72.2 2.60.6 9.01.5 90.51.9 92.91.4 32.3</cell></row><row><cell></cell><cell></cell><cell cols="2">95.71.2 90.81.4 1.90.4 5.71.6 91.61.6 93.81.3 30.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Design of the NTCA Module. .00.9 84.62.9 2.90.8 11.73.0 90.80.7 93.30.6 30.6 RoI-level 95.71.2 90.81.4 1.90.4 5.71.6 91.61.6 93.81.3 30.4 Both-level 94.61.0 88.71.8 2.50.9 7.92.4 90.81.5 93.80.9 26.9</figDesc><table><row><cell>Num</cell><cell>Pr80</cell><cell>Pr90 FP80 FP90 AP50 R@16 FPS</cell></row><row><cell cols="2">Feature-level 94</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2022ZD0114900</rs>) and <rs type="funder">National Science Foundation of China</rs> (<rs type="grantNumber">NSFC62276005</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_B39BnAZ">
					<idno type="grant-number">2022ZD0114900</idno>
				</org>
				<org type="funding" xml:id="_kCCtxYy">
					<idno type="grant-number">NSFC62276005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relation distillation networks for video object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7023" to="7032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FlowNet: learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A pilot study on convolutional neural networks for motion estimation from ultrasound images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Evain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Craene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ultrason. Ferroelectr. Freq. Control</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2565" to="2573" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">YOLOX: exceeding yolo series in 2021</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part III</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning in medical ultrasound analysis: a review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated analysis of ultrasound videos for detection of breast lesions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parsaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli Golpaygani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Haghighi Poya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Middle East J. Cancer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end real-time catheter segmentation with optical flowguided warping during endovascular intervention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9967" to="9973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolution neural network-based speckle tracking method for ultrasound elastography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Ultrasonics Symposium (IUS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="206" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BI-RADS® fifth edition: a summary of changes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaxco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagn. Interv. Imaging</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The feasibility to use artificial intelligence to aid detecting focal liver lesions in real-time ultrasound: a preliminary study based on videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tiyarattanachai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7749</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-memory CNNs enabling real-time ultrasound segmentation towards mobile deployment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Namburete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1059" to="1069" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PTSEFormer: progressive temporal-spatial enhanced transformer towards video object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20074-8_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20074-8_42" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part VIII</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13668</biblScope>
			<biblScope unit="page" from="732" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end object detection with fully convolutional network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15849" to="15858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-frame guided network for thyroid nodule recognition using ultrasound videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_23" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part IV</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CacheTrack-YOLO: real-time detection and tracking for thyroid nodules and surrounding tissues in ultrasound videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3812" to="3823" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TransVOD: end-to-end video object detection with spatialtemporal transformers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7853" to="7869" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
