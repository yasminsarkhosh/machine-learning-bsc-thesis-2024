<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer</title>
				<funder ref="#_3dmK3kp">
					<orgName type="full">Natural Science Foundation of Zhejiang Province, China</orgName>
				</funder>
				<funder>
					<orgName type="full">Zhejiang University-Angelalign Inc. R&amp;D Center for Intelligent Healthcare</orgName>
				</funder>
				<funder ref="#_TyR5PqF">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huimin</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<postCode>314400</postCode>
									<settlement>Haining</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Stomatology</orgName>
								<orgName type="institution">Stomatology Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunle</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<postCode>314400</postCode>
									<settlement>Haining</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiyuan</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<postCode>314400</postCode>
									<settlement>Haining</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Angelalign Research Institute</orgName>
								<orgName type="institution" key="instit2">Angel Align Inc</orgName>
								<address>
									<postCode>200011</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Centre for Frontier AI Research (CFAR)</orgName>
								<orgName type="institution">A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Institute of High Performance Computing (IHPC)</orgName>
								<orgName type="institution">A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Hao</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">ChohoTech Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haochao</forename><surname>Ying</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Public Health</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Public Health</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
							<email>zuozhuliu@intl.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<postCode>314400</postCode>
									<settlement>Haining</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Stomatology</orgName>
								<orgName type="institution">Stomatology Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University School of Medicine</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="421" to="432"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">44EE3A2FF6A6F80CD73F4F1574E13F55</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D tooth segmentation</term>
					<term>IOS mesh scans</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical Intraoral Scanners (IOS) are widely used in digital dentistry to provide detailed 3D information of dental crowns and the gingiva. Accurate 3D tooth segmentation in IOSs is critical for various dental applications, while previous methods are error-prone at complicated boundaries and exhibit unsatisfactory results across patients. In this paper, we propose TSegFormer which captures both local and global dependencies among different teeth and the gingiva in the IOS point clouds with a multi-task 3D transformer architecture. Moreover, we design a geometry-guided loss based on a novel point curvature to refine boundaries in an end-to-end manner, avoiding time-consuming post-processing to reach clinically applicable segmentation. In addition, we create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of our knowledge. The experimental results demonstrate that our TSegFormer consistently surpasses existing state-of-the-art baselines. The superiority of TSegFormer is corroborated by extensive analysis, visualizations and real-world clinical applicability tests. Our code is available at https://github.com/huiminxiong/TSegFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning is becoming increasingly popular in modern orthodontic treatments for tooth segmentation in intraoral scans (IOS), cone-beam CT (CBCT) and panoramic X-ray <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>. Accurate tooth segmentation in 3D IOS dental models is crucial for orthodontics treatment such as diagnosis, tooth crown-root analysis and treatment simulation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, tooth segmentation classifies each triangular face of a 3D IOS tooth model with about 100,000 to 400,000 faces and a spatial resolution of 0.008-0.02mm into teeth and gingiva categories, following the Federation Dentaire Internationale (FDI) standard <ref type="bibr" target="#b7">[8]</ref>.</p><p>There are two main categories for tooth segmentation in IOS: conventional methods that handle 2D image projections <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> or directly operate on 3D IOS meshes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, and deep learning methods that operate on meshes or point clouds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. However, many challenges persist. Complicated morphological topology or dental diseases (e.g. crowded or erupted teeth) can lead to unsatisfactory segmentation performance <ref type="bibr" target="#b5">[6]</ref>. Additionally, current methods often fail to recognize mesh faces between adjacent teeth or the tooth and gingiva, requiring time-consuming post-processing to refine the noisy boundary segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. Moreover, the state-of-the-art works such as MeshSegNet <ref type="bibr" target="#b11">[12]</ref>, TSGCNet <ref type="bibr" target="#b26">[27]</ref> and DCNet <ref type="bibr" target="#b5">[6]</ref> have only been evaluated with a limited amount of data samples and the clinical applicability need to be evaluated with large-scale dataset or in real-world scenarios.</p><p>Inspired by the success of transformers in various tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>, we propose a novel 3D transformer framework, named TSegFormer, to address the aforementioned challenges. In particular, the tooth segmentation task on 3D IOSs is formulated as a semantic segmentation task on point clouds sampled from raw IOS meshes. We design the 3D transformer with tailored self-attention layers to capture long-range dependencies among different teeth, learning expressive representations from inherently sophisticated structures across IOSs. In addition, we design a multi-task learning paradigm where another auxiliary segmentation head is introduced to assist in delimiting teeth and gingiva. Furthermore, in view of the confusing boundary segmentation, we devise a novel geometry guided loss based on a newly-defined point curvature to help learn accurate boundaries. The network is trained in an end-to-end manner and requires no complicated post-processing during inference, making it appealing to practical applications.</p><p>We collect a large-scale, high-resolution and heterogeneous 3D IOS dataset with 16,000 dental models where each contains over 100,000 triangular faces. To the best of our knowledge, it is the largest IOS dataset to date. Experimental results show that TSegFormer has reached 97.97% accuracy, 94.34% mean intersection over union (mIoU) and 96.01% dice similarity coefficient (DSC) on the large-scale dataset, outperforming previous works by a significant margin. To summarize, our main contributions are:</p><p>-We design a novel framework for 3D tooth segmentation with a tailed 3D transformer and a multi-task learning paradigm, aiming at distinguishing the permanent teeth with divergent anatomical structures and noisy boundaries.</p><p>-We design a geometry guided loss based on a novel point curvature for endto-end boundary refinement, getting rid of the two-stage and time-consuming post-processing for boundary smoothing. -We collect the largest ever 3D IOS dataset for compelling evaluation. Extensive experiments, ablation analysis and clinical applicability test demonstrate the superiority of our method, which is appealing in real-world applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The overall pipeline is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The original mesh M is converted to a point cloud P by taking the gravity center point of each mesh face. We downsample a point cloud P with N = 10, 000 points from P , and extract the input feature matrix h in ∈ R N ×8 as defined below. The network first employs a point embedding module to capture abundant local structure information h pe from h in . Thereafter, we design the 3D transformer encoder with self-attention layers to capture high-level semantic representations h a . With h a , the main segmentation head produces prediction scores ŷseg ∈ R N ×33 (32 permanant teeth and the gingiva), while the auxiliary head generates prediction scores ŷaux ∈ R N ×2 to assist distinguishing the tooth-gingiva boundary. Furthermore, we devise a geometry guided loss L geo , which is integrated with the main segmentation loss L seg and the auxiliary loss L aux to attain superior performance. During inference, we will extract the features h in ∈ R N ×8 for all points in P , process P into multiple sub-point clouds each with N points, then generate predictions for each point with N N rounds of inference, and map them back to raw mesh M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TSegFormer Network Architecture</head><p>Feature Extraction. We first transform input meshes to point clouds as directly handling meshes with deep nets is computationally expensive, especially for high-resolution IOSs. To compensate for potential topology loss, we extract 8-dimensional feature vectors h in ∈ R N ×8 /R N ×8 for each point to preserve sufficient geometric information, including the point's 3D Cartesian coordinates, 3-dimensional normal vector of mesh face, the Gaussian curvature and a novel point "curvature" m i . The m i is defined as</p><formula xml:id="formula_0">m i = 1 |K(i)| j∈K(i) θ(n i , n j )</formula><p>, where n i is the i-th point's normal vector, K(i) is the second-order neighborhood of the i-th point, |K(i)| is the number of points in K(i), and θ(•, •) denotes the angle in radians between two vectors. By definition, the curvature of a point reflects how much the local geometric structure around this point is curved, i.e., the local geometry on 3D tooth point clouds. Backbone Network. Delineating complicated tooth-tooth or tooth-gingiva boundaries requires decent knowledge of local geometry in IOS. Hence, we first learn local dependencies from the input h in . In particular, we design a point embedding module composed of two linear layers and two EdgeConv layers <ref type="bibr" target="#b19">[20]</ref>, which takes h in as input and learn local features h pe ∈ R N ×de . The point embedding module enriches point representations with local topological information, with ablation results in the Supplementary Material (SM ) Table <ref type="table" target="#tab_0">1</ref>.</p><p>Meantime, in view of the inherently sophisticated and inconsistent shapes and structures of the teeth, and the ability of attention mechanism to capture long-range dependencies and suitability for handling unordered point cloud data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>, we build an encoder module based on it. The encoder module, composed of four successive self-attention layers and a linear transformation, further yields the high-level point feature maps h p . To avoid misjudging jaw categories, an extra 2D category vector V is fed as input to help distinguish the maxillary and mandible and obtain the global feature maps h g . Specifically, h g = σ(V ) ⊕ MP (h p ) ⊕ AP (h p ), where "MP" and "AP" respectively denote the max and average pooling; ⊕ denotes concatenation and σ(•) is a linear layer. Finally, we obtain feature maps h a for all points, where h a = h p ⊕ h g . Segmentation Heads. To improve the network's ability to recognize different tooth and gingiva categories, we design two segmentation heads. The main segmentation head, an MLP (MLP seg ), generates point classification scores for 33 classes ŷseg = MLP seg (h a ) ∈ R N ×33 for tooth segmentation. Meanwhile, considering the prevalence of incorrect prediction of tooth-gingiva boundaries, we design an auxiliary segmentation head MLP aux to provide binary classification scores for each point belonging to either tooth or gingiva, i.e., ŷaux = MLP aux (h a ) ∈ R N ×2 . Experimental results indicate that the cooperation with MLP aux can refine tooth-gingiva segmentation boundary.</p><p>Geometry Guided Loss. Previous methods are usually unsatisfactory to delineate the complicated tooth-tooth boundaries. Observing that points with high point curvatures often lie on the upper sharp ends of tooth crowns and the teeth boundaries (Fig. <ref type="figure" target="#fig_1">2(c</ref>)), where mispredictions usually occur, we define the novel geometry guided loss L geo . L geo encourages TSegFormer to adaptively focus more on error-prone points with higher point curvatures with negligible extra computations. Specifically, we define it as</p><formula xml:id="formula_1">L geo = - i∈S(r) 33 c=1 (1 -pgeo ic ) γ • Φ(y S(r)i , c) • log(p geo ic ),<label>(1)</label></formula><p>where γ is the modulating factor (empirically set to 2 in experiments); y S(r)i ∈ R 33 represents the gold label of the i-th point in the point set S(r); pgeo ic denotes the predicted probability of the i-th point belonging to the c-th class, and Φ(y i , c) is an indicator function which outputs 1 if y i = c and 0 otherwise. Concretely, S(r) is a set of points whose point curvatures m i are among the top r • 100% (0 &lt; r ≤ 1) of all N points, i.e., S(r</p><formula xml:id="formula_2">) := {a 1 , a 2 , • • • , a rN }, where m a1 ≥ m a2 ≥ • • • &gt; m arN ≥ m arN+1 ≥ • • • ≥ m aN .</formula><p>The experimental results (Fig. <ref type="figure" target="#fig_1">2(d)</ref>) on a dataset of 2,000 cases indicate that L geo is more effective with our point curvature over traditional mean and Gaussian curvatures, even they are worst than no curvature. This is because our point curvature provides more clear tooth-tooth and tooth-gingiva boundary indications (Fig. <ref type="figure" target="#fig_1">2</ref>(a)-2(c)), thus avoiding misleading the model to focus too much on unimportant non-boundary points.</p><p>We employ the cross entropy loss as the loss of main segmentation head (L seg ) and the loss of auxiliary segmentation head (L aux ). The total loss L total is computed by combining L seg , L aux for all points and L geo for hard points: L total = L seg +ω geo •L geo +ω aux •L aux . We set the weights ω geo = 0.001, ω aux = 1 and the ratio r = 0.4, and detailed hyperparameter search results in SM Fig. <ref type="figure" target="#fig_0">1</ref> indicate that the performance is stable across different hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setup</head><p>We construct a large-scale 3D IOS dataset consisting of 16,000 IOS meshes with full arches (each with 100,000 to 350,000 triangular faces) collected between 2018-2021 in China, with evenly distributed maxillary and mandible scans labeled by human experts. Detailed data statistics are presented in SM Table <ref type="table" target="#tab_1">2</ref>, and 39.8% of the data have third-molars, 16.8% suffer from missing teeth, which all reveal the complexity of the dataset. The dataset is randomly split into training (12,000 IOSs), validation (2,000 IOSs) and test sets (2,000 IOSs). Furthermore, we collect an external dataset with 200 complex cases (disease statistics shown in SM Table <ref type="table" target="#tab_2">3</ref>) to evaluate the real-world clinical applicability of TSegFormer. Detailed training and architecture settings are in SM Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Results on Tooth Segmentation</head><p>To our best knowledge, there has been no prior work on Transformer-based segmentation on non-Euclidean 3D tooth point clouds/meshes. Hence, we compare our TSegFormer to seven representative and state-of-the-art baselines from three categories: 1) neural networks for point clouds, including PointNet++ <ref type="bibr" target="#b15">[16]</ref> and DGCNN <ref type="bibr" target="#b19">[20]</ref>; 2) transformers for point clouds, including point transformer <ref type="bibr" target="#b27">[28]</ref> and PVT <ref type="bibr" target="#b25">[26]</ref>; 3) domain-specific architectures for 3D tooth segmentation, including MeshSegNet <ref type="bibr" target="#b11">[12]</ref>, TSGCNet <ref type="bibr" target="#b26">[27]</ref> and DC-Net <ref type="bibr" target="#b5">[6]</ref>. For fair comparison, baselines that cannot achieve raw-resolution mesh prediction followed the same inference protocols in 2.1, while the rest kept their original inference schemes. We can firstly observe that TSegFormer outperforms existing best-performing point transformer model <ref type="bibr" target="#b27">[28]</ref> by 0.16% in accuracy, 1.04% in mIoU and 0.71% in DSC (Table <ref type="table" target="#tab_0">1</ref>). Such an improvement is surely significant considering the complicated real-world cases in our large-scale dataset and the relatively high performance of point transformer with an mIoU of 93.30%. Moreover, TSeg-Former consistently surpassed all baselines on both mandible and maxillary in terms of all metrics, demonstrating its universal effectiveness.</p><p>It is important to integrate advanced architectures with domain-specific design for superior performance. We can notice that though MeshSegNet, TSGC-Net, and DCNet are all domain-specific 3D tooth segmentation models, their performance, though on par with PVT and DGCNN, is worse than the point transformer. This is also consistent with the superior performance of transformerbased models on standard point cloud processing tasks, which could be mainly attributed to the larger dataset and powerful attention mechanism that better capture global dependencies. Hence, though models like MeshSegNet adopt some task-specific designs to achieve good performance, they still lag behind point transformer when a huge amount of data samples are available. In contrast, our TSegFormer employs the attention mechanism for point representation learning, and meanwhile, adopted task-specific architectures and geometry guided loss to further boost the performance. More statistical results are in SM Table <ref type="table" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>Effectiveness of Geometry Guided Loss. Table <ref type="table" target="#tab_1">2</ref> shows that introducing the geometry guided loss can improve the performance under all three metrics, e.g., around 0.4% improvement in mIoU. Besides, we show the universal effectiveness of the geometry guided loss by adding it to DCNet <ref type="bibr" target="#b5">[6]</ref>. The performance of DCNet is also enhanced by 1.43% in mIoU (Table <ref type="table" target="#tab_2">3</ref>) with this additional loss.</p><p>Effectiveness of the Auxiliary Segmentation Head. The auxiliary segmentation head is designed to rectify the inaccuracy brought by mislabeling teeth and gingiva near their boundaries. Adding a loss for the auxiliary branch leads to about 0.4% mIoU performance improvement (Table <ref type="table" target="#tab_1">2</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Clinically Applicability Test and Visualization</head><p>To show the effectiveness of TSegFormer in real-world scenarios, we conducted a clinical applicability test (Table <ref type="table" target="#tab_5">5</ref>) on a dataset with 200 complex IOS scans, whose diseases statistics are in SM Table <ref type="table" target="#tab_2">3</ref>. The segmentation with five different models were evaluated by a committee of dentists with more than 5-year experience. We can notice that TSegFormer significantly outperforms the other models regarding the clinical error rate. The feedback from dentists indicates that models such as TSGCNet cannot meet the requirement when dealing with complicated boundaries, while TSeg-Former apparently handles them better. The point transformer and DCNet also showed promising performance, but they are yet far behind our TSegFormer. As for the number of parameters and inference time, though TSegFormer has the second most parameters among all methods we tested, it is the second fastest method that only takes around 23 s to complete inference for 200 cases, which is certainly acceptable in real-world clinical scenarios.</p><p>By visualization, we show the superiority of TSegFormer on various complicated dental diseases in Fig. <ref type="figure" target="#fig_2">3</ref>. The baselines unavoidably produce false predictions or even fail to identify an entire third-molar, while TSegFormer can yield more accurate segmentation and smoother boundaries (see SM Fig. <ref type="figure" target="#fig_1">2</ref> for details), corroborating great potential for clinical applications. Specifically, SM Fig. <ref type="figure" target="#fig_2">3</ref> shows that with our geometry guided loss and auxiliary head, the isolated mispredictions and boundary errors are greatly reduced. However, TSegFormer fails in some complex samples, e.g. the missing tooth, the erupted wisdom tooth and sunken gingiva and alveolar bone regions, as illustrated in SM Fig. <ref type="figure">4</ref>, which needs to be further studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose TSegFormer, a 3D transformer-based model for high-resolution IOS segmentation. It combines a point embedding module and attention mechanism to effectively capture local and global features, and introduces a geometry guided loss based on a novel point curvature to handle boundary errors and multi-task segmentation heads for boundary refinement. Results of comprehensive experiments on a large-scale dataset and clinical applicability tests demonstrate TSeg-Former's state-of-the-art performance and its great potential in digital dentistry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of our proposed TSegFormer for 3D tooth segmentation</figDesc><graphic coords="3,60,03,158,75,332,56,131,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization and performance comparison of different curvatures(cur).</figDesc><graphic coords="4,44,31,243,71,335,44,73,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of segmentation of different methods across different diseases.</figDesc><graphic coords="9,77,49,59,87,316,12,142,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main segmentation results (Tested on 1,000 patients). DSC ↑ Acc ↑ mIoU ↑ DSC ↑ Acc ↑ mIoU ↑ DSC ↑ Acc ↑</figDesc><table><row><cell>Method</cell><cell cols="2">Mandible</cell><cell cols="2">Maxillary</cell><cell>All</cell></row><row><cell cols="2">mIoU ↑ PointNet++ 81.11</cell><cell cols="2">85.33 94.96 83.89</cell><cell cols="2">87.12 96.28 82.57</cell><cell>86.27 95.65</cell></row><row><cell>DGCNN</cell><cell>92.41</cell><cell cols="2">94.49 97.68 93.82</cell><cell cols="2">95.61 98.01 93.15</cell><cell>95.08 97.85</cell></row><row><cell cols="2">point transformer 92.61</cell><cell cols="2">94.83 97.55 93.93</cell><cell cols="2">95.72 98.06 93.3</cell><cell>95.3</cell><cell>97.81</cell></row><row><cell>PVT</cell><cell>90.66</cell><cell cols="2">93.59 96.64 92.46</cell><cell cols="2">94.72 97.44 91.6</cell><cell>94.19 97.06</cell></row><row><cell>MeshSegNet</cell><cell>82.21</cell><cell cols="2">86.55 91.98 85.37</cell><cell cols="2">89.28 93.72 83.87</cell><cell>87.98 92.90</cell></row><row><cell>TSGCNet</cell><cell>80.71</cell><cell cols="2">85.23 92.78 80.97</cell><cell cols="2">85.28 93.86 80.85</cell><cell>85.25 93.34</cell></row><row><cell>DCNet</cell><cell>91.18</cell><cell cols="2">93.89 97.11 92.78</cell><cell cols="2">95.18 97.44 92.02</cell><cell>94.57 97.28</cell></row><row><cell cols="6">TSegFormer (Our) 93.53 95.36 97.72 95.07 96.60 98.20 94.34 96.01 97.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on different main components.</figDesc><table><row><cell>Component</cell><cell>Mandible</cell><cell>Maxillary</cell><cell>All</cell></row><row><cell cols="2">Geometry guided loss Auxiliary branch mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell></row><row><cell></cell><cell cols="3">92.19 94.27 97.35 94.02 95.69 97.96 93.15 95.01 97.67</cell></row><row><cell></cell><cell cols="3">92.53 94.57 97.43 94.45 96.10 98.02 93.54 95.37 97.74</cell></row><row><cell></cell><cell cols="3">92.46 94.47 97.37 94.45 96.12 98.02 93.51 95.33 97.71</cell></row><row><cell></cell><cell cols="3">92.95 94.88 97.57 94.46 96.07 98.14 93.77 95.51 97.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Segmentation performance of DCNet<ref type="bibr" target="#b5">[6]</ref> with our geometry guided loss.</figDesc><table><row><cell>Model</cell><cell>Mandible</cell><cell>Maxillary</cell><cell>All</cell></row><row><cell></cell><cell>mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell></row><row><cell>DCNet</cell><cell cols="3">87.72 91.00 95.99 90.77 93.50 96.87 89.32 92.31 96.46</cell></row><row><cell cols="4">DCNet+Lgeo 89.66 92.58 96.46 91.75 94.28 97.14 90.75 93.47 96.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Segmentation performance of TSegFormer under different training set scales.</figDesc><table><row><cell cols="2">Training set scale Mandible</cell><cell>Maxillary</cell><cell>All</cell></row><row><cell></cell><cell>mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell><cell>mIoU DSC Acc</cell></row><row><cell>500</cell><cell cols="3">86.36 89.74 95.45 89.22 91.69 96.57 87.86 90.76 96.04</cell></row><row><cell>1,000</cell><cell cols="3">90.60 93.13 96.72 92.84 94.77 97.50 91.78 93.99 97.13</cell></row><row><cell>2,000</cell><cell cols="3">92.15 94.28 97.31 94.05 95.79 97.92 93.15 95.08 97.63</cell></row><row><cell>4,000</cell><cell cols="3">93.10 95.05 97.69 94.68 96.28 98.12 93.93 95.70 97.92</cell></row><row><cell>8,000</cell><cell cols="3">93.27 95.15 97.67 94.83 96.40 98.16 94.09 95.81 97.92</cell></row><row><cell>12,000</cell><cell cols="3">93.53 95.36 97.72 95.07 96.60 98.20 94.34 96.01 97.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Training Data Efficiency. In real-world orthodontic applications, large-scale training data may not be directly accessible due to privacy concerns. Therefore, to show our model's data efficiency, we train our model on datasets with different sizes (Table4). With only 500 training samples, TSeg-Former is able to surpass PointNet++, MeshSegNet and TSGCNet trained on 12,000 samples. Furthermore, TSegFormer trained with only 2,000 samples can almost outperform all previous models trained on 12,000 samples. Overall, these results demonstrate the exceptional data efficiency of our TSegFormer.Effectiveness of Local Point Embedding. SM Table1shows purely MLPbased structures perform worst due to the lack of local contexts, while EdgeConv layers can make up for this, and the collaboration of both performs best.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Clinical applicability test on the external IOS dataset (200 cases). "#success"/"#fail": number of segmentation that meets/does not meet the clinical criteria.</figDesc><table><row><cell>MeshSegNet</cell><cell>65</cell><cell>135</cell><cell>67.5</cell><cell>1.81M</cell><cell>128.56</cell></row><row><cell>TSGCNet</cell><cell>15</cell><cell>185</cell><cell>92.5</cell><cell>4.13M</cell><cell>31.40</cell></row><row><cell cols="2">Point Transformer 97</cell><cell>103</cell><cell>51.5</cell><cell>6.56M</cell><cell>437.21</cell></row><row><cell>DCNet</cell><cell>109</cell><cell>91</cell><cell>45.5</cell><cell>1.70M</cell><cell>5.79</cell></row><row><cell cols="2">TSegFormer (Our) 152</cell><cell>48</cell><cell>24.0</cell><cell>4.21M</cell><cell>23.15</cell></row></table><note><p>#param: number of parameters in the network. Inf-T: inference time for 200 cases. Model #success ↑ #fail ↓ clinical error rate (%) ↓ #param ↓ Inf-T(s) ↓</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62106222</rs>), the <rs type="funder">Natural Science Foundation of Zhejiang Province, China</rs> (Grant No. <rs type="grantNumber">LZ23F020008</rs>) and the <rs type="funder">Zhejiang University-Angelalign Inc. R&amp;D Center for Intelligent Healthcare</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TyR5PqF">
					<idno type="grant-number">62106222</idno>
				</org>
				<org type="funding" xml:id="_3dmK3kp">
					<idno type="grant-number">LZ23F020008</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_41.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TsegNet: an efficient and accurate tooth segmentation network on 3D dental model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101949</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning approach to semantic segmentation in 3D point cloud intra-oral scans of teeth</title>
		<author>
			<persName><forename type="first">Ghazvinian</forename><surname>Zanjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd International Conference on Medical Imaging with Deep Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting>The 2nd International Conference on Medical Imaging with Deep Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="557" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PCT: point cloud transformer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15908" to="15919" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward clinically applicable 3-Dimensional tooth segmentation via deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<idno type="DOI">10.1177/00220345211040459</idno>
		<ptr target="https://doi.org/10.1177/00220345211040459" />
	</analytic>
	<monogr>
		<title level="j">J. Dent. Res</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34719980</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training improves tooth segmentation in 3-Dimensional intraoral mesh scans</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="493" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the completion of federation dentaire internationale specifications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zahnarztliche Mitteilungen</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="1147" to="1149" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep instance segmentation of teeth in panoramic X-ray images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fontineli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pithon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 31st SIB-GRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tooth segmentation of dental study models using range images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Foong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="362" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MeshsNet: deep multi-scale mesh feature learning for end-to-end tooth labeling on 3D dental surfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="837" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep multi-scale mesh feature learning for automated labeling of raw dental surfaces from 3d intraoral scanners</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2440" to="2450" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical self-supervised learning for 3D tooth segmentation in intra-oral mesh scans</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3222388</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3222388" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="480" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conebeam CT of the head and neck, part 2: clinical applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherji</surname></persName>
		</author>
		<idno type="DOI">10.3174/ajnr.A1654</idno>
		<ptr target="http://www.ajnr.org/content/30/7/1285" />
	</analytic>
	<monogr>
		<title level="j">Am. J. Neuroradiol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1285" to="1292" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthodontics treatment simulation by teeth segmentation and setup</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sinthanayothin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tharanont</surname></persName>
		</author>
		<idno type="DOI">10.1109/ECTICON.2008.4600377</idno>
		<ptr target="https://doi.org/10.1109/ECTICON.2008.4600377" />
	</analytic>
	<monogr>
		<title level="m">2008 5th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tooth segmentation and labeling from digital dental casts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI45749.2020.9098397</idno>
		<ptr target="https://doi.org/10.1109/ISBI45749.2020.9098397" />
	</analytic>
	<monogr>
		<title level="m">IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="669" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>CoRR abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computerized algorithm for 3D teeth segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wongwaen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sinthanayothin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Electronics and Information Engineering</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tooth segmentation on dental meshes using morphologic skeleton</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3D tooth segmentation and labeling using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2336" to="2348" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient free-form surface representation with application in orthodontics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yamany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>El-Bialy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Three-Dimensional Image Capture and Applications II</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Nurre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Corner</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3640</biblScope>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-tooth modeling for 3D dental model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Biomed. Imaging</title>
		<imprint>
			<biblScope unit="page">535329</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">PVT: point-voxel transformer for point cloud learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06076</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TSGCNet: discriminative geometric feature learning with twostream graph convolutional network for 3D dental model segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6699" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive tooth segmentation of dental models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Engineering in Medicine and Biology 27th Annual Conference</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="654" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TeethGNN: semantic 3D teeth segmentation with graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2022.3153501</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2022.3153501" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3158" to="3168" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
