<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning</title>
				<funder ref="#_BZx7mTh">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Medicine &amp; Forensics</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="institution" key="instit1">Key Laboratory of National Ministry of Health for Forensic Sciences</orgName>
								<orgName type="institution" key="instit2">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinggong</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Medicine &amp; Forensics</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="institution" key="instit1">Key Laboratory of National Ministry of Health for Forensic Sciences</orgName>
								<orgName type="institution" key="instit2">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeyi</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Medicine &amp; Forensics</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="institution" key="instit1">Key Laboratory of National Ministry of Health for Forensic Sciences</orgName>
								<orgName type="institution" key="instit2">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kehan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Biomedical Information Engineering of Ministry of Education</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>fan.wang@xjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Biomedical Information Engineering of Ministry of Education</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Medicine &amp; Forensics</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="institution" key="instit1">Key Laboratory of National Ministry of Health for Forensic Sciences</orgName>
								<orgName type="institution" key="instit2">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunfeng</forename><surname>Lian</surname></persName>
							<email>chunfeng.lian@xjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710149</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shaanxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="528" to="538"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C2B8A198964782FF50000B0800F59C07</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Forensic Pathology</term>
					<term>Self-Supervised Learning</term>
					<term>Multiple Instance Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Forensic pathology is critical in analyzing death manner and time from the microscopic aspect to assist in the establishment of reliable factual bases for criminal investigation. In practice, even the manual differentiation between different postmortem organ tissues is challenging and relies on expertise, considering that changes like putrefaction and autolysis could significantly change typical histopathological appearance. Developing AI-based computational pathology techniques to assist forensic pathologists is practically meaningful, which requires reliable discriminative representation learning to capture tissues' finegrained postmortem patterns. To this end, we propose a framework called FPath, in which a dedicated self-supervised contrastive learning strategy and a context-aware multiple-instance learning (MIL) block are designed to learn discriminative representations from postmortem histopathological images acquired at varying magnification scales. Our self-supervised learning step leverages multiple complementary contrastive losses and regularization terms to train a double-tier backbone for fine-grained and informative patch/instance embedding. Thereafter, the context-aware MIL adaptively distills from the local instances a holistic bag/image-level representation for the recognition task. On a large-scale database of 19, 607 experimental rat postmortem images and 3, 378 real-world human decedent images, our FPath led to state-of-theart accuracy and promising cross-domain generalization in recognizing seven different postmortem tissues. The source code will be released on https://github.com/ladderlab-xjtu/forensic_pathology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computational pathology powered by artificial intelligence (AI) shows promising applications in various clinical studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, significantly easing the workload and promoting the development of clinical pathology. Inspired by such exciting progress, let's think step by step, so why not leverage advanced AI techniques to boost the research and applications in another important discipline, i.e., forensic pathology? Forensic pathology focuses on investigating the cause, manner, and time of (non-natural) deaths based on histopathological examinations of postmortem organ tissues <ref type="bibr" target="#b4">[5]</ref>. As an indispensable part of the medicolegal autopsy, it provides critical evidence from the microscopic aspect to confirm, perfect, or refute macroscopic findings, establishing a reliable factual basis for future inferences <ref type="bibr" target="#b3">[4]</ref>. Histopathological analysis in forensic pathology is challenging and time-consuming, since postmortem changes (e.g., putrefaction and autolysis) severely destroy tissues' typical image appearance, even making the manual differentiation between the tissues of different organs very difficult.</p><p>Although diverse deep-learning approaches have been proposed in clinical studies to process and analyze histopathological images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, no similar work has yet in the forensic pathology community. The main reason could be threefold. 1) Forensic and clinical pathology have distinct purposes. The former case analyzes the tissue images from multiple organs concurrently. In contrast, clinical diagnosis/prognosis usually focuses on one tissue type in one task <ref type="bibr" target="#b5">[6]</ref>. 2) Due to postmortem changes, histopathological images in forensic pathology have atypical appearances and more complex distributions than in clinical pathology, bringing additional challenges to deep representation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. 3) Data in forensic pathology are more difficult to obtain and have relatively lower quality. Therefore, to deploy a reliable computational pathology system for forensic investigation, fine-grained discriminative representation learning from complex postmortem histopathological images is a very precondition.</p><p>In this paper, we introduce a deep computational pathology framework (dubbed as FPath) for forensic histopathological analysis. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, FPath leverages the idea of self-supervised contrastive learning and multiple instance learning (MIL) to learn discriminative histopathological representations. Specifically, we propose a self-supervised contrastive learning strategy to learn a double-tier backbone network for fine-grained feature embedding of local image patches (i.e., instances in MIL). After that, a context-aware MIL block is designed, which adopts a self-attention mechanism to refine instancelevel representations by aggregating contextual information, and then applies an adaptive-pooling operation to produce a holistic image-level representation for prediction. Our FPath performs efficient predictions without the need for tedious pre-processing (e.g., foreground extraction/segmentation). To the best of our knowledge, this paper is the first attempt that shows promising appli-cations of advanced AI techniques (e.g., self-supervised contrastive learning) to forensic pathology.</p><p>The main technical contributions of our work are:</p><p>1) We design a double-tier backbone and a dedicated self-supervised learning strategy to capture discriminative instance-level histopathological patterns of postmortem organ tissues. The double-tier backbone combines CNN and transformer for local and non-local information fusion. To effectively train such a backbone to handle images acquired with varying microscopic magnifications, the dedicated self-supervised learning strategy leverages multiple complementary contrastive losses and regularization terms to concurrently maximize global and spatially fine-grained similarities between different views of the same instances/patches in an informative representation space. 2) We design a context-aware MIL branch to produce the bag-level discriminative representations for accurate and efficient postmortem histopathological recognition. Our MIL branch first refines instance embedding by leveraging a self-attention mechanism integrating positional embedding to model crosspatch associations for contextual information enhancement. Thereafter, an adaptive pooling operation is designed to learn deformable spatial attention to distill from contextually enhanced patch-level representations a holistic image-level representation for recognition. 3) Our FPath was applied to recognize postmortem organ tissues, a fundamental task in forensic pathology. To this end, we established a relatively largescale multi-domain database consisting of an experimental rat postmortem dataset and a real-world human decedent dataset, each with 19, 607 and 3, 378 images acquired at a specific microscopic magnification (e.g., 5×, 10×, 20×, and 40×), respectively. On such a multi-domain database, our FPath led to promising cross-domain generalization and state-of-the-art accuracy in recognizing seven different postmortem organs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The schematic diagram of our FPath is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of two steps: 1) Self-supervised contrastive learning of a double-tier backbone, and 2) Context-aware multiple instance learning for postmortem tissue recognition.</p><p>2.1 Self-supervised Contrastive Patch Embedding Double-Tier Backbone. Given patches from a postmortem histopathological image acquired at a specific magnification (i.e., 5×, 10×, 20×, or 40×), we adopt a backbone with a local branch (LB) and a global branch (GB) for instance/patch feature embedding. The LB is a ResNet50 <ref type="bibr" target="#b9">[10]</ref> consisting of 16 successive bottlenecks, each with three convolutional layers with the kernel size of 1 × 1, 3 × 3, and 1 × 1, respectively. The GB is a Swin Transformer <ref type="bibr" target="#b16">[17]</ref> that contains of a series of 12 window-based multi-head self-attention modules. Let an input patch be X ∈ R H×W ×3 . The corresponding feature embedding produced by the double-tier backbone will be M = M LB ⊕ M GB (∈ R h×w×C ), where M LB and M GB denotes the representations from the LB and GB branch, respectively, and ⊕ stands for the channel-wise concatenation operation.</p><p>Self-supervised Contrastive Learning Strategy. We leverage the idea of self-supervised representation learning to establish the double-tier backbone.</p><p>Referring to MoCo <ref type="bibr" target="#b8">[9]</ref>, our self-supervised learning is constructed by a teacher branch and a student branch. The student branch consists of six components, including a double-tier backbone (i.e., f θ (•)), three projection layers (i.e., g sg (•), g so (•), g sp (•)), and two prediction layers (i.e., p sg (•) and p so (•)). The teacher branch contains four components, including a double-tier backbone f η (•), and three projection layers (i.e., g tg (•), g to (•), and g tp (•)). By feeding the two branches with different views of same patches, f θ (•) in the student branch (i.e., parameterized by θ) is trained via back-propagation to update f η (•) in the teacher branch (i.e., parameterized by η) in a momentum-based moving average fashion, such as</p><formula xml:id="formula_0">η ← m • η + (1 -m) • θ,</formula><p>where m = 0.99 is the momentum parameter.</p><p>Another key issue that determines the quality of the embedding from such a self-supervised strategy is the formulation of respective contrastive loss functions and regularization terms. Accordingly, we design a thorough contrastive learning strategy to capture fine-grained discriminative patterns of postmortem tissues under varying microscopic magnifications. That is, let X s and X t be two different views of an image patch X generated by a random data augmentation process. Our contrastive learning strategy concurrently encourages the global similarity and spatially fine-grained similarity between the corresponding feature embedding M s = f θ (X s ) and M t = f η (X t ) (∈ R h×w×C ). Also, two regularization terms are applied as auxiliary guidance to protect the informativeness and avoid collapses of the embedding learned by the backbone. Specifically, the global similarity between M s and M t is encouraged by minimizing a general cosine contrastive loss, such as</p><formula xml:id="formula_1">L global = 2 -2 • &lt; z g s , z g t &gt; ||z g s || 2 • ||z g t || 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where z g s = p sg (g sg (GAP(M s ))) and z g t = g tg (GAP(M t )), with GAP(•) standing for the global average pooling that produces feature vectors.</p><p>In practice, forensic pathologists typically infer postmortem tissue type by evaluating the cellular compositions in multiple local regions. Accordingly, inspired by cross-view learning <ref type="bibr" target="#b10">[11]</ref>, we design a spatially fine-grained contrastive loss to explicitly encourage multi-parts similarity between M s and M t . Assume M s and M t are two (h • w) × C tensors flattened from M s and M t across the spatial dimension, respectively. They are further processed by g so (•) and g to (•) (followed by softmax normalization), respectively, to produce two (h • w) × K attention matrices, i.e., A s = g so (M s ) and A t = g to (M t ), where K denotes the predefined number of parts. Thereafter, we aggregate the backbone representations in terms of the attention matrices to deduce multi-parts representations, i.e., Z o s = p so (g sp (A T s ⊗ M s )) and</p><formula xml:id="formula_3">Z o t = g tp (A T t ⊗ M t )</formula><p>, where ⊗ denotes tensor multiplication. Finally, the spatially fine-grained contrastive loss is quantified as</p><formula xml:id="formula_4">L parts = K k=1 2 -2 • &lt; Z o s [k, :], Z o t [k, :] &gt; ||Z o s [k, :]|| 2 • ||Z o t [k, :]|| 2<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">Z o [k, :] denotes the kth part representation in Z o ∈ R K×D .</formula><p>Besides, two additional regularization terms are further included to stabilize contrastive representation learning. Following <ref type="bibr" target="#b0">[1]</ref>, we penalize small changes between the global representations of different image patches across each feature dimension. Also, we encourage the global representations to be diverse/orthogonal across different feature dimensions. Let Z g s be a set of feature representations for an input mini-batch of patches in the student branch, and Z g s and Z g s denote their channel-wise variation and mean. The regularization terms are defined as</p><formula xml:id="formula_6">L var = 1 D D d=1 max 0, 1 -Z g s [d] +<label>(3)</label></formula><formula xml:id="formula_7">L cov = 1 D 2 -D i =j (z g s -Z g s ) T (z g s -Z g s ) [i, j] 2<label>(4)</label></formula><p>where is a small scalar to stabilize numerical computation, Z g s [d] denotes the dth dimension of Z g s , and</p><formula xml:id="formula_8">{(z g s -Z g s ) T (z g s -Z g s )}[i, j] is the [i, j]</formula><p>th element in such a covariance matrix. According to <ref type="bibr" target="#b0">[1]</ref>, Eqs. ( <ref type="formula" target="#formula_6">3</ref>) and (4) jointly encourage the diversity across patches and feature dimensions, thus protecting the informativeness and avoid collapse of self-supervised contrastive learning.</p><p>Overall, we combine Eqs. ( <ref type="formula" target="#formula_1">1</ref>) to (4) as the final loss function to train the double-tier backbone, such as L all = L global + L parts + γL var + λL cov , where γ and λ are two tuning parameters balancing different terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context-Aware MIL</head><p>Given the patch/instance-level representations of a histopathological image from the double-tier backbone, we further design a context-aware MIL framework to aggregate their information for postmortem tissue recognition. Given patch embeddings of a Microscope image, our context-aware MIL part contains two main steps, i.e., a multi-head self-attention to refine each patch's feature and an adaptive pooling step to distill all patches' information.</p><p>In detail, we first adopt a multi-head self-attention (MSA) mechanism <ref type="bibr" target="#b18">[19]</ref> integrating relative positional embedding to explicitly model cross-patch associations for contextual enhancement of the instance representations from the backbone. Let Z = {z i } I i=1 be a set of the contextually enhanced instance embedding from an image. Thereafter, inspired by Deformable DETR <ref type="bibr" target="#b27">[28]</ref>, we further design an adaptive pooling operation, which is simple but effective to distill from Z a bag-level holistic representation for the classification purpose. Specifically, the bag-level holistic representation determined by the adaptive pooling is</p><formula xml:id="formula_9">z bag = 1 I I i=1 (sof tmax(h ω1 (z i )) • h ω2 (z i )),<label>(5)</label></formula><p>where h ω1 (•) and h ω2 (•) are two linear projections with the same number of output units, symbol • denotes the Hadamard product between two tensors, and sof tmax(•) is performed across different instances to filter out uninformative patches and preserve discriminative patches in quantifying z bag for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Experimental Setup</head><p>Rat Postmortem Histopathology Dataset. Ninety Sprague-Dawley adult male rats were executed by the spinal cord dislocation and placed in a constant temperature and humidity environment for 6-8 h. The animal experiments were approved by the Laboratory Animal Care Committee of the anonymous institution. Seven organs, i.e., brain, heart, kidney, liver, lung, pancreas, and spleen, were removed and placed in the formalin solution. Briefly, paraffin sections of these organ tissues were stained with the H&amp;E solution. The H&amp;E-stained sections were then analyzed by three forensic pathologists, who used Lercai LAS EZ microscopes to record the areas according to their expertise. Overall, five to ten images were recorded from a section at each magnification (i.e., 5×, 10×, 20×, and 40×). Finally, we split the 90 rats as training, validation, and test sets of 60, 10, and 20 rats, respectively, each with 13, 137, 2, 235, and 4, 325 images.</p><p>Human Forensic Histopathology Dataset. The real forensic images were provided by the Forensic Judicial Expertise Center of the anonymous institution, after getting the informed consent of relatives. All procedures followed the requirements of local laws and institutional guidelines, and were approved and supervised by the Ethics Committee. A total of 32 decedents participated in this study. Four to six images were recorded at each of three magnifications (5×, 10×, and 20×) per H&amp;E stained section. Similar to the rat dataset, the human dataset was selected from the same seven organs. Finally, the training, validation and test sets contain 1, 691 images, 628, and 1059 images, corresponding to 16, 6, and 10 different decedents, respectively.</p><p>Experimental Details. Notably, the double-tier backbone was self-supervised and learned on the rat training set for 100 epochs by setting the mini-batch size as 1024, with the parameters initialized by the ImageNet pre-trained models. The training data were augmented by a histopathology-oriented strategy by combining different kinds of staining jitters, random affine transformation, Gaussian blurring, resizing, etc. The image(patch) dimension in our implementation was 224*224. The tuning parameters γ and λ in L all were set as 5 and 0.005, respectively. Thereafter, the MIL blocks on two different datasets were both trained by minimizing the cross-entropy loss for 20 epochs with the minibatch size setting as 32. The experiments were conducted on three PCs with twenty NVIDIA GEFORCE RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results of Self-supervised Contrastive Learning</head><p>Our self-supervised double-tier backbone was compared with other state-ofthe-art self-supervised learning approaches, including balow twins <ref type="bibr" target="#b26">[27]</ref>, swin transformer (SSL) <ref type="bibr" target="#b25">[26]</ref>, TransPath <ref type="bibr" target="#b22">[23]</ref>, CTransPath <ref type="bibr" target="#b23">[24]</ref>,RetCCL <ref type="bibr" target="#b21">[22]</ref> and MOCOV3 <ref type="bibr" target="#b2">[3]</ref>. To evaluate the discriminative power of these competing methods, we adopted GAP to aggregate their instance representations from a whole image to train simple linear classifiers for the recognition of seven different organ tissues on both the rat and human datasets, with the test performance quantified in terms of four general classification metrics (i.e., ACC, F1 score, MCC(Matthews Correlation Coefficient), and Precision). The corresponding results are summarized in Table <ref type="table" target="#tab_0">1</ref>, from which we can have two observations. First, our self-supervised double-tier backbone consistently outperformed all other competing methods in terms of all metrics on two datasets. Second, our method led to better generalization, as the backbone trained on the rat dataset shows promising performance on the challenging real-world human dataset (e.g., resulting in an ACC higher than 90%). These results suggest the effectiveness of our self-supervised learning strategy. For a more detailed evaluation, we further conducted a series of ablation studies to evaluate the contributions of the contrastive losses (i.e., L global and L parts ) and regularization strategy (i.e., L var + L cov ). The corresponding results are summarized in Table <ref type="table" target="#tab_1">2</ref>, from which we can see that, given the baseline of L global , both the inclusion of the spatially fine-grained contrastive loss (i.e., L parts ) and informativeness regularization (i.e., L var and L cov ) led to respective performance gains. These results further justify our self-supervised design. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results of Multiple-Instance Learning</head><p>Based upon the double-tier backbone learned on the rat training set, we compared our context-aware MIL with other MIL methods, including the gated attention-based approach (i.e., AB-MIL <ref type="bibr" target="#b11">[12]</ref>, DSMIL <ref type="bibr" target="#b14">[15]</ref>, Transmil <ref type="bibr" target="#b18">[19]</ref> and MSA <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>) approaches with/without different positional embedding strategies, i.e., relative position embedding (MSA-RP <ref type="bibr" target="#b16">[17]</ref>), learnable position embedding (MSA-LP <ref type="bibr" target="#b6">[7]</ref>), and 2D sine-cosine position embedding (MSA-SP <ref type="bibr" target="#b7">[8]</ref>). Notably, our approach used MSA-RP as the baseline, based on which an adaptive pooling operation is designed to produce the final bag-level representation. To check the efficacy of adaptive pool, we further conducted a corresponding set of ablation studies by replacing it with other operations, including max pool, and soft pool <ref type="bibr" target="#b19">[20]</ref>. These comparison and ablations results are shown in Table <ref type="table" target="#tab_2">3</ref>, from which we can observe that our method led to the best results on both datasets, with relatively more significant improvements on the challenging human dataset. Also, compared with other pooling operations, the adaptive pool design brought consistent performance gains. These results suggest the efficacy of our contextaware MIL for postmortem tissue recognition.</p><p>In addition, we conducted LayerCAM-based analysis <ref type="bibr" target="#b12">[13]</ref> to check the explainability and reliability of our postmortem histopathological recognition results. From the representative examples shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we can have an interesting observation that our method tends to focus on tissue-specific postmortem patterns at different microscopic scales. For example, the spatial attention maps reliably highlighted the meningeal structures of the brain tissue, the glomeruli in the kidney cortex, and the central vein area between the liver lobules. On  the other hand, based on the pancreas example, we can see that our network can sensitively localize the pancreas glandular structure while filtering out the uninformative background in an end-to-end fashion, without the need for any pre-processing to segment first the foreground. These observations support our assumption that the proposed method is reliable and efficient in learning discriminative histopathological representations of postmortem organ tissues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we have proposed a context-aware MIL framework powered by self-supervised contrastive learning to learn fine-grained discriminative representations for postmortem histopathological recognition. The dedicated selfsupervised learning strategy concurrently maximizes multiple contrastive losses and regularization terms to deduce informative and discriminative instance embedding. Thereafter, the context-aware MIL framework adopts MSA followed by an adaptive pooling operation to distill from all instances a holistic bag/image-level representation. The experimental results on a relatively largescale database suggest the state-of-the-art postmortem recognition performance of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our FPath that consists of a self-supervised double-tier backbone (Step 1) and a context-aware MIL branch for postmortem recognition (Step 2).</figDesc><graphic coords="3,42,30,412,37,339,10,136,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Explainability analysis based on LayerCAM<ref type="bibr" target="#b12">[13]</ref> for representative postmortem tissue images acquired at different microscopic scales.</figDesc><graphic coords="9,42,00,217,88,340,03,72,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Linear classification results obtained by different self-supervised learning approaches on the rat and human testing sets, respectively.</figDesc><table><row><cell>Competing methods</cell><cell cols="2">Rat dataset</cell><cell></cell><cell></cell><cell cols="2">Human dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>F1</cell><cell>MCC</cell><cell cols="2">Precision ACC</cell><cell>F1</cell><cell>MCC</cell><cell>Precision</cell></row><row><cell>balow twins [27]</cell><cell>0.9232</cell><cell>0.9123</cell><cell>0.9076</cell><cell>0.9070</cell><cell>0.7306</cell><cell>0.7311</cell><cell>0.6854</cell><cell>0.7345</cell></row><row><cell cols="2">swin transformer(SSL) [26] 0.9450</cell><cell>0.9369</cell><cell>0.9299</cell><cell>0.9330</cell><cell>0.8079</cell><cell>0.8088</cell><cell>0.7758</cell><cell>0.8125</cell></row><row><cell>Transpath [23]</cell><cell>0.7351</cell><cell>0.7397</cell><cell>0.6958</cell><cell>0.7481</cell><cell>0.5838</cell><cell>0.5657</cell><cell>0.5264</cell><cell>0.6050</cell></row><row><cell>CTransPath [24]</cell><cell>0.9635</cell><cell>0.9610</cell><cell>0.9535</cell><cell>0.9596</cell><cell>0.8794</cell><cell>0.8799</cell><cell>0.8591</cell><cell>0.8842</cell></row><row><cell>RetCCL [22]</cell><cell>0.9794</cell><cell>0.9801</cell><cell>0.9768</cell><cell>0.9810</cell><cell>0.7796</cell><cell>0.7789</cell><cell>0.7448</cell><cell>0.7961</cell></row><row><cell>MOCOV3 [3]</cell><cell>0.9732</cell><cell>0.9738</cell><cell>0.9681</cell><cell>0.9745</cell><cell>0.8103</cell><cell>0.8124</cell><cell>0.7790</cell><cell>0.8187</cell></row><row><cell>Ours</cell><cell cols="4">0.9831 0.9831 0.9796 0.9831</cell><cell cols="4">0.9049 0.9044 0.8886 0.9056</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies to evaluate the contributions of different self-supervised contrastive losses and regularization terms.</figDesc><table><row><cell>Loss functions</cell><cell cols="2">Rat dataset</cell><cell></cell><cell></cell><cell cols="2">Human dataset</cell></row><row><cell cols="2">L global L parts Lvar Lcov ACC</cell><cell>F1</cell><cell>MCC</cell><cell cols="2">Precision ACC</cell><cell>F1</cell><cell>MCC</cell><cell>Precision</cell></row><row><cell></cell><cell cols="3">0.9732 0.9713 0.9660</cell><cell>0.9689</cell><cell cols="3">0.8918 0.8913 0.8734 0.8935</cell></row><row><cell></cell><cell cols="3">0.9817 0.9819 0.9778</cell><cell>0.9822</cell><cell cols="3">0.8953 0.8956 0.8779 0.8981</cell></row><row><cell></cell><cell cols="3">0.9793 0.9799 0.9757</cell><cell>0.9806</cell><cell cols="3">0.8978 0.8976 0.8802 0.8983</cell></row><row><cell></cell><cell cols="4">0.9831 0.9831 0.9796 0.9831</cell><cell cols="3">0.9049 0.9044 0.8886 0.9056</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Multiple-instance learning results obtained by the competing methods and our Context-Aware MIL with different pooling strategies.</figDesc><table><row><cell>Competing methods</cell><cell cols="2">Rat dataset</cell><cell></cell><cell></cell><cell cols="2">Human dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>F1</cell><cell>MCC</cell><cell cols="2">Precision ACC</cell><cell>F1</cell><cell>MCC</cell><cell>Precision</cell></row><row><cell>AB-MIL [12]</cell><cell>0.9815</cell><cell>0.9828</cell><cell>0.9793</cell><cell>0.9844</cell><cell>0.9011</cell><cell>0.9005</cell><cell>0.8838</cell><cell>0.9050</cell></row><row><cell>DSMIL [15]</cell><cell>0.9951</cell><cell>0.9948</cell><cell>0.9937</cell><cell>0.9945</cell><cell>0.9176</cell><cell>0.9166</cell><cell>0.9030</cell><cell>0.9170</cell></row><row><cell>Transmil [19]</cell><cell>0.9899</cell><cell>0.9888</cell><cell>0.9875</cell><cell>0.9878</cell><cell>0.8824</cell><cell>0.8813</cell><cell>0.8622</cell><cell>0.8821</cell></row><row><cell>MSA [2, 16]</cell><cell>0.9875</cell><cell>0.9883</cell><cell>0.9861</cell><cell>0.9892</cell><cell>0.9082</cell><cell>0.9082</cell><cell>0.8921</cell><cell>0.9100</cell></row><row><cell>MSA-LP [7]</cell><cell>0.9879</cell><cell>0.9875</cell><cell>0.9853</cell><cell>0.9873</cell><cell>0.9097</cell><cell>0.9087</cell><cell>0.8945</cell><cell>0.9109</cell></row><row><cell>MSA-SP [8]</cell><cell>0.9851</cell><cell>0.9839</cell><cell>0.981</cell><cell>0.9832</cell><cell>0.8915</cell><cell>0.8905</cell><cell>0.8748</cell><cell>0.8948</cell></row><row><cell>MSA-RP [17]</cell><cell>0.9915</cell><cell>0.9915</cell><cell>0.9896</cell><cell>0.9916</cell><cell>0.9218</cell><cell>0.9213</cell><cell>0.9085</cell><cell>0.9218</cell></row><row><cell>Ours + Max pool</cell><cell>0.9910</cell><cell>0.9909</cell><cell>0.9888</cell><cell>0.9909</cell><cell>0.9144</cell><cell>0.9147</cell><cell>0.9001</cell><cell>0.9191</cell></row><row><cell cols="2">Ours + Soft pool [20] 0.9935</cell><cell>0.9929</cell><cell>0.9915</cell><cell>0.9924</cell><cell>0.9047</cell><cell>0.9023</cell><cell>0.8883</cell><cell>0.9056</cell></row><row><cell cols="5">Ours + Adaptive pool 0.9956 0.9952 0.9943 0.9949</cell><cell cols="4">0.9229 0.9218 0.9093 0.9263</cell></row><row><cell>Pancreas (5X_rat)</cell><cell cols="2">Lung (10X_human)</cell><cell></cell><cell>Kidney (20X_rat)</cell><cell></cell><cell cols="2">Kidney (40X_rat)</cell><cell></cell></row><row><cell>Liver (5X_rat)</cell><cell cols="2">Brain (10X_rat)</cell><cell></cell><cell cols="2">Pancreas (20X_human)</cell><cell cols="2">Liver (40X_rat)</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funding Information. This work was supported in part by <rs type="funder">NSFC</rs> Grants (Nos. <rs type="grantNumber">62101430 &amp; 62101431</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BZx7mTh">
					<idno type="grant-number">62101430 &amp; 62101431</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">VICReg: Variance-invariance-covariance regularization for self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Usefulness of systematic histological examination in routine forensic autopsy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>De La Grandmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Charlier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Durigon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Forensic Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="88" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Dimaio</surname></persName>
		</author>
		<title level="m">Forensic Pathology</title>
		<meeting><address><addrLine>Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dolinak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matshes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Lew</surname></persName>
		</author>
		<title level="m">Forensic Pathology: Principles and Practice</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning where to learn in cross-view self-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Attention-based Deep Multiple Instance Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LayerCAM: exploring hierarchical class activation maps for localization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5875" to="5888" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Derivation of prognostic contextual histopathological features from whole-slide images of tumours via graph deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DT-MIL: deformable transformer for multi-instance learning on histopathological image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_20" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="206" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F K</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refining activation downsampling with softpool</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kalliatakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10357" to="10366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An emerging strategy for muscle evanescent trauma discrimination by spectroscopy and chemometrics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mol. Sci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">13489</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RetCCL: clustering-guided contrastive learning for whole-slide image retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102645</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TransPath: transformer-based self-supervised learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_18" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformer-based unsupervised contrastive learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102559</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pathological and ATR-FTIR spectral changes of delayed splenic rupture and medical significance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spectrochim. Acta. A Mol. Biomol. Spectrosc</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="page">121286</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<title level="m">Self-Supervised Learning with Swin Transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
