<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer-Based End-to-End Classification of Variable-Length Volumetric Data</title>
				<funder>
					<orgName type="full">National Foundation for Research, Technology and Development, and Heidelberg Engineering</orgName>
				</funder>
				<funder>
					<orgName type="full">Christian Doppler Research Association</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Federal Ministry for Digital and Economic Affairs</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marzieh</forename><surname>Oghbaie</surname></persName>
							<email>marzieh.oghbaie@meduniwien.ac.at</email>
							<idno type="ORCID">0000-0001-7391-1612</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Laboratory for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teresa</forename><surname>Araújo</surname></persName>
							<idno type="ORCID">0000-0001-9687-528X</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Laboratory for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taha</forename><surname>Emre</surname></persName>
							<idno type="ORCID">0000-0002-6753-5048</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Laboratory for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
							<idno type="ORCID">0000-0002-7788-7311</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hrvoje</forename><surname>Bogunović</surname></persName>
							<email>hrvoje.bogunovic@meduniwien.ac.at</email>
							<idno type="ORCID">0000-0002-9168-0894</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Laboratory for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer-Based End-to-End Classification of Variable-Length Volumetric Data</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="358" to="367"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">86F1BE948CCC323A901E04DF8302783A</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Optical coherence tomography</term>
					<term>3D volume classification</term>
					<term>Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The automatic classification of 3D medical data is memoryintensive. Also, variations in the number of slices between samples is common. Naïve solutions such as subsampling can solve these problems, but at the cost of potentially eliminating relevant diagnosis information. Transformers have shown promising performance for sequential data analysis. However, their application for long sequences is data, computationally, and memory demanding. In this paper, we propose an end-to-end Transformer-based framework that allows to classify volumetric data of variable length in an efficient fashion. Particularly, by randomizing the input volume-wise resolution(#slices) during training, we enhance the capacity of the learnable positional embedding assigned to each volume slice. Consequently, the accumulated positional information in each positional embedding can be generalized to the neighbouring slices, even for high-resolution volumes at the test time. By doing so, the model will be more robust to variable volume length and amenable to different computational budgets. We evaluated the proposed approach in retinal OCT volume classification and achieved 21.96% average improvement in balanced accuracy on a 9-class diagnostic task, compared to state-of-the-art video transformers. Our findings show that varying the volume-wise resolution of the input during training results in more informative volume representation as compared to training with fixed number of slices per volume.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Volumetric medical scans allow for comprehensive diagnosis, but their manual interpretation is time consuming and error prone <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. Deep learning methods have shown exceptional performance in automating this task <ref type="bibr" target="#b26">[27]</ref>, often at medical expert levels <ref type="bibr" target="#b5">[6]</ref>. However, their application in the clinical practice is still limited, partially because they require rigid acquisition settings. In particular, variable volume length, i.e. number of slices, is common for imaging modalities such as computed tomography, magnetic resonance imaging or optical coherence tomography (OCT). Despite the advantages of having data diversity in terms of quality and size, automated classification of dense scans with variable input size is a challenge. Furthermore, the 3D nature of medical volumes results in a memory-intensive training procedure when processing the entire volume. To account for this constraint and make the input size uniform, volumes are usually subsampled, ignoring and potentially hiding relevant diagnostic information.</p><p>Among approaches for handling variable input size, Multiple Instance Learning (MIL) is commonly used. There, a model classifies each slice or subgroup of slices individually, and the final prediction is determined by aggregating subdecisions via maximum or average pooling <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>, or other more sophisticated fusion approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. However, they often do not take advantage of the 3D aspect of the data. The same problem occurs when stacking slice-wise embeddings <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, applying self-attention <ref type="bibr" target="#b4">[5]</ref> for feature aggregation, or using principal component analysis (PCA) <ref type="bibr" target="#b8">[9]</ref> to reduce the variable number of embeddings to a fixed size. As an alternative, recurrent neural networks (RNNs) <ref type="bibr" target="#b17">[18]</ref> consider the volume as a sequence of arranged slices or the corresponding embeddings. However, their performance is overshadowed by arduous training and lack of parallelization.</p><p>Vision Transformers (ViTs) <ref type="bibr" target="#b7">[8]</ref>, on the other hand, allow parallel computation and effective analysis of longer sequences by benefiting from multi-head self-attention (MSA) and positional encoding. These components allow to model both local and global dependencies, playing a pivotal role for 3D medical tasks where the order of slices is important <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, ViTs are more flexible regarding input size. Ignoring slice positional information (bag-of-slices) or using sinusoidal positional encoding enables them to process sequences of arbitrary length with respect to computational resources. However, ViTs with learnable positional embeddings (PEs) have shown better performance <ref type="bibr" target="#b7">[8]</ref>. In this case, the only restriction in processing variable length sequences is the number of PEs. Although interpolating the PE sequence helps overcome this restriction, the resultant sequence will not model the exact positional information of the corresponding slices in the input sequence, affecting ViTs performance <ref type="bibr" target="#b1">[2]</ref>. Notably, Flexible ViT <ref type="bibr" target="#b1">[2]</ref> (FlexiViT) handles patch sequences of variable sizes by randomizing the patch size during training and, accordingly, resizing the embedding weights and parameters corresponding to PEs.</p><p>Despite the merits of the aforementioned approaches, three fundamental challenges still remain. First, the model should be able to process inputs with variable volume resolutions, where throughout the paper we refer to the resolution in the dimension across slices (number of slices), and simultaneously capture the size-independent characteristics of the volume and similarities among the constituent slices. The second challenge is the scalability and the ability of the model to adapt to unseen volume-wise resolutions at inference time. Lastly, the training of deep learning models with high resolution volumes is both computationally expensive and memory-consuming.</p><p>In this paper, we propose a late fusion Transformer-based end-to-end framework for 3D volume classification whose local-similarity-aware PEs not only improve the model performance, but also make it more robust to interpolation of PEs sequence. We first embed each slice by a spatial feature extractor and then aggregate the corresponding sequence of slice-wise embeddings with a Feature Aggregator Transformer (FAT) module to capture 3D intrinsic characteristics of the volume and produce a volume-level representation. To enable the model to process volumes with variable resolutions, we propose a novel training strategy, Variable Length FAT (VLFAT), that enables FAT module to process volumes with different resolutions both at training and test times. VLFAT can be trained with a proportionally few #slices, an efficient trait in case of training time/memory constraints. Consequently, even with drastic slice subsampling during training, the model will be robust against extreme PEs interpolation for high-resolution volumes at the test time. The proposed approach is modelagnostic and can be deployed with Transformer-based backbones. VLFAT beats the state-of-the-art performance in retinal OCT volume classification on a private dataset with nine disease classes, and achieves competitive performance on a two-class public dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our end-to-end Transformer-based volume classification framework (Fig. <ref type="figure" target="#fig_0">1</ref>) has three main components: 1) Slice feature extractor (SFE) to extract spatial biomarkers and create a representation of the corresponding slice; 2) Volume feature aggregator (VFA) to combine the slice-level representations into a volumelevel representation, and 3) Volume classification. Trained with the proposed strategy, our approach is capable of processing and classifying volumes with varying volume-wise resolutions. Let's consider a full volume v ∈ R (N ×W ×H) , where (N, H, W) are the #slices, its width and height respectively. The input to the network is a subsampled volume by randomly selecting n slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slice Feature Extractor (SFE).</head><p>To obtain the slice representations, we use ViT as our SFE due to its recent success in medical interpretation tasks <ref type="bibr" target="#b9">[10]</ref>. ViT mines crucial details from each slice and, using MSA and PE, accumulates the collected information in a learnable classification token, constituting the slicewise embedding. For each slice token, we then add a learnable 1D PE <ref type="bibr" target="#b7">[8]</ref> to retain the position of each slice in the volume.</p><p>Volume Feature Aggregator (VFA). The output of the previous step is a sequence of slice-wise embeddings, to which we append a learnable volumelevel classification token <ref type="bibr" target="#b6">[7]</ref>. The resulting sequence of embedding vectors is then processed by the FAT module to produce a volume-level embedding. In particular, we propose VLFAT, a FAT with enhanced learnable PEs, inspired on FlexiViT <ref type="bibr" target="#b1">[2]</ref>, where we modify #slices per input volume instead of patch sizes and correspondingly apply PEs interpolation. This allows handling arbitrary volume resolutions, which generally would not be possible except for an ensemble of models of different scales. Specifically, at initialization we set a fixed value, n, for #slices, resulting in PEs sequence with size (n + 1, dim), where an extra PE is assigned to the classification token and dim is the dimension of the slice representation. In each training step, we then randomly sample a new value for n from a predefined set and, accordingly, linearly interpolate the PEs sequence (Fig. <ref type="figure" target="#fig_0">1</ref>), using the known adjacent PEs <ref type="bibr" target="#b2">[3]</ref>. This allows to preserve the similarity between neighboring slices in the volume, sharing biomarkers in terms of locality, and propagating the corresponding positional information. The new PEs are then normalized according to a truncated normal distribution.</p><p>Volume Classification. Finally, the volume-level classification token is fed to a Fully Connected (FC) layer, which produces individual class scores. As a loss function, we employ the weighted cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We tested our model for volume classification of macula-centered retinal OCT scans, where large variation in volume resolution (#B-scans) between samples is very common. For multiclass classification performance metrics, we relied on Balanced Accuracy (BAcc) and one-vs-all Area Under the Receiver Operating Curve (AUROC). The source code is available at: github.com/marziehoghbaie/VLFAT. Comparison to State-of-the-Art Methods. We compared the performance of the proposed method with two state-of-the-art video ViTs (ViViT) <ref type="bibr" target="#b0">[1]</ref>, originally designed for natural video classification: 1) factorized encoder (FE) ViViT, that models the spatial and temporal dimensions separately; 2) factorised selfattention (FSA) ViViT, that simultaneously computes spatial and temporal interactions. We selected FE and FSA ViViTs as baselines to understand the importance of separate feature extractors and late fusion in our approach. FE ViViT, similar to ours, utilizes late fusion, while FSA ViViT is a slow-fusion model and processes spatiotemporal patches as tokens.</p><p>Ablation Studies. To investigate the contribution of SFE module, we deployed ViT and ResNet18 <ref type="bibr" target="#b25">[26]</ref>, a standard 2D convolutional neural network (CNN) in medical image analysis, with pooling methods as VFA where the quality of slicewise features is more influential. For VFA, we explored average pooling (AP), max pooling (MP), and 1D convolution (1DConv). As MIL-based baselines, pooling methods can be viable alternatives to VLFAT for processing variable volume resolutions. In addition to learnable PE, we deployed sinusoidal PE (sinPE) and bag-of-slices (noPE) for FAT to examine the effect of positional information.</p><p>Robustness Analysis. We investigate the robustness of VFLAT and FAT to PEs sequence interpolation at inference time by changing the volume resolution.</p><p>To process inputs with volume resolutions different from FAT's and VLFAT's input size, we linearly interpolate the sequence of PEs at the test time. For 9C dataset, we only assess samples with minimum #slices of 128 to better examine the PE's scalability to higher resolutions.</p><p>Implementation Details. The volume input size was 25 × 224 × 224 for all experiments except for FSA ViViT where #slices was set to 24 based on the corresponding tublet size of 2 × 16 × 16. During VLFAT training, the #slices varied between {5, 10, 15, 20, 25}, specified according to memory constraints. We randomly selected slices using a normal distribution with its mean at the central slice position, thus promoting the inclusion of the region near the fovea, essential for the diagnosis of macular diseases. Our ViT configuration is based on ViT-Base <ref type="bibr" target="#b7">[8]</ref> with patch size 16 × 16, and 12 Transformer blocks and heads. For FAT and VLFAT, we set the number of Transformer blocks to 12 and heads to 3. The slice-wise and volume-wise embedding dimension were set to 768. The configuration of ViViT baselines was set according to the original papers <ref type="bibr" target="#b0">[1]</ref>. Training was performed using AdamW <ref type="bibr" target="#b11">[12]</ref> optimizer with learning rate of 6×10 -6 with cosine annealing. All models were trained for 600 epochs with a batch size of 8. Data augmentation included random brightness enhancing, motion blur, salt/pepper noise, rotation, and random erasing <ref type="bibr" target="#b27">[28]</ref>. The best model was selected based on the highest BAcc on the validation set. All experiments were performed using Pytorch 1.13.0+cu117 and timm library <ref type="bibr" target="#b24">[25]</ref> on a server with 1 TB RAM, and NVIDIA RTX A6000 (48 GB VRAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>In particular, on large 9C dataset our VLFAT achieved 21.4% and 22.51% BAcc improvement compared to FE ViViT and FSA ViViT, respectively. Incorporating our training strategy, VLFAT, improved FAT's performance by 16.12% on 9C, and 8.79% on OLIVES, which verifies the ability of VLFAT in learning more location-aware PEs, something that is also reflected in the increase of AUROCs (0.96→ 0.98 on 9C dataset and 0.95→ 0.97 on OLIVES). Per-class AUROCs are shown in Table <ref type="table" target="#tab_1">2</ref>. The results show that for most of the classes, our VLFAT has better diagnostic ability and collects more disease-specific clues from the volume.  The ablation study (Table <ref type="table" target="#tab_0">1</ref>) showed that each introduced component in the proposed model contributed to the performance improvement. In particular, ViT was shown as a better slice feature extractor compared to ResNet18, particularly on 9C dataset where the differences between disease-related biomarkers are more subtle. Additionally, the poor performance of the pooling methods as compared to FAT and 1DConv, emphasizes the importance of contextual volumetric information, the necessity of a learnable VFA, and the superiority of Transformers over 1DConv. Although, on OLIVES, less complicated VFAs (pooling/1DConv) and FAT (noPE) also achieved comparable results, which can be attributed primarily to DR vs. DME <ref type="bibr" target="#b14">[15]</ref> being an easier classification task compared to the diverse disease severity in the 9C dataset. In addition, the competitive advantage of VLFAT in handling different resolutions was not fully exploited in OLIVES since the large majority of cases had the same #slices. On 9C, however, the comparison of positional encoding strategies demonstrated that although ignoring PEs and sinusoidal approach provide deterministic predictions, the importance of learnable PEs in modeling the anatomical order of slices in the volume is crucial. The robustness analysis is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. VLFAT was observed to have more scalable and robust PEs when the volume-wise resolutions at the test time deviated from those used during training. This finding highlights the VLFAT's potential for resource-efficient training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose an end-to-end framework for 3D volume classification of variable-length scans, benefiting from ViT to process volume slices and FAT to capture 3D information. Furthermore, we enhance the capacity of PE in FAT to capture sequential dependencies along volumes with variable resolutions. Our proposed approach, VLFAT, is more scalable and robust than vanilla FAT at classifying OCT volumes of different resolutions. On a large-scale retinal OCT datasets, our results indicate that this effective method performs in the majority of cases better than other common methods for volume classification. Besides its applicability for volumetric medical data analysis, our VFLAT has potential to be applied on other medical tasks including video analysis (e.g. ultrasound videos) and high-resolution imaging, as is the case in histopathology. Future work would include adapting VLFAT to ViViT models to make them less computationally expensive. Furthermore, PEs in VLFAT could be leveraged for improving the visual interpretation of decision models by collecting positional information about the adjacent slices sharing anatomical similarities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of the proposed Transformer-based approach for 3D volume classification. The shared SFE processes the input slices, and in line with VLFAT, both #slices and the PEs sequence are updated at each epoch. * 1D PE is added to each slice embedding for FAT and VLFAT.</figDesc><graphic coords="4,57,96,54,62,336,13,124,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Samples of central B-scan from all disease classes. * DR is only in OLIVES.</figDesc><graphic coords="5,103,80,54,44,216,31,121,27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Robustness analysis of VLFAT and vanilla FAT against PEs interpolation at the test time: (a) 9C dataset; (b) OLIVES</figDesc><graphic coords="7,63,30,53,99,297,64,107,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification performance in terms of balanced accuracy (BAcc) and mean one-vs-all AUROC. FE: Factorised Encoder, FSA: Factorised Self-Attention, SFE: Slice Feature Extractor, VFA: Volume Feature Aggregator.</figDesc><table><row><cell>Method (SFE/VFA)</cell><cell>9C</cell><cell>OLIVES</cell><cell>#slices  *</cell></row><row><cell></cell><cell cols="2">BAcc AUROC BAcc AUROC</cell><cell></cell></row><row><cell cols="2">FE ViViT (baseline) [1] 0.64 0.96</cell><cell>0.93 0.98</cell><cell>25</cell></row><row><cell cols="2">FSA ViViT (baseline) [1] 0.63 0.95</cell><cell>0.92 0.98</cell><cell>24</cell></row><row><cell>ViT/1DConv</cell><cell>0.61 0.94</cell><cell>0.95 0.97</cell><cell>25</cell></row><row><cell>ResNet18/AP</cell><cell>0.34 0.73</cell><cell>0.72 0.83</cell><cell>all</cell></row><row><cell>ResNet18/MP</cell><cell>0.41 0.87</cell><cell>0.83 0.93</cell><cell>all</cell></row><row><cell>ViT/AP</cell><cell>0.59 0.95</cell><cell>0.82 0.97</cell><cell>all</cell></row><row><cell>ViT/MP</cell><cell>0.61 0.96</cell><cell>0.90 0.98</cell><cell>all</cell></row><row><cell>ViT/FAT (noPE)</cell><cell>0.58 0.93</cell><cell>0.95 0.99</cell><cell>all</cell></row><row><cell>ViT/FAT (sinPE)</cell><cell>0.62 0.93</cell><cell>0.87 0.96</cell><cell>all</cell></row><row><cell>ViT/FAT +</cell><cell>0.67 0.96</cell><cell>0.88 0.95</cell><cell>25</cell></row><row><cell>ViT/VLFAT (ours)</cell><cell>0.78 0.98</cell><cell>0.95 0.97</cell><cell>all</cell></row></table><note><p>Legend: * #slices at the test time; + the input length is fixed in both training and test time</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Per-class classification performance (one-vs-all AUROC) on 9C dataset.</figDesc><table><row><cell cols="8">Method(SFE/VFA) CNV1 CNV2 CNV3 DME GA Healthy iAMD RVO Stargardt</cell></row><row><cell>FE ViViT [1]</cell><cell>0.93</cell><cell>0.91</cell><cell>0.95</cell><cell>0.94</cell><cell>0.99 0.95</cell><cell>0.92</cell><cell>0.95 0.99</cell></row><row><cell>FSA ViViT [1]</cell><cell>0.94</cell><cell>0.91</cell><cell>0.92</cell><cell>0.92</cell><cell>1.0 0.94</cell><cell>0.93</cell><cell>0.94 0.99</cell></row><row><cell>ViT/1DConv</cell><cell>0.88</cell><cell>0.92</cell><cell>0.94</cell><cell>0.91</cell><cell>0.98 0.92</cell><cell>0.92</cell><cell>0.92 1.0</cell></row><row><cell>ResNet18/AP</cell><cell>0.68</cell><cell>0.63</cell><cell>0.58</cell><cell>0.75</cell><cell>0.81 0.75</cell><cell>0.75</cell><cell>0.76 0.97</cell></row><row><cell>ResNet18/MP</cell><cell>0.78</cell><cell>0.77</cell><cell>0.79</cell><cell>0.87</cell><cell>0.91 0.84</cell><cell>0.84</cell><cell>0.84 0.91</cell></row><row><cell>ViT/AP</cell><cell>0.9</cell><cell>0.81</cell><cell>0.92</cell><cell>0.93</cell><cell>0.98 0.94</cell><cell>0.93</cell><cell>0.94 0.99</cell></row><row><cell>ViT/MP</cell><cell>0.95</cell><cell>0.85</cell><cell>0.95</cell><cell>0.94</cell><cell>0.98 0.94</cell><cell>0.93</cell><cell>0.96 0.99</cell></row><row><cell>ViT/FAT (noPE)</cell><cell>0.9</cell><cell>0.87</cell><cell>0.89</cell><cell>0.89</cell><cell>0.98 0.91</cell><cell>0.9</cell><cell>0.94 0.99</cell></row><row><cell>ViT/FAT (sinPE)</cell><cell>0.94</cell><cell>0.93</cell><cell>0.93</cell><cell>0.88</cell><cell>0.97 0.91</cell><cell>0.89</cell><cell>0.93 0.99</cell></row><row><cell>ViT/FAT</cell><cell>0.93</cell><cell>0.82</cell><cell>0.97</cell><cell>0.94</cell><cell>0.99 0.95</cell><cell>0.94</cell><cell>0.95 0.99</cell></row><row><cell cols="2">ViT/VLFAT (ours) 0.98</cell><cell>0.92</cell><cell>0.98</cell><cell cols="2">0.98 1.0 0.99</cell><cell>0.98</cell><cell>0.98 1.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">Christian Doppler Research Association</rs>, <rs type="funder">Austrian Federal Ministry for Digital and Economic Affairs</rs>, the <rs type="funder">National Foundation for Research, Technology and Development, and Heidelberg Engineering</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: a video vision transformer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08013</idno>
		<title level="m">Flexivit: one model for all patch sizes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linear interpolation revitalized</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thévenaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="710" to="719" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54184-6_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-54184-66" />
	</analytic>
	<monogr>
		<title level="m">ACCV 2016</title>
		<editor>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10112</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">B-scan attentive CNN for the classification of retinal optical coherence tomography volumes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prabhakararao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Bora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1025" to="1029" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic classification of retinal three-dimensional optical coherence tomography images using principal component analysis network with composite kernels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="116011" to="116011" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformers in medical image analysis: a review</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Med</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving ultrasound video classification: an evaluation of novel deep learning methods in echocardiography</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3D tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-916" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focused attention in transformers for interpretable classification of retinal images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Playout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102608</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Prabhushankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kokilepersaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wykoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.11195</idno>
		<title level="m">Olives dataset: Ophthalmic labels for investigating visual eye semantics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised iterative refinement learning for macular oct volumetric data classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">103327</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Macular OCT classification using a multi-scale convolutional neural network ensemble</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehridehnavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hajizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end deep learning model for predicting treatment requirements in neovascular AMD from longitudinal retinal OCT imaging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Romo-Bucheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3456" to="3465" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semivariogram and semimadogram functions as descriptors for AMD diagnosis on SD-OCT topographic maps using support vector machine</title>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D deep learning on medical images: a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gulyás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">5097</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic diagnosis of macular diseases from OCT volume based on its two-dimensional feature map and convolutional neural network with attention mechanism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="96004" to="096004" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making AI transferable across oct scanners from different vendors</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Vente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>González-Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Thee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Grinsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Klaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Visual Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2118" to="2118" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861.https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context-aware transformers for spinal cancer detection and radiological grading</title>
		<author>
			<persName><forename type="first">R</forename><surname>Windsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-826" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning-based survival prediction for multiple cancer types using histopathology images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wulczyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">233678</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
