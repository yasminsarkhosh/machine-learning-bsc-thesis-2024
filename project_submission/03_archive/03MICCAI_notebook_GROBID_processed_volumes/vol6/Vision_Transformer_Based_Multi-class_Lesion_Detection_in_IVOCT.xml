<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision Transformer Based Multi-class Lesion Detection in IVOCT</title>
				<funder ref="#_xFbECHV">
					<orgName type="full">National Key Research and Development Project of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zixuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyi</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhili</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Su</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiyong</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sichuan Provincial People&apos;s Hospital</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinsong</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
							<email>qianyu@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sichuan Provincial People&apos;s Hospital</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vision Transformer Based Multi-class Lesion Detection in IVOCT</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="327" to="336"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E3FBF2ED4ABBDA82F2C176A6382F2419</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>IVOCT</term>
					<term>Object Detection</term>
					<term>Vision Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cardiovascular disease is a high-fatality illness. Intravascular Optical Coherence Tomography (IVOCT) technology can significantly assist in diagnosing and treating cardiovascular diseases. However, locating and classifying lesions from hundreds of IVOCT images is time-consuming and challenging, especially for junior physicians. An automatic lesion detection and classification model is desirable. To achieve this goal, in this work, we first collect an IVOCT dataset, including 2,988 images from 69 IVOCT data and 4,734 annotations of lesions spanning over three categories. Based on the newly-collected dataset, we propose a multi-class detection model based on Vision Transformer, called G-Swin Transformer. The essential part of our model is grid attention which is used to model relations among consecutive IVOCT images. Through extensive experiments, we show that the proposed G-Swin Transformer can effectively localize different types of lesions in IVOCT images, significantly outperforming baseline methods in all evaluation metrics. Our code is available via this link. https://github.com/Shao1Fan/G-Swin-Transformer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the rapid development of new detection and treatment methods, the prevalence of cardiovascular disease continues to increase <ref type="bibr" target="#b0">[1]</ref>. It is still reported to be the most prevalent and deadly disease worldwide, with more than 1 million people diagnosed with acute coronary syndrome (ACS) in the U.S. in 2016. The average cost of hospital discharge for ACS patients is as high as $63,578 <ref type="bibr" target="#b1">[2]</ref>, which significantly increasing the financial burden on society and patients.</p><p>Optical coherence tomography (OCT) <ref type="bibr" target="#b2">[3]</ref> is a new biomedical imaging technique born in the 19901990ss. Intravascular optical coherence tomography (IVOCT) <ref type="bibr" target="#b3">[4]</ref> has a higher resolution compared with other imaging modalities in the vasculature and is considered to be the best imaging tool for plaque rupture, plaque erosion, and calcified nodules <ref type="bibr" target="#b4">[5]</ref>. Therefore, most existing work on IVOCT images focuses on identifying vulnerable plaques in the vasculature <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, while neglecting other characteristic manifestations of atherosclerotic plaques in IVOCT images, such as macrophage infiltration and thrombus formation. These lesions are closely related to the development of plaque changes <ref type="bibr" target="#b9">[10]</ref>. Studies have shown that atherosclerosis is an inflammatory disease dominated by macrophages and T lymphocytes, that a high density of macrophages usually represents a higher risk, and that thrombosis due to plaque rupture is a common cause of acute myocardial infarction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. In addition, some spontaneous coronary artery dissection (SCAD) can be detected in IVOCT images. The presence of the dissection predisposes to coronary occlusion, rupture, and even death <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. These lesions are inextricably linked to ACS. All three types of features observed through IVOCT images are valuable for clinical treatment, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. These lesions are inextricably linked to ACS and should be considered in clinical management. Achieving multi-class lesion detection in IVOCT images faces two challenges: 1) There is no public IVOCT dataset specifically designed for multi-class lesion detection. Most IVOCT datasets only focus on a single lesion, and research on the specific types of lesions in the cardiovascular system is still in its early stage.</p><p>2) It is difficult to distinguish between different lesions, even for senior radiologists. This is because these lesions vary in size and appearance within the same class, and some of them do not have regular form, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In clinical diagnosis, radiologists usually combine different pathological manifestations, lesion size, and the continuous range before and after in the IVOCT image to design accurate treatment strategies for patients. Unfortunately, most existing works ignore such information and do not consider the continuity of lesions in the 3D dimension. To address the above issues, we collaborated with the Cardiovascular Research Center of Sichuan Provincial People's Hospital to collect an IVOCT dataset and introduce a novel detection model that leverages the information from consecutive IVOCT images.</p><p>Overall, the contribution of this work can be summarized as follows: 1) We propose a new IVOCT dataset that is the first multi-class IVOCT dataset with bounding box annotations for macrophages, cavities/dissections, and thrombi. 2) We design a multi-class lesion detection model with a novel self-attention module that exploits the relationship between adjacent frames in IVOCT, resulting in improved performance. 3) We explore different data augmentation strategies for this task. 4) Through extensive experiments, we demonstrate the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>We collected and annotated a new IVOCT dataset consisting of 2,988 IVOCT images, including 2,811 macrophages, 812 cavities and dissections, and 1,111 thrombi. The collected data from 69 patients are divided into training/validation/test sets in a 55:7:7 ratio, respectively. Each split contains 2359/290/339 IVOCT frames. In this section, we will describe the data collection and annotation process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>We collaborated with the Cardiovascular and Cerebrovascular Research Center of Sichuan Provincial People's Hospital, which provided us with IVOCT data collected between 2019 and 2022. The data include OCT examinations of primary patients and post-coronary stenting scenarios. Since DICOM is the most widely-used data format in medical image analysis, the collecting procedure was exported to DICOM, and the patient's name and other private information contained in DICOM were desensitized at the same time. Finally, the 69 DICOM format data were converted into PNG images with a size of 575 × 575 pixels. It is worth noting that the conversion from DICOM to PNG did not involve any downsampling operations to preserve as much information as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Annotation</head><p>In order to label the lesions as accurately as possible, we designed a two-step annotation procedure. The first round was annotated by two expert physicians using the one-stop medical image labeling software Pair. Annotations of the two physicians may be different. Therefore, we asked them to discuss and reach agreement on each annotation. Next, the annotated data was sent to senior doctors to review. The review starts with one physician handling the labeling, including labeling error correction, labeling range modification, and adding missing labels. After that, another physician would continue to check and review the previous round's results to complete the final labeling. Through the above two steps, 2,988 IVOCT images with 4,734 valid annotations are collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Recently, object detection models based on Vision Transformers have achieved state-of-the-art (SOTA) results on various object detection datasets, such as the MS-COCO dataset. Among them, the Swin Transformer <ref type="bibr" target="#b18">[19]</ref> model is one of the best-performing models. Swin Transformer uses a self-attention mechanism within local windows to ensure computational efficiency. Moreover, its sliding window mechanism allows for global modeling by enabling self-attention computation between adjacent windows. Its hierarchical structure allows flexible modeling of information at different scales and is suitable for various downstream tasks, such as object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">G-Swin Transformer</head><p>In traditional object detection datasets such as the MS-COCO dataset, the images are typically isolated from each other without any correlation. However, in our proposed IVOCT dataset, each IVOCT scan contains around 370 frames with a strong inter-frame correlation. Specifically, for example, if a macrophage lesion is detected at the [x, y, w, h] position in frame F i of a certain IVOCT scan, it is highly likely that there is also a macrophage lesion near the [x, y, w, h] position in frame F i-1 or F i+1 , due to the imaging and pathogenesis principles of IVOCT and ACS. Doctors also rely on the adjacent frames for diagnosis rather than a single frame when interpreting IVOCT scans. But, the design of the Swin-Transformer did not consider the utilization of inter-frame information. Though global modeling is enabled by using the sliding window mechanism. In the temporal dimension, it still has a locality because the model did not see adjacent frames.</p><p>Based  feature pyramid network (FPN) for fusion of features at different resolutions. The RPN Head is then applied to obtain candidate boxes, and finally, the ROI Head is used for classification and refinement of candidate boxes to obtain class and bbox (bounding box) predictions. The inter-frame feature fusing is happend in the attention block, introduced in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Grid Attention</head><p>To better utilize information from previous and future frames and perform feature fusion, we propose a self-attention calculation mode called "Grid Attention". The structure shown in Fig. <ref type="figure" target="#fig_3">3</ref> is an application of Grid Attention. The input of the block is 3 feature maps respectively from frames 0, 1, and 2. (Here we use k = 3.) Before entering the W-MSA module for multi-head self-attention calculation, the feature maps from different frames are fused together.</p><p>Based on the feature map of the key frame (orange color), the feature maps of the previous (blue) and next (green) frames first do a dimensional reduction from [H, W, C] to [H, W, C/2]. Then they are down-sampled and a grid-like feature map are reserved. The grid-like feature map are then added to key-frame feature map, and the fusion progress finishes. In the W-MSA module, the self-attention within the local window and that between adjacent local windows are calculated, and the inter-frame information is fully used. The local window of key-frame has contained information from other frames, and self-attention calculation happens in inter-frames. The frame-level feature modeling can thus be achieved, simulating the way that doctors view IVOCT by combining information from previous and next frames. During feature fusion with Grid Attention, the feature maps from different frames are fused together in a grid-like pattern (as shown in the figure). The purpose of this is to ensure that when dividing windows, half of the grid cells within a window come from the current frame, and the other half come from other frames. If the number of channels in the feature map is C, and the number of frames being fused is 3 (current frame + previous frame + next frame), then the first C/2 channels will be fused between the current frame and the previous frame, and the last C/2 channels will be fused between the current frame and the next frame. Therefore, the final feature map consists of 1/4 of the previous frame, 1/2 of the current frame, and 1/4 of the next frame. The impact of the current frame on the new feature map remains the largest, as the current frame is the most critical frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Baseline Methods and Evaluation Metrics. The baseline is based on a PyTorch implementation of the open-source object detection toolbox MMDetection. We compare our proposed approach with Swin Transformer and four CNNbased network models including Faster-RCNN <ref type="bibr" target="#b14">[15]</ref>, YOLOv3 <ref type="bibr" target="#b15">[16]</ref>, YOLOv5 <ref type="bibr" target="#b16">[17]</ref>, Retinanet <ref type="bibr" target="#b17">[18]</ref>. All the baseline model is pre-trained on the ImageNet dataset.</p><p>To ensure objective comparison, all experiments were conducted in the MMdetection framework. The metric we used is the AP /AR for each lesion and the mAP , based on the COCO metric and the COCO API (the default evaluation method in the MMdetection framework). We trained the model for 60 epochs with an AdamW optimizer following Swin Transformer. The learning  rate and weight decay is set to be 1e-4 and 1e-2, respectively. The batch size is set to be 2.</p><p>Quantitative and Qualitative Results. All experiments are conducted on our newly-collected dataset. Each model is trained on the training set, selected based on the performance of the validation set, and the reported results are obtained on the test set. Table <ref type="table" target="#tab_0">1</ref> shows the comparison between the baseline methods and the G-Swin Transformer method. Methods based on Swin Transformer outperformed the four baseline methods in terms of precision and recall, and our proposed G-Swin Transformer outperforms the baseline method Swin Transformer by 2.15% in mAP. Figure <ref type="figure" target="#fig_4">4</ref> compares some results of our method and baselines. The first row is the detection of the macrophage. Our method's prediction is the most closed to the ground truth. The second row is the detection of cavities/dissections and thrombi. Only our method gets the right prediction. The YOLOv3, YOLOv5, Faster-RCNN and RetinaNet model failed to detect all lesions, while RetinaNet model even produced some false positive lesions.  Effect of Different Data Augmentation Methods. We compared the impact of different data augmentation strategies in our task. As shown in Table <ref type="table" target="#tab_1">2</ref>, Random Resize and Random Crop had a significant impact on performance improvement. Resize had the greatest impact on the model's performance because different-sized OCT images were generated after data augmentation, and the lesions were also enlarged or reduced proportionally. Since the sizes of lesions in different images are usually different, different-sized lesions produced through data augmentation are advantageous for the model to utilize multi-scale features for learning.</p><p>Effect of Different Hyper-parameters. Table <ref type="table" target="#tab_2">3</ref> shows the impact of hyperparameters on the performance of the G-Swin Transformer model. The best mAP was achieved when using a 3-layer image input. Using the upper and lower 5 layers of image input not only increased the training/inference time, but also may not provide more valuable information since frame 0 and frame 4 are too far away from the key frame. The fusion strategy indicates how the feature map from other frames are combined with the key-frame feature map. We can find add them up gets better result then simply replacement. We think this is because by this way, the 1 × 1 convolutional layer can learn a residual weights, keeps more detail of the key-frame.</p><p>Effect of Fusion Methods. In addition to Grid Attention, there are other methods of feature fusion. The first method is like 2.5D convolution, in which multiple frames of images are mapped into 96-dimensional feature maps directly through convolution in the Linear Embedding layer. This method is the simplest, but since the features are fused only once at the initial stage of the network, the use of adjacent frame features is very limited. The second method is to weight and sum the feature maps of different frames before each Attention Block, giving higher weight to the current frame and lower weight to the reference frames. Table <ref type="table" target="#tab_3">4</ref> shows the impact of other feature fusion methods on performance. Our method gets better mAP and AR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have presented the first multi-class lesion detection dataset of IVOCT scans. We have also proposed a Vision Transformer-based model, called G-Swin Transformer, which uses adjacent frames as input and leverages the temporary dimensional information inherent in IVOCT data. Our method outperforms traditional detection models in terms of accuracy. Clinical evaluation shows that our model's predictions provide significant value in assisting the diagnosis of acute coronary syndrome (ACS).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images and annotations of our dataset. Each IVOCT data is converted to PNG images for annotation. The blue/green/red boxes represent bounding box of macrophages, cavities/dissections, thrombi, respectively. (Color figure online)</figDesc><graphic coords="2,44,79,327,92,334,51,110,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>on the Swin Transformer, we propose a backbone called G-Swin Transformer. Our proposed G-Swin Transformer is used as the basic module of the encoder in the full model, which is developed based on Faster R-CNN. The overall structure of the model is shown in Fig. 2. The model input consists of k 3-channel RGB images, and the input dimension is [k * B, 3, H, W ], where k indicates the number of frames that used in an iteration. After passing through Patch Partition and Linear Embedding layers, k feature maps belonging to frame F 0 , F 1 , ... F k-1 , respectively, are obtained, each with a size of H/4 * W/4 * C. These feature maps are then input to the G-Swin Transformer, where they go through 4 layers and a total of 12 Transformer blocks. Between each layer, a patch merging layer is used to reduce resolution, and model features of different dimensions. The output feature maps at different scales are then passed to a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall model structure. The proposed G-Swin Transformer is used as backbone network. The detection head follows Faster-RCNN's head. W-MGSA and SW refer to Window-Multihead Grid Self Attention and Shifted Window, respectively.</figDesc><graphic coords="5,58,98,67,73,334,57,161,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed Grid Attention. The blue/orange/green feature map belongs to a local window of the previous/current/next frame. After the dimensional reduction and downsampling operation, the feature maps of previous/next frame is added to the current frame's feature map. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization results. From left to right are ground-truth, results of our model, Faster-RCNN, YOLOv3, YOLOv5 and RetinaNet. Our model achieves better results.</figDesc><graphic coords="7,58,98,215,75,334,57,110,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our proposed method and baseline methods.</figDesc><table><row><cell></cell><cell>AP50</cell><cell></cell><cell></cell><cell>mAP Recall50</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Macrophage cavities/</cell><cell>thrombus</cell><cell cols="2">Macrophage cavities/</cell><cell>thrombus</cell></row><row><cell></cell><cell></cell><cell>dissection</cell><cell></cell><cell></cell><cell>dissection</cell><cell></cell></row><row><cell>Faster-RCNN</cell><cell>27.34</cell><cell>44.32</cell><cell>31.86</cell><cell>34.51 74.65</cell><cell>76.04</cell><cell>70.60</cell></row><row><cell>YOLOv3</cell><cell>20.25</cell><cell>35.42</cell><cell>33.17</cell><cell>29.61 67.37</cell><cell>75.52</cell><cell>61.82</cell></row><row><cell>YOLOv5</cell><cell>27.34</cell><cell>40.63</cell><cell>46.93</cell><cell>38.30 79.31</cell><cell>82.67</cell><cell>83.86</cell></row><row><cell>RetinaNet</cell><cell>25.86</cell><cell>38.93</cell><cell>30.17</cell><cell>31.65 84.48</cell><cell>88.54</cell><cell>73.03</cell></row><row><cell>Swin-</cell><cell>27.91</cell><cell>44.94</cell><cell>48.87</cell><cell>40.57 89.11</cell><cell>89.06</cell><cell>92.85</cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G-Swin-</cell><cell>30.55</cell><cell>52.25</cell><cell>51.92</cell><cell>44.91 91.49</cell><cell>89.58</cell><cell>95.45</cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of using different data augmentation strategies.</figDesc><table><row><cell cols="2">Augmentation Strategy</cell><cell></cell><cell></cell><cell></cell><cell>Metric</cell></row><row><cell>Random</cell><cell>Random</cell><cell>Random</cell><cell>Random</cell><cell>Random</cell><cell>mAP AR</cell></row><row><cell>Resize</cell><cell>Crop</cell><cell>Flip</cell><cell>Bright-</cell><cell>Contrast</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ness</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.13 79.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.64 82.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.52 85.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.31 85.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.92 85.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.39 86.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.34 91.41</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.91 92.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effect of different hyper-parameters in Grid Attention.</figDesc><table><row><cell cols="3">Fusion Layers Fusion Strategy mAP AR</cell></row><row><cell>5</cell><cell>replace</cell><cell>41.69 89.28</cell></row><row><cell>5</cell><cell>add</cell><cell>40.31 90.52</cell></row><row><cell>3</cell><cell>replace</cell><cell>44.39 88.86</cell></row><row><cell>3</cell><cell>add</cell><cell>44.91 92.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different fusion strategies.</figDesc><table><row><cell cols="2">Fusion methods mAP AR</cell></row><row><cell>No fusion</cell><cell>40.57 90.34</cell></row><row><cell>2.5D</cell><cell>38.25 78.44</cell></row><row><cell cols="2">Weighted sum 40.64 83.54</cell></row><row><cell>Ours</cell><cell>44.91 92.18</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Key Research and Development Project of China</rs> (No. <rs type="grantNumber">2022ZD0117801</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xFbECHV">
					<idno type="grant-number">2022ZD0117801</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 32.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deaths: final data for</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tejada-Vera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heart disease and stroke statistics-2021 update: a report from the American heart association</title>
		<author>
			<persName><forename type="first">S</forename><surname>Virani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="254" to="e743" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optical coherence tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page" from="1178" to="1181" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intracoronary optical coherence tomography: a comprehensive review: clinical and research applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guagliumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rollins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACC: Cardiovas. Interv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In vivo diagnosis of plaque erosion and calcified nodule in patients with acute coronary syndrome by intravascular optical coherence tomography</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Coll. Cardiol</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1748" to="1758" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comprehensive assessment of coronary calcification in intravascular OCT using a spatial-temporal encoder-decoder network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="857" to="868" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coronary artery fibrous plaque detection based on multi-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Signal Process. Syst</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="325" to="333" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic plaque detection in IVOCT pullbacks using convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gessert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="426" to="434" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved U-net for plaque segmentation of intracoronary optical coherence tomography images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-86365-4_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-86365-448" />
	</analytic>
	<monogr>
		<title level="m">ICANN 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Farkaš</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Masulli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Otte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12893</biblScope>
			<biblScope unit="page" from="598" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The diagnostic value of intracoronary optical coherence tomography</title>
		<author>
			<persName><forename type="first">E</forename><surname>Regar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ligthart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bruining</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Herz: Kardiovaskulaere Erkraenkungen</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="417" to="429" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Plaque and thrombus evaluation by optical coherence tomography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ditzhuijzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bezerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Cardiovasc. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="289" to="298" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Update on acute coronary syndromes: the pathologists&apos; view</title>
		<author>
			<persName><forename type="first">E</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bentzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Virmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Heart J</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="719" to="728" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spontaneous coronary artery dissection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. J. Cardiol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1027" to="1033" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detection, segmentation, simulation and visualization of aortic dissections: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101773</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Yolov3: an incremental improvement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>ArXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">YOLOv5 by ultralytics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/yolov5" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference On Computer Vision</title>
		<meeting>the IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference On Computer Vision</title>
		<meeting>the IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
