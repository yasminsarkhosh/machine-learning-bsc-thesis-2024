<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Parmida</forename><surname>Ghahremani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<postCode>10065</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Marino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<postCode>10065</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><surname>Hernandez-Prera</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Moffitt Cancer Center</orgName>
								<address>
									<postCode>33612</postCode>
									<settlement>Tampa</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Janis</forename><forename type="middle">V</forename><surname>De la Iglesia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robbert</forename><forename type="middle">J C</forename><surname>Slebos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Moffitt Cancer Center</orgName>
								<address>
									<postCode>33612</postCode>
									<settlement>Tampa</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
							<email>christine.chung@moffitt.org</email>
							<affiliation key="aff1">
								<orgName type="department">Moffitt Cancer Center</orgName>
								<address>
									<postCode>33612</postCode>
									<settlement>Tampa</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saad</forename><surname>Nadeem</surname></persName>
							<email>nadeems@mskcc.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Memorial Sloan Kettering Cancer Center</orgName>
								<address>
									<postCode>10065</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="704" to="713"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1F86F24FC727B11217EF6D9F26174D39</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multiplex immuofluorescence</term>
					<term>multiplex immunohistochemistry</term>
					<term>tumor microenvironment</term>
					<term>virtual stain-to-stain translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new AI-ready computational pathology dataset containing restained and co-registered digitized images from eight head-and-neck squamous cell carcinoma patients. Specifically, the same tumor sections were stained with the expensive multiplex immunofluorescence (mIF) assay first and then restained with cheaper multiplex immunohistochemistry (mIHC). This is a first public dataset that demonstrates the equivalence of these two staining methods which in turn allows several use cases; due to the equivalence, our cheaper mIHC staining protocol can offset the need for expensive mIF staining/scanning which requires highly-skilled lab technicians. As opposed to subjective and error-prone immune cell annotations from individual pathologists (disagreement &gt; 50%) to drive SOTA deep learning approaches, this dataset provides objective immune and tumor cell annotations via mIF/mIHC restaining for more reproducible and accurate characterization of tumor immune microenvironment (e.g. for immunotherapy). We demonstrate the effectiveness of this dataset in three use cases: (1) IHC quantification of CD3/CD8 tumor-infiltrating lymphocytes via style transfer, (2) virtual translation of cheap mIHC stains to more expensive mIF stains, and (3) virtual tumor/immune cellular phenotyping on standard hematoxylin images. The dataset is available at https://github. com/nadeemlab/DeepLIIF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate spatial characterization of tumor immune microenvironment is critical for precise therapeutic stratification of cancer patients (e.g. via immunotherapy). Currently, this characterization is done manually by individual pathologists on standard hematoxylin-and-eosin (H&amp;E) or singleplex immunohistochemistry (IHC) stained images. However, this results in high interobserver variability among pathologists, primarily due to the large (&gt; 50%) disagreement among pathologists for immune cell phenotyping <ref type="bibr" target="#b9">[10]</ref>. This is also a big cause of concern for publicly available H&amp;E/IHC cell segmentation datasets with immune cell annotations from single pathologists. Multiplex staining resolves this issue by allowing different tumor and immune cell markers to be stained on the same tissue section, avoiding any phenotyping guesswork from pathologists. Multiplex staining can be performed using expensive multiplex immunofluorescence (mIF) or via cheaper multiplex immunohistochemistry (mIHC) assays. MIF staining (requiring expensive scanners and highly skilled lab technicians) allows multiple markers to be stained/expressed on the same tissue section (no co-registration needed) while also providing the utility to turn ON/OFF individual markers as needed. In contrast, current brightfield mIHC staining protocols relying on DAB (3,3'-Diaminobenzidine) alcohol-insoluble chromogen, even though easily implementable with current clinical staining protocols, suffer from occlusion of signal from sequential staining of additional markers. To this effect, we introduce a new brightfield mIHC staining protocol using alcoholsoluble aminoethyl carbazole (AEC) chromogen which allows repeated stripping, restaining, and scanning of the same tissue section with multiple markers. This requires only affine registration to align the digitized restained images to obtain non-occluded signal intensity profiles for all the markers, similar to mIF staining/scanning.</p><p>In this paper, we introduce a new dataset that can be readily used out-ofthe-box with any artificial intelligence (AI)/deep learning algorithms for spatial characterization of tumor immune microenvironment and several other use cases.</p><p>To date, only two denovo stained datasets have been released publicly: BCI H&amp;E and singleplex IHC HER2 dataset <ref type="bibr" target="#b6">[7]</ref> and DeepLIIF singleplex IHC Ki67 and mIF dataset <ref type="bibr" target="#b1">[2]</ref>, both without any immune or tumor markers. In contrast, we release the first denovo mIF/mIHC stained dataset with tumor and immune markers for more accurate characterization of tumor immune microenvironment. We also demonstrate several interesting use cases: (1) IHC quantification of CD3/CD8 tumor-infiltrating lymphocytes (TILs) via style transfer, (2) virtual translation of cheap mIHC stains to more expensive mIF stains, and (3) virtual tumor/immune cellular phenotyping on standard hematoxylin images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The complete staining protocols for this dataset are given in the accompanying supplementary material. Images were acquired at 20× magnification at Moffitt Cancer Center. The demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Region-of-Interest Selection and Image Registration</head><p>After scanning the full images at low resolution, nine regions of interest (ROIs) from each slide were chosen by an experienced pathologist on both mIF and mIHC images: three in the tumor core (TC), three at the tumor margin (TM), and three outside in the adjacent stroma (S) area. The size of the ROIs was standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total surface area of 0.343 mm 2 . Hematoxylin-stained ROIs were first used to align all the mIHC marker images in the open source Fiji software using affine registration. After that, hematoxylin-and DAPI-stained ROIs were used as references to align mIHC and mIF ROIs again using Fiji and subdivided into 512×512 patches, resulting in total of 268 co-registered mIHC and mIF patches (∼33 co-registered mIF/mIHC images per patient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concordance Study</head><p>We compared mIF and mIHC assays for concordance in marker intensities. The results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. This is the first direct comparison of mIF and mIHC using identical slides. It provides a standardized dataset to demonstrate the equivalence of the two methods and a source that can be used to calibrate other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Use Cases</head><p>In this section, we demonstrate some of the use cases enabled by this high-quality AI-ready dataset. We have used publicly available state-of-the-art tools such as Adaptive Attention Normalization (AdaAttN) <ref type="bibr" target="#b7">[8]</ref> for style transfer in the IHC CD3/CD8 quantification use case and DeepLIIF virtual stain translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> in the remaining two use cases.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IHC CD3/CD8 Scoring Using mIF Style Transfer</head><p>We generate a stylized IHC image (Fig. <ref type="figure" target="#fig_2">3</ref>) using three input images: (1) hematoxylin image (used for generating the underlying structure of cells in the stylized image), (2) its corresponding mIF CD3/CD8 marker image (used for staining positive cells as brown), and (3) sample IHC style image (used for transferring its style to the final image). The complete architecture diagram is given in the supplementary material. Specifically, the model consists of two sub-networks:</p><p>(a) Marker Generation: This sub-network is used for generating mIF marker data from the generated stylized image. We use a conditional generative adversarial network (cGAN) <ref type="bibr" target="#b3">[4]</ref> for generating the marker images. The cGAN network consists of a generator, responsible for generating mIF marker images given an IHC image, and a discriminator, responsible for distinguishing the output of the generator from ground truth data. We first extract the brown (DAB channel) from the given style IHC image, using stain deconvolution. Then, we use pairs of the style images and their extracted brown DAB marker images to train this sub-network. This sub-network improves staining of the positive cells in the final stylized image by comparing the extracted DAB marker image from the stylized image and the input mIF marker image at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Style Transfer:</head><p>This sub-network creates the stylized IHC image using an attention module, given (1) the input hematoxylin and the mIF marker images and (2) the style and its corresponding marker images. For synthetically generating stylized IHC images, we follow the approach outlined in AdaAttN <ref type="bibr" target="#b7">[8]</ref>. We use a pre-trained VGG-19 network <ref type="bibr" target="#b11">[12]</ref> as an encoder to extract multi-level feature maps and a decoder with a symmetric structure of VGG-19. We then use both shallow and deep level features by using AdaAttN modules on multiple layers of VGG. This sub-network is used to create a stylized image using the structure of the given hematoxylin image while transferring the overall color distribution of the style image to the final stylized image. The generated marker image from the first sub-network is used for a more accurate colorization of the positive cells against the blue hematoxylin counterstain/background; not defining loss functions based on the markers generated by the first sub-network leads to discrepancy in the final brown DAB channel synthesis.</p><p>For the stylized IHC images with ground truth CD3/CD8 marker images, we also segmented corresponding DAPI images using our interactive deep learning ImPartial <ref type="bibr" target="#b8">[9]</ref> tool https://github.com/nadeemlab/ImPartial and then classified the segmented masks using the corresponding CD3/CD8 channel intensities, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We extracted 268 tiles of size 512×512 from this final segmented and co-registered dataset. For the purpose of training and testing all the models, we extract four images of size 256 × 256 from each tile due to the size of the external IHC images, resulting in a total of 1072 images. We randomly extracted tiles from the LYON19 challenge dataset <ref type="bibr" target="#b13">[14]</ref> to use as style IHC images. Using these images, we created a dataset of synthetically generated IHC images from the hematoxylin and its marker image as shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells) <ref type="bibr" target="#b5">[6]</ref>. We randomly selected 840 and 230 patches of size 256 × 256 from the created dataset for training and validation, respectively. NuClick training and validation sets <ref type="bibr" target="#b5">[6]</ref> comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from LYON19 dataset <ref type="bibr" target="#b13">[14]</ref>. LYON19 IHC CD3/CD8 images are taken from breast, colon, and prostate cancer patients. We split their training set into training and validation sets, containing 553 and 118 images, respectively, and use their validation set for testing our trained models. We trained three models including UNet <ref type="bibr" target="#b10">[11]</ref>, FPN <ref type="bibr" target="#b4">[5]</ref>, UNet++ <ref type="bibr" target="#b14">[15]</ref> with the backbone of resnet50 for 200 epochs and early stopping on validation score with patience of 30 epochs, using binary cross entropy loss and Adam optimizer with learning rate of 0.0001. As shown in Table <ref type="table" target="#tab_1">2</ref>, models trained with our synthetic training set outperform those trained solely with NuClick data in all metrics.</p><p>We also tested the trained models on 1,500 randomly selected images from the training set of the Lymphocyte Assessment Hackathon (LYSTO) <ref type="bibr" target="#b0">[1]</ref>, containing image patches of size 299 × 299 obtained at a magnification of 40× from breast, prostate, and colon cancer whole slide images stained with CD3 and CD8 markers. Only the total number of lymphocytes in each image patch are reported in this dataset. To evaluate the performance of trained models on this dataset, we counted the total number of marked lymphocytes in a predicted mask and calculated the difference between the reported number of lymphocytes in each image with the total number of lymphocytes in the predicted mask by the model. In Table <ref type="table" target="#tab_1">2</ref>, the average difference value (DiffCount) of lymphocyte number for the whole dataset is reported for each model. As seen, the trained models on our dataset outperform the models trained solely on NuClick data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Virtual Translation of Cheap mIHC to Expensive mIF Stains</head><p>Unlike clinical DAB staining, as shown in style IHC images in Fig. <ref type="figure" target="#fig_2">3</ref>, where brown marker channel has a blue hematoxylin nuclear counterstain to stain for all the cells, our mIHC AEC-stained marker images (Fig. <ref type="figure" target="#fig_4">5</ref>) do not stain for all the cells including nuclei. In this use case, we show that mIHC marker images can be translated to higher quality mIF DAPI and marker images which stain effectively for all the cells. We used the publicly available DeepLIIF virtual translation module <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> for this task. We trained DeepLIIF on mIHC CD3 AECstained images to infer mIF DAPI and CD3 marker. Some examples of testing the trained model on CD3 images are shown in Fig. <ref type="figure" target="#fig_4">5</ref>. We calculated the Mean Squared Error (MSE) and Structural Similarity Index (SSIM) to evaluate the quality of the inferred modalities by the trained model. The MSE and SSIM for mIF DAPI was 0.0070 and 0.9991 and for mIF CD3 was 0.0021 and 0.9997, indicating high accuracy of mIF inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Virtual Cellular Phenotyping on Standard Hematoxylin Images</head><p>There are several public H&amp;E/IHC cell segmentation datasets with manual immune cell annotations from single pathologists. These are highly problematic given the large (&gt; 50%) disagreement among pathologists on immune cell phenotyping <ref type="bibr" target="#b9">[10]</ref>. In this last use case, we infer immune and tumor markers from the standard hematoxylin images using again the public DeepLIIF virtual translation module <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. We train the translation task of DeepLIIF model using the hematoxylin, immune (CD3) and tumor (PanCK) markers. Sample images/results taken from the testing dataset are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have released the first AI-ready restained and co-registered mIF and mIHC dataset for head-and-neck squamous cell carcinoma patients. This dataset can be used for virtual phenotyping given standard clinical hematoxylin images, virtual clinical IHC DAB generation with ground truth segmentations (to train highquality segmentation models across multiple cancer types) created from cleaner mIF images, as well as for generating standardized clean mIF images from neighboring H&amp;E and IHC sections for registration and 3D reconstruction of tissue specimens. In the future, we will release similar datasets for additional cancer types as well as release for this dataset corresponding whole-cell segmentations via ImPartial https://github.com/nadeemlab/ImPartial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data use Declaration and Acknowledgment:</head><p>This study is not Human Subjects Research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. This work was supported by MSK Cancer Center Support Grant/Core Grant (P30 CA008748) and by James and Esther King Biomedical Research Grant (7JK02) and Moffitt Merit Society Award to C. H. Chung. It is also supported in part by the Moffitt's Total Cancer Care Initiative, Collaborative Data Services, Biostatistics and Bioinformatics, and Tissue Core Facilities at the H. Lee Moffitt Cancer Center and Research Institute, an NCI-designated Comprehensive Cancer Center (P30-CA076292).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Dataset overview. Restained and co-registered mIHC and mIF sample with nuclear (hematoxylin/DAPI), immune (CD3 -T-cell marker, CD8 -Cytotoxic T-cell, FoxP3 -regulatory T-cell), and tumor (PanCK) markers. CD3 = CD8 + FoxP3.</figDesc><graphic coords="2,58,98,229,13,334,54,164,35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Concordance Study.Second column shows stains extracted from first column mIHC-AEC images using Otsu thresholding. Third column shows the corresponding perfectly co-registered original mIF images. Using the mIF image, we separated foreground of the mIHC-AEC image from its background and calculated the mean value of the foreground pixels as well as the background pixels. The fourth column shows the results of the concordance study. Each square represents an image in the dataset and the top half of each square shows the mean color value of the positive cells, extracted from mIHC-AEC using its corresponding mIF image and the bottom half of it shows the mean color value of its background.The high intensity of the top half of the squares represents positive cells and the low intensity of the bottom half represents non-positive cells (background), which is seen in almost all squares, demonstrating high concordance among mIHC-AEC and mIF data.The last column shows the RMSE and SSIM diagrams of all four stains calculated using the extracted stain from IHC-AEC images (second column) and the mIF images (third column). The low error rate of RMSE and high structural similarity seen in these diagrams show high concordance among mIHC-AEC and mIF images.</figDesc><graphic coords="4,59,97,190,76,332,68,234,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Examples of synthesized IHC images and corresponding input images. Style IHC images were taken from the public LYON19 challenge dataset<ref type="bibr" target="#b13">[14]</ref>. We used grayscale Hematoxylin images because they performed better with style transfer.</figDesc><graphic coords="5,44,79,215,78,334,57,176,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of Hematoxylin, mIF DAPI, mIF CD3 and classified segmentation mask for this marker. The DAPI images were segmented using Cellpose [13] and manually corrected by a trained technician and approved by a pathologist. The segmented masks were classified using the CD3 channel intensities.</figDesc><graphic coords="5,44,79,448,46,334,54,80,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of ground-truth and generated mIF data from mIHC-AEC images.</figDesc><graphic coords="7,44,79,376,61,334,48,197,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of ground-truth and generated mIF immune (CD3) and tumor (PanCK) markers from standard hematoxylin images.</figDesc><graphic coords="8,58,98,364,31,334,54,202,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Demographics and other relevant details of the eight anonymized head-andneck squamous cell carcinoma patients, including ECOG performance score, Pack-Year, and surgical pathology stage (AJCC8).</figDesc><table><row><cell>ID</cell><cell cols="6">Age Gender Race ECOG Smoking PY pStage Cancer Site Cancer Subsite</cell></row><row><cell cols="2">Case1 49 Male</cell><cell>White 3</cell><cell>Current</cell><cell>21 1</cell><cell cols="2">Oral Cavity Ventral Tongue</cell></row><row><cell cols="2">Case2 64 Male</cell><cell>White 3</cell><cell>Former</cell><cell>20 4</cell><cell>Larynx</cell><cell>Vocal Cord</cell></row><row><cell cols="2">Case3 60 Male</cell><cell>Black 2</cell><cell>Current</cell><cell>45 4</cell><cell>Larynx</cell><cell>False Vocal Cord</cell></row><row><cell cols="2">Case4 53 Male</cell><cell>White 1</cell><cell>Current</cell><cell>68 4</cell><cell>Larynx</cell><cell>Supraglottic</cell></row><row><cell cols="2">Case5 38 Male</cell><cell>White 0</cell><cell>Never</cell><cell>0 4</cell><cell cols="2">Oral Cavity Lateral Tongue</cell></row><row><cell cols="3">Case6 76 Female White 1</cell><cell>Former</cell><cell>30 2</cell><cell cols="2">Oral Cavity Lateral Tongue</cell></row><row><cell cols="2">Case7 73 Male</cell><cell>White 1</cell><cell cols="2">Former 100 3</cell><cell>Larynx</cell><cell>Glottis</cell></row><row><cell cols="2">Case8 56 Male</cell><cell>White 0</cell><cell>Never</cell><cell>0 2</cell><cell cols="2">Oral Cavity Tongue</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative metrics for NuClick and LYSTO testing sets. F1 is the harmonic mean of recall and precision, IOU is intersection over union, and pixel accuracy (PixAcc) is +F P +F N , where TP, FP, and FN represent the number of true positive, false positive, and false negative pixels, respectively.</figDesc><table><row><cell></cell><cell>T P</cell><cell></cell><cell></cell></row><row><cell>T P Model</cell><cell>Dataset</cell><cell></cell><cell>NuClick</cell><cell>LYSTO</cell></row><row><cell></cell><cell></cell><cell>F1↑</cell><cell>IOU↑</cell><cell>PixAcc↑</cell><cell>DiffCount↓</cell></row><row><cell>UNet [11]</cell><cell>NuClick</cell><cell cols="3">0.47 ± 0.30 0.36 ± 0.24 0.62 ± 0.37 10.06 ± 15.69</cell></row><row><cell></cell><cell cols="4">Our Dataset 0.48 ± 0.29 0.36 ± 0.25 0.69 ± 0.37 2.91 ± 5.47</cell></row><row><cell>FPN [5]</cell><cell>NuClick</cell><cell cols="3">0.50 ± 0.31 0.39 ± 0.26 0.64 ± 0.38 2.82 ± 3.49</cell></row><row><cell></cell><cell cols="4">Our Dataset 0.52 ± 0.31 0.40 ± 0.26 0.67 ± 0.36 1.90 ± 2.90</cell></row><row><cell cols="2">UNet++ [15] NuClick</cell><cell cols="3">0.49 ± 0.30 0.37 ± 0.25 0.63 ± 0.37 2.75 ± 5.29</cell></row><row><cell></cell><cell cols="4">Our Dataset 0.53 ± 0.30 0.41 ± 0.26 0.70 ± 0.36 2.19 ± 2.89</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 68.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lymphocyte assessment hackathon (LYSTO</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laak</surname></persName>
		</author>
		<ptr target="https://zenodo.org/record/3513571" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepliif: an online platform for quantification of clinical pathology slides</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21399" to="21405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A unified architecture for instance and semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<ptr target="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NuClick: a deep learning framework for interactive segmentation of microscopic images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahanifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Tajadin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101771</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BCI: Breast cancer immunohistochemical image generation through pyramid pix2pix</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11425</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AdaAttN: revisit attention mechanism in arbitrary neural style transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Impartial: partial annotations for cell instance segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2021" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prospective multi-institutional evaluation of pathologist assessment of pd-l1 assays for patient selection in triple negative breast cancer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Reisenbichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mod. Pathol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1746" to="1752" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cellpose: a generalist algorithm for cellular segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michaelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pachitariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect lymphocytes in immunohistochemistry with deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Swiderska-Chadaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101547</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
