<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement</title>
				<funder ref="#_xns2BEu">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manman</forename><surname>Fei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenrong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyun</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<email>lichizhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="652" to="661"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1A9E7F61BC0DA2CB3FAA2093050F4A21</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cervical abnormal cell detection</term>
					<term>Consistency learning</term>
					<term>Cervical cytologic images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated detection of cervical abnormal cells from Thinprep cytologic test (TCT) images is essential for efficient cervical abnormal screening by computer-aided diagnosis system. However, the detection performance is influenced by noise samples in the training dataset, mainly due to the subjective differences among cytologists in annotating the training samples. Besides, existing detection methods often neglect visual feature correlation information between cells, which can also be utilized to aid the detection model. In this paper, we propose a cervical abnormal cell detection method optimized by a novel distillation strategy based on local-scale consistency refinement. Firstly, we use a vanilla RetinaNet to detect top-K suspicious cells and extract region-of-interest (ROI) features. Then, a pre-trained Patch Correction Network (PCN) is leveraged to obtain local-scale features and conduct further refinement for these suspicious cell patches. We design a classification ranking loss to utilize refined scores for reducing the effects of the noisy label. Furthermore, the proposed ROI-correlation consistency loss is computed between extracted ROI features and local-scale features to exploit correlation information and optimize RetinaNet. Our experiments demonstrate that our distillation method can greatly optimize the performance of cervical abnormal cell detection without changing the detector's network structure in the inference. The code is publicly available at https:// github.com/feimanman/Cervical-Abnormal-Cell-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cervical cancer is the second most common cancer among adult women. If diagnosed early, it can be effectively treated and cured <ref type="bibr" target="#b19">[19]</ref>. Nevertheless, delayed diagnosis of cervical cancer until an advanced stage will have a negative impact on patient prognosis and consume medical resources. Currently, early screening of cervical cancer is recommended worldwide as an effective method to prevent and treat cervical cancer. Thin-prep cytologic test (TCT) is the most common and effective screening method for detecting cervical abnormal and premalignant cervical lesions <ref type="bibr" target="#b4">[5]</ref>. Conventionally it is performed by visually examining the stained cells collected through smearing on a glass slide, and generating a diagnosis report using the descriptive diagnosis method of the Bethesda system (TBS) <ref type="bibr" target="#b14">[15]</ref>. Although TCT has been widely used in clinical applications and has significantly reduced the mortality rates caused by cervical cancer, it is still unavailable for population-wide screening <ref type="bibr" target="#b18">[18]</ref>. This is partly due to its laborintensive, time-consuming, and high cost <ref type="bibr" target="#b0">[1]</ref>. Therefore, there is a high demand for automated cervical abnormality screening to facilitate efficient and accurate identification of cervical abnormalities.</p><p>With the development of deep learning <ref type="bibr" target="#b9">[10]</ref>, several attempts have been made to identify cervical abnormal cells using convolutional neural networks (CNNs). For example, Cao et al. <ref type="bibr" target="#b1">[2]</ref> developed an attention feature pyramid network (AttFPN) for automatic abnormal cervical cell detection in cervical cytopathological images to assist pathologists in making more accurate diagnoses. Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposed a new framework that decomposes tasks and compares cells for cervical lesion cell detection. Liang et al. <ref type="bibr" target="#b10">[11]</ref> proposed to explore contextual relationships to boost the performance of cervical abnormal cell detection. Lin et al. <ref type="bibr" target="#b22">[22]</ref> presented an automatic cervical cell detection approach based on the Dense-Cascade R-CNN. It is worth mentioning that all of the aforementioned detection methods inevitably produce false positive results, which should be further refined by pathologists for manual checking or classification models established for automatic screening. To solve this problem, Zhou et al. <ref type="bibr" target="#b23">[23]</ref> proposed a three-stage method including cell-level detection, image-level classification, and case-level diagnosis obtained by an SVM classifier. Zhu et al. <ref type="bibr" target="#b24">[24]</ref> developed an artificial intelligence assistive diagnostic solution, which integrated YOLOv3 <ref type="bibr" target="#b16">[16]</ref> for detection, Xception, and Patch-based models to boost classification.</p><p>Although the above-mentioned attempts can improve the screening performance significantly, there are several issues that need to be addressed: 1) Object detection methods often require accurate annotated data to guarantee performance with robustness and generalization. However, due to legal limitations, the scarcity of positive samples, and especially the subjectivity differences between cytopathologists for manual annotations <ref type="bibr" target="#b20">[20]</ref>, it is likely to generate noisy samples that affect the performance of the detection model. 2) Conventional object detection methods intend to directly extract the feature from the object area to locate and classify the object simultaneously. However, in clinical practice pathologists usually examine the target cells by comparing them to the surrounding cells to determine whether they are abnormal. Therefore, the visual feature correlations between the target cells and their surroundings can provide valuable information to aid the screening process, which also needs to be utilized when designing the cervical abnormal cell detection network.</p><p>To address these issues, we propose a novel method for cervical abnormal cell detection using distillation from local-scale consistency refinement. Inspired by knowledge distillation, we construct a pre-trained Patch Correction Network (PCN), which is designed to exploit the supervised information from the PCN to reduce the impact of noisy labels and utilize the contextual relationships between cells. In our approach, we begin by utilizing RetinaNet <ref type="bibr" target="#b11">[12]</ref> to locate suspicious cells and crop the top-K suspicious cells into patches. Then we feed them into the PCN to obtain classification scores and propose a ranking loss to refine the classifier of the detection network by correcting the score of the detection model. In addition, we propose an ROI-Correlation Consistency (RCC) loss between ROI features and local-scale features from the PCN, which encourages the detector to explore the feature correlations of the suspicious cells. Our proposed method achieves improved performance during inference without changing the detector structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The proposed framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which includes cervical abnormal cell detection and the PCN. Concerning the huge size of the Whole Slide Image (WSI) and the infeasibility to handle a WSI scan for detection, we crop the WSI into images with the size of 1024 × 1024 as input to the detection. Firstly, We choose RetinaNet as our cervical abnormal cell detection, which uses a Feature Pyramid Network (FPN) backbone and attaches two subnetworks to obtain bounding boxes and classification scores. We implement the detection to locate the suspicious lesion cervical cells and extract the top K patches from the original image. Besides, we add the ROI Align layer <ref type="bibr" target="#b17">[17]</ref> to the output of the FPN and generate ROI features. Then these patches are fed into the PCN to obtain refined scores and local-scale features. Subsequently, our ranking loss is employed to correct the score of the detection, followed by the RCC loss to capture the contextual relationships between the extracted cells for further optimizing the detection model. The distillation process involves leveraging the learned knowledge and expertise from the PCN to refine the detection results of RetinaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Patch Correction Network(PCN)</head><p>In Fig. <ref type="figure" target="#fig_0">1</ref>, the detection can automatically locate the suspicious cervical abnormal cells by providing their bounding boxes with the confidence scores. Due to the intrinsic architecture limitation of the detection and incomplete annotations, the confidence scores output by the RetinaNet may not be accurate, so we need another classification model to regrade the representative patches. Our framework leverages a local-scale classification refinement mechanism to guide the training of the detection model. We adopt SE-ResNext-50 <ref type="bibr" target="#b7">[8]</ref> as the PCN, which has demonstrated its effectiveness in this field. The PCN is employed to refine and enhance the RetinaNet proposal classifier, which is trained from a large number of patches collected in advance with more excellent classification performance.</p><p>More specifically, the input image is processed by the base detector F d (•) firstly to obtain the primary proposal information. The proposed PCN F c (•) takes the top-K patches as inputs, which are cropped from original images according to the proposal location, denoted as I p = Cr(I, p), where Cr(•) denotes the crop function, I and p denote input image and proposal boxes predicted by F d (•), respectively. Similar to the RetinaNet proposal classifier in F d (•), the PCN F c (•) outputs a classification distribution vector s c . Therefore, the proposed PCN F c (•) can be represented as:</p><formula xml:id="formula_0">s c = F c (I p ) = F c (Cr(I, p)).</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>The key idea is to augment the base detector F d (•) with the PCN F c (•) in parallel to enhance the proposal classification capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification Ranking Loss</head><p>Due to the inaccurate confidence scores output by RetinaNet, false positive cells are inevitable after detection. Hence, a good correction network is required to generate more precise scores. In this work, the suspicious ranking of the detected patches is updated by applying PCN to them. The detector is optimized by interscale pairwise ranking loss. Specifically, the ranking loss is given by:</p><formula xml:id="formula_2">L Rank (s d , s c ) = max {0, s c -s d + margin} , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where s c is the classification refinement score and s d is the detection score, which enforces s d &gt; s c + margin in training. We set margin = 0.05. Such a design can enable RetinaNet to take the prediction score as references, and utilize refined scores from PCN to obtain more confident predictions. The ranking loss optimizes the detection to generate higher confidence scores than the previous prediction, thereby suppressing false positives and enabling the detection network to better distinguish between positive and negative cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ROI-Correlation Consistency (RCC) Learning</head><p>In order to solve the problem of mismatched inputs to the detection and classification models, we add the ROI Align layer to the output of the FPN. However, for cervical abnormal cell detection, normal and abnormal cells may have very similar appearances, which might not be sufficient for conducting effective differentiation. In clinical practice, to determine whether a cervical cell is normal or abnormal, cytopathologists usually compare it to the surrounding reference cells. Therefore, we studied the correlation between the top K ROIs to help more accurate classification of abnormal cells.</p><p>Based on the consistency strategy <ref type="bibr" target="#b13">[14]</ref>, which enhances the consistency of the intrinsic relation among different models, we propose ROI-correlation consistency, which regularizes the network to maintain the consistency of the semantic relation between patches under ROI features and local-scale features, and thereby encourage the detector to explore the feature interaction between cells from the extracted patches to improve the network performance.</p><p>We model the structured relation among different patches with a case-level Gram Matrix <ref type="bibr" target="#b5">[6]</ref>. Given an input mini-batch with B samples, where B denotes the batch size. And each sample undergoes the ROI Align layer to obtain the top K ROIs, we denote the activation map of ROIs as F R ∈ R B×K×H×W ×C , where H and W are the spatial dimension of the feature map, and C is the channel number. We set K = 10, H = 7, W = 7, C = 256. We average pooling the feature map F R along the spatial dimension and reshape it into A R ∈ R BK×C , and then the Case-wise Gram Matrix G R ∈ R BK×BK is computed as:</p><formula xml:id="formula_4">G R = A R • (A R ) T ,<label>(3)</label></formula><p>where G ij is the inner product between the vectorized activation map A R i and A R j , whose intuitive meaning is the similarity between the activations of i th ROI and j th ROI within the input mini-batch. The final ROI relation matrix R R is obtained by conducting the L2 normalization for each row G R i of G R , which is expressed as:</p><formula xml:id="formula_5">R R = G R 1 G R 1 2 , • • • , G R BK G R BK 2 T . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>The proposed PCN F c (•) takes the B×K proposals of box regressor as inputs, we denote the local-scale feature map by PCN as F C ∈ R B×K×H ×W ×C , and set H = 56, W = 56. We perform average pooling on the feature map F C across the spatial dimension and then reshape it into A C ∈ R BK×HW C , the Case-wise Gram Matrix G C ∈ R BK×BK and the final relation matrix R C are computed as:</p><formula xml:id="formula_7">G C = A C • (A C ) T , (5) R C = G C 1 G C 1 2 , • • • , G C BK G C BK 2 T . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>The RCC requires the correlation matrix to be stable under ROI features and local-scale features to preserve the semantic relation between patches. We then define the proposed RCC loss as:</p><formula xml:id="formula_9">L RCC = 1 BK R C (X) -R R (X) 2 2 , (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where X is the proposals from the sampled mini-batch, R C (X) and R R (X) are the correlation matrices computed on X under different network. By minimizing L RCC during the training process, the network could be enhanced to capture the intrinsic relation between patches, thus helping to extract additional semantic information from cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>To better optimize the Retinanet detector in a reinforced way, we take the following training strategy, which consists of three major stages. In the first stage, we collect images with doctors' labels for training and initialized the detection net. In the second stage, we train PCN with cross-entropy loss until convergence.</p><p>In the last stage, we freeze the PCN and optimize the detector. The detector is optimized using the total objective function, which is written as follows:</p><formula xml:id="formula_11">L total = L cls + L reg + αL Rank + βL RRC ,<label>(8)</label></formula><p>where L cls and L reg are the ordinary detection loss for each detection head in RetinaNet. L cls is a Cross-Entropy loss for classification and L reg is a Smooth-L 1 loss for bounding box regression. L Rank is the classification ranking loss,L RRC is the RCC loss. α and β are hyper-parameters that denote the different weights of loss. During inference, only the optimized detector is used to output the final detection results without any additional modules.</p><p>3 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setup</head><p>Dataset. For cervical cell detection, our dataset includes 3761 images of 1024 × 1024 pixels cropped from WSIs. Our private dataset was collected and qualitycontrolled according to a standard protocol involving three pathologists: A, B, and C. Pathologist A had 33 years of experience in reading cervical cytology images, while pathologists B and C had 10 years of experience each. Initially, the images were randomly assigned to pathologist B or C for initial labeling. Later, the assigned pathologist's annotations were reviewed and verified by the other pathologist. Any discrepancies found were checked and re-labeled by pathologist A. These images were divided into the training set and the testing set according to the ratio of 9:1. We also collect a new dataset of 5000 positive and negative 224 × 224 cell patches to train the PCN.</p><p>Implementation Details. The backbone of the suspicious cell detection network is RetinaNet with ResNet-50 <ref type="bibr" target="#b6">[7]</ref>. The backbone of the pre-trained patch classification network is SE-ResNeXt-50. All parameters are optimized by Adam <ref type="bibr" target="#b8">[9]</ref> with an initial learning rate of 4 × 10 -5 . We set α to 0.25 and β to 1 during training. The model is implemented by PyTorch on 2 Nvidia Tesla P100 GPUs. We conduct a quantitative evaluation using two metrics: the COCO-style <ref type="bibr" target="#b12">[13]</ref> average precision (AP) and average recall (AR). We calculate the average AP over multiple IoU thresholds from 0.5 to 0.95 with a step size of 0.05, and individually evaluated AP at the IoU thresholds of 0.5 and 0.75 (denoted as AP.5 and AP.75), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Cervical Abnormal Cell Detection</head><p>Comparison with SOTA Methods. We compare the performance of our proposed method against known methods for cervical lesion detection as well as representative methods for object detection. Table <ref type="table" target="#tab_0">1</ref> presents the results, from which several observations can be drawn. (1) Among the models for object detection, Retinanet is generally superior to the other models. (2) Based on Retinanet, our method improves the detection performance significantly, especially AP.5 shows great performance improvement. This confirms the necessity and effectiveness of introducing the classification ranking and ROI-correlation consistency schemes for cervical lesion detection.</p><p>Ablation Study. We also perform an ablation study to further evaluate the contributions of each part in our method.   In addition, to further show the effectiveness of our method, we visualize the feature maps of Retinanet and the proposed method in Fig. <ref type="figure" target="#fig_0">1</ref>. Those feature maps are from the Conv3 stages of the class-subnet backbone. Specifically, we sum and average the features in the channel dimension, and upsample them to the original image size. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our method can really learn better feature representations for abnormal cells, with the help of our proposed classification ranking refinement and ROI-correlation consistency learning. By model learning, our method can gradually enhance the features of abnormal cell regions while repressing noise or other suspicious but non-lesion regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we integrate a distillation strategy that uses the knowledge learned from the pre-trained PCN to guide the training of the detection model to minimize the effects of noisy labels and explore the feature interaction between cells. Our method constructs RetinaNet with the PCN module which provides the refined scores and local-scale features of extracted patches. Specifically, we propose the ranking loss by utilizing refined scores to optimize the RetinaNet proposal classifier by reducing the impact of noisy labels. In addition, the ROI features generated by the detector and local-scale features from the PCN are used for correlation consistency learning, which explores the extracted cells' relationship. Our work can achieve better performance without adding new modules during inference. Experiments demonstrate the effectiveness and robustness of our method on the task of cervical abnormal cell detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of our proposed framework, where PCN provides refined scores and local-scale features. Ranking Loss(L Rank ) is proposed to optimize the RetinaNet proposal classifier on detection scores and refined scores. And we incorporate consistency learning between ROI features and Local-scale features by RCC Loss(LRCC ). Note that PCN is frozen and the cervical abnormal cell detection is updated by L Rank and LRCC during training.</figDesc><graphic coords="3,43,80,53,84,336,07,168,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Feature map visualization of RetinaNet and our method. (a) shows input images with ground-truth annotations. (b) shows feature maps from RetinaNet. (c) shows feature maps of our proposed method.</figDesc><graphic coords="8,64,98,54,17,323,02,225,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with state-of-the-art methods.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell cols="2">AP.5 AP.75 AR</cell></row><row><cell>Sparse R-CNN [21]</cell><cell cols="2">41.8 72.1 42.4</cell><cell>66.8</cell></row><row><cell>Deformable-DETR [25]</cell><cell cols="2">40.3 72.6 38.6</cell><cell>64.3</cell></row><row><cell>YoloV8 [4]</cell><cell cols="2">43.6 74.6 44.0</cell><cell>58.3</cell></row><row><cell>Faster R-CNN [17]</cell><cell cols="2">43.6 77.0 43.0</cell><cell>58.8</cell></row><row><cell cols="3">Cascade RRAM and GRAM [11] 44.6 77.5 47.7</cell><cell>60.0</cell></row><row><cell>RetinaNet [12]</cell><cell cols="2">45.7 81.3 46.2</cell><cell>58.8</cell></row><row><cell>Proposed method</cell><cell cols="2">51.1 86.6 54.3</cell><cell>62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note><p><p><p>reports the detailed ablation results, from which several observations can be drawn.</p><ref type="bibr" target="#b0">(1)</ref> </p>Compared with the baseline model, Retinanet, our classification ranking loss achieves considerably</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of ablation study for our local-scale consistency refinement.</figDesc><table><row><cell>Method</cell><cell cols="2">AP AP.5 AP.75 AR</cell></row><row><cell>Baseline</cell><cell>45.7 81.3 46.2</cell><cell>58.8</cell></row><row><cell>+Ranking Loss</cell><cell>47.8 83.2 49.0</cell><cell>59.7</cell></row><row><cell>+RCC Loss</cell><cell>47.4 82.7 46.1</cell><cell>59.2</cell></row><row><cell cols="3">+Ranking Loss and RCC Loss 51.1 86.6 54.3 62.5</cell></row><row><cell cols="3">better performance, especially in AP.75, with an improvement of 2.8. (2) The</cell></row><row><cell cols="3">RCC loss is also effective for learning better feature representations and dis-</cell></row><row><cell cols="3">tinguishing them well, and the AP is improved by 1.7. (3) With both ranking</cell></row><row><cell cols="3">loss and RCC loss, our method has the best performance, which surpasses the</cell></row><row><cell cols="3">baseline model by a large margin, validating the effectiveness of our method.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62001292</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xns2BEu">
					<idno type="grant-number">62001292</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Screening for cervical cancer using automated analysis of pap-smears</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bengtsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel attention-guided convolutional network for the detection of abnormal cervical cells in cervical cancer screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102197</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A task decomposing and cell comparing method for cervical lesion cell detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2432" to="2442" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<title level="m">Mmyolo: Openmmlab yolo series toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effect of study design and quality on unsatisfactory rates, cytology classifications, and accuracy in liquid-based versus conventional cervical cytology: a systematic review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Davey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">9505</biblScope>
			<biblScope unit="page" from="122" to="132" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring contextual relationships for cervical abnormal cell detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04693</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image classification with relation-driven self-ensembling model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3429" to="3440" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m">The Bethesda System for Reporting Cervical Cytology</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Nayar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wilbur</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-319-11074-5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11074-5" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: an incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">American society for colposcopy and cervical pathology, and American society for clinical pathology screening guidelines for the prevention and early detection of cervical cancer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saslow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Clin. Pathol</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="542" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>American cancer society</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human papillomavirus and cervical cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Castle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeronimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">9590</biblScope>
			<biblScope unit="page" from="890" to="907" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interobserver reproducibility of cervical cytologic and histologic interpretations: realistic estimates from the ascus-lsil triage study</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Stoler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schiffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1500" to="1505" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12450</idno>
		<title level="m">SparseR-CNN: end-to-end object detection with learnable proposals</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic detection of cervical cells using dense-cascade R-CNN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60639-8_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-60639-8_50" />
	</analytic>
	<monogr>
		<title level="m">PRCV 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12306</biblScope>
			<biblScope unit="page" from="602" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical pathology screening for cervical abnormality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">101892</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hybrid ai-assistive diagnostic model permits rapid tbs classification of cervical liquid-based thin-layer cell smears</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3541</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable detr: deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
