<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays</title>
				<funder>
					<orgName type="full">Helmut Horten Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ibrahim</forename><forename type="middle">Ethem</forename><surname>Hamamci</surname></persName>
							<email>ibrahim.hamamci@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sezgin</forename><surname>Er</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">International School of Medicine</orgName>
								<orgName type="institution">Istanbul Medipol University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enis</forename><surname>Simsar</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anjany</forename><surname>Sekuboyina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mustafa</forename><surname>Gundogar</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Endodontics</orgName>
								<orgName type="institution">Istanbul Medipol University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernd</forename><surname>Stadlinger</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Center of Dental Medicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Mehl</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Center of Dental Medicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bjoern</forename><surname>Menze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="389" to="399"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">887D52F573DF711C86AA0F3058447D96</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diffusion Network</term>
					<term>Hierarchical Learning</term>
					<term>Multi-Label Object Detection</term>
					<term>Panoramic Dental X-ray</term>
					<term>Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the necessity for precise treatment planning, the use of panoramic X-rays to identify different dental diseases has tremendously increased. Although numerous ML models have been developed for the interpretation of panoramic X-rays, there has not been an end-toend model developed that can identify problematic teeth with dental enumeration and associated diagnoses at the same time. To develop such a model, we structure the three distinct types of annotated data hierarchically following the FDI system, the first labeled with only quadrant, the second labeled with quadrant-enumeration, and the third fully labeled with quadrant-enumeration-diagnosis. To learn from all three hierarchies jointly, we introduce a novel diffusion-based hierarchical multi-label object detection framework by adapting a diffusion-based method that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Specifically, to take advantage of the hierarchically annotated data, our method utilizes a novel noisy box manipulation technique by adapting the denoising process in the diffusion network with the inference from the previously trained model in hierarchical order. We also utilize a multi-label object detection method to learn efficiently from partial annotations and to give all the needed information about each abnormal tooth for treatment planning. Experimental results show that our method significantly outperforms state-of-the-art object detection methods, including RetinaNet, Faster R-CNN, DETR, and DiffusionDet for the analysis of panoramic X-rays, demonstrating the great potential of our method for hierarchically and partially annotated datasets. The code and the datasets are available at https://github. com/ibrahimethemhamamci/HierarchicalDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of panoramic X-rays to diagnose numerous dental diseases has increased exponentially due to the demand for precise treatment planning <ref type="bibr" target="#b10">[11]</ref>. However, visual interpretation of panoramic X-rays may consume a significant amount of essential clinical time <ref type="bibr" target="#b1">[2]</ref> and interpreters may not always have dedicated training in reading scans as specialized radiologists have <ref type="bibr" target="#b12">[13]</ref>. Thus, the diagnostic process can be automatized and enhanced by getting the help of Machine Learning (ML) models. For instance, an ML model that automatically detects abnormal teeth with dental enumeration and associated diagnoses would provide a tremendous advantage for dentists in making decisions quickly and saving their time. Many ML models to interpret panoramic X-rays have been developed specifically for individual tasks such as quadrant segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">29]</ref>, tooth detection <ref type="bibr" target="#b5">[6]</ref>, dental enumeration <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>, diagnosis of some abnormalities <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">30]</ref>, as well as treatment planning <ref type="bibr" target="#b26">[27]</ref>. Although many of these studies have achieved good results, three main issues still remain. (1) Multi-label detection: there has not been an end-to-end model developed that gives all the necessary information for treatment planning by detecting abnormal teeth with dental enumeration and multiple diagnoses simultaneously <ref type="bibr" target="#b0">[1]</ref>. (2) Data availability: to train a model that performs this task with high accuracy, a large set of fully annotated data is needed <ref type="bibr" target="#b12">[13]</ref>. Because labeling every tooth with all required classes may require expertise and take a long time, such kind of fully labeled large datasets do not always exist <ref type="bibr" target="#b23">[24]</ref>. For instance, we structure three different available annotated data hierarchically shown in Fig. <ref type="figure" target="#fig_0">1</ref>, using the Fédération Dentaire Internationale (FDI) system. The first data is partially labeled because it only included quadrant information. The second data is also partially labeled but contains additional enumeration information along with the quadrant. The third data is fully labeled because it includes all quadrant-enumeration-diagnosis information for each abnormal tooth. Thus, conventional object detection algorithms would not be well applicable to this kind of hierarchically and partially annotated data <ref type="bibr" target="#b20">[21]</ref>. (3) Model performance: to the best of our knowledge, models designed to detect multiple diagnoses on panoramic X-rays have not achieved the same high level of accuracy as those specifically designed for individual tasks, such as tooth detection, dental enumeration, or detecting single abnormalities <ref type="bibr" target="#b17">[18]</ref>.</p><p>To circumvent the limitations of the existing methods, we propose a novel diffusion-based hierarchical multi-label object detection method to point out each abnormal tooth with dental enumeration and associated diagnosis concurrently on panoramic X-rays, see Fig. <ref type="figure" target="#fig_1">2</ref>. Due to the partial annotated and hierarchical characteristics of our data, we adapt a diffusion-based method <ref type="bibr" target="#b4">[5]</ref> that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Compared to the previous object detection methods that utilize conventional weight transfer <ref type="bibr" target="#b2">[3]</ref> or cropping strategies <ref type="bibr" target="#b21">[22]</ref> for hierarchical learning, the denoising process enables us to propose a novel hierarchical diffusion network by utilizing the inference from the previously trained model in hierarchical order to manipulate the noisy bounding boxes as in Fig. <ref type="figure" target="#fig_1">2</ref>. Besides, instead of pseudo labeling techniques <ref type="bibr" target="#b27">[28]</ref> for partially annotated data, we develop a multi-label object detection method to learn efficiently from partial annotations and to give all the needed information about each abnormal tooth for treatment planning. Finally, we demonstrate the effectiveness of our multi-label detection method on partially annotated data and the efficacy of our proposed bounding box manipulation technique in diffusion networks for hierarchical data.</p><p>The contributions of our work are three-fold. <ref type="bibr" target="#b0">(1)</ref> We propose a multi-label detector to learn efficiently from partial annotations and to detect the abnormal tooth with all three necessary classes, as shown in Fig <ref type="figure" target="#fig_2">3</ref> for treatment planning. <ref type="bibr" target="#b1">(2)</ref> We rely on the denoising process of diffusion models <ref type="bibr" target="#b4">[5]</ref> and frame the detection problem as a hierarchical learning task by proposing a novel bounding box manipulation technique that outperforms conventional weight transfer as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. (3) Experimental results show that our model with bounding box manipulation and multi-label detection significantly outperforms state-of-the-art object detection methods on panoramic X-ray analysis, as shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates our proposed framework. We utilize the DiffusionDet <ref type="bibr" target="#b4">[5]</ref> model, which formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Unlike other state-of-the-art detection models, the denoising property of the model enables us to propose a novel manipulation technique to utilize a hierarchical learning architecture by using previously inferred boxes. Besides, to learn efficiently from partial annotations, we design a multilabel detector with adaptable classification layers based on available labels. In addition, we designed our approach to serve as a foundational baseline for the Dental Enumeration and Diagnosis on Panoramic X-rays Challenge (DENTEX), set to take place at MICCAI 2023. Remarkably, the data and annotations we utilized for our method mirror exactly those employed for DENTEX <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Base Model</head><p>Our method employs the DiffusionDet <ref type="bibr" target="#b4">[5]</ref> that comprises two essential components, an image encoder that extracts high-level features from the raw image and a detection decoder that refines the box predictions from the noisy boxes using those features. The set of initial noisy bounding boxes is defined as:</p><formula xml:id="formula_0">q(z t |z 0 ) = N (z t | √ ᾱt z 0 , (1 -ᾱt )I)<label>(1)</label></formula><p>where z 0 represents the input bounding box b, and b ∈ R N ×4 is a set of bounding boxes, z t represents the latent noisy boxes, and ᾱt represents the noise variance schedule. The DiffusionDet model <ref type="bibr" target="#b4">[5]</ref> f θ (z t , t, x), is trained to predict the final bounding boxes defined as b i = (c i x , c i y , w i , h i ) where (c i x , c i y ) are the center coordinates of the bounding box and (w i , h i ) are the width and height of the bounding boxes and category labels defined as y i for objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Framework</head><p>To improve computational efficiency during the denoising process, Diffusion-Det <ref type="bibr" target="#b4">[5]</ref> is divided into two parts: an image encoder and a detection decoder. Iterative denoising is applied only for the detection decoder, using the outputs of the image encoder as a condition. Our method employs this approach with several adjustments, including multi-label detection and bounding box manipulation. Finally, we utilize conventional transfer learning for comparison.</p><p>Image Encoder. Our method utilizes a Swin-transformer <ref type="bibr" target="#b16">[17]</ref> backbone pretrained on the ImageNet-22k <ref type="bibr" target="#b6">[7]</ref> with a Feature Pyramid Network (FPN) architecture <ref type="bibr" target="#b14">[15]</ref> as it was shown to outperform convolutional neural network-based models such as ResNet50 <ref type="bibr" target="#b9">[10]</ref>. We also apply pre-training to the image encoder using our unlabeled data, as it is not trained during the training process. We utilize SimMIM <ref type="bibr" target="#b25">[26]</ref> that uses masked image modeling to finetune the encoder. Detection Decoder. Our method employs a detection decoder that inputs noisy initial boxes to extract Region of Interest (RoI) features from the encodergenerated feature map and predicts box coordinates and classifications using a detection head. However, our detection decoder has several differences from DiffusionDet <ref type="bibr" target="#b4">[5]</ref>. Our proposed detection decoder (1) has three classification heads instead of one, which allows us to train the same model with partially annotated data by freezing the heads according to the unlabeled classes, (2) employs manipulated bounding boxes to extract RoI features, and (3) leverages transfer learning from previous training steps.</p><p>Multi-label Detection. We utilize three classification heads as quadrantenumeration-diagnosis for each bounding box and freeze the heads for the unlabeled classes, shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Our model denoted by f θ is trained to predict:</p><formula xml:id="formula_1">f θ (z t , t, x, h q , h e , h d ) = ⎧ ⎨ ⎩ (y i q , b i ), h q = 1, h e = 0, h d = 0 (a) (y i q , y i e , b i ), h q = 1, h e = 1, h d = 0 (b) (y i q , y i e , y i d , b i ), h q = 1, h e = 1, h d = 1 (c)<label>(2)</label></formula><p>where y i q , y i e , and y i d represent the bounding box classifications for quadrant, enumeration, and diagnosis, respectively, and h q , h e , and h d represent binary indicators of whether the labels are present in the training dataset. By adapting this approach, we leverage the full range of available information and improve our ability to handle partially labeled data. This stands in contrast to conventional object detection methods, which rely on a single classification head for each bounding box <ref type="bibr" target="#b24">[25]</ref> and may not capture the full complexity of the underlying data. Besides, this approach enables the model to detect abnormal teeth with all three necessary classes for clinicians to plan the treatment, as seen in Fig. <ref type="figure" target="#fig_2">3</ref>. Bounding Box Manipulation. Instead of completely noisy boxes, we use manipulated bounding boxes to extract RoI features from the encoder-generated feature map and to learn efficiently from hierarchical annotations as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, to train the model (b) in Eq. ( <ref type="formula" target="#formula_1">2</ref>), we concatenate the noisy boxes described in Eq. ( <ref type="formula" target="#formula_0">1</ref>) with the boxes inferred from the model (a) in Eq. ( <ref type="formula" target="#formula_1">2</ref>) with a score greater than 0.5. Similarly, we manipulate the denoising process during the training of the model (c) in Eq. ( <ref type="formula" target="#formula_1">2</ref>) by concatenating the noisy boxes with boxes inferred from the model (b) in Eq. ( <ref type="formula" target="#formula_1">2</ref>) with a score greater than 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluate models' performances using a combination of Average Recall (AR) and Average Precision (AP) scores with various Intersection over Union (IoU) thresholds. This included AP [0.5,0.95] , AP 50 , AP 75 , and separate AP scores for large objects (AP l ), and medium objects (AP m ).</p><p>Data. All panoramic X-rays were acquired from patients above 12 years of age using the VistaPano S X-ray unit (Durr Dental, Germany). To ensure patient privacy and confidentiality, panoramic X-rays were randomly selected from the hospital's database without considering any personal information.</p><p>To effectively utilize FDI system <ref type="bibr" target="#b7">[8]</ref>, three distinct types of data are organized hierarchically as in Fig. <ref type="figure" target="#fig_0">1</ref> (a) 693 X-rays labeled only for quadrant detection, (b) 634 X-rays labeled for tooth detection with both quadrant and tooth enumeration classifications, and (c) 1005 X-rays fully labeled for diseased tooth detection with quadrant, tooth enumeration, and diagnosis classifications. In the diagnosis, there are four specific classes corresponding to four different diagnoses: caries, deep caries, periapical lesions, and impacted teeth. The remaining 1571 unlabeled X-rays are used for pre-training. All necessary permissions were obtained from the ethics committee.</p><p>Experimental Design. To evaluate our proposed method, we conduct two experiments: (1) Comparison with state-of-the-art object detection models, including DETR <ref type="bibr" target="#b3">[4]</ref>, Faster R-CNN <ref type="bibr" target="#b19">[20]</ref>, RetinaNet <ref type="bibr" target="#b15">[16]</ref>, and DiffusionDet <ref type="bibr" target="#b4">[5]</ref> in Table <ref type="table" target="#tab_0">1</ref>. (2) A comprehensive ablation study to assess the effect of our modifications to DiffusionDet in hierarchical detection performance in Fig. <ref type="figure" target="#fig_4">4</ref>.</p><p>Evaluation. Fig. <ref type="figure" target="#fig_2">3</ref> presents the output prediction of the final trained model. As depicted in the figure, the model effectively assigns three distinct classes to each well-defined bounding box. Our approach that utilizes novel box manipulation and multi-label detection, significantly outperforms state-of-the-art methods. The box manipulation approach specifically leads to significantly higher AP and AR scores compared to other state-of-the-art methods, including RetinaNet, Faster-R-CNN, DETR, and DiffusionDet. Although the impact of conventional transfer learning on these scores can vary depending on the data, our bounding box manipulation outperforms it. Specifically, the bounding box manipulation approach is the sole factor that improves the accuracy of the model, while weight transfer does not improve the overall accuracy, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>.  <ref type="bibr" target="#b19">[20]</ref> 0.588 29.5 48.6 33.0 39.9 29.5 DETR <ref type="bibr" target="#b3">[4]</ref> 0.659 39.1 60.5 47.6 55.0 39.1 Base (DiffusionDet) <ref type="bibr" target="#b4">[5]</ref> 0 Ablation Study. Our ablation study results, shown in Fig. <ref type="figure" target="#fig_4">4</ref> and Table <ref type="table" target="#tab_0">1</ref>, indicate that our approaches have a synergistic impact on the detection model's accuracy, with the highest increase seen through bounding box manipulation. We systematically remove every combination of bounding box manipulation and weight transfer, to demonstrate the efficacy of our methodology. Conventional transfer learning does not positively affect the models' performances compared to the bounding box manipulation, especially for enumeration and diagnosis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we introduce a novel diffusion-based multi-label object detection framework to overcome one of the significant obstacles to the clinical application of ML models for medical and dental diagnosis, which is the difficulty in getting a large volume of fully labeled data. Specifically, we propose a novel bounding box manipulation technique during the denoising process of the diffusion networks with the inference from the previously trained model to take advantage of hierarchical data. Moreover, we utilize a multi-label detector to learn efficiently from partial annotations and to assign all necessary classes to each box for treatment planning. Our framework outperforms state-of-the-art object detection models for training with hierarchical and partially annotated panoramic X-ray data.</p><p>From the clinical perspective, we develop a novel framework that simultaneously points out abnormal teeth with dental enumeration and associated diagnosis on panoramic dental X-rays with the help of our novel diffusion-based hierarchical multi-label object detection method. With some limits due to partially annotated and limited amount of data, our model that provides three necessary classes for treatment planning has a wide range of applications in the real world, from being a clinical decision support system to being a guide for dentistry students.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The annotated datasets are organized hierarchically as (a) quadrant-only, (b) quadrant-enumeration, and (c) quadrant-enumeration-diagnosis respectively.</figDesc><graphic coords="2,44,79,251,87,334,57,126,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Our method relies on a hierarchical learning approach utilizing a combination of multi-label detection, bounding box manipulation, and weight transfer.</figDesc><graphic coords="4,54,81,56,18,314,62,508,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Output from our final model showing well-defined boxes for diseased teeth with corresponding quadrant (Q), enumeration (N), and diagnosis (D) labels.</figDesc><graphic coords="6,44,79,286,82,334,57,154,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 .</head><label>5</label><figDesc>The set of manipulated boxes b m , and b m ∈ R N ×4 , can be defined as b m = [b n [: -k], b i ], where b n , and b n ∈ R N ×4 , represents the set of noisy boxes and, b i , and b i ∈ R k×4 , represents the set of inferred boxes from the previous training. Our framework utilizes completely noisy boxes during the inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The results of the ablation study reveals that our bounding box manipulation method outperforms conventional weight transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our method outperforms state-of-the-art methods, and our bounding box manipulation approach outperforms the weight transfer. Results shown here indicate the different tasks in the test set which is multi-labeled (quadrant-enumerationdiagnosis) for abnormal tooth detection.</figDesc><table><row><cell>Method</cell><cell>..... AR</cell><cell>AP AP50 AP75 APm APl</cell></row><row><cell></cell><cell>Quadrant</cell><cell></cell></row><row><cell>RetinaNet[16]</cell><cell cols="2">0.604 25.1 41.7 28.8 32.9 25.1</cell></row><row><cell>Faster R-CNN</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.4 61.3 47.9 49.7 39.5</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>.677 38.8 60.7 46.1 39.1 39.0</cell></row><row><cell>Ours w/o Transfer</cell><cell>0.699 42.7 64.7 52.4 50.5 42.8</cell></row><row><cell>Ours w/o Manipulation</cell><cell>0.727 40.0 60.7 48.2 59.3 40.0</cell></row><row><cell>Ours w/o Manipulation and Transfer</cell><cell>0.658 38.1 60.1 45.3 45.1 38.1</cell></row><row><cell>Ours (Manipulation+Transfer+Multilabel)</cell><cell>0.717 43.2 65.1 51.0 68.3 43.1</cell></row><row><cell cols="2">Enumeration</cell></row><row><cell>RetinaNet[16]</cell><cell>0.560 25.4 41.5 28.5 55.1 25.2</cell></row><row><cell>Faster R-CNN[20]</cell><cell>0.496 25.6 43.7 27.0 53.3 25.2</cell></row><row><cell>DETR[4]</cell><cell>0.440 23.1 37.3 26.6 43.4 23.0</cell></row><row><cell>Base (DiffusionDet)[5]</cell><cell>0.617 29.9 47.4 34.2 48.6 29.7</cell></row><row><cell>Ours w/o Transfer</cell><cell>0.648 32.8 49.4 39.4 60.1 32.9</cell></row><row><cell>Ours w/o Manipulation</cell><cell>0.662 30.4 46.5 36.6 58.4 30.5</cell></row><row><cell>Ours w/o Manipulation and Transfer</cell><cell>0.557 26.8 42.4 29.5 51.4 26.5</cell></row><row><cell>Ours (Manipulation+Transfer+Multilabel)</cell><cell>0.668 30.5 47.6 37.1 51.8 30.4</cell></row><row><cell>Diagnosis</cell><cell></cell></row><row><cell>RetinaNet[16]</cell><cell>0.587 32.5 54.2 35.6 41.7 32.5</cell></row><row><cell>Faster R-CNN[20]</cell><cell>0.533 33.2 54.3 38.0 24.2 33.3</cell></row><row><cell>DETR[4]</cell><cell>0.514 33.4 52.8 41.7 48.3 33.4</cell></row><row><cell>Base (DiffusionDet)[5]</cell><cell>0.644 37.0 58.1 42.6 31.8 37.2</cell></row><row><cell cols="2">Ours w/o Transfer 0.669 39Ours w/o Manipulation 0.688 36.3 55.5 43.1 45.6 37.4</cell></row><row><cell>Ours w/o Manipulation and Transfer</cell><cell>0.648 37.3 59.5 42.8 33.6 36.4</cell></row><row><cell>Ours (Manipulation+Transfer+Multilabel)</cell><cell>0.691 37.6 60.2 44.0 36.0 37.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank the <rs type="funder">Helmut Horten Foundation</rs> for supporting our research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 38.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of deep learning techniques for dental informatics: a systematic literature review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abusalim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Abdulkadir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Healthcare (Basel)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1892</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and confronting our mistakes: the epidemiology of error in radiology and strategies for error reduction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Abujudeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1668" to="1676" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GAIA: a transfer learning system of object detection that fits your needs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-End object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09788</idno>
		<title level="m">DiffusionDet: diffusion model for object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Individual tooth detection and identification from dental panoramic X-ray images via point-wise localization and distance regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">101996</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FDI vision 2020: shaping the future of oral health</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Dent. J</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">278</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DENTEX: an abnormal tooth detection with dental enumeration and diagnosis benchmark for panoramic X-rays</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Hamamci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19112</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of deep learning in the field of dentistry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imaging Sci. Dent</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for the radiographic detection of periodontal bone loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8495</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Descriptive analysis of dental X-ray images using various practical methods: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Bhadauria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">620</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tooth numbering and condition recognition on dental panoramic radiograph images using CNNs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="166008" to="166026" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tufts dental database: a multimodal panoramic X-ray dataset for benchmarking diagnostic systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1650" to="1659" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">GaNDLF: a generally nuanced deep learning framework for scalable end-to-end clinical workflows in medical imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01006</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label object detection framework for remote sensing images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">2734</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label object detection framework for remote sensing images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">2734</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tooth detection and numbering in panoramic radiographs using convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Tuzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dentomaxillofacial Radiol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">20180051</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Preparing medical imaging data for machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Willemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Detectron</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<title level="m">SimMIM: a simple framework for masked image modeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dental enumeration and multiple treatment detection on panoramic X-rays using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Yüksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection with a unified label space from multiple datasets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page" from="178" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58568-6_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58568-611" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TsasNet: tooth segmentation on dental panoramic X-ray images by two-stage attention segmentation network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">106338</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CariesNet: a deep learning approach for segmentation of multi-stage caries lesion from oral panoramic Xray image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-021-06684-2</idno>
		<idno>1007/s00521-021-06684-2</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="16051" to="16059" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
