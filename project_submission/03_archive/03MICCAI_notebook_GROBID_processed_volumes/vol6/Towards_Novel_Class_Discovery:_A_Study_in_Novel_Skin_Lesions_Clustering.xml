<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI Group</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Airdoc LLC</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">https://www.monash</orgName>
								<address>
									<settlement>edu/mmai-group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lie</forename><surname>Ju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI Group</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Airdoc LLC</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">https://www.monash</orgName>
								<address>
									<settlement>edu/mmai-group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI Group</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Airdoc LLC</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">https://www.monash</orgName>
								<address>
									<settlement>edu/mmai-group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaimin</forename><surname>Song</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Airdoc LLC</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">https://www.monash</orgName>
								<address>
									<settlement>edu/mmai-group</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<email>zongyuan.ge@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI Group</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">AIM for Health Lab</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="24" to="33"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B1B111E7CF5A14E8EAB0374BF99563CB</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Novel Class Discovery</term>
					<term>Skin Lesion Recognition</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing deep learning models have achieved promising performance in recognizing skin diseases from dermoscopic images. However, these models can only recognize samples from predefined categories, when they are deployed in the clinic, data from new unknown categories are constantly emerging. Therefore, it is crucial to automatically discover and identify new semantic categories from new data. In this paper, we propose a new novel class discovery framework for automatically discovering new semantic classes from dermoscopy image datasets based on the knowledge of known classes. Specifically, we first use contrastive learning to learn a robust and unbiased feature representation based on all data from known and unknown categories. We then propose an uncertainty-aware multiview cross pseudo-supervision strategy, which is trained jointly on all categories of data using pseudo labels generated by a self-labeling strategy. Finally, we further refine the pseudo label by aggregating neighborhood information through local sample similarity to improve the clustering performance of the model for unknown categories. We conducted extensive experiments on the dermatology dataset ISIC 2019, and the experimental results show that our approach can effectively leverage knowledge from known categories to discover new semantic categories. We also further validated the effectiveness of the different modules through extensive ablation experiments. Our code will be released soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic identification of lesions from dermoscopic images is of great importance for the diagnosis of skin cancer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. Currently, deep learning mod-els, especially those based on deep convolution neural networks, have achieved remarkable success in this task <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. However, this comes at the cost of a large amount of labeled data that needs to be collected for each class. To alleviate the labeling burden, semi-supervised learning has been proposed to exploit a large amount of unlabeled data to improve performance in the case of limited labeled data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. However, it still requires a small amount of labeled data for each class, which is often impossible in real practice. For example, there are roughly more than 2000 named dermatological diseases today, of which more than 200 are common, and new dermatological diseases are still emerging, making it impractical to annotate data from scratch for each new disease category <ref type="bibr" target="#b19">[20]</ref>. However, since there is a correlation between new and known diseases, a priori knowledge from known diseases is expected to help automatically identify new diseases <ref type="bibr" target="#b8">[9]</ref>.</p><p>One approach to address the above problem is novel class discovery (NCD) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>, which aims to transfer knowledge from known classes to discover new semantic classes. Most NCD methods follow a two-stage scheme: 1) a stage of fully supervised training on known category data and 2) a stage of clustering on unknown categories <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. For example, Han et al. <ref type="bibr" target="#b8">[9]</ref> further introduced self-supervised learning in the first stage to learn general feature representations. They also used ranking statistics to compute pairwise similarity for clustering. Zhong et al. <ref type="bibr" target="#b23">[24]</ref> proposed OpenMix based on the mixup strategy <ref type="bibr" target="#b20">[21]</ref> to further exploit the information from known classes to improve the performance of unsupervised clustering. Fini et al. <ref type="bibr" target="#b6">[7]</ref> proposed UNO, which unifies multiple objective functions into a holistic framework to achieve better interaction of information between known and unknown classes. Zhong et al. <ref type="bibr" target="#b22">[23]</ref> used neighborhood information in the embedding space to learn more discriminative representations. However, most of these methods require the construction of a pairwise similarity prediction task to perform clustering based on pairwise similarity pseudo labels between samples. In this process, the generated pseudo labels are usually noisy, which may affect the clustering process and cause error accumulation. In addition, they only consider the global alignment of samples to the category center, ignoring the local inter-sample alignment thus leading to poor clustering performance.</p><p>In this paper, we propose a new novel class discovery framework to automatically discover novel disease categories. Specifically, we first use contrastive learning to pretrain the model based on all data from known and unknown categories to learn a robust and general semantic feature representation. Then, we propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to perform clustering. It first uses a self-labeling strategy to generate pseudo-labels for unknown categories, which can be treated homogeneously with ground truth labels. The cross-pseudo-supervision strategy is then used to force the model to maintain consistent prediction outputs for different views of unlabeled images.</p><p>In addition, we propose to use prediction uncertainty to adaptively adjust the contribution of the pseudo labels to mitigate the effects of noisy pseudo labels. Finally, to encourage local neighborhood alignment and further refine the pseudo labels, we propose a local information aggregation module to aggregate the information of the neighborhood samples to boost the clustering performance. We conducted extensive experiments on the dermoscopy dataset ISIC 2019, and the experimental results show that our method outperforms other state-of-the-art comparison algorithms by a large margin. In addition, we also validated the effectiveness of different components through extensive ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Given an unlabeled dataset {x u i } N u i=1 with N u images, where x u i is the ith unlabeled image. Our goal is to automatically cluster the unlabeled data into C u clusters. In addition, we also have access to a labeled dataset {x l i , y l i } N l i=1 with N l images, where x l i is the ith labeled image and y l i ∈ Y = 1, . . . , C l is its corresponding label. In the novel class discovery task, the known and unknown classes are disjoint, i.e., C l ∩ C u = ∅. However, the known and unknown classes are similar, and we aim to use the knowledge of the known classes to help the clustering of the unknown classes. The overall framework of our proposed novel class discovery algorithm is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, we first learn general and robust feature representations through contrastive learning. Then, the uncertainty-aware multi-view cross-pseudo-supervision strategy is used for joint training on all category data. Finally, the local information aggregation module benefits the NCD by aggregating the useful information of the neighborhood samples.</p><p>Contrastive Learning. To achieve a robust feature representation for the NCD task, we first use noise contrastive learning <ref type="bibr" target="#b7">[8]</ref> to pretrain the feature extractor network, which effectively avoids model over-fitting to known categories. Specifically, we use x i and x i to represent different augmented versions of the same image in a mini-batch. The unsupervised contrastive loss can be formulated as:</p><formula xml:id="formula_0">L ucl i = -log exp (z i • z i /τ ) n 1 [n =i] exp (z i • z n /τ )<label>(1)</label></formula><p>where z i = E(x i ) is the deep feature representation of the image x i , E is the feature extractor network, and τ is the temperature value. 1 is the indicator function.</p><p>In addition, to help the feature extractor learn semantically meaningful feature representations, we introduce supervised contrastive learning <ref type="bibr" target="#b11">[12]</ref> for labeled known category data, which can be denoted as:</p><formula xml:id="formula_1">L scl i = - 1 |N (i)| q∈N (i) log exp (z i • z q /τ ) n 1 [n =i] exp (z i • z n /τ )<label>(2)</label></formula><p>where N (i) represents the sample set with the same label as x i in a mini-batch data. |N (i)| represents the number of samples. The overall contrastive loss can be expressed as:</p><formula xml:id="formula_2">L cl = (1 -μ) i∈B L ucl i + μ i∈B l L scl i</formula><p>, where μ denotes the balance coefficient. B l is the labeled subset of mini-batch data.</p><p>Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We now describe how to train uniformly on known and unknown categories using the uncertainty-aware multi-view cross-pseudo-supervision strategy. Specifically, we construct two parallel classification models M 1 and M 2 , both of them composed of a feature extractor and two category classification heads, using different initialization parameters. For an original image x i , we generate two augmented versions of x i , x v1 i and x v2 i . We then feed these two augmented images into M 1 and M 2 to obtain the predictions for x v1 i and x v2 i :</p><formula xml:id="formula_3">p v1 i,1 = M 1 (x v1 i ), p v2 i,1 = M 1 (x v2 i ), p v1 i,2 = M 2 (x v1 i ), p v2 i,2 = M 2 (x v2 i ).<label>(3)</label></formula><p>The prediction outputs are obtained by concatenating the outputs of the two classification heads and then passing a softmax layer <ref type="bibr" target="#b6">[7]</ref>. Then, we can compute the ensemble predicted output of M 1 and M 2 :</p><formula xml:id="formula_4">p M1 i = p v1 i,1 + p v2 i,1 /2, p M2 i = p v1 i,2 + p v2 i,2 /2.</formula><p>Next, we need to obtain training targets for all data. For an input image x i , if x i is from the known category, we construct the training target as one hot vector, where the first C l elements are ground truth labels and the last C u elements are 0. If x i is from the unknown category, we set the first C l elements to 0 and use pseudo labels for the remaining C u elements.</p><p>We follow the self-labeling method in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> to generate pseudo labels. Specifically, the parameters in the unknown category classification head can be viewed as prototypes of each category, and our training goal is to distribute a set of samples uniformly to each prototype while maximizing the similarity between samples and prototypes <ref type="bibr" target="#b0">[1]</ref>. Let P = p u 1 ; . . . ; p u Bu ∈ R Bu×C u denotes the ensemble prediction of data of unknown categories in a mini-batch, where B u represents the number of samples. Here we only consider the output of the unknown categories head due to the samples coming from unknown categories <ref type="bibr" target="#b6">[7]</ref>. We obtain the pseudo label by optimizing the following objective:</p><formula xml:id="formula_5">max Y∈S tr YP + δH(Y) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where Y = y u 1 ; . . . ; y u Bu ∈ R Bu×C u will assign B u unknown category samples to C u category prototypes uniformly, i.e., each category prototype will be selected B u /C u times on average. S is the search space. H is the entropy function used to control the smoothness of Y. δ is the hyperparameter. The solution to this objective can be calculated by the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b5">[6]</ref>. After generating the pseudo-labels, we can combine them with the ground truth labels of known categories as training targets for uniform training.</p><p>To mitigate the effect of noisy pseudo labels, we propose to use prediction uncertainty <ref type="bibr" target="#b13">[14]</ref> to adaptively adjust the weights of pseudo labels. Specifically, we first compute the variance of the predicted outputs of the models for the different augmented images via KL-divergence:</p><formula xml:id="formula_7">V 1 = E p v1 i,1 log p v1 i,1 p v2 i,1 , V 2 = E p v1 i,2 log p v1 i,2 p v2 i,2 , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where E represents the expected value. If the variance of the model's predictions for different augmented images is large, the pseudo label may be of low quality, and vice versa. Then, based on the prediction variance of the two models, the multi-view cross-pseudo supervision loss can be formulated as:</p><formula xml:id="formula_9">L cps = E e -V1 L ce p M2 , y v1 + V 1 + E e -V2 L ce p M1 , y v2 + V 2<label>(6)</label></formula><p>where L ce denotes the cross-entropy loss. y v1 and y v2 are the training targets.</p><p>Local Information Aggregation. After the cross-pseudo-supervision training described above, we are able to assign the instances to their corresponding clustering centers. However, it ignores the alignment between local neighborhood samples, i.e., the samples are susceptible to interference from some irrelevant semantic factors such as background and color. Here, we propose a local information aggregation to enhance the alignment of local samples. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we maintain a first-in-first-out memory bank</p><formula xml:id="formula_10">M = {z m k , y m k } N m k=1</formula><p>during the training process, which contains the features of N m most recent samples and their pseudo labels. For each sample in the current batch, we compute the similarity between its features and the features of each sample in the memory bank:</p><formula xml:id="formula_11">d k = exp (z • z m k ) N m k=1 exp (z • z m k ) . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>Then based on this feature similarity, we obtain the final pseudo labels as:</p><formula xml:id="formula_13">y u = ρy u +(1-ρ) N m k=1 d k y m</formula><p>k , where ρ is the balance coefficient. By aggregating the information of the neighborhood samples, we are able to ensure consistency between local samples, which further improves the clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. To validate the effectiveness of the proposed algorithm, we conduct experiments on the widely used public dermoscopy challenge dataset ISIC 2019 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. The dataset contains a total of 25,331 dermoscopic images from eight categories: Melanoma (MEL), Melanocytic Nevus (NV), Basal Cell Carcinoma (BCC), Actinic Keratosis (AK), Benign Keratosis (BKL), Dermatofibroma (DF), Vascular Lesion (VASC), and Squamous Cell Carcinoma (SCC). Since the dataset suffers from severe category imbalance, we randomly sampled 500 samples from those major categories (MEL, NV, BCC, BKL) to maintain category balance. Then, we construct the NCD task where we treat 50% of the categories (AK, MEL, NV, BCC) as known categories and the remaining 50% of the categories (BKL, SCC, DF, VASC) as unknown categories. We also swap the known and unknown categories to form a second NCD task. For task 1 and task 2, we report the average performance of 5 runs. Implementation Details. We used ResNet-18 <ref type="bibr" target="#b10">[11]</ref> as the backbone of the classification model. The known category classification head is an l2 -normalized linear classifier with C l output units. The unknown category classification head consists of a projection layer with 128 output units, followed by an l2 -normalized linear classifier with C u output units. In the first contrastive learning pre-training step, we used SGD optimizer to train the model for 200 epochs and gradually decay the learning rate starting from 0.1 and dividing it by 5 at the epochs 60, 120, and 180. μ is set to 0.5, τ is set to 0.5. In the joint training phase, we fix the parameters of the previous feature extractor and only fine-tune the parameters of the classification head. We use the SGD optimizer to train the model for 200 epochs with linear warm-up and cosine annealing (lr base = 0.1, lr min = 0.001), and the weight decay is set to 1.5 × 10 -4 . For data augmentation, we use random horizontal/vertical flipping, color jitter, and Gaussian blurring following <ref type="bibr" target="#b6">[7]</ref>. For pseudo label, we use the Sinkhorn-Knopp algorithm with hyperparameters inherited from <ref type="bibr" target="#b6">[7]</ref>: δ = 0.05 and the number of iterations is 3. We use a memory bank M of size 100 and the hyperparameter ρ is set to 0.6. The batch size in all experiments is 512. In the inference phase, we only use the output of the unknown category classification head of M 1 <ref type="bibr" target="#b8">[9]</ref>. Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, we report the clustering performance on the unlabeled unknown category dataset. We assume that the number of unknown categories is known and it can also be obtained by the category number estimation method proposed in <ref type="bibr" target="#b8">[9]</ref>.  <ref type="bibr" target="#b8">[9]</ref> 0.5652 0.2571 0.2203 0.4284 0.1164 0.1023 RankStats+ <ref type="bibr" target="#b8">[9]</ref> 0.5845 0.2633 0.2374 0.4362 0.1382 0.1184 OpenMix <ref type="bibr" target="#b23">[24]</ref> 0.6083 0.2863 0.2512 0.4684 0.1519 0.1488 NCL <ref type="bibr" target="#b22">[23]</ref> 0.5941 0.2802 0.2475 0.4762 0.1635 0.1573 UNO <ref type="bibr" target="#b6">[7]</ref> 0.6131 0.3016 0.2763 0.4947 0.1692 0.1796 Ours 0.6654 0.3372 0.3018 0.5271 0.1826 0.2033</p><p>Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, we use the average clustering accuracy (ACC), normalized mutual information (NMI) and adjusted rand index (ARI) to evaluate the clustering performance of different algorithms. Specifically, we first match the clustering assignment and ground truth labels by the Hungarian algorithm <ref type="bibr" target="#b12">[13]</ref>. After the optimal assignment is determined, we then compute each metric. We implement all algorithms based on the PyTorch framework and conduct experiments on 8 RTX 3090 GPUs.</p><p>Comparison with State-of-the-Art Methods. We compare our algorithms with some state-of-the-art NCD methods, including RankStats <ref type="bibr" target="#b8">[9]</ref>, RankStats+ (RankStats with incremental learning) <ref type="bibr" target="#b8">[9]</ref>, OpenMix <ref type="bibr" target="#b23">[24]</ref>, NCL <ref type="bibr" target="#b22">[23]</ref>, UNO <ref type="bibr" target="#b6">[7]</ref>. we also compare with the benchmark method (Baseline), which first trains a model using known category data and then performs clustering on unknown category data. Table <ref type="table" target="#tab_0">1</ref> shows the clustering performance of each comparison algorithm on different NCD tasks. It can be seen that the clustering performance of the benchmark method is poor, which indicates that the model pre-trained using only the known category data does not provide a good clustering of the unknown category. Moreover, the state-of-the-art NCD methods can improve the clustering performance, which demonstrates the effectiveness of the currently popular two-stage solution. However, our method outperforms them, mainly due to the fact that they need to generate pairwise similarity pseudo labels through features obtained based on self-supervised learning, while ignoring the effect of noisy pseudo labels. Compared with the best comparison algorithm UNO, our method yields 5.23% ACC improvement, 3.56% NMI improvement, and 2.55% ARI improvement on Task1, and 3.24% ACC improvement, 1.34% NMI improvement, and 2.37% ARI improvement on Task2, which shows that our method is able to provide more reliable pseudo labels for NCD.</p><p>Ablation Study of Each Key Component. We performed ablation experiments to verify the effectiveness of each component. As shown in Table <ref type="table" target="#tab_1">2</ref>, CL is contrastive learning, UMCPS is uncertainty-aware multi-view cross-pseudosupervision, and LIA is the local information aggregation module. It can be observed that CL brings a significant performance gain, which indicates that contrastive learning helps to learn a general and robust feature representation for NCD. In addition, UMCPS also improves the clustering performance of the model, which indicates that unified training helps to the category information interaction. LIA further improves the clustering performance, which indicates that local information aggregation helps to provide better pseudo labels. Finally, our algorithm incorporates each component to achieve the best performance.</p><p>Ablation Study of Contrastive Learning. We further examined the effectiveness of each component in contrastive learning. Recall that the contrastive learning strategy includes supervised contrastive learning for the labeled known category data and unsupervised contrastive learning for all data. As shown in Table <ref type="table" target="#tab_2">3</ref>, it can be observed that both components improve the clustering performance of the model, which indicates that SCL helps the model to learn semantically meaningful feature representations, while UCL makes the model learn robust unbiased feature representations and avoid its overfitting to known categories.</p><p>Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We also examine the effectiveness of uncertainty-aware multi-view cross-pseudosupervision. We compare it with 1) w/o CPS, which does not use cross-pseudosupervision, and 2) CPS, which uses cross-pseudo-supervision but not the uncertainty to control the contribution of the pseudo label. As shown in Table <ref type="table" target="#tab_2">3</ref>, it can be seen that CPS outperforms w/o CPS, which indicates that CPS encourages the model to maintain consistent predictions for different augmented versions of the input images, and enhances the generalization performance of the model. UMCPS achieves the best clustering performance, which shows its ability to use uncertainty to alleviate the effect of noisy pseudo labels and avoid causing error accumulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel class discovery framework for discovering new dermatological classes. Our approach consists of three key designs. First, contrastive learning is used to learn a robust feature representation. Second, uncertainty-aware multi-view cross-pseudo-supervision strategy is trained uniformly on data from all categories, while prediction uncertainty is used to alleviate the effect of noisy pseudo labels. Finally, the local information aggregation module further refines the pseudo label by aggregating the neighborhood information to improve the clustering performance. Extensive experimental results validate the effectiveness of our approach. Future work will be to apply this framework to other medical image analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall framework of our proposed novel class discovery algorithm.</figDesc><graphic coords="3,60,30,53,72,303,58,187,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Clustering performance of different comparison algorithms on different tasks.</figDesc><table><row><cell>Method</cell><cell>Task1</cell><cell></cell><cell></cell><cell>Task2</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Baseline</cell><cell cols="6">0.4685 0.2107 0.1457 0.3899 0.0851 0.0522</cell></row><row><cell>RankStats</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of each key component.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell>Task1</cell><cell></cell><cell></cell><cell>Task2</cell></row><row><cell cols="4">CL UMCPS LIA ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell cols="4">0.4685 0.2107 0.1457 0.3899 0.0851 0.0522</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell cols="4">0.5898 0.2701 0.2375 0.4402 0.1465 0.1322</cell></row><row><cell>✓</cell><cell>✓</cell><cell></cell><cell cols="4">0.6471 0.3183 0.2821 0.5012 0.1732 0.1851</cell></row><row><cell>✓</cell><cell></cell><cell>✓</cell><cell cols="4">0.6255 0.3122 0.2799 0.4893 0.1688 0.1781</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="4">0.6654 0.3372 0.3018 0.5271 0.1826 0.2033</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of contrastive learning and uncertainty-aware multi-view cross-pseudo-supervision.</figDesc><table><row><cell>Method</cell><cell>Task1</cell><cell></cell><cell></cell><cell>Task2</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Baseline</cell><cell cols="6">0.4685 0.2107 0.1457 0.3899 0.0851 0.0522</cell></row><row><cell>SCL</cell><cell cols="6">0.5381 0.2362 0.1988 0.4092 0.1121 0.1003</cell></row><row><cell>UCL</cell><cell cols="6">0.5492 0.2482 0.2151 0.4291 0.1173 0.1174</cell></row><row><cell cols="7">SCL+UCL 0.5898 0.2701 0.2375 0.4402 0.1465 0.1322</cell></row><row><cell>w/o CPS</cell><cell cols="6">0.6021 0.2877 0.2688 0.4828 0.1672 0.1629</cell></row><row><cell>CPS</cell><cell cols="6">0.6426 0.3201 0.2917 0.5082 0.1703 0.1902</cell></row><row><cell>UMCPS</cell><cell cols="6">0.6654 0.3372 0.3018 0.5271 0.1826 0.2033</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Open-world semi-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03526</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01264-99" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">BCN20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified objective for novel class discovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9284" to="9292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoNovel: automatically discovering and learning novel visual categories</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6767" to="6781" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local and global structure-aware entropy regularized mean teacher model for 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-855" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2020, Part I</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Q</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation. Pattern Recogn</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haseyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">108911</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ACPL: anticurriculum pseudo-labelling for semi-supervised medical image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20697" to="20706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusing finetuned deep features for skin lesion classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahbod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pitiot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GP-CNN-DTEL: global-part CNN model with data-transformed ensemble learning for skin lesion classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2870" to="2882" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single model deep learning on imbalanced small datasets for skin lesion classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1242" to="1254" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SimCVD: simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2228" to="2237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Opportunities and challenges: classification of skin disease based on deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chin. J. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention residual learning for skin lesion classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2092" to="2103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neighborhood contrastive learning for novel class discovery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10867" to="10875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">OpenMix: reviving known knowledge for discovering novel visual categories in an open world</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9462" to="9470" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
