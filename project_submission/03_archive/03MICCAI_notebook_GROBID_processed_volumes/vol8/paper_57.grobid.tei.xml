<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion-Based Data Augmentation for Nuclei Image Segmentation</title>
				<funder ref="#_3RreFau #_WNMUhjM">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_ebG4SdM #_vvnyFFz">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_PfHXV36">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_dutHf88 #_7u2sW2k">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Research Institute of Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Shenzhen Health Development Research and Data Management Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion-Based Data Augmentation for Nuclei Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="592" to="602"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">96DBA4933F5B2F9109E8960904C0FF5D</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nuclei segmentation is a fundamental but challenging task in the quantitative analysis of histopathology images. Although fullysupervised deep learning-based methods have made significant progress, a large number of labeled images are required to achieve great segmentation performance. Considering that manually labeling all nuclei instances for a dataset is inefficient, obtaining a large-scale human-annotated dataset is time-consuming and labor-intensive. Therefore, augmenting a dataset with only a few labeled images to improve the segmentation performance is of significant research and application value. In this paper, we introduce the first diffusion-based augmentation method for nuclei segmentation. The idea is to synthesize a large number of labeled images to facilitate training the segmentation model. To achieve this, we propose a two-step strategy. In the first step, we train an unconditional diffusion model to synthesize the Nuclei Structure that is defined as the representation of pixel-level semantic and distance transform. Each synthetic nuclei structure will serve as a constraint on histopathology image synthesis and is further post-processed to be an instance map. In the second step, we train a conditioned diffusion model to synthesize histopathology images based on nuclei structures. The synthetic histopathology images paired with synthetic instance maps will be added to the real dataset for</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>training the segmentation model. The experimental results show that by augmenting 10% labeled real dataset with synthetic samples, one can achieve comparable segmentation results with the fully-supervised baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nuclei segmentation is a fundamental step in medical image analysis. Accurately segmenting nuclei helps analyze histopathology images to facilitate clinical diagnosis and prognosis. In recent years, many deep learning based nuclei segmentation methods have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. Most of these methods are fully-supervised so the great segmentation performance usually relies on a large number of labeled images. However, manually labeling the pixels belonging to all nucleus boundaries in an image is time-consuming and requires domain knowledge. In practice, it is hard to obtain an amount of histopathology images with dense pixel-wise annotations but feasible to collect a few labeled images. A question is raised naturally: can we expand the training dataset with a small proportion of images labeled to reach or even exceed the segmentation performance of the fully-supervised baseline? Intuitively, since the labeled images are samples from the population of histopathology images, if the underlying distribution of histopathology images is learned, one can generate infinite images and their pixel-level labels to augment the original dataset. Therefore, it is demanded to develop a tool that is capable of learning distributions and generating new paired samples for segmentation.</p><p>Generative adversarial network (GANs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref> have been widely used in data augmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. Specially, a newly proposed GAN-based method can synthesize labeled histopathology image for nuclei segmentation <ref type="bibr" target="#b20">[21]</ref>. While GANs are able to generate high quality images, they are known for unstable training and lack of diversity in generation due to the adversarial training strategy. Recently, diffusion models represented by denoising diffusion probabilistic model (DDPM) <ref type="bibr" target="#b7">[8]</ref> tend to overshadow GANs. Due to the theoretical basis and impressive performance of diffusion models, they were soon applied to a variety of vision tasks, such as inpainting, superresolution <ref type="bibr" target="#b29">[30]</ref>, text-to-image translation, anomaly detection and segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. As likelihood-based models, diffusion models do not require adversarial training and outperform GANs on the diversity of generated images <ref type="bibr" target="#b2">[3]</ref>, which are naturally more suitable for data augmentation. In this paper, we propose a novel diffusionbased augmentation framework for nuclei segmentation. The proposed method consists of two steps: unconditional nuclei structure synthesis and conditional histopathology image synthesis. We develop an unconditional diffusion model and a nuclei-structure conditioned diffusion model (Fig. <ref type="figure" target="#fig_0">1</ref>) for the first and second step, respectively. On the training stage, we train the unconditional diffusion model using nuclei structures calculated from instance maps and the conditional diffusion model using paired images and nuclei structures. On the testing stage, the nuclei structures and the corresponding images are generated successively by the two models. As far as our knowledge, we are the first to apply diffusion models on histopathology image augmentation for nuclei segmentation.</p><p>Our contributions are: (1) a diffusion-based data augmentation framework that can generate histopathology images and their segmentation labels from scratch; (2) an unconditional nuclei structure synthesis model and a conditional histopathology image synthesis model; (3) experiments show that with our method, by augmenting only 10% labeled training data, one can obtain segmentation results comparable to the fully-supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our goal is to augment a dataset containing a limited number of labeled images with more samples to improve the segmentation performance. To increase the diversity of labeled images, it is preferred to synthesize both images and their corresponding instance maps. We propose a two-step strategy for generating new labeled images. Both steps are based on diffusion models. The overview of the proposed framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In this section, we introduce the two steps in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unconditional Nuclei Structure Synthesis</head><p>In the first step, we aim to synthesize more instance maps. Since it is not viable to directly generate an instance map, we instead choose to generate its surrogate nuclei structure, which is defined as the concatenation of pixel-level semantic and distance transform. Pixel-level semantic is a binary map where 1 or 0 indicates whether a pixel belongs to a nucleus or not. The distance transform consists of the horizontal and the vertical distance transform, which are obtained by calculating the normalized distance of each pixel in a nucleus to the horizontal and the vertical line passing through the nucleus center <ref type="bibr" target="#b4">[5]</ref>. Clearly, the nuclei structure is a 3-channel map with the same size as the image. As nuclei instances can be identified from the nuclei structure, we can easily construct the corresponding instance map by performance marker-controlled watershed algorithm on the nuclei structure <ref type="bibr" target="#b28">[29]</ref>. Therefore, the problem of synthesizing instance map transfers to synthesizing nuclei structures. We deploy an unconditional diffusion model to learn the distribution of nuclei structures.</p><p>Denote a true nuclei structure as y 0 , which is sampled from real distribution q(y). To maximize data likelihood, the diffusion model defines a forward and a reverse process. In the forward process, small amount of Gaussian noise are successively added to the sample y 0 in T steps by:</p><formula xml:id="formula_0">y t = 1 -β t y t-1 + β t t-1 , t = 1, ..., T,<label>(1)</label></formula><p>where t ∼ N (0, I) and {β t ∈ (0, 1)} T t=1 is a variance schedule. The resulting sequence {y 0 , ..., y T } forms a Markov chain. The conditional probability of y t given y t-1 follows a Gaussian distribution:</p><formula xml:id="formula_1">q(y t |y t-1 ) = N (y t ; 1 -β t y t-1 , β t I).<label>(2)</label></formula><p>In the reverse process, since q(y t-1 |y t ) cannot be easily estimated, a model p θ (y t-1 |y t ) (typically a neural network) will be learned to approximate q(y t-1 |y t ). Specifically, p θ (y t-1 |y t ) is a also Gaussian distribution:</p><formula xml:id="formula_2">p θ (y t-1 |y t ) = N (y t-1 ; μ θ (y t , t), Σ θ (y t , t)),<label>(3)</label></formula><p>The objective function is the variational lower bound loss: L = L T + L T -1 + ... + L 0 , where every term except L 0 is a KL divergence between two Gaussian distributions. In practice, a simplified version of L t is commonly used <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_3">L simple t = E y0, t t -θ ( √ ᾱt y t + √ 1 -ᾱt t , t) 2 , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where α t = 1β t and ᾱt = t i=1 α i . Clearly, the optimization objective of the neural network parameterized by θ is to predict the Gaussian noise t from the input y t at time t.</p><p>After the network is trained, one can progressively denoise a random point from N (0, I) by T steps to produce a new sample:</p><formula xml:id="formula_5">y t-1 = 1 √ α t (y t - 1 -α t √ 1 -ᾱt θ (y t , t)) + σ t z, z ∼ N (0, I)<label>(5)</label></formula><p>For synthesizing nuclei structures, we train an unconditional DDPM on nuclei structures calculated from real instance maps. Following <ref type="bibr" target="#b7">[8]</ref>, the network of this unconditional DDPM has a U-Net architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Histopathology Image Synthesis</head><p>In the second step, we synthesize histopathology images conditioned on nuclei structures. Without any constraint, an unconditional diffusion model will generate diverse samples. There are usually two ways to synthesize images constrained by certain conditions: classifier-guided diffusion <ref type="bibr" target="#b2">[3]</ref> and classifier-free guidance <ref type="bibr" target="#b9">[10]</ref>. Since classifier-guided diffusion requires training a separate classifier which is an extra cost, we choose classifier-free guidance to control sampling process.</p><p>Let θ (x t , t) and θ (x t , t, y) be the noise predictor of unconditional diffusion model p θ (x|y) and conditional diffusion model p θ (x), respectively. The two models can be learned with one neural network. Specifically, p θ (x|y) is trained on paired data (x 0 , y 0 ) and p θ (x) can be trained by randomly discarding y (i.e. y = ∅) with a certain drop rate ∈ (0, 1) so that the model learns unconditional and conditional generation simultaneously. The noise predictor θ (x t , t, y) of classifier-free guidance is a combination of the above two predictors:</p><formula xml:id="formula_6">θ (x t , t, y) = (w + 1) θ (x t , t, y) -w θ (x t , t),<label>(6)</label></formula><p>where θ (x t , t) = θ (x t , t, y = ∅), w is a scalar controlling the strength of classifier-free guidance.</p><p>Unlike the network of unconditional nuclei structure synthesis which inputs the noisy nuclei structure y t and outputs the prediction of t (y t , t), the network of conditional nuclei image synthesis takes the noisy nuclei image x t and the corresponding nuclei structure y as inputs and the prediction of t (x t , t, y) as output. Therefore, the conditional network should be equipped with the ability to well align the paired histopathology image and nuclei structure. Since nuclei structures and histopathology images have different feature spaces, simply concatenating or passing them through a cross-attention module <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> before entering the U-Net will degrade image fidelity and yield unclear correspondence between synthetic nuclei image and its nuclei structure. Inspired by <ref type="bibr" target="#b27">[28]</ref>, we embed information of the nuclei structure into feature maps of nuclei image by the spatially-adaptive normalization (SPADE) module <ref type="bibr" target="#b24">[25]</ref>. In other words, the spatial and morphological information of nuclei modulates the normalized feature maps such that the nuclei are generated in the right places while the background is left to be created freely. We include the SPADE module in different levels of the network to utilize the multi-scale information of nuclei structure. The network of conditional nuclei image synthesis also applies a U-Net architecture. The encoder is a stack of Resblocks and attention blocks (AttnBlocks). Each Resblock consists of 2 GroupNorm-SiLU-Conv and each Attnblocks calculates the self-attention of the input feature map. The decoder is a stack of CondResBlocks and attention blocks. Each CondResBlock consists of SPADE-SiLU-Conv which takes both feature map and nuclei structure as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Datasets. We conduct experiments on two datasets: MoNuSeg <ref type="bibr" target="#b12">[13]</ref> and Kumar <ref type="bibr" target="#b13">[14]</ref>. The MoNuSeg dataset has 44 labeled images of size 1000 × 1000, 30 for training and 14 for testing. The Kumar dataset consists of 30 1000×1000 labeled images from seven organs of The Cancer Genome Atlas (TCGA) database. The dataset is splited into 16 training images and 14 testing images. Paired Sample Synthesis. To validate the effectiveness of the proposed augmentation method, we create 4 subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance labels. Precisely, we first crop all images of each dataset into 256 × 256 patches with stride 128, then obtain the features of all patches with pretrained ResNet50 <ref type="bibr" target="#b5">[6]</ref> and cluster the patches into 6 classes by Kmeans. Patches close to the cluster centers are selected. The encoder and decoder of the two networks have 6 layers with channels 256, 256, 512, 512, 1024 and 1024. For the unconditional nuclei structure synthesis network, each layer of the encoder and decoder has 2 ResBlocks and last 3 layers contain AttnBlocks. The network is trained using the AdamW optimizer with a learning rate of 10 -4 and a batch size of 4. For the conditional histopathology image synthesis network, each layer of the encoder and the decoder has 2 ResBlocks and 2 CondResBlocks respectively, and last 3 layers contain AttnBlocks. The network is first trained in a fully-conditional style (drop rate = 0) and then finetuned in a classifier free style (drop rate = 0.2). We use AdamW optimizer with learning rates of 10 -4 and 2 × 10 -5 for the two training stages, respectively. The batch size is set to be 1. For the diffusion process of both steps, we set the total diffusion timestep T to 1000 with a linear variance schedule {β 1 , ..., β T } following <ref type="bibr" target="#b7">[8]</ref>.</p><p>For MoNuSeg dataset, we generate 512/512/512/1024 synthetic samples for 10%/20%/50%/100% labeled subsets; for Kumar dataset, 256/256/256/512 synthetic samples are generated for 10%/20%/50%/100% labeled subsets. The synthetic nuclei structures are generate by the nuclei structure synthesis network and the corresponding images are generated by the histopathology image synthesis network with the classifier-free guidance scale w = 2. Each follows the reverse diffusion process with 1000 timesteps <ref type="bibr" target="#b7">[8]</ref>. We then obtain the augmented subsets by adding the synthetic paired images to the corresponding labeled subsets.</p><p>Nuclei segmentation. The effectiveness of the proposed augmentation method can be evaluated by comparing the segmentation performance of using the four labeled subsets and using the corresponding augmented subsets to train a segmentation model. We choose to train two nuclei segmentation models -Hover-Net <ref type="bibr" target="#b4">[5]</ref> and PFF-Net <ref type="bibr" target="#b17">[18]</ref>. To quantify the segmentation performance, we use two metrics: Dice coefficient and Aggregated Jaccard Index (AJI) <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effectiveness of the Proposed Data Augmentation Method</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the synthetic samples from the models trained on the subset with 10% labeled images. We have the following observations. First, the synthetic samples look realistic: the patterns of synthetic nuclei structures and textures of synthetic images are close to the real samples. Second, due to the conditional mechanism of the image synthesis network and the classifier-guidance sampling, the synthetic images are well aligned with the corresponding nuclei structures, which is the prerequisite to be additional segmentation training samples. Third, the synthetic nuclei structures and images show great diversity: the synthetic samples resemble different styles of the real ones but with apparent differences.</p><p>We then train segmentation models on the four labeled subsets of MoNuSeg and Kumar dataset and corresponding augmented subsets with both real and synthetic labeled images. With a specific labeling proportion, say 10%, we name the original subset as 10% labeled subset and the augmented on as 10% augmented subset. Specially, 100% labeled subset is the fully-supervised baseline. Table <ref type="table" target="#tab_0">1</ref> show the segmentation performances with Hover-Net. For MoNuSeg dataset, it is clear that the segmentation metrics drop with fewer labeled images. For example, with only 10% labeled images, Dice and AJI reduce by 2.4% and 3.1%, respectively. However, by augmenting the 10% labeled subset, Dice and AJI exceed the fully-supervised baseline by 0.9% and 1.3%. For the 20% and 50% case, the two metrics obtained by augmented subset are of the same level as using all labeled images. Note that the metrics of 10% augmented subset are higher than those of 20% augmented subset, which might be attributed to the indetermination of the diffusion model training and sampling. Interestingly, augmenting the full dataset also helps: Dice increases by 1.3% and AJI increases by 1.6% compared with the original full dataset. Therefore, the proposed augmentation method consistently improves segmentation performance of different labeling proportion. For Kumar dataset, by augmenting 10% labeled subset, AJI increases to a level comparable with that using 100% labeled images; by augmenting 20% and 50% labeled subset, AJIs exceed the fully-supervised baseline. These results demonstrate the effectiveness of the proposed augmentation method that we can achieve the same or higher level segmentation performance of the fully-supervised baseline by augmenting a dataset with a small amount of labeled images.</p><p>Generalization of the Proposed Data Augmentation. Moreover, we have similar observations when using PFF-Net as the segmentation model. Table <ref type="table" target="#tab_1">2</ref> shows the segmentation results with PFF-Net. For both MoNuSeg and Kumar datasets, all the four labeling proportions metrics notably improve with synthetic samples. This indicates the generalization of our proposed augmentation method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel diffusion-based data augmentation method for nuclei segmentation in histopathology images. The proposed unconditional nuclei structure synthesis model can generate nuclei structures with realistic nuclei shapes and spatial distribution. The proposed conditional histopathology image synthesis model can generate images of close resemblance to real histopathology images and high diversity. Great alignments between synthetic images and corresponding nuclei structures are ensured by the special design of the conditional diffusion model and classifier-free guidance. By augmenting datasets with a small amount of labeled images, we achieved even better segmentation results than the fully-supervised baseline on some benchmarks. Our work points out the great potential of diffusion models in paired sample synthesis for histopathology images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The illustration of diffusion model in the context of nuclei structure.</figDesc><graphic coords="2,58,98,179,96,334,60,59,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed diffusion-based data augmentation framework. We first generate a nuclei structure with an unconditional diffusion model, and then generate images conditioned on the nuclei structure. The instance map from the nuclei structure is paired with the synthetic image to forms a new sample.</figDesc><graphic coords="5,48,45,67,55,323,56,102,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of synthetic samples. The first and second row show selected patches and corresponding nuclei structures from the 10% labeled subset of MoNuSeg dataset. The third and fourth row show selected synthetic images and corresponding nuclei with similar style as the real one in the same column.</figDesc><graphic coords="7,83,28,148,61,295,36,193,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Effectiveness of the proposed data augmentation method with Hover-Net.</figDesc><table><row><cell>Training data</cell><cell cols="2">MoNuSeg</cell><cell>Kumar</cell></row><row><cell></cell><cell>Dice</cell><cell>AJI</cell><cell>Dice</cell><cell>AJI</cell></row><row><cell>10% labeled</cell><cell cols="4">0.7969 0.6344 0.8040 0.5939</cell></row><row><cell cols="5">10% augmented 0.8291 0.6785 0.8049 0.6161</cell></row><row><cell>20% labeled</cell><cell cols="4">0.8118 0.6501 0.8078 0.6098</cell></row><row><cell cols="5">20% augmented 0.8219 0.6657 0.8192 0.6255</cell></row><row><cell>50% labeled</cell><cell cols="4">0.8182 0.6603 0.8175 0.6201</cell></row><row><cell cols="5">50% augmented 0.8291 0.6764 0.8158 0.6307</cell></row><row><cell>100% labeled</cell><cell cols="4">0.8206 0.6652 0.8150 0.6183</cell></row><row><cell cols="5">100% augmented 0.8336 0.6810 0.8210 0.6301</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Generalization of the proposed data augmentation method with PFF-Net.</figDesc><table><row><cell>Training data</cell><cell cols="2">MoNuSeg</cell><cell>Kumar</cell></row><row><cell></cell><cell>Dice</cell><cell>AJI</cell><cell>Dice</cell><cell>AJI</cell></row><row><cell>10% labeled</cell><cell cols="4">0.7489 0.5290 0.7685 0.5965</cell></row><row><cell cols="5">10% augmented 0.7764 0.5618 0.8051 0.6458</cell></row><row><cell>20% labeled</cell><cell cols="4">0.7691 0.5629 0.7786 0.6087</cell></row><row><cell cols="5">20% augmented 0.7891 0.5927 0.8019 0.6400</cell></row><row><cell>50% labeled</cell><cell cols="4">0.7663 0.5661 0.7797 0.6175</cell></row><row><cell cols="5">50% augmented 0.7902 0.5998 0.8104 0.6524</cell></row><row><cell>100% labeled</cell><cell cols="4">0.7809 0.5708 0.8032 0.6461</cell></row><row><cell cols="5">100% augmented 0.7872 0.5860 0.8125 0.6550</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), and the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2023A1515011464</rs>, <rs type="grantNumber">2020B1515020048</rs>), and the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62102267</rs>, No. <rs type="grantNumber">61976250</rs>), and the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>, <rs type="grantNumber">JCYJ20220530141211024</rs>), and the <rs type="institution">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PfHXV36">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_ebG4SdM">
					<idno type="grant-number">2023A1515011464</idno>
				</org>
				<org type="funding" xml:id="_vvnyFFz">
					<idno type="grant-number">2020B1515020048</idno>
				</org>
				<org type="funding" xml:id="_dutHf88">
					<idno type="grant-number">62102267</idno>
				</org>
				<org type="funding" xml:id="_7u2sW2k">
					<idno type="grant-number">61976250</idno>
				</org>
				<org type="funding" xml:id="_3RreFau">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
				<org type="funding" xml:id="_WNMUhjM">
					<idno type="grant-number">JCYJ20220530141211024</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 57.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SegDiff: image segmentation with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaharbany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00390</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hover-net: Simultaneous segmentation and classification of nuclei in multitissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local context encoder: robust biomedical image segmentation against adversarial attacks</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8417" to="8424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2249" to="2281" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classifier-free diffusion guidance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-aware semantic inpainting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4398" to="4411" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depthwise nonlocal module for fast salient object detection using a single thread</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6188" to="6199" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic feature fusion net: a novel instance segmentation paradigm for biomedical and biological images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2045" to="2059" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4243" to="4252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Which pixel to annotate: a labelefficient nuclei segmentation framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="947" to="958" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-stream cell segmentation with low-level cues for multimodality images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Competitions in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmentation of nuclei in histopathology images by deep regression of the distance map</title>
		<author>
			<persName><forename type="first">P</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="448" to="459" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glide: Towards photorealistic image generation and editing with text-guided diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16784" to="16804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Singan: learning a generative model from a single natural image</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00050</idno>
		<title level="m">Semantic image synthesis via diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nuclei segmentation using marker-controlled watershed, tracking using mean-shift, and kalman filter in time-lapse microscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. I: Regul. Papers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2405" to="2414" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust real-world image super-resolution against adversarial attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5148" to="5157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
