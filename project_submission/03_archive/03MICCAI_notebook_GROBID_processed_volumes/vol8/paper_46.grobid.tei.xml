<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiguo</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Howe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Rhodes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sumana</forename><surname>Ramanathan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Panagiotis</forename><surname>Korfiatis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kimberly</forename><surname>Amrami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Spinner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Timothy</forename><surname>Kline</surname></persName>
							<email>kline.timothy@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>Minnesota</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="470" to="480"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CF4E0A79780EBD56E9AAA452366C57C8</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brachial Plexus</term>
					<term>Magnetic Resonance Imaging</term>
					<term>Heterogeneity</term>
					<term>Radiomics</term>
					<term>Deep learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Texture pattern</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Brachial plexopathy is a form of peripheral neuropathy, which occurs when there is damage to the brachial plexus (BP). However, the diagnosis of breast cancer related BP from radiological imaging is still a great challenge. This paper proposes a texture pattern based convolutional neural network, called TPPNet, to carry out abnormal prediction of BP from multiple routine magnetic resonance image (MRI) pulse sequences, i.e. T2, T1, and T1 post-gadolinium contrast administration. Different from classic CNNs, the input of the proposed TPPNet is multiple texture patterns instead of images. This allows for direct integration of radiomic (i.e. texture) features into the classification models. Beyond conventional radiomic features, we also developed a new family of texture patterns, called triple point patterns (TPPs), to extract huge number of texture patterns as representations of BP' heterogeneity from its MRIs. These texture patterns share the same size and show very stable properties under several geometric transformations. Then, the TPPNet is proposed to carry out the differentiation task of abnormal BP for our study. It has several special characteristics including 1) avoidance of image augmentation, 2) huge number of channels, 3) simple end-to-end architecture, 4) free from the interference of multi-texture-pattern arrangements. Ablation study and comparisons demonstrate that the proposed TPPNet yields outstanding performances with the accuracies of 96.1%, 93.5% and 93.6% over T2, T1 and post-gadolinium sequences which exceed at least 1.3%, 5.3% and 3.4% over state-of-the-art methods for classification of normal vs. abnormal brachial plexus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Brachial plexopathy is a form of peripheral neuropathy <ref type="bibr" target="#b0">[1]</ref>. It occurs when there is damage to the brachial plexus (BP) which is a complex nerve network under the skin of the shoulder. There is a wide range of disease that may cause a brachial plexopathy.</p><p>Radiation fibrosis, primary and metastatic lung cancer, and metastatic breast cancer account for almost three-fourths of causes <ref type="bibr" target="#b1">[2]</ref>. Brachial plexus syndrome occurs not infrequently in patients with malignant disease. It is due to compression or direct invasion of the nerves by tumor which will bring many serious symptoms <ref type="bibr" target="#b2">[3]</ref>. Our research focuses on the brachial plexopathy caused by metastatic breast cancers.</p><p>Magnetic resonance imaging (MRI) and ultrasound of the brachial plexus have become two reliable diagnostic tools for brachial plexopathy <ref type="bibr" target="#b3">[4]</ref>. Automatic identification of the BP in MRI and ultrasound images has become a hot topic. Currently, most of relevant research in this field are focusing on Ultrasound modality <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Compared with ultrasound, MRI has become the primary imaging technique in the evaluation of brachial plexus pathology <ref type="bibr" target="#b8">[9]</ref>. However, to our knowledge, radiomics related BP studies utilizing MRI have not been reported previously.</p><p>Many radiomics studies have experimentally demonstrated that image texture has great potential for differentiation of different tissue types and pathologies <ref type="bibr" target="#b9">[10]</ref>. In the past several decades, many state-of-the-art methods have been proposed to extract texture patterns <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. However, how to most effectively combine texture features with deep learning, called deep texture, is still an open area of research. One prior approach, termed GLCM-CNN, was proposed to carry out a polyp differentiation task <ref type="bibr" target="#b12">[13]</ref>. However, how to arrange these GLCMs to form the 3D volume to optimize the performance is a major challenge.</p><p>With the goal of classifying normal from abnormal BP, we explored the approach of deep texture learning. This paper constructed a BP dataset with the most commonly used BP MRIs in our clinical practice. Considering the shortcoming of traditional patterns, triple point pattern (TPP) is proposed for the quantitative representation of the heterogeneity of abnormal BP's. In contrast to GLCM-CNN, TPPNet is designed to train models by feeding TPP matrices as the input with a huge number of channels. Finally, we analyze the model's performance in the experimental section. The major contributions of this study include 1) directed triangle construction idea for TPP, 2) huge number of TPP matrices as the heterogeneity representations of BP, 3) TPPNet with 15 layers and huge number of channels, 4) the BP dataset containing MR images and their corresponding ROI masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Preparation and Preprocessing</head><p>Following IRB approval for this study, we search for patients with metastatic breast cancer who had a breast cancer MRI performed between 2010 and 2020 and had morphologically positive BP on the MRI report from our electronic medical records (EMR) in * hospital. Totally, we collect approximate 807 series which include 274 T2, 254 T1 and 279 Post-gadolinium. Since some scans are seriously degraded due to motion artifacts. Therefore, each case underwent several essential image adjustments such as multi-series splitting, two-series merging, slice swapping, artifact checking and boundary corrections. To yield the ROI, firstly, we randomly sampled -40% of the sequences including both normal and abnormal ones that were manually segmented with ITK-snap by two skilled trainees <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Then, the manual segmentations were utilized to train a 3D nnUNet model which was utilized to train the model which was used to predict ROIs for the rest series <ref type="bibr" target="#b15">[16]</ref>. The predicted segmentations were manually divided into three groups, i.e. good, fair and poor. Good cases were added to the training set. This process was repeated until no improvements in the predictions for the remaining sequences was seen. The final dataset for radiomic analysis was constructed by merging the datasets for each sequence type. Only patients that had all three sequences segmented (T2, T1 and Post-gadolinium) were included in the dataset. Table <ref type="table" target="#tab_1">1</ref> shows a breakdown of the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Triple Point Pattern (TPP)</head><p>Theoretically, some texture pattern methods such as LBP, LTP, and GLDM, are based on single-variance pixel functions <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. Therefore, they extract local texture features coded by the difference or difference counts between the concerned pixel and its neighboring ones. One obvious shortcoming is the absence of global properties which need other statistical methods as the aid to yield, such as histogram and invariants <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Meanwhile, some other texture methods are generally defined by two-variance functions that only focus on two-variance patterns, such as (pixel, pixel), (pixel, neighbor count) <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>. In general, image textures extracted by these methods contain both local texture properties and global texture information. Their shortcomings might come from pattern shapes which might lead to the overfitting risk while combining with deep learning since the yielded texture matrix might have slim shapes or adaptive columns. In summary, as the requirement of the image texture and deep learning, an excellent image texture pattern should have some essential features including 1) local properties to characterize the micro-unit of the image texture, 2) global properties to represent the macro-structure of the image texture, 3) uniform shapes under nonuniform-shape images, 4) invariant or robustness under some common geometric transforms such as rotation, scaling and so on.</p><p>According above requirements, we developed a method to produce a serial of novel texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a ternary group, called triple point pattern (TPP), to extract the local texture information. Then, a statistical method like histogram is employed to count the number of the same type of pixel-triplets within the ROI or throughout the whole image. Finally, a threedimensional (3D) TPP matrix is formed to characterize the image texture globally as the following:</p><p>Two-dimensional image:</p><formula xml:id="formula_0">TPP (pi,pc,pj) (x, y, z) = M -1 m=0 N -1 j=0 ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 I ((m, n) + p i ) = x&amp; I ((m, n) + p c ) = y&amp; I (m, n) + p j = z 0 others (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where I is a MxN image, x, y, and z is the pixel triplet, x,y,z ∈ [0,L), L is its gray level, p c = (0,0) denotes the concerned pixel such as p 0 in Fig. <ref type="figure" target="#fig_0">1</ref>, p i and p j are p c 's two adjacent pixels, i,j ∈ [1,H], H is the number of p c 's adjacent points. Three-dimensional image:</p><formula xml:id="formula_2">TPP (pi,pc,pj) (x, y, z) = M -1 m=0 N -1 n=0 K-1 k=0 ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 I ((m, n, k) + p i ) = x&amp; I ((m, n, k) + p c ) = y&amp; I (m, n, k) + p j = z 0 others (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where I is a three dimensional image with the shape of MxNxK, p c = (0,0,0), other parameters are similar to the Two-dimensional image.</p><p>In statistics, a TPP matrix should a 3D distribution of directed triangle. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the TPP is formed by the concerned pixel and its two adjacent pixels in two-dimensional(2D) images. Similarly, the TPP in 3D images is constructed by one concerned voxel and its two neighboring voxels. More details could be found in the supplementary material. As the construction idea of TPP, there are four independent modes categorized by the concerned angle, i.e. 45°, 90°, 135°and 180°in 2D images, which produce 8 TPPs, 8 TPPs, 8 TPPs and 8 TPPs respectively. Analogously, the 3D image has twelve independent angle modes, i.e. 35.26°, 45°, 54.74°, 60°, 70.53°, 90°, 109.47°, 120°, 125.26°, 135°, 144.74°, and 180°. These angle modes could generate 24 TPPs, 24 TPPs, 24 TPPs, 24 TPPs, 12 TPPs, 96 TPPs, 12 TPPs, 24 TPPs, 24 TPPs, 24 TPPs, 24 TPPs and 13 TPPs respectively. Totally there are 32 TPPs in 2D images and 325 TPPs in 3D images and every TPP could produce one corresponding TPP matrix.</p><p>By further analysis, we could find some TPP pairs have an isomorphism relationship since its TPP matrix could be generated by transposing or flipping another TPP matrix on some certain conditions when two triangles formed by the pixel triplet have the relevance of shifting or scaling. For an instance, the TPP matrix by pixel-triplet (p 1 , p 0 , p 2 ) in Fig. <ref type="figure" target="#fig_0">1</ref> (1), the TPP matrix by (p 4 , p 0 , p 5 ) in Fig. <ref type="figure" target="#fig_0">1</ref> (1) and the TPP matrix by (p 6 , p 0 , p 8 ) in Fig. <ref type="figure" target="#fig_0">1</ref>(3) have the following relevance:</p><formula xml:id="formula_4">TPP (p 1 ,p 0 ,p 2 ) = T * F TPP (p 4 ,p 0 ,p 5 ) = T * F TPP (p 6 ,p 0 ,p 8 )<label>(3)</label></formula><p>where T denotes matrix transposing, F represents matrix flipping, * is the product operator. Since both T and F are continuous bijective mappings, these three TPPs are isomorphic.</p><p>In our study, these isomorphic TPP matrices are not dropped from the TPP matrix set because they are equivalent to image rotations and re-scaling. Image scaling can result in the image pixels increasing. The normalization could almost remove the effect of pixel increase caused by scaling transformations. Moreover, TPPs generated by 135°c ould also be treated as affine transformations. Therefore, data augmentation could be omitted when we combine TPP with deep learning for this study. As its definition, the TPP matrix should be a cubic array with the shape of LxLxL where L is the gray level of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TPPNet</head><p>The pipeline of the proposed method TPPNet is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The TPP matrix calculation is the preprocessing module which feeds MRI and its ROI and yield TPP matrix set. The following step is the TPPNet architecture to yield training models. Based on the construction idea of TPP, the size of the TPP matrix depends on the gray level of the image. For the same image or ROI, the larger the gray level, the sparser the matrix will be. The sparse matrix would lead to overfitting while training the model. Therefore, the image requires a re-scaling step to lower its gray level to avoid the sparsity of the TPP matrix. Consequently, our proposed TPPNet only contains three convolution blocks consisting of 15 layers. Each block has two convolution, one normalization, one max-pooling and one dropout layer. It has four particular features as follows:</p><p>1) Avoidance of image augmentation. Due to the stability of TPP matrix under rotation, scale and affine transformations, image augmentation could be omitted in the preprocessing step which can lead to image deformation. 2) Huge number of channels. TPPNet treats each TPP as an independent channel. For 2D images, there are at least 32 channels if more displacements of TPP is considered. Similarly, we could generate no less than 325 TPPs in 3D images. 3) Simple end-to-end architecture. We integrate the k-fold cross-validation, TPP generation and model training into one framework. Since the TPP matrix is always small, there are only 15 layers in TPP which could reduce the risk of overfitting issue met in deeper neural networks. 4) Free from the interference of multi-texture-pattern arrangements. Since each channel is corresponding with one TPP, it can solve the pattern arrangement issue occurred in GLCM-CNN. 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparations</head><p>Some important specificities of our computing platform contain: one AMD EPYC 7352 24-Core Processor, 1 TB memory and four Nivida A100-SXM GPUs with 320 GB GPU memory. The whole dataset is divided into three subsets according to the MR sequence, i.e. T2, T1 and post-gad. For each subset, both normal cases and abnormal cases were randomly and evenly split into five subgroups. A five-fold cross-validation scheme was employed to generate five cohorts. Totally, 15 cohorts were produced. Each cohort consists of training set, validation set and testing set by the ratio of 6:2:2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>Since all images in our dataset are 3D images, therefore, the initial channel is set 325 which is equal to the TPP number. The loss functions in the following experiments shared categorical_crossentropy. Nadam is adopted as the optimizer with the learning rate of 0.0001 and batch size of 8 for 200 epochs. All performances listed in this section are the average of performances with 5-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Impact of Gray Level</head><p>The image gray level determines the shape of the TPP matrix. To avoid its sparsity, we compute the TPP matrix set via Eq. ( <ref type="formula" target="#formula_2">2</ref>) with gray levels of 8, 12, 16, 20, 24. While rescaling the image intensity, an arc tangent approach is utilized to yield the new image.</p><p>The models are trained and tested over 15 cohorts. Their performances evaluated by accuracies are listed in Table2 which tells us that T2 sequence yields the highest accuracy of 96.1% when the gray level is 12. T1 and post-gadolinium also get acceptable results with the accuracies of 93.5% and 93.6% respectively. Other performances could be read in supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc</head><p>Loss Acc Loss Acc Loss 325 0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0. 124  1 0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107</p><p>Input channel T2 T1 Pg</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Impact of Intensity Rescaling Approaches</head><p>Rescaling approaches of image intensity could also bring impacts on the BP's differentiation while producing the TPP matrix. The commonly used methods include minmax-linear approach <ref type="bibr" target="#b24">[25]</ref>, arc tangent approach <ref type="bibr" target="#b25">[26]</ref>, and adaptive rescaling approach <ref type="bibr" target="#b26">[27]</ref>. To test the performances fairly, we test above rescaling methods at the same gray level 12. The yielded performances evaluated by accuracies are shown in Table <ref type="table" target="#tab_3">3</ref> where arc tangent method achieves the highest accuracy of 96.1% over the T2 sequence. Other performances are shown in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Multi-Channels vs Solo-Channel</head><p>We carry out experiments to train the TPPNet model and make tests with arc tangent rescaling approach under gray level 16. As a comparison, we test single channel mode as well. By sharing every TPP matrix's label with the original case label, our TPPNet works well by assigning one channel for the initial input. Once the trained model with solo channel is generated, all TPP matrices of the testing set could be tested. Hereafter, we adopted a voting method to determine if the prediction is normal, if the predicted probability is less than 0.5, otherwise, it is considered abnormal. Performances with accuracies and loss are listed in Table <ref type="table" target="#tab_5">4</ref>. Other performances are listed in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons</head><p>We evaluated our proposed TPPNet by comparing it to the recent state-of-the-art approaches over our BP dataset including VGG16 <ref type="bibr" target="#b27">[28]</ref>, InceptionNet <ref type="bibr" target="#b28">[29]</ref>, MobileNet <ref type="bibr" target="#b29">[30]</ref>, GLCM-CNN <ref type="bibr" target="#b12">[13]</ref>, ViT <ref type="bibr" target="#b30">[31]</ref>. The GLCM size of 32 × 32 is used in GLCM_CNN. All approaches shared the same image shape of 128 × 128 × 64 with 1 channel. The patch shape of ViT is 8 × 8 × 8, projection dim is 64, attention head number is set 4 with 8 transformer layers. The intensity rescaling step adopts the arc tangent approach. Other parameters are similar to the ablation study. Their performances are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we develop an approach to carry out the pioneer study of differentiating abnormal BP from normal ones relevant to breast cancer. In particular, TPP is proposed to extract texture features as the representation of BP's heterogeneity from MRIs. Moreover, a TPPNet with huge number of initial channels is designed to train the model. To testify our proposed TPPNet, a BP dataset is constructed with 452 series including three most commonly used MR sequences in clinical practice, i.e. T2, T1 and Post-gadolinium. The best result is yielded when the gray level is 12, intensity rescaling method adopts arc tangent approach. Experimental outcomes also demonstrate that the proposed TPPNet not only exhibit more stable performances but also outperform six famous state-of-the-art approaches over three most commonly used BP MR sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Directed triangle idea for TPP construction in 2D images where p0 is the concerned pixel, p1 …p8 are its adjacent pixels. Totally, they are four modes according to four concerned angles, i.e. 45°, 90°, 135°and 180°. (1) is the 1 st mode. Both (2) and (3) are the 2 nd mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of the proposed TPPNet over 3D images where NML denote normal, ABN denotes abnormal, i in (2) is the block id, r is the adaptive argument to control the filter number.</figDesc><graphic coords="5,53,31,232,70,112,60,58,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. ROC curves of TPPNet and five state-of-the-art approaches.</figDesc><graphic coords="8,129,96,391,19,192,19,125,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Totally, 189 patients including 141 normal patients and 41 abnormal ones are obtained. The range of the age are varying from 15 to 85 years old. The female patient number and male patient number are almost even. Their weights by kg are in the range of [40.8 kg, 145 kg]. All patient experiences three kinds of MRI sequences including T2, T1 and post-gadolinium. Approximately 25% sequences are conducted with GE SIGNA HDI 1.5T and the rest are produced by GE SIGNA™ Hero 3.0T. Patients are required to maintain a decubitus position while scanning. All series are scanned in sagittal view.Slice thicknesses are varying in [1.2 mm, 6.0 mm] and approximate 96% of them share the slice thickness of 4mm. Their slice number is in the range of[38, 72]. Most of slice resolutions are 512 * 512 while less than 2% of them are 256 * 256.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The final BP dataset including MRIs and their corresponding masks of T2, T1, Postgadolinium (Pg).</figDesc><table><row><cell>Image</cell><cell>Total</cell><cell>T2</cell><cell>Normal T1</cell><cell>Pg</cell><cell>T2</cell><cell>Abnormal T1</cell><cell>Pg</cell></row><row><cell>MRI</cell><cell>462</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test performances for different gray levels over T2, T1 and Post-gadolinium where Acc denotes accuracy and Pg denotes Post-gadolinium.</figDesc><table><row><cell>Gray</cell><cell>T2</cell><cell></cell><cell>T1</cell><cell></cell><cell>Pg</cell><cell></cell></row><row><cell>level</cell><cell>Acc</cell><cell>Loss</cell><cell>Acc</cell><cell>Loss</cell><cell>Acc</cell><cell>Loss</cell></row><row><cell>8</cell><cell cols="6">0.942±0.032 0.319±0.028 0.902±0.022 0.395±0.207 0.929±0.023 0.292±0.087</cell></row><row><cell>12</cell><cell cols="6">0.961±0.025 0.277±0.107 0.935±0.021 0.307±0.164 0.936±0.027 0.279±0.046</cell></row><row><cell>16</cell><cell cols="6">0.948±0.034 0.350±0.333 0.922±0.028 0.306±0.107 0.921±0.041 0.313±0.053</cell></row><row><cell>20</cell><cell cols="6">0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.942±0.034 0.302±0.124</cell></row><row><cell>24</cell><cell cols="6">0.948±0.034 0.348±0.162 0.929±0.023 0.309±0.067 0.922±0.046 0.322±0.176</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Test performances for three intensity rescaling approaches over T2, T1 and Postgadolinium where the gray level is set 12, Acc denotes accuracy, Pg denotes Post -gadolinium, and Artan is arc tangent function.</figDesc><table><row><cell></cell><cell>Acc</cell><cell>Loss</cell><cell>Acc</cell><cell>Loss</cell><cell>Acc</cell><cell>Loss</cell></row><row><cell cols="7">Min-max 0.947±0.027 0.303±0.082 0.928±0.026 0.312±0.138 0.920±0.028 0.475±0.327</cell></row><row><cell cols="7">Adaptive 0.921±0.035 0.385±0.158 0.902±0.022 0.462±0.092 0.856±0.020 0.517±0.092</cell></row><row><cell>Artan</cell><cell>0.961±0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.025 0.277±0.107 0.935±0.021 0.307±0.164 0.936±0.027 0.279±0.046</head><label></label><figDesc></figDesc><table><row><cell>Rescaling</cell><cell>T2</cell><cell>T1</cell><cell>Pg</cell></row><row><cell>approach</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Test performances comparison between multi-channel and solo-channel over T2, T1 and Post-gadolinium where the intensity rescaling function is arc tangent, the gray level is set 20, Acc denotes accuracy and Pg denotes Post -gadolinium.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison between TPPNet T1 sequence where Acc is accuracy, AUC denotes Spe denotes specificity. The grey level is set 12. demonstrated in Table5which contains the accuracy, AUC score, specificity, sensitivity, and F1 score. Their ROC curves over T1 sequence are plotted in Fig.3. T2 and Postgadolinium's performances could be found in supplementary materials.</figDesc><table><row><cell>Approach</cell><cell>Acc</cell><cell>AUC</cell><cell>Sen</cell><cell>1-Spe</cell><cell>F1-Score</cell></row><row><cell>VGG16</cell><cell cols="2">0.837±0.106 0.739±0.110</cell><cell>0.910±0.125</cell><cell>0.548±0.124</cell><cell>0.749±0.114</cell></row><row><cell>IncepƟon</cell><cell cols="2">0.805±0.014 0.465±0.102</cell><cell>1.000±0.000</cell><cell>0.033±0.067</cell><cell>0.474±0.061</cell></row><row><cell>Mobilenet</cell><cell cols="2">0.869±0.033 0.616±0.170</cell><cell>0.985±0.030</cell><cell>0.405±0.258</cell><cell>0.719±0.107</cell></row><row><cell>GLCM-CNN</cell><cell cols="2">0.851±0.019 0.704±0.113</cell><cell>0.975±0.050</cell><cell>0.362±0.249</cell><cell>0.707±0.073</cell></row><row><cell>ViT3D</cell><cell cols="2">0.882±0.020 0.675±0.163</cell><cell>0.969±0.043</cell><cell>0.533±0.245</cell><cell>0.782±0.058</cell></row><row><cell>TPPNet</cell><cell cols="2">0.935±0.021 0.832±0.101</cell><cell>1.000±0.000</cell><cell>0.676±0.107</cell><cell>0.881±0.045</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Microscopy</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brachial plexopathy as a complication of radio-therapy: a systematic review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shabeeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Musa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Cancer Ther. Rev</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="120" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MR imaging of nontraumatic brachial plex-opathies: frequency and spectrum of findings</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Wittenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Adkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1024" to="1032" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Radiation therapy of brachial plexus syndrome from breast cancer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Nisce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C H</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1022" to="1025" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MR imaging of the brachial plexus</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging Clin. N. Am</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="826" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ultrasound nerve segmentation of brachial plexus based on optimized resu-net</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Imaging Systems and Techniques (IST)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep networks for brachial plexus nerves segmentation and detection using ultrasound images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pisda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Sisodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kesswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Vella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Xuereb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-86223-7_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-86223-7_13" />
	</analytic>
	<monogr>
		<title level="m">ISMS 2020</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Randhawa</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="132" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of ultrasound brachial plexus based on u-net</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Communications, Information System and Computer Engineering (CISCE)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="482" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brachial plexus nerve trunk recognition from ultra-sound images: a comparative study of deep learning models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="82003" to="82014" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MRI of brachial plex-opathies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sureka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Radiol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="218" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Radiomics: the bridge between medical imaging and personalized medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lambin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T H</forename><surname>Leijenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="749" to="762" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computational radiomics system to decode the radiographic phenotype</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J M</forename><surname>Van Griethuysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Res</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="104" to="e107" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of membrane process model from black box to machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ramel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2788" to="7669" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D-GLCM CNN: a 3-dimensional gray-level cosoccur-rence matrixbased CNN model for polyp classification via CT colonography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2013" to="2024" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Engineering and algorithm design for an image processing API: a technical report on ITK -the insight toolkit</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Medicine Meets Virtual Reality</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Westwood</surname></persName>
		</editor>
		<meeting>eeding of Medicine Meets Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="586" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enabling reproducible research and open science</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced local texture feature sets for face recognition under difficult lighting conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1635" to="1650" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Responsible radiomics research for faster clinical translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vallières</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zwanenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="193" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic digital modulation classification using extreme learning machine with local binary pattern histogram features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Güner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><forename type="middle">F</forename><surname>Alçin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Şengür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="214" to="225" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Verifiable privacy-enhanced rotation invariant LBP feature extraction in fog computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A dynamic lesion model for differentiation of malignant and benign pathologies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3485</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variability of textural features in FDG PET images due to different acquisition modes and reconstruction parameters</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Galavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hollensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Oncol</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1012" to="1016" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The precision of textural analysis in 18F-FDG-PET scans of oesophageal cancer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doumou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsoumpas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2805" to="2812" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intensity standardization methods in magnetic reso-nance imaging of head and neck cancer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phy. Imaging Radiat. Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="88" to="93" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lesion classification by model-based feature extraction: a differential affine invariant model of soft tissue elasticity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pomeroy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14029</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv pre-print</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histogram-based adaptive gray level scaling for texture feature classification of colorectal polyps</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pomeroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">10575</biblScope>
			<biblScope unit="page" from="507" to="513" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolution newtworks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MobileNets: efficient convolutional neural net-works for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual transformers: where do transformers really belong in vi-sion models?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="579" to="589" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
