<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents</title>
				<funder ref="#_ksFGgzB #_HbnJWQW #_qsNjAD5">
					<orgName type="full">STCSM</orgName>
				</funder>
				<funder ref="#_Bm3pQAf">
					<orgName type="full">State Key Laboratory of UHD Video and Audio Production and Presentation</orgName>
				</funder>
				<funder ref="#_sASe4sG">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weixiong</forename><surname>Lin</surname></persName>
							<email>wxlin@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziheng</forename><surname>Zhao</surname></persName>
							<email>zhaoziheng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="525" to="536"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D6BE5900C5C36FF0508442AF771C7FC6</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal Dataset â€¢ Vision-Language Pretraining</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M imagecaption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before, PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and subcaption. While pretraining a CLIP-style model on PMC-OA, our model named PMC-CLIP outperform previous state-of-the-art models on various downstream tasks, including imagetext retrieval on ROCO, MedMNIST image classification, Medical VQA, for example, +8.1% R@10 on image-text retrieval, +3.9% accuracy on image classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the recent literature, development of foundational models has been the main driving force in artificial intelligence, for example, large language models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">30]</ref> trained with either autoregressive prediction or masked token inpainting, and computer vision models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">29]</ref> trained by contrasting visuallanguage features. In contrast, development in the biomedical domain lags far behind due to limitations of data availability from two aspects, (i) the expertise required for annotation, (ii) privacy concerns. This paper presents our preliminary study for constructing a large-scale, high-quality, image-text biomedical dataset using publicly available scientific papers, with minimal manual efforts involved.</p><p>In particular, we crawl figures and corresponding captions from scientific documents on PubMed Central, which is a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM) <ref type="bibr" target="#b31">[31]</ref>. This brings two benefits: (i) the contents in publications are generally well-annotated and examined by experts, (ii) the figures have been well-anonymized and de-identified. In the literature, we are clearly not the first to construct biomedical datasets in such manner, however, existing datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38]</ref> suffer from certain limitations in diversity or scale from today's standard. For example, as a pioneering work, ROCO <ref type="bibr" target="#b27">[28]</ref> was constructed long time ago with only 81k radiology images. MedICAT <ref type="bibr" target="#b34">[34]</ref> contains 217k images, but are mostly consisted of compound figures.</p><p>In this work, we tackle the above-mentioned limitations by introducing an automatic pipeline to generate dataset with subfigure-subcaption correspondence from scientific documents, including three major stages: medical figure collection, subfigure separation, subcaption separation &amp; alignment. The final dataset, PMC-OA, consisting of 1.65M image-text pairs (not including samples from ROCO), covers a wide scope of diagnostic procedures and diseases, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>. Along with the constructed dataset, we train a CLIP-style vision-language model for the biomedical domain, termed as PMC-CLIP. To achieve such a goal, the model is trained on PMC-OA with standard image-text contrastive (ITC) loss, and to encourage the joint interaction of image and text, masked language modeling (MLM) is also applied. We evaluate the pre-trained model on several downstream tasks, including medical image-text retrieval, medical image classification, and medical visual question answering (VQA). PMC-CLIP achieves state-of-the-art performance on various downstream tasks, surpassing previous methods significantly.</p><p>Overall, in this paper, we make the following contributions: First, we propose an automatic pipeline to construct high-quality image-text biomedical datasets from scientific papers, and construct an image-caption dataset via the proposed pipeline, named PMC-OA, which is 8Ã— larger than before. With the proposed pipeline, the dataset can be continuously updated. Second, we pre-train a vision-language model on the constructed image-caption dataset, termed as PMC-CLIP, to serve as a foundation model for biomedical domain. Third, we conduct thorough experiments on various downstream tasks (retrieval, classification, and VQA), and demonstrate state-of-the-art performance. The dataset and pre-trained model will be made available to the community. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The PMC-OA Dataset</head><p>In this section, we start by describing the dataset collection procedure in Sect. 2.1, followed by a brief overview of PMC-OA in Sect. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Collection</head><p>In this section, we detail the proposed pipeline to create PMC-OA, a large-scale dataset that contains 1.65M image-text pairs. The whole procedure consists of three major stages: (i) medical figure collection, (ii) subfigure separation, (iii) subcaption separation &amp; alignment, as summarised in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Figure Collection (</head><p>Step 1 and 2 in Fig. <ref type="figure" target="#fig_1">2</ref>). We first extract figures and captions from PubMedCentral (till 2022-09-16) <ref type="bibr" target="#b31">[31]</ref>. 2.4M papers are covered and 12M figure-caption pairs are extracted. To derive medical figures, inspired by MedICat <ref type="bibr" target="#b34">[34]</ref>, we first filter out the captions without any medical keywords<ref type="foot" target="#foot_0">1</ref> and then use a classification network trained on DocFigure <ref type="bibr" target="#b13">[14]</ref> to further pick out the medical figure, ending up with 381K medical figures.</p><p>Subfigure Separation (Step 3 and 4 in Fig. <ref type="figure" target="#fig_1">2</ref>). We randomly check around 300 figures from previous step, and find that around 80% of figures are compound, i.e. multiple pannels. We thus train a subfigure detector on MedICaT subfigure-subcaption subset <ref type="bibr" target="#b34">[34]</ref> (MedICatSub) to break the compound figures into subfigures. After separation, to filter out non-medical subfigures missed in the former step, we apply the aforementioned classifier again on the derived subfigures, obtaining 1.6M subfigure-caption pairs. We termed this dataset as PMC-OA Beta version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subcaption Separation and Alignment (</head><p>Step 5 and 6 in Fig. <ref type="figure" target="#fig_1">2</ref>). To further align subfigure to its corresponding part within the full caption, i.e., subcaption, we need to break the captions into subcaptions first, we apply an off-shelf caption distributor <ref type="bibr" target="#b32">[32]</ref>. We pretrain a CLIP-style model (termed as PMC-CLIP-Beta, training detail will be described in Sect. 3) on PMC-OA-Beta, then finetune it on MedICaTSub for subfigure-subcaption alignment, which achieves alignment accuracy=73% on test set. We finally align 1,003,911 subfigure-subcaption pairs, along with the remaining 642,681 subfigure-caption pairs, we termed this dataset as PMC-OA. Note that, we have explicitly removed duplication between our data and ROCO by identify each image-caption pair with paperID and image source link. We consequently pretrain the PMC-CLIP on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Overview</head><p>In this section, we provide a brief statistical overview of the collected dataset PMC-OA with UMLS parser <ref type="bibr" target="#b0">[1]</ref> from three different perspectives, i.e., diagnostic procedure, diseases and findings, and fairness. First, PMC-OA covers a wide range of diagnostic procedures, spanning from common (CT, MRI) to rare ones (mitotic figure), which is more diverse than before (Fig. <ref type="figure" target="#fig_2">3(a)</ref>). Second, PMC-OA contains various diseases and findings, and is more up-to-date, covering new emergent diseases like COVID-19 (Fig. <ref type="figure" target="#fig_2">3(b)</ref>). And the wide disease coverage in our dataset supports learning the shared patterns of diseases, promoting accurate auto-diagnosis. Third, we also provide the sex-ratio across ages in Fig. <ref type="figure" target="#fig_2">3(c</ref>), as we can see PMC-OA is approximately gender-balanced, with 54% males. The fairness on population ensures our dataset sightly suffers from patient characteristic bias, thus providing greater cross-center generalize ability.</p><p>Discussion. Compared to pioneering works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">34]</ref> for constructing dataset based on PubMedCentral, our proposed PMC-OA is of larger scale, diversity, and has more accurate alignment: First, PMC-OA covers a wider range of papers (2.4M) than ROCO <ref type="bibr" target="#b27">[28]</ref>(1.8M) and MedICaT <ref type="bibr" target="#b34">[34]</ref>(131K), and thus enlarge our dataset(1.6M). Second, unlike ROCO <ref type="bibr" target="#b27">[28]</ref>, we maintain the nonradiology images, which makes PMC-OA a more diverse biomedical dataset as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Third, to the best of our knowledge, we are the first to integrate subfigures separation, subcaptions separation and the alignment into the data collection pipeline, which explicitly enlarges our dataset (8 times of MedICaT and 20 times of ROCO), while reducing the noise as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual-language Pre-training</head><p>With our constructed image-caption dataset, we further train a visual-language model, termed as PMC-CLIP as shown in Fig. <ref type="figure" target="#fig_1">2</ref> (bottom). We describe the architecture first and then introduce the two training objectives separately.</p><p>Architecture. Given N image-caption training pairs, i.e., D = {(I i , T i )| N i=1 }, where I i âˆˆ R HÃ—W Ã—C represents images, H, W, C are height, width, channel, and T i represents the paired text. We aim to train a CLIP-style visual-language model with an image encoder Î¦ visual and a text encoder Î¦ text .</p><p>In detail, given a specific image-caption pair (I, T ), we encode it separately with a ResNet-based Î¦ visual and a BERT-based Î¦ text , the embedding dimension is denoted as d and the text token length as l:</p><formula xml:id="formula_0">v = Î¦ visual (I) âˆˆ R d , (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">T = Î¦ text (T ) âˆˆ R lÃ—d , t = T 0 âˆˆ R d , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where v represents the embedding for the whole image, T refers to the sentence embedding, and t denotes the embedding for [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Text Contrastive Learning (ITC).</head><p>We implement ITC loss following CLIP <ref type="bibr" target="#b29">[29]</ref>, that aims to match the corresponding visual and text representations from one sample. In detail, denoting batch size as b, we calculate the softmaxnormalized cross-modality dot product similarity between the current visual/text embedding (v / t) and all samples within the batch, termed as p i2t , p t2i âˆˆ R b , and the final ITC loss is:</p><formula xml:id="formula_4">L ITC = E (I,T )âˆ¼D CE(y i2t , p i2t ) + CE(y t2i , p t2i ) ,<label>(3)</label></formula><p>where y i2t , y t2i refer to one-hot matching labels, CE refers to InfoNCE loss <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Language Modeling (MLM).</head><p>We implement MLM loss following BERT <ref type="bibr" target="#b6">[7]</ref>. The network is trained to reconstruct the masked tokens from context contents and visual cues. We randomly mask the word in texts with a probability of 15% and replace it with a special token '[MASK]'. We concatenate the image embedding v with the text token embeddings T , input it into a self-attention transformer-based fusion module Î¦ fusion , and get the prediction for the masked token at the corresponding position in the output sequence, termed as p mask = Î¦ fusion (v, T ). Let y mask denote the ground truth, and the MLM loss is:</p><formula xml:id="formula_5">L MLM = E (I,T )âˆ¼D CE(y mask , p mask )<label>(4)</label></formula><p>Total Training Loss. The final loss is defined as L = L ITC + Î»L MLM , where Î» is a hyper-parameter deciding the weight of L MLM , set as 0.5 by default.</p><p>Disscussion. While we recognize a lot of progress in VLP methodology <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, PMC-CLIP is trained in an essential way to demonstrate the potential of the collected PMC-OA, and thus should be orthogonal to these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-training Datasets</head><p>ROCO <ref type="bibr" target="#b27">[28]</ref> is a image-caption dataset collected from PubMed <ref type="bibr" target="#b31">[31]</ref>. It filters out all the compound or non-radiological images, and consists of 81K samples.</p><p>MedICaT <ref type="bibr" target="#b34">[34]</ref> extends ROCO to 217K samples (image-caption pairs), however, 75% of its figures are compound ones, i.e. one figure with multiple subfigures. <ref type="bibr" target="#b14">[15]</ref> is the largest chest X-ray dataset, containing 377,110 samples (image-report pairs). Each image is paired with a clinical report describing findings from doctors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-CXR</head><p>PMC-OA contains 1.65M image-text pairs, which we have explicitly conducted deduplication between ROCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Text Retrieval (ITR). ITR contains both image-to-text(I2T</head><p>) and text-to-image(T2I) retrieval. We train PMC-CLIP on different datasets, and sample 2,000 image-text pairs from ROCO's testset for evaluation, following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34]</ref>. Note that, as we have explicitly conducted deduplication, the results thus resemble zero-shot evaluation.</p><p>Classification. We finetune the model for different downstream tasks that focus on image classification. Spcifically, MedMINIST <ref type="bibr" target="#b37">[37]</ref> contains 12 tasks for 2D images, and it covers primary data modalities in biomedical images, including Colon Pathology, Dermatoscope, Retinal OCT, etc.</p><p>Visual Question Answering (VQA). We evaluate on the official dataset split of SLAKE <ref type="bibr" target="#b21">[22]</ref>, and follow previous work's split <ref type="bibr" target="#b23">[24]</ref> on VQA-RAD <ref type="bibr" target="#b17">[18]</ref>, where SLAKE is composed of 642 images and 14,028 questions and VQA-RAD contains 315 images and 3,515 questions. The questions in VQA-RAD and Slake are categorized as close-ended if answer choices are limited, otherwise openended. The image and text encoders are initialized from PMC-CLIP and finetuned, we refer the reader for more details in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For the visual and text encoders, we adopt ResNet50 <ref type="bibr" target="#b11">[12]</ref> and PubmedBERT <ref type="bibr" target="#b10">[11]</ref>. And we use 4 transformer layers for the fusion module. For input data, we resize each image to 224 Ã— 224. During pre-training, our text encoder is initialized from PubmedBERT, while the vision encoder and fusion module are trained from scratch. We use AdamW <ref type="bibr" target="#b22">[23]</ref> optimizer with lr = 1 Ã— 10 -4 . We train on GeForce RTX 3090 GPUs with batch size 128 for 100 epochs. The first 10 epochs are set for warming up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result</head><p>We conduct experiments to validate our proposed dataset, and the effectiveness of model trained on it. In Sec. 5.1, we first compare with existing large-scale biomedical datasets on the image-text retrieval task to demonstrate the superiority of PMC-OA. In Sect. 5.2, we finetune the model (pre-trained on PMC-OA) across three different downstream tasks, namely, retrieval, classification, and visual question answering. And we also perform a thorough empirical study of the pretraining objectives and the model architectures in Sect. 5.3. Note that, for all experiments, we use the default setting: ResNet50 for image encoder, and pre-train with both ITC and MLM objectives, unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PMC-OA surpasses SOTA large-scale biomedical dataset</head><p>As shown in Table <ref type="table">1</ref>, we pre-train PMC-CLIP on different datasets and evaluate retrieval on ROCO test set. The performance can be largely improved by simply switching to our dataset, confirming the significance of it.</p><p>Table <ref type="table">1</ref>. Ablation studies on pre-training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PMC-CLIP achieves SOTA across downstream tasks</head><p>To evaluate the learnt representation in PMC-CLIP, we compare it with several state-of-the-art approaches across various downstream tasks, including imagetext retrieval, image classification, and visual question answering.</p><p>Image-Text Retrieval. As shown in Table <ref type="table" target="#tab_1">2</ref>, we report a state-of-the-art result on image-text retrieval. On I2T Rank@10, PMC-CLIP outperforms previous state-of-the-art by 8.1%. It is worth mentioning that, the training set of ROCO has been used during pretraining in M3AE <ref type="bibr" target="#b3">[4]</ref>, ARL <ref type="bibr" target="#b4">[5]</ref>. While our dataset does not contain data from ROCO. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10</p><p>ViLT <ref type="bibr" target="#b15">[16]</ref> COCO <ref type="bibr" target="#b19">[20]</ref>, VG <ref type="bibr" target="#b16">[17]</ref>, SBU, GCC Image Classification. To demonstrate the excellent transferability of PMC-CLIP, we validate it on MedMNIST and compare it with SOTA methods i.e., DWT-CV <ref type="bibr" target="#b5">[6]</ref> and SADAE <ref type="bibr" target="#b9">[10]</ref>. We present the results of 3 of 12 sub-tests here, and the full results can be found in the supplementary material. As shown in Table <ref type="table" target="#tab_3">3</ref>, PMC-CLIP obtains consistently higher results, and it is notable that finetuning from PMC-CLIP achieves significant performance gains compared with training from scratch with ResNet.</p><p>Visual Question Answering. VQA requires model to learn finer grain visual and language representations. As Table <ref type="table" target="#tab_4">4</ref> shows, we surpass SOTA method M3AE in 5 out of 6 results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Training Objectives. We pre-train PMC-CLIP with different objectives (ITC, MLM) for ablation studies, and summarize the results in the supplementary material (Table <ref type="table">5</ref> in the supplementary). Here, we present a summary of the observations: First, ITC objective is essential for pretraining, and contributes most of the performance. Second, MLM using only text context works as a regularization term. Third, With incorporation of visual features, the model learns finer grain correlation between image-caption pairs, and achieve the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection Pipeline.</head><p>To demonstrate the effectiveness of subfiguresubcaption alignment, we compare PMC-CLIP with the model pretrained on dataset w/o alignment (Table <ref type="table">6</ref>(1-3) in the supplementary). The result verify that subfigure-subcaption alignment reduces dataset's noise thus enhance the pretrained model.</p><p>Visual Backbone. We have also explored different visual backbones, using the same setting as CLIP <ref type="bibr" target="#b29">[29]</ref> (Table <ref type="table">6</ref>(4-7) in the supplementary). We observe that all ResNet variants have close performance with RN50, outperforming ViT-B/32, potentially due to the large patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a large-scale dataset in biomedical domain, named PMC-OA, by collecting image-caption pairs from abundant scientific docu-ments. We train a CLIP-style model on PMC-OA, termed as PMC-CLIP, it achieves SOTA performance across various downstream biomedical tasks, including image-text retrieval, image classification, visual question answering. With the automatic collection pipeline, the dataset can be further expanded, which can be beneficial to the research community, fostering development of foundation models in biomedical domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Statistics over the pipeline and the collected PMC-OA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed pipeline to collect PMC-OA (upper) and the architecture of PMC-CLIP (bottom).</figDesc><graphic coords="3,55,98,172,25,340,18,274,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Statistical overview of PMC-OA.</figDesc><graphic coords="5,290,40,58,07,104,23,65,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Zero-shot Image-Text Retrieval on ROCO.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification results on MedMNIST.</figDesc><table><row><cell>Methods</cell><cell cols="3">PneumoniaMNIST BreastMNIST DermaMNIST AUCâ†‘ ACCâ†‘ AUCâ†‘ ACCâ†‘ AUCâ†‘ ACCâ†‘</cell></row><row><cell cols="2">ResNet50 [12] 96.20</cell><cell>88.40</cell><cell>86.60 84.20 91.20 73.10</cell></row><row><cell cols="2">DWT-CV [6] 95.69</cell><cell>88.67</cell><cell>89.77 85.68 91.67 74.75</cell></row><row><cell>SADAE [10]</cell><cell>98.30</cell><cell>91.80</cell><cell>91.50 87.80 92.70 75.90</cell></row><row><cell>PMC-CLIP</cell><cell>99.02</cell><cell>95.35</cell><cell>94.56 91.35 93.41 79.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>VQA results on VQA-RAD and Slake.</figDesc><table><row><cell>Methods</cell><cell>VQA-RAD Open Closed Overall Open Closed Overall Slake</cell></row><row><cell cols="2">MEVF-BAN [24] 49.20 77.20 66.10 77.80 79.80 78.60</cell></row><row><cell cols="2">CPRD-BAN [21] 52.50 77.90 67.80 79.50 83.40 81.10</cell></row><row><cell>M3AE [4]</cell><cell>67.23 83.46 77.01 80.31 87.82 83.25</cell></row><row><cell>PMC-CLIP</cell><cell>67.00 84.00 77.60 81.90 88.00 84.30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Follow Class TUI060 "Diagnostic Procedure" defined in UMLS<ref type="bibr" target="#b0">[1]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2022ZD0160702</rs>), <rs type="funder">STCSM</rs> (No. <rs type="grantNumber">22511106101</rs>, No. <rs type="grantNumber">18DZ2270700</rs>, No. <rs type="grantNumber">21DZ1100100</rs>), 111 plan (No. <rs type="grantNumber">BP0719010</rs>), and <rs type="funder">State Key Laboratory of UHD Video and Audio Production and Presentation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sASe4sG">
					<idno type="grant-number">2022ZD0160702</idno>
				</org>
				<org type="funding" xml:id="_ksFGgzB">
					<idno type="grant-number">22511106101</idno>
				</org>
				<org type="funding" xml:id="_HbnJWQW">
					<idno type="grant-number">18DZ2270700</idno>
				</org>
				<org type="funding" xml:id="_qsNjAD5">
					<idno type="grant-number">21DZ1100100</idno>
				</org>
				<org type="funding" xml:id="_Bm3pQAf">
					<idno type="grant-number">BP0719010</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 51.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The unified medical language system (umls): integrating biomedical terminology</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 16</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal masked autoencoders for medical visionand-language pre-training</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="679" to="689" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align, reason and learn: Enhancing medical vision-and-language pre-training with knowledge</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dwt-cv: Dense weight transfer-based cross validation strategy for model selection in biomedical data analysis</title>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><surname>Hulin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Qichang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yahui</surname></persName>
		</author>
		<author>
			<persName><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno>ArXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cogview2: Faster and better text-to-image generation via hierarchical transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<idno>ArXiv:2204.14217</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of training end-to-end vision-and-language transformers</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18166" to="18176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A self-adaptive discriminative autoencoder for medical applications</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8875" to="8886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Gu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><surname>Naoto</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Docfigure: A dataset for scientific document figure classification</title>
		<author>
			<persName><forename type="first">Ajoy</forename><surname>Kv Jobin</surname></persName>
		</author>
		<author>
			<persName><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">Alistair</forename><forename type="middle">Ew</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><surname>Akhilesh</surname></persName>
		</author>
		<author>
			<persName><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Caiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><surname>Chu Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V 13</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference</title>
		<meeting><address><addrLine>Strasbourg, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part II 24</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2019</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno>ArXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<idno>ArXiv:2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Workshop on Large-scale Annotation of Biomedical Data and Expert Label Synthesis (LABELS) 2018</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<idno>ArXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pubmed central: The genbank of the published literature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exsclaim!-an automated pipeline for the construction of labeled materials imaging datasets from literature</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Schwenker</surname></persName>
		</author>
		<idno>ArXiv:2103.10631</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Medicat: A dataset of medical images, captions, and textual references</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Medclip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">Contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Medklip: Medical knowledge enhanced language-image pre-training</title>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification</title>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pre-training</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00534</idno>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
