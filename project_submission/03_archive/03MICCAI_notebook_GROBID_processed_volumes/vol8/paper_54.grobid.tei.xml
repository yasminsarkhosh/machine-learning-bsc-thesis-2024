<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</title>
				<funder ref="#_sySJHye">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_nZZpYYt">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pingyi</forename><surname>Chen</surname></persName>
							<email>chenpingyi@westlake.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenglu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyi</forename><surname>Shui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiatong</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunyi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shichuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Yang</surname></persName>
							<email>yanglin@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Industries of the Future</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="559" to="568"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">468953C9E5DA7404068A3BEDE905FD6F</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised method</term>
					<term>Self-supervised learning</term>
					<term>Cell recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior selfactivation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cell recognition serves a key role in exploiting pathological images for disease diagnosis. Clear and accurate cell shapes provide rich details: nucleus structure, cell counts, and cell density of distribution. Hence, pathologists are able to conduct a reliable diagnosis according to the information from the segmented cell, which also improves their experience of routine pathology workflow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>In recent years, the advancement of deep learning has facilitated significant success in medical images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>. However, the supervised training requires massive manual labels, especially when labeling cells in histopathology images. A large number of cells are required to be labeled, which results in inefficient and expensive annotating processes. It is also difficult to achieve accurate labeling because of the large variations among different cells and the variability of reading experiences among pathologists.</p><p>Work has been devoted to reducing dependency on manual annotations recently. Qu et al. use points as supervision <ref type="bibr" target="#b18">[19]</ref>. It is still a labor-intensive task due to the large number of objects contained in a pathological image. With regard to unsupervised cell recognition, traditional methods can segment the nuclei by clustering or morphological processing. But these methods suffer from worse performance than deep learning methods. Among AI-based methods, some works use domain adaptation to realize unsupervised instance segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>, which transfers the source domain containing annotations to the unlabeled target. However, their satisfactory performance depends on the appropriate annotated source dataset. Hou et al. <ref type="bibr" target="#b9">[10]</ref> proposed to synthesize training samples with GAN. It relies on predefined nuclei texture and color. Feng et al. <ref type="bibr" target="#b5">[6]</ref> achieved unsupervised detection and segmentation by a mutual-complementing network. It combines the advantage of correlation filters and deep learning but needs iterative training and finetuning.</p><p>CNNs with inductive biases have priority over local features of the nuclei with dense distribution and semi-regular shape. In this paper, we proposed a simple but effective framework for unsupervised cell recognition. Inspired by the strong representation capability of self-supervised learning, we devised the prior self-activation maps (PSM) as the supervision for downstream cell recognition tasks. Firstly, the activation network is initially trained with self-supervised learning like predicting instance-level contrastiveness. Gradient information accumulated in the shallow layers of the activation network is then calculated and aggregated with the raw input information. These features extracted from the activation network are then clustered to generate pseudo masks which are used for downstream cell recognition tasks. In the inferring stage, the networks which are supervised by pseudo masks are directly applied for cell detection or segmentation. To evaluate the effectiveness of PSM, we evaluated our method on two datasets. Our framework achieved comparable performance on cell detection and segmentation on par with supervised methods. Code is available at https://github.com/cpystan/PSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The structure of our proposed method is demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Firstly, an activation network U ss is trained with self-supervised learning. After the backpropagation of gradients, gradient-weighted features are exploited to generate the self-activation maps (PSM). Next is semantic clustering where the PSM is combined with the raw input to generate pseudo masks. These pseudo masks can be used as supervision for downstream tasks. Related details are discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proxy Task</head><p>We introduce self-supervised learning to encourage the network to focus on the local features in the image. And our experiments show that neural networks are capable of adaptively recognizing nuclei with dense distribution and semi-regular shape. Here, we have experimented with several basic proxy tasks below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet Pre-training:</head><p>It is straightforward to exploit the models pre-trained on natural images. In this strategy, we directly extract the gradient-weighted feature map in the ImageNet pre-trained network and generate prior self-activation maps.</p><p>Contrastiveness: Following the contrastive learning <ref type="bibr" target="#b2">[3]</ref> methods, the network is encouraged to distinguish between different patches. For each image, its augmented view will be regarded as the positive sample, and the other image sampled in the training set is defined as the negative sample. The network is trained to minimize the distance between positive samples. It also maximizes the distance between the negative sample and the input image. The optimization goal can be denoted as:</p><formula xml:id="formula_0">L dis (Z l , Z r , Z n ) = dif f (Z l , Z r ) -dif f (Z l , Z n ),<label>(1)</label></formula><p>where L dis is the loss function. Z l , Z r , and Z n are representations of the input sample, the positive sample, and the negative sample, respectively. In addition, dif f (•) is a function that measures the difference of embeddings.</p><p>Similarity: LeCun et al. <ref type="bibr" target="#b3">[4]</ref> proposed a Siamese network to train the model with a similarity metric. We also adopted a weight-shared network to learn the similarity discrimination task. In specific, the pair of samples (each input and its augmented view) will be fed to the network, and then embedded as highdimensional vectors Z l and Z r in the high-dimensional space, respectively. Based on the similarity measure sim(•), L dis is introduced to reduce the distance, which is denoted as,</p><formula xml:id="formula_1">L dis (Z l , Z r ) = -sim(Z l , Z r ) = dif f (Z l , Z r ).<label>(2)</label></formula><p>Here, maximizing the similarity of two embeddings is equal to minimizing their difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior Self-activation Map</head><p>The self-supervised model U ss is constructed by sequential blocks which contain several convolutional layers, batch normalization layers, and activation layers.</p><p>The self-activation map of a certain block can be obtained by nonlinearly mapping the weighted feature maps A k :</p><formula xml:id="formula_2">I am = ReLU ( k α k A k ),<label>(3)</label></formula><p>where I am is the prior self-activation map. A k indicates the k-th feature map in the selected layer. α k is the weight of each feature map, which is defined by global-average-pooling the gradients of output z with regard to A k :</p><formula xml:id="formula_3">α k = 1 N i j ∂z ∂A k ij ,<label>(4)</label></formula><p>where i, j denote the height and width of output, respectively, and N indicates the input size. The obtained features are visualized in the format of the heat map which is later transformed to pseudo masks by clustering.</p><p>Semantic Clustering. We construct a semantic clustering module (SCM) which converts prior self-activation maps to pseudo masks. In SCM, the original information is included to strengthen the detailed features. It is defined as:</p><formula xml:id="formula_4">I f = I am + β • I raw ,<label>(5)</label></formula><p>where I f denotes the fused semantic map, I raw is the raw input, β is the weight of I raw .</p><p>To generate semantic labels, an unsupervised clustering method K-Means is selected to directly split all pixels into several clusters and obtain foreground and background pixels. Given the semantic map I f and its N pixel features</p><formula xml:id="formula_5">F = {f i |i ∈ {1, 2, ...N }}, N features are partitioned into K clusters S = {S i |i ∈ {1, 2, ...K}}, K &lt; N.</formula><p>The goal is to find S to reach the minimization of withinclass variances as follows:</p><formula xml:id="formula_6">min K i=1 fj ∈Si ||f j -c i || 2 , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where c i denotes the centroid of each cluster S i . After clustering, the pseudo mask I sg can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Downstream Tasks</head><p>In this section, we introduce the training and inferring of cell recognition models.</p><p>Cell Detection. For the task of cell detection, a detection network is trained under the supervision of pseudo mask I sg . In the inferring stage, the output of the detection network is a score map. Then, it is post-processed to obtain the detection result.</p><p>The coordinates of cells can be got by searching local extremums in the score map, which is described below:</p><formula xml:id="formula_8">T (m,n) = 1, p (m,n) &gt; p (i,j) , ∀(i, j) ∈ D (m,n) , 0, otherwise,<label>(7)</label></formula><p>where T (m,n) denotes the predicted label at location of (m, n), p is the value of the score map and D (m,n) indicates the neighborhood of point (m, n). T (m,n) is exactly the detection result.</p><p>Cell Segmentation. Due to the lack of instance-level supervision, the model does not perform well in distinguishing adjacent objects in the segmentation.</p><p>To further reduce errors and uncertainties, the Voronoi map I vor which can be transformed from I sg is utilized to encourage the model to focus on instance-wise features. In the Voronoi map, the edges are labeled as background and the seed points are denoted as foreground. Other pixels are ignored.</p><p>We train the segmentation model with these two types of labels. The training loss function can be formulated as below,</p><formula xml:id="formula_9">L = λ[y log I vor + (1 -y) log(1 -I vor )] + (1 -y) log(1 -I sg ), (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where λ is the partition enhancement coefficient. In our experiment, we discovered that false positives hamper the effectiveness of segmentation due to the ambiguity of cell boundaries. Since that, only the background of I sg will be concerned to eliminate the influence of false positives in instance identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Dataset. We validated the proposed method on the public dataset of Multi-Organ Nuclei Segmentation (MoNuSeg) <ref type="bibr" target="#b12">[13]</ref> and Breast tumor Cell Dataset (BCData) <ref type="bibr" target="#b10">[11]</ref>. MoNuSeg consists of 44 images of size 1000 × 1000 with around 29,000 nuclei boundary annotations. BCData is a public large-scale breast tumor dataset containing 1338 immunohistochemically Ki-67 stained images of size 640 × 640. Evaluation Metrics. In our experiments on MoNuSeg, F1-score and IOU are employed to evaluate the segmentation performance. Denote T P , F P , and F N as the number of true positives, false positives, and false negatives. Then F1score and IOU can be defined as: F 1 = 2T P/(2T P + F P + F N), IOU = T P/(T P + F P + F N). In addition, common object-level indicators such as Dice coefficient and Aggregated Jaccard Index (AJI) <ref type="bibr" target="#b12">[13]</ref> are also considered to assess the segmentation performance.</p><p>In the experiment on BCData, precision (P), recall (R), and F1-score are used to evaluate the detection performance. Predicted points will be matched to ground-truth points one by one. And those unmatched points are regarded as false positives. Precision and recall are: P = T P/(T P + F P ), and R = T P/(T P + F N). In addition, we introduce MP and MN to evaluate the cell counting results. 'MP' and 'MN' denote the mean average error of positive and negative cell numbers.</p><p>Hyperparameters. Res2Net101 <ref type="bibr" target="#b6">[7]</ref> is adopted as the activation network U ss with random initialization of parameters. The positive sample is augmented by rotation. The weights β are set to 2.5 and 4 for training in MoNuSeg and BCData, respectively. The weight λ is 0.5. The analysis for β and λ is included in the supplementary. Pixels of the fused semantic map will be decoupled into three piles by K-Means. The following segmentation and detection are constructed with ResNet-34. They are optimized using CrossEntropy loss by the Adam optimizer for 100 epochs with the initial learning rate of 1e -4 . The function dif f (•) is instantiated as the measurement of Manhattan distance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Result</head><p>This section includes the discussion of results which are visualized in Fig. <ref type="figure" target="#fig_1">2</ref> Segmentation. In MoNuSeg Dataset, four fully-supervised methods Unet <ref type="bibr" target="#b19">[20]</ref>, MedT <ref type="bibr" target="#b23">[24]</ref>, CDNet <ref type="bibr" target="#b7">[8]</ref>, and the competition winner <ref type="bibr" target="#b12">[13]</ref> are adopted to estimate the upper limit as shown in the first four rows of Table <ref type="table" target="#tab_0">1</ref>. Two weakly-supervised models trained with only point annotations are also adopted as the comparison.</p><p>Compared with the method <ref type="bibr" target="#b21">[22]</ref> fully exploiting localization information, ours can achieve better performance without any annotations in object-level metrics (AJI). In addition, two unsupervised methods using traditional image processing tools <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> and two unsupervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> with deep learning are compared. Our framework has achieved promising performance because robust low-level features are exploited to generate high-quality pseudo masks.</p><p>Detection. Following the benchmark of BCData, metrics of detection and counting are adopted to evaluate the performance as shown in Table <ref type="table" target="#tab_1">2</ref>. The first three methods are fully supervised methods which predict probability maps to achieve detection. Furthermore, TransCrowd <ref type="bibr" target="#b15">[16]</ref> with the backbone of Swin-Transformer is employed as the weaker supervision trained by cell counts regression. By con-  trast, even without any annotation supervision, compared to CSRNet <ref type="bibr" target="#b14">[15]</ref>, NP-CNN <ref type="bibr" target="#b22">[23]</ref> and U-CSRNet <ref type="bibr" target="#b10">[11]</ref>, our proposed method still achieved comparable performance. Especially in terms of MP, our model surpasses all the baselines. It is challenging to realize multi-class recognition in an unsupervised framework. Our method still achieves not bad counting results on negative cells.</p><p>Ablation Study. Ablation experiments are built on MoNuSeg. In our pipeline, the activation network can be divided into four layers which consists of multiple basic units including ReLU, BacthNorm, and Convolution. We exploit the prior self-activation maps generated from different depths in the model after training with the same proxy tasks. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the performance goes down and up with we extracting features from deeper layers. Due to the relatively small receptive field, the shallowest layer in the activation network is the most capable to translate local descriptions We have also experimented with different types of proxy tasks in a selfsupervised manner, as shown in Table <ref type="table" target="#tab_2">3</ref>. We can see that relying on the pretrained models with external data can not improve the results of subsequent segmentation. The model achieves similar pixel-level performance (F1) when learning similarity or contrastiveness. But similarity learning makes the model performs better in object-level metrics (AJI) than contrastive learning. The high intra-domain similarity hinders the comparison between constructed image pairs. Unlike natural image datasets containing diverse samples, the minor inter class differences in biomedical images may not fully exploit the superiority of contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed the prior self-activation map (PSM) based framework for unsupervised cell segmentation and multi-class detection. The framework is composed of an activation network, a semantic clustering module (SCM), and the networks for cell recognition. The proposed PSM has a strong capability of learning low-level representations to highlight the area of interest without the need for manual labels. SCM is designed to serve as a pipeline between representations from the activation network and the downstream task. And our segmentation and detection network are supervised by the pseudo masks. In the whole training process, no manual annotation is needed. Our unsupervised method was evaluated on two publicly available datasets and obtained competitive results compared to the methods with annotations. In the future, we will apply our PSM to other types of medical images to further release the dependency on annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of our proposed model. (a) The top block shows the activation network which aggregates gradient information to generate self-activation maps. The bottom block presents the work process of semantic clustering. (b-c) The right part is the cell detection and the cell segmentation network which are supervised by the pseudo masks.</figDesc><graphic coords="3,65,13,53,81,330,46,123,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization. (a) Typical results of multi-class detection, the red dot and green dot represent positive and negative cells respectively. (b) Typical results of cell segmentation. (Color figure online)</figDesc><graphic coords="7,56,97,175,31,338,11,108,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Analysis of the depth of the extracted layer.</figDesc><graphic coords="8,46,38,53,78,151,24,102,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on MoNuSeg. According to the requirements of each method, various labels are used: localization (Loc) and contour (Cnt). * indicates the model is trained from scratch with the same hyperparameter as ours.</figDesc><table><row><cell>Methods</cell><cell cols="2">Loc Cnt Pixel-level</cell><cell cols="2">Object-level</cell></row><row><cell></cell><cell cols="4">IoU F1 score Dice AJI</cell></row><row><cell>Unet* [20]</cell><cell cols="2">0.606 0.745</cell><cell cols="2">0.715 0.511</cell></row><row><cell>MedT [24]</cell><cell cols="2">0.662 0.795</cell><cell>-</cell><cell>-</cell></row><row><cell>CDNet [8]</cell><cell>-</cell><cell>-</cell><cell cols="2">0.832 0.633</cell></row><row><cell>Competition Winner [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.691</cell></row><row><cell>Qu et al. [19]</cell><cell cols="2">0.579 0.732</cell><cell cols="2">0.702 0.496</cell></row><row><cell>Tian et al. [22]</cell><cell cols="2">0.624 0.764</cell><cell cols="2">0.713 0.493</cell></row><row><cell>CellProfiler [1]</cell><cell>-</cell><cell>0.404</cell><cell cols="2">0.597 0.123</cell></row><row><cell>Fiji [21]</cell><cell>-</cell><cell>0.665</cell><cell cols="2">0.649 0.273</cell></row><row><cell>CyCADA [9]</cell><cell>-</cell><cell>0.705</cell><cell>-</cell><cell>0.472</cell></row><row><cell>Hou et al. [10]</cell><cell>-</cell><cell>0.750</cell><cell>-</cell><cell>0.498</cell></row><row><cell>Ours</cell><cell cols="2">0.610 0.762</cell><cell cols="2">0.724 0.542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on BCData. According to the requirements of each method, various labels are used: localization (Loc) and the number (Num) of cells.</figDesc><table><row><cell>Methods</cell><cell>Loc Num Backbone</cell><cell>P</cell><cell>R</cell><cell cols="2">F1 score MP↓ MN↓</cell></row><row><cell>CSRNet [15]</cell><cell>ResNet50</cell><cell cols="3">0.824 0.834 0.829</cell><cell>9.24 24.90</cell></row><row><cell>SC-CNN [23]</cell><cell>ResNet50</cell><cell cols="3">0.770 0.828 0.798</cell><cell>9.18 20.60</cell></row><row><cell>U-CSRNet [11]</cell><cell>ResNet50</cell><cell cols="3">0.869 0.857 0.863</cell><cell>10.04 18.09</cell></row><row><cell>TransCrowd [16]</cell><cell cols="2">Swin-Transformer -</cell><cell>-</cell><cell>-</cell><cell>13.08 33.10</cell></row><row><cell>Ours</cell><cell>ResNet34</cell><cell cols="3">0.855 0.771 0.811</cell><cell>8.89 28.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The training strategy analysis</figDesc><table><row><cell>Training Strategy</cell><cell>F1</cell><cell>AJI</cell></row><row><cell cols="3">ImageNet Pretrained 0.658 0.443</cell></row><row><cell>Similarity</cell><cell cols="2">0.750 0.542</cell></row><row><cell>Contrastiveness</cell><cell cols="2">0.741 0.498</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This study was partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant no. <rs type="grantNumber">92270108</rs>), <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (Grant no. <rs type="grantNumber">XHD23F0201</rs>), and the <rs type="institution">Research Center for Industries of the Future (RCIF) at Westlake University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sySJHye">
					<idno type="grant-number">92270108</idno>
				</org>
				<org type="funding" xml:id="_nZZpYYt">
					<idno type="grant-number">XHD23F0201</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 54.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CellProfiler: image analysis software for identifying and quantifying cell phenotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<idno type="DOI">10.1186/gb-2006-7-10-r100</idno>
		<ptr target="https://doi.org/10.1186/gb-2006-7-10-r100" />
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synergistic image and feature adaptation: towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pathological prognostic factors in breast cancer. I. The value of histological grade in breast cancer: experience from a large study with long-term follow-up</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Elston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Histopathology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutual-complementing framework for nuclei detection and segmentation in pathology image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4036" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Res2Net: a new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CDNet: centripetal direction network for nuclear instance segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CyCADA: cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust histopathology image analysis: to label or to synthesize?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BCData: a large-scale dataset and benchmark for cell detection and counting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-128" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diversify and match: a domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prognostic value of histologic grade nuclear components of Scarff-Bloom-Richardson (SBR). An improved score modification based on a multivariate analysis of 1262 invasive ductal breast carcinomas</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Doussal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tubiana-Hulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hacene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spyratos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1914" to="1921" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CSRNet: dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TransCrowd: weakly-supervised crowd counting with transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11432-021-3445-y</idno>
		<ptr target="https://doi.org/10.1007/s11432-021-3445-y" />
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">160104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nuclei segmentation via a deep panoptic model with semantic feature fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="861" to="868" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation of nuclei in histopathology images by deep regression of the distance map</title>
		<author>
			<persName><forename type="first">P</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="448" to="459" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised deep nuclei segmentation using points annotation in histopathology images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fiji: an open-source platform for biological-image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schindelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="676" to="682" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-supervised nucleus segmentation based on point annotations: a coarse-to-fine self-stimulated learning strategy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-129" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prior information guided regularized deep learning for cell nucleus detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K P</forename><surname>Vanamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2047" to="2058" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
