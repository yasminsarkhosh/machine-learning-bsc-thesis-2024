<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations</title>
				<funder ref="#_MpEqREE">
					<orgName type="full">ERA-NET NEURON Cofund</orgName>
				</funder>
				<funder ref="#_MfczZJ4">
					<orgName type="full">Nvidia</orgName>
				</funder>
				<funder ref="#_UAdvTWg #_G7RPj4X">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_8C6kaz5">
					<orgName type="full">Bavarian State Ministry for Science and Art (Collaborative Bilateral Research Program Bavaria -Québec: AI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcginnis</surname></persName>
							<email>julian.mcginnis@tum.de</email>
							<idno type="ORCID">0009-0000-2224-7600</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">TUM-Neuroimaging Center</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suprosanna</forename><surname>Shit</surname></persName>
							<email>suprosanna.shit@tum.de</email>
							<idno type="ORCID">0000-0003-4435-7207</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongwei</forename><forename type="middle">Bran</forename><surname>Li</surname></persName>
							<idno type="ORCID">0000-0002-5328-6407</idno>
							<affiliation key="aff4">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vasiliki</forename><surname>Sideri-Lampretsa</surname></persName>
							<idno type="ORCID">0000-0003-0135-7442</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Graf</surname></persName>
							<idno type="ORCID">0000-0001-6656-3680</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maik</forename><surname>Dannecker</surname></persName>
							<idno type="ORCID">0000-0001-9012-9606</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiazhen</forename><surname>Pan</surname></persName>
							<idno type="ORCID">0000-0002-6305-8117</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nil</forename><surname>Stolt-Ansó</surname></persName>
							<idno type="ORCID">0009-0001-4457-0967</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Mühlau</surname></persName>
							<idno type="ORCID">0000-0002-9545-2709</idno>
							<affiliation key="aff1">
								<orgName type="department">TUM-Neuroimaging Center</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">S</forename><surname>Kirschke</surname></persName>
							<idno type="ORCID">0000-0002-7557-0003</idno>
							<affiliation key="aff3">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<idno type="ORCID">0000-0002-5683-5889</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benedikt</forename><surname>Wiestler</surname></persName>
							<idno type="ORCID">0000-0002-2963-7772</idno>
							<affiliation key="aff3">
								<orgName type="department">Department of Neuroradiology</orgName>
								<orgName type="institution">TU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="173" to="183"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">51F90587B67FC2B887631432C02CAFE4</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-contrast Super-resolution</term>
					<term>Implicit Neural Representations</term>
					<term>Mutual Information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clinical routine and retrospective cohorts commonly include multi-parametric Magnetic Resonance Imaging; however, they are mostly acquired in different anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints. Thus acquired views suffer from poor outof-plane resolution and affect downstream volumetric image analysis that typically requires isotropic 3D scans. Combining different views of multi-contrast scans into high-resolution isotropic 3D scans is challenging due to the lack of a large training cohort, which calls for a subjectspecific framework. This work proposes a novel solution to this problem leveraging Implicit Neural Representations (INR). Our proposed INR jointly learns two different contrasts of complementary views in a continuous spatial function and benefits from exchanging anatomical information between them. Trained within minutes on a single commodity GPU, our model provides realistic super-resolution across different pairs of contrasts in our experiments with three datasets. Using Mutual Information (MI) as a metric, we find that our model converges to an optimum MI amongst sequences, achieving anatomically faithful reconstruction. (Code is available at: https://github.com/jqmcginnis/ multi_contrast_inr/).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In clinical practice, Magnetic Resonance Imaging (MRI) provides important information for diagnosing and monitoring patient conditions <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b16">16]</ref>. To capture the complex pathophysiological aspects during disease progression, multiparametric MRI (such as T1w, T2w, DIR, FLAIR) is routinely acquired. Image acquisition inherently poses a trade-off between scan time, resolution, and signalto-noise ratio (SNR) <ref type="bibr" target="#b19">[19]</ref>. To maximize the source of information within a reasonable time budget, clinical protocol often combines anisotropic 2D scans of different contrasts in complementary viewing directions. Although acquired 2D scans offer an excellent in-plane resolution, they lack important details in the orthogonal out-of-plane. For a reliable pathological assessment, radiologists often resort to a second scan of a different contrast in the orthogonal viewing direction. Furthermore, poor out-of-plane resolution significantly affects the accuracy of volumetric downstream image analysis, such as radiomics and lesion volume estimation, which usually require isotropic 3D scans. As multi-parametric isotropic 3D scans are not always feasible to acquire due to time-constraints <ref type="bibr" target="#b19">[19]</ref>, motion <ref type="bibr" target="#b9">[9]</ref>, and patient's condition <ref type="bibr" target="#b10">[10]</ref>, super-resolution offers a convenient alternative to obtain the same from anisotropic 2D scans. Recently, it has been shown that acquiring three complementary 2D views of the same contrast may yield higher SNR at reduced scan time <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b29">29]</ref>. However, it remains under-explored if orthogonal anisotropic 2D views of different contrasts can benefit from each other based on the underlying anatomical consistency. Additionally, whether such strategies can further decrease scan times while preserving similar resolution and SNR remains unanswered. Moreover, unlike conventional super-resolution models trained on a cohort, a personalized model is of clinical relevance to avoid the danger of potential misdiagnosis caused by cohort-learned biases. In this work, we mitigate these gaps by proposing a novel multi-contrast super-resolution framework that only requires the patient-specific low-resolution MR scans of different sequences (and views) as supervision. As shown in various settings, our approach is not limited to specific contrasts or views but provides a generic framework for super-resolution. The contributions in this paper are three-fold: 1. To the best of our knowledge, our work is the first to enable subject-specific multi-contrast super-resolution from low-resolution scans without needing any high-resolution training data. We demonstrate that Implicit Neural Representations (INR) are good candidates to learn from complementary views of multi-parametric sequences and can efficiently fuse low-resolution images into anatomically faithful super-resolution. 2. We introduce Mutual Information (MI) <ref type="bibr" target="#b26">[26]</ref> as an evaluation metric and find that our method preserves the MI between high-resolution ground truths in its predictions. Further observation of its convergence to the ground truth value during training motivates us to use MI as an early stopping criterion. 3. We extensively evaluate our method on multiple brain MRI datasets and show that it achieves high visual quality for different contrasts and views and preserves pathological details, highlighting its potential clinical usage.</p><p>Related Work. Single-image super-resolution (SISR) aims at restoring a highresolution (HR) image from a low-resolution (LR) input from a single sequence and targets applications such as low-field MR upsampling or optimization of MRI acquisition <ref type="bibr" target="#b3">[3]</ref>. Recent methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b8">8]</ref> incorporate priors learned from a training set <ref type="bibr" target="#b3">[3]</ref>, which is later combined with generative models <ref type="bibr" target="#b1">[2]</ref>. On the other hand, multi-image super-resolution (MISR) relies on the information from complementary views of the same sequence <ref type="bibr" target="#b29">[29]</ref> and is especially relevant to capturing temporal redundancy in motion-corrupted low-resolution MRI <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b27">27]</ref>. Multi-contrast Super-resolution (MCSR) targets using inter-contrast priors <ref type="bibr" target="#b20">[20]</ref>. In conventional settings <ref type="bibr" target="#b15">[15]</ref>, an isotropic HR image of another contrast is used to guide the reconstruction of an anisotropic LR image. Zeng et al. <ref type="bibr" target="#b30">[30]</ref> use a two-stage architecture for both SISR and MCSR. Utilizing a feature extraction network, Lyu et al. <ref type="bibr" target="#b14">[14]</ref> learn multi-contrast information in a joint feature space. Later, multi-stage integration networks <ref type="bibr" target="#b6">[6]</ref>, separatable attention <ref type="bibr" target="#b7">[7]</ref> and transformers <ref type="bibr" target="#b13">[13]</ref> have been used to enhance joint feature space learning. However, all current MCSR approaches are limited by their need for a large training dataset. Consequently, this constrains their usage to specific resolutions and further harbors the danger of hallucination of features (e.g., lesions, artifacts) present in the training set and does not generalize well to unseen data.</p><p>Originating from shape reconstruction <ref type="bibr" target="#b18">[18]</ref> and multi-view scene representations <ref type="bibr" target="#b17">[17]</ref>, Implicit Neural Representations (INR) have achieved state-of-the-art results by modeling a continuous function on a space from discrete measurements. Key reasons behind INR's success can be attributed to overcoming the low-frequency bias of Multi-Layer Perceptrons (MLP) <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. Although MRI is a discrete measurement, the underlying anatomy is a continuous space. We find INR to be a good fit to model a continuous intensity function on the anatomical space. Once learned, it can be sampled at an arbitrary resolution to obtain the super-resolved MRI. Following this spirit, INRs have recently been successfully employed in medical imaging applications ranging from k-space reconstruction <ref type="bibr" target="#b11">[11]</ref> to SISR <ref type="bibr" target="#b29">[29]</ref>. Unlike <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b29">29]</ref>, which learn anatomical priors in single contrasts, and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">28]</ref>, which leverage INR with latent embeddings learned over a cohort, we focus on employing INR in subject-specific, multi-contrast settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we first formally introduce the problem of joint super-resolution of multi-contrast MRI from only one image per contrast per patient. Next, we describe strategies for embedding information from two contrasts in a shared space. Subsequently, we detail our model architecture and training configuration.</p><p>Problem Statement. We denote the collection of all 3D coordinates of interest in this anatomical space as Ω = {(x, y, z)} with anatomical function q : Ω → A. The image intensities are a function of the underlying anatomical properties A. Two contrasts C 1 and C 2 can be scanned in a low-resolution subspace Ω 1 , Ω 2 ⊂ Ω. Let us consider g 1 , g 2 : A → R that map from anatomical properties to contrast intensities C 1 and C 2 , respectively. We obtain sparse observations</p><formula xml:id="formula_0">I 1 = {g 1 (q(x)) = f 1 (x); ∀x ∈ Ω 1 } and I 2 = {g 2 (q(x)) = f 2 (x); ∀x ∈ Ω 2 }</formula><p>, where f i is composition of g i and q. However, one can easily obtain the global anatomical space Ω by knowing Ω 1 and Ω 2 , e.g., by rigid registration between the two images. In this paper, we aim to estimate f 1 , f 2 : Ω → R given I 1 and I 2 .</p><p>Joint Multi-contrast Modelling. Since both component-functions f 1 and f 2 operate on a subset of the same input space, we argue that it is beneficial to model them jointly as a single function f : Ω → R 2 and optimize it based on their estimation error incurred in their respective subsets. This will enable information transfer from one contrast to another, thus improving the estimation and preventing over-fitting in single contrasts, bringing consistency to the prediction.</p><p>To this end, we propose to leverage INR to model a continuous multi-contrast function f from discretely sampled sparse observations I 1 and I 2 .</p><p>MCSR Setup. Without loss of generalization, let us consider two LR input contrasts scanned in two orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. We assume they are aligned by rigid registration requiring no coordinate transformation. Their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s 2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. Note that s 1 &lt; t 1 and s 2 &lt; t 2 imply high in-plane and low out-of-plane resolution. In the end, we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1 , s 2 .</p><p>Implicit Neural Representations for MCSR. We intend to project the information available in one contrast into another by embedding both in the shared weight space of a neural network. However, a high degree of weight sharing could hinder contrast-specific feature learning. Based on this reasoning, we aim to hit the sweet spot where maximum information exchange can be encouraged without impeding contrast-specific expressiveness. We propose a split-head architecture, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where the initial layers jointly learn the common anatomical features, and subsequently, two heads specialize in contrast-specific information. The model takes Fourier <ref type="bibr" target="#b25">[25]</ref> Features v = [cos(2πBx), sin(2πBx)] T as input and predicts [ Î1 , Î2 ] = f (v), where x = (x, y, z) and B is sampled from a Gaussian distribution N (μ, σ 2 ). We use mean-squared error loss, L MSE , for training.</p><formula xml:id="formula_1">L MSE = α x∈Ω1 ( Î1 (x) -I 1 (x)) 2 + β x∈Ω2 ( Î2 (x) -I 2 (x)) 2<label>(1)</label></formula><p>where α and β are coefficients for the reconstruction loss of two contrasts. Note that for points {(x, y, z)} ∈ Ω 2 \ Ω 1 , there is no explicit supervision coming from low resolution C 1 . For these points, one can interpret learning C 1 from the loss in C 2 , and vice versa, to be a weakly supervised task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and Training.</head><p>Given the rigidly registered LR images, we compute Ω 1 , Ω 2 ∈ Ω in the scanner reference space using their affine matrices. Subsequently, we normalize Ω to the interval [-1, 1] 3 and independently normalize each contrast's intensities to [0, 1]. We use 512-dimensional Fourier Features in the input. Our model consists of a four-layer MLP with a hidden dimension of 1024 for the shared layers and two layers with a hidden dimension of 512 for the heads. We use Adam optimizer with a learning rate of 4e-4 and a Cosine annealing rate scheduler with a batch size of 1000. For the multi-contrast INR models, we use MI as in Eq. 2 for early stopping. Implemented in PyTorch, we train our model on a single A6000 GPU. Please refer to Table <ref type="table">3</ref> in supplementary for an exhaustive hyper-parameter search.</p><p>Model Selection and Inference. Since our model is trained on sparse sets of coordinates, it is prone to overfitting them and has little incentive to generalize in out-of-plane predictions for single contrast settings. A remedy to this is to hold random points as a validation set. However, this will reduce the number of training samples and hinder the reconstruction of fine details. For multi-contrast settings, one can exploit the agreement between the two predicted contrasts. Ideally, the network should reach an equilibrium between the contrasts over the training period, where both contrasts optimally benefit from each other. We empirically show that Mutual Information (MI) <ref type="bibr" target="#b26">[26]</ref> is a good candidate to capture such an equilibrium point without the need for ground truth data in its computation. For two predicted contrasts Î1 and Î2 , MI can be expressed as:</p><formula xml:id="formula_2">MI( Î1 , Î2 ) = y∈ Î2 x∈ Î1 P ( Î1, Î2) (x, y) log P ( Î1, Î2) (x, y) P Î1 (x)P Î2 (y)<label>(2)</label></formula><p>Compared to image registration, we do not use MI as a loss for aligning two images; instead, we use it as a quantitative assessment metric. Given two ground truth HR images for a subject, one can compute the optimum state of MI. We observe that the MI between our model predictions converges close to such an optimum state over the training period without any explicit knowledge about it, c.f. Fig. <ref type="figure">3</ref> in the supplementary. This observation motivates us to detect a plateau in MI between the predicted contrasts and use it as a stopping criterion for model selection in multi-contrast INR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets. To enable fair evaluation between our predictions and the reference HR ground truths, the in-plane SNR between the LR input scan and corresponding ground truth has to match. To synthetically create 2D LR images, it is necessary to downsample out-of-plane in the image domain anisotropically <ref type="bibr" target="#b32">[32]</ref> while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical protocol, which often has higher in-plane details than that of 3D scans, we use spline interpolation to model partial volume and downsampling. We demonstrate our network's modeling capabilities for different contrasts (T1w, T2w, FLAIR, DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We conduct experiments on two public datasets, BraTS <ref type="bibr" target="#b16">[16]</ref>, and MSSEG <ref type="bibr" target="#b4">[4]</ref>, and an in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that fulfill the isotropic acquisition criteria for both ground truth HR scans. Note that we only use the ground truth HR for evaluation, not anywhere in training. We optimize separate INRs for each subject with supervision from only its two LR scans. If required, we employ skull-stripping <ref type="bibr" target="#b12">[12]</ref> and rigid registration to the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer to Table <ref type="table">2</ref> in the supplementary.</p><p>Metrics. We evaluate our results by employing common SR <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b29">29]</ref> quality metrics, namely PSNR and SSIM. To showcase perceptual image quality, we additionally compute the Learned Perceptual Image Patch Similarity (LPIPS)  <ref type="bibr" target="#b31">[31]</ref> and measure the absolute error MI in mutual information of two upsampled images to their ground truth counterparts as follows:</p><formula xml:id="formula_3">ε C i MI = 1 N N k=1 |MI( Îk i , I k j ) -MI(I k i , I k j )|; εMI = 1 N N k=1 |MI( Îk 1 , Îk 2 ) -MI(I k 1 , I k 2 )|</formula><p>Baselines and Ablation. To the best of our knowledge, there are no prior data-driven methods that can perform MCSR on a single-subject basis. Hence, we provide single-subject baselines that operate solely on single contrast and demonstrate the benefit of information transfer from other contrasts with our proposed models.  Quantitative Analysis. Table <ref type="table" target="#tab_0">1</ref> demonstrates that our proposed framework poses a trustworthy candidate for the task of MCSR. As observed in <ref type="bibr" target="#b32">[32]</ref>, LRTV struggles for anisotropic up-sampling while SMORE's overall performance is better than cubic-spline, but slightly worse to single-contrast INR. However, the benefit of single-contrast INR may be limited if not complemented by additional views as in <ref type="bibr" target="#b29">[29]</ref>. For MCSR from single-subject scans, we achieve encouraging results across all metrics for all datasets, contrasts, and views. Since T1w and T2w both encode anatomical structures, the consistent improvement in BraTS for both sequences serves as a proof-of-concept for our approach. As FLAIR is the go-to-sequence for MS lesions, and T1w does not encode such information, the results are in line with the expectation that there could be a relatively higher transfer of anatomical information to pathologically more relevant FLAIR than vice-versa. Lastly, given their similar physical acquisition and lesion sensitivity, we note that DIR/FLAIR benefit to the same degree in the cMS dataset.</p><p>Qualitative Analysis. Figure <ref type="figure" target="#fig_1">2</ref> shows the typical behavior of our models on cMS dataset, where one can qualitatively observe that the split-head INR pre-serves the lesions and anatomical structures shown in the yellow boxes, which other models fail to capture. While our reconstruction is not identical to the GT HR, the coronal view confirms anatomically faithful reconstructions despite not receiving any in-plane supervision from any contrast during training. We refer to Fig. <ref type="figure">4</ref> in the supplementary for similar observations on BraTS and MSSEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Given the importance and abundance of large multi-parametric retrospective cohorts <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b16">16]</ref>, our proposed approach will allow the upscaling of LR scans with the help of other sequences. Deployment of such a model in clinical routine would likely reduce acquisition time for multi-parametric MRI protocols maintaining an acceptable level of image fidelity. Importantly, our model exhibits trustworthiness in its clinical applicability being 1) subject-specific, and 2) as its gain in information via super-resolution is validated by MI preservation and is not prone to hallucinations that often occur in a typical generative model.</p><p>In conclusion, we propose the first subject-specific deep learning solution for isotropic 3D super-resolution from anisotropic 2D scans of two different contrasts of complementary views. Our experiments provide evidence of inter-contrast information transfer with the help of INR. Given the supervision of only single subject data and trained within minutes on a single GPU, we believe our framework to be potentially suited for broad clinical applications. Future research will focus on prospectively acquired data, including other anatomies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed approach (best viewed in full screen). a) Given a realistic clinical scenario, two MRI contrasts are acquired in complementary 2D views. b) Our proposed INR models both contrast from the supervision available in the 2D scans and, by doing so, learn to transfer knowledge from in-plane measurements to out-of-plane of the other contrast. Although our model is trained on MSELoss only for the observed coordinates, it constructs a continuous function space, converging to an optimum state of mutual information between the contrasts on the global space of Ω. c) Once learned, we can sample an isotropic grid and obtain the anatomically faithful and pathology-preserving super-resolution.</figDesc><graphic coords="4,46,77,69,92,335,26,241,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results for MCSR for cMS. The predictions of the split-head INR demonstrate the transfer of anatomical and lesion knowledge from complementing views and sequences. Yellow boxes highlight details recovered by the split-head INR in the out-of-plane reconstructions, where others struggle. (Color figure online)</figDesc><graphic coords="8,46,80,54,44,330,28,242,26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results for MCSR on two public and one in-house datasets. All metrics consistently show that our split-head INR performs the best for MCSR.</figDesc><table><row><cell cols="2">BraTS 2019 Contrasts</cell><cell>T1w</cell><cell></cell><cell></cell><cell>T2w</cell><cell></cell><cell></cell><cell>T1w &amp; T2w</cell></row><row><cell></cell><cell>Methods</cell><cell cols="7">PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ ε T 1 MI ↓ ε T 2 MI ↓</cell><cell>εMI ↓</cell></row><row><cell></cell><cell>Cubic Spline</cell><cell>21.201</cell><cell>0.896</cell><cell>0.098</cell><cell>26.201</cell><cell>0.932</cell><cell>0.058</cell><cell>0.096 0.087</cell><cell>0.145</cell></row><row><cell></cell><cell>LRTV</cell><cell>21.328</cell><cell>0.919</cell><cell>0.052</cell><cell>24.206</cell><cell>0.915</cell><cell>0.053</cell><cell>0.126 0.127</cell><cell>0.203</cell></row><row><cell></cell><cell>SMORE</cell><cell>26.266</cell><cell>0.942</cell><cell>0.030</cell><cell>28.466</cell><cell>0.942</cell><cell>0.030</cell><cell>0.157 0.127</cell><cell>0.225</cell></row><row><cell></cell><cell cols="2">Single Contrast INR 26.168</cell><cell>0.952</cell><cell>0.036</cell><cell>29.920</cell><cell>0.957</cell><cell>0.028</cell><cell>0.051 0.030</cell><cell>0.079</cell></row><row><cell></cell><cell>Our vanilla INR</cell><cell>26.196</cell><cell>0.960</cell><cell>0.032</cell><cell>29.777</cell><cell>0.962</cell><cell>0.026</cell><cell>0.008 0.015</cell><cell>0.065</cell></row><row><cell></cell><cell cols="4">Our split-head INR 28.746 0.965 0.028</cell><cell cols="3">31.802 0.966 0.024</cell><cell cols="2">0.007 0.014 0.062</cell></row><row><cell cols="2">MSSEG 2016 Contrasts</cell><cell>T1w</cell><cell></cell><cell></cell><cell>Flair</cell><cell></cell><cell></cell><cell>T1w &amp; Flair</cell></row><row><cell></cell><cell>Methods</cell><cell cols="7">PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ ε T 1 MI ↓ ε F lair MI</cell><cell>↓ εMI ↓</cell></row><row><cell></cell><cell>Cubic Spline</cell><cell>30.102</cell><cell>0.953</cell><cell>0.051</cell><cell>28.724</cell><cell>0.945</cell><cell>0.054</cell><cell>0.062 0.087</cell><cell>0.115</cell></row><row><cell></cell><cell>LRTV</cell><cell>22.848</cell><cell>0.860</cell><cell>0.050</cell><cell>23.920</cell><cell>0.872</cell><cell>0.044</cell><cell>0.068 0.052</cell><cell>0.095</cell></row><row><cell></cell><cell>SMORE</cell><cell>25.729</cell><cell>0.937</cell><cell>0.030</cell><cell>27.430</cell><cell>0.940</cell><cell>0.029</cell><cell>0.138 0.100</cell><cell>0.183</cell></row><row><cell></cell><cell cols="2">Single Contrast INR 30.852</cell><cell>0.956</cell><cell>0.029</cell><cell>31.156</cell><cell>0.955</cell><cell>0.030</cell><cell>0.047 0.074</cell><cell>0.095</cell></row><row><cell></cell><cell>Our vanilla INR</cell><cell>31.599</cell><cell>0.966</cell><cell>0.019</cell><cell>32.312</cell><cell>0.969</cell><cell>0.019</cell><cell>0.008 0.025</cell><cell>0.024</cell></row><row><cell></cell><cell cols="4">Our split-head INR 31.769 0.967 0.019</cell><cell cols="3">32.514 0.970 0.018</cell><cell cols="2">0.008 0.023 0.024</cell></row><row><cell>cMS</cell><cell>Contrasts</cell><cell>DIR</cell><cell></cell><cell></cell><cell>Flair</cell><cell></cell><cell></cell><cell>DIR &amp; Flair</cell></row><row><cell></cell><cell>Methods</cell><cell cols="7">PSNR ↑ SSIM ↑ LPIPS ↓ PSNR ↑ SSIM ↑ LPIPS ↓ ε DIR MI ↓ ε F lair MI</cell><cell>↓ εMI ↓</cell></row><row><cell></cell><cell>Cubic Spline</cell><cell>28.106</cell><cell>0.929</cell><cell>0.065</cell><cell>26.545</cell><cell>0.923</cell><cell>0.079</cell><cell>0.083 0.096</cell><cell>0.136</cell></row><row><cell></cell><cell>LRTV</cell><cell>28.725</cell><cell>0.904</cell><cell>0.033</cell><cell>22.766</cell><cell>0.835</cell><cell>0.057</cell><cell>0.269 0.088</cell><cell>0.312</cell></row><row><cell></cell><cell>SMORE</cell><cell>28.933</cell><cell>0.926</cell><cell>0.040</cell><cell>25.336</cell><cell>0.921</cell><cell>0.039</cell><cell>0.124 0.079</cell><cell>0.139</cell></row><row><cell></cell><cell cols="2">Single Contrast INR 29.941</cell><cell>0.937</cell><cell>0.037</cell><cell>28.655</cell><cell>0.936</cell><cell>0.041</cell><cell>0.063 0.072</cell><cell>0.096</cell></row><row><cell></cell><cell>Our vanilla INR</cell><cell>30.816</cell><cell cols="2">0.956 0.024</cell><cell>29.749</cell><cell>0.950</cell><cell>0.029</cell><cell cols="2">0.022 0.033 0.009</cell></row><row><cell></cell><cell cols="4">Our split-head INR 31.686 0.956 0.023</cell><cell cols="3">30.246 0.952 0.028</cell><cell cols="2">0.021 0.033 0.009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In addition, we show ablations of our proposed split head model compared to our vanilla INR. Precisely, our experiments include: Baseline 1 : Cubic-spline interpolation is applied on each contrast separately.</figDesc><table><row><cell>Baseline 2 : LRTV [23] applied on each contrast separately.</cell></row><row><cell>Baseline 3 : SMORE (v3.1.2) [32] applied on each contrast separately.</cell></row><row><cell>Baseline 4 : Two single-contrast INRs with one output channel each.</cell></row><row><cell>Our vanilla INR (ablation): Single INR with two output channels that</cell></row><row><cell>jointly predicts the two contrast intensities.</cell></row></table><note><p><p><p>Our proposed split-head INR: Single INR with two separate heads that jointly predicts the two contrast intensities (cf. Fig.</p>1</p>).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. JM, MM and JSK are supported by <rs type="funder">Bavarian State Ministry for Science and Art (Collaborative Bilateral Research Program Bavaria -Québec: AI</rs> in medicine, grant <rs type="grantNumber">F.4-V0134.K5.1/86/34</rs>). SS, RG and JSK are supported by <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation program</rs> (<rs type="grantNumber">101045128-iBack-epic-ERC2021-COG</rs>). MD and DR are supported by <rs type="funder">ERC</rs> (<rs type="grantNumber">Deep4MI -884622</rs>) and <rs type="funder">ERA-NET NEURON Cofund</rs> (<rs type="grantNumber">MULTI-FACT -8810003808</rs>). HBL is supported by an <rs type="funder">Nvidia</rs> <rs type="grantName">GPU grant</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8C6kaz5">
					<idno type="grant-number">F.4-V0134.K5.1/86/34</idno>
				</org>
				<org type="funding" xml:id="_UAdvTWg">
					<idno type="grant-number">101045128-iBack-epic-ERC2021-COG</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_G7RPj4X">
					<idno type="grant-number">Deep4MI -884622</idno>
				</org>
				<org type="funding" xml:id="_MpEqREE">
					<idno type="grant-number">MULTI-FACT -8810003808</idno>
				</org>
				<org type="funding" xml:id="_MfczZJ4">
					<orgName type="grant-name">GPU grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_17.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning shape reconstruction from sparse measurements with neural implicit functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amiranashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lüdke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning (MIDL)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multilevel densely connected network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_11" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Brain MRI super resolution using 3D deep densely connected neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MSSEG challenge proceedings: multiple sclerosis lesions segmentation challenge using a data management and processing infrastructure</title>
		<author>
			<persName><forename type="first">O</forename><surname>Commowick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cervenansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ameli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="issue">MICCAI</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-contrast MRI super-resolution via a multi-stage integration network</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring separable attention for multi-contrast MR image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with intermediate loss for 3D super-resolution of CT and MRI scans</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="49112" to="49124" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust super-resolution volume reconstruction from slice acquisitions: application to fetal brain MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Estroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1739" to="1758" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-minute ultrafast brain MRI with full basic sequences: can it be a promising way forward for pediatric neuroimaging?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AJR</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="205" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural implicit k-space for binning-free non-cartesian cardiac MR imaging</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hammernik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-34048-2_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-34048-2_42" />
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging. IPMI 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wassermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13939</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated brain extraction of multisequence MRI using artificial neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4952" to="4964" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformer-empowered multi-scale contextual matching and aggregation for multi-contrast MRI super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20636" to="20645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-contrast super-resolution MRI through a progressive network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2738" to="2749" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MRI superresolution using self-similarity and image priors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Manjón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coupé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Imaging</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_24" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepSDF: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolution methods in MRI: can they improve the trade-off between resolution, signal-to-noise ratio, and acquisition time?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Plenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1993" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A non-local approach for image superresolution using intermodality priors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="605" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WIRE: wavelet implicit neural representations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saragadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lejeune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18507" to="18516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NeRP: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LRTV: MR image superresolution with low-rank and total variation regularizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2459" to="2466" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7462" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7537" to="7547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-modal volume registration by maximization of mutual information</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Atsumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="51" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining short-axis and long-axis cardiac MR images by applying a super-resolution reconstruction algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wesarg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2010: Image Processing</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7623</biblScope>
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An arbitrary scale super-resolution approach for 3D MR images via implicit neural representation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JBHI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1015" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">IREM: high-resolution magnetic resonance image reconstruction via implicit neural representation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous singleand multi-contrast super-resolution for brain MRI images based on a convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SMORE: a self-supervised anti-aliasing and super-resolution algorithm for MRI using deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Calabresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="817" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
