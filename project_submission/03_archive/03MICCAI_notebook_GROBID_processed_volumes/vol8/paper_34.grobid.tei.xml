<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN</title>
				<funder ref="#_dPN6HPY">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_rqjfSNu">
					<orgName type="full">Guangdong Key Laboratory of Human Digital Twin Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<email>eexinzhang@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Future Technology</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dajiang</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="348" to="357"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DE2B457BB3BE47F199B082396454F7F8</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Structural Connectivity</term>
					<term>Functional Connectivity</term>
					<term>Diverse Generation</term>
					<term>GAN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Revealing structural-functional relationship is an important issue in neuroscience study since it helps to understand brain activities. Structural Connectivity (SC) represents the fibers connection between the brain regions, which is relatively static. Functional Connectivity (FC) represents the active signal correlations between the brain regions, which is relatively dynamic and diverse. Many works predict FC from SC and achieve unique FC prediction. However, FC is diverse since it represents brain activities. In this work, we propose the MCGAN, a multi-contexts discriminator based generative adversarial network for predicting diverse FC from SC. The proposed multi-contexts discriminator provides three kinds of supervisions to strengthen the generator, i.e. edge-level, nodelevel and graph-level. Since FC represents the connection of the brain regions, it can be regarded as edge-based graph. We adopt edge-based graph convolution method to model the context encoding. Moreover, to introduce the diversity of generated FC, we utilize monte-carlo mean samples to bring in more FC data for training. We validate our MCGAN on Human Connextome Project (HCP) dataset and Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The results show that our method can generate diverse and meaningful FC from SC, revealing the one-to-many relationship between the individual SC and the multiple FC. The significance of this work is that once we have anatomical structure of brain represented by SC, we can predict diverse developments of brain activities represented by FC, which helps to reveal individual brain's static-dynamic structural-functional mode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Illustration of GAN <ref type="bibr" target="#b2">[3]</ref> (a), GAN with reconstruct supervision in generator <ref type="bibr" target="#b8">[9]</ref> (b), GAN with more supervisions in discriminator <ref type="bibr" target="#b13">[14]</ref> (c), and our MCGAN (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the study of exploring structural-functional relationship raises lots of attentions in neuroscience which helps to reveal individual behaviors of human brain <ref type="bibr" target="#b0">[1]</ref>. Typically, Structural Connectivity (SC) <ref type="bibr" target="#b15">[16]</ref> represents the fibers connection between the brain regions while Functional Connectivity (FC) <ref type="bibr" target="#b3">[4]</ref> represents the Blood-Oxygen-Level-Dependent (BOLD) signal correlations between the brain regions. In comparing, SC is relatively static since it demonstrates the anatomical structure of brain, and FC is relatively dynamic and diverse since it demonstrates the development of the brain activities <ref type="bibr" target="#b1">[2]</ref>. To explore the relationship and mapping between them, some works predict SC from FC <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> while some works map the structural to functional <ref type="bibr" target="#b20">[21]</ref> and both of these works achieve unique prediction. It is accountable for predicting SC from FC since SC is relatively static. However, since the FC is relatively dynamic, the predicting FC from SC is quite challenging and it should be diverse predictions rather than deterministic prediction. In general, it is necessary and challenging to explore the one-to-many relationship between the one subjects's SC and the FC by predicting diverse FC from SC <ref type="bibr" target="#b1">[2]</ref>. The significance of this work is that once we have anatomical structure of brain represented by SC, we can predict diverse developments of brain activities represented by FC, which helps to reveal individual brain's static-dynamic structural-functional mode.</p><p>In this work, we propose the diverse generations with the idea of conditional GAN <ref type="bibr" target="#b11">[12]</ref>, i.e. the input of the generator is random noise to realize diverse generations and is in condition of SC to generate FC. To improve the quality of generation, some works introduce the reconstruct supervision in generator while some works provide more supervisions in discriminator. In this work, we propose MCGAN, a multi-contexts discriminator based generative adversarial network to take both advantages. The comparison is shown in Fig. <ref type="figure">1</ref>. Specifically, the multi-contexts discriminator provides three kinds of supervisions, i.e. edge-level, node-level and graph-level, to strengthen the generator. We adopt edge-based graph convolution method <ref type="bibr" target="#b10">[11]</ref> to model the FC encoding. In addition, we utilize monte-carlo mean samples to enlarge the FC data for supervision. We validate our MCGAN on the HCP dataset <ref type="bibr" target="#b14">[15]</ref> and ADNI dataset <ref type="bibr" target="#b9">[10]</ref> and the results show that our method can generate diverse and meaningful FC. To the best of our knowledge, our method is the first to explore one-to-many SC-FC relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Overview</head><p>The framework of our proposed MCGAN is shown in Fig. <ref type="figure" target="#fig_0">2</ref> and detailed below. The objective of this work is to predict diverse functional connectivity (FC) with the instruction of structural connectivity (SC). The proposed MCGAN is based on the generative adversarial network (GAN) framework <ref type="bibr" target="#b2">[3]</ref>, which consists of two parts. (a) Generator. For FC generation of subject i, the input of the generator is random noise which introduces the diverse generations and is in condition of SC of subject i. We utilize monte-carlo mean FC samples of subject i for supervision. (b) Multi-contexts Discriminator. The discriminator distinguishes the generated FC and real one. We introduce multi contexts supervision to discriminator, i.e. edge-level, node-level and graph-level, to strengthen the generator. We adopt edge-based graph convolution method <ref type="bibr" target="#b10">[11]</ref> to model the FC encoding. In the prediction stage, for a specific subject, the generator predicts diverse FC with corresponding numbers of random noise input and in condition of the same SC of subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MCGAN</head><p>The adversarial objective of generating FC from SC with GAN model is defined as:</p><formula xml:id="formula_0">L adv = L G + L D , L G = -E x,y [logD(G(x, z))], L D = -E x,y [logD(x, y)] -E x,z [log(1 -D(G(x, z)))].</formula><p>( where z ∈ R n×n denotes the noise sample from N (0, I) to introduce the diversity of the generated FC. x ∈ R n×n denotes the SC is the condition which instructs the generation of FC <ref type="bibr" target="#b11">[12]</ref> and y ∈ R n×n denotes the real FC.</p><formula xml:id="formula_1">) E2N module 4 ,1<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-contexts Discriminator</head><p>The original discriminator learns the global representation of synthetic or real data to distinguish them. However, it is insufficient to provide powerful feedback to encourage the generator to predict realistic data. Many works improve the discriminator and show that the stronger the discriminator is, the better the generator is <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Motivated by this insight, we propose a multi-contexts discriminator, which provides edge-level, node-level and graph-level supervisions to implicitly and explicitly strengthen the generator. The multi-contexts discriminator shown in Fig. <ref type="figure" target="#fig_1">3</ref> consists of two parts, context encoding and multi contexts discrimination.</p><p>Context Encoding. The context encoding is for feature extraction. For brain network, regions are represented as nodes and the links between regions are represented as edges. Since FC is defined as the correlation between the active brain regions, it represents the edge feature. However, most of the graph convolution networks (GCN) model the node feature extraction. To address the FC encoding, we adopt the edge-based graph convolution method for feature extraction <ref type="bibr" target="#b10">[11]</ref>.</p><p>There are three basic modules, i.e. E2N module (edge to node), N2E module (node to edge), N2G module (node to graph). E2N module. Given a FC E ∈ R n×n , where E i,j denotes the edge between region i and region j, it aggregates the linking edges of region i into a node representation:</p><formula xml:id="formula_3">N i = n k=1 E i,k w k , i = 1, 2, ..., n<label>(2)</label></formula><p>where w i is trainable weight and N i is the feature of i t h node.</p><p>N2E module. It propagates the feature of node i and node j to their linking edge:</p><formula xml:id="formula_4">E i,j = N i + N j = n k=1 (E i,k + E j,k )w k ,<label>(3)</label></formula><p>The network stacks the graph convolution layers for FC encoding. A graph convolution layer is comprised of a E2N module and a N2E module for FC feature learning and reserve the structure of FC. Meanwhile, the skip connection <ref type="bibr" target="#b4">[5]</ref> is applied in graph convolution layer. N2G module. It integrates all the nodes feature into a graph representation:</p><formula xml:id="formula_5">G = n i=1 N i w i . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>The output layer of the network is comprised of a E2N module and a N2G module to encode the FC representation into the node representation and the graph representation in series.</p><p>Multi Contexts Discrimination. The original discriminator D(•) classifies the input data to be real or fake. In contrast, our proposed multi-contexts discriminator provides three kind of supervisions, i.e. edge-level, node-level and graph-level, to implicitly and explicitly strengthen the generator. Mathematically, our discriminator loss is comprised of three parts:</p><formula xml:id="formula_7">L D = L D edge + L D node + L D graph , (<label>5</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">L D edge = -E x,y [ i,j logD edge (x, y) i,j ] -E x,z [ i,j log(1 -D edge (x, G(x, z)) i,j )], L D node = -E x,y [ n logD node (x, y) n ] -E x,z [ n log(1 -D node (x, G(x, z)) n )], L D graph = -E x,y [logD graph (x, y)] -E x,z [log(1 -D graph (x, G(x, z)))].</formula><p>In the output layer, we first use three MLP to transform the three kinds of context representation. Then the D edge (•), D node (•), D graph (•) respectively classify the transformed edge representation, node representation and graph representation to be real or fake. Correspondingly, the objective of the generator is:</p><formula xml:id="formula_10">L G = -E x,z [ i,j logD edge (x, G(x, z)) i,j + n logD node (x, G(x, z)) n + logD graph (x, G(x, z)) ].<label>(6)</label></formula><p>The multi contexts supervision improves the discriminator, which implicitly strengthens the generator in adversarial training process. Meanwhile, it feedbacks the fine-grained information to the generator by backpropagation, which explicitly strengthens the generator and encourages it to predict realistic FC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte-Carlo Mean Samples of FC</head><p>FC represents the development of brain activities so that it is relatively dynamic. While SC represents the fibers connection to indicate the anatomical structure of brain <ref type="bibr" target="#b1">[2]</ref>, which is relatively static. Since the limited of FC data and the supervision of the generated FC should not be limited in finite subspace, we augment the FC data via monte-carlo mean:</p><formula xml:id="formula_11">y = 1 m y i . (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where y i denotes multiple real FC of each subject and m can be fixed or dynamic during training, which denotes the monte-carlo sampling, m = 1, 2, ...M , M is the maximum samples for each subject of dataset. The linear combination of FC enlarges the data space for supervision, making the generator predict diverse FC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Loss of Generator</head><p>We adopt reconstruction loss in generator to make it predict realistic FC which includes the mean absolute error (MAE) and Pearson's correlation coefficient (PCC) between the generated FC and the training FC sample.</p><formula xml:id="formula_13">L rec = E x,z,y [|y -G(x, z)|] + E x,z,y P CC(y, G(x, z)). (<label>8</label></formula><formula xml:id="formula_14">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Objective</head><p>In summary, the objective function of the MCGAN is defined as:</p><formula xml:id="formula_15">L = L adv + L rec . (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>where L adv optimizes the adversarial training of the generator and discriminator, L rec optimizes the generator for realistic prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Datasets. We evaluate our MCGAN on the Human Connectome Project (HCP) dataset <ref type="bibr" target="#b14">[15]</ref> and Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset <ref type="bibr" target="#b9">[10]</ref>. We apply the standard preprocessing procedures <ref type="bibr" target="#b19">[20]</ref> for both datasets. We use diffusion magnetic resonance imaging (diffusion MRI) as the SC and resting state functional magnetic resonance imaging (rs-fMRI) as FC. We divide the BOLD signal into 20 slices for HCP and 10 slices for ADNI for each subject and obtain FC by calculating Pearson's correlation coefficient (PCC) between the segmented signal series. Both of the SC and FC are normalized.</p><p>Evaluation Metrics. (a) Quality. Instinctively, for each subject, the generated FC should be similar to one of the real FC. Therefore, for each generated FC of a subject, we calculate the minimum mean absolute error and the maximum Pearson's correlation coefficient to evaluate the quality of generation with the most similar real FC. (b) Diversity. We use Frechet Inception distance (FID) <ref type="bibr" target="#b5">[6]</ref> to evaluate the diversity of the FC generation. FID compares the distribution of the generated and real data, to simultaneously evaluate the quality and diversity.</p><p>The lower FID score indicates the better quality and diversity.</p><p>Implementation Details. Both of the generator and discriminator are in condition of SC and we set the almost equivalent parameters for them to ensure the efficient adversarial training. We respectively set the learning rate of 0.0001 and 0.0004 for generator and discriminator. At prediction stage, we generate corresponding number of FC samples with dataset for each subject (i.e. 20 for HCP and 10 for ADNI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Main Results. We compare our MCGAN with the original GAN <ref type="bibr" target="#b2">[3]</ref>, Pixel-GAN which is with reconstruct supervision in generator <ref type="bibr" target="#b8">[9]</ref>, UNet-GAN which is with more supervisions in discriminator <ref type="bibr" target="#b13">[14]</ref>. In addition, many works perform the deterministic prediction on structural or functional prediction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. We compare our MCGAN with MGCN-GAN which is a deterministic prediction to map FC to SC <ref type="bibr" target="#b17">[18]</ref>. The results are shown in Table <ref type="table" target="#tab_0">1</ref>. PixelGAN achieves good reconstruction but not well in diversity, which is demonstrated in original paper. UNet-GAN achieves ordinary diversity but fails to reconstruct meaningful FC. MGCN-GAN achieves the best reconstruction since it is a deterministic prediction method but lacks of diversity. In comparing, our MCGAN achieves quality-diversity trade-off.</p><p>Ablation Study. We conduct ablation study to investigate the contribution of different components and the results are shown in Table <ref type="table" target="#tab_1">2</ref>. The multi contexts discrimination improves the quality and diversity, indicating that it strengthens the generator for better prediction. The reconstruction provides the supervision to better quality and the monte-carlo mean samples provide the better diversity.</p><p>Visualization. We generate multiple FC for each subject and we visualize two FC of each subject. The visualization of the generated and real FC is shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Each column denotes the generated FC and the corresponding most similar real FC. It shows that our method achieves diverse predictions rather than only one deterministic prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Despite this work predicts diverse FC to explore one-to-many SC-FC relationship, the generated FC is not quite diverse enough to some extent. In our implementation, we attempt to use the diffusion model <ref type="bibr" target="#b6">[7]</ref> to generate FC and it achieves better effect in diversity than this work. By visualization, we find that it generates diverse FC. However, since the diffusion model is trained with the only objective of noise prediction for its particularity and without the reconstruction task, it does not perform well in the evaluation metrics of MAE and PCC, indicating that it can not generate meaningful FC similar to this work. In general, the prediction of FC from SC should be diverse since the SC is relatively static and the FC is relatively dynamic and diverse. To the best of our knowledge, our method is the first to explore one-to-many SC-FC relationship and this task is challenging and significant. We lead to the future work for further exploration and there are many questions to be resolved. Moreover, to reveal individual brain's static-dynamic structural-functional mode, predicting original functional signals is more flexible than predicting FC since it directly explores the development of the brain activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, for diverse generations, we propose a multi-contexts discriminator based GAN named MCGAN, which provides three kind of supervisions to strengthen the generator, including edge-level, node-level and graph-level. We adopt edge-based graph convolution method for FC encoding and we utilize monte-carlo mean samples to enlarge the FC data for supervision. The experiments show that our method can generate diverse and meaningful FC from SC. We are the first to explore the one-to-many relationship between one subject's individual SC and multiple FC, which helps to reveal individual brain's staticdynamic structural-functional mode. We lead to the future work for further exploration and there are many questions to be resolved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed MCGAN.</figDesc><graphic coords="3,51,96,187,88,288,16,95,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed multi-contexts discriminator which consists of the context encoding and multi contexts discrimination.</figDesc><graphic coords="4,55,98,53,99,340,30,115,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of the generated and the real FC. Each column denotes the generated FC (first row) and the corresponding most similar real FC (second row) of multi-slices label of subject. It shows that our method can generate the diverse and realistic FC.</figDesc><graphic coords="8,55,98,157,79,340,30,108,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of the quality and diversity.</figDesc><table><row><cell>HCP/ADNI</cell><cell>Prediction</cell><cell>MAE ↓</cell><cell>PCC ↑</cell><cell>FID ↓</cell></row><row><cell>GAN [3]</cell><cell>Diverse</cell><cell cols="3">1.09/1.12 0.02/0.04 116.5/130.2</cell></row><row><cell>PixelGAN [9]</cell><cell>Diverse</cell><cell cols="3">0.28/0.30 0.52/0.49 63.5/70.2</cell></row><row><cell>UNet-GAN [14]</cell><cell>Diverse</cell><cell cols="3">0.89/0.96 0.09/0.05 40.5/49.2</cell></row><row><cell cols="5">MGCN-GAN [18] Deterministic 0.12/0.12 0.80/0.76 76.6/79.2</cell></row><row><cell>MCGAN</cell><cell>Diverse</cell><cell cols="3">0.24/0.26 0.56/0.51 18.8/22.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of ablation study.</figDesc><table><row><cell>HCP/ADNI</cell><cell>MAE ↓</cell><cell>PCC ↑</cell><cell>FID ↓</cell></row><row><cell>MCGAN</cell><cell cols="3">0.24/0.26 0.56/0.51 18.8/22.3</cell></row><row><cell cols="4">w/o multi contexts discrimination 0.30/0.33 0.49/0.47 29.8/33.5</cell></row><row><cell>w/o mae&amp;pcc</cell><cell cols="3">0.84/0.93 0.12/0.10 39.7/45.3</cell></row><row><cell>w/o monte-carlo samples</cell><cell cols="3">0.19/0.26 0.58/0.52 31.5/31.9</cell></row><row><cell>HCP dataset</cell><cell></cell><cell cols="2">ADNI dataset</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">Guangdong Key Laboratory of Human Digital Twin Technology</rs> (<rs type="grantNumber">2022B1212010004</rs>) and <rs type="funder">Fundamental Research Funds for the Central Universities</rs> <rs type="grantNumber">2022ZYGXZR104</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rqjfSNu">
					<idno type="grant-number">2022B1212010004</idno>
				</org>
				<org type="funding" xml:id="_dPN6HPY">
					<idno type="grant-number">2022ZYGXZR104</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neuroimaging evidence of the anatomo-functional organization of the human cingulate motor areas</title>
		<author>
			<persName><forename type="first">C</forename><surname>Amiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="563" to="578" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anatomo-functional study of the temporo-parietooccipital region: dissection, tractographic and brain mapping evidence from a neurosurgical perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Benedictis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Anat</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Functional brain networks are dominated by stable group and individual factors, not cognitive or daily variation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gratton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="452" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reference-relation guided autoencoder with deep CCA restriction for awake-to-sleep brain functional connectome prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging: Official J. Int. Soc. Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brain connectivity based graph convolutional networks and its application to infant age prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2764" to="2776" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A U-Net based discriminator for generative adversarial networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8207" to="8216" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The human connectome project: a data acquisition perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2222" to="2231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantifying differences and similarities in whole-brain white matter architecture using local connectome fingerprints</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1005203</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Longitudinal infant functional connectivity prediction via conditional intensive triplet network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_25" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recovering brain structural connectivity from functional connectivity via multi-GCN based generative adversarial network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_6" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting brain structural network using functional connectivity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102463</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Connectome-scale assessments of structural and functional connectivity in MCI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2911" to="2923" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From sMRI to task-fMRI: a unified geometric deep learning framework for cross-modal brain anatomofunctional mapping</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102681</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
