<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI</title>
				<funder ref="#_K9xVzKE #_w8GfUYW">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Research Imaging Center</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinjian</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The First School of Clinical Medicine</orgName>
								<orgName type="institution">Guangzhou University of Chinese Medicine</orgName>
								<address>
									<settlement>Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Psychiatry</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">University of Connecticut</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<settlement>Farmington</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Research Imaging Center</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Steffens</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Psychiatry</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">University of Connecticut</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<settlement>Farmington</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shijun</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The First School of Clinical Medicine</orgName>
								<orgName type="institution">Guangzhou University of Chinese Medicine</orgName>
								<address>
									<settlement>Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><forename type="middle">G</forename><surname>Potter</surname></persName>
							<email>guy.potter@duke.edu</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Psychiatry and Behavioral Sciences</orgName>
								<orgName type="department" key="dep2">Duke University Medical Center</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingxia</forename><surname>Liu</surname></persName>
							<email>mxliu@med.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Biomedical Research Imaging Center</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="109" to="119"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">562E38C38A4EAAB70C890FF338EE6278</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Structural MRI</term>
					<term>Brain anatomy</term>
					<term>Cognitive impairment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Brain structural MRI has been widely used for assessing future progression of cognitive impairment (CI) based on learning-based methods. Previous studies generally suffer from the limited number of labeled training data, while there exists a huge amount of MRIs in large-scale public databases. Even without task-specific label information, brain anatomical structures provided by these MRIs can be used to boost learning performance intuitively. Unfortunately, existing research seldom takes advantage of such brain anatomy prior. To this end, this paper proposes a brain anatomy-guided representation (BAR) learning framework for assessing the clinical progression of cognitive impairment with T1-weighted MRIs. The BAR consists of a pretext model and a downstream model, with a shared brain anatomy-guided encoder for MRI feature extraction. The pretext model also contains a decoder for brain tissue segmentation, while the downstream model relies on a predictor for classification. We first train the pretext model through a brain tissue segmentation task on 9,544 auxiliary T1-weighted MRIs, yielding a generalizable encoder. The downstream model with the learned encoder is further fine-tuned on target MRIs for prediction tasks. We validate the proposed BAR on two CI-related studies with a total of 391 subjects with T1-weighted MRIs. Experimental results suggest that the BAR outperforms several state-of-the-art (SOTA) methods. The source code and pretrained models are available at https://github.com/goodaycoder/BAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Brain magnetic resonance imaging (MRI) has been increasingly used to assess future progression of cognitive impairment (CI) in various clinical and research fields by providing structural brain anatomy <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Many learning-based methods have been developed for automated MRI analysis and brain disorder prognosis, which usually heavily rely on labeled training data <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>. However, it is generally time-consuming and tedious to collect category labels for brain MRIs in practice, resulting in a limited number of labeled MRIs <ref type="bibr" target="#b11">[11]</ref>.</p><p>Fig. <ref type="figure">1</ref>. Illustration of brain anatomy-guided representation (BAR) learning framework for assessing the clinical progression of cognitive impairment. The BAR consists of a pretext model and a downstream model, with a shared brain anatomy-guided encoder for MRI feature extraction. The pretext model also contains a decoder for brain tissue segmentation, while the downstream model relies on a predictor for prediction. The pretext model is trained on the large-scale ADNI <ref type="bibr" target="#b12">[12]</ref> with 9,544 T1-weighted MRIs, yielding a generalizable encoder. With this learned encoder frozen, the downstream model is then fine-tuned on target MRIs for prediction tasks.</p><p>Even without task-specific category label information, brain anatomical structures provided by auxiliary MRIs can be employed as a prior to boost disease progression prediction performance. Considering that there are a large number of unlabeled MRIs in existing large-scale datasets <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>, several deep learning methods propose to extract brain anatomical features from MRI without requiring specific category labels. For instance, Song et al. <ref type="bibr" target="#b14">[14]</ref> suggest that the anatomy prior can be utilized to segment brain tumors, while Yamanakkanavar et al. <ref type="bibr" target="#b16">[15]</ref> discuss how brain MRI segmentation improves disease diagnosis. Unfortunately, there are few studies that try to utilize such brain anatomy prior for assessing the clinical progression of cognitive impairment with structural MRIs.</p><p>To this end, we propose a brain anatomy-guided representation (BAR) learning framework for cognitive impairment prognosis with T1-weighted MRIs, incorporated with brain anatomy prior provided by a brain tissue segmentation task. As shown in Fig. <ref type="figure">1</ref>, the BAR consists of a pretext model and a downstream model, with a shared anatomy-guided encoder for MRI feature extraction. These two models also use a decoder and a predictor for brain tissue segmentation and disease progression prediction, respectively. The pretext model is trained on 9,544 MRI scans from the public Alzheimer's Disease Neuroimaging Initiative (ADNI) <ref type="bibr" target="#b12">[12]</ref> without any category label information, yielding a generalizable encoder. The downstream model is further fine-tuned on target MRIs for CI progression prediction. Experiments are performed on two CI-related studies with 391 subjects, with results suggesting the efficacy of BAR compared with state-ofthe-art (SOTA) methods. The pretext model can also be used for brain tissue segmentation in other MRI-based studies. To the best of our knowledge, this is the first work that utilizes anatomy prior derived from large-scale T1-weighted MRIs for automated cognitive decline analysis. To promote reproducible research, the source code and trained models have been made publicly available to the research community (see https://github.com/goodaycoder/BAR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Proposed Method</head><p>Data and Preprocessing. The pretext model is trained via a tissue segmentation task on auxiliary MRIs (without category label) from ADNI. A total of 9,544 T1-weighted MRIs from 2,370 ADNI subjects with multiple scans are used in this work. To provide accurate brain anatomy, we perform image preprocessing and brain tissue segmentation for these MRIs to generate ground-truth segmentation of three tissues, i.e., white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF), using an in-house toolbox iBEAT <ref type="bibr" target="#b17">[16]</ref> with manual verification.</p><p>The downstream model is trained on 1) a late-life depression (LLD) study with 309 subjects from two sites <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b19">18]</ref>, and 2) a type 2 diabetes mellitus (DM) study with 82 subjects from the First Affiliated Hospital of Guangzhou University of Chinese Medicine. Subjects in LLD are categorized into three groups: 1) 89 non-depressed cognitively normal (CN), 2) 179 depressed but cognitively normal (CND), 3) 41 depressed subjects (called CI) who developed cognitive impairment or even dementia in the follow-up years. Category labels in the LLD study are determined based on subjects' 5-year follow-up diagnostic information, while MRIs are acquired at baseline time. The DM contains 1) 45 health control (HC) subjects and 2) 37 diabetes mellitus patients with mild CI (MCI). Detailed image acquisition protocols are given in Table <ref type="table">SI</ref> of Supplementary Materials. All MRIs are preprocessed via the following pipeline: 1) bias field correction, 2) skull stripping, 3) affine registration to the MNI space, 4) resampling to 1 × 1 × 1 mm 3 , 5) deformable registration to AAL3 <ref type="bibr" target="#b20">[19]</ref> with SyN <ref type="bibr" target="#b21">[20]</ref>, and 6) warping 166 regions-of-interest (ROIs) of AAL3 back to MRI volumes.</p><p>Proposed Method. While it is often challenging to annotate MRIs in practice, there are a large number of MRIs (without task-specific category labels) in existing large-scale datasets. Even without category labels, previous studies propose to extract anatomical features (e.g., ROI volumes of GM segmentation maps) to characterize brain anatomy <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b23">22]</ref>. Such brain anatomy prior learned via tissue segmentation can be employed to boost learning performance intuitively. Accordingly, we propose a brain anatomy-guided representation (BAR) learning framework for progression prediction of cognitive impairment, incorporated with brain anatomy prior provided by brain tissue segmentation. As shown in Fig. <ref type="figure">1</ref>, the BAR consists of a pretext model for brain tissue segmentation and a downstream model for disease progression prediction, both equipped with brain anatomy-guided encoders (shared weights) for MRI feature learning.</p><p>(1) Pretext Model for Segmentation. To learn brain anatomical features from MRIs in a data-driven manner, we propose to employ a segmentation task for pretext model training. As shown in the top of Fig. <ref type="figure">1</ref>, the pretext model consists of 1) a brain anatomy-guided encoder for MRI feature extraction and 2) a decoder for segmentation. The brain anatomy-guided encoder takes large-scale auxiliary 3D MRIs without category labels as input, and outputs 512 feature maps. It contains 8 convolution blocks, with each block containing a convolution layer (kernel size: 3 × 3 × 3), followed by instance normalization and Parametric Rectified Linear Unit (PReLU) activation. The first 4 blocks downsample the input with a stride of 2 × 2 × 2. The channel numbers of the eight blocks are [64, 128, 256, 512, 512, 512, 512, 512], respectively. A skip connection is applied to sum the input and output of every two of the last 4 blocks for residual learning.</p><p>The decoder takes the 512 feature maps as input and outputs segmentation maps of three tissues (i.e., WM, GM, and CSF), thus guiding the encoder to learn brain anatomical features. The decoder contains four deconvolution blocks with 256, 128, 64 and 4 channels, respectively. Each deconvolution block shares the same architecture as the convolution block in the encoder. The output of the decoder is then fed into a SoftMax layer to get four probability maps that indicate the probability of a voxel belonging to a specific tissue (i.e., background, WM, GM, and CSF). Besides, the reconstruction task can be used to train the pretext model instead of segmentation when lacking ground-truth segmentation maps. For problems without ground-truth segmentation maps, we can resort to an MRI reconstruction task to train the pretext model in an unsupervised manner.</p><p>(2) Downstream Model for Prediction. As shown in the bottom panel of Fig. <ref type="figure">1</ref>, the downstream model takes target MRIs as input and outputs probabilities of category labels. It consists of 1) a brain anatomy-guided encoder and 2) a predictor for prognosis. This encoder shares the same architecture and parameters as that of the pre-trained pretext model. With the encoder frozen, predictor parameters will be updated when the downstream model is trained on target MRIs. The predictor has two convolution blocks (kernel size: 3 × 3 × 3, stride: 2 × 2 × 2, channel: 256) with a skip connection, followed by a flatten layer, an FC layer, and a SoftMax layer. The architecture of the predictor can be flexibly adjusted according to the requirements of different downstream tasks.</p><p>(3) Implementation. The proposed BAR is trained via two steps. 1) The pretext model is first trained on 9,544 MRIs from ADNI, with ground-truth segmentation as supervision. The Adam optimizer <ref type="bibr" target="#b24">[23]</ref> with a learning rate of 10 -4 and dice loss are used for training (batch size: 4, epoch: 30). 2) We then share  <ref type="table">SII</ref> of Supplementary Materials. Such partition is repeated five times independently to avoid any bias introduced by random partition, and the mean and standard deviation results are recorded. The training data is duplicated and augmented using random affine transform. Five evaluation metrics are used, including area under ROC curve (AUC), accuracy (ACC), sensitivity (SEN), specificity (SPE), and F1-Score (F1s). Besides, we perform tissue segmentation by directly applying the trained pretext model to target MRIs from LLD and DM studies and visually compare the results of our BAR with those of FSL <ref type="bibr" target="#b25">[24]</ref>.</p><p>Competing Methods. We compare our BAR with two classic machine learning methods and five SOTA deep learning approaches, including (1) support vector machine (SVM) <ref type="bibr" target="#b26">[25]</ref> with a radial basis function kernel (regularization: 1.0), ( <ref type="formula">2</ref>) XGBoost (XGB) <ref type="bibr" target="#b27">[26]</ref> (estimators: 300, tree depth: 4, learning rate: 0.2), (3) ResNetx with x convolution layers, (4) Med3Dx <ref type="bibr" target="#b28">[27]</ref> with x convolution layers, (5) SEResNet <ref type="bibr" target="#b29">[28]</ref> that is an improved model by adding squeeze and excitation blocks to ResNet, (6) EfficientNet <ref type="bibr" target="#b30">[29]</ref>, and (7) MobileNet <ref type="bibr" target="#b31">[30]</ref> that is an efficient lightweight CNN model. For SVM and XGB, we extract ROIbased WM and GM volumes of each MRI as input. All competing deep learning methods (with default architectures) take whole 3D MRIs as input and share the same training strategy as that used in the downstream model of the BAR. An early-stop training strategy (epoch: 90) is used in all deep learning models. Results of Depression and CI Identification. In this task, we aim to recognize cognitively normal subjects with depression with a higher risk of progressing to CI than healthy subjects. The results of fourteen methods on the LLD study are reported in Table <ref type="table" target="#tab_0">1</ref>, where '*' denotes that the results of BAR and a competing method are statistically significantly different (p &lt; 0.05 via paired t-test).</p><p>From the left of Table <ref type="table" target="#tab_0">1</ref>, we have the following observations on CND vs. CN classification. First, our BAR model generally outperforms thirteen competing methods in most cases. For instance, the BAR yields the results of AUC = 70.5% and SEN = 77.3%, which are 4.1% and 10.0% higher than those of the secondbest methods (i.e., SEResNet and ResNet50), respectively. This implies that the brain anatomical MRI features learned by our pretext model on large-scale datasets would be more discriminative, compared with those used in the competing methods. Second, among 10 deep models, our BAR produces the lowest standard deviation in most cases (especially on SEN and SPE), suggesting its robustness to bias introduced by random data partition in the downstream task. This could be due to the strong generalizability of the feature encoder guided by brain anatomy prior (derived from the auxiliary tissue segmentation task). In addition, our BAR significantly outperforms four machine learning methods and two lightweight deep models (i.e., EfficientNet and MobileNet) with p &lt; 0.05.</p><p>From the right of Table <ref type="table" target="#tab_0">1</ref>, we can see that the overall results of fourteen methods in CI vs. CND classification are usually worse than CND vs. CN classification. This suggests that the task of CI vs. CND classification is more challenging, which could be due to the more imbalanced training data in this task (as shown in Table SII of Supplementary Materials). On the other hand, the proposed BAR still performs best in terms of AUC=64.5% and SPE=67.0%, which are 2.8% and 2.0% higher than those of the second-best competing methods (i.e., XGB-WM and ResNet34), respectively. These results further demonstrate the superiority of the BAR in MRI-based depression recognition.</p><p>Results of MCI Detection. The results of different methods in MCI detection (i.e., MCI vs. HC classification) on the DM study are reported in Table <ref type="table" target="#tab_1">2</ref>. There are a total of 42 subjects (i.e., 17 MCI and 25 HC) used for training in this task, which are fewer but more balanced than the two tasks in the LLD study (see Table <ref type="table">SII</ref>). It can be observed from Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> that the proposed BAR yields relatively lower standard deviations in terms of AUC and ACC in MCI vs. HC classification, compared with the two tasks on the LLD study. These results imply that data imbalance may be an important issue affecting the performance of deep learning models when the number of training samples is limited.</p><p>Segmentation Results. The pre-trained pretext model can also be used for brain tissue segmentation in downstream studies. Thus, we visualize brain segmentation maps generated by FSL and our BAR for target MRIs in both LLD and DM studies in Fig. <ref type="figure" target="#fig_0">2</ref>. Note that T1-weighted MRIs in the LLD study are collected from 2 sites and have more inconsistent image quality when compared to those from DM. From Fig. <ref type="figure" target="#fig_0">2</ref>, we have several interesting observations.</p><p>First, the segmentation results generated by the proposed BAR are generally better than those of FSL in most cases, especially for those cortical surface areas on the two studies. For instance, the WM region in segmentation maps generated by our BAR is much cleaner than that of FSL, indicating that our model is not sensitive to noise in MRI. Even for the LLD study with significant inter-site data heterogeneity, the boundary of WM and GM produced by BAR is more continuous and smoother, which is in line with the brain anatomy prior. Second, for MRIs with severe motion artifacts in the LLD study (IDs: 1240, 1334, and 1653), our method can produce high-quality segmentation maps, and the results are even comparable to those of MRIs without motion artifacts. This demonstrates that our model is robust to motion artifacts. The underlying reason could be that the pretext model is trained on large-scale MRIs, and thus, has good generalization ability when applied to MRIs with different image quality. In addition, both BAR and FSL often achieve better results in the DM study, since DM has relatively higher image quality than LLD. Still, the proposed BAR can achieve better segmentation results in many fine-grained brain regions, such as the putamen region (see HC001 and MCI003) and the vermis region (see HC004). These results demonstrate that our method has good adaptability when applied to classification and segmentation tasks in MRI-based studies.</p><p>Ablation Study. To validate the effectiveness of the learned brain anatomical MRI features, we further compare the BAR with its two variants (called BAR-B and BAR-R) that use anatomy prior derived from different pretext tasks in CND vs. CN classification on LLD. Specifically, the BAR-B is trained from scratch as a baseline on target data without any pre-trained encoder. The BAR-R trains the pretext model through an MRI reconstruction task in an unsupervised learning manner. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>(a), the BAR consistently performs better than its variants in terms of all five metrics. This implies that the learned MRI features guided by the segmentation task help promote prediction performance. Also, BAR and BAR-R outperform BAR-B in most cases, implying that brain anatomy prior derived from tissue segmentation or MRI reconstruction can help improve discriminative ability of MRI features and boost prediction performance.</p><p>Influence of Training Data Size. We also study the influence of training data size on BAR in CND vs. CN classification on LLD. With fixed test data, we randomly select a part of MRIs (i.e., [20%, 40%, • • • , 100%]) from target training data to fine-tune the downstream prediction model. It can be observed from Fig. <ref type="figure" target="#fig_1">3</ref>(b) that the overall performance in terms of AUC and ACC of our BAR increases with the increase of training data, and it produces the best results when using all training data for model fine-tuning. This suggests that using more data for downstream model fine-tuning helps promote learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we develop a brain anatomy-guided representation (BAR) learning framework for MRI-based progression prediction of cognitive impairment, incorporated by brain anatomy prior (derived from an auxiliary tissue segmentation task). We validate the proposed BAR on two CI-related studies with T1-weighted MRIs, and the experimental results demonstrate its effectiveness compared with SOTA methods. Besides, the pretext model trained on 9,544 MRIs from ADNI can be well adapted to tissue segmentation in the two CI-related studies. There is significant intra-and inter-site data heterogeneity in LLD with two sites. It is interesting to reduce such heterogeneity using domain adaptation <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b33">32]</ref>, which will be our future work. Aside from tissue segmentation, one can use other auxiliary tasks to model brain anatomy, such as brain parcellation and brain MRI to CT translation. Besides, it is meaningful to compare our method with other model pre-training strategies <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34]</ref>, which will also be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Tissue segmentation (seg.) maps of white matter, gray matter and cerebrospinal fluid produced by FSL and our BAR on (a) LLD study and (b) DM study.</figDesc><graphic coords="7,55,98,54,02,340,24,164,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Results achieved by BAR with different pretext tasks in CND vs. CN classification on LLD. (b) Results of BAR with different numbers of MRIs from the target domain for downstream model training in CND vs. CN classification on LLD.</figDesc><graphic coords="8,261,84,54,62,119,68,61,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of fourteen methods in two MRI-based depression recognition tasks (i.e., CND vs. CN and CI vs. CND classification) on the LLD study.</figDesc><table><row><cell>Method</cell><cell cols="3">CND vs. CN Classification</cell><cell></cell><cell></cell><cell cols="3">CI vs. CND Classification</cell><cell></cell></row><row><cell></cell><cell cols="2">AUC (%) ACC (%)</cell><cell>SEN (%)</cell><cell>SPE (%)</cell><cell>F1s (%)</cell><cell>AUC (%)</cell><cell>ACC (%)</cell><cell>SEN (%)</cell><cell>SPE (%)</cell><cell>F1s(%)</cell></row><row><cell>SVM-GM</cell><cell>51.6(0.8)</cell><cell cols="3">52.3(3.8)* 48.7(12.2) 56.0(8.6)</cell><cell>49.9(8.4)</cell><cell>54.0(1.5)</cell><cell>50.8(1.1)*</cell><cell>51.6(2.4)</cell><cell>50.0(0.0)</cell><cell>50.5(1.8)</cell></row><row><cell>SVM-WM</cell><cell>57.9(2.6)</cell><cell cols="2">53.3(4.7)* 59.3(6.4)</cell><cell>47.3(6.4)</cell><cell>55.9(4.9)</cell><cell>38.7(3.5)</cell><cell>48.7(3.1)*</cell><cell>42.1(9.8)</cell><cell>55.0(7.1)</cell><cell>44.0(6.8)</cell></row><row><cell>XGB-GM</cell><cell>43.9(2.8)</cell><cell cols="2">42.3(1.5)* 54.0(2.8)</cell><cell>30.7(4.3)</cell><cell>48.3(1.4)</cell><cell>49.3(7.0)</cell><cell>44.1(9.5)*</cell><cell>36.8(12.9)</cell><cell>51.0(8.2)</cell><cell>38.7(12.5)</cell></row><row><cell>XGB-WM</cell><cell>50.8(2.5)</cell><cell cols="2">53.0(2.7)* 64.0(3.6)</cell><cell>42.0(3.8)</cell><cell>57.6(2.6)</cell><cell>61.7(5.0)</cell><cell>57.4(3.9)*</cell><cell>50.5(7.1)</cell><cell>64.0(6.5)</cell><cell>53.5(4.9)</cell></row><row><cell>ResNet18</cell><cell>65.4(7.2)</cell><cell cols="3">58.7(8.0)* 52.7(10.1) 64.7(9.0)</cell><cell>61.0(7.7)</cell><cell>61.3(8.1)</cell><cell>56.9(7.8)</cell><cell cols="3">59.0(14.6) 55.0(26.7) 56.8(4.9)</cell></row><row><cell>ResNet34</cell><cell>58.9(7.3)</cell><cell cols="2">57.7(6.1)* 56.0(8.3)</cell><cell>59.3(7.6)</cell><cell>58.3(6.2)</cell><cell>58.5(4.6)</cell><cell>56.4(8.1)</cell><cell>47.4(5.3)</cell><cell cols="2">65.0(13.7) 51.6(6.9)</cell></row><row><cell>ResNet50</cell><cell>63.0(5.4)</cell><cell>57.0(3.6)</cell><cell>67.3(8.6)</cell><cell>46.7(8.2)</cell><cell>60.9(4.5)</cell><cell>55.3(2.6)</cell><cell>50.8(4.6)</cell><cell>41.0(10.1)</cell><cell cols="2">60.0(13.7) 44.3(6.7)</cell></row><row><cell>Med3D18</cell><cell>57.9(2.4)</cell><cell cols="2">54.7(1.8)* 53.3(9.7)</cell><cell>56.0(9.3)</cell><cell>55.0(4.3)</cell><cell>59.6(9.1)</cell><cell cols="2">57.4(10.3)* 50.5(16.9)</cell><cell cols="2">64.0(10.8) 52.8(15.4)</cell></row><row><cell>Med3D34</cell><cell>58.7(7.2)</cell><cell cols="2">56.7(4.9)* 50.0(7.8)</cell><cell>63.3(11.1)</cell><cell>59.1(6.6)</cell><cell>57.6(4.6)</cell><cell>55.9(2.1)*</cell><cell>56.8(6.9)</cell><cell>55.0(7.9)</cell><cell>55.5(3.2)</cell></row><row><cell>Med3D50</cell><cell>60.2(2.5)</cell><cell cols="4">59.0(4.3)* 50.0(16.8) 68.0(13.9) 54.0(8.5)</cell><cell>47.3(9.2)</cell><cell>46.7(6.4)</cell><cell>45.3(24.6)</cell><cell cols="2">48.0(23.9) 43.0(13.6)</cell></row><row><cell>SEResNet</cell><cell>66.4(2.1)</cell><cell>57.7(4.3)</cell><cell cols="2">60.0(12.7) 55.3(14.6)</cell><cell>58.2(6.1)</cell><cell>59.5(5.5)</cell><cell>53.9(2.6)*</cell><cell>55.8(28.5)</cell><cell cols="2">52.0(28.0) 51.4(13.6)</cell></row><row><cell cols="2">EfficientNet 59.3(9.9)</cell><cell cols="3">58.0(5.9)* 64.7(25.7) 51.3(17.1)</cell><cell>54.1(6.7)</cell><cell>56.4(8.9)</cell><cell>53.3(5.2)*</cell><cell>56.8(31.9)</cell><cell cols="2">50.0(29.8) 49.4(22.9)</cell></row><row><cell>MobileNet</cell><cell>55.3(8.6)</cell><cell cols="3">54.3(3.5)* 44.7(31.2) 64.0(31.7)</cell><cell cols="3">53.9(20.6) 58.5(10.1) 53.3(5.8)*</cell><cell>56.8(40.0)</cell><cell cols="2">50.0(35.9) 47.2(27.5)</cell></row><row><cell cols="3">BAR (Ours) 70.5(4.1) 65.0(1.7)</cell><cell>77.3(5.5)</cell><cell>52.7(3.6)</cell><cell>68.8(2.4)</cell><cell>64.5(5.8)</cell><cell>59.0(5.1)</cell><cell>50.5(8.0)</cell><cell>67.0(6.7)</cell><cell>54.4(6.4)</cell></row><row><cell cols="11">the encoder of the pre-trained pretext model with the downstream model and</cell></row><row><cell cols="11">fine-tune the predictor via a cross-entropy loss (batch size: 2, learning rate: 10 -4 ,</cell></row><row><cell cols="11">epoch: 90). The learning rate of fine-tuning decays by 0.1 every 30 epochs. The</cell></row><row><cell cols="11">BAR is implemented on PyTorch with NVIDIA TITAN Xp (memory: 12GB).</cell></row><row><cell cols="3">3 Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Experimental Setting. Three classification tasks are performed: (1) depression recognition (i.e., CND vs. CN classification) on LLD, (2) CI identification (i.e., CI vs. CND classification) on LLD, and (3) MCI detection (i.e., MCI vs. HC classification) on DM. The partition of training/test set is given in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of fourteen methods in the MRI-based MCI detection task (i.e., MCI vs. HC classification) on the DM study.</figDesc><table><row><cell>Method</cell><cell cols="2">AUC (%) ACC (%) SEN (%) SPE (%) F1s (%)</cell></row><row><cell>SVM-GM</cell><cell cols="2">52.5(1.7) 50.5(2.1)* 42.0(10.4) 59.0(9.6) 45.3(7.3)</cell></row><row><cell>SVM-WM</cell><cell cols="2">41.4(1.9) 47.5(1.8)* 43.0(13.0) 52.0(14.0) 44.0(9.2)</cell></row><row><cell>XGB-GM</cell><cell>39.1(5.2) 46.5(3.4)* 42.0(4.5)</cell><cell>51.0(4.2) 43.9(4.0)</cell></row><row><cell>XGB-WM</cell><cell>61.5(3.2) 55.5(2.1)* 48.0(2.7)</cell><cell>63.0(2.7) 51.9(2.5)</cell></row><row><cell>ResNet18</cell><cell>58.8(1.4) 55.5(2.7)* 60.0(7.1)</cell><cell>51.0(2.2) 57.3(4.5)</cell></row><row><cell>ResNet34</cell><cell>60.9(2.9) 55.5(1.1)* 59.0(5.5)</cell><cell>52.0(4.5) 56.9(2.7)</cell></row><row><cell>ResNet50</cell><cell>60.8(2.5) 58.0(2.1) 65.0(7.9)</cell><cell>51.0(4.2) 60.6(4.1)</cell></row><row><cell>Med3D18</cell><cell cols="2">56.0(3.1) 53.5(4.9) 58.0(22.5) 49.0(12.9) 53.9(11.7)</cell></row><row><cell>Med3D34</cell><cell cols="2">55.1(6.9) 51.5(5.8) 59.0(14.8) 44.0(20.7) 54.4(6.5)</cell></row><row><cell>Med3D50</cell><cell>61.3(5.4) 57.5(5.3) 63.0(6.7)</cell><cell>52.0(8.4) 59.7(5.0)</cell></row><row><cell>SEResNet</cell><cell cols="2">62.8(1.9) 59.0(2.9) 63.0(16.4) 55.0(17.0) 59.8(7.0)</cell></row><row><cell cols="3">EfficientNet 55.6(3.6) 53.5(8.0)* 64.0(19.8) 43.0(4.5) 56.9(12.5)</cell></row><row><cell>MobileNet</cell><cell cols="2">58.5(2.1) 53.0(4.1)* 59.0(26.1) 47.0(24.4) 53.3(13.5)</cell></row><row><cell cols="3">BAR (Ours) 67.6(1.7) 60.5(1.1) 66.0(10.8) 55.0(9.4) 62.2(4.5)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. L. Zhang, <rs type="person">G.G. Potter</rs>, and <rs type="person">M. Liu</rs> were supported by <rs type="funder">NIH</rs> grants <rs type="grantNumber">RF1AG073297</rs> and <rs type="grantNumber">R01MH108560</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_K9xVzKE">
					<idno type="grant-number">RF1AG073297</idno>
				</org>
				<org type="funding" xml:id="_w8GfUYW">
					<idno type="grant-number">R01MH108560</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multi-stream convolutional neural network for classification of progressive MCI in Alzheimer&apos;s disease using structural MCI images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashtari-Majlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dehshibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3918" to="3926" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A personalized computer-aided diagnosis system for mild cognitive impairment (MCI) using structural MRI (sMRI)</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E Z A</forename><surname>El-Gamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">5416</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Brain signatures based on structural MRI: classification for MCI, PMCI, and AD. Hum</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gonuguntla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2845" to="2860" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel conversion prediction method of MCI to AD based on longitudinal dynamic morphological features using ADNI structural MRIs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00415-020-09890-5</idno>
		<ptr target="https://doi.org/10.1007/s00415-020-09890-5" />
	</analytic>
	<monogr>
		<title level="j">J. Neurol</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2983" to="2997" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structural magnetic resonance imaging for the early diagnosis of dementia due to Alzheimer&apos;s disease in people with mild cognitive impairment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cochrane Database Syst. Rev</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anatomically interpretable deep learning of brain age captures domain-specific cognitive impairment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2214634120</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting neurodegenerative disease from MRI: A brief review on a deep learning perspective</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B T</forename><surname>Noor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Zenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al Mamun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BI 2019</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11976</biblScope>
			<biblScope unit="page" from="115" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37078-7_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37078-7_12" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diagnostic accuracy study of automated stratification of Alzheimer&apos;s disease and mild cognitive impairment via deep learning based on MRI</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review on Alzheimer&apos;s disease classification from normal controls and mild cognitive impairment using structural MR images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Choudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bodade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page">109745</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Translating research findings into clinical practice: a systematic and critical review of neuroimaging-based clinical tools for brain disorders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scarpazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparison of transfer learning and conventional machine learning applied to structural brain MRI for the early diagnosis and prognosis of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">576194</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging Off. J. Int. Soc. Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OASIS-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer disease</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lamontagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<date type="published" when="2019-12">2019-12 (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anatomy-guided brain tumor segmentation and classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BrainLes 2016</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Maier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Winzeck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Handels</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">10154</biblScope>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-55524-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-55524-9_16" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MRI segmentation and classification of human brain using deep learning for diagnosis of Alzheimer&apos;s disease: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yamanakkanavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3243</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">iBEAT V2.0: a multisiteapplicable, deep learning-based pipeline for infant cerebral cortical surface reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Protocols</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1488" to="1509" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Methodology and preliminary results from the neurocognitive outcomes of depression in the elderly study</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Steffens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Geriatr. Psychiatry Neurol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="202" to="211" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Negative affectivity, aging, and depression: results from the neurobiology of late-life depression (NBOLD) study</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Steffens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Pearlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Geriatr. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1135" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated anatomical labelling atlas 3</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joliot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">116189</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Region of interest based image classification: a study in MRI brain scan categorization. The University of Liverpool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S A</forename><surname>Elsayed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>United Kingdom</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector machine-based classification of Alzheimer&apos;s disease from whole-brain anatomical MRI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Magnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroradiology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="73" to="83" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="790" />
			<date type="published" when="2012">2012</date>
			<publisher>FSL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Schnyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">XGBoost: a scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EfficientNet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DomainATM: domain adaptation toolbox for medical data analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page">119863</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models genesis. Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101840</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Preservational learning improves self-supervised medical image models by reconstructing diverse contexts</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3499" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
