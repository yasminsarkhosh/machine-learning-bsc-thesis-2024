<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model</title>
				<funder ref="#_8BcXCUa #_HJ2jymn #_rWGvsYx #_DpRhxvV #_CQtxPtx #_2PKhvCn #_ccrrf7J">
					<orgName type="full">National Institute of Health</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford School of Medicine Department of Psychiatry and Behavioral Sciences Faculty Development and Leadership Award</orgName>
				</funder>
				<funder ref="#_Ky2VCEt">
					<orgName type="full">Ministry of Science and ICT of KOREA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
							<idno type="ORCID">0000-0002-2892-5764</idno>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
							<idno type="ORCID">0000-0002-0579-7763</idno>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomas</forename><surname>Bosschieter</surname></persName>
							<idno type="ORCID">0000-0001-9726-6400</idno>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
							<idno type="ORCID">0000-0001-7476-1046</idno>
							<affiliation key="aff1">
								<orgName type="department">Daegu Gyeongbuk Institute of Science and Technology</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyu</forename><surname>Zhao</surname></persName>
							<idno type="ORCID">0000-0002-6368-0889</idno>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kilian</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
							<email>kpohl@stanford.edu</email>
							<idno type="ORCID">0000-0001-5416-5159</idno>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="14" to="24"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C89BF15906C4CE7F8F2CA0721545572D</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As acquiring MRIs is expensive, neuroscience studies struggle to attain a sufficient number of them for properly training deep learning models. This challenge could be reduced by MRI synthesis, for which Generative Adversarial Networks (GANs) are popular. GANs, however, are commonly unstable and struggle with creating diverse and highquality data. A more stable alternative is Diffusion Probabilistic Models (DPMs) with a fine-grained training strategy. To overcome their need for extensive computational resources, we propose a conditional DPM (cDPM) with a memory-efficient process that generates realistic-looking brain MRIs. To this end, we train a 2D cDPM to generate an MRI subvolume conditioned on another subset of slices from the same MRI. By generating slices using arbitrary combinations between condition and target slices, the model only requires limited computational resources to learn interdependencies between slices even if they are spatially far apart. After having learned these dependencies via an attention network, a new anatomy-consistent 3D brain MRI is generated by repeatedly applying the cDPM. Our experiments demonstrate that our method can generate high-quality 3D MRIs that share a similar distribution to real MRIs while still diversifying the training set. The code is available at https://github. com/xiaoiker/mask3DMRI_diffusion and also will be released as part of MONAI, at https://github.com/Project-MONAI/GenerativeModels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The synthesis of medical images has great potential in aiding tasks like improving image quality, imputing missing modalities <ref type="bibr" target="#b29">[30]</ref>, performing counterfactual analysis <ref type="bibr" target="#b16">[17]</ref>, and modeling disease progression <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref>. However, synthesizing brain MRIs is non-trivial as they are of high dimension, yet the training data are relatively small in size (compared to 2D natural images). High-quality synthetic MRIs have been produced by conditional models based on real MRI of the same subject acquired with different MRI sequences <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. However, such models require large data sets (which are difficult to get) and fail to significantly improve data diversity <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>, i.e., producing MRIs substantially deviating from those in the training data; data diversity is essential to the generalizability of large-scale models <ref type="bibr" target="#b25">[26]</ref>. Unconditional models based on Generative Adversarial Networks (GANs) bypass this drawback by generating new, independent MRIs from random noise <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. However, these models often produce lower quality MRIs as they currently can only be trained on lower resolution MRIs or 2D slices due to their computational needs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. Furthermore, GANbased models are known to be unstable during training and even can suffer from mode collapse <ref type="bibr" target="#b3">[4]</ref>. An alternative is diffusion probabilistic models (DPMs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, which formulate the fine-grained mapping between data distribution and Gaussian noise as a gradual process modeled within a Markov chain. Due to their multi-step, fine-grained training strategy, DPMs tend to be more stable during training than GANs and therefore are more accurate for certain medical imaging applications, such as segmentation and anomaly detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. However, DPMs tend to be computationally too expensive to synthesize brain MRI at full image resolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. We address this issue by proposing a memory-efficient 2D conditional DPM (cDPM) that relies on learning the interdependencies between 2D slices to produce high-quality 3D MRI volumes. Unlike the sequence of 2D images defining a video, all 2D slices of an MRI are interconnected with each other as they define a 3D volume capturing brain anatomy. Our cDPM learns these interdependencies (even between distant slices) by training an attention network <ref type="bibr" target="#b23">[24]</ref> on arbitrary combinations of condition and target slices. Once learned, the cDPM creates new samples while capturing brain anatomy in 3D. It does so by producing the first few slices from random noise and then using those slices to synthesize subsequent ones (see Fig. <ref type="figure" target="#fig_0">1</ref>). We show that this computationally efficient conditional DPM can produce MRIs that are more realistic than those produced by GAN-based architectures. Furthermore, our experiments reveal that cDPM is able to generate synthetic MRIs, whose distribution matches that of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We first review the basic DPM framework for data generation (Sect. 2.1). Then, we introduce our efficient strategy for generating 3D MRI slices (Sect. 2.2) and finally describe the neural architecture of cDPMs (Sect. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Diffusion Probabilistic Model</head><p>The Diffusion Probabilistic Model (DPM) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> generates MRIs from random noise by iterating between mapping 1) data gradually to noise (a.k.a., Forward Diffusion Process) and 2) noise back to data (a.k.a., Reverse Diffusion Process).</p><p>Forward Diffusion Process (FDP). Let real data X 0 ∼ q sampled from the (real data) distribution q be the input to the FDP. FDP then simulates the diffusion process that turns X 0 after T perturbations into Gaussian noise X T ∼ N (0, I ), where N is the Gaussian distribution with zero mean and the variance being the identity matrix I. This process is formulated as a Markov chain, whose transition kernel q(X t |X t-1 ) at time step t ∈ {0, . . . , T } is defined as</p><formula xml:id="formula_0">q(X t |X t-1 ) := N (X t ; 1 -β t • X t-1 , β t • I).<label>(1)</label></formula><p>The weight β t ∈ (0, 1) is changed so that the chain gradually enforces drift, i.e., adds Gaussian noise to the data. Let α t := 1β t and ᾱt :=</p><formula xml:id="formula_1">t s=1 (1 -β t ), then X t is a sample of the distribution conditioned on X 0 as q(X t |X 0 ) := N (X t ; √ ᾱt • X 0 , (1 -ᾱt ) • I). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Given this closed-form solution, we can sample X t at any arbitrary time step t without needing to iterate through the entire Markov chain.</p><p>Reverse Diffusion Process (RDP). The RDP aims to generate realistic data from random noise X T by approximating the posterior distribution p(X t-1 |X t ).</p><p>It does so by going through the entire Markov chain from time step T to 0, i.e.,</p><formula xml:id="formula_3">p(X 0:T ) := p(X T ) T t=1 p θ (X t-1 |X t ).<label>(3)</label></formula><p>Defining the conditional distribution p θ (X t-1 |X t ) := N (X t-1 ; μ θ (X t , t), Σ) with fixed variance Σ, then (according to <ref type="bibr" target="#b7">[8]</ref>) the mean can be rewritten as</p><formula xml:id="formula_4">μ θ (X t , t) = 1 √ α t X t - β t (1 -ᾱt ) θ (X t , t) ,<label>(4)</label></formula><p>with θ (•) being the estimate of a neural network defined by parameters θ. θ minimizes the reconstructing loss defined by the following expected value E X0∼q,t∈[0,...,T], ∼N (0,I) ||θ (X t , t)|| 2  2 , where || • || 2 is the L2 norm, and X t is inferred from Eq. ( <ref type="formula" target="#formula_1">2</ref>) based on X 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Generation with DPM (cDPM)</head><p>To synthetically create high-resolution 3D MRI, we propose an efficient cDPM model that learns the interdependencies between 2D slices of an MRI so that it can generate slices based on another set of already synthesized ones (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Specifically, given an MRI X ∈ R D×H×W , we randomly sample two sets of slice indexes: the condition set C and the target set P. Let len(•) be the number of slices in a set, then the 'condition' slices are defined as X C ∈ R len(C)×H×W and the 'target' slices as X P ∈ R len(P)×H×W with len(P) ≥ 1. Confining the FDP of Sect. 2.1 just to the target X P , the RDP now aims to reconstruct X P t for each time t = T, T -1, . . . , 0 starting from random noise at t = T and conditioned on X C . Let X t be the subvolume consisting of X P t and X C , then the joint distribution of the Markov chain defined by Eq. (3) now reads</p><formula xml:id="formula_5">p(X P 0:T ) := p(X P T ) T t=1 p θ (X P t-1 | X t ).<label>(5)</label></formula><p>Observe that Eq. ( <ref type="formula" target="#formula_5">5</ref>) is equal to Eq. ( <ref type="formula" target="#formula_3">3</ref>) in case len(C) = 0.</p><p>To estimate μ θ ( X t , t) as described in Eq. ( <ref type="formula" target="#formula_4">4</ref>), we sample arbitrary index sets C and P so that len(C) + len(P) ≤ τ max , where τ max is the maximum number of slices based on the available resources. We then capture the dependencies across slices by feeding the index sets C and P and the corresponding slices (i.e., X C and X P t built from X 0 ∼ q) into an attention network <ref type="bibr" target="#b19">[20]</ref>. The neural network aims to minimize the canonical loss function Loss(θ) := E X0∼q, ∼N (0,I),C+P≤τmax,t ||θ (X P t , X C , C, P, t)|| 2 2 .</p><p>(6)</p><p>Fig. <ref type="figure">2</ref>. The architecture of cDPM is a U-shape neural network with skip connections and the input at step 't' are slice indexes {C, P}, condition sub-volume X C , and current target sub-volume X P t .</p><p>As the neural network can now be trained on many different (arbitrary) slice combinations (defined by C and P), the cDPM only requires a relatively small number of MRIs for training. Furthermore, it will learn short-and longrange dependencies across slices as the spatial distance between slices from C and P varies. Learning these dependencies (after being trained for a sufficiently large number of iterations) enables cDPMs to produce 2D slices that, when put together, result in realistic looking, high-resolution 3D MRIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Architecture</head><p>As done by <ref type="bibr" target="#b7">[8]</ref>, cDPMs are implemented as a U-Net <ref type="bibr" target="#b18">[19]</ref> with a time embedding module (see Fig. <ref type="figure">2</ref>). We add a multi-head self-attention mechanism <ref type="bibr" target="#b23">[24]</ref> to model the relationship between slices. After training the cDPM as in Fig. <ref type="figure" target="#fig_0">1</ref>, a 3D MRI is generated in N stages. Specifically, the cDPM produces the initial set of slices of that MRI volume from random noise (i.e., unconditioned). Conditioned on those synthetic slices, the cDPM then runs again to produce a new set of slices. The process of synthetically creating slices based on ones generated during prior stages is repeated until an entire 3D MRI is produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We use 1262 t1-weighted brain MRIs of subjects from three different datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI-1), UCSF (PI: V. Valcour), and SRI International (PI: E.V. Sullivan and A. Pfefferbaum) <ref type="bibr" target="#b27">[28]</ref>. Processing includes denoising, bias field correction, skull stripping, and affine registration to a template, and normalizing intensity values between 0 and 1. In addition, we padded and resized the MRIs to have dimensions 128 × 128 × 128 resulting in a voxel resolution of 1.375 mm × 1.375 mm × 1.0 mm. Splitting the MRI along the axial direction results in 2D slices. Note, this could have also been done along the sagittal or coronal direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Our experiments are conducted on an NVIDIA A100 GPU using the PyTorch framework. The model is trained using 200,000 iterations with the AdamW optimizer adopting a learning rate of 10 -4 and a batch size of 3. τ max is set to 20. After the training, cDPM generates a synthetic MRI consisting of 128 slices by following the process outlined in Fig. <ref type="figure" target="#fig_0">1</ref> in N=13 stages. Each stage generates 10 slices starting with pure noise (X C = ∅) and (after the first stage) being conditioned on the 10 slices produced by the prior stage. After training on all real MRIs, we use the resulting conditional DPM to generate 500 synthetic MRIs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative Comparison</head><p>We evaluate the quality of synthetic MRIs based on 3 metrics: (i) computing the distance between synthetic and 500 randomly selected real MRIs via the Maximum-Mean Discrepancy (MMD) score <ref type="bibr" target="#b4">[5]</ref>, (ii) measuring the diversity of the synthetic MRIs via the pair-wise multi-scale Structure Similarity (MS-SSIM) <ref type="bibr" target="#b11">[12]</ref>, and (iii) comparing the distributions of synthetic to real MRIs with respect to the 3 views via the Frèchet Inception Distance (FID) <ref type="bibr" target="#b25">[26]</ref> (a.k.a, FID-Axial, FID-Coronal, FID-Sagittal).</p><p>We compare those scores to ones produced by six recently published methods: (i) 3D-DPM <ref type="bibr" target="#b2">[3]</ref>, (ii) 3D-VAE-GAN <ref type="bibr" target="#b13">[14]</ref>, (iii) 3D-GAN-GP <ref type="bibr" target="#b5">[6]</ref>, (iv) 3Dα-WGAN <ref type="bibr" target="#b11">[12]</ref>, (v) CCE-GAN <ref type="bibr" target="#b25">[26]</ref>, and (vi) HA-GAN <ref type="bibr" target="#b22">[23]</ref>. We needed to reimplement the first 5 methods and used the open-source code available for HA-GAN. 3D-DPM was only able to generate 32 slices at a time (due to GPU limitations) so that we computed its quality metrics by also cropping the corresponding real MRI to those 32 slices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Qualitative Results. The center of the axial, coronal, and sagittal views of five MRIs generated by cDPM shown in Fig. <ref type="figure" target="#fig_1">3</ref> look realistic. Compared to the MRIs produced by the other approaches other than 3D-DPM (see Fig. <ref type="figure" target="#fig_2">4</ref>), the MRIs of cDPM are sharper; specifically, the gray matter boundaries are more distinct and the scan provides greater anatomical details. As expected, 3D-DPM produced synthetic slices of similar quality as cDPM but failed to do so for the entire MRI.</p><p>The synthetic MRIs of cDPM shown in Fig. <ref type="figure" target="#fig_1">3</ref> are also substantially different from each other, suggesting that our method could be used to create an augmented data set that is anatomically diverse. Figure <ref type="figure" target="#fig_3">5</ref> further substantiates the claim, which plots the t-SNE embedding <ref type="bibr" target="#b14">[15]</ref> of 200 synthetic MRIs (blue) and their closest real counterpart (orange) according to MS-SSIM for each method. Note, matching of all 500 synthetic MRIs was computationally too expensive to perform (takes days to complete per method). Based on those plots, cDPM is the only approach able to generate MRIs, whose distribution resembled that of the real MRIs. This finding is somewhat surprising given that the MRI subvolumes generated by 3D-DPM looked real. Unlike the real data, however, their distributions are clustered around the average. Thus, 3D-DPM fails to diversify the data set even if (in the future) more computational resources would allow the method to generate a complete 3D MRI. Quantitative Results. Table <ref type="table" target="#tab_0">1</ref> lists the average scores of MS-SSIM, MMD, and FID for each method. Among all models that generated complete MRI volumes, cDPM performed best. Only the absolute difference between the MS-SSIM score of 3D-DPM and the real MRIs was slightly lower (i.e., 0.005) than the absolute difference for cDPM (i.e., 0.006). This comparison, however, is not fair as the MS-SSIM score for 3D-DPM was only computed on 32 slices. Further supporting this argument is that FID-A (the only score computed for the same slice across all methods) was almost 5 times worse for 3D-DPM than cDPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel conditional DPM (cDPM) for efficiently generating 3D brain MRIs. Starting with random noise, our model can progressively generate MRI slices based on previously generated slices. This conditional scheme enables training the cDPM with limited computational resources and training data. Qualitative and quantitative results demonstrate that the model is able to produce high-fidelity 3D MRIs and outperform popular and recent generative models such as the CCE-GAN and 3D-DPM. Our framework can easily be extended to other imaging modalities and can potentially assist in training deep learning models on a small number of samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A memory efficient DPM. Left: Based on 'condition' slices, cDPM learns to generate 'target' slices. Right: A new 3D MRI is created by repeatedly running the trained model to synthesize target slices conditioned on those it created in prior stages.</figDesc><graphic coords="2,55,98,276,56,340,24,130,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 5 MRIs generated by our conditional DPM visualized in the axial, coronal, and sagittal plane. The example in the first row is enlarged to highlight the high quality of synthetic MRIs generated by our approach.</figDesc><graphic coords="6,55,98,66,92,340,18,219,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. 3 views of MRIs generated by 7 models. Compared to the MRIs produced by the other approaches, our cDPM model generates the most realistic MRI scans that provide more distinct gray matter boundaries and greater anatomical details. (Color figure online)</figDesc><graphic coords="6,55,98,364,10,340,24,168,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Left: One MRI generated by our model and its closest real MRI based on MS-SSIM. Right: tSNE embedding of 200 generated samples (blue) of each model and their closest real MRIs (orange). Only our model generated independent and diverse samples as the data points overlay but are not identical to the training data. (Color figure online)</figDesc><graphic coords="7,63,30,264,23,297,64,162,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Measuring the quality of 500 synthetic MRIs. '( )' contains the absolute difference to the MS-SSIM score of the real MRIs, which was 0.792. In bold are the optimal scores among methods that generate the entire volume. Scores denoted with an asterisk '*' are only computed on 32 slices.</figDesc><table><row><cell></cell><cell>MS-SSIM</cell><cell>MMD↓</cell><cell cols="3">FID-A ↓ FID-C ↓ FID-S ↓</cell></row><row><cell></cell><cell>(%)</cell><cell>(10 3 )</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3D-VAE-GAN [14] 88.3 (9.1)</cell><cell>5.15</cell><cell>320</cell><cell>247</cell><cell>398</cell></row><row><cell>3D-GAN-GP [6]</cell><cell>81.0 (1.8)</cell><cell>15.7</cell><cell>141</cell><cell>127</cell><cell>281</cell></row><row><cell cols="2">3D-α-WGAN [12] 82.6 (3.4)</cell><cell>13.2</cell><cell>121</cell><cell>116</cell><cell>193</cell></row><row><cell>CCE-GAN [26]</cell><cell>81.5 (2.3)</cell><cell>3.54</cell><cell>69.4</cell><cell>869</cell><cell>191</cell></row><row><cell>HA-GAN [23]</cell><cell cols="2">36.8 (42.4) 226</cell><cell>477</cell><cell>1090</cell><cell>554</cell></row><row><cell>3D-DPM [3]</cell><cell cols="2">79.7 (0.5)* 15.2*</cell><cell>188</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (cDPM)</cell><cell cols="2">78.6 (0.6) 3.14</cell><cell>32.4</cell><cell>45.8</cell><cell>91.1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was partly supported by funding from the <rs type="funder">National Institute of Health</rs> (<rs type="grantNumber">MH113406</rs>, <rs type="grantNumber">DA057567</rs>, <rs type="grantNumber">AA021697</rs>, <rs type="grantNumber">AA017347</rs>, <rs type="grantNumber">AA010723</rs>, <rs type="grantNumber">AA005965</rs>, and <rs type="grantNumber">AA028840</rs>), the <rs type="programName">DGIST R&amp;D program</rs> of the <rs type="funder">Ministry of Science and ICT of KOREA</rs> (<rs type="grantNumber">22-KUJoint-02</rs>), <rs type="funder">Stanford School of Medicine Department of Psychiatry and Behavioral Sciences Faculty Development and Leadership Award</rs>, and by the <rs type="institution">Stanford HAI Google Cloud Credit</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8BcXCUa">
					<idno type="grant-number">MH113406</idno>
				</org>
				<org type="funding" xml:id="_HJ2jymn">
					<idno type="grant-number">DA057567</idno>
				</org>
				<org type="funding" xml:id="_rWGvsYx">
					<idno type="grant-number">AA021697</idno>
				</org>
				<org type="funding" xml:id="_DpRhxvV">
					<idno type="grant-number">AA017347</idno>
				</org>
				<org type="funding" xml:id="_CQtxPtx">
					<idno type="grant-number">AA010723</idno>
				</org>
				<org type="funding" xml:id="_2PKhvCn">
					<idno type="grant-number">AA005965</idno>
				</org>
				<org type="funding" xml:id="_ccrrf7J">
					<idno type="grant-number">AA028840</idno>
					<orgName type="program" subtype="full">DGIST R&amp;D program</orgName>
				</org>
				<org type="funding" xml:id="_Ky2VCEt">
					<idno type="grant-number">22-KUJoint-02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning implicit brain MRI manifolds with deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Plassard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Newton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page" from="408" to="414" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image synthesis in multi-contrast MRI with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Çukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2375" to="2388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-dimensional medical image synthesis with denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dorjsembe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Odonchimed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Oz7lKWVh45H" />
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5769" to="5779" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GAN-based synthetic brain MR image generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<biblScope unit="page" from="734" to="738" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional GAN with an attention-based generator and a 3D discriminator for 3D medical image generation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_31" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conditional GAN with 3D discriminator for MRI generation of Alzheimer&apos;s disease progression. Pattern Recogn</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109061</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generation of 3D brain MRI using auto-encoding generative adversarial networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anatomically constrained CT image translation for heterogeneous blood vessel segmentation</title>
		<author>
			<persName><forename type="first">La</forename><surname>Barbera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Virtual Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">776</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation disentanglement for multi-modal brain MRI Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-0_25" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="321" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep structural causal models for tractable counterfactual inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="857" to="869" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Brain imaging generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Generative Models: DGM4MICCAI 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2074</idno>
		<ptr target="https://aclanthology.org/N18-2074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical image synthesis for data augmentation and anonymization using generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">11037</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical amortized GAN for 3D high resolution medical image synthesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3966" to="3975" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diffusion models for medical anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cycle consistent embedding of 3D brains with auto-encoding generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D cGAN based cross-modality MR image synthesis for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-label, multi-domain learning identifies compounding effects of HIV and cognitive impairment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102246</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Longitudinal self-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102051</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffusion models for missing value imputation in tabular data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Charoenphakdee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Table Representation Learning (TRL) Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
