Paper Title,Header Number,Header Title,Text
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,1.0,Introduction,"Despite their remarkable success in numerous tasks, deep learning models trained on a source domain face the challenges to generalize to a new target domain, especially for segmentation which requires dense pixel-level prediction. This is attributed to a large semantic gap between these two domains. Unsupervised Domain Adaptation (UDA) has lately been investigated to bridge this semantic gap between labeled source domain, and unlabeled target domain [27], including adversarial learning for aligning latent representations [23], image translation networks [24], etc. However, these methods produce subpar performance because of the lack of supervision from the target domain and a large semantic gap in style and content information between the source and target domains. Moreover, when an image's content-specific information is entangled with its domain-specific style information, traditional UDA approaches fail to learn the correct representation of the domain-agnostic content while being distracted by the domain-specific styles. So, they cannot be generalized for multi-domain segmentation tasks [4].Compared to UDA, obtaining annotation for a few target samples is worthwhile if it can substantially improve the performance by providing crucial target domain knowledge. Driven by this speculation, and the recent success of semisupervised learning (SemiSL), we investigate semi-supervised domain adaptation (SSDA) as a potential solution. Recently, Liu et al. [14] proposed an asymmetric co-training strategy between a SemiSL and UDA task, that complements each other for cross-domain knowledge distillation. Xia et al. [22] proposed a co-training strategy through pseudo-label refinement. Gu et al. [7] proposed a new SSDA paradigm using cross-domain contrastive learning (CL) and selfensembling mean-teacher. However, these methods force the model to learn the low-level nuisance variability, which is insignificant to the task at hand, hence failing to generalize if similar variational semantics are absent in the training set. Fourier Domain Adaptation (FDA) [26] was proposed to address these challenges by an effective spectral transfer method. Following [26], we design a new Gaussian FDA to handle this cross-domain variability, without feature alignment.Contrastive learning (CL) is another prospective direction where we enforce models to learn discriminative information from (dis)similarity learning in a latent subspace [2,10]. Liu et al. [15] proposed a margin-preserving constraint along with a self-paced CL framework, gradually increasing the training data difficulty. Gomariz et al. [6] proposed a CL framework with an unconventional channel-wise aggregated projection head for inter-slice representation learning. However, traditional CL utilized for DA on images with entangled style and content leads to mixed representation learning, whereas ideally, it should learn discriminative content features invariant to style representation. Besides, the instance-level feature alignment of CL is subpar for segmentation, where dense pixel-wise predictions are indispensable [1].To alleviate these three underlined shortcomings, we propose a novel contrastive learning with pixel-level consistency constraint via disentangling the style and content information from the joint distribution of source and target domain. Precisely, our contributions are as follows: (1) We propose to disentangle the style and content information in their compact embedding space using a joint-learning framework; (2) We propose encoder pre-training with two CL strategies: Style CL and Content CL that learns the style and content information respectively from the embedding space; (3) The proposed CL is complemented with a pixel-level consistency constraint with dense feature propagation module, where the former provides better categorization competence whereas the later enforces effective spatial sensitivity; (4) We experimentally validate that our SSDA method can be extended in the UDA setting easily, achieving superior performance as compared to the SoTA methods on two widely-used domain adaptive segmentation tasks, both in SSDA and UDA settings."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.0,Proposed Method,"Given the source domain image-label pairs {(x i s , y i s ) Ns i=1 ∈ S}, a few image-label pairs from target domain {(x i t1 , y i t1 ) Nt1 i=1 ∈ T 1}, and a large number of unlabeled target images {(x i t2 ) Nt2 i=1 ∈ T 2}, our proposed pre-training stage learns from images in {S ∪ T ; T = T 1 ∪ T 2} in a self-supervised way, without requiring any labels. The following fine-tuning in SSDA considers image-label pairs in {S ∪T 1} for supervised learning alongside unlabeled images T 2 in the target domain for unsupervised prediction consistency. Our workflow is shown in Fig. 1."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.1,Gaussian Fourier Domain Adaptation (GFDA),"Manipulating the low-level amplitude spectrum of the frequency domain is the easiest way for style transfer between domains [26], without notable alteration in the visuals of high-level semantics. However, as observed in [26], the generated images consist of incoherent dark patches, caused by abrupt changes in amplitude around the rectangular mask. Instead, we propose a Gaussian mask for a smoother transition in frequency. Let, F A (•) and F P (•) be the amplitude and phase spectrum in frequency space of an RGB image, and F -1 indicates inverse Fourier transform. We define a 2D Gaussian mask g σ of the same size as F A , with σ being the standard deviation. Given two randomly sampled images x s ∼ S and x t ∼ T , our proposed GFDA can be formulated as:where indicates element-wise multiplication. It generates an image preserving the semantic content from S but preserving the style from T . Reciprocal pair x t→s is also formulated using the same drill. The source and target images, and the style-transferred versions {x s , x s→t , x t , x t→s } are then used for contrastive pre-training below. Visualization of GFDA is shown in the supplementary file."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.2,CL on Disentangled Domain and Content,"We aim to learn discriminative content-specific features that are invariant of the style of the source or target domain, for a better pre-training of the network for the task at hand. Hence, we propose to disentangle the style and content information from the images and learn them jointly in a novel disentangled CL paradigm: Style CL (SCL) and Content CL (CCL). The proposed SCL imposes learning of domain-specific attributes, whereas CCL enforces the model to identify the ROI, irrespective of the spatial semantics and appearance. In joint learning, they complement each other to render the model to learn domain-agnostic and content-specific information, thereby mitigating the domain dilemma. The set of images {x s , x s→t , x t , x t→s }, along with their augmented versions are passed through encoder E, followed by two parallel projection heads, namely style head (G S ) and content head (G C ) to obtain the corresponding embeddings. Two different losses: style contrastive loss L SCL and content contrastive loss L CCL , are derived below. Assuming {x s , x t→s } (along with their augmentations) having source-style representation (style A), and {x t , x s→t } (and their augmentations) having target-style representation (style B), in style CL, embeddings from the same domain (style) are grouped together whereas embeddings from different domains are pushed apart in the latent space. Considering the i th anchor point x i t ∈ T in a minibatch and its corresponding style embedding s i t ← G S (E(x i t )) (with style B), we define the positive set consisting of the same target domain representations as Λ + = {s j+ t , s j+ s→t } ← G S (E({x j t , x j s→t })), ∀j ∈ minibatch, and negative set having unalike source domain representation as Λ -= {s j- s , s j- t→s } ← G S (E({x j s , x j t→s })), ∀j ∈ minibatch. Following SimCLR [5] our style contrastive loss can be formulated as:where {s i , s j+ } ∈ style B; s j-∈ style A, sim(•, •) defines cosine similarity, τ is the temperature parameter [5]. Similarly, we define L CCL for content head as:where {c i , c j } ← G C (E({x i , x j })). These contrastive losses, along with the consistency constraint below enforce the encoder to extract domain-invariant and content-specific feature embeddings."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.3,Consistency Constraint,"The disentangled CL aims to learn global image-level representation, which is useful for instance discrimination tasks. However, segmentation is attributed to learning dense pixel-level representations. Hence, we propose an additional Dense Feature Propagation Module (DFPM) along with a momentum encoder E with exponential moving average (EMA) of parameters from E. Given any pixel m of an image x, we transform its feature f m E obtained from E by propagating other pixel features from the same image:where K is a linear transformation layer, ⊗ denotes matmul operation. This spatial smoothing of learned representation is useful for structural sensitivity, which is fundamental for dense segmentation tasks. We enforce consistency between this smoothed feature fE from E and the regular feature f E from E as:where d(•, •) indicates the spatial distance, T h is a threshold. The overall pretraining objective can be summarized as:"
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.4,Semi-supervised Fine-Tuning,"The pre-training stage is followed by semi-supervised fine-tuning using a studentteacher framework [18]. The pre-trained encoder E, along with a decoder D are used as a student branch, whereas an identical encoder-decoder network (but differently initialized) is used as a teacher network. We compute a supervised loss on the labeled set {S ∪ T 1} along with a regularization loss between the prediction of the student and teacher branches on the unlabeled set {T 2} as:where CE indicates cross-entropy loss, E S , D S , E T , D T indicate the student and teacher encoder and decoder networks. The student branch is updated using a consolidated loss L = L Sup + λ 3 L Reg , whereas the teacher parameters (θ T ) are updated using EMA from the student parameters (θ S ):where t tracks the step number, and α is the momentum coefficient [9]. In summary, the overall SSDA training process contains pre-training (Subsect. 2.1-Subsect. 2.3) and fine-tuning (Subsect. 2.4), whereas, we only use the student branch (E S , D S ) for inference."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.0,Experiments and Results,"Datasets: We evaluate our work on two different DA tasks to evaluate its generalizability: (1) Polyp segmentation from colonoscopy images in Kvasir-SEG [11] and CVC-EndoScene Still [20], and (2) Brain tumor segmentation in MRI images from BraTS2018 [16]. Kvasir and CVC contain 1000 and 912 images respectively and were split into 4 : 1 training-testing sets following [10]. BraTS consists of brain MRIs from 285 patients with T1, T2, T1CE, and FLAIR scans. The data was split into 4 : 1 train-test ratio, following [14]. Source→Target: We perform experiments on CV C → Kvasir and Kvasir → CV C for polyp segmentation, and T 2 → {T 1, T 1CE, F LAIR} for tumor segmentation. The SSDA accesses 10 -50% and 1 -5 labels from the target domain for the two tasks, respectively. For UDA, only S is used for L Sup , whereas T 1 ∪ T 2 is used for L Reg . Implementation details: Implementation is done in a PyTorch environment using a Tesla V100 GPU with 32GB RAM. We use U-Net [17] backbone for the encoder-decoder structure, and the projection heads G S and G C are shallow FC layers. The model is trained for 300 epochs for pre-training and 500 epochs for fine-tuning using an ADAM optimizer with a batch size of 4 and a learning rate of 1e -4. λ1, λ2, λ3, and T h are set to 0.75, 0.75, 0.5, 0.6, respectively by validation, τ, α are set to 0.07, 0.999 following [9]. Augmentations include random rotation and translation. Metrics: Segmentation performance is evaluated using Dice Similarity Score (DSC) and Hausdorff Distance (HD)."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.1,Performance on SSDA,"Quantitative comparison of our proposed method with different SSDA methods [4,14,21,24] for both tasks are shown in Table 1 andTable 2. ACT [14] simply ignores the domain gap and only learns content semantics, resulting in substandard performance on the BraTS dataset that has a significant domain gap. FSM [24], on the other hand, is adaptable to learning explicit domain information, but Table 1. Comparison with state-of-the-art UDA and SSDA methods for polyp segmentation on KVASIR and CVC. SSDA results are shown for 10%-labeled (10%L) and 50%-labeled (50%L) data in the target domain. The results of cited methods are directly reported from the corresponding papers. No DA: the encoder-decoder model trained only using labeled data from the source domain is applied to the target domain without adaptation. Supervised: model is trained using all labeled data from source and target domains. The best and second-best results are highlighted in RED and BLUE, respectively. lacks strong pixel-level regularization on its prediction, resulting in subpar performance. We address both of these shortcomings in our work, resulting in superior performance on both tasks. Other methods like [4,21], which are originally designed for natural images, lack critical refining abilities even after fine-tuning for medical image segmentation and hence are far behind our performance in both tasks. The margins are even higher for less labeled data (1L) on the BraTS dataset, which is promising considering the difficulty of the task. Moreover, our method produces performance close to its fully-supervised counterpart (last row in Table 1 and Table 2), using only a few target labels."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.2,Performance on UDA,"Unlike SSDA methods, UDA fully relies on unlabeled data for domain-invariant representation learning. To analyze the effectiveness of DA, we extend our model to the UDA setting (explained in Sect. 3 [Source → Target]) and compare it with SoTA methods [3,8,10,12,13,25,26,28] in Table 1 and Table 2. Methods like [10,19] rely on adversarial learning for aligning multi-level feature space, which is not effective for small-sized medical data. Other methods [12,25] rely on an image-translation network but fail in effective style adaptation, resulting  in source domain-biased subpar performance. Our method, although relies on FDA [26], outperforms it with a large margin of upto 12.5% DSC for polyp segmentation, owing to its superior learning ability of disentangled style and content semantics. Similar results are observed for the BraTS dataset in Table 2, where our work achieved a margin of upto 2.4% DSC than its closest performer."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.3,Ablation Experiments,"We perform a detailed ablation experiment, as shown in Table 3. The effectiveness of disentangling and joint-learning of style and content information is evident from the experiment (b)&(c) as compared to (a), where the introduction of SCL and CCL boosts overall performance significantly. Moreover, when combined together (experiment (d)), they provide a massive 9.54% and 8.52% DSC gain over traditional CL (experiment (a)) for CV C → Kvasir and Kvasir → CV C, respectively. This also points out a potential shortfall of traditional CL: its inability to adapt to a complex domain in DA. The proposed DFPM (experiment (e)) provides local pixel-level regularization, complementary to the global disentangled CL, resulting in a further boost in performance (∼ 1.5%). We have similar ablation study observations on the BraTS2018 dataset, which is provided in the supplementary file, along with some qualitative examples along with available ground truth."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,4.0,Conclusion,"We propose a novel style-content disentangled contrastive learning, guided by a pixel-level feature consistency constraint for semi-supervised domain adaptive medical image segmentation. To the best of our knowledge, this is the first attempt for SSDA in medical image segmentation using CL, which is further extended to the UDA setting. Our proposed work, upon evaluation on two different domain adaptive segmentation tasks in SSDA and UDA settings, outperforms the existing SoTA methods, justifying its effectiveness and generalizability."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_25.
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,1.0,Introduction,"Medical image segmentation plays a crucial role in enabling better diagnosis, surgical planning, and image-guided surgery [8]. The inherent ambiguity and high uncertainty of medical images pose significant challenges [5] for accurate segmentation, attributed to factors such as unclear tumor boundaries in brain Magnetic Resonance Imaging (MRI) images and multiple plausible annotations of lung nodule in Computed Tomography (CT) images. Existing medical image segmentation methods typically provide a single, deterministic, most likely hypothesis mask, which may lead to misdiagnosis or sub-optimal treatment. Therefore, providing accurate and diverse segmentation masks as valuable references [17] for radiologists is crucial in clinical practice.Recently, diffusion models [10] have shown strong capacities in various visual generation tasks [21,22]. However, how to better deal with discrete segmentation tasks needs further consideration. Although many researches [1,26] have combined diffusion model with segmentation tasks, all these methods do not take full account of the discrete characteristic of segmentation task and still use Gaussian noise as their diffusion kernel.To achieve accurate and diverse segmentation masks, we propose a novel Conditional Bernoulli Diff usion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for segmentation, resulting in more accurate segmentation masks. Moreover, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernoulli noise and intermediate latent variables multiple times to produce a range of diverse segmentation masks, highlighting salient regions of interest (ROI) that can serve as a valuable reference for radiologists. In addition, our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion based on the rationale behind the Denoising Diffusion Implicit Models (DDIM) [25], thereby speeding up the segmentation process.The contributions of this work are summarized as follows. 1) Instead of using the Gaussian noise, we propose a novel conditional diffusion model based on the Bernoulli noise for discrete binary segmentation tasks, achieving accurate and diverse medical image segmentation masks. 2) Our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion, thereby speeding up the segmentation process. 3) Experimental results on LIDC-IDRI and BRATS 2021 datasets demonstrate that our BerDiff outperforms other state-of-the-art methods."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.0,Methodology,"In this section, we first describe the problem definitions, and then demonstrate the Bernoulli forward and diverse reverse processes of our BerDiff, as shown in Fig. 1. Finally, we provide an overview of the training and sampling procedures."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.1,Problem Definition,"Let us assume that x ∈ R H×W×C denotes the input medical image with a spatial resolution of H × W and C channels. The ground-truth mask is represented as y 0 ∈ {0, 1}H×W , where 0 represents background while 1 ROI. Inspired by diffusion-based models such as denoising diffusion probabilistic model (DDPM) and DDIM, we propose a novel conditional Bernoulli diffusion model, which can be represented as p θ (y 0 |x) := p θ (y 0:T |x)dy 1:T , where y 1 , . . . , y T are latent variables of the same size as the mask y 0 . For medical binary segmentation tasks, the diverse reverse process of our BerDiff starts from the initial Bernoulli noise y T ∼ B(y T ; 1 2 •1) and progresses through intermediate latent variables constrained by the input medical image x to produce segmentation masks, where 1 denotes an all-ones matrix of the size H ×W ."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.2,Bernoulli Forward Process,"In previous generation-related diffusion models, Gaussian noise is progressively added with increasing timestep t. However, for segmentation tasks, the groundtruth masks are represented by discrete values. To address this, our BerDiff gradually adds more Bernoulli noise using a noise schedule β 1 , . . . , β T , as shown in Fig. 1. The Bernoulli forward process q(y 1:T |y 0 ) of our BerDiff is a Markov chain, which can be represented as:where B denotes the Bernoulli distribution with the probability parameters (1 -Using the notation α t = 1β t and ᾱt = t τ =1 α τ , we can efficiently sample y t at an arbitrary timestep t in closed form:(To ensure that the objective function described in Sect. 2.4 is tractable and easy to compute, we use the sampled Bernoulli noise ∼ B( ; 1-ᾱt 2 •1) to reparameterize y t of Eq. ( 3) as y 0 ⊕ , where ⊕ denotes the logical operation of ""exclusive or (XOR)"". Additionally, let denote elementwise product, and Norm(•) denote normalizing the input data along the channel dimension and then returning the second channel. The concrete Bernoulli posterior can be represented as:where])."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.3,Diverse Reverse Process,"The diverse reverse process p θ (y 0:T ) can also be viewed as a Markov chain that starts from the Bernoulli noise y T ∼ B(y T ; 1 2 • 1) and progresses through intermediate latent variables constrained by the input medical image x to produce diverse segmentation masks, as shown in Fig. 1. The concrete diverse reverse process of our BerDiff can be represented as:Specifically, we utilize the estimated Bernoulli noise ˆ (y t , t, x) of y t to parameterize μ(y t , t, x) via a calibration function F C , as follows: (7) where | • | denotes the absolute value operation. The calibration function aims to calibrate the latent variable y t to a less noisy latent variable y t-1 in two steps: 1) estimating the segmentation mask y 0 by computing the absolute deviation between y t and the estimated noise ˆ ; and 2) estimating the distribution of y t-1 by calculating the Bernoulli posterior, p(y t-1 |y t , y 0 ), using Eq. (4)."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.4,Detailed Procedure,"Here, we provide an overview of the training and sampling procedure in Algorithms 1 and 2. During the training phase, given an image and mask data pair {x, y 0 }, we sample a random timestep t from a uniform distribution {1, . . . , T }, which is used to sample the Bernoulli noise .We then use to sample y t from q(y t | y 0 ), which allows us to obtain the Bernoulli posterior q(y t-1 | y t , y 0 ). We pass the estimated Bernoulli noise ˆ (y t , t, x) through the calibration function F C to parameterize p θ (y t-1 | y t , x). Based on the variational upper bound on the negative log-likelihood in previous diffusion models [3], we adopt Kullback-Leibler (KL) divergence and binary cross-entropy (BCE) loss to optimize our BerDiff as follows:Finally, the overall objective function is presented as:During the sampling phase, our BerDiff first samples the initial latent variable y T , followed by iterative calculation of the probability parameters of y t-1 for different t. In Algorithm 2, we present two different sampling strategies from DDPM and DDIM for the latent variable y t-1 . Finally, our BerDiff is capable of producing diverse segmentation masks. By taking the mean of these masks, we can further obtain a saliency segmentation mask to highlight salient ROI that can serve as a valuable reference for radiologists. Note that our BerDiff has a novel parameterization technique, i.e. calibration function, to estimate the Bernoulli noise of y t , which is different from previous works [3,11,24]."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,"Dataset and Preprocessing. The data used in this experiment are obtained from LIDC-IDRI [2,7] and BRATS 2021 [4] datasets. LIDC-IDRI contains 1,018 lung CT scans with plausible segmentation masks annotated by four radiologists. We adopt a standard preprocessing pipeline for lung CT scans and the trainvalidation-test partition as in previous work [5,15,23]. BRATS 2021 consists of four different sequence (T1, T2, FlAIR, T1CE) MRI images for each patient. All 3D scans are sliced into axial slices and discarded the bottom 80 and top 26 slices. Note that we treat the original four types of brain tumors as one type following previous work [25], converting the multi-target segmentation problem into binary. Our training set includes 55,174 2D images scanned from 1,126 patients, and the test set comprises 3,991 2D images scanned from 125 patients. Finally, the sizes of images from LIDC-IDRI and BRAST 2021 are resized to a resolution of 128 × 128 and 224 × 224, respectively. Implementation Details. We implement all the methods with the PyTorch library and train the models on NVIDIA V100 GPUs. All the networks are trained using the AdamW [19] optimizer with a mini-batch size of 32. The initial learning rate is set to 1 × 10 -4 for BRATS 2021 and 5 × 10 -5 for LIDC-IDRI. The Bernoulli noise estimation U-net network in Fig. 1 of our BerDiff is the same as previous diffusion-based models [20]. We employ a linear noise schedule for T = 1000 timesteps for all the diffusion models. And we use the sub-sequence sampling strategy of DDIM to accelerate the segmentation process. During minibatch training of LIDC-IDRI, our BerDiff learns diverse expertise by randomly sampling one from four annotated segmentation masks for each image. Four metrics are used for performance evaluation, including Generalized Energy Distance (GED), Hungarian-Matched Intersection over Union (HM-IoU), Soft-Dice and Dice coefficient. We compute GED using varying numbers of segmentation samples (1, 4, 8, and 16), HM-IoU and Soft-Dice using 16 samples."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.2,Ablation Study,"We start by conducting ablation experiments to demonstrate the effectiveness of different losses and estimation targets, as shown in Table 1. All experiments are trained for 21,000 training iterations on LIDC-IDRI. We first explore the selection of losses in the top three rows. We find that the combination of KL divergence and BCE loss can achieve the best performance. Then, we explore the selection of estimation targets in the bottom two rows. We observe that estimating Bernoulli noise, instead of directly estimating the ground-truth mask, is The U-net has the same architecture as the noise estimation network in our BerDiff and previous diffusion-based models. Fig. 2. Diverse segmentation masks and the corresponding saliency mask of two lung nodules randomly selected in LIDC-IDRI. y i 0 and y i gt refer to the i-th generated and ground-truth segmentation masks, respectively. Saliency Mask is the mean of diverse segmentation masks. more suitable for our binary segmentation task. All of these findings are consistent with previous works [3,10].Here, we conduct ablation experiments on our BerDiff with Gaussian or Bernoulli noise, and the results are shown in Table 2. For discrete segmentation tasks, we find that using Bernoulli noise can produce favorable results when training iterations are limited (e.g. 21,000 iterations) and even outperform using Gaussian noise when training iterations are sufficient (e.g. 86,500 iterations). We also provide a more detailed performance comparison between Bernoulliand Gaussian-based diffusion models over training iterations in Fig. S3.   3, and find that our BerDiff performs well for discrete segmentation tasks. Probabilistic U-net (Prob.U-net), Hierarchical Prob.U-net (Hprob.U-net), and Joint Prob.U-net (JPro.U-net) use conditional variational autoencoder (cVAE) to accomplish segmentation tasks. Calibrated Adversarial Refinement (CAR) employs generative adversarial networks (GAN) to refine segmentation. PixelSeg is based on autoregressive models, while SegDiff and MedSegDiff are diffusion-based models. There are also methods that attempt to model multi-annotators explicitly [13,18,27]. We have the following three observations: 1) diffusion-based methods demonstrate significant superiority over traditional approaches based on VAE, GAN, and autoregression models for discrete segmentation tasks; 2) our BerDiff outperforms other diffusion-based models that use Gaussian noise as the diffusion kernel; and 3) our BerDiff also outperforms the methods that explicitly model the annotator, striking a good balance between diversity and accuracy. At the same time, we present comparison segmentation results in Fig. 2. Compared to other models, our BerDiff can effectively learn diverse expertise, resulting in more diverse and accurate segmentation masks. Especially for small nodules that can create ambiguity, such as the lung nodule on the left, our BerDiff approach produces segmentation masks that are more in line with the ground-truth masks.Results on BRATS 2021. Here, we present the quantitative and qualitative results of BRATS 2021 in Table 4 and Fig. 3, respectively. We conducted a comparative analysis of our BerDiff with other models such as nnUnet, transformer-based models like TransU-net and Swin UNETR, as well as diffusion-based methods like SegDiff. First, we find that diffusion-based methods show superior performance compared to traditional U-net and transformer-based approaches. Besides, the high performance achieved by U-net, which shares the same architecture as our noise estimation network, highlights the effectiveness of the backbone design in diffusion-based models. Moreover, our proposed BerDiff surpasses other diffusion-based models that use Gaussian noise as the diffusion kernel. Finally, from Fig. 3, we find that our BerDiff segments more accurately on parts that are difficult to recognize by the human eye, such as the tumor in the 3rd row. At the same time, we can also generate diverse plausible segmentation masks to produce a saliency segmentation mask. We note that some of these masks may be false positives, as shown in the 1st row, but they can be filtered out due to low saliency. Please refer to Figs. S1 andS2 for more examples of diverse segmentation masks generated by our BerDiff."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,4.0,Conclusion,"In this paper, we proposed to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, achieving accurate and diverse medical image segmentation results. Our BerDiff only focuses on binary segmentation tasks and takes much time during the iterative sampling process as other diffusion-based models; e.g. our BerDiff takes 0.4 s to segment one medical image, which is ten times of traditional U-net. In the future, we will extend our BerDiff to the multi-target segmentation problem and implement additional strategies for speeding up the segmentation process."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_47.
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,1.0,Introduction,"Medical image segmentation is a challenging task that requires accurate delineation of structures and regions of interest in complex and noisy images. Multiple expert annotators are often employed to address this challenge, to provide binary segmentation annotations for the same image. However, due to differences in experience, expertise, and subjective judgments, annotations can vary significantly, leading to inter-and intra-observer variability. In addition, manual annotation is a time-consuming and costly process, which limits the scalability and applicability of segmentation methods.To overcome these limitations, automated methods for multi-annotator prediction have been proposed, which aim to fuse the annotations from multiple annotators and generate an accurate and consistent segmentation result. Existing approaches for multi-annotator prediction include majority voting [7], label fusion [3], and label sampling [12].In recent years, diffusion models have emerged as a promising approach for image segmentation, for example by using learned semantic features [2]. By modeling the diffusion of image intensity values over the iterations, diffusion models capture the underlying structure and texture of the images and can separate regions of interest from the background. Moreover, diffusion models can handle noise and image artifacts, and adapt to different image modalities.In this work, we propose a novel method for multi-annotator prediction, using diffusion models for medical segmentation. The goal is to fuse multiple annotations of the same image from different annotators and obtain a more accurate and reliable segmentation result. In practice, we leverage the diffusionbased approach to create one map for each level of consensus. To obtain the final prediction, we average the obtained maps and obtain one soft map.We evaluate the performance of the proposed method on a dataset of medical images annotated by multiple annotators. Our results demonstrate the effectiveness and robustness of the proposed method in handling inter-and intra-observer variability and achieving higher segmentation accuracy than the state-of-the-art methods. The proposed method could improve the efficiency and quality of medical image segmentation and facilitate the clinical decision-making process."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,2.0,Related Work,"Multi-annotator Strategies. Research attention has recently been directed towards the issues of multi-annotator labels [7,12]. During training, Jensen et al. [12] randomly sampled different labels per image. This method produced a more calibrated model. Guan et al. [7] predicted the gradings of each annotator individually and acquired the corresponding weights for the final prediction. Kohl et al. [15] used the same sampling strategy to train a probabilistic model, based on a U-Net combined with a conditional variational autoencoder. Another recent probabilistic approach [20] combines a diffusion model with KL divergence to capture the variability between the different annotators. In our work, we use consensus maps as the ground truth and compare them to other strategies.Diffusion Probabilistic Models (DPM). [23] are a class of generative models based on a Markov chain, which can transform a simple distribution (e.g. Gaussian) to data sampled from a complex distribution. Diffusion models are capable of generating high-quality images that can compete with and even outperform the latest GAN methods [5,9,19,23]. A variational framework for the likelihood estimation of diffusion models was introduced by Huang et al. [11]. Subsequently, Kingma et al. [14] proposed a Variational Diffusion Model that produces state-of-the-art results in likelihood estimation for image density.Conditional Diffusion Probabilistic Models. In our work, we use diffusion models to solve the image segmentation problem as conditional generation, given the image. Conditional generation with diffusion models includes methods for class-conditioned generation, which is obtained by adding a class embedding to the timestep embedding [19]. In [4], a method for guiding the generative process in DDPM is present. This method allows the generation of images based on a given reference image without any additional learning. In the domain of super-resolution, the lower-resolution image is upsampled and then concatenated, channelwise, to the generated image at each iteration [10,21]. A similar approach passes the low-resolution images through a convolutional block [16] prior to the concatenation.A previous study directly applied a diffusion model to generate a segmentation mask based on a conditioned input image [1]. Baranchuk et al. [2] extract features from a pretrained diffusion model for training a segmentation network, while our diffusion model generates the output mask. Compared to the diffusionbased image segmentation method of Wolleb et al. [26], our architecture differs in two main aspects: (i) the concatenation method of the condition signal, and (ii) an encoder that processes the conditioning signal. We also use a lower value of T, which reduces the running time."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,3.0,Method,"Our approach for binary segmentation with multi-annotators employs a diffusion model that is conditioned on the input image I ∈ R W ×H , the step estimation t, and the consensus index c. The diffusion model updates its current estimate x t iteratively, using the step estimation function θ . See Fig. 1 for an illustration.Given a set of C annotations {A i k } C i=1 associated with input sample I k , we define the ground truth consensus map at level c to beDuring training, our algorithm iteratively samples a random level of the consensus c ∼ U [1, 2, ..., C] and an input-output pair (I k , M c k ). The iteration number 1 ≤ t ≤ T is sampled from a uniform distribution and X T is sampled from a normal distribution.We then compute x t from X T , M c k and t according to:where ᾱ is a constant that defines the schedule of added noise. The current step index t, and the consensus index c are integers that are translated to z t ∈ R d and z c ∈ R d , respectively with a pair of lookup tables. The embeddings are passed to the different networks F , D and E.In the next step, our algorithm encodes the input signal x t with network F and encodes the condition image I k with network G. We compute the conditioned signal u t = F (x t , z c , z t ) + G(I k ), and apply it to the networks E and D, where the output is the estimation of x t-1 .The loss function being minimized is:The training procedure is depicted in Algorithm 1. The total number of diffusion steps T is set by the user, and C is the number of different annotators in the dataset. Our model is trained using binary consensus maps (M c k ) as the ground truth, where k is the sample id, and c is the consensus index.The inference process is described in Algorithm 2. We sample our model for each consensus index, and then calculate the mean of all results to obtain our target, which is a soft-label map representing the annotator agreement. Mathematically, if the consensus maps are perfect, this is equivalent to assigning each image location with the fraction of annotations that consider this location to be part of the mask (if c annotators mark a pixel, it would appear in levels 1..c). In Sect. 4, we compare our method with other variants and show that estimating the fraction map directly, using an identical diffusion model, is far inferior to estimating each consensus level separately and then averaging."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Employing Multiple Generations. Since calculating x t-1 during inference includes the addition of 1 [t>1],"β 1 is significant variability between different runs of the inference method on the same inputs, see Fig. 2(b).In order to exploit this phenomenon, we run the inference algorithm multiple times, then average the results. This way, we stabilize the results of segmentation and improve performance, as demonstrated in Fig. 2(c). We use twenty-five generated instances in all experiments. In the ablation study, we quantify the gain of this averaging procedure.Architecture. In this architecture, the U-Net's decoder D is conventional and its encoder is broken down into three networks: E, F , and G. The last encodes the input image, while F encodes the segmentation map of the current step x t . The two processed inputs have the same spatial dimensionality and number of channels. Based on the success of residual connections [8], we sum these signals F (x t , z t , z c ) + G(I). This sum then passes to the rest of the U-Net encoder E.The input image encoder G is built from Residual in Residual Dense Blocks [24] (RRDBs), which combine multi-level residual connections without batch normalization layers. G has an input 2D-convolutional layer, an RRDB with a residual connection around it, followed by another 2D-convolutional layer, leaky RELU activation and a final 2D-convolutional output layer. F is a 2Dconvolutional layer with a single-channel input and an output of L channels.The encoder-decoder part of θ , i.e., D and E, is based on U-Net, similarly to [19]. Each level is composed of residual blocks, and at resolution 16 × 16 and 8 × 8 each residual block is followed by an attention layer. The bottleneck contains two residual blocks with an attention layer in between. Each attention layer contains multiple attention heads.The residual block is composed of two convolutional blocks, where each convolutional block contains group-norm, SiLU activation, and a 2D-convolutional layer. The residual block receives the time embedding through a linear layer, SiLU activation, and another linear layer. The result is then added to the output of the first 2D-convolutional block. Additionally, the residual block has a residual connection that passes all its content.On the encoder side (network E), there is a downsample block after the residual blocks of the same depth, which is a 2D-convolutional layer with a stride of two. On the decoder side (network D), there is an upsample block after the residual blocks of the same depth, which is composed of the nearest interpolation that doubles the spatial size, followed by a 2D-convolutional layer. Each layer in the encoder has a skip connection to the decoder side."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,4.0,Experiments,"We conducted a series of experiments to evaluate the performance of our proposed method for multi-annotator prediction. Our experiments were carried out on datasets of the QUBIQ benchmark1 . We compared the performance of our proposed method with several state-of-the-art methods. Datasets. The Quantification of Uncertainties in Biomedical Image Quantification Challenge (QUBIQ), is a recently available challenge dataset specifically for the evaluation of inter-rater variability. QUBIQ comprises four different segmentation datasets with CT and MRI modalities, including brain growth (one task, MRI, seven raters, 34 cases for training and 5 cases for testing), brain tumor (one task, MRI, three raters, 28 cases for training and 4 cases for testing), prostate (two subtasks, MRI, six raters, 33 cases for training and 15 cases for testing), and kidney (one task, CT, three raters, 20 cases for training and 4 cases for testing). Following [13], the evaluation is performed using the soft Dice coefficient with five threshold levels, set as (0.1, 0.3, 0.5, 0.7, 0.9). Implementation Details. The number of diffusion steps in previous works was 1000 [9] and even 4000 [19]. The literature suggests that more is better [22]. In our experiments, we employ 100 diffusion steps, to reduce inference time.The AdamW [18] optimizer is used in all our experiments. Based on the intuition that the more RRDB blocks, the better the results, we used as many blocks as we could fit on the GPU without overly reducing batch size.Following [13], for all datasets of the QUBIQ benchmark the input image resolution, as well as the test image resolution, was 256 × 256. The experiments were performed with a batch size of four images and eight RRDB blocks. The network depth was seven, and the number of channels in each depth was [L, L, L, 2L, 2L, 4L, 4L], with L = 128. The augmentations used were: random  scaling by a factor sampled uniformly in the range [0.9, 1.1], a rotation between 0 and 15 • , translation between [0, 0.1] in both axes, and horizontal and vertical flips, each applied with a probability of 0.5.Results. We compare our method with FCN [17], MCD [6], FPM [27], DAF [25], MV-UNet [13], LS-UNet [12], MH-UNet [7], and MRNet [13].We also compare with models that we train ourselves, using public code AMIS [20], and DMISE [26]. The first is trained in a scenario where each annotator is a different sample (""No annotator"" variant of our ablation results below), and the second is trained on the consensus setting, similar to our method. As can be seen in Table 1, our method outperforms all other methods across all datasets of QUBIQ benchmark.Ablation Study. We evaluate alternative training variants as an ablation study in Table 2. The ""Annotator"" variant, in which our model learns to produce each annotator binary segmentation map and then averages all the results to obtain the required soft-label map, achieves lower scores compared to the ""Consensus"" variant, which is our full method. The ""No annotator"" variant, where images were paired with random annotators without utilizing the annotator IDs, achieves a slightly lower average score compared to the ""Annotator"" variant. We also note that our ""No annotator"" variant outperforms the analog AMIS model in four out of five datasets, indicating that our architecture is somewhat preferable. In a third variant, our model learns to predict the soft-label map that denotes the fraction of annotators that mark each image location directly. Since this results in fewer generated images, we generate C times as many images per test sample. The score of this variant is also much lower than that of our method.Next, we study the effect of the number of generated images on performance. The results can be seen in Fig. 3. In general, increasing the number of generated instances tends to improve performance. However, the number of runs required to reach optimal performance varies between classes. For example, for the Brain and the Prostate 1 datasets, optimal performance is achieved using 5 generated images, while on Prostate 2 the optimal performance is achieved using 25 gen-  erated images. Figure 4 depicts samples from multiple datasets and presents the progression as the number of generated images increases. As can be seen, as the number of generated images increases, the outcome becomes more and more similar to the target segmentation."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,5.0,Discussion,"In order to investigate the relationship between the annotator agreement and the performance of our model, we conducted an analysis by calculating the average Dice score between each pair of annotators across the entire dataset. The results of this pairwise Dice analysis can be found in Table 3, where higher mean-scores indicate a greater consensus among the annotators. We observed that our proposed method demonstrated improved performance on datasets with higher agreement among annotators, specifically the kidney and prostate 1 datasets. Conversely, the performance of the other methods significantly deteriorated on the kidney dataset, leading to a lower correlation between the Dice score and the overall performance. Additionally, we examined the relationship between the number of annotators and the performance of our model. Surprisingly, we found no significant correlation between the number of annotators and the performance of our model."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,6.0,Conclusions,"Shifting the level of consensus required to mark a region from very high to as low as one annotator, can be seen as creating a dynamic shift from a very conservative segmentation mask to a very liberal one. As it turns out, this dynamic is wellcaptured by diffusion models, which can be readily conditioned on the level of consensus. Another interesting observation that we make is that the mean (over the consensus level) of the obtained consensus masks is an effective soft mask. Applying these two elements together, we obtain state-of-the-art results on multiple binary segmentation tasks."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,1.0,Introduction,"Semantic segmentation aims to segment objects in an image by classifying each pixel into an object class. Training a deep neural network (DNN) for such a task is known to be data-hungry, as labeling dense pixel-level annotations requires laborious and expensive human efforts in practice [23,32]. Furthermore, semantic segmentation in medical imaging suffers from privacy and data sharing issues [13,35] and a lack of experts to secure accurate and clinically meaningful regions of interest (ROIs). This data shortage problem causes overfitting for training DNNs, resulting in the networks being biased by outliers and ignorant of unseen data.To alleviate the sample size and overfitting issues, diverse data augmentations have been recently developed. For example, CutMix [31] and CutOut [4] augment images by dropping random-sized image patches or replacing the removed regions with a patch from another image. Random Erase [33] extracts noise from a uniform distribution and injects it into patches. Geometric transformations such as Elastic Transformation [26] warp images and deform the original shape of objects. Alternatively, feature perturbation methods augment data by perturbing data in feature space [7,22] and logit space [9].Although these augmentation approaches have been successful for natural images, their usage for medical image semantic segmentation is quite restricted as objects in medical images contain non-rigid morphological characteristics that should be sensitively preserved. For example, basalioma (e.g., pigmented basal cell carcinoma) may look similar to malignant melanoma or mole in terms of color and texture [6,20], and early-stage colon polyps are mostly small and indistinguishable from background entrail surfaces [14]. In these cases, the underlying clinical features of target ROIs (e.g., polyp, tumor and cancer) can be distorted if regional colors and textures are modified with blur-based augmentations or geometric transformations. Also, cut-and-paste and crop-based methods carry risks of dropping or distorting key objects such that expensive pixel-level annotations could not be properly used. Considering the ROIs are usually small and underrepresented compared to the backgrounds, the loss of information may cause a fatal class imbalance problem in semantic segmentation tasks.In these regards, we tackle these issues with a novel augmentation method without distorting the semantics of objects in image space. This can be achieved by slightly but effectively perturbing target objects with adversarial noises at the object level. We first augment hard samples with adversarial attacks [18] that deceive a network and defend against such attacks with anti-adversaries. Specifically, multi-step adversarial noises are injected into ROIs to maximize loss and induce false predictions. Conversely, anti-adversaries are obtained with antiadversarial perturbations that minimize a loss which eventually become easier samples to predict. We impose consistency regularization between these contrasting samples by evaluating their prediction ambiguities via supervised losses with true labels. With this regularization, the easier samples provide adaptive guidance to the misclassified data such that the difficult (but object-relevant) pixels can be gradually integrated into the correct prediction. From active learning perspective [12,19], as vague samples near the decision boundary are augmented and trained, improvement on a downstream prediction task is highly expected.We summarize our main contributions as follows: 1) We propose a novel online data augmentation method for semantic segmentation by imposing objectspecific consistency regularization between anti-adversarial and adversarial data."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,2),"Our method provides a flexible regularization between differently perturbed data such that a vulnerable network is effectively trained on challenging samples considering their ambiguities. 3) Our method preserves underlying morphological characteristics of medical images by augmenting data with quasiimperceptible perturbation. As a result, our method significantly improves sensitivity and Dice scores over existing augmentation methods on Kvasir-Seg [11] and ETIS-Larib Polyp DB [25] benchmarks for medical image segmentation. "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,2.0,Preliminary: Adversarial Attack and Anti-adversary,"Adversarial attack is an input perturbation method that adds quasiimperceptible noises into images to deceive a DNN. Given an image x, let μ be a noise bounded by l ∞ -norm. While the difference between x and the perturbed sample x = x + μ is hardly noticeable to human perception, a network f θ (•) can be easily fooled (i.e., f θ (x) = f θ (x + μ)) as the μ pushes x across the decision boundary.To fool a DNN, Fast Gradient Sign Method (FGSM) [8] perturbs x toward maximizing a loss function L by defining a noise μ as the sign of loss derivative with respect to x as follows: x = x + sign(∇ x L), where controls the magnitude of perturbation. The authors in [18] proposed an extension of FGSM, i.e., Projected Gradient Descent (PGD), which is an iterative adversarial attack that also finds x with a higher loss. Given an iteratively perturbed sample x t at t-th perturbation where x 0 = x, the x t of PGD is defined asRecently, anti-adversarial methods were proposed for the benign purpose to defend against such attacks. The work in [15] used an anti-adversarial class activation map to identify objects and the authors in [1] proposed an anti-adversary layer to handle adversaries. In contrast to adversarial attacks, these works find μ that minimizes a loss to make easier samples to predict. Figure 1a shows multistep adversarial and anti-adversarial perturbations in the latent space. To increase a classification score, the anti-adversarial noises move data away from the decision boundary, which is the opposite direction of the adversarial perturbations."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,3.0,Method,"Let {X i } N i=1 be an image set with N samples each paired with corresponding ground truth pixel-level annotations Y i . Our proposed method aims to 1) generate realistic images with adversarial attacks and 2) train a segmentation model f θ (X i ) = Y i for robust semantic segmentation with anti-adversarial consistency regularization (AAC). Figure 2 shows the overall training scheme with three phases: 1) online data augmentation, 2) computing adaptive AAC between differently perturbed samples, and 3) updating the segmentation model using the loss from the augmented and original data. First, we generate plausible images with iterative adversarial and anti-adversarial perturbations. We separate the roles of perturbed data: adversaries are used as training samples and anti-adversaries are used to provide guidance (i.e., pseudo-labels) to learn the adversaries. Specifically, consistency regularization is imposed between these contrasting data by adaptively controlling the regularization magnitude in the next phase. Lastly, considering each sample's ambiguity, the network parameters θ are updated for learning the adversaries along with the given data so that discriminative regions are robustly expanded for challenging samples.Data Augmentation with Object-Targeted Adversarial Attack. In many medical applications, false negatives (i.e., failing to diagnose a critical disease) are much more fatal than false positives. To deal with these false negatives, we mainly focus on training a network to learn diverse features at target ROIs (e.g., polyps) where disease-specific variations exist. To do so, we first exclude the background and perturb only the objects in the given image. Given o as the target object class and (p, q) as a pixel coordinate, a masked object is defined as Xi = {(p, q)|X i (p,q) = o}. As in PGD [18], we perform iterative perturbations on the Xi for K steps. Given Xi,k as a perturbed sample at k-th step (k = 1, ..., K), the adversarial and anti-adversarial perturbations use the same initial image as X - i,0 = Xi and X + i,0 = Xi , respectively. With this input, the iterative adversarial attack is defined aswhere), Y i ) is a quasi-imperceptible adversarial noise that fools f θ (•) and is a perturbation magnitude that limits the noise (i.e., |μ (p,q) | ≤ , s.t. (p, q) ∈ Xi ). Similarly, iterative anti-adversarial perturbation is defined asIn contrast to the adversarial attack in Eq. 1, the anti-adversarial noise), Y i ) manipulates samples to increase the classification score.Note that, generating noises and images are online and training-free as the loss derivatives are calculated with freezed network parameters. The adversaries X - i,1 , ..., X - i,K are used as additional training samples so that the network includes the non-discriminative yet object-relevant features for the prediction. On the other hand, as the anti-adversaries are sufficiently discriminative, we do not use them as training samples. Instead, only the K-th anti-adversary X + i,K (i.e., the most perturbed sample with the lowest loss) is used for downstream consistency regularization to provide informative guidance to the adversaries.Computing Adaptive Consistency Toward Anti-adversary. Let X i be either X i or X - i,k . As shown in Fig. 1b, consistency regularization is imposed between the anti-adversary X + i,K and X i to reduce the gap between samples with different prediction uncertainties. The weight of regularization between X i and X + i,K is automatically determined by evaluating the gap in their prediction quality via supervised losses with ground truth Y i as w(X i , Xwhere l(•) is Dice loss [28] and P i is the output of f θ (•) for X i . Specifically, if X i is a harder sample to predict than X + i,K , i.e., l(P i , Y i )>l(P + i,K , Y i ), the weight gets larger, and thus consistency regularization is intensified between the images.Training a Segmentation Network. Let Ŷ + i,K be a segmentation outcome, i.e., one-hot encoded pseudo-label from the network output P + i,K of anti-adversary X + i,K . Given X i and {X - i,k } K k=1 as training data, the supervised segmentation loss L sup and the consistency regularization R con are defined asUsing the pseudo-label from anti-adversary as a perturbation of the ground truth, the network is supervised by diverse and realistic labels that contain auxiliary information that the originally given labels do not provide. With a hyperparameter α, the whole training loss L = L sup +αR con is minimized via backpropagation to optimize the network parameters for semantic segmentation."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.1,Experimental Setup,"Dataset. We conducted experiments on two representative public polyp segmentation datasets: Kvasir-SEG [11] and ETIS-Larib Polyp DB [25] (ETIS). Both are comprised of two classes: polyp and background. They provide 1000/196 (Kvasir-SEG/ETIS) input-label pairs in total and we split train/validation/test sets into 80%/10%/10% as in [5,10,24,27,29]. The images of Kvasir-SEG were resized to 512 × 608 (H × W ) and that of ETIS was set with 966 × 1255 resolution.Implementation. We implemented our method on Pytorch framework with 4 NVIDIA RTX A6000 GPUs. Adam optimizer with learning rates of 4e-3/1e-4 (Kavsir-SEG/ETIS) were used for 200 epochs with a batch size of 16. We set the number of perturbation steps K as 10 and the magnitude of perturbation as 0.001. The weight α for R con was set to 0.01."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Baselines.,"Along with conventional augmentation methods (i.e., random horizontal and vertical flipping denoted as 'Basic' in Table 1), recent methods such as CutMix [31], CutOut [4], Elastic Transform [26], Random Erase [33], Drop-Block [7], Gaussian Noise Training (GNT) [22], Logit Uncertainty (LU) [9] and Tumor Copy-Paste (TumorCP) [30] were used as baselines. Their hyperparameters were adopted from the original papers. The Basic augmentation was used in all methods including ours by default. For the training, we used K augmented images with the given images for all baselines as in ours for a fair comparison."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Evaluation.,"To verify the effectiveness of our method, evaluations are conducted using various popular backbone architectures such as U-Net [21], U-Net++ [34], LinkNet [2], and DeepLabv3+ [3]. As the evaluation metric, mean Intersection over Union (mIoU) and mean Dice coefficient (mDice) are used for all experiments on test sets. Additionally, we provide recall and precision scores to offer a detailed analysis of class-specific misclassification performance."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.2,Comparison with Existing Methods,"As shown in Table 1, our method outperforms all baselines for all settings by at most 10.06%p and 5.98%p mIoU margin on Kvasir-SEG and ETIS, respectively. Moreover, in Fig. 3, our method with U-Net on Kvasir-SEG surpasses the baselines by ∼8.2%p and ∼7.2%p in precision and recall, respectively. Note that, all baselines showed improvements in most cases. However, our method  performed better even compared with the TumorCP which uses seven different augmentations methods together for tumor segmentation. This is because our method preserves the semantics of the key ROIs with small but effective noises unlike geometric transformations [26,30], drop and cut-and-paste-based methods [4,7,30,31,33]. Also, as we augment uncertain samples that deliberately deceive a network as in Active Learning [12,16], our method is able to sensitively include the challenging (but ROI-relevant) features into prediction, unlike existing noise-based methods that extract noises from known distributions [9,22,30].  "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.3,Analysis on Anti-adversaries and Adversaries,"In Fig. 4, we visualize data augmentation results with (anti-) adversarial perturbations on Kvasir-SEG dataset. The perturbed data (c and e) are the addition of noise (d and f) to the given data (a), respectively. Interestingly, while adversaries (c) and anti-adversaries (e) are visually indistinguishable, they induce totally opposite model decisions towards different classes. In Fig. 5, we qualitatively and quantitatively compared their effects via visualizing perturbation trajectories in the feature space projected with t-SNE [17] and comparing their supervision losses. In Fig. 5a, the adversarial attacks send a pixel embedding of a polyp class to the background class, anti-adversarial perturbations push it towards the true class with a higher classification score. Also, loss comparisons in Fig. 5b and 5c demonstrate that the anti-adversaries (blue) are consistently easier to predict than the given data (grey) and adversaries (red) during the training and their differences get larger as the perturbations are iterated. These results confirm that the anti-adversaries send their pseudo label Ŷ + K closer to the ground truth with a slight change. Therefore, they can be regarded as a perturbation of the ground truth that contain a potential to provide additional information to train a network on the adversaries. We empirically show that Ŷ + K is able to provide such auxiliary information that the true labels do not provide, as our method performs better with R con (i.e., L = L sup +αR con , 92.43% mIoU) than the case without R con (i.e., L = L sup , 92.15% mIoU) using U-Net on Kvasir-SEG. Training samples in Fig. 6 show that the pseudo-labels Ŷ + K can capture detailed abnormalities (marked in red circles) which are not included in the ground truths. Moreover, as the AAC considers sample-level ambiguity, the effect from Ŷ + K is sensitively controlled and a network can selectively learn the under-trained yet object-relevant features from adversarial samples."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,5.0,Conclusion,"We present a novel data augmentation method for semantic segmentation using a flexible anti-adversarial consistency regularization. In particular, our method is tailored for medical images that contain small and underrepresented key objects such as a polyp and tumor. With object-level perturbations, our method effectively expands discriminative regions on challenging samples while preserving the morphological characteristics of key objects. Extensive experiments with various backbones and datasets confirm the effectiveness of our method."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,1.0,Introduction,"The successes of deep convolutional neural networks (CNNs) [9] in image tasks [12,[20][21][22] rely heavily on a large number of intensively annotated training samples, and obtaining these annotated images is time-consuming, laborious and costly. In addition to the extreme lack of training data, these segmentation models also have low generalization ability due to the specificity of training classes. This limitation is more significant in medical images. Therefore, few-shot learning (FSL) [11,14,17] emerged to deal with these challenges. By learning the generalized representation of a certain semantic class from a few labeled samples (i.e., support images) to guide the segmentation of that class in a large number of unlabeled images (i.e., query images).The development of FSL has derived many effective medical image segmentation methods, including pure CNNs [2,10] and CNNs with a nonparametric learning module embedded [1], among which the prototype network is the most widely used [8,15,23]. For example, Yu et al. [23] proposed a location-sensitive local prototype network. Tang et al. [15] proposed a recurrent mask refinement algorithm based on a prototype network. In addition, Ouyang et al. [8] introduced a self-supervised super-pixel few-shot segmentation (FSS) method to get rid of the demand for labeled data in the training stage. Although FSS methods based on prototype networks have the ability to achieve image segmentation, when the prototype learned from the support image is used to judge the pixel category of the query image, it still cannot avoid the problem of insufficient prototype discriminability caused by the differences between the images as well as the complexity of the target. Furthermore, it is easy to lose important information when the target features are taken as a prototype by GAP (global average pooling). In this regard, Wang et al. [18] introduced a prototype alignment regularization between the support image and the query image to better utilize the information of the support set. Meanwhile, compared with one-shot mode, the multi-shot prototype strategy is widely used to improve segmentation performance [6,8,16]. However, the existing multi-shot strategies usually take the average of multiple support prototypes as the final prototype, which not only weakens the independent contribution of multiple support images but also destroys the semantic information of support prototypes. Especially for medical images with lower image intensity and contrast, prototype alignment regularization and the mean prototype strategy do not significantly increase the discriminant ability and prediction accuracy of prototypes. Especially for medical images with lower image intensity and contrast, prototype alignment regularization and mean prototype strategy may aggravate the confusion of target pixel categories. For pure prototype technology, Zhou et al. [25] explored a better non-parametric segmentation model based on non-learnable prototypes in their latest work and successfully applied it to the semantic segmentation of natural images, but this method has not been verified in medical images.In this paper, we propose a prototype contrastive learning and semantic reasoning network based on multi-shot strategy (MPSNet). To our knowledge, this is a great improvement in medical image analysis. First, we design a multi-shot learning network (MLN) within the support set to generate prior semantic features and a prior segmentation model for the query image. Cross-validation mode is used to construct the support-query image pairs within the support set, and a complete FSS model is formed by prototype contrastive learning and supervised training. Second, we propose a prototype contrastive learning module (PCLM) to ascertain the positive contribution of multiple support images to the query image in segmentation guidance, leading to better segmentation performance of the query image. Third, we design a semantic reasoning network (SRN) that is convenient to directly transfer the prior semantic features and prior segmentation model to the query image to deduce its segmentation mask quickly. Our contributions can be summarized as follows:1) A novel FSS method based on a multi-shot prototype strategy is proposed for the first time to replace the commonly used mean prototype method to improve the guidance ability of the support images to the query images in segmentation. 2) A multi-shot prototype contrastive learning network within the support set is constructed with supervised training to generate prior semantic features and a prior segmentation model, and transfer them to the query image to deduce its segmentation mask.3) The proposed method achieves the latest performance on three public datasets that is superior to the state-of-the-art (SOTA) methods.  n=1 . The support images, masks and the query image constitute the model input Input model = {I s , M s , I q }, query mask M q as the supervisory information. At testing stage, we set episodes with the same mode, that is, we use the trained FSS model to segment the query set under the function of the support set. Since our research is carried out based on multi-shot strategy, we mainly take 5-shot as an example to verify the effectiveness of the proposed method."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.2,Multi-shot Learning Network,"The framework of MLN is shown in Fig. 1(a), it is a supervised learning network that exists independently within the support set, mainly including 5 same structures but different input and output of 4-shot learning branch {P ath l } 5 l=1 . When one support image is used as the fake query image, the remaining 4 support images are used as the fake support images. Therefore, in the support set, each support image can be used as a fake query image to form a 4-shot learning network. The specific implementation process can be expressed as follows:msi / mfq = P CLM(GAP (RN (where RN (•) refers to the shared feature extractor ResNet101, GAP (•) refers to global average pooling, P CLM (•) refers to the proposed prototype contrastive learning module. x fsi , m fsi , x fq represent the fake support image, mask and query image. We use cross-entropy (CE) loss to supervise the training process:As shown in the lower part of Fig. 1, we take P ath 1 branch as an example in detail the specific structures. Fake support images x fs i 4 i=1 ∈ R 3×H×W and fake query image x fq ∈ R 3×H×W are fed into ResNet101 for support features F ea fs i 4 i=1 ∈ R C×h×w and query features F ea fq ∈ R C×h×w . Under the function of support masks m fs i 4 i=1 ∈ R 1×H×W , the support prototypes P = {P i } 4 i=1 have been extracted from the support features, denoted as P i ∈ R C×1×1 , this process is implemented by GAP:where support features F ea fs i are resized to the mask size (H, W ) , (h, w) denotes the shape of the feature maps, C denotes the channel number of the feature maps, denotes the Hadamard product. The obtained multi-shot prototypes P and query features F ea fq are sent into PCLM to get the final segmentation of the query image."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.3,Prototype Contrastive Learning Module,"As shown in Fig. 1(c), PCLM is embedded in MLN and SRN as a core segmentation module, assumes the task of predicting the pixel-level categories of the query image by taking the multi-shot prototypes as evaluation criteria. Following Hansen et al. [3], we consider using a single foreground prototype as the feature compression of the target region, and adopt the anomaly score thresholding method to segment the query image. This method not only avoids the construction of the prototype for the background with more impurities, but also saves the calculation cost compared with the decoder. First of all, the similarity maps S i (x, y) 4 i=1 are obtained by doing cosine similarity between the multishot prototypes P and the query features F ea fq :The foreground prediction mask is then obtained:where (x, y) refers to the spatial location of the query image, and the prototype generates a similarity coefficient for each pixel. δ (•) denotes the Sigmoid function with a steepness parameter k = 0.5. T fq is the correlation threshold obtained by the query features F ea fq through the full connection layers g θ (•). Then, we assume that the prediction mask is no different from the real mask, and GAP it with F ea fq to obtain the hypothetical prototypes P h = P h i 4 i=1 :Theoretically, under the assumption that there is no pixel intensity and contrast difference between the query image and the support image, the hypothetical prototypes P h should be similar to the multi-shot prototypes P . But in reality, it is difficult to make the query image and the support images have a consistent feature hierarchy. That is to say, the prior knowledge provided by multiple support images for the query image is not equally important. We need to assign a weight factor w i to each support image to represent its positive contribution to query image segmentation. To this end, we determine the weight of each foreground prediction mask by comparing the hypothetical prototypes P h with real support prototypes P :At this time, the foreground and background segmentation mask can be modified as:we take m fq as the output of PCLM that is also the final segmentation mask."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.4,Semantic Reasoning Network,"In our 5-shot segmentation task, the query image needs to be accurately segmented under the guidance of the given 5 support images. Such an FSS task can be realized through the migration of the prior semantic features and prior segmentation model from MLN. Specifically, as shown in Fig. 1(b), we directly prototype the support features obtained from MLN with the corresponding support masks. It is worth noting that the support images in each supervised training path are gathered together to form the support feature-mask cluster to obtain the prototype cluster. The purpose of this is to make the FSS network designed for query images conform to the laws of prior semantic features and the prior segmentation model. At the same time, prior prototypes are obtained by internal averaging of the prototype cluster. On this basis, the implementation of the same PCLM module as MLN can output the segmentation mask mq of the query image, and use the query mask m q to conduct supervised training:L q seg = CE( mq , m q ), ( 12)so far, the overall loss of the proposed segmentation network MPSNet is L seg ."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.1,Experimental Setup,"Datasets: We evaluate the proposed MPSNet on three public datasets: the Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS) dataset [4] (Abd-MRI) with 20 3D T2-SPIR MRI scan images, the MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge [5] (Abd-CT) with 30 3D CT scan images and the MICCAI 2019 Multi-sequence Cardiac MRI Segmentation Challenge (bSSFP fold) [26] (Cardiac-MRI) with 35 3D cardiac MRI scans. Our task is to segment the liver, right kidney (RK), left kidney (LK), and spleen from Abd-MRI and Abd-CT, and the left ventricle (LV), myocardium (Myo), and right ventricle (RV) from Cardiac MRI. In the preprocessing of these datasets, we follow Ouyang et al.'s scheme [8] to make a fair comparison with the SOTA methods. Similarly, we also use the super-pixels technology to generate the pseudo-labels for the preprocessed datasets.Implementation Details: We use 5-fold cross validation to conduct training and testing. We chose the same allocation strategy for the support slices and the query slices as Roy et al. [10], and we randomly selected 1000 support-query image pairs to form the training set. All codes are based on PyTorch. We use the stochastic gradient descent (SGD) optimizer with a batch size of 1 and set the initial learning rate to 0.001, momentum to 0.9 and weight decay to 0.0005. The total number of iterations of the training is 30,000, and it takes an average of 9h to complete end-to-end training on an Nvidia RTX 3090 graphics card. To facilitate direct comparison with the SOTA methods, we follow the practice of most papers, using the pretrained ResNet101 framework as the feature encoder and the mean Sørensen-Dice coefficient (DSC) as the evaluation metric."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.2,Superior Performance over the SOTA Methods,"We compare the quantitative results of our method with the SOTA methods, including SE-Net [10], RP-Net [15], GCN-DE [13], ADNet [3], ALPNet [8] and RSCNet [19] are FSS methods specially proposed for medical images that have been verified on the datasets used by us. The remaining methods are classical FSS methods that are used in natural image. In particular, PANet [18] contains a prototype alignment strategy that is also applicable to medical images. As shown in Table 1, our method has obtained the best mean DSC of 82.01%, 74.72% and 77.90% for Abd-MRI, Abd-CT and Cardiac MRI respectively. Compared with the second-best method ALPNet [8], the mean DSC achieved by our method increased by 3.17%, 1.37% and 1%."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.3,Ablation Studies,"The specific implementation scheme of the ablation studies can be divided into three aspects: 1) MLN is removed from MPSNet to prove that the prior semantic   prototype network is reverted, that is, MLN and PCLM are removed, and the multi-shot support prototypes are directly averaged for similarity measurement with the query features (PNet). Note that SRN cannot be implemented when MLN and PCLM are absent.Table 2 shows the quantitative results of the ablation studies. In the absence of MLN, the average DSC is 8.11%, 8.67% and 9.78% lower than MPSNet. In the absence of PCLM, the average DSC is 1.26%, 1.26% and 2.66% lower than MPSNet. More significantly, our method has improved average DSC by 6.28%, 5.48% and 11.15% over PNet due to the inclusion of MLN and PCLM. Ablation studies fully demonstrate the effectiveness of the proposed method and the necessity of the internal parts. As shown in Fig. 2, one subject is randomly selected for each dataset to present the visualization results. In Fig. 3, we plot the training loss of the support images and the query image. It's obvious that the query loss decreases faster and becomes more stable than the support loss, indicating that our proposed MLN has a positive effect. "
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a novel FSS method MPSNet for medical image segmentation to replace the approach of averaging multiple support prototypes in the existing FSS methods. This is the first time that a multi-shot learning pattern is built within the support set and applied to the query image to significantly improve segmentation performance. Compared with the SOTA methods, our method is evaluated on three public datasets and achieves improvements in DSC of 3.17%, 1.37% and 1%, respectively. Relevant ablation studies also demonstrate the necessity and validity of our method."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,MPSNet 81.88 87.26 82.10 76.80 82.01 81.13 69.56 70.26 77.96 74.72 87.98 65.08 80.63 77.90,w/o MLN 79.18 78.88 70.62 66.90 73.90 82.25 57.21 58.48 66.28 66.05 84.04 51.83 68.47 68.12 w/o PCLM 80.52 86.51 81.26 74.72 80.75 80.89 69.29 68.29 75.37 73.46 86.01 62.51 77.19 75.24 PNet 81.13 78.63 74.35 68.81 75.73 82.85 62.48 64.20 67.43 69.24 83.42 50.08 66.76 66.75
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,1.0,Introduction,"The precise processing and analysis of brain MR images are critical in advancing neuroscience research. As a widely used animal model with high systematic similarity to humans in various aspects, macaques play an indispensable role in understanding brain mechanisms in development, aging and evolution, exploring the pathogenesis of neurological diseases, and validating the effectiveness of clinical techniques and drugs [1]. In both humans and macaques, early brain development is a complex, dynamic, and regionally heterogeneous process that plays a crucial role in shaping later brain structural, functional, and cognitive outcomes [19]. While the human neuroimaging research is advancing rapidly, the macaque neuroimaging research lags behind, partly due to challenges in data acquisition, lack of tailored processing tools, and the inapplicability of human neuroimaging analysis tools to macaques [2]. Therefore, the macaque neuroimaging research is a rapidly evolving field that demands specialized tools and expertise. Of particular importance is brain tissue segmentation, a crucial prerequisite for quantitative volumetric analysis and surface-based studies, which aim to partition the brain into distinct regions such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) [3]. As illustrated in Fig. 1, the anatomical structures, age-related contrasts, and size of macaque brains undergo significant changes during early development [18], posing challenges for accurate tissue segmentation. Especially during the early postnatal months, the WM and GM show extremely low contrast, making their boundaries difficult to detect. The development of automatic and robust tissue segmentation algorithms for macaque brains during early developing stages is thus of great importance.Deep learning has emerged as a powerful tool in medical image processing and analysis, with Convolutional Neural Networks (CNNs) showing remarkable performance in various applications, e.g., image segmentation [4], cross-modality generation [5,6] and multi-task learning [7,9] with state-of-the-art results. However, there is currently a lack of dedicated deep learning-based tools for brain tissue segmentation of macaque MRI data during early development, which can be attributed to several reasons. First, collecting macaque brain MRI data is challenging, and there is a dearth of publicly available large-scale labeled datasets for deep learning-based tissue segmentation. Second, brain tissue segmentation in early postnatal stages usually requires multi-modality images to provide more useful information than a single modality [8]. However, macaque datasets that are publicly available often only contain 3D T1-weighted (T1w) images and lack corresponding tissue labels and 3D T2-weighted (T2w) information. For example, the UW-Madison Rhesus MRI dataset [24] includes 592 macaque samples with T1w images only. To compensate for the missing modality information, generative adversarial networks (GAN) [13] have been applied to impute the missing modality in neuroimages, due to their potential for image-to-image translation [14][15][16]. Among them, pix2pix algorithm first proposes a solution for style transfer from image to image, which can be applied in the modality translation of medical images. Recently PTNet3D transforms the CNN-based frame into a Transformer-based frame, which reduces the computation amount required when processing high-resolution images. In these solutions, modality generation and downstream tasks are treated as two cascading independent tasks, as shown in Fig. 2(a). However, it is difficult to determine whether the generated missing modality has a natural positive effect on the downstream tasks [11]. In fact, both modality generation and tissue segmentation tasks require the feature extraction and learning of brain anatomical structures to achieve voxel-wise translation, showing a high task similarity and correlation. Essentially, the tissue map can guide the missing modality generation with anatomical information, while the generated missing modality can also provide useful complementary information for refining tissue segmentation [12]. To effectively capture and utilize the features and task correlation between two tasks, it is desirable to integrate tissue segmentation and modality generation into a unified framework. This can be achieved by imputing the missing modality image in a tissue-oriented manner, as illustrated in Fig. 2(b).In this study, we present a 3D Collaborative Segmentation-Generation Framework (CSGF) for early-developing macaque MR images. The CSGF is designed in the form of multi-task collaboration and feature sharing, which enables it to complete the missing modality generation and tissue segmentation simultaneously. As depicted in Fig. 3(a), the CSGF comprises two modules: a modality generation module (MGM) and a tissue segmentation module (TSM), which are trained collaboratively through two forward information flows. Specifically, MGM and TSM will be linked by transferring generated missing-modality and cross-module feature sharing (CFS), ensuring that the MGM is trained under the supervision of the TSM, thereby imposing a constraint on anatomical feature consistency and providing multi-modality information.The proposed CSGF offers several advantages over existing methods: (1) The collaborative learning of both MGM and TSM enables the missing modality to be imputed in a tissue-oriented manner, and hence the generated neuroimages are more consistent with real neuroimages from an anatomical point of view; (2) The CFS mechanism between the two modules provides more prosperous feature guidance for the TSM, thus also imposing constraints on the anatomical feature consistency of the MGM encoder; (3) The CSGF achieves improvements in both missing modality generation and brain tissue segmentation, especially for infant macaques with exceptionally low contrast between different tissues."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.1,Dataset,"The experimental dataset was from the public UNC-Wisconsin Neurodevelopment Rhesus Database [10], acquired by a GE MR750 3.0T MRI scanner and covered by animal research protocols approved by relevant Institutional Animal Care and Use committees. A total of 155 developing macaque structural MRI samples between 0 and 36 months of age were used for experiments. Each sample contains T1w image and T2w image with following parameters: T1w, matrix = 256 × 256, resolution = 0.5469 × 0.5469 × 0.8 mm 3 ; T2w, matrix = 256 × 256, resolution= 0.6016×0.6016×0.6 mm 3 . For image preprocessing, FMRIB's Linear Image Registration Tool (FLIRT) in FSL (version 5.0) [20] was used to rigidly align each T2w image to its corresponding T1w image, followed by resampling all images to 0.5469 mm mm isotropic resolution. Brain skulls were removed using the method in [2] and intensity inhomogeneity correction was performed using N4 bias correction [25]. The initial tissue segmentation of each scan was obtained by LINKS [17], which was then manually corrected significantly by experienced experts using ITK-SNAP software [21]."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.2,Problem Formulation,"We construct this Collaborative Segmentation-Generation Framework (CSGF) based on the primary modality T1w (denoted as T 1 i ) and the auxiliary modality T2w (denoted as T 2 i ) data. Let {T 1 i , T 2 i , M i } i=1∼N be the dataset consisting of N scans, where T 1 i and T 2 i represent the T1w and T2w of i th scan, respectively; M i represents the tissue label of i th scan. This framework can be formulated aswhere Mi is the estimated tissue label of the i th scan. However, in practical application, it is often the case that some s only contain the T1w modality. Therefore we construct a modality generation module (MGM) by a mapping function G to generate the missing T2w modality aswhere T2 i is the generated T2w based on T1w modality for the i th scan. Then, the tissue segmentation module (TSM) constructs a mapping function S as"
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.3,Framework Overview and Collaborative Learning,"The proposed CSGF consists of two key modules: the MGM and the TSM. These two modules are linked through the cross-module feature sharing (CFS), thus being trained collaboratively, as illustrated in Fig. 3(a). The CFS was created by a skip connection between the generator's down-sampling and the segmenter's up-sampling, as depicted in Fig. 3(b). This connection enables greater interaction and anatomy constraints between the two modules by additional back-propagating the gradient of the loss function.The proposed MGM and TSM are trained collaboratively. Given a dataset {T 1  i , T 2 i , M i } i=1∼N , the generator in MGM can be trained aswhere * GAN represents the GAN loss and * MSE represents mean square error (MSE) loss. Meanwhile, the segmenter in TSM can be trained aswhere * CE represents the cross-entropy loss and * DICE represents dice coefficient loss. During testing, for each input scan, if it has both T1w and T2w, we predict its tissue label as Mi = S(T 1 i , T 2 i ). If it only has T1w modality, we first generate its T2w modality by T2 i = G(T 1 i ), and then predict its tissue label as Mi = S(T 1 i , G(T 1 i ))."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,3.1,Experimental Details,"The total scans of MRI data used in this study was 155, divided into training and test sets in a 4:1 ratio. Before training, the data had the redundant background removed and cropped into a size of 160 × 160 × 160, and then overlapping patches of size 64 were generated. The intensity values of all patches were normalized to [-1, 1]. Both MGM and TSM were optimized simultaneously during training. We employed the Adam optimizer with a learning rate of 0.0002 for the first 50 epochs, after which the learning rate gradually decreased to 0 over the following 50 epochs. In the testing phase, overlapping patches with stride 32 generated from testing volumes were input into the model, and the obtained results were rebuilt to a volume, during which the overlaps between adjacent patches were averaged. All experiments were implemented based on Pytorch 1.13.0 and conducted on NVIDIA RTX 4090 GPUs with 24GB VRAM in Ubuntu 18.04."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,3.2,Evaluation of Framework,"Totally 31 scans of early-developing macaques were used for framework evaluation. To provide a comparison baseline, we used PTNet3D (PTNet), one stateof-the-art algorithm for medical image translation, and U-Net [23], the most widely used segmentation model, to construct the CSGF. First, we trained two U-Net models as baseline models, one of which used only the T1w modality as input, while the other used both T1w and T2w modalities. Next, we trained a PTNet model, which generates corresponding T2w images based on the input T1w images. Finally, we constructed the CSGF framework using the PTNet and U-Net models described in Sect. 2. It is important to note that we constructed two versions of CSGF, one with the embedding of CFS to reflect the role of feature sharing and one without. According to the developmental stage of macaques [22], we divided the evaluation scans into three groups, including 7 in early infancy (age range of 0 to 6 months), 9 in infancy (age range of 7 to 12 months) and 15 in yearlings and juveniles (age range of 13 to 36 months).Compared with the other two stages, the brain in the early infancy stage from 0 to 6 months is in a state of vigorous development, especially the extremely low contrast between tissues from 0 to 2 months, which brings great challenges to this study. In the evaluation of tissue segmentation, we assumed the practical scenario that the test scans contain only T1w images. We compared four results, including U-Net with T1w input, U-Net with T1w and generated T2w input from PTNet (i.e. independent-task mode), CSGF without CFS embedding and the full CSGF framework. We quantitatively evaluate the results by using the average surface distance (ASD) and the dice coefficient, as reported in Table 1. The results show that compared to U-Net with a single T1w images input, U-Net with independent PTNet-generated T2w images as an additional modality input has worse results, especially for data during infancy. This may be due to the fact that the T2w images generated in the independent-task mode do not accurately express the anatomical structures of different tissues based on the real T1w images, thus introducing unnecessary noise for subsequent tissue segmentation. In comparison, the results of the two CSGF methods are superior to those of the above two, which may be due to the fact that under task-oriented supervised training, the T2w images generated by PTNet tend to retain anatomical structures that are conducive to subsequent tissue segmentation. Notably, the method in which CFS is embedded achieves the best results, showing that tissue segmentation also benefits from the deconstruction of features by the MGM during the encoding stage. Figure 4 presents representative visual results of tissue segmentation at two age stages. It can be seen that the result of CSGF is the closest to the ground truth, retaining more anatomical details.In the evaluation of the modality generation, we compared generated T2w images from T1w images based on three methods, including PTNet, CSGF w/o CFS (i.e. PTNet under the supervision of U-Net) and the final CSGF framework. We quantitatively evaluated the results by using the peak signal to noise ratio (PSNR) and structural similarity index (SSIM), as reported in Table 2. It demonstrates that co-training supervised by subsequent tissue segmentation leads to PTNet in CSGF with improved generative results, as compared to a single PTNet. Especially in the infancy stage, the results of CSGF have been significantly improved. This improvement can be attributed to the fact that the generation module is constrained by task-oriented anatomical features, which encourage the preservation of complete tissue structures. This helps to address the challenge of low tissue contrast to some extent. The stable performance of the generative module suggests that anatomical feature consistency and task relevance can coexist and be co-optimized between MGM loss and TSM loss, leading to the preservation of image quality in the generative model. Furthermore, the incorporation of CFS enhances the generation results, possibly due to the added anatomical feature constraints that encourage the MGM to prioritize the encoding of anatomical information."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,4.0,Conclusion,"We propose the novel Collaborative Segmentation-Generation Framework (CSGF) to deal with the missing-modality brain MR images for tissue segmentation in early-developing macaques. Under this framework, the modality generation module (MGM) and tissue segmentation module (TSM) are jointly trained through cross-module feature sharing (CFS). The MGM is trained under the supervision of the TSM, while the TSM is trained with real and generated neuroimages. Comparative experiments on 155 scans of developing macaque data show that our CSGF outperforms conventional independent-task mode in both modality generation and tissue segmentation, showing its great potential in neuroimage research. Furthermore, as the proposed CSGF is a general framework, it can be easily extended to other types of modality generation, such as CT and PET, combined with other image segmentation tasks."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,1.0,Introduction,"Automatic medical image segmentation is the implementation of data-driven image segmentation concepts to identify a specific anatomical structure's surface or volume in a medical image ranging from X-ray and ultrasonography to CT and MRI scans. Deep learning algorithms are exquisitely suited for this task because they can generate measurements and segmentations from medical images without the time-consuming manual work as in traditional methods. However, the performance of deep learning algorithms depends heavily on the availability of large-scale, high-quality, fully pixel-wise annotations, which are often expensive to acquire. To this end, few-shot learning is considered as a more promising approach and introduced into the medical image segmentation by [13].Through revisiting existing FSMS algorithms [3][4][5]16,17,19], they can be grouped into two folders, including the interactive method originated from SENet [15] (shown in Fig. 1(a)) and the prototype networks [18,20] (demonstrated in Fig. 1(b)). For the interaction-based approach, the ideas of attention [19], and contrastive learning [22] are introduced to work interactively between parallel support and query arms. In contrast, prototype network-based approach almost dominates the FSMS research, such as SSL-ALPNet [13], ADNet [5] and SR&CL [21], whose core idea is to obtain semantic-level prototypes by compressing support features, and then make predictions by matching with query features. However, the problem of how to obtain an accurate and representative prototype remains.The main reason affecting the representativeness of the prototype is the significant discrepancy between support and query. Specifically, in general, different protocols are taken for different patients, which results in a variety of superficial organ appearances, including the size, shape, and contour of features. In this case, the prototype generated from the support features may not accurately represent the key attributes of the target organ in the query image. In addition, it is also challenging to extract useful information (prototypes of novel classes) from the cluttered background due to the extremely heterogeneous texture between the target and its surroundings, which may contain information belonging to some novel classes or redundant information issue [19].To mitigate the impact of intra-class diversity, it considers subdividing the foreground of the supporting prototypes to produce some regional prototypes, which are then rectified to suppress or exclude areas inconsistent with the query targets, as illustrated in Fig. 1(c). Concretely, in the prototype learning stage, multiple subdivided regional prototypes are enhanced with a more accurate class center, which can be derived from the newly designed Regional Prototype Generation (RPG) and Query Prototype Generation (QPG) modules. Then, a designed Region-enhanced Prototypical Transformer (RPT) that is mainly composed of a number of stacked Bias-alleviated Transformer (BaT) blocks, each of which contains the core debiasing function-Search and Filter (S&F) modules, to filter out undesirable prototypes. As shown in Fig. 2, Our contributions are summarized as follows:-A Region-enhanced Prototypical Transformer (RPT) consisting of stacked Bias-alleviated Transformer (BaT) blocks is proposed to mitigate the effects of large intra-class variations present in FSMS through Search and Filter (S&F) modules devised based on the self-selection mechanism. -A subdivision strategy is proposed to perform in the foreground of the support prototype to generate multiple regional prototypes, which can be further iteratively optimized by the RPT to produce the optimal prototype. -The proposed method can achieve state-of-the-art performance on three experimental datasets commonly used in medical image segmentation."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.1,Overall Architecture,"Before introducing the overall architecture, it is necessary to briefly explain how data is processed. Specifically, the 3D supervoxel clustering method [5] is employed to generate pseudo-masks as supervision, which is learned in a selfsupervised learning manner without any manual annotations. Meta-learningbased episodic tasks can then be constructed using the generated pseudo-masks. Notably, the pseudo-masks obtained by the 3D clustering method is more consistent with the volumetric properties of medical images than the 2D superpixel clustering method adopted in [13].As depicted in Fig. 2, the overall architecture includes three main components: the Regional Prototype Generation (RPG) module, the Query Prototype Generation (QPG) module and the Region-enhanced Prototypical Transformer (RPT) consisting of three Bias-alleviated Transformer (BaT) blocks. The pipeline first extracts features from support and query images using a weightshared ResNet-101 [6] as a backbone, which has been pretrained on the MS-COCO dataset [10]. We employ the ResNet101 pretrained on MS-COCO for optimal performance, and the comparison with ResNet50 pretrained on Ima-geNet dataset [2] is also included in the appendix. The extracted features are then taken as the input of the RPG and QPG modules to generate multiple region prototypes, which will be rectified by the following RPT to produce the optimal prototype."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.2,Regional Prototype Generation,"The core problem considered in this paper is what causes prototype bias. By examining the input data, it can be observed that images of healthy and diseased organs have a chance to be considered as support or query. This means that if there are lesioned or edematous regions in some areas of the support images, they will be regarded as biased information which in reality cannot be accurately transferred for the query images containing only healthy organs. When these prototypes that contain the natural heterogeneity of the input images are processed by the Masked Average Pooling (MAP) operation, they inevitably lead to significant intra-class biases.To cope with the above problems, we propose a Region Prototype Generation (RPG) module to generate multi-region prototypes by performing subdivisions in the foreground of the support images. Given an input support image I s and the corresponding foreground mask M f , the foreground of this image can be obtained by calculating their product. The foreground image then can be partitioned into N f regions, where N f is set to 10 by default. By using the Voronoi-based partition method [1,23], a set of regional masks {V n } N f n=1 can be derived for subsequent use of Masked Average Pooling (MAP) to generate a set of coarse regional prototypeswhere F s ∈ R C×H×W is the feature extracted from the support images and V n denotes the regional masks."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.3,Query Prototype Generation,"Once a set of coarse regional prototypes Ps have been generated for the support images, we can employ the method introduced in [11] to learn the coarse query prototype Pq ∈ R 1×C . Concretely, it first uses the MAP(•) operator as introduced in Eq. ( 1) to learn a global support prototype P g = MAP(F s , M s ) with P g ∈ R 1×C , whose output can then be used to calculate the coarse query foreground mask Mf q . Considering that the empirically designed threshold described in [11] may affect the quality of the Mf q , we hereby introduce a learnable threshold τ . This process can be denoted aswhere F q ∈ R C×H×W is feature extracted from query images, S(a, b) = -αcos(a, b) is the negative cosine similarity with a fixed scaling factor α = 20, σ denotes the Sigmoid activation, and τ is obtained by applying one averagepooling and two fully-connected layers (FC) to the query feature, expressed as τ = FC(F q ). After this, the coarse query foreground prototype can be achieved by using Pq = MAP(F q,i , Mf q,i )."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.4,Region-Enhanced Prototypical Transformer,"The above received prototypes Ps and Pq are taken as input to the proposed Region-enhanced Prototypical Transformer (RPT) to rectify and regenerate the optimal global prototype P s . As shown in Fig. 2, our RPT mainly consists of L stacked Bias-alleviated Transformer (BaT) blocks each of which contains a Search and Filter (S&F) module, and QPG modules that maintain the query prototypes continuously updated. Taking the first BaT block as an example, it calculates an affinity map A = Ps P q ∈ R N f ×1 to reveal the correspondence between the query and N f support regional prototypes by taking an input containing the query prototype Pq and the support prototype Ps ∈ R N f ×C obtained by concatenating all elements in Ps . Then, a selective map S ∈ R N f ×1 can be derived from the proposed self-selection based S&F module bywhere ξ is the selection threshold achieved by ξ = (min(A) + mean(A))/2, S indicates the chosen regions from the support image that performs compatible with the query at the prototypical level. Then, the heterogeneous or disturbing regions of support foreground will be weeded out with softmax(•) function. The preliminary rectified prototypes Po s ∈ R N f ×C is aggregated as:The refined Po s will be fed into the following components designed based on the multi-head attention mechanism to produce the outputwhere Po+1 s ∈ R N f ×C is the intermediate generated prototype, LN(•) denotes the layer normalization, MHA(•) represents the standard multi-head attention module and MLP(•) is the multilayer perception.By stacking multiple BaT blocks, our RPT can iteratively rectify and update all coarse support and the query prototype. Given the prototypes P l-1 s and P l-1 q from the previous BaT block, the updates for the current BaT block are computed by:where P l s ∈ R N f ×C and P l q ∈ R 1×C (l = 1, 2, ..., L) are updated prototypes, GAP(•) denotes the global average pooling operation. The final output prototypes P s optimized by the RPT can be used to predict the foreground of the query image by using Eq. (2: Mf q = 1σ(S(F q , GAP(P 3 s ))τ ), while its background can be obtained by Mb q = 1 -Mf q accordingly."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.5,Objective Function,"The binary cross-entropy loss L ce is adopted to determine the error between the predict masks Mq and the given ground-truth M q . Formally,Considering the prevalent class imbalance problem in medical image segmentation, the boundary loss [8] L B is also adopted and it is written aswhere θ denotes the network parameters, Ω denotes the spatial domain, φG : Ω → R denotes the level set representation of the ground-truth boundary, φG(q) = -D G (q) if q ∈ G and φG(q) = D G (q) otherwise, D G is distance map between the boundary of prediction and ground-truth, and s θ (q) : Ω → [0, 1] denotes softmax(•) function.Overall, the loss used for training our RPT is defined aswhere L dice is the Dice loss [12], η is initially set to 1 and decreased by 0.01 every epoch. Experiment Setup: The model is trained for 30k iterations with batch size set to 1. During training, the initial learning rate is set to 1 × 10 -3 with a step decay of 0.8 every 1000 iterations. The values of N f and iterations L are set to 10 and 3, respectively. To simulate the scarcity of labeled data in medical scenarios, all experiments embrace a 1-way 1-shot setting, and 5-fold cross-validation is also carried out in the experiments, where we only record the mean value.Evaluation: For a fair comparison, the metric used to evaluate the performance of 2D slices on 3D volumetric ground-truth is the Dice score used in [13]. Furthermore, two different supervision settings are used to evaluate the generalization ability of the proposed method: in Setting 1, the test classes may appear in the background of the training slices, while in Setting 2, the training slices containing the test classes are removed from the dataset to ensure that the test classes are unseen. Note that Setting 2 is impractical for Card-MRI scans, since all classes typically co-occur on one 2D slice, making label exclusion impossible.In addition, as in [13], abdominal organs are categorized into upper abdomen  (liver, spleen) and lower abdomen (left, right kidney) to demonstrate whether the learned representations can encode spatial concepts."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,3.1,Quantitative and Qualitative Results,"Table 1 shows the performance comparison of the proposed method with stateof-the-art methods, including the vanilla PA-Net [20], SE-Net [15], ADNet [5], CRAPNet [3], SSL-ALPNet [13,14], AAS-DCL [22] and SR&CL [21] under two experimental settings. From Table 1, it can be seen that the proposed method outperforms all listed methods in terms of the Mean values obtained under two different settings. Especially, the Mean value on Abd-CT dataset under Setting 1 reaches 77.83, which is 3.31 higher than the best result achieved by AAS-DCL. Consistent improvements are also indicated for Card-MRI dataset and can be found in the Appendix. In addition to the quantitative comparisons, qualitative results of our model and the other model on Abd-MRI and Abd-CT are shown in Fig. 3 (See Appendix for CMR dataset). It is not difficult to see that our model shows considerable bound-preserving and generalization capabilities."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,3.2,Ablation Studies,"The ablation studies were conducted on Abd-MRI dataset under Setting 2. As can be seen from Fig. 4, the use of three stacked BaT blocks is suggested to obtain the best Dice score. From Table 2, using a combination of boundary and dice loss gives a 0.61 increase in terms of the dice score compared to using only the cross-entropy loss. More ablation study results can be found in Appendix."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,4.0,Conclusion,"In this paper, we introduced a Region-enhanced Prototypical Transformer (RPT) to mitigate the impact of large intra-class variations present in medical image segmentation. The model is mainly beneficial from a subdivision-based strategy used for generating a set of regional support prototypes and a self-selection mechanism introduced to the Bias-alleviated Transformer (BaT) blocks. The proposed RPT can iteratively optimize the generated regional prototypes and output a more precise global prototype for predictions. The results of extensive experiments and ablation studies can demonstrate the advancement and effectiveness of the proposed method."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_26.
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,1.0,Introduction,"Head and neck (HaN) cancer is a prevalent type of cancer [3] with a yearly incidence of above 1 million cases and prevalence of above 4 million cases worldwide, accounting for around 5% of all cancer sites [17]. Radiotherapy (RT) is a standard treatment modality for HaN cancer, which aims to deliver high doses of radiation to cancerous cells while sparing nearby healthy organs-at-risk (OARs) [21]. To optimize radiation dose distribution, accurate three-dimensional (3D) segmentation of target volumes and OARs is required. Computed tomography (CT) is the primary imaging modality used for RT planning due to its ability to provide information about electron density, however, its low image contrast for soft tissues, including tumors, makes accurate segmentation of soft tissue OARs challenging. Therefore, the integration of complementary imaging modalities, such as magnetic resonance (MR), has been strongly recommended in clinical practice to enhance the segmentation of several soft tissue OARs in the HaN region [1]. This naturally poses a question of whether automatic OAR segmentation can benefit from the MR image modality. Our study therefore aims to evaluate the impact of MR integration on the quality and robustness of automatic OAR segmentation in the HaN region, therefore contributing to the growing body of research on multimodal methods for medical image analysis."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Related Work.,"A literature review by Zhang et al. [24] divides deep learning (DL)-based multimodal segmentation methods into three fusion strategy groups: early, late and hybrid (also named layer ) fusion. The first two groups of methods are most commonly applied; early fusion comprises simple concatenation of modalities along the channel dimension before feeding them into the deep neural network. Additionally, concatenating feature maps (FMs) from separate modality encoders can also be considered as early fusion [7]. Late fusion, on the other hand, employs separate branches for each input modality and then fuses the output features by either plain concatenation or by weighing the contributions of separate branches at the decision level. For example, Zhang et al. [23] proposed an attention mechanism to fuse FMs from two separate U-Nets that accepted contrast-enhanced arterial and venous phase CT images. The third group, hybrid fusion, aims to combine the strengths of early and late fusion [24] by employing two or more separate encoders (i.e. one for each modality) and a single decoder, where features from different resolution levels of the encoder are fused and fed into the decoder that produces the final full-resolution segmentation. Such hybrid or multi-level fusion along with the adaptive fusion method represents the current trend in computer vision [24], with the self-supervised model adaptation method as a prime example [18]. One important aspect is also the missing modality scenario, meaning that the multimodal model should produce satisfactory results even if only one input modality is available. Nevertheless, the optimal fusion strategy remains an open question in need of further exploration. Similar conclusions were reached in a review of multimodal segmentation methods in the medical imaging community by Zhou et al. [25]. Most methods implement either early or late fusion, however, the layer fusion strategy was identified as a better choice, since dense connections among layers can exploit more complex and complementary information to enhance training. The highlight is HyperDenseNet, a dual-path 3D network proposed by Dolz et al. [4] that employs dense connections between two convolutional paths, and achieves improvements compared to other fusion strategies and single modality variants. However, other studies have shown that the best fusion strategy depends on the specific nature of the problem, e.g. Yan et al. [22] demonstrated that the late fusion outperforms the other two approaches for the longitudinal detection of diabetic retinopathy. Relevant to the field of multimodal segmentation are also developments on unpaired multimodal segmentation, where cross-modality learning is employed to take advantage of different image modalities covering the same anatomy, but without the constraint to collect images from the same patients [5,10,19]. Although the methodologies comprising CycleGANs and/or multiple segmentation networks [10,19] seem promising, they can be excessively complex for the task of HaN OAR segmentation where both CT and MR image modalities from the same patient are often available. Consequently, our primary focus is the paired multimodal segmentation problem, including the missing modality scenario.Motivation. When segmenting OARs in the HaN region for the purpose of RT planning, a multimodal segmentation model that can leverage the information from CT and MR images of the same patient might be beneficial compared to separate single-modal models. Firstly, as intuition suggests, such a model would rely on the CT image for bone structures and on the MR image for soft tissues, and therefore improve the overall segmentation quality by exploiting the complementary information from both modalities. Secondly, a multimodal model would facilitate cross-modality learning by extracting knowledge from one and applying that knowledge to the other modality, potentially improving the segmentation accuracy. Several studies indicated that such an approach is feasible, for example, for improving video classification by training a model on an auxiliary audio reconstruction task [12], or for audio-based detection by using the multimodal knowledge distillation concept, where teacher networks trained on RGB, depth and thermal images improve a student network trained only on audio data [20]. Finally, from the DL infrastructure maintenance perspective, it is easier to maintain a single model that can handle both modalities than two separate models for each modality. However, clinical practice differs considerably from theory, meaning that a number of considerations must be taken into account. Firstly, although MR image acquisition is recommended, it is not always feasible due to time constraints, scanner occupancy and financial aspects. Consequently, automatic OAR multimodal segmentation is required to handle the missing modality scenario, and provide a similar segmentation quality as a single-modality system. Secondly, because CT and MR images are not acquired simultaneously and with the same acquisition parameters (e.g. resolution), there is an inherent misalignment between both modalities. This can be mitigated with image registration, but not completely, mainly due to different patient positioning that especially affects the deformation of soft tissues, and various modality-specific artifacts (e.g. motion, implants, partial volume effect, etc.)."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Contributions.,"To tackle these considerations, we propose a mechanism named modality fusion module (MFM) that can generally be applied to any network architecture that learns features from multiple modalities, and shows promising performance also in the missing modality scenario. The advantages of the proposed MFM are the following: 1) it enables the spatial alignment of FMs from one with FMs from the other modality to further reduce errors that persist after deformable registration of input images, and enrich the FMs to improve the final OAR segmentation, 2) it significantly improves the performance of the missing modality scenario compared to other baseline fusion approaches, and 3) it performs well also on single modality out-of-distribution data, therefore facilitating cross-modality learning and contributing to better model generalizability."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,2.0,Methods,"Backbone Architecture. Our chosen backbone network is based on nnU-Net, a publicly available framework for DL-based segmentation [8] that builds on the U-Net architecture [16], adds self-configurable pre-processing, augmentation and post-processing, and employs efficient training strategies. However, nnU-Net, which uses an early fusion strategy by concatenating input images or patches before feeding them to the first network layer, may not be the optimal strategy for multimodal segmentation. Recent studies have shown that this approach does not allow the network to learn meaningful high-level features from each modality before their fusion, resulting in only simple relationships between intensities from each input modality [4,23]. This is particularly problematic when fusing CT and MR images, which differ in several aspects, such as the type and location of artifacts, acquisition parameters, and visibility of soft tissues and bone structures. While MR images can help to improve the delineations of OARs that are poorly visible in CT images, the primary delineation is always performed on CT images with the help of registered MR images. An important repercussion is that image registration errors propagate into OAR delineations, which is particularly salient in the HaN region. To address these challenges, we propose an upgraded nnU-Net network with two separate encoders, one for each modality, and a common decoder that fuses FMs using the proposed MFM that learns to infer affine transformation parameters in a single forward pass. This approach efficiently pseudo-registers FMs from the MR encoder with those from the CT encoder, mitigating the effects of registration errors caused by non-rigid deformation of OARs and imaging artifacts."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Modality Fusion Module.,"The proposed MFM draws inspiration from the work of Jaderberg et al. [9], who introduced a spatial transformer network (STN) that learns to infer transformation parameters in a single forward pass, and then uses them to transform images and/or FMs. The fundamental idea is that STN can learn meaningful features that are spatially invariant to characteristics of the input data, without the need for extra supervision, thereby enhancing task performance. While it was demonstrated that complete spatial invariance cannot be achieved with STNs [6], the work of Jaderberg et al. is crucial in showing that STNs can be implemented as differentiable modules, enabling the loss to be propagated through the sampling (interpolation) mechanism. The same underlying principle of STNs has also been leveraged in optical flow and its derivative work semantic flow, where the flow alignment module was proposed to resample low-resolution FMs and align them with high-resolution FMs [2]. We capitalize on the same principle to register FMs from MR images to those from CT images. Notably, MFM is different from semantic flow, as it takes two FMs of the same resolution but from different modalities, and aligns FMs from the auxiliary modality to FMs of the primary modality. We propose to use MFM at each resolution level of the nnU-Net backbone, which is schematically presented in Fig. 1, and consists of three blocks: localization network, grid generator and sampler. The localization network is a regressor network that accepts concatenated FMs from both encoders and applies four blocks of strided convolutions followed by the ReLU activation to reduce their spatial dimensions. The final FMs are flattened and fed into a simple two-layer fully connected network, which outputs 12 affine 3D transformation parameters that are then passed to the grid generator. The generated sampling grid is used by the sampler to resample FMs from the second encoder, which are then concatenated with the untouched FMs from the first encoder and the decoder (right before the bottleneck, only the first two are concatenated, as there are no decoder FMs at that level). Both the grid generator and the sampler and readily implemented in the PyTorch library [9], and because they are both differentiable, no special optimization is needed for the localization network, allowing localization parameters to be optimized with the main (segmentation) loss function. Since there is no additional supervision that would assure perfect registration, we refer to this process as pseudo-registration. The purpose of this architecture is to align FMs from both modalities and improve their fusion, leading to better segmentation results.Baseline Comparison. We evaluate the performance of the proposed MFM nnU-Net against three baseline networks: 1) a single modality nnU-Net trained only on CT images, 2) a nnU-Net trained on concatenated CT and MR image pairs, and 3) a model with separate encoders for both modalities, but with a simple concatenation along the channel axis instead of the proposed MFM. In addition, we compare our model with the state-of-the-art modality-aware mutual learning nnU-Net (MAML) that was presented at MICCAI 2021 [23]."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,3.0,Experiments and Results,"Image Datasets. The proposed methodology was evaluated on two publicly available datasets: our recently released HaN-Seg dataset [14] and the PDDCA dataset [15]. The HaN-Seg dataset comprises CT and T1-weighted MR images of 56 patients, which were deformably registered with the SimpleElastix registration tool, and corresponding curated manual delineations of 30 OARs (for details, please refer to [14]). Although only a subset of images is publicly available1 due to the ongoing HaN-Seg challenge2 , both the publicly available training as well as the privately withheld test images were used in our 4-fold cross-validation experiments. On the other hand, to evaluate the generalization ability of our method, we also conducted experiments on the CT-only PDDCA dataset (for details, please refer to [15]), from which we collected 15 images from the offand on-site test sets of the corresponding challenge for our evaluation. As this dataset is widely used for evaluating the performance of automatic HaN OAR segmentation methods, it serves as a valuable benchmark for comparison with other state-of-the-art methods. Note that none of the images from the CT-only PDDCA dataset were used for training, and as our model expects two inputs, we substituted the missing MR modality with an empty matrix (i.e. zeros).Implementation Details. All models were trained for all OARs using the 3d fullres configuration of nnU-Net, with the only modification that we reduced rotation around the axial axis and disabled image flipping along the sagittal plane, which eliminated segmentation errors that were previously observed for the paired (left and right) OARs. The same modification was also used with the MAML model. To ensure a fair model comparison, we set the number of filters in the encoder of the single modality baseline model to match the number of filters of the entry-level concatenation encoder. We also halved the number of filters in networks that have separate encoders so that the overall number of parameters in the proposed model and the baselines remains approximately the same (excluding the parameters in the localization part of MFM  block). Note that the MAML model, which is composed of two U-Nets, had a considerably higher number of parameters. To address the challenge of a relatively small dataset, we adopted a 4-fold cross-validation strategy without using any external training images. All models were trained until convergence, i.e. when the validation loss plateaued, and we selected the model with the best validation loss for inference.Results. The quality of the obtained OAR segmentation masks was evaluated by computing the Dice similarity coefficient (DSC) and the 95 th -percentile Hausdorff distance (HD 95 ) against reference manual delineations, and the results for all OARs are presented in Figs. 2 and3, respectively. Since not all images contain all 30 OARs (due to a different field-of-view), we first calculated the mean metric for each OAR and then the overall mean across all OARs to ensure that the contributions were equally weighted. We also performed analysis of statistical significance by applying paired sample t-tests with the Bonferroni correction, presented with bars on top of the box plots (non-significant: ns (p > 0.05), significant: * (0.01 < p < 0.05), * * (0.001 < p < 0.01), * * * (0.0001 < p < 0.001) and * * * * (p < 0.0001))."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,4.0,Discussion,"In this study, we evaluated the impact on the quality and robustness of automatic OAR segmentation in the HaN region caused by the incorporation of the MR modality into the segmentation framework. We devised a mechanism named MFM and combined it with nnU-Net as our backbone segmentation network. The choice of using nnU-Net as the backbone was based on the rationale that nnU-Net already incorporates numerous state-of-the-art DL innovations proposed in recent years, and therefore validation of the proposed MFM is more challenging in comparison to simply improving a vanilla U-Net architecture, and consequently also more valuable to the research community.Segmentation Results. The obtained results demonstrate that our model performs best in terms of DSC (Fig. 2). The resulting gains are significant compared to separate encoders and CT-only models, and were achieved with 4-fold crossvalidation, therefore reducing the chance of a favorable initialization. However, DSC has been identified not to be the most appropriate metric for evaluating the clinical adequacy of segmentations, especially when the results are close to the interrater variability [13], moreover, it is not appropriate for volumetrically small structures [11]. On the other hand, distance-based metrics, such as HD 95 (Fig. 3), are preferred as they better measure the shape consistency between the reference and predicted segmentations. Although MAML achieved the best results in terms of HD 95 , indicating that late fusion can efficiently merge the information from both modalities, it should be noted that MAML has a considerate advantage due to having two decoders and an additional attention fusion block compared to the baseline nnU-Net with separate encoders and a single decoder. On the other hand, our approach based on separate encoders with MFM is not far behind, with a mean HD 95 of 4.06 mm, which is more than 15% better than the early concatenation fusion. The comparison to the baseline nnU-Net with separate encoders offers the most direct evaluation of the proposed MFM. An approximate 10% improvement in HD 95 suggests that MFM allows the network to learn more informative FMs that lead to a better overall performance.Missing Modality Scenario. The overall good performance on the HaN-Seg dataset suggests that all models are close to the maximal performance, which is bounded by the quality of reference segmentations. However, the performance on the PDDCA dataset that consists only of CT images allows us to test how the models handle the missing modality scenario and perform on an out-ofdistribution dataset, as images from this dataset were not used for training. As expected, the CT-only model performed best in its regular operating scenario, with a mean DSC of 74.7% (Fig. 2) and HD 95 of 6.02 mm (Fig. 3). However, significant differences can be observed between multimodal methods, where the proposed model outperformed MAML and other baselines by a large margin in both metrics. The MAML model with a mean DSC of less than 15% and HD 95 of more than 190 mm was not able to handle the missing modality scenario, whereas the MFM model performed almost as good as the CT-only model, with a mean DSC of 67.8% and HD 95 of 8.18 mm. It should be noted that we did not employ any training strategies to improve handling of missing modalities, such as swapping input images or intensity augmentations. A possible explanation is that the proposed MFM facilitates cross-modality learning, enabling nnU-Net to extract better FMs from CT images even in such extreme scenarios."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,5.0,Conclusions,"In this study, we introduced MFM, a fusion module that aligns FMs from an auxiliary modality (e.g. MR) to FMs from the primary modality (e.g. CT). The proposed MFM is versatile, as it can be applied to any multimodal segmentation network. However, it has to be noted that it is not symmetrical, and therefore requires the user to specify the primary modality, which is typically the same as the primary modality used in manual delineation (i.e. in our case CT). We evaluated the performance of MFM combined with the nnU-Net backbone for segmentation of OARs in the HaN region, an important task in RT cancer treatment planning. The obtained results indicate that the performance of MFM is similar to other state-of-the-art methods, but it outperforms other multimodal methods in scenarios with one missing modality."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 71.
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,1.0,Introduction,"Atrial Fibrillation (AFib) is a prevalent cardiac arrhythmia affecting over 45 million individuals worldwide as of 2016 [7]. Catheter ablation, which involves the elimination of affected cardiac tissue, is a widely used treatment for AFib. To ensure procedural safety and minimize harm to healthy tissue, Intracardiac Echocardiography (ICE) imaging is utilized to guide the intervention.Intracardiac Echocardiography imaging utilizes an ultrasound probe attached to a catheter and inserted into the heart to obtain real-time images of its internal structures. In ablation procedures for Left Atrium (LA) AFib treatment, the ICE ultrasound catheter is inserted in the right atrium to image the left atrial structures. The catheter is rotated clockwise to capture image frames that show the LA body, the LA appendage and the pulmonary veins [12]. Unlike other imaging modalities, such as transesophageal echocardiography, ICE imaging does not require general anesthesia [3]. Therefore, it is a safer and more convenient option for cardiac interventions using ultrasound imaging.The precise segmentation of cardiac structures, particularly the LA, is crucial for the success and safety of catheter ablation. However, segmentation of the LA is challenging due to the constrained spatial resolution of 2D ICE images and the manual manipulation of the ICE transducer. Additionally, the sparse sampling of ICE frames makes it difficult to train automatic segmentation models. Consequently, there is a persistent need to develop interactive editing tools to help experts modify the automatic segmentation to reach clinically satisfactory accuracy.During a typical ICE imaging scan, a series of sparse 2D ICE frames is captured and a Clinical Application Specialist (CAS) annotates the boundaries of the desired cardiac structure in each frame1 (Fig. 1a). To construct dense 3D masks for training segmentation models, Liao et al. utilized the 3D geometry information from the ICE transducer, to project the frames and their annotations onto a 3D grid [8]. They deformed a 3D template of the LA computed from 414 CT scans to align as closely as possible with the CAS contours, producing a 3D mesh to train a segmentation model [8]. However, the resulting mesh may not perfectly align with the original CAS contours due to factors such as frames misalignment and cardiac motion (Fig. 1b). Consequently, models trained with such 3D mesh as ground truth do not produce accurate enough segmentation results, which can lead to serious complications (Fig. 1c).A natural remedy is to allow clinicians to edit the segmentation output and create a model that incorporates and follows these edits. In the case of ICE data, the user interacts with the segmentation output by drawing a scribble on one of the 2D frames (Fig. 1d). Ideally, the user interaction should influence the segmentation in the neighboring frames while preserving the original segmentation in the rest of the volume. Moreover, the user may make multiple edits to the segmentation output, which must be incorporated in a sequential manner without compromising the previous edits.In this paper, we present a novel interactive editing framework for the ICE data. This is the first study to address the specific challenges of interactive editing with ICE data. Most of the editing literature treats editing as an interactive segmentation problem and does not provide a clear distinction between interactive segmentation and interactive editing. We provide a novel method that is specifically designed for editing. The novelty of our approach is two-fold: 1) We introduce an editing-specific novel loss function that guides the model to incorporate user edits while preserving the original segmentation in unedited areas. 2) We present a novel evaluation metric that best reflects the editing formulation. Comprehensive evaluations of the proposed method on ICE data demonstrate that the presented loss function achieves superior performance compared to traditional interactive segmentation losses and training strategies, as evidenced by the experimental data."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.1,Problem Definition,"The user is presented first with an ICE volume, x ∈ R H×W ×D , and its initial imperfect segmentation, y init ∈ R H×W ×D , where H, W and D are the dimensions of the volume. To correct inaccuracies in the segmentation, the user draws a scribble on one of the 2D ICE frames. Our goal is to use this 2D interaction to provide a 3D correction to y init in the vicinity of the user interaction. We project the user interaction from 2D to 3D and encode it as a 3D Gaussian heatmap, u ∈ R H×W ×D , centered on the scribble with a standard deviation of σ enc [9]. The user iteratively interacts with the output until they are satisfied with the quality of the segmentation.We train an editing model f to predict the corrected segmentation output ŷt ∈ R H×W ×D given x, y t init , and u t , where t is the iteration number. The goal is for ŷt to accurately reflect the user's correction near their interaction while preserving the initial segmentation elsewhere. Since y t+1 init ≡ ŷt , subsequent user inputs u {t+1,...,T } should not corrupt previous corrections u {0,...,t} (Fig. 2)."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.2,Loss Function,"Most interactive segmentation methods aim to incorporate user guidance to enhance the overall segmentation [2,5,9]. However, in our scenario, this approach may undesirably modify previously edited areas and may not align with clinical expectations since the user has corrected these areas, and the changes are unexpected. To address the former issue, Bredell et al. proposed an iterative training strategy in which user edits are synthesized and accumulated over a fixed number of steps with every training iteration [1]. However, this approach comes with a significant increase in training time and does not explicitly instruct the model to preserve regions away from the user input.We propose an editing-specific loss function L that encourages the model to preserve the initial segmentation while incorporating user input. The proposed loss function incentivizes the model to match the prediction ŷ with the ground truth y in the vicinity of the user interaction. In regions further away from the user interaction, the loss function encourages the model to match the initial segmentation y init , instead. Here, y represents the 3D mesh, which is created by deforming a CT template to align with the CAS contours y cas [8]. Meanwhile, y init denotes the output of a segmentation model that has been trained on y.We define the vicinity of the user interaction as a 3D Gaussian heatmap, A ∈ R H×W ×D , centered on the scribble with a standard deviation of σ edit . Correspondingly, the regions far from the interaction are defined as Ā = 1 -A. The loss function is defined as the sum of the weighted cross entropy losses L edit and L preserve w.r.t y and y init , respectively, as followswhereThe Gaussian heatmaps facilitate a gradual transition between the edited and unedited areas, resulting in a smooth boundary between the two regions."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.3,Evaluation Metric,"The evaluation of segmentation quality typically involves metrics such as the Dice coefficient and the Jaccard index, which are defined for binary masks, or distancebased metrics, which are defined for contours [13]. In our scenario, where the ground truth is CAS contours, we use distance-based metrics 2 . However, standard utilization of these metrics computes the distance between the predicted and ground truth contours, which misleadingly incentivizes alignment with the ground truth contours in all regions. This approach incentivizes changes in the unedited regions, which is undesirable from a user perspective, as users want to see changes only in the vicinity of their edit. Additionally, this approach incentivizes the corruption of previous edits.We propose a novel editing-specific evaluation metric that assesses how well the prediction ŷ matches the CAS contours y cas in the vicinity of the user interaction, and the initial segmentation y init in the regions far from the interaction.where, ∀(i, j, k) ∈ {1, . . . , H} × {1, . . . , W } × {1, . . . , D}, D edit is the distance from y cas to ŷ in the vicinity of the user edit, as follows2 Contours are inferred from the predicted mask ŷ.where d is the minimum Manhattan distance from y cas i,j,k to any point on ŷ. For D preserve , we compute the average symmetric distance between y init and ŷ, since the two contours are of comparable length. The average symmetric distance is defined as the average of the minimum Manhattan distance from each point on y init contour to ŷ contour and vice versa, as followsThe resulting D represents a distance map ∈ R H×W ×D with defined values only on the contours y cas , y init , ŷ. Statistics such as the 95 th percentile and mean can be computed on the corresponding values of these contours on the distance map.3 Experiments"
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.1,Dataset,"Our dataset comprises ICE scans for 712 patients, each with their LA CAS contours y cas and the corresponding 3D meshes y generated by [8]. Scans have an average of 28 2D frames. Using the 3D geometry information, frames are projected to a 3D grid with a resolution of 128 × 128 × 128 and voxel spacing of 1.1024 × 1.1024 × 1.1024 mm. We performed five-fold cross-validation on 85% of the dataset (605 patients) and used the remaining 15% (107 patients) for testing."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.2,Implementation Details,"To obtain the initial imperfect segmentation y init , a U-Net model [11] is trained on the 3D meshes y using a Cross-Entropy (CE) loss. The same U-Net architecture is used for the editing model. The encoding block consists of two 3D convolutional layers followed by a max pooling layer. Each convolutional layer is followed by batch normalization and ReLU non-linearity layers [4]. The number of filters in the segmentation model convolutional layers are 16, 32, 64, and 128 for each encoding block, and half of them for the editing model. The decoder follows a similar architecture.The input of the editing model consists of three channels: the input ICE volume x, the initial segmentation y init , and the user input u. During training, the user interaction is synthesized on the frame with maximum error between y init and y. 3 The region of maximum error is selected and a scribble is drawn on the boundary of the ground truth in that region to simulate the user interaction. During testing, the real contours of the CAS are used and the contour with the maximum distance from the predicted segmentation is chosen as the user interaction. The values of σ enc and σ edit are set to 20, chosen by cross-validation. Adam optimizer is used with a learning rate of 0.005 and a batch size of 4 to train the editing model for 100 epochs [6]. Table 1. Results on Cross-Validation (CV) and test set. We use the editing evaluation metric D and report the 95 th percentile of the overall editing error, the error near the user input, and far from the user input (mm). The near and far regions are defined by thresholding A at 0.5. For the CV results, we report the mean and standard deviation over the five folds. The statistical significance is computed for the difference with InterCNN. † : p-value < 0.01, ‡ : p-value < 0.001. "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.3,Results,"We use the editing evaluation metric D (Sect. 2.3) for the evaluation of the different methods. For better interpretability of the results, we report the overall error, the error near the user input, and the error far from the user input. We define near and far regions by thresholding the Gaussian heatmap A at 0.5. We evaluate our loss (editing loss) against the following baselines: (1) No Editing: the initial segmentation y init is used as the final segmentation ŷ, and the overall error in this case is the distance from the CAS contours to y init . This should serve as an upper bound for error. (2) CE Loss: an editing model trained using the standard CE segmentation loss w.r.t y. (3) Dice Loss [10]: an editing model trained using Dice segmentation loss w.r.t y. (4) InterCNN [1]: for every training sample, simulated user edits based on the prediction are accumulated with any previous edits and re-input to the model for 10 iterations, trained using CE loss. We report the results after a single edit (the furthest CAS contour from ŷ) in Table 1. A single training epoch takes ≈ 3 min for all models except InterCNN, which takes ≈ 14 min, on a single NVIDIA Tesla V100 GPU. The inference time through our model is ≈ 20 milliseconds per volume.Our results demonstrate that the proposed loss outperforms all baselines in terms of overall error. Although all the editing methods exhibit comparable performance in the near region, in the far region where the error is calculated relative to y init , our proposed loss outperforms all the baselines by a significant margin. This can be attributed to the fact that the baselines are trained using loss functions which aim to match the ground truth globally, resulting in deviations from the initial segmentation in the far region. In contrast, our loss takes into account user input in its vicinity and maintains the initial segmentation elsewhere.Sequential Editing. We also investigate the scenario in which the user iteratively performs edits on the segmentation multiple times. We utilized the same models that were used in the single edit experiment and simulated 10 editing iterations. At each iteration, we selected the furthest CAS contour from ŷ, ensuring that the same edit was not repeated twice. For the interCNN model, we aggregated the previous edits and input them into the model, whereas for all other models, we input a single edit per iteration. We assessed the impact of the number of edits on the overall error. In Fig. 3, we calculated the distance from all the CAS contours to the predicted segmentation and observed that the editing loss model improved with more edits. In contrast, the CE and Dice losses degraded with more edits due to compromising the previous corrections, while InterCNN had only marginal improvements.Furthermore, in Fig. 4, we present a qualitative example to understand the effect of follow-up edits on the first correction. Edits after the first one are on other frames and not shown in the figure. We observe that the CE and InterCNN methods did not preserve the first correction, while the editing loss model maintained it. This is a crucial practical advantage of our loss, which allows the user to make corrections without compromising the previous edits. "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,4.0,Conclusion,"We presented an interactive editing framework for challenging clinical applications. We devised an editing-specific loss function that penalizes the deviation from the ground truth near user interaction and penalizes deviation from the initial segmentation away from user interaction. Our novel editing algorithm is more robust as it does not compromise previously corrected regions. We demonstrate the performance of our method on the challenging task of volumetric segmentation of sparse ICE data. However, our formulation can be applied to other editing tasks and different imaging modalities."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_73.
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is an essential task in computer-aided diagnosis. On this task, convolutional neural networks (CNNs) have demonstrated their effectiveness in an extensive literature [2,12]. However, these CNN models obtained on training (i.e., source domain) data can hardly generalize well on the unseen test (i.e., target domain) data. The poor generalization ability, which hinders CNNs to be used in real-world clinical applications, can be attributed to the fact that the quality of medical images varies greatly across healthcare centers with different scanners and imaging protocols, resulting in large distribution discrepancy (a.k.a., domain shift). To improve the generalization ability, domain generalization (DG) methods have been proposed [20,24]. These methods can be trained on the data from one or multiple source domains. Considering the diversity of training data, we focus on multi-source DG in this study.Most studies on DG attempt to alleviate the distribution discrepancy by standardizing the features [4,[14][15][16] and/or adding extra structures [7,18] to the network. However, the former suffers from over-standardization and may hinder the network to preserve semantic contents, while the latter may introduce excess misjudgment risk when estimating the distance between source-and target-domain data. A recent mainstream in DG research is to simulate the distributions of unseen target-domain data via domain randomization, i.e., perturbing the styles of source-domain data. The perturbation function can be defined in the input space [11,17,21]. Thus, it is easy to evaluate the quality of perturbed images, but the definition generally requires domain knowledge and expertise [3]. By contrast, the perturbation can be performed in the feature space [9,22,25]. However, this may cause difficulties in monitoring the perturbation degree of semantic contents in the feature space due to the lack of visualization. Recently, two critical attributes of the feature space are revealed by IBN-Net [14] and AdaIN [8], respectively. First, most style-texture information resides in the low-level features extracted by shallow layers. Second, the content-preserving style transformation can be performed by changing the statistics (e.g., mean and standard deviation) of the low-level features. Inspired by them, MixStyle [25] perturbs the feature styles using augmented statistics, which are generated by randomly mixing the statistics of the low-level features from two samples. Subsequently, more research efforts have been devoted to designing the search space that covers a larger area in the feature-style space [9,22]. Despite their improved performance, using the statistics of source-domain data for feature perturbation may limit the search space and can hardly explore in the feature-style space evenly (see Fig. 1(a),(b) and (c)). The points indicating the augmented statistics are scattered and do not completely cover the points from unseen target domain. Moreover, since all feature channels are perturbed, these methods lack a reference to the original feature, which prevents them from learning the domain-invariant representations explicitly.To address these issues, in this paper, we propose a simple but effective multisource DG method called Treasure in Distribution (TriD), which consists of two major steps: statistics randomization (SR) and style mixing (SM). SR aims to tap the potential of distribution by randomly sampling the augmented statistics from a uniform distribution to perturb the original intermediate features, which can expand the search space to cover more cases evenly (see Fig. 1(d)). It can be observed that the red points are distributed evenly and cover not only the unseen target domain, but also the source domains. This leads to the issue that the perturbed features may have unreal styles. We hypothetically extend the unreal styles to the feature space with the inspiration from [17], which demonstrated the effectiveness of unreal styles in the input space. SM is devised to mix the feature styles by randomly mixing the augmented and original statistics in the channel dimension, thus making it feasible to learn the domain-invariant representations explicitly. We have evaluated our proposed TriD on two medical segmentation tasks: (1) the prostate segmentation using magnetic resonance imaging (MRI) from six domains and (2) joint segmentation of optic disc (OD) and optic cup (OC) in fundus images from five domains. Extensive experiments demonstrate that our TriD achieves a superior generalization ability to the state-of-the-art DG methods on unseen target-domain data.Our contributions are three-fold: (1) The proposed multi-source DG method called TriD can boost the robustness of model and alleviate the performance drop on the unseen target-domain data. (2) We focus on expanding the search space of feature styles and therefore devise the statistics-randomization strategy, which allows exploring in the feature-style space evenly. (3) Different from perturbing all feature channels, we introduce the original statistics to the augmented statistics to learn the domain-invariant representations explicitly."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.1,Preliminaries,"Let f ∈ R B×C×H×W be the intermediate features in a mini-batch, where B, C, H, and W respectively denote the mini-batch size, channel, height, and width. MixStyle [25] perturbs the features by randomly mixing different feature statistics, formulated as follows:where λ m ∈ R B is a weight coefficient sampled from a Beta distribution [25],the features from two different images in a minibatch, and μ( * ), σ( * ) ∈ R B×C are the mean and standard deviation computed across the spatial dimension within each channel of each image."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.2,Treasure in Distribution (TriD),"The TriD is designed to perturb the intermediate feature styles by randomly changing the feature statistics (i.e., mean and standard deviation), as shown in Fig. 2. The feature statistics is substituted by the mixed statistics, which is generated by mixing the augmented and original statistics in channel dimension.It can be implemented as a plug-and-play module inserted into any CNN-based architecture. In this study, we use ResNet-34 [5] as the backbone to construct the segmentation network in a U-shape architecture [6]. The TriD is inserted behind the first and second residual blocks during training, and will be removed in the inference phase. We now delve into the details of our TriD."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Statistics Randomization (SR).,"Inspired by effectiveness of unreal styles in the input space [17], we hypothesis that unreal styles can also be extended to the feature space. To cover more cases evenly, we randomly sample the augmented statistics σ r , μ r ∈ R B×C from a uniform distribution which contains most feature statistics: σ r ∼ U (0, 1), μ r ∼ U (0, 1)."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Style Mixing (SM).,"To learn the domain-invariant representations explicitly, SM strategy is designed to randomly mix the augmented and original statistics along the channel wise. We first sample P ∈ R B×C from the Beta distribution: P ∼ Beta(α, α), and use P as the probability to generate the Bernoulli distribution from which to sample λ ∈ R B×C : λ ∼ Bern(P ), where α is set to 0.1 empirically [25]. Then the mixed statistics is calculated as:where f denotes the intermediate features. Finally, the mixed feature statistics is applied to perturb the normalized f similar to Eq. ( 1),Different from MixStyle, we replace the batch-wise fusion with channel-wise mixing, which avoids the sampling preference and introduces original-feature reference, so as to learn the domain-invariant representations explicitly."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.3,Training and Inference,"Let D s = {(x di , y di ) N d i=1 } K d=1 be a set including K source domains, where x di is the i-th image in the d-th source domain, and y di is the corresponding segmentation mask of x di . Our goal is to train a segmentation model that can generalize well to an unseen target domainDuring training, we empirically set a probability of 0.5 to activate TriD in the forward pass [25]. The segmentation network is trained on the source domains D s by using the combination of Dice loss (L Dice ) and cross-entropy loss (L ce ) as the objective:Inference. During inference, all the TriD modules are removed, and the segmentation network is tested on the unseen target domain D t ."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.1,Datasets and Evaluation Metrics,"Two datasets are used for this study, whose details are summarized in Table 1.The first dataset contains 116 MRI cases from six domains for prostate segmentation [10]. We preprocess these MRI cases same as a previous study [7] and only preserve the slices with the prostate region for consistent and objective segmentation evaluation. These slices are resized to 384×384 with same voxel spacing. On this dataset, we employ the Dice Similarity Coefficient (DSC) and Average Surface Distance (ASD) to evaluate the prostate segmentation. Note that we regard prostate segmentation as a 2D segmentation task, but calculate metrics on 3D volumes. The second dataset is a collection of two large and three small public datasets used for joint segmentation of optic disc (OD) and optic cup (OC) [1,13,19,23],  [6]. We employ DSC to evaluate the joint segmentation of OD and OC."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.2,Implementation Details,"We set the mini-batch size to 8 and adopt the SGD optimizer with a momentum of 0.99 for both tasks. The initial learning rate l 0 is set to 0.01 (prostate) and 0.001 (OD/OC) respectively and decays according to the polynomial rule l t = l 0 × (1t/T ) 0.9 , where l t is the learning rate of the t-th epoch and T is the number of total epochs that is set to 200 for prostate segmentation and 100 for joint segmentation of OD and OC. For both tasks, the leave-one-domain-out strategy was used to evaluate the performance of each DG method, i.e., training on K-1 source domains and evaluating on the left domain. We consistently apply the above implementation settings to our TriD and other competing methods."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.3,Results,"Comparing to Other DG Methods. We used the same segmentation network and loss function to compare our TriD with seven DG methods, including (1) DCAC: dynamic structure [7], (2) SAN-SAW: based on normalization  and whitening [16], (3) RandConv: input-space domain randomization [21], (4-6) MixStyle, EFDM, DSU: feature-space domain randomization [9,22,25] and (7) MaxStyle: adversarial noise [3]. Note that since SAN-SAW requires the data with at least two classes, we did not provide its results for prostate segmentation. Besides, we also compared our method with another two settings, including the 'Intra-Domain' and 'DeepAll'. Under the 'Intra-Domain' setting, training and test data are from the same domain, where three-fold cross-validation is used for prostate segmentation due to the lack of data split. Under the 'DeepAll' setting, the model is directly trained on the data aggregated from all source domains and tested on the unseen target domain. The results are shown in Table 2 and Table 3. It can be observed that the overall performance of our TriD is not only superior to the 'DeepAll' baseline but also better than other DG approaches. Furthermore, we found that the performance ranking of MixStyle, EFDM, DSU and our TriD is T riD > M ixStyle ≈ EF DM > DSU , which is consistent with the ranking of search scope in Fig. 1. It reveals that the unreal feature styles are indeed effective, and a larger search space is beneficial to boost the robustness.   Extendibility of SM. Under the same segmentation task, we further evaluate the extendibility of SM by combining it with other DG methods (i.e., MixStyle and EFDM), and the results are shown in Fig. 3. It shows that each approach plus SM achieves better performance in most scenarios and has superior average DSC, proving that our SM strategy can be extended to other DG methods.Location of TriD. To discuss where to apply our TriD, we repeated the experiments in joint segmentation of OD and OC and listed the results in Table 5. These four residual blocks of ResNet-34 are denoted as 'res1-4', and we trained different variants via applying TriD to different blocks. The results reveal that (1) the best performance is achieved by applying TriD to 'res12' that extract the low-level features with the most style-texture information; (2) the performance degrades when applying TriD to the third and last blocks that tend to capture semantic content rather than style texture [14].Uniform Distribution vs. Normal Distribution. It is particularly critical to choose the distribution from which to randomly sample the augmented statistics.To verify the advantages of uniform distribution, we repeated the experiments in joint segmentation of OD and OC by replacing the uniform distribution with a normal distribution N (0.5, 1) and compared the effectiveness of them in Table 6.It shows that the normal distribution indeed results in performance drop due to the limited search space."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,4.0,Conclusion,"We proposed the TriD, a domain-randomization based multi-source domain generalization method, for medical image segmentation. To solve the limitations existing in preview methods, TriD perturbs the intermediate features with two steps: (1) SR: randomly sampling the augmented statistics from a uniform distribution to expand the search space of feature styles; (2) SM: mixing the feature styles for explicit domain-invariant representation learning. Through extensive experiments on two medical segmentation tasks with different modalities, the proposed TriD is demonstrated to achieve superior performance over the baselines and other state-of-the-art DG methods."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 2 .,"(94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62 which can evaluate our TriD under different data-amount scenarios. Each of these five datasets has a training/test split, and in total, we have 1,102 cases for training and 339 cases for test. Each image is center-cropped and resized to 512 × 512"
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 4 .,"TriD (DeepAll+SR+SM) (94.72, 80.26) (93.95, 82.70) (92.09, 81.92) (90.37, 78.02) (95.64, 86.54) 87.62"
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 6 .,"robustness of model due to the limited search space; (3) the best performance is achieved when SR and SM are jointly used (i.e., our TriD)."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_9.
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,1.0,Introduction,"Magnetic resonance imaging (MRI) plays a pivotal role in knee clinical examinations. It provides a non-invasive and accurate way to visualize various internal knee structures and soft tissues [2]. With its excellent contrast and resolution, MRI can help radiologists detect and diagnose knee injuries or diseases. The choice of MRI sequences and acquisition techniques depends on the specific clinical scenario. While the acquisition of a 3D sequence can render the entire knee as shown in Fig. 1(a), it is not always feasible in clinical practice where scanner resource is limited [13]. In contrast, multi-view 2D scans can offer higher intra-slice resolution, better tissue contrast, and shorter scanning time [7,8], as displayed in Fig. 1(b)-(d). Therefore, multi-view 2D scans are widely used for diagnostics in clinical practice.Although multi-view 2D MRI scanning can provide sufficient image quality for clinical diagnosis in most cases [7,16], it is challenging for estimating the corresponding 3D knee segmentation, which is essential for the functional and morphological analysis [15,17]. Previous studies mostly rely on 3D sequences to conduct image segmentation and morphology-based knee analysis [3,20], which is impractical for clinical data of only 2D scans.The cross-view consistency of multi-view knee MR scans provides the basis for generating 3D segmentation from 2D scans, as shown in Fig. 1(e). Some studies have already utilized the cross-view consistency for medical image applications. For example, Liu et al. [10] proposed a Transformer-based method to fuse multi-view 2D scans for ventricular segmentation, by learning the crossview consistency with the self-attention mechanism. Perslev et al. proposed to extract multi-view 2D slices from 3D images for 3D segmentation on knees [12] and brains [11]. Li et al. [9] transferred widely used sagittal segmentation to coronal and axial views based on cross-view consistency. Zhuang et al. [22] introduced a unified knee graph architecture to fuse the multi-view MRIs in a local manner for knee osteoarthritis diagnosis. These studies have demonstrated the value of cross-view consistency in medical analysis.In this paper, we propose a novel framework, named Cross-view Aligned Segmentation Network (CAS-Net), to generate 3D knee segmentation from clinical multi-view 2D scans. Moreover, following the convention of radiologists, we require the supervising annotation for the sagittal segmentation only. We first align the multi-view knee MRI scans into an isotropic 3D volume by superresolution. Then we sample multi-view patches and construct a knee graph to cover the whole knee joint. Next, we utilize a graph-based network to derive a fine 3D segmentation. We evaluate our proposed CAS-Net on the Osteoarthritis Initiative (OAI) dataset, demonstrating its effectiveness in cross-view 3D segmentation.  "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2.0,Method,"The goal of CAS-Net is to generate 3D knee segmentation with multi-view 2D scans and sagittal segmentation annotation. The overall architecture is shown in Fig. 2, which consists of two major parts.1 Knee Graph Construction. We conduct super-resolution on multi-view 2D scans and align them in a 3D volume. Then, we sample multi-view patches along bone-cartilage interfaces and construct a knee graph. 2 Graph-based Segmentation Network. We deal with the 3D multi-view patches on the knee graph by a 3D UNet [14] combined with Graph Transformer Network (GTN) [4,21]. We also randomly mask out views of patches to enforce cross-view consistency during network learning."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2.1,Knee Graph Construction,"The 2D clinical multi-view scans are acquired from non-orthogonal angles and have large differences in inter-and intra-slice spacing (e.g. 3.5 mm v.s. 0.3 mm in clinical practice). Therefore, we use super-resolution to unify them into one 3D volume with a pseudo segmentation (with tolerable errors), and sample the volume into graph representation.Isotropic Super-Resolution. We first apply super-resolution [19] to the multiview 2D scans, which include sagittal, coronal and axial scans, for representing them in the same space and reducing the large inter-slice spacing. The process is shown in the left part of Fig. 2.Specifically, we follow [19] by projecting multi-view 2D scans into the world coordinate system (WCS) and treating each scan as a continuous function of coordinates. Then, we take advantage of the continuity of coordinates to synthesize an isotropic 3D volume, by querying from the learned function for the coordinates at a equidistantly distributed interval. Since each 2D scan is reconstructed from the same set of WCS coordinates, the multi-view volumes after super-resolution are naturally aligned in the WCS space. Therefore, we concatenate these multi-view volumes as a multi-channel 3D image.Moreover, we train a 2D nnUNet [6] on the sagittal 2D scan and apply it to the sagittal 3D volume slice-by-slice, to acquire a pseudo-3D segmentation. The pseudo-3D segmentation is the basis of knee graph construction. It also provides supervision for the graph-based segmentation network; yet it contains many errors caused by 2D segmentation, e.g., unsmooth segmentation boundaries of bones and cartilages. Knee Graph Sampling. Errors in the pseudo-3D segmentation are distributed around bone surfaces. We sample multi-view patches from these areas and refine their segmentations in 3D. Previous studies have demonstrated the usefulness of organizing knee patches with graph-based representation [22]. Therefore, we follow this approach and construct the knee graph G.We start extracting points on the bone surface by calculating the bone boundaries on the pseudo-3D segmentation. Then, we uniformly sample from these points until the average distance between them is about 0.8 times the patch size. These sampled points are collected as the vertices V in the graph. V stores the center-cropped multi-view patches P and their WCS positional coordinates C. The distribution of the sampled points ensures that multi-view patches centercropped on V can fully cover the bone surfaces and bone-cartilage interfaces. Finally, we connect the vertices in V using edges E, which enables the vertex features to be propagated on the knee graph. The edges are established by connecting each vertex to its k (k = 10, following [22]) nearest neighbors in V ."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2.2,Graph-Based Segmentation Network,"Graph-Based UNet. The knee graph, which is constructed early, is input to a UNet combined with a 3-layer Graph Transformer Network (GTN) [4,21] to generate patch segmentation, as shown on the right part of Fig. 2.To encode the multi-view patches P as vertex features H in the bottleneck of the UNet, we first apply average pooling to H and then add the linearlyprojected WCS coordinate C to enable convolution of H along the graph. Specifically, we compute H (0) = AvgPooling(H) + Linear(C). Next, we pass the resulting vertex features H (0) through a graph convolutional network. Note that the knee graphs have varying numbers of vertices and edges, we utilize the Graph Transformer Networks (GTN) to adapt it.Similar to Transformers [18], the GTN generates three attention embeddings Q, K, and V from H (i) by linear projection in the i-th layer. The difference is that GTN uses the adjacency matrix A to restrict the self-attention computation to between adjacent vertices instead of all of them:where α is the self-attention, denotes dot product, and d is the length of attention embeddings. Then, the local feature H (i) in the i-th layer can be computed as H (i) = σ(αV ), where σ is feedforward operation.After the attention-based graph representation, H (3) is repeated to match the shape of H and then summed up, serving as the input to the UNet decoder. And the UNet decoder derives the 3D patch segmentation according to H (3)  and skip connection. We further project the patch segmentations back into the pseudo-3D segmentation, thereby obtaining a rectified 3D knee segmentation."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Random View Masking.,"To reduce impacts of the errors in the initial pseudo-3D segmentation, we randomly masked out views (channels) in the multi-view (multi-channel) patches during training. The 3D volume, obtained from 2D scans by super-resolution, has a clearer tissue texture in the plane parallel to its original scanning plane. Thus, the random view masking helps to produce more accurate segmentation boundaries by forcing the network to learn from different combinations of patch views (channels). We set the probability of each view to be masked as 0.25. During inference, complete multi-view patches are used, reducing errors caused by the pseudo-3D segmentation."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.1,Data and Experimental Settings,"We have tested the CAS-Net on the Osteoarthritis Initiative (OAI) dataset [5], which contains multi-view 2D scans and 3D volumes aligned in the same WCS space. The 3D DESS modality in the OAI dataset has segmentation annotation [1], which is used as the ground truth to evaluate the 3D segmentation performance in our work. The SAG IW TSE LEFT, COR MPR LEFT, and AX MPR LEFT modalities in the OAI dataset are selected as multi-view 2D scans. For the 3D DESS is unsmooth when it is resampled to 2D scans, we recruit a radiologist to manually annotate the femur, tibia, their attached cartilages, and meniscus in 2D scans. The sagittal annotation is used in our method, and the coronal or axial annotation is set as the ground truth for evaluation. We extract 350 subjects for our experiments, with 210 for training, 70 for validation, and 70 for testing. We report the results of the testing set in the tables below. The 2D annotations are released in https://github.com/zixuzhuang/OAI seg.In the following, we refer to the femur bone and tibia bone as FB and TB for short, their attached cartilages as FC and TC, and meniscus as M. We use GTN and RM to denote the Graph Transformer Network and random masking strategies, respectively. SR and NN stand for super-resolution and nearest neighbor interpolation.To ensure the fairness of the comparison, we use the same training and testing settings for all experiments: PyTorch1.9.0, RTX 3090, learning rate is 3e-4, 100 epochs, cosine learning rate decay, Adam optimizer."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.2,Ablation Study,"To find the optimal setting for the CAS-Net, we apply different hyper-parameters on it and test them on cross-view 2D segmentation. We evaluate the performance of the CAS-Net with different patch size, and then remove the GTN, random masking, and super-resolution to see the effect of each component. To ensure the multi-view slices can be aligned in the same space without super-resolution, we use trilinear interpolation as the alternative. The results are shown in Table 1. As shown in Table 1, the best performance appears when patch size is 128 (86.8% mean Dice ratio in coronal view), and the removing of GTN (86.2%), random masking (85.7%) or super-resolution (82.4%) always lead to performance drop. The patch size of 64 is too small to capture local appearance of the knee, and thus causes performance degradation. GTN enables the network to capture knee features globally and therefore brings performance improvement. Random masking encourages CAS-Net to learn knee segmentation with aligned views across different planes, avoiding errors caused by super-resolution and 3D pseudo segmentation, thus resulting in better performance. Super-resolution reduces the aliasing effect during resampling, resulting in better segmentation. Therefore, we select the patch size as 128, and keep GTN, RM and SR enabled in the next experiments."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.3,Evaluation of 3D Segmentation Performance,"We report the performance of the CAS-Net for 3D knee segmentation. To the best of our knowledge, CAS-Net is the first method that can perform knee segmentation from multi-view 2D scans and sagittal segmentation annotation. Therefore, we compare CAS-Net with the following settings: (1) the sagittal annotation that is resampled to 3D DESS space by nearest neighbor interpolation; (2) the pseudo-3D segmentation generated in Sect. 2.1; (3) the 3D prediction from 3D nnUNet based on 3D DESS images and annotations, which is considered as the upper bound for this task.As can be seen from Table 2, the CAS-Net is capable of producing excellent 3D segmentation results with only sagittal annotation, as demonstrated in   "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.4,Evaluation of Cross-View Projection of 3D Segmentation,"The 3D segmentation results inferred by CAS-Net can be resampled onto unannotated 2D scans to achieve cross-view segmentation. We evaluate the cross-view segmentation performance on the coronal view, which is shown in Table 3. We compare with the three alternative methods: (1) the sagittal annotation that is projected to the coronal view by nearest neighbor interpolation, named NN in Table 3, which is considered as the lower bound for the cross-view segmentation task;(2) the pseudo-segmentation generated in Sect. 2.1 projected to the coronal view, named as Pseudo Seg. in Table 3;   CAS-Net not only eliminates some segmentation errors but also provides smooth segmentation boundaries in cross-view segmentation tasks. As seen in Table 3 (Row 1/2) and Fig. 3(b) and (c), while the super-resolution method has improved the mean Dice ratio by only 0.7%, it has resulted in noticeably smoother segmentation boundary of the knee tissue. The CAS-Net then further eliminates errors in Pseudo Segmentation, resulting in better segmentation outcomes and leading to additional improvements (86.8% mean coronal Dice).Compared with nnUNet, there is still improvement room for CAS-Net in the case of small tissues, such as meniscus (79.9% vs. 92.9% Dice ratio). This is likely caused by subtle movements of subjects during the acquisition of multi-view 2D scans. As shown in Fig. 4, compared to sagittal MRI, FC is slightly shifted upwards in coronal and 3D MRIs. Thus the segmentation is aligned in sagittal view but not in coronal and 3D. We have examined subjects in the dataset that have lower performance (around 50% mean Dice ratio) and confirmed this observation. On the contrary, given correctly aligned multi-view 2D scans, CAS-Net can achieve a mean Dice ratio of 95%."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,4.0,Conclusion,"We have proposed a novel framework named CAS-Net for generating cross-view consistent 3D knee segmentation via super-resolution and graph representation with clinical 2D multi-view scans and sagittal annotations. By creating a detailed 3D knee segmentation from clinical 2D multi-view MRI, our framework provides significant benefits to morphology-based knee analysis, with promising applications in knee disease analysis. We believe that this framework can be extended to other 3D segmentation tasks, and we intend to explore those possibilities. However, it should be noted that the performance of the CAS-Net is affected by misaligned multi-view images, as shown in Fig. 4. After removing the misaligned cases, the femur cartilage dice can be increased from 81.7% to 87.3% in 3D segmentation. We plan to address this issue by incorporating multimodal alignment in future work."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,1.0,Introduction,"Medical image segmentation is an essential task in clinical practice, enabling accurate diagnosis, treatment planning, and disease monitoring. However, existing medical segmentation methods often encounter challenges related to changes in imaging protocols and variations in patient populations. These challenges can significantly impact the performance and generalizability of segmentation models. For instance, a segmentation model trained on MRI images from a specific S. Bera and V. Ummadi-These authors contributed equally to this work and share the first authorship. patient population may not perform well when applied to a different population with distinct demographic and clinical characteristics. Similarly, variations in imaging protocols, such as the use of different contrast agents or imaging parameters, can also affect the model's accuracy and reliability. To ensure accurate segmentation, it is necessary to retrain or fine-tune the model with current data before deploying it. However, this process often leads to catastrophic forgetting, where the model loses previously acquired knowledge while being trained on the current data. Catastrophic forgetting occurs due to the neural network's inability to learn from a continuous stream of data without disregarding previously learned information. Retraining the network using the complete training set, including both old and current data, is not always feasible or practical due to reasons such as the unavailability of old data or data privacy concerns. Moreover, training the network from scratch every time for every perturbation is a resource-intensive and time-sensitive process. Continual learning aims to address this limitation of catastrophic forgetting by enabling the model to learn continuously from a stream of incoming data without the need to retrain the model from scratch. Continual learning algorithms have gained significant interest lately for computer vision tasks like image denoising, superresolution, and image classification. However, the development of efficient continual learning algorithms specifically designed for medical image segmentation has been largely overlooked in the literature. To address the above gap, our study proposes a continual learning based approach for medical image segmentation, which can be used to train any backbone network. In our approach, we leverage the recently proposed concept of the memory replay-based continual learning (MBCL) [6,15,17]. In MBCL, a memory buffer is used to store and replay previously learned data, enabling the model to retain important information while learning from new data. MBCL is, however, hampered by a few bottlenecks associated with medical images that pose a serious obstacle to its proper use in medical image segmentation. The efficiency of MBCL largely depends on the images stored in the memory bank [4,18], as the stored images must faithfully represent the previous task. It is known that medical image segmentation faces a major challenge of class imbalance. If an image with an under-representation of the positive class is stored in the memory bank, then it impedes the network from effectively remembering the previous task. In addition, not all medical images for training contribute equally to the learning process. So, images that pose greater challenges for the segmentation network should be saved in the memory bank. The importance of identifying atypical examples before creating memory banks cannot be overstated.We propose a simple yet effective algorithm for image selection while creating the memory bank. Two different ranking mechanisms, which address the bottlenecks related to medical images discussed above, are proposed to rank all the images present in the training set. Then, images to be stored in the memory bank are selected using a combined ranking. Further, we suggest the cropping of the images around the organ of interest in order to minimize the size of the memory bank. An extensive evaluation is performed on three different problems, i.e., continual prostrate segmentation, continual hippocampus segmentation, and task incremental segmentation of the prostate, hippocampus and spleen. We consider several baselines including EWC [12], L2 regularization-based [10], and representation learning-based [16]. Our method is found to outperform the conventional MBCL, and all the baseline mentioned above by a significant margin, creating a new benchmark for continual learning-based medical image segmentation."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.0,Proposed Methodology,"We are given a sequential stream of images from K sites, which are sequentially used to train a segmentation model. In a round k ∈ [1, K] of this continual learning procedure, we can only obtain images and ground truths {(x k,i , y k,i )} n k i=1 from a new incoming site (dataset) D k without access to old data from previous sites D k-1 . Due to catastrophic forgetting, this type of sequential learning results in a drop in performance for all the previous sites (≤ D k-1 ) after training with images from D k site as the parameters of the previous site or task are overwritten while learning a new task. In naive memory replay-based continual learning, a memory buffer, M, is used to store a small number of examples of past sites (≤ D k-1 ), which can be used to train the model along with the new data. Unlike other tasks like image classification or image restoration, for downstream tasks like medical image segmentation, the selection of images for storing in the M is very crucial. A medical image segmentation (like hippocampus segmentation) approach typically has a very small target organ. It is very likely that randomly selected images for storage in the M will have an under-representation of the positive (hippocampus) class. Further, the contribution of each training sample is not equal towards the learning, as a network usually learns more from examples that are challenging to segment. Based on the above observations, we propose two image ranking schemes to sort the images for storing in M (see Fig. 1)."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.1,Positive Class Based Ranking (PCR),"In this ranking scheme, we rank an input image volume according to the percentage of voxels corresponding to the positive class available in the volume.Let cr k,i be the positive class based ranking score for the sample input-ground truth pair (x k,i , y k,i ) from the dataset D k . We use the ground truth label y k,i to calculate the score of each volume. Let H, W and D respectively be the height, width and number of slices in the 3D volume of y k,i . The voxel value at location (h, w, d) in the ground truth label y k,i is represented by y k,i h,w,d ∈ 0, 1. If the voxel value at a location (h, w, d) is equal to 1, then the voxel belongs to the positive class. Let us use |y k,i | to represent the total number of voxels in the 3D volume. For a sample pair (x k,i , y k,i ), cr k,i is computed as follows:The rationale behind this ranking scheme is that by selecting volumes with a higher positive class occupancy, we can minimize the risk of underrepresentation of the positive class leading to a continuously trained network that remembers previous tasks more faithfully."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.2,Gradient Based Ranking (GBR),"Here, we intend to identify the examples which the segmentation network finds hard to segment. In this ranking, we leverage the relationship between example difficulty and gradient variance, which has been previously observed in other studies [2]. Specifically, the neural network tends to encounter high gradients on examples that are hard to learn. With this motivation, we devise a simple method for calculating the score of every sample based on gradients, outlined in Algorithm 1. This algorithm enables the calculation of the gradient-based score during the network's training in real-time. In the algorithm, f (θ) refers to the image segmentation network with parameter θ and end for 13: end for gradients for the current sample are represented by cg, and gr k,i accumulates the absolute gradient difference between cg and previous gradients pg k,i for all training epochs. Essentially, a high value of gr k,i signifies a large gradient variance and implies that the example is difficult."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.3,Gradient Plus Class Score (GPCS) Sampling for Memory,"Once the score gr k,i and cr k,i are available for a dataset D k , they are normalized to [0, 1] range and stored for future use. If p samples are to be selected for the replay memory M, then p 2 will be selected using gr k,i and other p 2 using cr k,i . However, we do not store entire image volumes in the memory. We propose a straightforward yet effective strategy to optimally utilize the memory bank size for continual medical segmentation tasks. Typically, the organ of interest in these tasks occupies only a small portion of the entire image, resulting in significant memory wastage when storing the complete volume. For example, the hippocampus, which is a common region of interest in medical image segmentation, occupies less than 0.5% of the total area. We propose to store only a volumetric crop of the sequence where the region of interest is present rather than the complete volume. This enables us to make more efficient use of the memory, allowing significant amounts of memory for storing additional crops within a given memory capacity. Thus, we can reduce memory wastage and optimize memory usage for medical image segmentation.Consider that an image-label pair (x k,i , y k,i ) from a dataset D k has dimensions H ×W ×D (height×width×sequence length). Instead of a complete imagelabel pair, a crop-sequence of dimension h c ×w c ×d c with h c ≤ H, w c ≤ W, d c ≤ D is stored. We also use an additional hyperparameter, foreground background ratio fbr ∈ [0, 1] to store some background areas, as described below fbr ="
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Number of crops having RoI Number of crops not having RoI,"(2)Using only foreground regions in problems such as task incremental learning may result in a high degree of false positive segmentation. In such cases, having a few crops of background regions helps in reducing the forgetting in background regions as discussed in Sect. 3.3."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.1,Datasets,"We conduct experiments to evaluate the effectiveness of our methods in two different types of incremental learning tasks. To this end, we use seven openly available datasets for binary segmentation tasks: four prostate datasets (Prostate158 [1], NCI-ISBI [3], Promise12 [14], and Decathlon [5]), two hippocampus datasets (Drayd [8] and HarP [7]), and one Spleen dataset from Decathlon [5]. The first set of experiments involved domain incremental prostate segmentation, with the datasets being trained in the following order: Prostate158 → ISBI → Promise12 → Decathlon. The second experiment involved domain incremental hippocampus segmentation, with the datasets being trained in the order HarP → Drayd. Finally, we conducted task incremental segmentation for three organs -prostate, spleen, and hippocampus -following the sequence: Promise12 (prostate) → MSD (spleen) → Drayd (hippocampus)."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.2,Evaluation Metrics,"To evaluate the effectiveness of our proposed methods against baselines, we use the segmentation evaluation metric Dice Similarity Coefficient (DSC) along with standard continual learning (CL) metrics. The CL metrics [9] comprise Average Accuracy (ACC), Backward Transfer (BWT) [13], and Average Forgetting (AFGT) [19]. BWT measures the model's ability to apply newly learned knowledge to previously learned tasks, While AFGT measures the model's retention of previously learned knowledge after learning a new task."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.3,Training Details,"In this work, we consider a simple segmentation backbone network UNet, which is widely used in medical image segmentation. Both the proposed and baseline methods were used to train a Residual UNet [11]. All the methods for comparison (Tables 2, 3 and 4), except for Sequential (SGD), are trained using the Adam optimizer with a learning rate of 0.001, a momentum of 0.9, and a weight decay of 0.00001. Sequential(SGD) employs an SGD optimizer with the same hyperparameters as Adam. The training loss used is DiceCELoss, which combines Dice loss and Cross Entropy loss. During task incremental segmentation training using GPCC, the fbr parameter is set to 0.7, while the default value of 1.0 is used for other tasks. In parts of our GPCC experiments, where we examine continual prostate segmentation as well as continual prostate, spleen, and hippocampus segmentation, both the height (h c ) and width (w c ) of each volume are fixed to 160. In continual hippocampus segmentation experiments, these values are reduced to 128. Note that although we perform the experiments using UNet network, any sophisticated network like VNet, or DeepMedic can also be trained continually using our method."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,4.0,Results and Discussion,"Ablation Study of Our Atypical Sample Selection: 1 In order to evaluate the effectiveness of every module proposed in our work, an ablation study is conducted. The results of the analysis shown in Table 1 indicates that randomly storing samples leads to a significant decrease in performance during in the earlier trained domains due to insufficient representation of the domain distributions. Both PCR and GBR shows improvements in ACC over random replay Next, the objective comparison of different methods of continual hippocampus segmentation is shown in Table 3. This task poses a significant challenge due to the small region of interest (RoI) in whole brain MRI scans. Our proposed approach outperforms all other baseline methods in terms of CL metrics. When comparing GPCC Replay with six crops of 128 × 128 × D images to Random Replay(3) using full-size images of 233 × 189 × D, we find that GPCC Replay is more memory-efficient consuming 26% less memory while still achieving an 1.1% ACC performance improvement. While some baselines showed higher DSC scores on the second domain, a high value of AFGT indicates their inability to  retain previously learned knowledge, which suggests limitations in their ability to perform continual training of the backbone. We finally assess the performance of our method on an even more challenging task of incremental learning segmentation. This involves continuously training a single model to accurately segment various organs while incorporating new organs as segmentation targets during each episode. Utilizing a single model for segmenting multiple organs offers potential advantages, particularly when there are constraints such as limited annotation data that hinder joint training. Task or class incremental learning becomes invaluable in such scenarios, as it allows us to incorporate new organs as segmentation targets without requiring a complete retraining process. To investigate the feasibility of this concept, we dedicate a section in our study to experimental analysis. The results of this analysis are presented in Table 4. We find that GPCC replay (12) shows substantial improvement by increasing the ACC score by 45.4% over Sequential(SGD) and achieves a 5% increase in ACC compared to Random Replay(3), outperforming all the baselines. In this case, GPCC Replay is also lighter in memory consumption by up to 32%.Visually analyzing the predictions for a test sample in Fig. 2 from the task of incremental segmentation, L2 regression and Random Replay(3) are seen to produce only partial segmentation of RoI. On the other hand, GPCC predictions outperform joint learning and are very close to Ground Truths, with a DSC score ≥ 90%. More visual comparison among different methods is given in the supplementary."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,5.0,Conclusion,This paper proposes a novel approach to address the challenge of catastrophic forgetting in medical image segmentation using continual learning. The paper presents a memory replay-based continual learning paradigm that enables the model to learn continuously from a stream of incoming data without the need to retrain from scratch. The proposed algorithm includes an effective image selection method that ranks and selects images based on their contribution to the learning process and faithful representation of the task. The study evaluates the proposed algorithm on three different problems and demonstrates significant performance improvements compared to several relevant baselines.
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 1 .,"2 , our method with GPCC(6) shows an ACC improvement of 9.2% and 4.4% respectively. With a memory footprint, GPCC with six crops of 160 × 160 × D is 65% lighter compared to Random Replay (3) with an average image size of 384 × 384 × D."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 3 .,"8 ± 0.6 -2.1 ± 0.5 2.1 ± 0.5 Fig.2. Qualitative results for task incremental segmentation of prostate, spleen, and hippocampus using the methods in Table4. Ground truths are in yellow borders and predictions are in peach. The bolded method has the highest DSC score."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 49.
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,1.0,Introduction,"Benefiting from the prominent ability to model long-range dependency, transformers have become the de-facto standard for natural language processing [1]. Compared with convolutional neural networks (CNNs), which encourage locality, weight sharing, and translation equivariance, transformers build global dependency through self-attention layers, bringing more possibilities for feature exaction and breaking the performance ceiling of CNNs in return [2][3][4][5][6].Inspired by this, transformers are introduced into medical image segmentation and arouse wide concerns [7][8][9][10][11]. In vision transformers, each medical image is first split into a series of patches and then projected into a 1D sequence of patch embeddings [4]. Through building pairwise interaction among patches/tokens, transformers are supposed to aggregate global information for robust feature exaction. However, learning well-convergence global dependency in transformers is highly data-intensive, making transformers less effective given relatively limited medical imaging data. To figure out how transformers work in medical image segmentation, we trained four state-of-the-art transformer-based models [5,[12][13][14] on the ACDC dataset and visualized the learned self-attention matrices across different layers as illustrated in Fig. 1. For all approaches, the attention matrices tend to become uniform among patches (i.e., attention collapse [15]), especially in deeper layers. Attention collapse is more noticeable, especially in CNN-Transformer hybrid approaches (i.e., TransUNet, TransFuse, and FAT-Net). On the one hand, insufficient training data would make transformers learn sub-optimal long-range dependency. On the other hand, directly combining CNNs with transformers would make the network biased to the learning of CNNs, as the convergence of CNNs is more achievable compared to transformers, especially on small-scale training data. Therefore, how to address attention collapse and improve the convergence of transformers is crucial for performance improvement.In this work, we propose a plug-and-play module named ConvFormer to address attention collapse by constructing a kernel-scalable CNN-style transformer. In ConvFormer, 2D images can directly build sufficient long-range dependency without being split into 1D sequences. Specifically, corresponding to tok-enization, self-attention, and feed-forward network in vanilla vision transformers, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) respectively. For an input image/feature map, its resolution is first reduced by applying convolution and max-pooling alternately. Then, CSA builds appropriate dependency for each pixel by adaptively generating a scalable convolutional, being smaller to include locality or being larger for long-range global interaction. Finally, CFFN refines the features of each pixel by applying continuous convolutions. Extensive experiments on three datasets across five state-of-the-art transformer-based methods validate the effectiveness of ConvFormer, outperforming existing solutions to attention collapse."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,2.0,Related Work,"Recent transformer-based approaches for medical image analysis mainly focus on introducing transformers for robust features exaction in the encoder, crossscale feature interactive in skip connection, and multifarious feature fusion in the decoder [16][17][18][19]. The study about addressing attention collapse for transformers in medical imaging is under-explored. Even in natural image processing, attention collapse, usually existing in the very deep layers of deep transformerbased models, has not been fully studied. Specifically, Zhou et al. [15] developed Re-attention to re-generate self-attention matrices aiming at increasing their diversity on different layers. Zhou et al. [20] projected self-attention matrices into a high-dimensional space and applied convolutions to promote the locality and diversity of self-attention matrices. Touvron et al. [21] proposed to re-weight the channels of the outputs from the self-attention module and the feed-forward module to facilitate the convergence of transformers."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.0,Method,"The comparison between the vision transformer (ViT) and ConvFormer is illustrated in Fig. 2. The greatest difference is that our ConvFormer is conducted on 2D inputs while ViT is applied to 1D sequences. Specifically, the pooling module is utilized to replace tokenization in ViT, which well preserves locality and positional information without extra positional embeddings. The CNN-style selfattention (CSA) module, i.e. the core of ConvFormer, is developed to replace the self-attention (SA) module in ViT to build long-range dependency by constructing self-attention matrices in a similar way like convolutions with adaptive and scalable kernels. The convolutional feed-forward network (CFFN) is developed to refine the features for each pixel corresponding to the feed-forward network (FFN) in ViT. No upsampling procedure is adopted to resize the output of Con-vFormer back to the input size as the pooling module can match the output size by adjusting the maxpooling times. It should be noticed that ConvFormer is realized based on convolutions, which eliminates the training tension between CNNs and transformers as analyzed in Sect. 1. Each module of ConvFormer is described in the following. "
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.1,Pooling vs. Tokenization,"The pooling module is developed to realize the functions of tokenization (i.e., making the input suitable to transformers in the channel dimension and shaping and reducing the input size when needed) while without losing details in the grid lines in tokenization. For an input X in ∈ R c×H×W , convolution with a kernel size of 3 × 3 followed by batch normalization and Relu, is first applied to capture local features. Then, corresponding to each patch size S in ViT, total d = log 2 S downsampling operations are applied in the pooling module to produce the same resolutions. Here, each downsampling operation consists of a max-pooling with a kernel size of 2 × 2 and a combination of 3 × 3 convolution, batch normalization, and Relu. Finally,through the pooling module where c m is corresponding to the embedding dimension in ViT."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.2,CNN-Style vs. Sequenced Self-attention,"The building of long-range dependency in ConvFormer is relying on CNN-style self-attention, which creates an adaptive receptive field for each pixel by constructing a customized convolution kernel. Specifically, for each pixel x i, j of X 1 , the convolution kernel A i, j is constructed based on two intermediate variables:where E q and E k ∈ R c q ×c m ×3×3 are the learnable projection matrices and c q is corresponding to the embedding dimension of Q, K, and V in ViT, which incorporates the features of adjacent pixels in 3 × 3 neighborhood into x i, j . Then, the initial customized convolutional kernel2 d for x i, j is calculated by computing the cosine similarity:Here,m,n corresponds to attention score calculation in ViT (constrained to be positive while I i, j m,n can be either positive or negative). Then, we dynamically determine the size of the customized convolution kernel for x i, j by introducing a learnable Gaussian distance map M:where θ ∈ (0, 1) is a learnable network parameter to control the receptive field of A and α is a hyper-parameter to control the tendency of the receptive field. θ is proportional to the receptive field. For instance, under the typical setting H = W = 256, d = 3, and α = 1, when θ = 0.003, the receptive field only covers five adjacent pixels, when θ > 0.2, the receptive field is global. The larger α is, the more likely A tends to have a global receptive field. Based on I i, j and M i, j , A i, j is calculated by A i, j = I i, j × M i, j . In this way, every pixel x i, j has a customized size-scalable convolution kernel A i, j . By multiplying A with V, CSA can build adaptive long-range dependency, where V can be formulated similarly according to Eq. ( 1). Finally, the combination of 1 × 1 convolution, batch normalization, and Relu is utilized to integrate features learned from long-range dependency."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.3,Convolution Vs. Vanilla Feed-Forward Network,"The convolution feed-forward network (CFFN) is to refine the features produced by CSA, just consisting of two combinations of Approaches outperforming the state-of-the-art 2D approaches on the publiclyavailable ACDC (i.e., FAT-Net [14]: 91.46% in Avg. DSC) and ISIC (i.e., Ms Red [28]: 90.25% in Avg. DSC) datasets respectively. More comprehensive quantitative comparison results can be found in the supplemental materials. myocardium (MYO), and right ventricle (RV) are available [22]. Following [12,17,18], 70, 10, and 20 cases are used for training, validation, and testing respectively. ISIC 20182 . A publicly-available dataset for skin lesion segmentation. Totally 2594 dermoscopic lesion images with pixel-level annotations are available [23,24]. Following [25,26], the dataset is randomly divided into 2076 images for training and 520 images for testing.ICH. A locally-collected dataset for hematoma segmentation. Totally 99 CT scans consisting of 2648 slices were collected and annotated by three radiologists. The dataset is randomly divided into the training, validation, and testing sets according to a ratio of 7:1:2. Implementation Details. For a fair comparison, all the selected state-of-theart transformer-based baselines were trained with or without ConvFormer under the same settings. All models were trained by an Adam optimizer with a learning rate of 0.0001 and a batch size of 4 for 400 rounds. Data augmentation includes random rotation, scaling, contrast augmentation, and gamma augmentation."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,4.2,Results,"ConvFormer can work as a plug-and-play module and replace the vanilla transformer blocks in transformer-based baselines. To evaluate the effectiveness of ConvFormer, five state-of-the-art transformer-based approaches are selected as backbones, including SETR [5], TransUNet [12], TransFuse [13], FAT-Net [14], and Patcher [27]. SETR and Patcher utilize pure-transformer encoders, while TransUNet, TransFuse, and FAT-Net adopt CNN-Transformer hybrid encoders. In addition, three state-of-the-art methods for addressing attention collapse, including Re-attention [15], LayerScale [21], and Refiner [20], are equipped with the above transformer-based baselines for comparison. Quantitative Results. Quantitative results of ConvFormer embedded into various transformer-based baselines on the three datasets are summarized in Table 1. ConvFormer achieves consistent performance improvements on all five backbones. Compared to CNN-Transformer hybrid approaches (i.e., TransUNet, TransFuse, and FAT-Net), ConvFormer is more beneficial on pure-transformer approaches (i.e., SETR and Patcher). Specifically, with ConvFormer, SETR achieves an average increase of 3.86%, 1.38%, and 1.39% in Dice on the ACDC, ISIC, and ICH datasets respectively, while the corresponding performance improvements of Patcher are 0.66%, 1.07%, and 1.15% respectively. Comparatively, in CNN-Transformer hybrid approaches, as analyzed above, CNNs would be more dominating against transformers during training. Despite this, re-balancing CNNs and Transformers through ConvFormer can build better longrange dependency for consistent performance improvement. Comparison with SOTA Approaches. Quantitative results compared with the state-of-the-art approaches to addressing attention collapse are summarized in Table 1. In general, given relatively limited training data, existing approaches designed for natural image processing are unsuitable for medical image segmentation, resulting in unstable performance across different backbones and datasets. Comparatively, ConvFormer consistently outperforms these approaches and brings stable performance improvements to various backbones across datasets, demonstrating the excellent generalizability of ConvFormer as a plug-and-play module. Visualization of Self-Attention Matrices. To qualitatively evaluate the effectiveness of ConvFormer in addressing attention collapse and building efficient long-range dependency, we visualize the self-attention matrices with and  without ConvFormer as illustrated in Fig. 3. By introducing ConvFormer, attention collapse is effectively alleviated. Compare to the self-attention matrices of baselines, the matrices learned by ConvFormer are more diverse. Specifically, the interactive range for each pixel is scalable, being small for locality preserving or being large for global receptive fields. Besides, dependency is no longer constrained to be positive like ViT, which is more consistent with convolution kernels. Qualitative segmentation results of different approaches on the three datasets can be found in the supplemental materials. Ablation Study As described in Sec. 3.2, α is to control the receptive field tendency in ConvFormer, The larger the α, the more likely ConvFormer contains larger receptive fields. To validate this, we conduct an ablation study on α as summarized in Table 2. In general, using a large α does not necessarily lead to more performance improvements, which is consistent with our observation that not every pixel needs global information for segmentation."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,5.0,Conclusions,"In this paper, we construct the transformer as a kernel-scalable convolution to address the attention collapse and build diverse long-range dependencies for efficient medical image segmentation. Specifically, it consists of pooling, CNNstyle self-attention (CSA), and convolution feed-forward network (CFFN). The pooling module is first applied to extract the locality details while reducing the computational costs of the following CSA module by downsampling the inputs. Then, CSA is developed to build adaptive long-range dependency by constructing CSA as a kernel-scalable convolution, Finally, CFFN is used to refine the features of each pixel. Experimental results on five state-of-the-art baselines across three datasets demonstrate the prominent performance of ConvFormer, stably exceeding the baselines and comparison methods across three datasets."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Table 1 .,"1 . A publicly-available dataset for the automated cardiac diagnosis challenge. Totally 100 scans with pixel-wise annotations of left ventricle (LV),"
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Table 2 .,Dice (%) 90.71 91.00 90.76 90.66 90.45
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_61.
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,1.0,Introduction,"Deep learning can significantly reduce the workload of medical specialists during image segmentation tasks, which are essential for patient diagnosis and follow-up management [8,14,16]. For most tasks, segmentation masks have the same dimensionality as the input. However, there are some tasks for which segmentation has to be performed in a subset of the dimensions of the data, e.g. 3D→2D [13,21]. This occurs, for example, for the segmentation of geographic atrophy (GA) in optical coherence tomography (OCT), where the segmentation is performed on the OCT projection. In recent years, several methods have been proposed for this type of tasks [9,[11][12][13]. Li et al. [11] proposed an image projection network (IPN) that reduces the features to the target dimensionality using unidirectional pooling layers in the encoder. However, IPN follows a patch-based approach with fixed patch size, which prevents its direct application to full 3D volumes of varying size. Also, it does not have skip connections, which have proven to be highly useful for accurate segmentation. Later, Lachinov et al. [9] proposed a U-Net-like convolutional neural network (CNN) for 3D→2D segmentation that overcomes the limitations of IPN, which were also later overcome by the second version of IPN (IPNv2) [12]. However, they still require a large amount of labeled data to provide adequate performance. In addition, there are works that explore the use of CNNs for 3D→2D regression, where Seeböck et al. [20] proposed ReSensNet, a novel CNN based on Residual 3D U-Net [10], with a 3D encoder and a 2D decoder connected by 3D→2D blocks. However, ReSensNet only works at concrete input resolutions, and it is applied pixel-wise.In general, one of the issues of these and other deep learning segmentation methods is that their performance strongly depends on the amount of annotated data [22], which hinders their deployment to real-world medical image analysis settings. Transfer learning from ImageNet is the standard approach to mitigate this issue [22]. However, specifically for segmentation, ImageNet pre-training has shown minimal performance gains [5,17], partially because it can only be performed on the encoder part of the very common encoder-decoder architectures.A possible alternative is to pre-train the models using a self-supervised learning (SSL) paradigm [1,2,4,6,7,15,18]. However, only some of these approaches have the potential to be applied for 3D→2D segmentation, as many of them, such as image denoising [2], require input and output images to have the same dimensionality. Among the suitable approaches, multi-modal reconstruction pretraining (MMRP) shows great potential in multi-modal scenarios [7]. In this approach, models are trained to reconstruct pairs of images from different modalities, learning relevant patterns in the data without requiring manual annotations. MMRP, however, has only been proven useful for localizing non-pathological structures on 2D color fundus photography, using fluorescein angiography as the modality to reconstruct. Moreover, image pairs of these modalities have to be registered using a separate method.Contributions. In this work, we propose a novel approach for label-efficient 3D→2D segmentation. In particular, our contributions are as follows: (1) As an alternative to state-of-the-art network architectures, we propose a 3D→2D segmentation CNN based on ReSensNet [20] that has a 3D encoder and a 2D decoder connected by novel 3D→2D projective blocks. (2) We propose a novel SSL strategy for 3D→2D models based on the reconstruction of modalities of different dimensionality, and show that it significantly improves the performance of the models in the target segmentation tasks. This is the first data efficient method proposed for 3D→2D models and the first work exploring 3D→2D reconstruction. (3) Lastly, the performed experiments deepen the understanding of the proposed SSL paradigm, by exploring different settings with different image modalities. The proposed approach was validated on two clinically-relevant tasks: the en-face segmentation of GA and reticular pseudodrusen (RPD) in retinal OCT. The results demonstrate that the proposed approach clearly outperforms the state of the art in scenarios with scarce labeled data. Our code is publicly available on GitHub1 .Clinical Background. Geographic atrophy (GA) is an advanced form of agerelated macular degeneration (AMD) that corresponds to a progressive loss of retinal photoreceptors and leads to irreversible visual impairment. GA is typically assessed with OCT and/or fundus autofluorescence (FAF) imaging modalities [3,24]. In OCT, it is characterized by the loss of retinal pigment epithelium (RPE) tissue, accompanied by the contrast enhancement of the signal below the retina, and in FAF, by the loss of RPE autofluorescence [19] (see Fig. 1). In both cases, GA lesion is delineated as a 2D en-face area. Also, GA frequently appears brighter than the surrounding areas on scanning laser ophthalmoscopy (SLO) images due to its higher reflectance.Reticular pseudodrusen (RPD) are accumulations of extracellular material that commonly occur in association with AMD. In OCT scans, these lesions are shown as granular hyperreflective deposits situated between the RPE layer and the ellipsoid zone. SLO visualizes RPD as a reticular pattern of iso-reflective round lesions surrounded by a hyporeflective border (see Fig. 1)."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,2.0,Methods and Experimental Setup,"The proposed approach, illustrated in Fig. 2, is as follows. Let x ∈ X ⊂ R n and y ∈ Y ⊂ R n-1 be two images from modalities X and Y, and z ∈ Z ⊂ R n-1 , their corresponding target segmentation mask. Let images and masks from Y and Z have related anatomical features. We optimize a reconstruction model y = f r (x; θ r ) and transfer its knowledge by initializing the weights of the segmentation model z = f s (x; θ s ) with the optimized weights of the reconstruction model f r . With this approach, modality Y serves as a free source of supervision, and images of this modality serve as soft segmentation targets. Thus, models can learn relevant patterns in a self-supervised way. In this work, we propose a new CNN for the special case of 3D→2D segmentation, and we evaluate the proposed SSL approach for this case. In particular, we pre-train the new CNN to reconstruct SLO/FAF images from OCT (3D→2D), and then fine-tune it for GA/RPD segmentation. The advantage of reconstructing SLO over FAF is that several modern OCT devices allow to obtain co-registered OCT and SLO scans, providing the coordinates of each OCT slice within the SLO; thus, there is no need to use a separate registration method.Network Architecture. The proposed network architecture (Fig. 3) is based on ReSensNet [20], and consists of a 3D encoder and a 2D decoder connected by novel 3D→2D feature projection blocks (FPBs). In the original work [20], training and inference are performed pixel-wise using fixed-size input patches. In contrast, we use full-size volumes of arbitrary resolution. To this end, we propose a novel type of FPB. In particular, all convolutions whose kernel size was equal to the expected feature size (calculated from the fixed size of the input patch) were replaced by 1×1×4 convolutions. Then, to project 3D features at the output of each FPB to the 2D feature space, we add an adaptive average pooling of size 1 in the depth dimension at the end of each block. With this setting, feature selection and dimension reduction are performed at different scales, and the decoder processes only 2D features in the selected dimensions. This allows the model to learn the 3D structure of the data while being able to perform the segmentation in 2D. In addition, to overcome memory constraints and avoid overfitting, we reduce by half the number of kernels in each convolutional block.  Training Losses. As reconstruction loss, we use negative mean structural similarity index (NMSSIM) [23]. We empirically found that this loss performs equally or better than modern perceptual losses (e.g. LPIPS [25]) for our approach. NMSSIM loss can be defined as L NMSSIM (x, y) = -1 HW h,w SSIM(x hw , y hw ), where x hw and y hw denote image patches of images x and y centered on the pixel with coordinates (h, w), h ∈ H and w ∈ W , and SSIM(x hw , y hw ) is the SSIM map for those patches, as described in [23]. As segmentation loss, we use the direct sum of Dice loss and Binary Cross-Entropy. These two losses are standard for binary segmentation tasks [8,9,11,12,16].Datasets. Experiments were performed using three datasets (Table 1). GA-M samples come from a clinical study on GA progression. OCT and SLO images were automatically co-registered by the imaging device, while FAF images were registered with SLO using an in-house pipeline based on aligning retinal vessel segmentation. FAF and SLO images were cropped and resized to the same area and resolution as the OCT en-face projection. GA-M-S (35 samples) is a subset of GA-M with GA en-face masks annotated by a retinal expert on the OCT B-Scans. GA-S is composed of OCT volumes from another study with en-face GA annotations created by a retinal expert on the OCT B-scans. This dataset is divided patient-wise into two subsets: GA-S-2, containing volumes with annotations of two different experts, and GA-S-1, of only one. RPD-S is composed of OCT volumes with en-face RPD annotations created by retinal experts.Training and Evaluation Details. OCT volumes were flattened along the Bruch's membrane, rescaled depth-wise to 128 voxels, and then Z-score normalized along the cross-sectional plane. To make FAF and GA masks more similar and thus facilitate fine-tuning, FAF images were inverted. In all cases, models were trained for 800 epochs using SGD with a learning rate of 0.1 and a momentum of 0.9. Batch size was set to 4 for reconstruction and 8 for segmentation.All datasets were split patient-wise into training (60%), validation (10%) and test (30%). For reconstruction, models were trained on GA-M. For GA segmentation, they were trained/fine-tuned on GA-S-1 and evaluated on GA-S-1, GA-S-2 and GA-M-S. For RPD segmentation, RPD-S was used. To evaluate the performance under label scarcity, we train with 5%, 10%, 20% and 100% of the data in GA-S, and 20% and 100%, in RPD-S. More details about the hardware used and the carbon footprint of our method are included in the Supplement.To reduce inference variability, we average the predictions of the top-5 checkpoints of the models in terms of Dice (validation). Segmentations are evaluated via Dice and absolute area difference (Area diff.) of predicted and manual masks."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,3.0,Results and Discussion,"Baseline Comparison. We compared our approach to current state-of-the-art methods (IPN [11], IPNv2 [12], Lachinov et al. [9], and ReSensNet [20]), showing that we greatly improve the state of the art in GA segmentation in scenarios with limited labeled data (Fig. 4). When using only 5% of the data (40 samples), the mean Dice score was 23% higher than the best state-of-the-art approach. Even without SSL, the proposed CNN improves the Dice score by 8%. This gain is even greater in terms of Area diff. The improvement is also visible in the predicted segmentation masks (Fig. 5). When using our approach, the number of false positives and negatives is highly reduced. On the other hand, the improvement for RPD segmentation is more modest (in this case, we only compared with the current state-of-the-art-method: Lachinov et al. [9]). This can be explained by the greater visibility of GA features compared to RPD features in FAF and SLO (see Figs. 1 and5). This suggests that SSL benefits from images with similar pathomorphological manifestations. A table with all means and standard deviations, as well as the results of a Wilcoxon signed rank test between our proposal and the others is included in the Supplement. SSL Effect. To further assess the effect of the SSL, we also applied the strategy to the CNN by Lachinov et. al [9]. Figure 4 shows that SSL clearly improves the GA and RPD segmentation performance of both proposed and Lachinov et al. methods. These results are in line with the qualitative results in Fig. 5. This demonstrates that the SSL strategy is beneficial regardless of the architecture and the data. Notwithstanding, as discussed in the baseline comparison, the proposed SSL is more beneficial for GA segmentation than for RPD.Reconstructed Modality Effect. We also conducted experiments to assess the effect of the reconstructed modality (SLO and FAF). Figure 4 shows that using FAF for SSL usually leads to better segmentation performance than using SLO. However, in multiple cases, the differences were not statistically significant. This is important because, unlike FAF, SLO does not require an external registration method."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,4.0,Conclusions,"Labeled data scarcity is one of the main limiting factors for the application of deep learning in medical imaging. In this work, we have proposed a new model and SSL strategy for label-efficient 3D→2D segmentation. The proposed approach was validated in two tasks with clinical relevance: the en-face segmentation of GA and RPD in OCT. The results demonstrate that: (1) the proposed CNN architecture clearly outperforms the state of the art when there is limited annotated data, (2) regardless of the architecture and the modality to be reconstructed, the proposed SSL strategy improves the performance of the models on the target tasks in those cases; (3) despite the greater diagnostic utility of FAF over SLO, SSL with FAF does not always result in a significant gain in model performance, with the advantage of the latter not requiring a supplementary registration method. On the other hand, although the proposed approach shows promising results in the en-face segmentation of RPD, further evaluation is needed.Based on our findings, we believe that the proposed approach has the potential to be used in other common 3D→2D tasks, such as the prediction of retinal sensitivity in OCT, the segmentation of different structures in OCT-A, or the segmentation of intravascular ultrasound (IVUS). In addition, we also believe that the proposed SSL strategy could be easily extended to other imaging domains, such as magnetic resonance, where multi-modal data is widely used."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_56.
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,1.0,Introduction,"Hand imaging examination is a standard clinical procedure commonly utilized for various medical purposes such as predicting biological bone age [16] and diagnosing finger bone and joint diseases [4,8]. Ultrasound (US) is a promising alternative imaging modality for clinical examinations due to its radiationfree and cost-effective nature, especially the three-dimensional (3D) US volume, which is increasingly preferred for its intuitive visualization and comprehensive clinical information. However, the current US imaging technology is limited by low signal-to-noise ratio and inherent imaging artifacts, making the examination of hand with complex and delicate anatomical structure highly dependent on high-level expertise and experience.To address this challenge, deep learning-based US image segmentation methods have been explored. For instance, Liu et al. [10] propose an attention-based network to segment seven key structures in the neonatal hip bone. Rahman et al. [14] present a graph convolutional network with orientation-guided supervision to segment bone surfaces. Studies such as [2,11] use the convolutional-based network for efficient bone surface segmentation. Additionally, some studies have focused on the automatic identification and segmentation of soft tissues, such as finger tendons and synovial sheaths [9,12]. Although these methods are effective in segmenting specific objects, they lack fine-grained analysis. Some studies have revealed that each hand bone has clinical analysis value [1], thus making fine-grained segmentation clinically significant. However, this is a challenging task. The hand comprises numerous structures, with a closely related anatomical relationship between the phalanges, metacarpal bones, and epiphysis. Moreover, different categories exhibit similar imaging features, with the epiphysis being particularly indistinguishable from the phalanges and metacarpal bones.Fine-grained segmentation demands a model to maintain the inter-slice anatomical relationships while extracting intra-slice detailed features. The 2D convolution excels at capturing dense information but lacks inter-slice information, while the 3D convolution is complementary [6]. This motivates us to develop a proper adaptive fusion method. Previous studies used 2D/3D layouts to address data anisotropy problems. For example, Wang et al. [17] propose a 2.5D UNet incorporating 2D and 3D convolutions to improve the accuracy of MR image segmentation. Dong et al. [6] present a mesh network fusing multi-level features for better anisotropic feature extraction. However, the effectiveness of these methods in capturing fine-grained feature representations is limited due to their relatively fixed convolution distributions and feature fusion approaches. Moreover, the lack of supervision on complex anatomical relationships makes them inevitably suffer from anatomical errors such as missing or confusing categories.To overcome the deficiencies of existing methods, this study proposes a novel Adaptive Multi-dimensional Convolutional Network (AMCNet) with an anatomy-constraint loss for fine-grained hand bone segmentation. Our contribution is three-fold. 1) First, to the best of our knowledge, this is the first work to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. We propose a novel multi-dimensional network to tackle the issue of multiple categories and insignificant feature differences. 2) Second, we propose an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives, thus improving the fine-grained feature representation of the model.3) Finally, we propose an anatomy-constraint loss that minimizes the anatomical error and mines hard samples, further improving the performance of the model."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.1,Network Design,"As shown in Fig. 1, the architecture of AMCNet consists of four down-sampling layers, four up-sampling layers, and four skip-connections. Each layer contains an adaptive 2D/3D convolutional module (ACM) which is proposed to dynamically balance inter-layer and intra-layer feature weight through adaptive 2D and 3D convolutions for better representations. In the encoder, each ACM block is followed by a max-pooling layer to compress features. In the decoder, the trilinear interpolate is used to up-sample features. The number of channels across each layer is empirically set to 64, 128, 256, and 512. The output layer uses the 1×1×1 convolutional layer to obtain the segmentation map. "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.2,Adaptive 2D/3D Convolutional Module (ACM),"To enable the model to capture the inter-layer anatomical connection and intralayer dense semantic feature, the ACM is proposed to adaptively fuse the 2D and 3D convolution at different levels. Figure 1(b) illustrates the architecture of the ACM. Firstly, the feature map F i ∈ R c ×w×h×d passes through 2D and 3D convolutional block respectively to obtain the 2D convolutional feature F 2D ∈ R c×w×h×d and 3D convolutional feature F 3D ∈ R c×w×h×d . Figure 1(c) shows the details of the 2D and 3D convolutional block, which includes two 1 × 3 × 3 or 3 × 3 × 3 convolution, instance normalization (Instance-Norm), and LeakyRuLU operations. The use of Instance-Norm considers the limitation of batch size in 3D medical image segmentation. Then, the F 2D and F 3D are performed the voxelwise adding and the global average pooling (GAP ) to generate channel-wise statistics F G ∈ R c×1×1×1 , which can be expressed as:where w, h, and d are the width, height, and depth of the input feature map, respectively. Further, a local cross-channel information interaction attention mechanism is applied for the fusion of multi-dimensional convolutional features. Specifically, the feature map F G is squeezed to a one-dimension tensor of length c, which is the number of channels, and then a one-dimensional (1D) convolution with a kernel size of K is applied for information interaction between channels. The obtained feature layer is re-expanded into a 3D feature map F G , which can be expressed as:where C1D K denotes the 1D convolution with the kernel size of K, G S and G U denote the operation of squeezing and re-expanding respectively.To adaptively select the feature information from different convolutions, the softmax operation is performed channel-wise to compute the weight vectors α and β corresponding to F 2D and F 3D respectively, which can be expressed as:where A, B ∈ R c×c denote the learnable parameters, A i and B i denote to the i-th row of A and B respectively, α i and β i denote to i-th element of α and β respectively.where F o denotes the output feature map of the ACM."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.3,Anatomy-Constraint Loss,"Fine-grained hand bone segmentation places stringent demands on the anatomical relationships between categories, but the lack of supervision during model training renders it the primary source of segmentation error. For instance, the epiphysis is highly prone to neglect due to its small and imbalanced occupation, while the index and ring phalanx bones can easily be mistaken due to their symmetrical similarities. To address this issue, we propose the anatomy-constraint loss to facilitate the model's learning of anatomical relations. Anatomical errors occur when pixels significantly deviate from their expected anatomical locations. Thus, we utilize the loss to compute and penalize these deviating pixels. Assume that the Y and P are the label and segmentation map respectively. First, the map representing anatomical errors is generated, where only pixels in the segmentation map that do not correspond to the anatomical relationship are activated. To mitigate subjective labeling errors caused by the unclear boundaries in US images, we perform morphological dilation on the segmentation map P. This operation expands the map and establishes an error tolerance, allowing us to disregard minor random errors and promote training stability. To make it differentiable, we implement this operation with a kernel=3 and stride=1 max-pooling operation. Subsequently, the anatomical error map F E is computed by pixel-wise subtracting the Y and the expanded segmentation map P. The resulting difference map is then activated by ReLU, which ensures that only errors within the label region and beyond the anatomically acceptable range are penalized. The process can be expressed as:where Y Ci and P Ci denote the i-th category maps of the label Y and segmentation map P respectively, G mp (•) denotes the max-pooling operation, and σ R (•) denotes the ReLU activation operation.Next, we intersect F E with the segmentation map P and label Y, respectively, based on which the cross entropy is computed, which is used to constrain the anatomical error:where Loss AC (•) denotes the proposed anatomy-constraint loss, Loss CE (•) denotes the cross-entropy loss, and denotes the intersection operation.To reduce the impact of class imbalance on model training and improve the stability of segmentation, we use a combination of Dice loss and anatomyconstraint loss function:where L is the overall loss, Loss Dice denotes the Dice loss, and γ denotes the weight-controlling parameter of the anatomy-constraint loss.3 Experiments and Results"
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.1,Dataset and Implementation,"Our method is validated on an in-house dataset, which consists of 103 3D ultrasound volumes collected using device IBUS BE3 with a 12MHz linear transducer from pediatric hand examinations. The mean voxel resolution is 0.088×0.130×0.279 mm 3 and the mean image size is 512×1023×609. Two expert ultrasonographers manually annotated the data based on ITK-snap [18]. Each phalanx, metacarpal, and epiphysis were labeled with different categories, and there are a total of 39 categories including the background. The dataset was randomly spitted into 75% training set and 25% test set. All data were resized to 256×512 in the transverse plane and maintained the axial size. We extracted 256×512×16 voxels training patches from the resized images as the training samples.The training and test phases of the network were implemented by PyTorch on an NVIDIA GeForce RTX 3090 GPU. The network was trained with Adam optimization with momentum of 0.9. The learning rate was set as 10e-3. For the hyperparameter, the kernel size K of 1D convolution in ACM was set as 3 and the weight-controlling parameter γ was set as 0.5. We used the Dice coefficient (DSC), Jaccard Similarity (Jaccard), Recall, F1-score, and Hausdorff Distance (HD95) as evaluation metrics."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.2,Performance Comparison,"We compared our network with recent and outstanding medical image segmentation methods, which contain UNet [15], UNet++ [19], 3D UNet [5], VNet [13], MNet [6], and the transformer baseline SwinUNet [3]. For a fair comparison, we used publicly available hyperparameters for each model. For the 3D network, the data processing method is consistent with ours, while for the 2D network, we slice the images along the axis to convert the 3D data into 2D.Table 1 lists the results. Note that our method achieved the highest quantitative performance of DSC, Jaccard, Recall and F1-score, with values of 0.900, 0.819, 0.871, and 0.803, respectively. These results improved by 1.3%, 2.1%, 0.8%, and 1.3% compared to the best values of other methods. Note that our method outperformed the MNet that is a state-of-the-art (SOTA) method, which demonstrated the effectiveness of adaptive multi-dimensional feature fusion and anatomy-constraint for enhancing model performance. Figure 2 shows the visualization results of our method and comparative methods (Due to page limitations, we only presented the comparison results of our method with baseline and SOTA methods). Compared to other methods, our method has advantages in effectively mining difficult samples, particularly in accurately identifying and classifying clinically important but difficultto-distinguish epiphysis. Additionally, it can effectively learn the anatomical relationships of different categories, reducing the occurrence of category confusion, particularly in the phalanges of the index finger, middle finger, and ring finger. "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.3,Ablation Study,"To validate the effect of anatomy-constraint loss, we compare the results of training with Loss Dice and the combination of Loss Dice and Loss AC on both 3D UNet and the proposed AMCNet. Table 2 lists the results. Note that compared with only Loss Dice , UNet and our method have improved in various metrics after adding Loss AC , boosting 0.6% in DSC and 1.1% in Jaccard. The results indicate that enforcing anatomical constraints to encourage the model to learn anatomical relationships improves the model's feature representation, resulting in better performance. Additionally, to verify the effect of the adaptive multidimensional feature fusion mechanism, we modified the ACM module to only 2D and 3D convolutional blocks, respectively. The results are shown in Table 2. Note that the 2D and 3D feature adaptive fusion mechanism improves model performance. Specifically, under Loss Dice and Loss AC , it has resulted in an increase of 1.0% and 0.9% in DSC, 1.1% and 1.5% in Jaccard, respectively, compared to using only 2D or 3D convolution. "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.4,Software Development and Application,"Based on the method described above, a user-friendly and extensible module was developed on the 3D Slicer platform [7] to facilitate user access, as shown in Fig. 3. To use the module, users simply select the input and output file formats on the module interface and click the ""apply"" button. The software will then automatically perform image preprocessing, call the model for inference, and deliver the segmentation result within twenty seconds (see supplementary material 1). This plugin will be released globally to promote the greater value of the proposed method in clinical applications. "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,4.0,Conclusion,"In this work, we have presented an adaptive multi-dimensional convolutional network, called AMCNet, to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. It adopts an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives. Furthermore, an anatomy-constraint loss is designed to encourage the model to learn anatomical relationships and effectively mine hard samples. Experiments show that our proposed method outperforms other comparison methods and effectively addresses the task of fine-grained hand bone segmentation in ultrasound volume. The proposed method is general and could be applied to more medical segmentation scenarios in the future."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_38.
Diffusion Transformer U-Net for Medical Image Segmentation,1.0,Introduction,"Deep Learning (DL) methods like Convolutional Neural Networks (CNN) and Vision-Transformers (ViT) have been applied to medical image segmentation [7,8,17] with good performance. However, these DL methods have some inherent limitations on their network architectures. For example, CNNs are capable of extracting local features but not direct global features, whereas ViTs employ a fixed window which limit their capability to extract fine contextual details that are necessary for accurate pixel-level segmentation.Recently, Denoising Diffusion Probabilistic Model (DDPM) [9] shows great performance in various conditional and unconditional generation tasks, and it is also applied to medical image segmentation [23,24]. Despite of the success, there are a few shortcomings to overcome: (1) The semantic embedding extracted from the source image is not well aligned with the noise embedding in the diffusion process, leading to poor conditioning and subpar performance; and (2) The U-Net backbone in these DDPM-based methods is not sensitive to various scales of contextual information during the reverse diffusion (denoising) process, observed in CNNs and ViTs as well.Motivated by the underlined limitations, we propose a Diffusion Transformer U-Net, with the following contributions:-A conditional diffusion model with forward and backward processes is proposed to train segmentation networks. In the backward denoising process, the feature embedding of a noise image is aligned with that of the conditional source image by a new cross-attention module. Then, it is denoised into a segmentation mask of the source image by the segmentation network. -A transformer-based U-Net with multi-sized windows, named as MT U-Net, is designed to extract both pixel-level and global contextual features for achieving good segmentation performance. -The MT U-Net trained by the diffusion model has a great generalization capability on various imaging modalities, and outperforms all the current state-of-the-art on five benchmark datasets including polyp segmentation from colonoscopy images [1,10], skin lesion segmentation from dermoscopy images [4,5], and optic-cup segmentation from retinal fundus images [14]. "
Diffusion Transformer U-Net for Medical Image Segmentation,2.1,Diffusion Model,"The diffusion has two processes (Fig. 1): forward and reverse. In the forward process, the ground truth M 0 is transformed into noisy ground truth M T by gradually adding Gaussian noise through T time steps. In the reverse process, first, the source image I and noise map Mt+1 pass through an encoder E (two residual-inception blocks [18]) to obtain embedding f I ∈ R h×w×c1 and f M ∈ R h×w×c2 (subscripts I and M denote image and map), where h, w and c 1 (c 2 ) are the height, width and channels of the embeddings, respectively. Then, the two embeddings are aligned by a Cross-Attention (CA) module in the feature space. The aligned feature map is given as a noisy input to MT U-Net to recover Mt . This reverse process iterates from t = T -1 till t = 0 (i.e., the initial Mt+1 when t = T -1, MT , is set as M T , and M0 is recovered eventually, which is expected to be identical to the ground truth M 0 ). Figure 2 presents the architecture of our CA module, which is used to align f M and f I in order to improve the conditioning of the diffusion model. First, f M and f I are divided into patches and flattened to vectors by a Patch Encoding (PE) layer. Then, the position information of patches is obtained using a Position Encoding layer (PoE), and is added to the original patch embeddings for preserving their positional information. The dimensions of the two positionbuilt-in patch embeddings are aligned using Linear Projection (LP) layers, and are normalized by a Layer Normalization (LN), denoting the output after the two LN's as f p M ∈ R d and f p I ∈ R d (d-dim feature vectors of patches). Thirdly, we use a Self-Attention for efficient feature fusion:where f p M is the query (Q), the concatenation of f p M and f p I is the key (K) and value (V ). denotes the transpose. Fourthly, following [20], we encode the output of L SA by a Layer Normalization (LN) and a two-layered Multi-Layer Perceptron (MLP) for extracting more contextual information. An auxiliary connection (residual) is used to enhance the information propagation. Lastly, we apply a Reshape (RS) layer to reshape and assemble the patches into the same size as f M .  These patches, along with the time embedding are flattened into a D × 1 dimension linear embedding using a Linear Embedding layer. Then positional information obtained from the PoE is added to the linear embedding before passing through the four Encoder blocks. Each Encoder block consists of a Multi-sized Transformer (MT) module and a Patch Merging layer, except the last encoder block which only contains the MT module. The MT module extracts multiscale contextual features (to be elaborated later), and the Patch Merging layer down-samples the feature maps. With the inspiration from U-Net [15], a skip connection is employed for using the multi-scale contextual information from the encoder to overcome the loss of spatial information during down-sampling. Similar to the Encoder block, each Decoder block consists of an MT module and a patch-expanding layer, except the first decoder block which only contains the MT module. The patch-expanding layer performs the up-sampling, and reshaping operation on feature maps. Finally, we employ a Linear Projection layer to obtain the pixel-level predictions."
Diffusion Transformer U-Net for Medical Image Segmentation,2.2,Multi-sized Transformer U-Net (MT U-Net),"The proposed Multi-sized Transformer (MT) module (Fig. 3(b)) is different from the conventional transformer [6]. The MT module consists of two parts: multi-sized window and shifted-window. The multi-sized window part extracts multi-scale contextual information, and the shifted-window part enriches the extracted information. The multi-sized window part has K parallel branches, with each branch consisting of a Layer Normalization (LN), multi-head Self-Attention (SA), auxiliary connection (residual), and a Multi-layer perceptron (MLP) with two layers followed by the GELU activation function. The window size used in the multi-head self-attention is varied to extract multi-scale contextual features. The output of these individual branches is combined, and is sent to the shifted-window part. The shifted-window part has a structure similar to the individual branch in the multi-sized window, but it uses shifting windows in the self attention (SW-SA)."
Diffusion Transformer U-Net for Medical Image Segmentation,2.3,Training and Inference,"During training, a source image and its segmentation ground truth map are given as input to the diffusion model. The diffusion model is trained using the noise prediction loss (L Noise ) [12] and cross-entropy loss (L CE ).During inference, a noise image sampled from the Gaussian distribution, along with the testing image, is given as the input to the reverse process."
Diffusion Transformer U-Net for Medical Image Segmentation,3.1,Datasets and Evaluation Metrics,"To evaluate the effectiveness and generalization ability of the proposed method, different medical image segmentation tasks are tested, including: (1) Polyp segmentation from colonoscopy images (Kvasir-SEG (KSEG) [10], CVC-Clinic DB (CVC) [1]), (2) Skin lesion segmentation from dermoscopy images (ISIC 2017 (IS17') [5], ISIC 2018 (IS18') [4,19]), and (3) Optic-cup segmentation from retinal fundus images (REFUGE (REF) [14]). Dice Coefficient (DC) and Intersection over Union (IoU) are used as evaluation metrics."
Diffusion Transformer U-Net for Medical Image Segmentation,3.2,Implementation Details,"The number of branches in the MT module is set to 3 by cross-validation, with window sizes as 4, 8, and 16 respectively. The diffusion transformer U-Net is trained for 40, 000 iterations using SGD optimizer with a momentum of 0.6, with a batch size of 16, and the learning rate is set to 0.0005. In the diffusion, we use a linear noise scheduler with T = 1000 steps. For fair comparisons with the recent diffusion-based segmentation models [23,24], during inference an average ensemble of 25 predictions is considered as the final prediction. All the experiments are conducted using a NVIDIA Tesla V-100 GPU with 32 GB RAM."
Diffusion Transformer U-Net for Medical Image Segmentation,3.3,Performance Comparison,"First, we quantitatively compare our method with several well-known U-Net and/or Transformer-related segmentation models, including U-Net [15], U-Net++ [26], Attention U-Net [13], Swin U-Net [2], Trans U-Net [3], and Seg-Former [25]. With their source codes, these models are trained, and evaluated on the experiment datasets. For fair comparisons, all models use the same experimental protocol for each dataset. The quantitative results are shown in Table 1.  Our Diffusion Transformer U-Net outperforms all other U-Net or Transformer related models on the five datasets with various imaging modalities, validating its effectiveness and generalization capability. Secondly, we qualitatively compare our Diffusion Transformer U-Net with other U-Net or Transformer related models. From the randomly sampled testing images in Fig. 4, we observe that the other models produce either oversegmented (e.g., Trans U-Net, SegFormer) or under-segmented results (e.g., U- Net, U-Net++, Attention U-Net, Swin U-Net), and our segmentation masks are closest to the ground truth, demonstrating the effectiveness of our method. Lastly, we compare our Diffusion Transformer U-Net with all the latest best models on the five datasets, as summarized in Table 2. The results from cited methods are copied from their papers directly, except for MedSegDiff, and MedSegDiff-V2. These two approaches are re-trained, and evaluated on the REF dataset. Note, since some methods use different experiment protocols on the IS18' dataset. For fair comparisons, we train/cross-validate/test our method using two different protocols, and compare ours with other methods with the same protocol. As shown in Table 2, our method consistently outperforms all the current best models on these five datasets, which again verifies its effectiveness and superiority."
Diffusion Transformer U-Net for Medical Image Segmentation,3.4,Ablation Study,"We perform a set of ablation studies to evaluate the contribution of each module in our Diffusion Transformer U-Net, as shown in Table 3: -The original U-Net [15] is used as a baseline (row 1 in Table 3). -Using our diffusion model with the CA module (row 3), the performance is further improved, compared to the basic concatenation operation (row 2), which validates the contribution of the CA model for aligning feature embeddings during the denoising process of the diffusion model. -Using our diffusion model with the CA module, we add the basic transformer units [6] without the multi-sized window into the U-Net (row 4). This also increases the segmentation performance, compared to row 3, which demonstrates that transformers can help the U-Net on segmentation. -Based on the model from row 4, we add multi-sized windows into the transformer (i.e., our Diffusion Transformer U-Net, row 5). This gives the highest performance, compared to other configurations in the ablation studies."
Diffusion Transformer U-Net for Medical Image Segmentation,4.0,Conclusion,"A Diffusion Transformer U-Net is proposed for medical image segmentation. Instead of a standard U-Net in the diffusion model, we propose a transformer based U-Net with multi-sized windows for enhancing the contextual information extraction and reconstruction. We also design a cross-attention module to align feature embeddings, providing a better conditioning from the source image to the diffusion model. The evaluation on various datasets of different modalities shows the effectiveness and generalization ability of the proposed method."
Diffusion Transformer U-Net for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 59.
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,1.0,Introduction,"With the recent development of semi-supervised learning (SSL) [3], rapid progress has been made in medical image segmentation, which typically learns rich anatomical representations from few labeled data and the vast amount of unlabeled data. Existing SSL approaches can be generally categorized into adversarial training [16,32,36], deep co-training [23,40], mean teacher schemes [7,[13][14][15]27,34,38,39], multi-task learning [11,19,22], and contrastive learning [2,24,29,33,35,37]. Contrastive learning (CL) has become a remarkable approach to enhance semi-supervised medical image segmentation performance without significantly increasing the amount of parameters and annotation costs [2,29,35]. In realworld clinical scenarios, since the classes in medical images follow the Zipfian distribution [41], the medical datasets usually show a long-tailed, even heavytailed class distribution, i.e., some minority (tail) classes involving significantly fewer pixel-level training instances than other majority (head) classes, as illustrated in Fig. 1. Such imbalanced scenarios are usually very challenging for CL methods to address, leading to noticeable performance drop [18].To address long-tail medical segmentation, our motivations come from the following two perspectives in CL training schemes [2,35]: ❶ Training objective -the main focus of existing approaches is on designing proper unsupervised contrastive loss in learning high-quality representations for long-tail medical segmentation. While extensively explored in the unlabeled portion of long-tail medical data, supervised CL has rarely been studied from empirical and theoretical perspectives, which will be one of the focuses in this work; ❷ Temperature scheduler -the temperature parameter τ , which controls the strength of attraction and repulsion forces in the contrastive loss [4,5], has been shown to play a crucial role in learning useful representations. It is affirmed that a large τ emphasizes anatomically meaningful group-wise patterns by group-level discrimination, whereas a small τ ensures a higher degree of pixel-level (instance) discrimination [25,28]. On the other hand, as shown in [25], group-wise discrimination often results in reduced model's instance discrimination capabilities, where the model will be biased to ""easy"" features instead of ""hard"" features. It is thus unfavorable for long-tailed medical segmentation to blindly treat τ as a constant hyperparameter, and a dynamic temperature parameter for CL is worth investigating. In this paper, we introduce ACTION++, which further optimizes anatomically group-level and pixel-level representations for better head and tail class separations, on both labeled and unlabeled medical data. Specifically, we devise two strategies to improve overall segmentation quality by focusing on the two aforementioned perspectives: (1) we propose supervised adaptive anatomical contrastive learning (SAACL) for long-tail medical segmentation. To prevent the feature space from being biased toward the dominant head class, we first pre-compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed class centers; (2) we find that blindly adopting the constant temperature τ in the contrastive loss can negatively impact the segmentation performance. Inspired by an average distance maximization perspective, we leverage a dynamic τ via a simple cosine schedule, resulting in significant improvements in the learned representations. Both of these enable the model to learn a balanced feature space that has similar separability for both the majority (head) and minority (tail) classes, leading to better generalization in long-tail medical data. We evaluated our ACTION++ on the public ACDC and LA datasets [1,31]. Extensive experimental results show that our ACTION++ outperforms prior methods by a significant margin and sets the new state-of-the-art across two semi-supervised settings. We also theoretically show the superiority of our method in label efficiency (Appendix A). Code is released at here."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.1,Overview,"Problem Statement. Given a medical image dataset (X, Y ), our goal is to train a segmentation model F that can provide accurate predictions that assign each pixel to their corresponding K-class segmentation labels.Setup. Figure 2 illustrates an overview of ACTION++. By default, we build this work upon ACTION pipeline [35], the state-of-the-art CL framework for semi-supervised medical image segmentation. The backbone model adopts the student-teacher framework that shares the same architecture, and the parameters of the teacher are the exponential moving average of the student's parameters. Hereinafter, we adopt their model as our backbone and briefly summarize its major components: ( 1 Global and Local Pre-training. [35] first creates two types of anatomical views as follows: (1) augmented viewsx1 and x 2 are augmented from the unlabeled input scan with two separate data augmentation operators; (2) mined views -n samples (i.e., x 3 ) are randomly sampled from the unlabeled portion with additional augmentation. The pairs x 1 , x 2 are then processed by studentteacher networks [F s , F t ] that share the same architecture and weight, and similarly, x 3 is encoded by F t . Their global latent features after the encoder E (i.e., h 1 , h 2 , h 3 ) and local output features after decoder D (i.e., f 1 , f 2 , f 3 ) are encoded by the two-layer nonlinear projectors, generating global and local embeddings v g and v l . v from F s are separately encoded by the non-linear predictor, producing w in both global and local manners 1 . Third, the relational similarities between augmented and mined views are processed by SoftMax function as follows:where τ s and τ t are two temperature parameters. Finally, we minimize the unsupervised instance discrimination loss (i.e., Kullback-Leibler divergence KL) as:We formally summarize the pretraining objective as the equal combination of the global and local L inst , and supervised segmentation loss L sup (i.e., equal combination of Dice loss and cross-entropy loss).Anatomical Contrast Fine-Tuning. The underlying motivation for the finetuning stage is that it reduces the vulnerability of the pre-trained model to long-tailed unlabeled data. To mitigate the problem, [35] proposed to fine-tune the model by anatomical contrast. First, the additional representation head ϕ is used to provide dense representations with the same size as the input scans.Then, [35] explore pulling queries r q ∈R to be similar to the positive keys r + k ∈R, and push apart the negative keys r - k ∈R. The AnCo loss is defined as follows:where C denotes a set of all available classes in the current mini-batch, and τ an is a temperature hyperparameter. For class c, we select a query representation set R c q , a negative key representation set R c k whose labels are not in class c, and the positive key r c,+ k which is the c-class mean representation. Given P is a set including all pixel coordinates with the same size as R, these queries and keys can be defined as:We formally summarize the fine-tuning objective as the equal combination of unsupervised L anco , unsupervised cross-entropy loss L unsup , and supervised segmentation loss L sup . For more details, we refer the reader to [35]."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.2,Supervised Adaptive Anatomical Contrastive Learning,"The general efficacy of anatomical contrast on long-tail unlabeled data has previously been demonstrated by the authors of [35]. However, taking a closer look, we observe that the well-trained F shows a downward trend in performance, which often fails to classify tail classes on labeled data, especially when the data shows long-tailed class distributions. This indicates that such well-trained F is required to improve the segmentation capabilities in long-tailed labeled data. To this end, inspired by [17] tailored for the image classification tasks, we introduce supervised adaptive anatomical contrastive learning (SAACL), a training framework for generating well-separated and uniformly distributed latent feature representations for both the head and tail classes. It consists of three main steps, which we describe in the following.Anatomical Center Pre-computation. We first pre-compute the anatomical class centers in latent representation space. The optimal class centers are chosen as K positions from the unit sphere S d-1 = {v ∈ R d : v 2 = 1} in the ddimensional space. To encourage good separability and uniformity, we compute the class centers {ψ c } K c=1 by minimizing the following uniformity loss L unif :In our implementation, we use gradient descent to search for the optimal class centers constrained to the unit sphere S d-1 , which are denoted by {ψ c } K c=1 . Furthermore, the latent dimension d is a hyper-parameter, which we set such that d K to ensure the solution found by gradient descent indeed maximizes the minimum distance between any two class centers [6]. It is also known that any analytical minimizers of Eq. 3 form a perfectly regular K-vertex inscribed simplex of the sphere S d-1 [6]. We emphasize that this first step of pre-computation of class centers is completely off-line as it does not require any training data.Adaptive Allocation. As the second step, we explore adaptively allocating these centers among classes. This is a combinatorial optimization problem and an exhaustive search of all choices would be computationally prohibited. Therefore, we draw intuition from the empirical mean in the K-means algorithm and adopt an adaptive allocation scheme to iteratively search for the optimal allocation during training. Specifically, consider a batch B = {B 1 , • • • , B K } where B c denotes a set of samples in a batch with class label c,i∈Bc φ i 2 be the empirical mean of class c in current batch, where φ i is the feature embedding of sample i. We compute assignment π by minimizing the distance between pre-computed class centers and the empirical means:In implementation, the empirical mean is updated using moving average. That is, for iteration t, we first compute the empirical mean φ c (B) for batch B as described above, and then update byAdaptive Anatomical Contrast. Finally, the allocated class centers are wellseparated and should maintain the semantic relation between classes. To utilize these optimal class centers, we want to induce the feature representation of samples from each class to cluster around the corresponding pre-computed class center. To this end, we adopt a supervised contrastive loss for the label portion of the data. Specifically, given a batch of pixel-feature-label tupleswhere ω i is the i-th pixel in the batch, φ i is the feature of the pixel and y i is its label, we define supervised adaptive anatomical contrastive loss for pixel i as:whereis the pre-computed center of class y i . The first term in Eq. 5 is supervised contrastive loss, where the summation over φ + i refers to the uniformly sampled positive examples from pixels in batch with label equal to y i . The summation over φ j refers to all features in the batch excluding φ i . The second term is contrastive loss with the positive example being the pre-computed optimal class center."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.3,Anatomical-Aware Temperature Scheduler (ATS),"Training with a varying τ induces a more isotropic representation space, wherein the model learns both group-wise and instance-specific features [12]. To this end, we are inspired to use an anatomical-aware temperature scheduler in both the supervised and the unsupervised contrastive losses, where the temperature parameter τ evolves within the range [τ -, τ + ] for τ + > τ -. Specifically, for iteration t = 1, • • • , T with T being the total number of iterations, we set τ t as:"
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,3.0,Experiments,"Experimental Setup. We evaluate ACTION++ on two benchmark datasets: the LA dataset [31] and the ACDC dataset [1]. The LA dataset consists of 100 gadolinium-enhanced MRI scans, with the fixed split [29] using 80 and 20 scans for training and validation. The ACDC dataset consists of 200 cardiac cine MRI scans from 100 patients including three segmentation classes, i.e., left ventricle (LV), myocardium (Myo), and right ventricle (RV), with the fixed split2 using 70, 10, and 20 patients' scans for training, validation, and testing. For all our experiments, we follow the identical setting in [19,29,30,39], and perform evaluations under two label settings (i.e., 5% and 10% ) for both datasets.Implementation Details. We use an SGD optimizer for all experiments with a learning rate of 1e-2, a momentum of 0.9, and a weight decay of 0.0001. Following [19,29,30,39] on both datasets, all inputs were normalized as zero mean and unit variance. The data augmentations are rotation and flip operations. Our work is built on ACTION [35], thus we follow the identical model setting except for temperature parameters because they are of direct interest to us. For the sake of completeness, we refer the reader to [35] for more details. We set λ a , d as 0.2, 128, and regarding all τ , we use τ + = 1.0 and τ -= 0.1 if not stated otherwise.On ACDC, we use the U-Net model [26] as the backbone with a 2D patch size of 256 × 256 and batch size of 8. For pre-training, the networks are trained for 10K iterations; for fine-tuning, 20K iterations. On LA, we use the V-Net [21]  Main Results. We compare our ACTION++ with current state-of-the-art SSL methods, including UAMT [39], SASSNet [16], DTC [19], URPC [20], MC-Net [30], SS-Net [29], and ACTION [35], and the supervised counterparts (UNet [26]/VNet [21]) trained with Full/Limited supervisions -using their released code. To evaluate 3D segmentation ability, we use Dice coefficient (DSC) and Average Surface Distance (ASD). Table 2 and Table 1 display the results on the public ACDC and LA datasets for the two labeled settings, respectively. We next discuss our main findings as follows. ( 1) LA: As shown in Table 1, our method generally presents better performance than the prior SSL methods under all settings. Figure 4 (Appendix) also shows that our model consistently outperforms all other competitors, especially in the boundary region; (2) ACDC: As Table 2 shows, ACTION++ achieves the best segmentation performance in terms of Dice and ASD, consistently outperforming the previous SSL methods across two labeled settings. In Fig. 3 (Appendix), we can observe that ACTION++ can yield the segmentation boundaries accurately, even for very challenging regions (i.e., RV and Myo). This suggests that ACTION++ is inherently better at long-tailed learning, in addition to being a better segmentation model in general. Ablation Study. We first perform ablation studies on LA with 10% label ratio to evaluate the importance of different components. Table 3 shows the effectiveness of supervised adaptive anatomical contrastive learning (SAACL). Table 4 (Appendix) indicates that using anatomical-aware temperature scheduler (ATS) and SAACL yield better performance in both pre-training and fine-tuning stages.We then theoretically show the superiority of our method in Appendix A. Finally, we conduct experiments to study the effects of cosine boundaries, cosine period, different methods of varying τ , and λ a in Table 5, Table 6 (Appendix), respectively. Empirically, we find that using our settings (i.e., τ -= 0.1, τ + = 1.0, T /#iterations=1.0, cosine scheduler, λ a = 0.2) attains optimal performance (Table 4)."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,4.0,Conclusion,"In this paper, we proposed ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Our work is inspired by two intriguing observations that, besides the unlabeled data, the class imbalance issue exists in the labeled portion of medical data and the effectiveness of temperature schedules for contrastive learning on longtailed medical data. Extensive experiments and ablations demonstrated that our model consistently achieved superior performance compared to the prior semisupervised medical image segmentation methods under different label ratios. Our theoretical analysis also revealed the robustness of our method in label efficiency. In future, we will validate CT/MRI datasets with more foreground labels and try t-SNE."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 19.
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,1.0,Introduction,"Integrating multi-modality medical images for tumor segmentation is crucial for comprehensive diagnosis and surgical planning. In the clinic, the consistent information and complementary information in multi-modality medical images provide the basis for tumor diagnosis. For instance, the consistent anatomical structure information offers the location feature for tumor tracking [22], while the complementary information such as differences in lesion area among multimodality medical images provides the texture feature for tumor characterization. Multi-modality machine learning aims to process and relate information from multiple modalities [4]. But it is still tricky to integrate multi-modality medical images due to the complexity of medical images.Existing methods for multi-modality medical image integration can be categorized into three groups: (1) input-based integration methods that concatenate multi-modality images at the beginning of the framework to fuse them directly [ 19,21], (2) feature-based fusion methods that incorporate a fusion module to merge feature maps [16,23], and (3) decision-based fusion methods that use weighted averaging to balance the weights of different modalities [11,15]. Essentially, these methods differ in their approach to modifying the number of channels, adding additional convolutional layers with a softmax layer for attention, or incorporating fixed modality-specific weights. However, there is no mechanism to evaluate the reliability of information from multi-modality medical images. Since the anatomical information of different modality medical images varies, the reliability provided by different modalities may also differ. Therefore, it remains challenging to consider the reliability of different modality medical images when combining multi-modality medical image information.Dempster-Shafer theory (DST) [18], also known as evidence theory, is a powerful tool for modeling information, combining evidence, and making decisions by integrating uncertain information from various sources or knowledge [10].Some studies have attempted to apply DST to medical image processing [8]. However, using evidence theory alone does not enable us to weigh the different anatomical information from multi-modality medical images. To explore the reliability of different sources when using evidence theory, the work by Mercier et al. [13] proposed a contextual discounting mechanism to assign weights to different sources. Furthermore, for medical image segmentation, denoising diffusion probabilistic models (DDPM [7]) have shown remarkable performance [9,20]. Inspired by these studies, if DDPM can parse the reliability of multi-modality medical images to weigh the different anatomy information from them, it will provide a significant approach for tumor segmentation.In this paper, we propose an evidence-identified DDPM (EI-DDPM) with contextual discounting for tumor segmentation via integrating multi-modality medical images. Our basic assumption is that we can learn the segmentation feature on single modality medical images using DDPM and parse the reliability of different modalities medical images by evidence theory with a contextual discounting mechanism. Specifically, the EI-DDPM first utilizes parallel conditional DDPM to learn the segmentation feature from a single modality image. Next, the evidence-identified layer (EIL) preliminarily integrates multi-modality images by comprehensively using the multi-modality uncertain information. Lastly, the contextual discounting operator (CDO) performs the final integration of multimodality images by parsing the reliability of information from multi-modality medical images. The contributions of this work are:-Our EI-DDPM achieves tumor segmentation by using DDPM under the guidance of evidence theory. It provides a solution to integrate multi-modality medical images when deploying the DDPM algorithm. -The proposed EIL and CDO apply contextual discounting guided DST to parse the reliability of information from different modalities of medical images. This allows for the integration of multi-modality medical images with learned weights corresponding to their reliability. -We conducted extensive experiments using the BraTS 2021 [12] dataset for brain tumor segmentation and a liver MRI dataset for liver tumor segmentation. Experimental results demonstrate the superiority of EI-DDPM over other State-of-The-Art (SoTA) methods."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.0,Method,"The EI-DDPM achieves tumor segmentation by parsing the reliability of multimodality medical images. Specifically, as shown in Fig. 1, the EI-DDPM is fed with multi-modality medical images into the parallel DDPM path and performs the conditional sampling process to learn the segmentation feature from the single modality image (Sect. 2.1). Next, the EIL preliminary integrates multimodality images by embedding the segmentation features from multi-modality images into the combination rule of DST (Sect. 2.2). Lastly, the CDO integrates multi-modality medical images for tumor segmentation by contextual discounting mechanism (Sect. 2.3)."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.1,Parallel DDPM Path for Segmentation Feature Learning,"Background of DDPM: As an unconditional generative method, DDPM [7] has the form of p θ (x 0 ) := p θ (x 0:T )dx 1:T , where x 1 , ..., x T represent latents with the same dimensionality as the data x 0 ∼ q(x 0 ). It contains the forward process of diffusion and the reverse process of denoising. The forward process of diffusion that the approximate posterior q(x 1:T |x 0 ), it is a Markov Chain by gradually adding Gaussian noise for converting the noise distribution to the data distribution according to the variance schedule β 1 , ..., β T :The reverse process of denoising that the joint distribution p θ (x 0:T ), it can be defined as a Markov chain with learnt Gaussian transitions starting from p(x T ) = N (x T ; 0, I):where α t := 1β t , ᾱt := t s=1 α s , and Multi-modality Medical Images Conditioned DDPM: In Eq. 2 from DDPM [7], the unconditional prediction x t-1 at each step is obtained by subtracting the predicted noise from the previous x t , which can be defined as:As shown in Fig. 2, to perform the conditional sampling in EI-DDPM, the prediction x t-1 at each step t is on the basis of the concatenation ""⊕"" of previous x t and the conditional image x C,t . Thus, the x t-1 here can be defined as:where the conditional image x C ∈ {T 1, T 2, F lair, T 1ce} corresponding to the four parallel conditional DDPM path. And in each step t, the x C,t was also performed the operation of adding Gaussian noise to convert the distribution:where x C,0 presents the multi-modality medical images."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.2,EIL Integrates Multi-modality Images,"As shown in Fig. 1, the EIL is followed at the end of the parallel DDPM path.The hypothesis is to regard multi-modality images as independent and different sources knowledge. And then embedding multi-phase features into the combination rule of DST for evidence identification, which comprehensively parses the multi-phase uncertainty information for confident decision-making. The basic concepts of evidence identification come from DST [18]. It is assumed that Θ = {θ 1 , θ 2 , ..., θ n } is a finite domain called discriminant frame. Mass function is defined as M. So, the evidence about Θ can be represented by M as:where the M(A) denotes the whole belief and evidence allocated to A. The associated belief (Bel) and plausibility (Pls) functions are defined as:And using the contour function pls to restrict the plausibility function P ls of singletons (i.e. pls(θ) = P ls({θ}) for all θ ∈ Θ) [18]. According to the DST, the mass function M of a subset always has lower and upper bound as Bel(A) ≤ M(A) ≤ P ls(A). From the evidence combination rule of DST, for the mass functions of two independent items M 1 and M 2 (i.e. two different modality images), it can be calculated by a new mass function M 1 ⊕ M 2 (A), which is the orthogonal sum of M 1 and M 2 as:where γ = B∩C=∅ M 1 (B)M 2 (C) is conflict degree between M 1 and M 2 . And the combined contour function P ls 12 corresponding to M 1 ⊕ M 2 is:The specific evidence identification layer mainly contains three sub-layers (i.e. activation layer, mass function layer, and belief function layer). For activation layer, the activation of i unit can be defined as:. The w i is the weight of i unit. λ i > 0 and a i are parameters. The mass function layer calculates the mass of each K classes using M i ({θ k }) = u ik y i , where K k=1 u ik = 1. u ik means the degree of i unit to class θ k . Lastly, the third layer yields the final belief function about the class of each pixel using the combination rule of DST (Eq. 8). where M ? is a vacuous mass function defined by M(Θ) = 1, η M is a mixture of M and M ? . The coefficient η plays the role of weighting mass function η M.The corresponding contour function of η M is:Advantage: EIL and CDO parse the reliability of different modality medical images in different contexts. For example, if we feed two modality medical images like T 1 and T 2 into the EI-DDPM, with the discount rate 1η T 1 and 1η T 2 , we will have two contextual discounted contour functions ηT1 pls T 1 and ηT 2 pls T 2 .The combined contour function in Eq. 9 is proportional to the ηT 1 pls T 1 ηT 2 pls T 2 . In this situation, the η T 1 and η T 2 can be trained to weight the two modality medical images by parsing the reliability."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,3.0,Experiment and Results,"Dataset. We used two MRI datasets that BraTS 2021 [1,2,12] and a liver MRI dataset. BraTS 2021 1 contains 1251 subject with 4 aligned MRI modalities: T1, T2, Flair, and contrast-enhanced T1 (T1ce). The segmentation labels consist of Implementation Details. For the number of DDPM paths, BraTS 2021 dataset is equal to 4 corresponding to the input 4 MRI modalities and the liver MRI dataset is equal to 3 corresponding to the input 3 MRI modalities. In the parallel DDPM path, the noise schedule followed the improved-DDPM [14], and the U-Net [17] was utilized as the denoising model with 300 sampling steps.In EIL, the initial values of a i equal 0.5 and λ i is equal to 0.01. For CDO, the initial of parameter η k is equal to 0.5. With the Adam optimization algorithm, the denoising process was optimized using L 1 , and the EIL and CDO were optimized using Dice loss. The learning rate of EI-DDPM was set to 0.0001. The Quantitative and Visual Evaluation. The performance of EI-DDPM is evaluated by comparing with three methods: a classical CNN-based method (U-Net [17]), a Transformer-based method (TransU-Net [5]), and a DDPM-based method for multi-modality medical image segmentation (SegDDPM [20]). The Dice score is used for evaluation criteria. Figure 3 shows the visualized segmentation results of EI-DDPM and compared methods. It shows some ambiguous area lost segmentation in three compared methods but can be segmented by our EI-DDPM. Discussion of Learnt Reliability Coefficients η. Table 3 and Table 4 show the learned reliability coefficients η on BraTS 2021 dataset and liver MRI dataset. The higher η value, the higher reliability of the corresponding region segmentation. As shown in Table 3, the Flair modality provides the highest reliability for ED segmentation. And both the T2 modality and T1ce modality provide relatively high reliability for ET and NCR segmentation. As shown in Table 4, the T1ce modality provides the highest reliability for hemangioma and HCC segmentation. These reliability values are the same as clinical experience [1,3]."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,4.0,Conclusion,"In this paper, we proposed a novel DDPM-based framework for tumor segmentation under the condition of multi-modality medical images. The EIL and CDO enable our EI-DDPM to capture the reliability of different modality medical images with respect to different tumor regions. It provides a way of deploying contextual discounted DST to parse the reliability of multi-modality medical images. Extensive experiments prove the superiority of EI-DDPM for tumor segmentation on multi-modality medical images, which has great potential to aid in clinical diagnosis. The weakness of EI-DDPM is that it takes around 13 s to predict one segmentation image. In future work, we will focus on improving sampling steps in parallel DDPM paths to speed up EI-DDPM."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,1.0,Introduction,"Deep convolutional neural networks (DCNNs) have significantly advanced medical image segmentation [8,15,22]. However, their success relies heavily on accurately labeled training data [27], which are often unavailable for medical image segmentation tasks since manual annotation is highly subjective and requires the observer's perception, expertise, and concentration [4,10,12,20,24]. In a study of liver lesion segmentation using abdominal CT, three trained observers delineated the lesion twice over a one-week interval, resulting in the variation of delineated areas up to 10% per observer and more than 20% between observers [21].To analyze the annotation process, it is assumed that a latent true segmentation, called meta segmentation for this study, exists as the consensus of annotators [14,29]. Annotators prefer to produce annotations that are different from the meta segmentation to facilitate their own diagnosis. Additionally, stochastic errors may arise during the annotation process. To predict accurate meta segmentation and annotator-specific segmentation, research efforts have been increasingly devoted to addressing the issue of annotator-related bias [9,14,29].Existing methods can be categorized into three groups. Annotator decision fusion [9,18,25] methods model annotators individually and use a weighted combination of multiple predictions as the meta segmentation. Despite their advantages, these methods ignore the impact of stochastic errors on the modeling of annotator-specific segmentation [11]. Annotator bias disentangling [28,29] methods estimate the meta segmentation and confusion matrices and generate annotator-specific segmentation by multiplying them. Although confusion matrices characterize the annotator bias, their estimation is challenging due to the absence of ground truth. Thus, the less-accurate confusion matrices seriously affect the prediction of meta segmentation. Preference-involved annotation distribution learning (PADL) framework [14] disentangles annotator-related bias into annotator preference and stochastic errors and, consequently, outperforms previous methods in predicting both meta segmentation and annotatorspecific segmentation. Although PADL has recently been simplified by replacing its multi-branch architecture with dynamic convolutions [6], it still has two limitations. First, PADL uses a stack of convolutional layers after the decoder to model the annotator preference, which may not be effective in modeling the variable degrees of preference at the full resolution level, and the structure of this block, such as the number of layers and kernel size, needs to be adjusted by trial and error. Second, PADL adopts the Gaussian assumption and learns the annotation distribution per pixel independently to disentangle stochastic errors, resulting in a discontinuous boundary.To address these issues, we advocate extracting the features, on which different preferences focus. Recently, Transformer [16,17,23] has drawn increasing attention due to its ability to model the long-range dependency. Among its variants, DETR [2] has a remarkable ability to detect multiple targets at different locations in parallel, since it takes a set of different positional queries as conditions and focuses on features at different positions. Inspired by DETR, we suggest utilizing such queries to represent annotators' preferences, enabling Transformer to extract different preference-focused features. To further address the issue of missing pixel correlation, we suggest using a non-diagonal multivariate normal distribution [19] to replace the pixel-wise independent Gaussian distribution.In this paper, we propose a Transformer-based Annotation Bias-aware (TAB) medical image segmentation model, which can characterize the annotator preference and stochastic errors and deliver accurate meta segmentation and annotator-specific segmentation. TAB consists of a CNN encoder, a Preference Feature Extraction (PFE) module, and a Stochastic Segmentation (SS) head. The CNN encoder performs image feature extraction. The PFE module takes the image feature as input and produces R + 1 preference-focused features in parallel for meta/annotator-specific segmentation under the conditions of R + 1 different queries. Each preference-focused feature is combined with the image feature and fed to the SS head. The SS head produces a multivariate normal distribution that models the segmentation and annotator-related error as the mean and variance respectively, resulting in more accurate segmentation. We conducted comparative experiments on a public dataset (two tasks) with multiple annotators. Our results demonstrate the superiority of the proposed TAB model as well as the effectiveness of each component.The main contributions are three-fold. (1) TAB employs Transformer to extract preference-focused features under the conditions of various queries, based on which the meta/annotator-specific segmentation maps are produced simultaneously. (2) TAB uses the covariance matrix of a multivariate normal distribution, which considers the correlation among pixels, to characterize the stochastic errors, resulting in a more continuous boundary. ( 3) TAB outperforms existing methods in addressing the issue of annotator-related bias."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.1,Problem Formalization and Method Overview,"Let a set of medical images annotated by R annotators be denoted by, where x i ∈ R C×H×W represents the i-th image with C channels and a size of H × W , and y ir ∈ {0, 1}K×H×W is the annotation with K classes given by the r-th annotator. We simplify the K -class segmentation problem as K binary segmentation problems. Our goal is to train a segmentation model on D so that the model can generate a meta segmentation map and R annotator-specific segmentation maps for each input image.Our TAB model contains three main components: a CNN encoder for image feature extraction, a PFE module for preference-focused feature production, and a SS head for segmentation prediction (see Fig. 1). We now delve into the details of each component."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.2,CNN Encoder,"The ResNet34 [7] pre-trained on ImageNet is employed as the CNN encoder. We remove its average pooling layer and fully connected layer to adjust it to our tasks. Skip connections are built from the first convolutional block and the first three residual blocks in the CNN encoder to the corresponding locations of the decoder in the SS head. The CNN encoder takes an image x ∈ R C×H×W as its input and generates a high-level low-resolution feature mapMoreover, f img is fed to a 1×1 convolutional layer for channel reduction, resulting in f re ∈ R d×H ×W , where d = 256."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.3,PFE Module,"The PFE module consists of an encoder-decoder Transformer and a multi-head attention block. Feeding the image feature f re to the PFE module, we have R+1 enhanced feature maps {f pref r } R r=0 , on which different preferences focus (r = 0 for meta segmentation and others for R annotator-specific segmentation). Note that meta segmentation is regarded as a special preference for simplicity.The Transformer Encoder is used to enhance the image feature f re . The Transformer encoder consists of a multi-head self-attention module and a feedforward network. Since the encoder expects a sequence as input, we collapse the spatial dimensions of f re and reshape it into the size of d × H W . Next, f re is added to the fixed positional encodings E pos and fed to the encoder. The output of the Transformer encoder is denoted by f E and its size is d × H W .The Transformer Decoder accepts f E and R+1 learnable queries {Q pref r } R r=0 of size d = 256 as its input. We aim to extract different preference-focused features based on the conditions provided by {Q pref r } R r=0 , which are called 'preference queries' accordingly. This decoder consists of a multi-head self-attention module for the intercommunication between queries, a multi-head attention module for feature extraction under the conditions of queries, and a feed-forward network. And it produces R + 1 features {f D r } R r=0 of size d = 256 in parallel.Multi-head Attention Block has m heads in it. It takes the output of the Transformer decoder {f D r } R r=0 as its input and computes multi-head attention scores of f D r over the output of the encoder f E , generating m attention heatmaps per segmentation. The output of this block is denoted as {f pref r } R r=0 , and the size ofr=0 are individually decoded by SS head, resulting in R + 1 different preference-involved segmentation maps."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.4,SS Head,"The SS head aims to disentangle the annotator-related bias and produce meta and annotator-specific segmentation maps. Given an input image, we assume that the annotation distribution over annotators follows a multivariant normal distribution. Thus, we can learn the annotation distribution and disentangle the annotator-related bias by modeling it as the covariance matrix that considers pixel correlation. First, we feed the concatenation of f re and f pref r to a CNN decoder, which is followed by batch normalization and ReLU, and obtain the feature map f seg r ∈ R 32×H×W . Second, we establish the multivariant normal distribution N (μ(x), Σ(x)) via predicting the μ(x) ∈ R K×H×W and Σ(x) ∈ R (K×H×W ) 2 based on f seg r . However, the size of the covariance matrix scales with (K × H × W )2 , making it computationally intractable. To reduce the complexity, we adopt the low-rank parameterization of the covariance matrix [19]where P ∈ R (K×H×W )×α is the covariance factor, α defines the rank of the parameterization, D is a diagonal matrix with K × H × W diagonal elements.We employ three convolutional layers with 1 × 1 kernel size to generate μ(x), P , and D, respectively. In addition, the concatenation of μ(x) and f seg r is fed to the P head and D head, respectively, to facilitate the learning of P and D. Finally, we get R+1 distributions. Among them, N (μ MT (x), Σ MT (x)) is for meta segmentation and N (μ r (x), Σ r (x)), r = 1, 2, ..., R are for annotator-specific segmentation. The probabilistic meta/annotator-specific segmentation map (ŷ MT /ŷ r ) is calculated by applying the sigmoid function to the estimated μ MT /μ r . We can also produce the probabilistic annotator bias-involved segmentation maps ŷs MT /ŷ s r by applying the sigmoid function to the segmentation maps sampled from the established distribution N (μ MT (x), Σ MT (x))/N (μ r (x), Σ r (x))."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.5,Loss and Inference,"The loss of our TAB model contains two items: the meta segmentation loss L MT and annotator-specific segmentation loss L AS , shown as followswhere L MT and L AS are the binary cross-entropy loss, y s is a randomly selected annotation per image, y r is the delineation given by annotator A r .During inference, the estimated probabilistic meta segmentation map ŷMT is evaluated against the mean voting annotation. The estimated probabilistic annotator-specific segmentation map ŷr is evaluated against the annotation y r given by the annotator A r ."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,3.1,Dataset and Experimental Setup,"Dataset. The RIGA dataset [1] is a public benchmark for optic disc and optic cup segmentation, which contains 750 color fundus images from three sources, including 460 images from MESSIDOR, 195 images from BinRushed, and 95 images from Magrabia. Six ophthalmologists from different centers labeled the optic disc/cup contours manually on each image. We use 655 samples from BinRushed and MESSIDOR for training and 95 samples from Magrabia for test [9,14,26]. The 20% of the training set that is randomly selected is used for validation.Implementation Details. All images were normalized via subtracting the mean and dividing by the standard deviation. The mean and standard deviation were counted on training cases. We set the batch size to 8 and resized the input image to 256 × 256. The Adam optimizer [13] with default settings was adopted. The learning rate lr was set to 5e -5 and decayed according to the polynomial policy lr = lr 0 ×(1t/T ) 0.9 , where t is the epoch index and T = 300is the maximum epoch. All results were reported over three random runs. Both mean and standard deviation are given.Evaluation Metrics. We adopted Soft Dice (D s ) as the performance metric.At each threshold level, the Hard Dice is calculated between the segmentation and annotation maps. Soft Dice is calculated via averaging the hard Dice values obtained at multiple threshold levels, i.e., (0.1, 0.3, 0.5, 0.7, 0.9) for this study.Based on the Soft Dice, there are two performance metrics, namely Average and Mean Voting. Mean Voting is the Soft Dice between the predicted meta segmentation and the mean voting annotation. A higher Mean Voting represents better performance on modeling the meta segmentation. The annotator-specific predictions are evaluated against each annotator's delineations, and the average Soft Dice of all annotators is denoted as Average. A higher Average represents better performance on mimicking all annotators."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,3.2,Comparative Experiments,"We compared our TAB to three baseline models and four recent segmentation models that consider the annotator-bias issue, including (1) the baseline 'Multi-Net' setting, under which R U-Nets (denoted by M r , r = 1, 2, ..., R, R = 6 for RIGA) were trained and tested with the annotations provided by annotator A r , respectively; (2) MH-UNet [5]: a U-Net variant with multiple heads, each Table 1. Performance (D s disc (%), D s cup (%)) of our TAB, seven competing models, and two variants of TAB on the RIGA dataset. From left to right: Performance in mimicking the delineations of each annotator (Ar, r = 1, 2, ..., 6), Average, and Mean Voting. The standard deviation is shown as the subscript of the mean. Except for two variants of TAB and the 'Multi-Net' setting (Mr), the best results in Average and Mean Voting columns are highlighted in blue. accounting for imitating the annotations from a specific annotator; (3) MV-UNet [3]: a U-Net trained with the mean voting annotations; (4) MR-Net [9]: an annotator decision fusion method that uses an attention module to characterize the multi-rater agreement; (5) CM-Net [29]: an annotator bias disentangling method that uses a confusion matrix to model human errors; (6) PADL [14]: a multi-branch framework that models annotator preference and stochastic errors simultaneously; and (7) AVAP [6]: a method that uses dynamic convolutional layers to simplify the multi-branch architecture of PADL. Analysis of the SS Head. The effect of each component in the SS head was accessed using Average and Mean Voting. Table 2 gives the performance of the TAB with complete SS head and its three variants. We compare the stochastic errors modeling capacity of multivariate normal distribution N (μ, Σ) and the pixel-wise independent Gaussian distribution N (μ, σ) [14]. Though both N (μ, Σ) and N (μ, σ) can reduce the impact of stochastic errors, N (μ, Σ) performs better than N (μ, σ) on the test set. We also explored the influence of μ prior. It reveals that the μ prior can facilitate the learning of N (μ, Σ) and improve the capacity of meta/annotator-specific segmentation."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose the TAB model to address the issue of annotatorrelated bias that existed in medical image segmentation. TAB leverages the Transformer with multiple learnable queries on extracting preference-focused features in parallel and the multivariate normal distribution on modeling stochastic annotation errors. Extensive experimental results on the public RIGA dataset with annotator-related bias demonstrate that TAB achieves better performance than all competing methods."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Table 1 .,"cupon Average. We also visualize the segmentation maps predicted by TAB and other competing methods (see Fig.2). It reveals that TAB can produce the most accurate segmentation map compared to the ground truth. Fig. 2. Visualization of predicted meta segmentation maps obtained by applying six competing methods and our TAB to four cases from the RIGA dataset, together with ground truths (GTs, i.e., mean voting annotations)."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Table 2 .,"of the TAB with complete SS head and its three variants on the RIGA dataset. The Average and Mean Voting (D s disc (%), D s cup (%)) are used as the performance metrics. The standard deviation is shown as the subscript of the mean."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_3.
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,1.0,Introduction,"Breast cancer is the leading cause of cancer-related fatalities among women. Currently, it holds the highest incidence rate of cancer among women in the U.S., and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1]. Due to the high incidence rate, early breast cancer detection is essential for reducing mortality rates and expanding treatment options. BUS imaging is an effective screening option because it is cost-effective, nonradioactive, and noninvasive. However, BUS image analysis is also challenging due to the large variations in tumor shape and appearance, speckle noise, low contrast, weak boundaries, and occurrence of artifacts.In the past decade, deep learning-based approaches achieved remarkable advancements in BUS tumor classification [2,3]. The progress has been driven by the capability of CNN-based models to learn hierarchies of structured image representations as semantics. To extract deep context features, CNNs apply a series of convolutional and downsampling layers, frequently organized into blocks with residual connections. Nevertheless, one disadvantage of such architectural choice is that the feature representations in the deeper layers become increasingly abstract, leading to a loss of spatial and contextual information. The intrinsic locality of convolutional operations hinders the ability of CNNs to model longrange dependencies while preserving spatial information in images effectively.Vision Transformer (ViT) [5] and its variants recently demonstrated superior performance in image classification tasks. These models convert input images into smaller patches and utilize the self-attention mechanism to model the relationships between the patches. Self-attention enables ViTs to capture long-range dependencies and model complex relationships between different regions of the image. However, the effectiveness of ViT-based approaches heavily relies on access to large datasets for learning meaningful representations of input images. This is primarily because the architectural design of ViTs does not rely on the same inductive biases in feature extraction which allow CNNs to learn spatially invariant features.Accordingly, numerous prior studies introduced modifications to the original ViT network specifically designed for BUS image classification [13,14,23]. In addition, several works proposed network architectures that combined Transformers and CNNs [4,15,16]. For instance, Mo et al. [15] proposed a hybrid CNN-Transformer incorporating BUS anatomical priors. Qu et al. [16] employed squeeze and excitation blocks to enhance the feature extraction capacity in a hybrid CNN-based VGG16 network and ViT. Similarly, Iqbal et al. [4] designed two hybrid CNN-Transformer networks intended either for classification or segmentation of multi-modal breast cancer images. Despite the promising results of such hybrid approaches, effectively capturing the local patterns and global long-range dependencies in BUS images remains challenging [4,5,24].Multitask learning leverages shared information across related tasks by jointly training the model. It constrains models to learn representations that are relevant to all tasks rather than learning task-specific details. Moreover, multitask learning acts as a regularizer by introducing inductive bias and prevents overfitting [25] (particularly with ViTs), and with that, can mitigate the challenges posed by small BUS dataset sizes. In [3], the authors demonstrated that multitask learning outperforms single-task learning approaches for BUS classification.In this study, we introduce a hybrid multitask approach, Hybrid-MT-ESTAN, which encompasses tumor classification as a primary task and tumor segmentation as a secondary task. Hybrid-MT-ESTAN combines the advantages of CNNs and Transformers in a framework incorporating anatomical tissue information in BUS images. Specifically, we designed a novel attention block named Anatomy-Aware Attention (AAA), which modifies the attention block of Swin Transformer by considering the breast anatomy. The anatomy of the human breast is categorized into four primary layers: the skin, premammary (subcutaneous fat), mammary, and retromammary layers, where each layer has a distinct texture and generates different echo patterns. The primary layers in BUS images are arranged in a vertical stack, with similar echo patterns appearing horizontally across the images. The kernels in the introduced AAA attention blocks are organized in rows and columns to capture the anatomical structure of the breast tissue. In the published literature, the closest approach to ours is the work by Iqbal et al. [4], in which the authors used hybrid single-task CNN-Transformer networks for either classification or segmentation of BUS images. Conversely, Hybrid-MT-ESTAN employs a multitask approach and introduces novel architectural design. The main contributions of this work are summarized as:   [17], which employs row-column-wise kernels to learn and fuse context information in BUS images at different context scales (see Fig. 2). Specifically, each MT-ESTAN block is composed of two parallel branches consisting of four square convolutional kernels and two consecutive row-column-wise kernels. These specialized convolutional kernels effectively extract contextual information of small tumors in BUS images. Refer to [17,22], and [3] for the implementation details of ESTAN and MT-ESTAN. The source codes of these works are available at http://busbench.midalab.net. f l = W-MSA(LN(f l-1 )) + f l-1 (1)where f l and f l are the output features of the MLP module and the (S)W-MSA module for block l, respectively; in the proposed Anatomy-Aware Attention (AAA) block, we redesigned the Swin blocks to enhance their ability to model both global and local features by adding an attention block based on the breast anatomy (see Fig. 3). The additional layers are defined byConcretely, we first reconstruct the i-th feature map (y i ) by merging (M ) all patches, and afterward, we applied average pooling (AVG-P) and max pooling (MAX-P) layers with size (2, 2). The outputs of (AVG-P) and (MAX-P) layers are concatenated and up-sampled (U ) with size (2, 2) and stride (2, 2). Rowcolumn-wise kernels (A) with size (9 , 1) and (1 , 9) are then employed to adapt to the anatomy of the breast, and finally a sigmoid function (σ) is applied to the output of (A) multiplied by the input feature map (y i ). "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.3,Segmentation and Classification Branches/Tasks,"The segmentation branch in Fig. 1 outputs dense mask predictions of BUS tumors. It consists of four Up Blocks, each with three convolutional layers and one upsampling layer (with size (2, 2) and stride (2, 2)). The settings of the convolutional layers are adopted from [3]. In addition, the blocks receive four skip connections from the MT-ESTAN encoder, i.e., there is a skip connection from each MT-ESTAN block 1 to 4. The classification branch consists of three dense layers, a dropout layer (50%), and the final dense layer that predicts the tumor class into benign or malignant."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.4,Loss Function,"We applied a multitask loss function (L mt ) that aggregates two terms: a focal loss L F ocal for the classification task and dice loss L Dice for the segmentation task. Therefore, the composite loss function is L mt = w 1 • L F ocal + L Dice , where the weight coefficient w 1 is set to apply greater importance to the classification task as the primary task. Since in medical image diagnosis achieving high sensitivity places emphasis on the detection of malignant lesions, we employed the focal loss for the classification task to trade off between sensitivity and specificity. Because malignant tumors are more challenging to detect due to greater differences in margin, shape, and appearance in BUS images, focal loss forces the model to focus more on difficult predictions. Specifically, focal loss adds a factor (1 -p i ) γ to the cross-entropy loss where γ is a focusing parameter, resulting inIn the formulation, α is a weighting coefficient, N denotes the number of image samples, t i is the target label of the i th training sample, and p i denotes the prediction. The segmentation loss is calculated using the commonly-employed Dice loss (L Dice ) function."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.1,Datasets,"We evaluated the performance of Hybrid-MT-ESTAN using four public datasets, HMSS [9], BUSI [10], BUSIS [20], and Dataset B [6]. We combined all four datasets to build a large and diverse dataset with a total of 3,320 B-mode BUS images, of which 1,664 contain benign tumors and 1,656 have malignant tumors. Table 1 shows the detailed information for each dataset. HMSS dataset does not provide the segmentation ground-truth masks, and for this study we arranged with a group of experienced radiologists to prepare the masks for HMSS. Refer to the original publications of the datasets for more details. "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.2,Evaluation Metrics,"For performance evaluation of the classification task, we used the following metrics: accuracy (Acc), sensitivity (Sens), specificity (Spec), F1 score, Area Under the Curve of Receiver Operating Characteristic (AUC), false positive rate (FPR), and false negative rate (FNR). To evaluate the segmentation performance, we used dice similarity coefficient (DSC) and Jaccard index (JI)."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.3,Implementation Details,"The proposed approach was implemented with Keras and TensorFlow libraries.All experiments were performed on a machine with NVIDIA Quadro RTX 8000 GPUs and two Intel Xeon Silver 4210R CPUs (2.40GHz) with 512 GB of RAM. All BUS images in the dataset were zero-padded and reshaped to form square images. To avoid data leakage and bias, we selected the train, test, and validation sets based on the cases, i.e., the images from one case (patient) were assigned to only one of the training, validation, and test sets. Furthermore, we employed horizontal flip, height shift (20%), width shift (20%), and rotation (20 • C) for data augmentation. The proposed approach utilizes the building blocks of ResNet50 and Swin-Transformer-V2, pretrained on ImageNet dataset. Namely, MT-ESTAN uses pretrained ResNet50 as a base model for the five encoder blocks (the implementation details of MT-ESTAN can be found in [3]). The encoder with AAA blocks uses the SwinTransformer V2 Base 256 pretrained model as a backbone. For the composite loss function, we adopted a weight coefficient w 1 = 3, and in the focal loss α = 0.5 and γ = 2. For model training we utilized Adam optimizer with a learning rate of 10 -5 and mini batch size of 4 images."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.4,Performance Evaluation and Comparative Analysis,"We compared the performance of Hybrid-MT-ESTAN for BUS classification to nine deep learning approaches commonly used for medical image analysis. The compared models include CNN-based, ViT-based, and hybrid approaches. CNNbased networks are SHA-MTL [8], MobileNet [19], DenseNet121 [7], and EMT-Net [12]. ViT-based approaches include the original ViT [5], Chowdery [10], and Swin Transformer [18]. VGGA-ViT [16] is a hybrid CNN-Transformer network. The values of the performance metrics are shown in Table 2, indicating that the proposed Hybrid-MT-ESTAN outperformed all nine approaches by achieving the best accuracy, sensitivity, F1 score, and AUC with 82.8%, 86.4%, 86.0%, and 82.8%, respectively. Although SHA-MTL [8] obtained the highest specificity of 90.8% and FNR of 9.2%, the trade-off between sensitivity and specificity should be taken into consideration, as that approach had sensitivity of 48.1%. The preferred trade-off in medical image analysis typically is high sensitivity without significant degradation in specificity.We evaluated the segmentation performance of Hybrid MT-ESTAN and compared the results to five multitask approaches, including SHA-MTL [8], EMT-Net [12], Chowdery [10], MT-ESTAN [3], and VGGA-ViT [16]. As shown in Table 2,the proposed Hybrid MT-ESTAN achieved the highest performance and increased DSC and JI by 5.9% and 6.4%, respectively compared to MT-ESTAN. Note that results of single-task models in Table 2 are not provided."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.5,Effectiveness of the Anatomy-Aware Attention (AAA) Block,"To verify the effectiveness of the Anatomy-Aware Attention (AAA) block, we conducted an ablation study that quantified the impact of the different components in Hybrid-MT-ESTAN on the classification and segmentation performance. Table 3 presents the values of the performance metrics for MT-ESTAN (pure CNN-based approach), Swin Transformer (pure Transformer network), a hybrid architecture of MT-ESTAN and Swin Transformer, and our proposed Hybrid-MT-ESTAN with AAA block. According to the results in Table 3, MT-ESTAN achieved better sensitivity and F1 score than Swin Transformer, with 83.7% and 83%, respectively. The hybrid architectures of MT-ESTAN with Swin Transformer improved the classification performance and has higher accuracy, sensitivity, F1 score, and AUC with 80.3%, 84.2%, 83%, and 80.2%, compared to MT-ESTAN and Swin Transformer individually. The proposed approach, Hybrid-MT-ESTAN with AAA block, further improved accuracy, sensitivity, F1 score, and AUC by 2.5%, 2.2%, 3%, and 2.6%, respectively, relative to the hybrid model without the AAA block.To evaluate the segmentation performance, we compared the proposed approach with and without the AAA block and Swin Transformer. As shown in Table 3, MT-ESTAN combined with Swin Transformer improved DSC and JI by 4.1% and 4.3%, respectively compared to MT-ESTAN. Employing the proposed AAA block further improved DSC and JI by 1.8% and 2.1%, respectively."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,4.0,Conclusion,"In this paper, we introduced the Hybrid-MT-ESTAN, a multitask learning approach for BUS image analysis that alleviates the lack of global contextual infor-mation in the low-level layers of CNN-based approaches. Hybrid-MT-ESTAN concurrently performs BUS tumor classification and segmentation, with a hybrid architecture that employs CNN-based and Swin Transformer layers. The proposed approach exploits multi-scale local patterns and global long-range dependencies provided by MT-ESTA and AAA Transformer blocks for learning feature representations, resulting in improved generalization. Experimental validation demonstrated significant performance improvement by Hybrid-MT-ESTAN in comparison to current state-of-the-art models for BUS classification."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,1.0,Introduction,"Large open-access medical datasets are integral to the future of deep learning (DL) in medicine because they provide much-needed training data and a method of public comparison between researchers [20]. Researchers often curate their data for DL models; yet, even the selection process itself may contain inherent biases, confounding factors, and other ""hidden"" issues that cause failure on real clinical data [1]. In DL, out-of-distribution (OOD) generalizability refers to a model's ability to maintain its performance on data that is independent of the model's development [23]. OOD generalizability represents a critical issue in DL research since the point of artificial intelligence (AI) in medicine is to be capable of handling new patient cases. However, this important aspect of DL is often not considered. On the other hand, overcoming challenges such as scanner-induced variance are critical in the success of neuroimaging studies involving AI [5].We hypothesize that adaptable domain-aware model calibration that combines expert-level and data-level knowledge can effectively generalize to OOD data. DL calibration is correlated with better OOD generalizability [21]. Calibrated models may accomplish this by learning less spurious connections between features and classes. This observation relates to how calibrated models reflect the true likelihood of a data point for a class. A calibrated model may let a confusing data point naturally lay closer to the class boundaries, rather than forcing tight decision boundaries that over-fit points. Calibration affects decision-making such that the models can better detect and handle OOD data [19].In this work, we introduce DOMINO++, an adaptable regularization framework to calibrate DL models based on expert-guided and data-guided knowledge. DOMINO++ builds on the work DOMINO [18] with three important contributions: 1) combining expert-guided and data-guided regularization to fully exert the domain-aware regularization's potential. 2) Instead of using static scaling, DOMINO++ dynamically brings the domain-aware regularization term to the same order of magnitude as the base loss across epochs. 3) DOMINO++ adopts an adaptive regularization scheme by weighing the domain-aware regularization term in a progressive fashion. The strengths of DOMINO++'s regularization lie in its ability to take advantage of the benefits of both the semantic confusability derived from domain knowledge and data distribution, as well as its adaptive balance between the data term and the regularization strength. This work shows the advantages of DOMINO++ in a segmentation task from magnetic resonance (MR) images. DOMINO++ is tested in OOD datasets including synthesized noise additions, synthesized rotations, and a different MR scanner.2 Dynamic Framework for DL Regularization"
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.1,DL Backbone,"U-Net transformer (UNETR) [10] serves as the DL backbone. UNETR is inspired by the awe-inspiring results of transformer modules in Natural Language Processing [22]. These modules use self-attention-based mechanisms to learn language range sequences better than traditional fully convolutional networks (FCNs). UNETR employs a transformer module as its encoder, whereas its decoder is an FCN like in the standard U-Net. This architecture learns threedimensional (3D) volumes as sequences of one-dimensional (1D) patches. The FCN decoder receives the transformer's global information via skip connections and concatenates this information with local context that eventually recovers the original image dimensions. The baseline model does not include advanced calibration. However, basic principles to improve OOD generalizability are still incorporated for a more meaningful comparison. These principles include standard data augmentations like random Gaussian noise, rotations along each axis, and cropping. The model includes 12 attention heads and a feature size of 16."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.2,DOMINO++ Loss Regularization,"Derivation. The original DOMINO's loss regularization is as follows:where L can be any uncalibrated loss function (e.g., DiceCE which is a hybrid of cross-entropy and Dice score [12]). y and ŷ are the true labels and model output scores, respectively. β is an empirical static regularization rate that ranges between 0-1, and s is a pre-determined fixed scaling factor to balance the data term and the regularization term. The penalty matrix W has dimensions N × N , where N is the number of classes. W HC and W CM represent the hierarchical clustering (HC)-based and confusion matrix (CM)-based penalty matrices. We improved its loss function to DOMINO++'s dual-guidance penalty matrix with adaptive scaling and regularization rate as follows:where β dynamically changes over epochs. s is adaptively updated to balance the data and regularization terms. W HCCM is the dual-guidance penalty matrix.Combining Expert-Guided and Data-Guided Regularization. DOMINO-HC regularizes classes by arranging them into hierarchical groupings based on domain. DOMINO-HC is data-independent and thus immune to noise. Yet, it becomes less useful without clear hierarchical groups. DOMINO-CM calculates class penalties using the performance of an uncalibrated model on a held-out dataset. The CM method does not require domain knowledge, but it can be more susceptible to messy data. Overall, DOMINO-HC is expert-crafted and DOMINO-CM is data-driven. These approaches have complementary advantages and both perform very well on medical image segmentation [18]. Hence, this work combines these methods to learn from experts and data. Fig. 1 shows the process for creating this matrix term.The combined regulation (a.k.a. DOMINO-HCCM) requires first replicating DOMINO-HC. For this step, we recreate the exact hierarchical groupings from the DOMINO paper [18]. A confusion matrix is generated using DOMINO-HC on an additional validation set for matrix penalty. Next, the confusion matrix is normalized by the number of true pixels in each class. The normalized terms are subtracted from the identity matrix. Finally, all diagonals are set to 0's. Next, a second DL model trains using the resulting penalty matrix in its regularization. This process differs from DOMINO-CM because DOMINO-HC was used to generate the final model's matrix penalty. The uncalibrated model may produce a matrix penalty that is susceptible to variable quality depending on the model's training data. In comparison, the initial regularization term adds an inductive bias in the first model that encodes more desirable qualities about the class mappings [13]. Namely, the initial model contains information about the hierarchical class groupings that drives the generation of the second model's matrix penalty. The final model can now use a regularization term that is more based on task than dataset. Figure 2 displays the final DOMINO++-HCCM matrix. Dynamic Scaling Term. DOMINO++ adds a domain-aware regularization term to any standard loss. The resulting loss function combines the standard loss's goal of increasing accuracy with DOMINO++'s goal of reweighing the importance of different class mix-ups when incorrect. DL models are at risk of being dominated by a specific loss during training if the losses are of different scales [14]. DOMINO [18] neglects to account for this and provides a static scaling for the regularization term based on the first epoch standard loss. In comparison, DOMINO++ updates the scaling on the regularization term to be within the same scale as the current epoch standard loss. Specifically, the scaling is computed based on the closest value to the baseline loss on the log scale. For example, an epoch with L = 13 will have a scaling factor S = 10."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.3,Adaptive Regularization Weighting,"Multiple loss functions must be balanced properly [6]. Most studies linearly balance the separate loss terms using hyper-parameters. Hyper-parameter selection is nontrivial and can greatly alter performance. Indeed, the timing of regularization during training is critical to the final performance [8]. Hence, the current work investigates the role of regularization timing in the final model performance. Equation 2 is similar to the original DOMINO equation [18]; however, the equation is modified to include a weighting term (e.g., 1 -β) on the standard loss function. In DOMINO, the β term was simply set at a constant value of 1. As shown in Eq. 3, DOMINO++ weighs the loss regularization to decay (β) across epochs, while the standard loss is scaled reversely with regard to the regularization term (see Eq. 2).3 Experiments and Results Reference Segmentations. The T1 MRIs are segmented into 11 different tissues, which include grey matter (GM), white matter (WM), cerebrospinal fluid (CSF), eyes, muscle, cancellous bone, cortical bone, skin, fat, major artery (blood), and air. Trained labelers performed a combination of automated segmentation and manual correction. Initially, base segmentations for WM, GM, and bone are obtained using Headreco [16], while air is generated in the Statistical Parametric Mapping toolbox [2]. Afterward, these automated outputs are manually corrected using ScanIP Simpleware TM . Thresholding and morphological operations are employed to differentiate between the bone compartments. Eyes, muscle, skin, fat, and blood are manually segmented in Simpleware. Finally, CSF is generated by subtracting the ten other tissues from the whole head.Out-of-Domain (OOD) Testing Data Most DL work selects a testing set by splitting a larger dataset into training and testing participants. This work also incorporates ""messy"" or fully independent data. Thus, three additional testing datasets are used along with the traditional testing data (Site A -Clean). Site A Noisy -MRI noise may be approximated as Gaussian for a signal-tonoise ratio (SNR) greater than 2 [9]. Therefore, this work simulates noisy MRI images using Gaussian noise of 0 mean with a variance of 0.01.Site A Rotated -Rotated MRI data simulates other further disturbances or irregularities (e.g., head tilting) during scanning. The rotation dataset includes random rotation of 5-to 45 • clockwise or counter-clockwise with respect to each 3D axis. The rotation angles are based on realistic scanner rotation [15].Site B -Site A uses a 64-channel head coil and Site B uses a 32-channel head coil. The maximum theoretical SNR of an MRI increases with the number of channels [17]. Hence, this work seeks to test the performance of a model trained exclusively on a higher channel scanner on a lower channel testing dataset. Thus, the Site A data serves as the exclusive source of the training and validation data, and Site B serves as a unique and independent testing dataset."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.2,Implementation Details,"This study implements UNETR using the Medical Open Network for Artificial Intelligence (MONAI-1.1.0) in Pytorch 1.10.0 [4]. The Site A data is split from 123 MRIs into 93 training/10 validation/10 held-out validation (matrix penalty)/10 testing. 10 images from Site B serve as an additional testing dataset. Each DL model requires 1 GPU, 4 CPUs, and 30 GB of memory. Each model is trained for 25,000 iterations with evaluation at 500 intervals. The models are trained on 256 × 256 × 256 images with batch sizes of 1 image. The optimization consists Adam optimization using stochastic gradient descent. All models segment a single head in 3-4 s during inference."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.3,Analysis Approach,"This section compares the results of 11 tissue head segmentation on each of the datasets using the baseline model, the best performing DOMINO approach, and the best performing DOMINO++ approach. The results are evaluated using Dice score [3] and Hausdorff Distance [7,11]. The Dice score represents the overlap of the model outputs with the true labels. It is better when greater and is optimally 1. Hausdorff distance represents the distance between the model outputs with the true labels. It is better when lesser and is optimally 0."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.4,Segmentation Results,"Qualitative Comparisons. Figure 3 features segmentation of one example slice from the Site A MRI dataset with Gaussian Noise. DOMINO substantially exaggerates the blood regions in this slice. In addition, DOMINO entirely misses a section of white matter near the eyes. However, DOMINO can also capture certain regions of the white matter, particularly in the back of the head, better than the baseline model. In general, all outputs have noisy regions where there appear to be ""specks"" of an erroneous tissue. For instance, grey matter is incorrectly identified as specks within the white matter. This issue is far more common in the DOMINO output compared to the baseline or DOMINO++ outputs.Quantitative Comparisons. Tables 1 and2 show that DOMINO++ achieves the best Dice scores and Hausdorff Distances across all test sets, respectively. As such, DOMINO++ produces the most accurate overall segmentation across tissue types. The supplementary material provides individual results across every dataset and tissue type. So far, DOMINO++ improves the model generalizability to the noisy and rotated datasets the most. These improvements are important in combating realistic MR issues such as motion artifacts. Future work will  DOMINO++ performs better in most tissues and the overall segmentation. GM, cortical bone, and blood show the most significant differences with DOMINO++. This is highly relevant to T1 MRI segmentation. Bone is difficult to differentiate from CSF with only T1 scans due to similar contrast. Available automated segmentation tools use young adult heads as reference, whereas the bone structure between older and younger adults is very different (e.g., more porous in older adults). Hence, DOMINO++ is an important step in developing automated segmentation tools that are better suited for older adult heads. Training Time Analysis. DOMINO-HC took about 12 h to train whereas DOMINO-CM and DOMINO++ took about 24 h to train. All models took 3-4 s per MR volume at the inference time. A task that has very clear hierarchical groups may still favor DOMINO-HC for the convenient training time. This might include a task with well-documented taxonomic levels (e.g., animal classification). However, medical data is often not as clear, which is why models that can learn from the data are valuable. DOMINO++ makes up for the longer training time by learning more specific class similarities from the data. Tasks that benefit from DOMINO++ over DOMINO-HC are those that only have loosely-defined categories. Tissue segmentation falls under this domain because tissues largely occur in similar anatomical locations (strength of DOMINO-HC) but the overall process is still variable with individual heads (strength of DOMINO-CM)."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,4.0,Conclusions,"DOMINO [18] established a framework for calibrating DL models using the semantic confusability and hierarchical similarity between classes. In this work, we proposed the DOMINO++ model which builds upon DOMINO's framework with important novel contributions: the integration of data-guided and expertguided knowledge, better adaptability, and dynamic learning. DOMINO++ surpasses the equivalent uncalibrated DL model and DOMINO in 11-tissue segmentation on both standard and OOD datasets. OOD data is unavoidable and remains a pivotal challenge for the use of artificial intelligence in clinics, where there is great variability between different treatment sites and patient populations. Overall, this work indicates that DOMINO++ has great potential to improve the trustworthiness and reliability of DL models in real-life clinical data.We will release DOMINO++ code to the community to support open science research."
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 1 .,* 
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 2 .,* 
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 3 .,*   
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is a core step for quantitative and precision medicine. In the past decade, Convolutional Neural Networks (CNNs) became the SOTA method to achieve accurate and fast medical image segmentation [10,12,21]. nn-UNet [12], which is based on UNet [21], has achieved top performances on over 20 medical segmentation challenges. Parallel to manually created networks such as nn-UNet, DiNTS [10], a CNN designed by automated neural network search, also achieved top performances in medical segmentation decathlon (MSD) [1] challenges. The convolution operation in CNN provides a strong inductive bias which is translational equivalent and efficient in capturing local features like boundary and texture. However, this inductive bias limits the representation power of CNN models which means a potentially lower performance ceiling on more challenging tasks [7]. Additionally, CNN has a local receptive field and are not able to capture long-range dependencies unlike transformers. Recently, vision transformers have been proposed, which adopt the transformers in natural language processing by splitting images into patches (tokens) [6], and use self-attention to learn features. The self-attention mechanism enables learning longrange dependencies between far-away tokens. This is intriguing and numerous works have been proposed to incorporate transformer attentions into medical image segmentation [2,3,9,23,24,30,32,35]. Among them, SwinUNETR [23] has achieved the new top performance in the MSD challenge and Beyond the Cranial Vault (BTCV) Segmentation Challenge by pretraining on large datasets. It has a U-shaped structure where the encoder is a Swin-Transformer [16].Although transformers have achieved certain success in medical imaging, the lack of inductive bias makes them harder to be trained and requires much more training data to avoid overfitting. The self-attentions are good at learning complicated relational interactions for high-level concepts [5] but are also observed to be ignoring local feature details [5]. Unlike natural image segmentation benchmarks, e.g. ADE20k [34], where the challenge is in learning complex relationships and scene understanding from a large amount of labeled training images, many medical image segmentation networks need to be extremely focused on local boundary details while less in need of highlevel relationships. Moreover, the number of training data is also limited. Hence in real clinical studies and challenges, CNNs can still achieve better results than transformers. For example, the top solutions in the last year MICCAI challenges HECTOR [19], FLARE [11], INSTANCE [15,22] and AMOS [13] are all CNN based. Besides lacking inductive bias and enough training data, one extra reason could be that transformers are computationally much expensive and harder to tune. More improvements and empirical evidence are needed before we say transformers are ready to replace CNNs for medical image segmentation.In this paper, we try to develop a new ""to-go"" transformer for 3D medical image segmentation, which is expected to exhibit strong performance under different data situations and does not require extensive hyperparameter tuning. SwinUNETR reaches top performances on several large benchmarks, making itself the current SOTA, but without effective pretraining and excessive tuning, its performance on new datasets and challenges is not as high-performing as expected.A straightforward direction to improve transformers is to combine the merits of both convolutions and self-attentions. Many methods have been proposed and most of them fall into two directions: 1) a new self-attention scheme to have convolutionlike properties [5,7,16,25,26,29]. Swin-transformer [16] is a typical work in the first direction. It uses a local window instead of the whole image to perform self-attention. Although the basic operation is still self-attention, the local window and relative position embedding give self-attention a conv-like local receptive field and less computation cost. Another line in 1) is changing the self-attention operation directly. CoAtNet [5] unifies convolution and self-attention with relative attention, while ConViT [7] uses gated positional self-attention which is equipped with a soft convolutional inductive bias. Works in the second direction 2) employs both convolution and self-attention in the network [3,4,8,20,27,28,30,31,33,35]. For the works in this direction, we sum-marize them into three major categories as shown in Fig. 1: 2.a) dual branch feature fusion. MobileFormer [4], Conformer [20], and TransFuse [33] use a CNN branch and a transformer branch in parallel to fuse the features, thus the local details and global features are learned separately and fused altogether. However, this doubles the computation cost. Another line of works 2.b) focuses on the bottleneck design. The low-level features are extracted by convolution blocks and the bottleneck is the transformer, like the TransUNet [3], Cotr [30] and TransBTS [27]. The third direction 2.c) is a new block containing both convolution and self-attention. MOAT [31] removes the MLP in self-attention and uses a mobile convolution block at the front. The MOAT block is then used as the basic block in building the network. CvT [28] uses convolution as the embedding layer for key, value, and query. nnFormer [35] replaces the patch merging with convolution with stride. Although those works showed strong performances, which works best and can be the ""to go"" transformer for 3D medical image segmentation is still unknown. For this purpose, we design the SwinUNETR-V2, which improves the current SOTA Swi-nUNETR by introducing stage-wise convolutions into the backbone. Our network belongs to the second category, which employs convolution and self-attention directly. At each resolution level, we add a residual convolution (ResConv) block at the beginning, and the output is then used as input to the swin transformer blocks (contains a swin block and a shifted window swin block). MOAT [31] and CvT [28] add convolution before self-attention as a micro-level building block, and nnFormer has a similar design that uses convolution with stride to replace the patch merging layer for downsampling. Differently, our work only adds a ResConv block at the beginning of each stage, which is a macro-network level design. It is used to regularize the features for the following transformers. Although simple, we found it surprisingly effective for 3D medical image segmentation. The network is evaluated extensively on a variety of benchmarks and achieved top performances on the WORD [17], FLARE2021 [18], MSD prostate, MSD lung cancer, and MSD pancreas cancer datasets [1]. Compared to the original Swin-UNETR which needs extensive recipe tuning on a new dataset, we utilized the same training recipe with minimum changes across all benchmarks, showcasing the straightforward applicability of SwinUNETR-V2 to reach state-of-the-art without extensive hyperparameter tuning or pretraining. We also experimented with four design variations inspired by existing works to justify the SwinUNETR-V2 design."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,2.0,Method,"Our SwinUNETR-V2 is based on the original SwinUNETR, and we focus on the transformer encoder. The overall framework is shown in Fig. 2."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Fig. 2. The SwinUNETR-V2 architecture,"Swin-Transformer. We briefly introduce the 3D swin-transformer as used in Swin-UNETR [23]. A patch embedding layer of 3D convolution (stride = 2,2,2, kernel size = 2,2,2) is used to embed the patch into tokens. Four stages of swin transformer block followed by patch merging are used to encode the input patches. Given an input tensor z i of size (B, C, H, W, D) at swin block i, the swin transformer block splits the tensor into ( H/M , W/M , D/M windows. It performs four operationsW-MSA and SW-MSA represent regular window and shifted window multi-head selfattention, respectively. MLP and LN represent multilayer perceptron and layernorm, respectively. A patch merging layer is applied after every swin transformer block to reduce each spatial dimension by half.Stage-Wise Convolution. Although Swin-transformer uses local window attention to introduce inductive bias like convolutions, self-attentions can still mess up with the local details. We experimented with multiple designs as in Fig. 3 and found that interleaved stage-wise convolution is the most effective for swin: convolution followed by swin blocks, then convolution goes on. At the beginning of each resolution level (stage), the input tokens are reshaped back to the original 3D volumes. A residual convolution (ResConv) block with two sets of 3 × 3x3 convolution, instance normalization, and leaky relu are used. The output then goes to a set of following swin transformer blocks (we use 2 in the paper). There are in total 4 ResConv blocks at 4 stages. We also tried inverted convolution blocks with depth-wise convolution like MOAT [31] or with original 3D convolution, they improve the performance but are worse than the ResConv block.Decoder. The decoder is the same as SwinUNETR [23], where convolution blocks are used to extract outputs from those four swin blocks and the bottleneck. The extracted features are upsampled by deconvolutional layers and concatenated with features from a higher-resolution level(long-skip connection). A final convolution with 1 × 1 × 1 kernel is used to map features to segmentation maps."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,3.0,Experiments,"We use extensive experiments to show its effectiveness and justify its design for 3D medical image segmentation. To make fair comparisons with baselines, we did not use any pre-trained weights.Datasets. The network is validated on five datasets of different sizes, targets and modalities:1) The WORD dataset [17]  The challenge comes from segmenting small tumors from large full 3D CT images. The pancreas dataset contains 281 3D CT scans with annotated pancreas and tumors (or cysts). The challenge is from the large label imbalances between the background, pancreas, and tumor structures. For all three MSD tasks, we perform 5-fold crossvalidation with 70%/10%/20% train, validation, and test splits. These 20% test data will not overlap with other folds and cover all data by 5 folds."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Implementation Details,"The training pipeline is based on the publicly available SwinUNETR codebase (https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BT CV, our training recipe is the same as that by SwinUNETR). We changed the initial learning rate to 4e-4, and the training epoch is adapted to each task such that the total training iteration is about 40k. Random Gaussian smooth, Gaussian noise, and random gamma correction are also added as additional data augmentation. There are differences in data preprocessing across tasks. MSD data are resampled to 1 × 1x1 mm resolution and normalized to zero mean and standard deviation (CT images are firstly clipped by .5% and 99.5% foreground intensity percentile). For WORD and FLARE preprocessing, we use the default transforms in SwinUNETR codebase (https://github. com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BTCV, our training recipe is the same as that by SwinUNETR) and 3D UXNet codebase (see footnote 1). Besides these, all other training hyperparameters are the same. We only made those minimal changes for different tasks and show surprisingly good generalizability of the SwinUNETR-V2 and the pipeline across tasks. "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Results,"WORD Result. We follow the data split in [17] and report the test scores. All the baseline scores are from [17] except nnFormer and SwinUNETR. To make a fair comparison, we didn't use any test-time augmentation or model ensemble. The test set dice      1 and Table 2. We don't have the original baseline results for statistical testing (we reproduced some baseline results but the results are lower than reported), so we report the standard deviation of our methods. SwinUNETR has 62.5M parameters/295 GFlops and SwinUNETR-V2 has 72.8M parameters/320 GFlops. The baseline parameters/flops can be found in [14].FLARE 2021 Result. We use the 5-fold cross-validation data split and baseline scores from [14]. Following [14], the five trained models are evaluated on 20 held-out test scans, and the average dice scores (not model ensemble) are shown in Table 3. We can see our SwinUNETR-V2 surpasses all the baseline methods by a large margin."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,MSD Results.,"For MSD datasets, we perform 5-fold cross-validation and ran the baseline experiments with our codebase using exactly the same hyperparameters as mentioned. nnunet2D/3D baseline experiments are performed using nnunet's original codebase 2 since it has its own automatic hyperparameter selection. The test dice score and standard deviation (averaged over 5 fold) are shown in Table 4. We did not do any postprocessing or model ensembling, thus there can be a gap between the test values and online MSD leaderboard values. We didn't compare with leaderboard results because the purpose of the experiments is to make fair comparisons, while not resorting to additional training data/pretraining, postprocessing, or model ensembling."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Variations of SwinUNetR-V2,"In this section, we investigate other variations of adding convolutions into swin transformer. We follow Fig. 1   [31] work. 4) Swin-Var-Down: the patch merging is replaced by convolution with stride 2 like nnFormer [35]. We perform the study on the WORD dataset, and the mean test Dice and HD95 scores are shown in Table 5. We can see that adding convolution at different places does affect the performances, and the SwinUNETR-V2 design is the optimal on WORD test set."
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,4.0,Discussion and Conclusion,"In this paper, we propose a new 3D medical image segmentation network SwinUNETR-V2. For some tasks, we found the original SwinUNETR with pure transformer backbones (or other ViT-based models) may have inferior performance and training stability than CNNs. To improve this, our core intuition is to combine convolution with window-based self-attention. Although existing window-based attention already has a convolution-like inductive bias, it is still not good enough for learning local details as convolutions. We tried multiple combination strategies as in Table 5 and found our current design most effective. By only adding one ResConv block at the beginning of each resolution level, the features can be well-regularized while not too constrained by the convolution inductive bias, and the computation cost will not increase by a lot. Extensive experiments are performed on a variety of challenging datasets, and SwinUNETR-V2 achieved promising improvements. The optimal combination of swin transformer and convolution still lacks a clear principle and theory, and we can only rely on trial and error in designing new architectures. We will apply the network to active challenges for more evaluation."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,1.0,Introduction,"Automatic mitochondria segmentation from electron microscopy (EM) volume is a fundamental task, which has been widely applied to basic scientific research and R. Sun and H. Mai-Equal Contribution.clinical diagnosis [1,12,13,15,17]. Recent works like [6,7,16,21,22] have achieved conspicuous achievements attributed to the development of deep neural networks [10,23,26,27]. However, these approaches tend to suffer from severe performance degradation when evaluated on target volume (i.e., target domain) that are sampled from a different distribution (caused by different devices used to image different organisms and tissues) compared to that of training volume (i.e., source volume/domain). Therefore, how to alleviate this gap to empower the learned model generalization capability is very challenging.In this work, we tackle the unsupervised domain adaptive (UDA) problem, where there are no accessible labels in the target volume. To alleviate this issue, representative and competitive methods [5,8,18,19,29,30] attempt to align feature distribution from source and target domains via adversarial training in a pixel-wise manner, that is, learning domain-invariant features against the substantial variances in data distribution. However, after an in-depth analysis of adversarial training, we find two key ingredients lacking in previous works. (1) Structure-entangled feature. In fact, there exists a large variation in the complex mitochondrial structure from different domains, as proven in [28]. However, previous methods only focus on domain gap either in each individual section or in adjacent sections, and neglect the differences in morphology and distribution (i.e., structure) of long-range sections, leading to sub-optimal results in the form of hard-to-align features. (2) Noisy discrimination. Intuitively, humans can quickly distinguish domain of images with the same categories from cluttered backgrounds by automatically decomposing the foreground into multiple local parts, and then discriminate them in a fine-grained manner. Inspired by this, we believe that during adversarial learning, relying solely on context-limited pixel-level features to discriminate domains will inevitably introduce considerable noise, considering that the segmentation differences between the source and target domain are usually in local parts (e.g., boundary). Thus, it is highly desirable to make full use of the context information to merge the appropriate neighboring pixel features to construct a part-level discriminator.In this paper, we propose a Structure-decoupled Adaptive Part Alignment Network (SAPAN) including a structure decoupler and a part miner for robust mitochondria segmentation. (1) In the structure decoupler, we draw inspiration from [25] to model long-range section variation in distribution and morphology (i.e., structural information), and decouple it from features in pursuit of domain invariance. In specific, we first prepend a spatial smoothing mechanism for each pixel in the current section to seek the corresponding location of other sections to attain the smoothed features, which are subsequently modulated to obtain decoupled features with easy-to-align properties. (2) Then, we devise a part miner as discriminator, which can dynamically absorb the suitable pixels to aggregate diverse parts against noise in an adaptive manner, thus the detailed local differences between the source and target domain can be accurately discriminated. Extensive experimental results on four challenging benchmarks demonstrate that our method performs favorably against SOTA UDA methods."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,2.0,Related Work,"Domain adaptation in EM volume [5,8,18,19,29,30] has attracted the attention of more and more researchers due to the difficulty in accessing manual annotation. In [29], they employ self-training paradigm while in [18] adversarial training is adopted and performs better. To further improve the domain generalization, [5] considering the inter-section gap. However, those methods neglect the differences in morphology and distribution (i.e., structure) of long-range sections. In addition, existing adversarial training [4,5,18] adopt context-limited pixel-wise discriminator leading to sub-optimal results. Fig. 1. Overview of our method. There are two main modules in SAPAN, i.e., the structure decoupler for decoupling the feature from the domain-specific structure information and the part miner for adaptively discovering different parts."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.1,Overview,"The domain adaptive mitochondria segmentation task aims at learning an accurate segmentation model in the target domain based on a labeled source domain EM volumeAs shown in Fig. 1, given an EM section X i ∈ R H×W (omit the superscript S/T for convenience) from either domain, the encoder of U-Net [20] first extracts its feature map F i ∈ R h×w×C , where h, w and C denote the height, width and channel number of the feature map respectively. A structure decoupler is applied to decouple the extracted feature map from domain-specific structure knowledge, which involves a channel-wise modulation mechanism. Subsequently, the decoupled feature map F i is fed into the decoder of U-Net which outputs prediction of the original size. In the end, we design a part miner to dynamically divide foreground and background into diverse parts in an adaptive manner, which are utilized to facilitate adversarial training. The details are as follows."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.2,Structure Decoupler,"In order to bridge the gap in mitochondrial structure between domains, a structure decoupler is designed to decouple the extracted feature map from domainspecific structure information, realized by a channel-wise modulation mechanism.To better model the structure information, we first apply attention-based spatial smoothing for adjacent sections. Concretely, given feature maps of adjacent sections F i and F i+1 , we define the spatial smoothing S i+1 (•) w.r.t F i+1 as: each pixel f i,j ∈ R 1×C (j = 1, 2, ..., hw) in feature map F i as query, the feature map of adjacent section F i+1 ∈ R h×w×C as keys and values. Formally,where the T refers to the matrix transpose operation and the √ C is a scaling factor to stabilize training. We compute the structure difference D i ∈ R h×w×C between F i and its adjacent F i+1 and F i-1 by:(The final structure embedding e ∈ R 1×C for each domain is calculated by exponential momentum averaging batch by batch:where GAP(•) denotes the global average pooling, B denotes the batch size and θ ∈ [0, 1] is a momentum coefficient. In this way, e S and e T condense the structure information for the whole volume of the corresponding domain. To effectively mitigate the discrepancy between different domains, we employ channel-wise modulation to decouple the feature from the domain-specific structure information. Taking source domain data F S i as example, we first produce the channel-wise modulation factor γ S ∈ R 1×C and β S ∈ R 1×C conditioned on e S :where G γ (•) and G β (•) shared by the source domain and target domain, are implemented by two linear layers and an activation layer. Then the decoupled source feature can be obtained by:where denotes element-wise multiplication. The decoupled target feature F T i can be acquired in a similar way by Eq. ( 4) and Eq. ( 5). Subsequently, the structure decoupled feature is fed into the decoder of U-Net, which outputs the foreground-background probability map Y i ∈ R 2×H×W . The finally prediction Y i ∈ R H×W can be obtained through simply apply argmax(•) operation on Y i ."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.3,Part Miner,"As far as we know, a good generator is inseparable from a powerful discriminator.To inspire the discriminator to focus on discriminative regions, we design a part miner to dynamically divide foreground and background into diverse parts in an adaptive manner, which are classified by the discriminator D part subsequently.To mine different parts, we first learn a set of part filters P = {p k } 2K k=1 , each filter p k is represented as a C-dimension vector to interact with the feature map F (omit the subscript i for convenience). The first half {p k } K k=1 are responsible for dividing the foreground pixels into K groups and vice versa. Take the foreground filters for example. Before the interaction between p k and F, we first filter out the pixels belonging to the background using downsampled prediction Y . Then we get K activation map A i ∈ R h×w by multiplying p k with the masked feature map:In this way, the pixels with a similar pattern will be highlighted in the same activation map. And then, the foreground part-aware prototypes P = { p k } K k=1 can be got by:Substituting Y with (1 -Y ), we can get the background part-aware prototypes P = { p k } 2K k=K+1 in the same manner."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.4,Training Objectives,"During training, we calculate the supervise loss with the provided label Y S i of the source domain by:where the CE(•, •) refers to the standard cross entropy loss.Considering the part-aware prototypes may focus on the same, making the part miner degeneration, we impose a diversity loss to expand the discrepancy among part-aware prototypes. Formally,where the cos(•, •) denotes cosine similarity between two vectors. The discriminator D part takes p k as input and outputs a scalar representing the probability that it belongs to the target domain. The loss function of D part can be formulated as: As a result, the overall objective of our SAPAN is as follows:where the λ div and λ part are the trade-off weights. The segmentation network and the D part are trained alternately by minimizing the L and the L part , respectively."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.1,Dataset and Evaluation Metric,"Dataset. We evaluate our approach on three widely used EM datasets: the VNC III [3] dataset, the Lucchi dataset [9] and the MitoEM dataset [28]. These datasets exhibit significant diversity making the domain adaptation task challenging. The VNC III [3]  Metrics. Following [5,29], four widely used metrics are used for evaluation, i.e., mean Average Precision (mAP), F1 score, Mattews Correlation Coefficient (MCC) [14] and Intersection over Union (IoU).   "
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.2,Implementation Details,"We adopt a five-stage U-Net with feature channel number of [16,32,48,64,80].During training, we randomly crop the original EM section into 512 × 512 with random augmentation including flip, transpose, rotate, resize and elastic transformation. All models are trained 20,000 iterations using Adam optimizer with batch size of 12, learning rate of 0.001 and β of (0.9, 0.99). The λ div , λ part and K are set as 0.01, 0.001 and 4, respectively."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.3,Comparisons with SOTA Methods,"Two groups of experiments are conducted to verify the effectiveness of our method. Note that ""Oracle"" means directly training the model on the target domain dataset with the ground truth and ""NoAdapt"" means training the model only using the source domain dataset without the target domain dataset. Table 1 and Table 2 present the performance comparison between our method and other competitive methods. Due to the similarity between time series (video) and space series (EM volume), the SOTA domain adaptive video segmentation method [4] is also compared. We consistently observe that our method outperforms all other models on two groups of experiments, which strongly proves the effectiveness of our method. For example, our SAPAN enhances the IoU of VNC III → Lucchi-Test and Lucchi-Train to 72.8% and 77.1%, outperforming DA-ISC by significant margins of 4.1% and 2.8%. Compared with the pixel-wise alignment, the part-aware alignment can make the model fully utilize the global context information so as to perceive the mitochondria comprehensively.On the MitoEM dataset with a larger structure discrepancy, our SAPAN can achieve 75.6% and 80.6% IoU on the two subsets respectively, outperforming DA-ISC by 0.8% and 1.2%. It demonstrates the remarkable generalization capacity of SAPAN, credited to the structure decoupler, which can effectively alleviate the domain gap caused by huge structural difference.Figure 2 shows the qualitative comparison between our SAPAN and other competitive methods including UALR [29], DAMT-Net [18], DA-VSN [4] and ISC [5]. We can observe that other methods tend to incorrectly segment the background region or fail to activate all the mitochondria. We deem the main reason is that the large domain gap severely confuses the segmentation model. With the assistance of the structure decoupler and the part miner, SAPAN is more robust in the face of the large domain gap and generates a more accurate prediction."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.4,Ablation Study and Analysis,"To look deeper into our method, we perform a series of ablation studies on VNC III → Lucchi-Test to analyze each component of our SAPAN, including the Structure Decoupler (SD) and the Part Miner (PM). Note that we remove all modules except the U-Net and the pixel-wise discriminator as our baseline. Hyperparameters are discussed in the supplementary material."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Effectiveness of Components.,"As shown in Table 3, both structure decoupler and part miner bring a certain performance lift compared with the baseline. ( 1) With the utilization of SD, a 3.0% improvement of IoU can be observed, indicating that decoupling the feature from domain-specific information can benefit the domain adaptation task. In Fig. 3, we visualize the feature distribution with/without SD using t-SNE [11]. We can see that SD makes the feature distribution more compact and effectively alleviates the domain gap. (2) The introduction of PM achieves further accuracy gains, mainly ascribed to the adaptive part alignment mechanism. As shown in Fig. 4, the different prototypes focus on significant distinct areas. The discriminator benefits from the diverse parts-aware prototypes, which in turn promotes the segmentation network.Effectiveness of the Attention-Based Smoothing. As shown in Table 4, abandoning the spatial smoothing operation makes the performance decrease. Compared with simply employing a convolution layer for smoothing, attentionbased smoothing contributes to a remarkable performance (72.8% vs. 70.6% IoU), thanks to the long-range modeling capabilities of the attention mechanism.Effectiveness of the Ways Modeling Part-Aware Prototypes. In Table 5, fg. means only focusing on foreground and vice versa. Neglecting L div leads to severe performance degradation, that is because L div is able to prevent the prototypes from focusing on similar local semantic clues. And simultaneously modeling foreground/background prototypes brings further improvement, demonstrating there is a lot of discriminative information hidden in the background region."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,5.0,Conclusion,"We propose a structure decoupler to decouple the distribution and morphology, and a part miner to aggregate diverse parts for UDA mitochondria segmentation. Experiments show the effectiveness."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 50.
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,1.0,Introduction,"It has been widely recognized that the success of supervised learning approaches, such as deep learning, relies on the i.i.d. assumption for both training and test samples [11]. This assumption, however, is less likely to be held on medical image segmentation tasks due to the imaging distribution discrepancy caused by non-uniform characteristics of the imaging equipment, inconsistent skills of the operators, and even compromise with factors such as patient radiation exposure and imaging time [14]. Therefore, the imaging distribution discrepancy across different healthcare centers renders a major hurdle that prevents deep learningbased medical image segmentation models from clinical deployment [7,18].To address this issue, unsupervised domain adaptation (UDA) [8,17] and multi-source domain generalization (MSDG) [10,16] have been studied. UDA needs access to the data from source domain(s) and unlabeled target domain, while MSDG needs access to the data from multiple source domains. In clinical practice, both settings are difficult to achieve, considering the cost of acquiring target-domain data and the privacy concerns associated with redistributing the data from multiple source domains [9,22].By contrast, single domain generalization (SDG) [2,13,15,19,22,23] is a more practical setting, under which only the labeled data from one source domain are used to train the segmentation model, which is thereafter applied to the unseen target-domain data. The difficulty of SDG is that, due to the existence of imaging distribution discrepancy, the trained segmentation model is prone to overfit the source-domain data but generalizes poorly on target-domain data. An intuitive solution is to increase the diversity of training data by performing data augmentation at the image-level [13,15,19,21]. This solution has recently been demonstrated to be less effective than a more comprehensive one, i.e., conducting domain adaptation on both image-and feature-levels [2,8,12]. As a more comprehensive solution, Dual-Norm [23] first augments source-domain images into 'source-similar' images with similar intensities and 'source-dissimilar' images with inverse intensities, and then processes these two sets of images using different batch normalization layers in the segmentation model. Although achieving promising performance in cross-modality CT and MR image segmentation, Dual-Norm may not perform well under the cross-center SDG setting, where the source-and target-domain data are acquired at different healthcare centers, instead of using different imaging modalities. In this case, the 'source-dissimilar' images with inverse intensities do not really exist, and it remains challenging to determine the way to generate both 'source-similar' and 'source-dissimilar' images [1,4]. To address this challenge, we suggest resolving 'similar' and 'dissimilar' from the perspective of contrastive learning. Given a source image and its style-augmented counterpart, only the structure representations between them are 'similar', whereas their style representations should be 'dissimilar'. Based on contrastive learning, we can disentangle and then discard the style representations, which are structure-irrelevant, using images from only a single domain.Specifically, to disentangle the style representations, we train a segmentation model, i.e., the baseline, using single domain data and assess the impact of the features extracted by the first convolutional layer on the segmentation performance, since shallower features are believed to hold more style-sensitive information [8,18]. A typical example was given in Fig. 1(a) and(b), where the green line is the average Dice score obtained on the target domain (the BASE2 dataset) versus the index of the feature channel that has been dropped. It reveals that, in most cases, removing a feature does not affect the model performance, indicating that the removed feature is redundant. For instance, the performance even increases slightly after removing the 24th channel. This observation is consistent with the conclusion that there exists a sub-network that can achieve comparable performance [6]. On the contrary, it also shows that some features, such as the 36th channel, are extremely critical. Removing this feature results in a significant performance drop. We visualize the 24th and 36th channels obtained on three target-domain images in Fig. 1(c) and (d), respectively. It shows that the 36th channel is relatively 'clean' and most structures are visible on it, whereas the 24th channel contains a lot of 'shadows'. The poor quality of the 24th channel can be attributed to the fact that the styles of source-and target-domain images are different and the style representation ability learned on source-domain images cannot generalize well on target-domain images. Therefore, we suggest that the 24th channel is more style-sensitive, whereas the 36th channel contains more structure information. This phenomenon demonstrates that 'the devil is in channels'. Fortunately, contrastive learning provides us a promising way to identify and expel those style-sensitive 'devil' channels from the extracted image features.In this paper, we incorporate contrastive feature disentanglement into a segmentation backbone and thus propose a novel SDG method called Channel-level Contrastive Single Domain Generalization (C 2 SDG) for joint optic cup (OC) and optic disc (OD) segmentation on fundus images. In C 2 SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. This method has been evaluated against other SDG methods on a public dataset and improved performance has been achieved. Our main contributions are three-fold: (1) we propose a novel contrastive perspective for SDG, enabling contrastive feature disentanglement using the data from only a single domain; (2) we disentangle the style representations and structure representations explicitly and channel-wisely, and then diminish the impact of style-sensitive 'devil' channels; and (3) our C 2 SDG outperforms the baseline and six state-of-the-art SDG methods on the joint OC/OD segmentation benchmark."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.1,Problem Definition and Method Overview,"Let the source domain be denoted by D s = {x s i , y s i } Ns i=1 , where x s i is the i-th source domain image, and y s i is its segmentation mask. Our goal is to train a segmentation model F θ : x → y on D s , which can generalize well to an unseen target domain D t = {x t i } Nt i=1 . The proposed C 2 SDG mainly consists of a segmentation backbone, a style augmentation (StyleAug) module, and a contrastive feature disentanglement (CFD) module. For each image x s , the StyleAug module generates its style-augmented counterpart x a , which shares the same structure but different style to x s . Then a convolutional layer extracts high-dimensional representations f s and f a from x s and x a . After that, f s and f a are fed to the CFD module to perform contrastive training, resulting in the disentangled style representations f sty and structure representations f str . The segmentation backbone only takes f str as its input and generates the segmentation prediction y. Note that, although we take a U-shape network [5] as the backbone for this study, both StyleAug and CFD modules are modularly designed and can be incorporated into most segmentation backbones. The diagram of our C 2 SDG is shown in Fig. 2. We now delve into its details."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.2,Style Augmentation,"Given a batch of source domain data {x s n } NB n=1 , we adopt a series of stylerelated data augmentation approaches, i.e., gamma correction and noise addition in BigAug [21], and Bezier curve transformation in SLAug [15], to generate {x BA n } NB n=1 and {x SL n } NB n=1 . Additionally, to fully utilize the style diversity inside single domain data, we also adopt low-frequency components replacement [20] within a batch of source domain images. Specifically, We reverse {x s n } NB n=1 to match x s n with x s r , where r = N B + 1 -n to ensure x s r provides a different reference style. Then we transform x s n and x s r to the frequency domain and exchange their low-frequency components Low(Amp(x s ); β) in the amplitude map, where β is the cut-off ratio between low and high-frequency components and is randomly selected from (0.05, 0.15]. After that, we recover all low-frequency exchanged images to generate, and {x F R n } NB n=1 in turn to perform contrastive training and segmentation."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.3,Contrastive Feature Disentanglement,"Given x s and x a , we use a convolutional layer with parameter θ c to generate their shallow features f s and f a , which are 64-channel feature maps for this study.Then we use a channel mask prompt P ∈ R 2×64 to disentangle each shallow feature map f into style representation f sty and structure representation f str explicitly channel-wiselywhere SM (•) is a softmax function, the subscript i denotes i-th channel, and τ = 0.1 is a temperature factor that encourages P sty and P str to be binaryelement vectors, i.e., approximately belonging to {0, 1} 64 . After channel-wise feature disentanglement, we have {f s sty , f s str } from x s and {f a sty , f a str } from x a . It is expected that (a) f s sty and f a sty are different since we want to identify them as the style-sensitive 'devil' channels, and (b) f s str and f a str are the same since we want to identify them as the style-irrelevant channels and x s and x a share the same structure. Therefore, we design two contrastive loss functions L sty and L strwhere the P roj(•) with parameters θ p reduces the dimension of f str and f sty . Only f s str and f a str are fed to the segmentation backbone with parameters θ seg to generate the segmentation predictions y s and y a ."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.4,Training and Inference,"Training. For the segmentation task, we treat OC/OD segmentation as two binary segmentation tasks and adopt the binary cross-entropy loss as our objectivewhere y represents the segmentation ground truth and y is the prediction. The total segmentation loss can be calculated asDuring training, we alternately minimize L seg to optimize {θ c , P, θ seg }, and minimize L str + L sty to optimize {P, θ p }.Inference. Given a test image x t , its shallow feature map f t can be extracted by the first convolutional layer. Based on f t , the optimized channel mask prompt P can separate it into f t sty and f t str . Only f t str is fed to the segmentation backbone to generate the segmentation prediction y t ."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,3.0,Experiments and Results,"Materials and Evaluation Metrics. The multi-domain joint OC/OD segmentation dataset RIGA+ [1,4,8] was used for this study. It contains annotated fundus images from five domains, including 195 images from BinRushed, 95 images from Magrabia, 173 images from BASE1, 148 images from BASE2, and 133 images from BASE3. Each image was annotated by six raters, and only the first rater's annotations were used in our experiments. We chose BinRushed and Magrabia, respectively, as the source domain to train the segmentation model, and evaluated the model on the other three (target) domains. We adopted the Dice Similarity Coefficient (D, %) to measure the segmentation performance.Implementation Details. The images were center-cropped and normalized by subtracting the mean and dividing by the standard deviation. The input batch contains eight images of size 512 × 512. The U-shape segmentation network, whose encoder is a modified ResNet-34, was adopted as the segmentation backbone of our C 2 SDG and all competing methods for a fair comparison. The Table 1. Average performance of three trials of our C 2 SDG and six competing methods in joint OC/OD segmentation using BinRushed (row 2-row 9) and Magrabia (row 10-row 17) as source domain, respectively. Their standard deviations are reported as subscripts. The performance of 'Intra-Domain' and 'w/o SDG' is displayed for reference. The best results except for 'Intra-Domain' are highlighted in blue. projector in our CFD module contains a convolutional layer followed by a batch normalization layer, a max pooling layer, and a fully connected layer to convert f sty and f str to 1024-dimensional vectors. The SGD algorithm with a momentum of 0.99 was adopted as the optimizer. The initial learning rate was set to lr 0 = 0.01 and decayed according to lr = lr 0 ×(1-e/E) 0.9 , where e is the current epoch and E = 100 is the maximum epoch. All experiments were implemented using the PyTorch framework and performed with one NVIDIA 2080Ti GPU."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Methods,"Comparative Experiments. We compared our C 2 SDG with two baselines, including 'Intra-Domain' (i.e., training and testing on the data from the same target domain using 3-fold cross-validation) and 'w/o SDG' (i.e., training on the source domain and testing on the target domain), and six SDG methods, including BigAug [21], CISDG [13], ADS [19], MaxStyle [2], SLAug [15], and Dual-Norm [23]. In each experiment, only one source domain is used for training, ensuring that only the data from a single source domain can be accessed during training. For a fair comparison, all competing methods are re-implemented using the same backbone as our C 2 SDG based on their published code and paper. The results of C 2 SDG and its competitors were given in Table 1. It shows that C 2 SDG improves the performance of 'w/o SDG' with a large margin and outperforms all competing SDG methods. We also visualize the segmentation predictions generated by our C 2 SDG and six competing methods in Fig. 3. It reveals that our C 2 SDG can produce the most accurate segmentation map.Ablation Analysis. To evaluate the effectiveness of low-frequency components replacement (FR) in StyleAug and CFD, we conducted ablation experiments using BinRushed and Magrabia as the source domain, respectively. The average performance is shown in Table 2. The performance of using both BigAug and SLAug is displayed as 'Baseline'. It reveals that both FR and CFD contribute to performance gains.Analysis of CFD. Our CFD is modularly designed and can be incorporated into other SDG methods. We inserted our CFD to ADS [19] and SLAug [15], respectively. The performance of these two approaches and their variants, denoted as C 2 -ADS and C 2 -SLAug, was shown in Table 3. It reveals that our CFD module can boost their ability to disentangle structure representations and improve the segmentation performance on the target domain effectively. We also adopted     dropout (see Table 4). It shows that the adversarial training strategy fails to perform channel-level feature disentanglement, due to the limited training data [3] for SDG. Nonetheless, our channel-level contrastive learning strategy achieves the best performance compared to other strategies, further confirming the effectiveness of our CFD module."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a novel SDG method called C 2 SDG for medical image segmentation. In C 2 SDG, the StyleAug module generates style-augmented counterpart of each source domain image and enables contrastive learning, the CFD module performs channel-level style and structure representations disentanglement via optimizing a channel prompt P, and the segmentation is performed based solely on structure representations. Our results on a multi-domain joint OC/OD segmentation benchmark indicate the effectiveness of StyleAug and CFD and also suggest that our C 2 SDG outperforms the baselines and six completing SDG methods with a large margin."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_2.
Robust Segmentation via Topology Violation Detection and Feature Synthesis,1.0,Introduction,"Performance of automated medical image segmentation methods is widely evaluated by pixel-wise metrics, e.g., the Dice score or the Intersection-over-Union [16]. Taking the average accuracy of each pixel/voxel into account, these evaluation methods are well suited to describe the overall segmentation performance. However, errors in some regions may be more important than others, e.g., certain errors may lead to the misrepresented topology. In some cases, topological correctness is more important than pixel-wise classification correctness. For example, when segmentation is a fundamental step for surface reconstruction [4].To address the above challenge, several methods explore the idea of topology constraints in image segmentation. Many utilize persistent homology (PH) [2,3,9,24] by adding an additional topology loss term to the pixel-wise, e.g., cross entropy, loss. These methods manipulate the prediction probability of critical points that are sensitive to topological changes. However, the detection of these critical points is neither computationally efficient nor accurate.In this work, we propose a novel deep learning (DL)-based topology-aware segmentation method, utilizing the concept of the Euler Characteristic (EC) [7], which is an integer defined as the number of connected components (β 0 ) minus the number of holes (β 1 ) in a 2D image. We develop a DL-compatible extension to the classical EC calculation approach, i.e., the Gray algorithm [7]. Given the difference in EC between prediction and ground truth (GT), which we refer to as EC error, we can visualize topology violations in the predictions, denoted as a topology violation map, by backpropagating the EC error with respect to the segmentations. With the resulting topology violation map, we further design an efficient correction network that can improve the connectedness of foreground elements by synthesising plausible alternatives that preserve learned topological priors (i.e. the EC)."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Contributions.,"(1) This paper presents a novel DL-based method for fast EC calculation, which, to the best of our knowledge, is the first paper introducing DL-compatible EC computation. (2) The proposed EC calculation network enables the visualization of a topology violation map, by backpropagating the EC error between the predicted and GT segmentations. (3) Leveraging the topology violation map, we design a topology-aware feature synthesis network to correct regions with topological errors. It can be easily incorporated into any existing segmentation pipeline for topology correction. (4) We demonstrate the effectiveness of our method on two datasets with different foreground structures, and achieve superior performance compared to existing methods. Related Work. Segmentation networks such as U-Net [21] are typically trained using loss functions such as binary cross-entropy or soft Dice loss. These metrics measure the pixel-wise overlap between the prediction and the GT. However, they do not guarantee topological correctness. Shape priors have been explored to mitigate this issue, e.g., utilizing shape templates [14,19] with diffeomorphic deformation. These methods require the predictions to be very close to the shape priors, which is often unachievable in practice. Other methods inject shape information explicitly into the training process [20]. Implicit shape awareness has also been explored in [12,22] based on boundary and centerline information that can model connectivity. Unfortunately, neither of these methods is directly based on insights from topology theory, nor do they guarantee the corrections of topology.So far, topology in segmentation has only been explored through the use of PH [1][2][3]5,9,15,23]. PH extracts topological features by tracking the birth and death of connected components, holes, and voids, as the filtration varies. In practice, these PH-based methods first detect critical points that are related to changes in topology and then manipulate the values of these critical points to encourage a predicted topology that matches the GT topology. This loss term is differentiable since it only changes the probability of critical points in the segmentation. Despite the available packages for computing PH [11,18], the polynomial computational complexity of the PH is a limitation that hinders its use in large-scale datasets. To address this unmet need, we propose an EC-based method that can serve as a guidance function for DL segmentation networks and can achieve real-time performance. Our code is publicly available here1 ."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,2.0,Method,"Preliminaries. Algebraic topology utilizes algebraic techniques to study the properties of topological spaces. This field aims to identify and characterize topological invariants, such as the number of holes, the connectivity, or the homotopy groups of a space. For image segmentation, gray-scale images defined on grids I ∈ R h×w can be modeled as cubical complexes K, which is a set consisting of points, line segments, squares, and cubes. We show the modeling process in Fig. 1 in the appendix.Euler Characteristic (EC), denoted as χ ∈ Z, is a topological invariant that describes the structure and properties of a given topological space. For a cubical complex K defined in a n-dimensional space, EC χ is defined as the alternating sum of the number of k-dimensional cells N k , or alternating sum of the ranks of the k-dimensional homology groups, called Betti number β k . Mathematically, EC can be formalized as:Specifically, for 2-dimensional images, ECwhere N 0 , N 1 and N 2 are the number of vertices, edges, and faces; and β 0 and β 1 are the number of connected components and holes.Gray Algorithm for EC Calculation. The Gray algorithm is a conventional approach to calculate EC on binary images [7,26]. Specifically, instead of directly counting the number of vertices N 0 , edges N 1 , and faces N 2 , the Gray algorithm calculates the EC by counting the number of occurrences of 10 different 2 × 2 patterns, called bit-quads, which are categorized into K 1 , K 2 and K 3 . We show these patterns in Fig. 1. The number of occurrences of all the bit-quads from K l is notated as M l , l ∈ {1, 2, 3}. The EC χ can then be calculated by a linear transformation f :  Overview. We illustrate our method in Fig. 1, which consists of three components: (1) a segmentation network, (2) a topological violation detection (TVD) block and (3) a topology-aware feature synthesis (TFS) network.In (1), we utilize a U-Net to predict a segmentation probability map S ∈ [0, 1] h×w , with the supervision of GT segmentation S ∈ {0, 1} h×w . The predicted map may contain topological errors. ( 2) is the main contribution of our approach, consisting of two sub-nets: an EC-Net and a Visualization Net. The EC-Net takes the predicted S and GT segmentation S as inputs and predicts their corresponding EC maps X ∈ Z h×w and X ∈ Z h×w . We then measure the Euler error e ∈ R by L1 distance as e = X -X 1 . The Visualization Net takes e as input and produces a topology violation map V ∈ R h×w that highlights the regions with topological errors. Finally, in (3), we design a TFS network, which learns to fill in the missing segmentation in the erroneous regions. This subnetwork takes the predicted segmentation S and violation map V as input and generates a topology-preserving segmentation , which is the final prediction.During training, we use the TVD block (red arrows in Fig. 1) to generate the violation maps V to further guide the next feature synthesis network. During inference, we only run the first segmentation network and TFS network, as indicated by the upper blue arrows in Fig. 1 to produce the final topology-preserving segmentation results.Topology-violation Detection. TVD consists of two parts: an EC-Net and a Visualization Net (Fig. 1 bottom). The Gray algorithm that calculates EC cannot be directly integrated into the gradient-based optimization process as is not differentiable. To overcome this problem, we propose a DL-compatible EC-Net as a CNN-based method that leverages the Gray algorithm to calculate the EC. The EC-Net serves as a function that maps the segmentation space to the Euler number space g : S → χ. It is worth mentioning that in order to preserve spatial information, for each segmentation S, EC-Net locally produces Euler numbers χ i,j with the input of a segmentation patch P i,j,Δ = S i:i+Δ,j:j+Δ , where Δ is the patch size. We can therefore obtain an Euler map X by combining all the local Euler numbers χ i,j .EC-Net consists of three parts: 1) fixed kernel CNN layers, 2) an averaged pooling layer and 3) a linear transformation f . Following the Gray algorithm [7], we first utilize three CNN layers with fixed kernels to localize the bit-quads in the segmentation. The values of the kernels are the same as the bit-quadsas shown in Fig. 1. Note that we first binarize the prediction probability map S and further normalize it to {-1, 1}. Therefore, if and only if the prediction has the same pattern as the bit-quads, it will be activated to 4 after convolution. Subsequently, we apply an average pooling layer to obtain the local number of bit-quads M 1 , M 2 , and M 3 . The process can be summarized as:where l ∈ {1, 2, 3}, * represents the convolutional operation, and 1(•) is an indicator function that equals 1 if and only if the input is true. Note that the patch size of average pooling is the same as the patch size of the segmentation Δ.Finally, following Eq. 2, a linear transformation is used to calculate the EC χ.During training, we separately take both S and S as the input of EC-Net and obtain their corresponding Euler maps X and X.In the second part of TVD, we measure the Euler error by the L1 distance as e = X -X 1 . We can calculate the gradient of e with respect to the segmentation maps as the visualization of the EC error, called topology violation map V = ∂e/∂S, which is the output of TVD.Topology-Aware Feature Synthesis. In this module, we aim to improve the segmentation's topological correctness by utilizing the detected topology violation maps. We observe that topological errors are often caused by poor feature representation in the input image, e.g. blurry boundary regions. These errors are difficult to be corrected when trained from the image space. Therefore, we propose a TFS network that directly learns how to repair the topological structures from the segmentations.During training, we mask out the topological error regions in the segmentation map and send it as the input of the feature synthesis network. We then use the GT segmentation to supervise this network to learn to repair these error regions. The input S (with its element Si,j ) of the feature synthesis network is generated from the segmentation probability map S (with its element Ŝi,j ) and the topology violation map V (with its element V i,j ) as follows: We first filter out the coordinates {(i, j)} with severe topological errors if | V i,j |≥ t, where t is a filtration threshold. Then we replace the values of the segmentation at these coordinates by a random probability sampled from standard Gaussian distribution σ ∼ N (0, 1) and followed by a Sigmoid function to map it to (0, 1). The process of generating Si,j can be summarized as:During inference, we feed the feature synthesis network with pure predictions S. We show the effectiveness of our design in the next section."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,3.0,Evaluation,"Datasets: We conduct experiments on two datasets: CREMI for neuron boundary segmentation [6] and the developing human connectome project (dHCP2 ) dataset for fetal cortex segmentation. These datasets are employed to evaluate different topology challenges, where neuron boundaries in CREMI have a random and diverse number of holes β 1 and connected components β 0 , while the cortex in the dHCP dataset is expected to have fixed β 1 and β 0 . CREMI consists of 3 subsets, each of which consists of 125 1250 × 1250 grayscale images and corresponding label maps. We randomly selected 100 samples from each subset (300 samples in total) as the training set and use the remaining 25 samples (75 samples in total) as validation and test set. We further divided each 1250 × 1250 image into 25 256 × 256 patches with an overlap of 8 pixels, in order to fit the GPU memory and enlarge the size of the training set. Thus, training and test sets consist of 7,500 and 1,875 samples, respectively.The dHCP dataset has 242 fetal brain T2 Magnetic Resonance Imaging (MRI) scans with gestational ages ranging from 20.6 to 38.2 weeks. All MR images were motion corrected and reconstructed to 0.8 mm isotropic resolution for the fetal head region of interest (ROI) [10,13]. The images are affinely aligned to the MNI-152 coordinate space and clipped to the size of 144 × 192 × 192. We randomly split the data into 145 samples for training, 73 for testing, and 24 for validation. The GT cortical gray matter label is first generated by DrawEM method [17] and then refined manually to improve the segmentation accuracy."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Settings:,"The hyper-parameters in Eq. 3 and Eq. 4 are empirically chosen as Δ = 32 for the CREMI dataset, Δ = 8 for the dHCP dataset, and t = 0.6 for both datasets. We choose Δ to be higher for CREMI than for dHCP because:(1) the resolution of CREMI is higher and (2) the topology of the fetal cortex may change in smaller regions. We choose a default U-Net as the backbone for all methods for comparison, but our method can also be Incorporated into other segmentation frameworks. For the training process, we first use default crossentropy loss to train the first segmentation network, then the TFS network. Note that the parameters in TVD are fixed.Implementation: We use PyTorch 1.13.1 and calculate the Betti number with the GUDHI package [18]. The training time is evaluated on an NVIDIA RTX 3080 GPU with a batch size of 20.Evaluation Metrics. Segmentation performance is evaluated by Dice score and averaged surface distance (ASD), and the performance of topology is evaluated by Betti errors, which is defined as: e i =| β pred i -β gt i |, where i ∈ {0, 1} indicates the dimension. We also report the mean Betti error as e = e 0 + e 1 .Quantitative Evaluation. We compare the segmentation performance of our method with three baselines which are proposed to preserve shape and topology: cl-Dice loss [22], boundary loss [12], warp loss [8] and PH loss [9]. For pixel-wise accuracy, our method achieves the best Dice score on both datasets, as demonstrated in Tab. 1. Despite the comparable ASD score in the dHCP dataset, we achieve significant improvements in terms of Betti errors. Compared to all baseline methods, our approach achieves an average Betti error of 4.970 on the CREMI dataset, representing an improvement of at least 4.132 (45.25%) in the mean average Betti error. We observe similar improvements for the dHCP dataset. We also conduct Wilcoxon signed-rank tests [25] comparing our method to all other baselines. The p-values are <0.05 across all metrics. Note that compared to the PH based loss [9], which is a similarly well grounded concept in algebraic topology, our method is 46.4 times faster at 0.189 s/batch. Qualitative Evaluation. As highlighted in Fig. 2, our method can effectively eliminate the topological errors in both the CREMI and dHCP datasets, outperforming all the other methods. For instance, in the first row of the CREMI dataset, none of the baseline methods could segment the tiny neuron structure when the boundary of the cells is blurry. Similarly, in the dHCP dataset, all the baseline methods fail to segment the fetal cortex as a closed surface, whereas our method can successfully resolve this issue. Second, we show the topology violation maps from the TVD block in the third column in Fig. 2, which indicate the topology error regions between the GT (second column) and the prediction from our first segmentation network (fourth column). For example, in the second raw of the CREMI evaluation, we observe that the topology violation map can highlight the disconnected boundary, therefore driving our method to correct these errors. We provide more visual examples in the supplementary materials.Ablation Study. We first evaluate the effectiveness of our TVD design. We train our pipeline without the TVD block. Instead, we use the difference map between the prediction and GT as a substitute for the topology violation map. We also summarize the qualitative results in Fig. 2. Feature synthesis with the difference map can correct some of the false negative errors, however, the ring structure remains to be incorrect for the CREMI dataset. In contrast, our approach successfully predicts all the ring structures. Secondly, we further remove the TFS network. Quantitative results are provided in Tab. 1. The topology performance without TVD+TFS is significantly inferior to our method in terms of Betti errors, which illustrates the effectiveness of our design.Discussion. This study sheds new light on improving and evaluation of topology-aware medical image segmentation. We observe that most existing methods either do not consider topological constraints, or are limited by their high computational complexity. As a computation-efficient block, our method can be easily integrated into existing segmentation methods to improve the topological structure. A limitation for topology-aware segmentation methods is that they are easy to be affected by noises, so they might be more suitable to datasets with clear topology structures."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,4.0,Conclusion,"We propose a novel EC-based method to include topology constraints in the segmentation network. Different from PH-based approaches, our method has a distinct advantage in computational efficiency while providing improved performance. In this paper, we generate a topology violation map from TVD and employ a post-processing feature synthesis network to correct topological errors. We believe this map is valuable and could be explored for other scenarios, such as serving as a spatial prior to regularize various loss functions."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_7.
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"Breast cancer is the most common cause of cancer-related deaths among women all around the world [8]. Early diagnosis and treatment is beneficial to improve the survival rate and prognosis of breast cancer patients. Mammography, ultrasonography, and magnetic resonance imaging (MRI) are routine imaging modalities for breast examinations [15]. Recent clinical studies have proven that dynamic contrast-enhanced (DCE)-MRI has the capability to reflect tumor morphology, texture, and kinetic heterogeneity [14], and is with the highest sensitivity for breast cancer screening and diagnosis among current clinical imaging modalities [17]. The basis for DCE-MRI is a dynamic T1-weighted contrast enhanced sequence (Fig. 1). T1-weighted acquisition depicts enhancing abnormalities after contrast material administration, that is, the cancer screening is performed by using the post-contrast images. Radiologists will analyze features such as texture, morphology, and then make the treatment plan or prognosis assessment. Computer-aided feature quantification and diagnosis algorithms have recently been exploited to facilitate radiologists analyze breast DCE-MRI [12,22], in which automatic cancer segmentation is the very first and important step.To better support the radiologists with breast cancer diagnosis, various segmentation algorithms have been developed [20]. Early studies focused on image processing based approaches by conducting graph-cut segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. These methods may encounter the issue of high computational complexity when analyzing volumetric data, and most of them require manual interactions. Recently, deep-learning-based methods have been applied to analyze breast MRI. Zhang et al. [28] proposed a mask-guided hierarchical learning framework for breast tumor segmentation via convolutional neural networks (CNNs), in which breast masks were also required to train one of CNNs. This framework achieved a mean Dice value of 72% on 48 testing T1-weighted scans. Li et al. [16] developed a multi-stream fusion mechanism to analyze T1/T2-weighted scans, and obtained a Dice result of 77% on 313 subjects. Gao et al. [7] proposed a 2D CNN architecture with designed attention modules, and got a Dice result of 81% on 87 testing samples. Zhou et al. [30] employed a 3D affinity learning based multi-branch ensemble network for the segmentation refinement and generated 78% Dice on 90 testing subjects. Wang et al. [24] integrated a combined 2D and 3D CNN and a contextual pyramid into U-net to obtain a Dice result of 76% on 90 subjects. Wang et al. [25] proposed a tumor-sensitive synthesis module to reduce false segmentation and obtained 78% Dice value. To reduce the huge annotation burden for the segmentation task, Zeng et al. [27] presented a semi-supervised strategy to segment the manually cropped DCE-MRI scans, and attained a Dice value of 78%.Although [27] has been proposed to alleviate the annotation effort, to acquire the voxel-level segmentation masks is still time-consuming and laborious, see Fig. 1(c). Weakly-supervised learning strategies such as extreme points [5,21], bounding box [6] and scribbles [4] can be promising solutions. Roth et al. [21] utilized extreme points to generate scribbles to supervise the training of the segmentation network. Based on [21], Dorent et al. [5] introduced a regularized loss [4] derived from a Conditional Random Field (CRF) formulation to encourage the prediction consistency over homogeneous regions. Du et al. [6] employed bounding boxes to train the segmentation network for organs. However, the geometric prior used in [6] can not be an appropriate strategy for the segmentation of lesions with various shapes. To our knowledge, currently only one weakly-supervised work [18] has been proposed for breast mass segmentation in DCE-MRI. This method employed three partial annotation methods including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6 slices) to alleviate the annotation cost, and then constrained segmentation by estimated volume using the partial annotation. The method obtained a Dice value of 83% using the interval-slice annotation, on a testing dataset containing only 28 patients.In this study, we propose a simple yet effective weakly-supervised strategy, by using extreme points as annotations (see Fig. 1(d)) to segment breast cancer. Specifically, we attempt to optimize the segmentation network via the conventional trainfine-tuneretrain process. The initial training is supervised by a contrastive loss to pull close positive voxels in feature space. The fine-tune is conducted by using a similarity-aware propagation learning (SimPLe) strategy to update the pseudo-masks for the subsequent retrain. We evaluate our method on a collected DCE-MRI dataset containing 206 subjects. Experimental results show our method achieves competitive performance compared with fully supervision, demonstrating the efficacy of the proposed SimPLe strategy."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.0,Method,"The proposed SimPLe strategy and the trainfine-tuneretrain procedure is illustrated in Fig. 2. The extreme points are defined as the left-, right-, anterior-, posterior-, inferior-, and superior-most points of the cancerous region in 3D. The initial pseudo-masks are generated according to the extreme points by using the random walker algorithm. The segmentation network is firstly trained based on the initial pseudo-masks. Then SimPLe is employed to fine-tune the network and update the pseudo-masks. At last, the network is retrained from random initialization using the updated pseudo-masks. "
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.1,Generate Initial Pseudo-masks,"We use the extreme points to generate pseudo-masks based on random walker algorithm [9]. To improve the performance of random walker, according to [21], we first generate scribbles by searching the shortest path on gradient magnitude map between each extreme point pair via the Dijkstra algorithm [3]. After generating the scribbles, we propose to dilate them to increase foreground seeds for random walker. Voxels outside the bounding box (note that once we have the six extreme points, we have the 3D bounding box of the cancer) are expected to be the background seeds. Next, the random walker algorithm is used to produce a foreground probability map Y :where Ω is the spatial domain. To further increase the area of foreground, the voxel at location k is considered as new foreground seed if Y (k) is greater than 0.8 and new background seed if Y (k) is less than 0.1. Then we run the random walker algorithm repeatedly. After seven times iterations, we set foreground in the same way via the last output probability map. Voxels outside the bounding box are considered as background. The rest of voxels remain unlabeled. This is the way initial pseudo-masks Y init : Ω ⊂ R 3 → {0, 1, 2} generated, where 0, 1 and 2 represent negative, positive and unlabeled."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.2,Train Network with Initial Pseudo-masks,"Let X : Ω ⊂ R 3 → R denotes a training volume. Let f and θ be network and its parameters, respectively. A simple training approach is to minimize the partial cross entropy loss L pce , which is formulated as:(Moreover, supervised contrastive learning is employed to encourage voxels of the same label to gather around in feature space. It ensures the network to learn discriminative features for each category. Specifically, features corresponding to N negative voxels and N positive voxels are randomly sampled, then the contrastive loss L ctr is minimized:where P(k) denotes the set of points with the same label as the voxel k and N (k) denotes the set of points with the different label. Z(k) denotes the feature vector of the voxel at location k. sim(•, •) is the cosine similarity function. σ denotes sigmoid function. τ is a temperature parameter.To summarize, we employ the sum of the partial cross entropy loss L pce and the contrastive loss L ctr to train the network with initial pseudo-masks:(3)"
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.3,SimPLe-Based Fine-Tune and Retrain,"The performance of the network trained by the incomplete initial pseudo-masks is still limited. We propose to fine-tune the entire network using the pre-trained weights as initialization. The fine-tune follows the SimPLe strategy which evaluates the similarity between unlabeled voxels and positive voxels to propagate labels to unlabeled voxels. Specifically, N positive voxels are randomly sampled as the referring voxel. For each unlabeled voxel k, we evaluate its similarity with all referring voxels:where I(•) is the indicator function, which is equal to 1 if the cosine similarity is greater than λ and 0 if less. If S(k) is greater than αN , the voxel at location k is considered as positive. Then the network is fine-tuned using the partial cross entropy loss same as in the initial train stage. The loss function L f inetune is formulated as:where w is the weighting coefficient that controls the influence of the pseudo labels. To reduce the influence of possible incorrect label propagation, pseudo labels for unlabeled voxels are valid only for the current iteration when they are generated.After the fine-tune completed, the network generates binary pseudo-masks for every training data, which are expected to be similar to the ground-truths provided by radiologists. Finally the network is retrained from random initialization by minimizing the cross entropy loss with the binary pseudo-masks."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,3.0,Experiments,"Dataset. We evaluated our method on an in-house breast DCE-MRI dataset collected from the Cancer Center of Sun Yat-Sen University. In total, we collected 206 DCE-MRI scans with biopsy-proven breast cancers. All MRI scans were examined with 1.5T MRI scanner. The DCE-MRI sequences (TR/TE = 4.43 ms/1.50 ms, and flip angle = 10 • ) using gadolinium-based contrast agent were performed with the T1-weighted gradient echo technique, and injected 0.2 ml/kg intravenously at 2.0 ml/s followed by 20 ml saline. The DCE-MRI volumes have two kinds of resolution, 0.379×0.379×1.700 mm 3 and 0.511×0.511×1.000 mm 3 .All cancerous regions and extreme points were manually annotated by an experienced radiologist via ITK-SNAP [26] and further confirmed by another radiologist. We randomly divided the dataset into 21 scans for training and the remaining scans for testing 1 . Before training, we resampled all volumes into the same target spacing 0.600×0.600×1.000 mm 3 and normalized all volumes as zero mean and unit variance.Implementation Details. The framework was implemented in PyTorch, using a NVIDIA GeForce GTX 1080 Ti with 11GB of memory. We employed 3D Unet [2] as our network backbone.• Train: The network was trained by stochastic gradient descent (SGD) for 200 epochs, with an initial learning rate η = 0.01. The ploy learning policy was used to adjust the learning rate, (1epoch/200) 0.9 . The batch size was 2, consisting of a random foreground patch and a random background patch located via initial segmentation Y init . Such setting can help alleviate class imbalance issue. The patch size was 128 × 128 × 96. For the contrastive loss, we set N = 100, temperature parameter τ = 0.1. • Fine-tune: We initialized the network with the trained weights. We trained it by SGD for 100 iterations, with η = 0.0001. The ploy learning policy was also used. For the SimPLe strategy, we set N = 100, λ = 0.96, α = 0.96, w = 0.1. Quantitative and Qualitative Analysis. We first verified the efficacy of our SimPLe in the training stage. Figure 3 illustrates the pseudo-masks at different training stages. It is obvious that our SimPLe effectively updated the pseudomasks to make them approaching the ground-truths. Therefore, such fune-tuned pseudo-masks could be used to retrain the network for better performance.   1 reports the quantitative Dice, Jaccard, average surface distance (ASD), and Hausdorff distance (95HD) results of different methods. We compared our method with an end-to-end approach [4] that proposed to optimize network via CRF-regularized loss L crf . Although our L ctr supervised method outcompeted L crf [4], the networks trained only using the initial pseudo-masks could not achieve enough high accuracy (Dice values<70%). In contrast, the proposed SimPLe largely boosted the performance of the basically trained networks, by +14.74% Dice and +15.16% Jaccard (v.s. L crf ), +11.81% Dice and +12.65% Jaccard (v.s. L ctr ). Table 1 also shows the comparison results of three general weakly-supervised strategies, including entropy minimization [10], mean teacher [23], and bounding box [13]. Our method consistently outperformed these strategies with respect to all evaluation metrics. Furthermore, our method achieved competitive Dice results compared with fully supervision, which again proves the efficacy of the proposed SimPLe strategy. Note that the average annotation time for extreme points and full masks were 31 s and 95 s per scan, respectively. Figure 5 visualizes the 3D distance map between the segmented surface and ground-truth. It can be observed that our SimPLe consistently enhanced the segmentation."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,4.0,Conclusion,We introduce a simple yet effective weakly-supervised learning method for breast cancer segmentation in DCE-MRI. The primary attribute is to fully exploit the simple trainfine-tuneretrain process to optimize the segmentation network via only extreme point annotations. This is achieved by employing a similarityaware propagation learning (SimPLe) strategy to update the pseudo-masks. Experimental results demonstrate the efficacy of the proposed SimPLe strategy for weakly-supervised segmentation.
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,1.0,Introduction,"Accurate and robust classification and segmentation of the medical image are powerful tools to inform diagnostic schemes. In clinical practice, the image-level classification and pixel-wise segmentation tasks are not independent [8,27]. Joint classification and segmentation can not only provide clinicians with results for both tasks simultaneously, but also extract valuable information and improve performance. However, improving the reliability and interpretability of medical image analysis is still reaching.Considering the close correlation between the classification and segmentation, many researchers [6,8,20,22,24,27,28] proposed to collaboratively analyze the two tasks with the help of sharing model parameters or task interacting. Most of the methods are based on sharing model parameters, which improves the performance by fully utilizing the supervision from multiple tasks [8,27]. For example, Thomas et al. [20] combined whole image classification and segmentation of skin cancer using a shared encoder. Task interacting is also a widely used method [12,24,28] as it can introduce the high-level features and results produced by one task to benignly guide another. However, there has been relatively little research on introducing reliability into joint classification and segmentation. The reliability and interpretability of the model are particularly important for clinical tasks, a single result of the most likely hypothesis without any clues about how to make the decision might lead to misdiagnoses and sub-optimal treatment [10,22]. One potential way of improving reliability is to introduce uncertainty for the medical image analysis model.The current uncertainty estimation method can roughly include the Dropoutbased [11], ensemble-based [4,18,19], deterministic-based methods [21] and evidential deep learning [5,16,23,30,31]. All of these methods are widely utilized in classification and segmentation applications for medical image analysis. Abdar et al. [1] employed three uncertainty quantification methods (Monte Carlo dropout, Ensemble MC dropout, and Deep Ensemble) simultaneously to deal with uncertainty estimation during skin cancer image classification. Zou et al. [31] proposed TBraTS based on evidential deep learning to generate robust segmentation results for brain tumor and reliable uncertainty estimations. Unlike the aforementioned methods, which only focus on uncertainty in either medical image classification or segmentation. Furthermore, none of the existing methods have considered how pixel-wise and image-level uncertainty can help improve performance and reliability in mutual learning.Based on the analysis presented above, we design a novel Uncertaintyinformed Mutual Learning (UML) network for medical image analysis in this study. Our UML not only enhances the image-level and pixel-wise reliability of medical image classification and segmentation, but also leverages mutual learning under uncertainty to improve performance. Specifically, we adopt evidential deep learning [16,31] to simultaneously estimate the uncertainty of both to estimate image-level and pixel-wise uncertainty. We introduce an Uncertainty Navigator for segmentation (UN) to generate preliminary segmentation results, taking into account the uncertainty of mutual learning features. We also propose an Uncer- tainty Instructor for classification (UI) to screen reliable masks for classification based on the preliminary segmentation results. Our UML represents pioneering work in introducing reliability and interpretability to joint classification and segmentation, which has the potential to the development of more trusted medical analysis tools1 ."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.0,Method,"The overall architecture of the proposed UML, which leverages mutual learning under uncertainty, is illustrated in Fig. 1. Firstly, Uncertainty Estimation for Classification and Segmentation adapts evidential deep learning to provide image-level and pixel-wise uncertainty. Then, Trusted Mutual Learning not only utilizes the proposed UN to fully exploit pixel-wise uncertainty as the guidance for segmentation but also introduces the UI to filter the feature flow between task interaction.Given an input medical image I, I ∈ R H,W , where H, W are the height and width of the image, separately. To maximize the extraction of specific information required for two different tasks while adequately mingling the common feature which is helpful for both classification and segmentation, I is firstly fed into the dual backbone network that outputs the classification feature maps f c i , i ∈ 1, ..., 4 and segmentation feature maps f s i , i ∈ 1, ..., 4, where i denotes the i th layer of the backbone. Then following [29], we construct the Feature Mixer using Pairwise Channel Map Interaction to mix the original feature and get the mutual feature maps f m i , i ∈ 1, ..., 4. Finally, we combine the last layer of mutual feature with the original feature."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.1,Uncertainty Estimation for Classification and Segmentation,"Classification Uncertainty Estimation. For the K classification problems, we utilize Subjective Logic [7] to produce the belief mass of each class and the uncertainty mass of the whole image based on evidence. Accordingly, given a classification result, its K + 1 mass values are all non-negative and their sum is one:where b c k ≥ 0 and U c ≥ 0 denote the probability belonging to the k th class and the overall uncertainty value, respectively. As shown in Fig. 1 whererepresents the Dirichlet strength. Actually, Eq. 2 describes such a phenomenon that the higher the probability assigned to the k th class, the more evidence observed for k th category should be.Segmentation Uncertainty Estimation. Essentially, segmentation is the classification for each pixel of a medical image. Given a pixel-wise segmentation result, following [31] the seg Dirichlet distribution can be parameterized by α s(h,w) = [α s(h,w) 1"
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,", . . . , α s(h,w) Q","], (h, w) ∈ (H, W ). We can compute the belief mass and uncertainty mass of the input image bywhere b s(h,w) q ≥ 0 and u s(h,w) ≥ 0 denote the probability of the pixel at coordinate (h, w) for the q th class and the overall uncertainty value respectively. We also define U s = {u s(h,w) , (h, w) ∈ (H, W )} as the pixel-wise uncertainty of the segmentation result."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.2,Uncertainty-Informed Mutual Learning,"Uncertainty Navigator for Segmentation. Actually, we have already obtained an initial segmentation mask M = α s , M ∈ (Q, H, W ) through estimating segmentation uncertainty, and achieved lots of valuable features such as lesion location. In our method, appropriate uncertainty guided decoding on the feature list can obtain more reliable information and improve the performance of segmentation [3,9,26]. So we introduce Uncertainty Navigator for Segmentation(UN) as a feature decoder, which incorporates the pixel-wise uncertainty in U s and lesion location information in M with the segmentation feature maps to generate the segmentation result and reliable features. Having a UNet-like architecture [15], UN computes segmentation s i , i ∈ 1, .., 4 at each layer, as well as introduces the uncertainty in the bottom and top layer by the same way. Take the top layer as an example, as shown in Fig. 2(a), UN calculates the reliable mask M r by:Then, the reliable segmentation feature r s , which combines the trusted information in M r with the original features, is generated by:where f s 1 derives from jump connecting and f b 2 is the feature of the s 2 with one up-sample operation. Conv(•) represents the convolutional operation, Cat(•, •) denotes the concatenation. Especially, the U s is also used to guide the bottom feature with the dot product. The r s is calculated from the segmentation result s 1 and contains uncertainty navigated information not found in s 1 .Uncertainty Instructor for Classification. In order to mine the complementary knowledge of segmentation as the instruction for the classification and eliminate intrusive features, we devise an Uncertainty Instructor for classification (UI) following [22].  and the rich information (e.g., lesion location and boundary characteristic) in r s , which can be expressed by:where d n (•) denotes that the frequency of down-sampling operations is n. Then the produced features are transformed into a semantic feature vector by the global average pooling. The obtained vector is converted into the final result (belief values) of classification with uncertainty estimation."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.3,Mutual Learning Process,"In a word, to obtain the final results of classification and segmentation, we construct an end-to-end mutual learning process, which is supervised by a joint loss function. To obtain an initial segmentation result M and a pixel-wise uncertainty estimation U s , following [31], a mutual loss is used as:where y s is the Ground Truth (GT) of the segmentation. The hyperparameters λ m 1 and λ m 2 play a crucial role in controlling the Kullback-Leibler divergence (KL) and Dice score, as supported by [31]. Similarly, in order to estimate the imagelevel uncertainty and classification results. a classification loss is constructed following [5], as:where y c is the true class of the input image. The hyperparameter λ c serves as a crucial hyperparameter governing the KL, aligning with previous work [5]. To obtain reliable segmentation results, we also adopt deep supervision for the final segmentation result S = {s i , i = 1, ..., 4}, which can be denoted as:where υ n indicates the number of up-sampling is 2 n . Thus, the overall loss function of our UML can be given as:where w m , w c , w s denote the weights and are set 0.1, 0.5, 0.4, separately."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,3.0,Experiments,"Dataset and Implementation. We evaluate the our UML network on two datasets REFUGE [14] and ISPY-1 [13]. REFUGE contains two tasks, classification of glaucoma and segmentation of optic disc/cup in fundus images. The overall 1200 images were equally divided for training, validation, and testing. All images are uniformly adjusted to 256 × 256 px. The tasks of ISPY-1 are the pCR prediction and the breast tumor segmentation. A total of 157 patients who suffer the breast cancer are considered -43 achieve pCR and 114 non-pCR.For each case, we cut out the slices in the 3D image and totally got 1,570 2D images, which are randomly divided into the train, validation, and test datasets with 1,230, 170, and 170 slices, respectively.  We implement the proposed method via PyTorch and train it on NVIDIA GeForce RTX 2080Ti. The Adam optimizer is adopted to update the overall parameters with an initial learning rate 0.0001 for 100 epochs. The scale of the regularizer is set as 1 × 10 -5 . We choose VGG-16 and Res2Net as the encoders for classification and segmentation, separately.Compared Methods and Metrics. We compared our method with singletask methods and multi-task methods. (1) Single-task methods: (a) EC [17], (b) TBraTS [31] and (c) TransUNet [2]. Evidential deep learning for classification (EC) first proposed to parameterize classification probabilities as Dirichlet distributions to explain evidence. TBraTS then extended EC to medical image segmentation. Meriting both Transformers and U-Net, TransUNet is a strong model for medical image segmentation. (2) Multi-task methods: (d) BCS [25] and (e) DSI [28]. The baseline of the Joint Classification and Segmentation framework (BCS) is a simple but useful way to share model parameters, which utilize two different encoders and decoders for learning respectively. The Deep Synergistic Interaction Network (DSI) has demonstrated superior performance in joint task. We adopt overall Accuracy (ACC) and F1 score (F1) as the evaluation criteria for the classification task. Dice score (DI) and Average Symmetric Surface Distance (ASSD) are chosen for the segmentation task. Comparison Under Noisy Data. To further valid the reliability of our model, we introduce Gaussian noise with various levels of standard deviations (σ) to the input medical images. The comparison results are shown in Table 2. As can be observed that, the accuracy of classification and segmentation significantly decreases after adding noise to the raw data. However, benefiting from the uncertainty-informed guiding, our UML consistently deliver impressive results. In Fig. 3, we show the output of our model under the noise. It is obvious that both the image-level uncertainty and the pixel-wise uncertainty respond reasonably well to noise. These experimental results can verify the reliability and interpre of the uncertainty guided interaction between the classification and segmentation in the proposed UML. The results of more qualitative comparisons can be found in the Supplementary Material.Ablation Study. As illustrated in Table 3, both of the proposed UN and UI play important roles in trusted mutual learning. The baseline method is BCS.MD represents the mutual feature decoder. It is clear that the performance of classification and segmentation is significantly improved when we introduce supervision of mutual features. As we thought, the introduction of UN and UI takes the reliability of the model to a higher level."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,4.0,Conclusion,"In this paper, we propose a novel deep learning approach, UML, for joint classification and segmentation of medical images. Our approach is designed to improve the reliability and interpretability of medical image classification and segmentation, by enhancing image-level and pixel-wise reliability estimated by evidential deep learning, and by leveraging mutual learning with the proposed UN and UI modules. Our extensive experiments demonstrate that UML outperforms baselines and introduces significant improvements in both classification and segmentation. Overall, our results highlight the potential of UML for enhancing the performance and interpretability of medical image analysis."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Acknowledgements,". This work was supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-TC-2021-003), A*STAR AME Programmatic Funding Scheme Under Project A20H4b0141, A*STAR Central Research Fund, the Science and Technology Department of Sichuan Province (Grant No. 2022YFS0071 & 2023YFG0273), and the China Scholarship Council (No. 202206240082)."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 4.
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,1.0,Introduction,"Due to the broad success of U-Nets [17] for image segmentation, it has become the go-to architecture in the medical image computing community. Since its creation in 2015, much research has been dedicated to exploring variants and improvements over the standard base model [3]. However, Isensee et al. [12] showed with their not-new-U-Net (nnU-Net) that the success of the U-Net relies on a well-prepared data pipeline incorporating appropriate data normalization, class balancing checks, and preprocessing, rather than on architecture changes. Arguably the two most important challenges at present for medical image segmentation are generalization and robustness. A lack of generalization decreases the performance levels of a model on data sets not well characterized by the training data set, while poor robustness appears when models under-perform on data sets presenting noise or other corruptions [13]. Modern neural networks have been shown to be highly susceptible to distribution shifts and corruptions that are modality-specific [6]. While the average accuracy of U-Net-based models has increased over the years, it is evident from the literature that their robustness level has not improved at the same rate [4,5,9].One of the key elements of the U-Net are the skip-connections, which propagate information directly (i.e., without further processing) from the encoding to the decoding branch at different scales. Azad et al. [3] mention that this novel design propagates essential high-resolution contextual information along the network, which encourages the network to re-use the low-level representation along with the high-context representation for accurate localization. Nonetheless, there is no clear evidence supporting this intuition and moreover, there is limited knowledge in the literature describing to what extent skip-connections of the U-Net are necessary, and what their interplay is in terms of model robustness when they are subjected to different levels of task complexity.Currently, the U-Net is used more as a ""Swiss-army knife"" architecture across different image modalities and image quality ranges. In this paper, we describe the interplay between skip-connections and their effective role of ""transferring information"" into the decoding branch of the U-Net for different degrees of task complexity, based on controlled experiments conducted on synthetic images of varying textures as well as on clinical data comprising Ultrasound (US), Computed tomography (CT), and Magnetic Resonance Imaging (MRI). In this regard, the work of [10] showed that neural networks are biased toward texture information. Recently, [19,20] similarly showed the impact of texture modifications on the performance and robustness of trained U-Net models. Contrary to these prior works analyzing the impact of data perturbation to model performance (e.g. [6,13]), in this study we focus on analyzing the role of skipconnections to model performance and its robustness. We hypothesize therefore that skip-connections may not always lead to beneficial effects across varying task complexities as measured with texture modifications. Our major contributions through this paper are: (i) We describe a novel analysis pipeline to evaluate the robustness of image segmentation models as a function of the difference in texture between foreground and background. (ii) We confirm the hypothesis that severing these skip-connections could lead to more robust models, especially in the case of out-of-domain (OOD) test data. Furthermore, we show that severing skip-connections could work better than filtering feature maps from the encoder with attention-gating. (iii) Finally, we also demonstrate failure modes of using skip-connections, where robustness across texture variations appear to be sacrificed in the pursuit of improvements within domain.  evaluate how the model behaves at varying task complexities, we construct training data sets where each training sample is subjected to a linear transformation where its foreground is blended with the background:"
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.1,Experiment Design,"By increasing α from zero to one, more of the foreground texture is added in the foreground mask, which otherwise is made up of the background texture (See Fig. 2), while the background itself is unimpacted. We then quantify the similarity between foreground and background regions by measuring the Kullback-Leibler divergence between their local-binary-pattern (LBP) [16] histograms. We selected LBP since it is a commonly used and benchmarked texture descriptor in machine learning applications [8,15].where T S refers to the level of texture similarity, H() corresponds to histogram, and L(I) {BG,F G} refers to LBP calculated for BG or FG. The LBP histogram was computed using a 3×3 neighbourhood with 8 points around each pixel in the image. Three U-Net models were trained featuring three different skip-connection strategies: NoSkipU-Net, U-Net, and AGU-Net, representing the absence of skipconnections, the use of an identity transform (i.e., information through skips is kept as is), and filtering information via attention through skip-connections, respectively. Models were trained at different levels of T S between the foreground and background regions, determined based on the Kullback-Leibler divergence of Local Binary Pattern (LBP) histograms, Eq. 1. For each level of α used to create a training set, we trained a model to be evaluated on a synthetic test set using the same α to measure within-domain performance and across a range of α, to measure their out-of-domain robustness. Next, using Eq. 1 and ground truth labels, we computed the T S of images from the test set of the medical data sets and applied corruptions by way of noise or blurring in order to increase and decrease T S depending on the imaging modality being analyzed. Then we evaluated the robustness of these models to texture changes in these data sets. We did this at two levels of task complexity (easier -where T S is higher, and harder, where T S is lower) and different from the original T S. We report all model performances using dice scores."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.2,Description of Data,"Synthetic Textures: We took two representative grayscale textures from the synthetic toy data set described in [11] and used them as the background and foreground patterns. These patterns were chosen such that the T S values matched the range of medical data sets described next. We also generated synthetic segmentation masks using bezier curves setup such that the curvature and size of the foreground simulate clinical image segmentation problems. Examples of such images are shown in Fig. 2. We generated 100 such image-mask pairs at 9 levels (for α ∈ {0.1, 0.2, ..., 0.9}), so that we create training data sets at various task complexities. These images are generated by randomly cropping the grayscale textures to 256 × 256 pixels. 70 of these were used as the training set, 10 were reserved for validation, and the rest of the 20 formed the test set, identically split for all task complexities. Figure 2 show kernel density estimates of each of these 9 data sets along the texture similarity axis. The curve in orange (α = 0.1) indicates that the foreground mask in this set contains only 10% of the actual foreground texture and 90% of the background texture blended together. This represents a situation where it is texturally hard for humans as well as for segmentation models. The data set in green (α = 0.9) shows the reverse ratio -the foreground region now contains 90% of the foreground texture, thereby making it an easier task to segment."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Medical Data Sets:,"We tested the three variants of the U-Net architecture on three medical binary segmentation data sets: a Breast Ultrasound [1], a spleen CT and a heart MRI data set [2]. The breast ultrasound data set contained 647 images, 400 of which were used as training, 100 as validation and 147 as the test set. We used the benign and malignant categories in the breast ultrasound data and excluded images with no foreground to segment (i.e. the ""normal"" category). The spleen data set contained 899 images, 601 of which were used as training, 82 as validation and 216 as test set images. The heart data set contained 829 images, 563 of which were used as training, 146 as validation, and 120 as test set images. We selected 2D axial slices from the spleen, and sagittal slices from the heart data sets, both of which were originally 3D volumes, such that there is at least one pixel corresponding to the foreground. Care was taken to ensure that 2D slices were selected from 3D volumes and split at the patient level to avoid cross-contamination of images across training/test splits. To vary T S of images in the test set, and to evaluate the robustness of the U-Net variants, speckle noise with variance 0.1 was added to both the foreground and background. This made the textures more similar, hence lowered T S, and essentially rendered them harder to segment. This is shown in the red boxes in Fig. 3. We also created another test set with textures that are less similar by blurring the background using a Gaussian kernel of variance 3.0 while not blurring the foreground pixels. These are shown in the green boxes in Fig. 3, where it can be seen they are easier to segment."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.3,Model Architecture and Training Settings,"The network architectures were created using MONAI [7] v1.1 and were all trained with random weight initialization. The U-Net was implemented with one input and output channel, with input image size set to 256 × 256 pixels across all experiments. The model had five levels with 16, 32, 64, 128, 256 channels each for synthetic experiments and six levels (an additional level with 512 channels) for medical image experiments, all intermediate channels with a stride of 2. The ReLU activation was used, and no residual units were included. To reduce stochasticity, no dropout was used in any variant.The NoSkipU-Net was identical to the U-Net except for severed skipconnections. This led to the number of channels in the decoder to be smaller as there is no concatenation from the corresponding encoder level. The AGU-Net was setup to be the same as the U-Net, except with attention gating through the skip-connections.The training parameters were kept constant across compared models for fair comparison. Our experiments1 were implemented in Python 3.10.4 using the PyTorch implementation of the adam [14] optimizer. We set the learning rate to be 1e -3 for synthetic experiments (and 1e -2 for medical image experiments), maintaining it constant without using a learning rate scheduler. No early stopping criteria were used while training, and all models were allowed to train to 100 epochs. We trained our models to optimize the dice loss, and saved the model with the best validation dice (evaluated once every two epochs) for inference on the test set.We did not perform any data augmentation that could change the scale of the image content, thereby also changing the texture characteristics. Therefore, we only do a random rotation by 90 degrees with a probability of 0.5 for training, and no other augmentations. We also refrained from fine-tuning hyperparameters and did not perform any ensembling as our study design is not meant to achieve the best possible performance metric as much as it attempts to reliably compare performance across architecture variants while keeping confounding factors to a minimum. We therefore trained each model using the same random seeds (three times) and report the dice score statistics. Training and testing were performed on an NVIDIA A5000 GPU with 24 GB RAM and CUDA version 11.4."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,3.1,On Synthetic Texture Variants,"In-domain (Performance): Figure 4 (left) indicates the relative improvement in dice scores between the three U-Net variants using the NoSkipU-Net as the baseline. To make the interpretation easier, the α value is used as a proxy for T S on the horizontal axis. It is worth noting that for α values > 0.3, there is negligible difference between the dice score performances of all the U-Net variants, indicating their ability with or without the skip-connections to learn the distributions of the foreground and background textures at that level of task complexity. Below α values of 0.3, the benefits of using attention gating in the skip-connections start to appear. This indicates that the benefit of attentiongating as a function of complexity is non-linear: models do not benefit from skip-connections at lower ranges of task complexity, but at larger ones, filtering the information flowing through the skip connections is important. What is interesting is also how the standard U-Net performance is noisy compared to NoSkipU-Net, indicating that passing through the entire encoder feature map to be concatenated with the decoder feature maps may not always be beneficial."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Out-of-Domain (Robustness):,"Rows in Fig. 4 (right) includes a heatmap to represent the α values that the model was trained on, and columns correspond to the α value it was tested on. The entries in the matrix are normalized dice score differences between AGU-Net and NoSkipU-Net (comparisons between standard U-Net and NoSkipU-Net show similar trends). The diagonal entries here correspond to the AGU-Net plot in Fig. 4 (left). For α values 0.3 and 0.4 in training and 0.9 on testing (corresponding to an out-of-domain testing scenario), the NoSkipU-Net performs better than the AGU-Net, indicating that there indeed are situations where skip-connections cause more harm than benefit. "
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,3.2,On Medical Image Textures,"In-Domain (performance): Looking at the ""In-domain"" rows in Table 1, on all three data sets, the AGU-Net outperforms both the other variants. However, the relative improvements in performance vary across modalities, with the performance differences on CT being the most stark. On the Ultrasound data set, the NoSkipU-Net performs as well as the standard U-Net, supporting our hypothesis that skip-connections may not always be beneficial."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Out-of-Domain (Robustness):,"Focusing on the rows ""Harder"" and ""Easier"" in Table 1, we observe for the Ultrasound data set that the AGU-Net improves in the easier task, but declines in performance in the harder one. The drop in performance is most pronounced for the U-Net, but moderate for the NoSkipU-Net. For the spleen data set, both the AGU-Net and the standard U-Net demonstrate severe drop in performance in the harder case. However, AGU-Net is better and the standard U-Net is worse than the NoSkipU-Net in the easier texture situations. The heart data set shows the same trend as in the spleen data set."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,4.0,Discussion and Conclusion,"Through extensive experiments using synthetic texture images at various levels of complexity and validating these findings on medical image data sets from three different modalities, we show in this paper that the use of skip-connections can both be beneficial as well as harmful depending on what can be traded off: robustness or performance. A limitation of our work is that we vary only the foreground in synthetic experiments but background variations could demonstrate unexpected asymmetric behavior. We envision the proposed analysis pipeline to be useful in quality assurance frameworks where U-Net variants could be compared to analyse potential failure modes."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,1.0,Introduction,"Automatic brain ROI segmentation for magnetic resonance images (MRI) of severe traumatic brain injuries (sTBI) patients is crucial in brain damage assessment and brain network analysis [8,11], since manual labeling is time-consuming and labor-intensive. However, conventional brain segmentation pipelines, such as FSL [14] and FreeSurfer [4], suffer significant performance deteriorations due to skull deformation and lesion erosions in traumatic brains. Although automatic segmentation based on deep learning has shown promises in accurate segmentation [10,12], these methods are still constrained by the scarcity of annotated sTBI scans. Thus, researches on traumatic brain segmentation under insufficient annotations needs further exploration.Recently, one-shot medical image segmentation based on learned transformations (OSSLT) has shown great potential [3,17] to deal with label scarcity. These methods typically utilize deformable image registration to learn spatial and appearance transformations and perform data augmentation on the single labeled image to train the segmentation, which is shown in Fig. 1(a). Given a labeled image as the atlas, two unlabeled images are provided as spatial and appearance references. Appearance transform and spatial transform learned by deformable registration are applied to the atlas image to generate a pseudolabeled image to train the segmentation, and the label warped by spatial transform serves as the ground-truth. In this way, the data diversity is ensured by a large amount of unlabeled data, and the segmentation is learned by abundant pseudo images.However, despite the previous success, the generalization ability of these methods is challenged by two issues in traumatic brain segmentation: 1) Limited diversity of generated data due to the amount of available unlabeled images. Although several studies [6,7] have proposed transformation sampling to introduce extra diversity for alleviating this issue, their strategies rely on a manualdesigned distribution, which is not learnable and limits the capacity of data augmentation. 2) The assumption that appearance transforms in atlas augmentation do not affect semantic labels in the images [6,17]. However, this assumption neglects the presence of abnormalities in traumatic brains, such as brain edema, herniation, and erosions, which affect the appearance of brain tissues and introduce label errors.To address the aforementioned issues, we propose a novel one-shot traumatic brain segmentation method that leverages adversarial training and uncertainty rectification. We introduce an adversarial training strategy that improves both the diversity of generated data and the robustness of segmentation, and incorporate an uncertainty rectification strategy that mitigates potential label errors in generated samples. We also quantify the segmentation difference of the same image with and without the appearance transform, which is used to estimate the uncertainty of segmentation and rectify the segmentation results accordingly. The main contributions of our method are summarized as follows: First, we develop an adversarial training strategy to enhance the capacity of data augmentation, which brings better data diversity and segmentation robustness. Second, we notice the potential label error introduced by appearance transform in current one-shot segmentation attempts, and introduce uncertainty rectification for compensation. Finally, we evaluate the proposed method on brain segmentation of sTBI patients, where our method outperforms current state-of-the-art methods. "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.1,Overview of One-Shot Medical Image Segmentation,"After training the unsupervised deformable registration, one-shot medical image segmentation based on learned transformations typically consists of two steps: 1) Data augmentation on the atlas image by learned transformations; 2) Training the segmentation using the augmented images. The basic workflow is shown in Fig. 1(a). Specifically, given a labeled image x A as the atlas and its semantic labels y A , two reference images, including the spatial reference x s and appearance reference x a , are provided to augment the atlas by spatial transform φ and appearance transform ψ that are calculated by the same pretrained registration network.For spatial transform, given an atlas image x A and a spatial reference x s , the registration network performs the deformable registration between them and predicts a deformation field φ, which is used as the spatial transform to augment the atlas image spatially. For appearance transform, given an atlas image x A and an appearance reference x a , we warp x a to x A via an inverse registration φ -1  xa and generates a inverse-warped xa = x a •φ -1  xa , and appearance transform ψ = xa -x A is calculated by the residual of inverse-warped appearance reference xa and the atlas image x A . It should be noted that the registration here is diffeomorphic to allow for inverse registration.After acquiring both the spatial and appearance transform, the augmented atlas x g = (x A + ψ) • φ is generated by applying both transformations. The corresponding ground-truth y g = y A • φ is the atlas label warped by φ, as it is hypothesized that appearance transform does not alter the semantic labels in the atlas image. During segmentation training, a large amount of spatial and appearance reference images are sampled to ensure the diversity of x g , and the segmentation is trained with generated pairs of x g and y g . In this work, we focus on both of the two steps in OSSLT by adversarial training and uncertainty rectification, which is shown in Fig. 1(b). Specifically, we generate an adversarial image x ag along with x g by adversarial training to learn better data augmentation for the atlas image, and uncertainty rectification is utilized during segmentation learning to bypass the potential label errors introduced by appearance transforms. We discuss our proposed adversarial training and uncertainty rectification in Sect. 2.2 and Sect. 2.3, respectively."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.2,Adversarial Training,"Although the diversity of generated pairs of x g and y g is ensured by the increased number of unlabeled images as references, such a setting requires a large amount of unlabeled data. Inspired by [2,9], we adopt the adversarial training strategy to increase both the data diversity and the segmentation robustness, which is shown in Fig. 2. Given a learned spatial transform φ, appearance transform ψ, and a generated image x g augmented by φ and ψ, our adversarial training is decomposed into the following 3 steps: First, we feed x g into the adversarial network and generate two sampling layers α and β activated by Sigmoid function. The sampling layers α and β have the same spatial shape with φ and ψ respectively, and each location in the sampling layers represents the sampling amplitude of the original transform, ranging from 0 to 1. In this way, the diversity of spatial and appearance transforms is significantly improved by the infinite possibilities of sampling layers:Second, by applying the sampled transformations φ a and ψ a to the atlas x A , we acquire an adversarial generated image x ag . We expect x ag to add extra diversity of data augmentation and maintain realistic as well:Finally, both the original generated image x g and the adversarial generated image x ag are fed to the segmentation network, and the training objective is the min-max game of the adversarial network and the segmentation network. Thus, we ensure the diversity of generation and robustness of segmentation simultaneously by adversarial training:where f (•; θ g ) and g(•; θ h ) denote the adversarial network and the segmentation network, respectively. ŷg and ŷag are the segmentation predictions of x g and x ag .It should be noted that since the spatial transformation applied to x g and x ag is different, the loss calculation is performed in atlas space by inverse registration."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.3,Uncertainty Rectification,"Most of the current methods hypothesize that appearance transformation does not alter the label of the atlas. However, in brain scans with abnormalities such as sTBI, the appearance transformation may include edema, lesions, and etc, which may affect the actual semantic labels of the atlas and weaken the accuracy of segmentation. Inspired by [18], we introduce uncertainty rectification to bypass the potential label errors. Specifically, given a segmentation network, fully augmented image x g = (x A + ψ) • φ and spatial-augmented image x As = x A • φ are fed to the network. The only difference between x g and x As is that the latter lacks the transformation on appearance. Thus, the two inputs x g and x As serve different purposes. Fully augmented image x g is equipped with more diversity compared with x As , as appearance transform has been applied to it, while the spatial augmented image x As has more label authenticity and could guide a more accurate segmentation.The overall supervised loss consists of two items. First, the segmentation loss L seg = L ce (ŷ As , y g ) of spatial-augmented image x As guides the network to learn spatial variance only, where ŷAs is the prediction of x As , and L ce denotes cross-entropy loss. Second, the rectified segmentation loss L rseg of x g guides the network to learn segmentation under both spatial and appearance transformations. We adopt the KL-divergence D KL of the segmentation results ŷAs and ŷg as the uncertainty in prediction [18]. Compared with Monte-Carlo dropout [5], KL-divergence for uncertainty estimation does not require multiple forward runs. A voxel with a greater uncertainty indicates a higher possibility of label error in the corresponding location of x g , thus, the supervision signal of this location should be weakened to reduce the effect of label errors:Thus, the overall supervised loss L sup = L seg +L rseg is the segmentation loss L seg of x As and the rectified segmentation loss L rseg of x g . We apply the overall supervised loss L sup on both x g and x ag in practice. During segmentation training, the linear summation of supervised segmentation loss L sup and adversarial loss L adv is minimized."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.1,Data,"We have collected 165 MR T1-weighted scans with sTBI from 2014-2017, acquired on a 3T Siemens MR scanner from Huashan hospital. Among the 165 MR scans, 42 scans are labeled with the 17 consciousness-related brain regions (see appendix for details) while the remaining are left unlabeled, since the manual labeling requires senior-level expertise. Informed consent was obtained from all patients for the use of their information, medical records, and MRI data. All MR scans are linearly aligned to the MNI152 template using FSL [14]. For the atlas image, we randomly collect a normal brain scan at the same institute and label its 17 consciousness-related brain regions as well. During training, the labeled normal brain scan serves as the atlas image, and the 123 unlabeled sTBI scans are used as spatial or appearance references. For one-shot setting, the labeled sTBI scans are used for evaluation only and completely hidden during training. In order to validate the effectiveness of the proposed one-shot segmentation method, a U-Net [13] trained on the labeled scans by 5-fold cross validation is used as the reference of fully supervised segmentation."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.2,Implementation Details,"The framework is implemented with PyTorch 1.12.1 on a Debian Linux server with an NVIDIA RTX 3090 GPU. In practice, the registration network is based on VoxelMorph [1] and pretrained on the unlabeled sTBI scans. During adversarial training, the registration is fixed, while the adversarial network and segmentation network are trained alternately. Both the adversarial network and the segmentation network is based on U-Net [13] architectures and optimized by SGD optimizer with a momentum of 0.9 and weight decay of 1 × 10 -4 . The initial learning rate is set to 1×10 -2 and is slowly reduced with polynomial strategy. We have pretrained the registration network for 100 epochs, and trained the adversarial network and segmentation network for 100 epochs as well. The batch size is set to 1 during training. "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.3,Evaluation,"We have evaluated our one-shot segmentation framework on the labeled sTBI MR scans in the one-shot setting, which means that only one labeled image is available to learn the segmentation. We explore the effectiveness of the proposed adversarial training and uncertainty rectification, and make the comparison with state-of-the-art alternatives, by reporting the Dice coefficients.First, we conduct an ablation study to evaluate the impact of adversarial training and uncertainty rectification, which is shown in Table 1   1). This is because the rectified segmentation loss L rseg does not provide enough supervision signal in regions where the segmentation uncertainty is too high, and thus we need segmentation loss L seg to compensate it. Finally, by applying both adversarial training and uncertainty rectification, the proposed method yields the best results with an improved Dice coefficient of approximately 5% and a lower standard deviation of segmentation performance, which is shown in No. (5). Experimental results demonstrate that the proposed adversarial training and uncertainty rectification can both contribute to the segmentation performance, compared with the baseline setting.Then, we compare the proposed method with three cutting-edge alternatives in one-shot medical image segmentation, including BrainStorm [17], LT-Net [15], and DeepAtlas [16], which is shown in Table 2. The proposed method outperforms other segmentation methods with an average Dice score of 56.3%, higher than all of the previous state-of-the-art methods, and achieves the highest and second highest segmentation performance in all of the 17 brain regions. Also, it should be noted that the proposed method has a lower standard deviation in terms of segmentation performance, which also demonstrates the robustness of our method. However, despite the promising results of the proposed method, we have observed that the performance gain of proposed method in certain brain regions that are usually very small is not significant. The plausible reason is that the uncertainty of these small brain regions is too high and affects the segmentation.For qualitative evaluation, we have visualized the segmentation results of the proposed method and the above-mentioned alternatives, which are shown in Fig. 3. The red bounding boxes indicate the regions where our method achieves better segmentation results compared with the alternatives. Overall, our method achieves more accurate segmentation, especially in the brain regions affected by ventriculomegaly, compared with BrainStorm, LT-Net, and DeepAtlas."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,4.0,Conclusion,"In this work, we present a novel one-shot segmentation method for severe traumatic brain segmentation, a difficult clinical scenario where limited annotated data is available. Our method addresses the critical issues in sTBI brain segmentation, namely, the need for diverse training data and mitigation of potential label errors introduced by appearance transforms. The introduction of adversarial training enhances both the data diversity and segmentation robustness, while uncertainty rectification is designed to compensate for the potential label errors. The experimental results on sTBI brains demonstrate the efficacy of our proposed method and its advantages over state-of-the-art alternatives, highlighting the potential of our method in enabling more accurate segmentation in severe traumatic brains, which may aid clinical pipelines."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 12.
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,1.0,Introduction,"The automated detection and analysis of small vessels from non-invasive imaging data is critical for many clinical studies such as the research on cerebral small vessel disease(CSVD) [3], which is the most common vascular cause of dementia in Alzheimer's disease and related dementia (ADRD) [5]. According to [12], a brain vessel with a diameter less than 0.5mm is considered as a small vessel by most pathologists. Fortunately, with the recent advances in MRA at 7-Tesla [7] and black-blood MRI at 3-and 7-Tesla [10], it is now possible to detect the small cerebral vessels directly. Although the segmentation of large vascular structures has been well studied for many years [4,6,8,11,15], accurate and reliable small vessel segmentation remains a challenging task. In contrast to large vessels, small vessels usually exhibit the following two main characteristics (Fig. 1). ( 1) Its cross-section only occupies a few image voxels due to the limited resolution of imaging techniques such as the magnetic resonance angiography(MRA). This makes the regular assumption of tube-like shape for vessels often does not hold.(2) Small vessels often have weak intensities and low contrasts, which would be easily affected by noise or surrounding backgrounds. These characteristics are generally not well modeled by existing methods for vessel detection [4,6,8,11,15]. In each case, a maximal(minimal) intensity projection (left) and two cross sections of small vessels (right) were plotted. In (a) and (b), each cross-section corresponds to the line of the same color overlaid on the left panel (green: irregular appearances of small vessel cross-sections , red: low contrasts of small vessel cross-sections).Traditional vesselness filters typically characterize blood vessels based on hand-crafted features [4,8,15]. The inherent assumption about the regularity of tube-like vessel geometry, however, makes them sub-optimal for small vessel segmentation. In addition, most Hessian-based filters rely on complicated preprocessing including smoothing for the calculation of second derivatives, which can further weaken or even eliminate the contrasts of small vascular structures. While deep learning methods have been successfully applied in various segmentation tasks, large-scale annotated labels are hard to obtain to train supervised networks for the segmentation of highly variable small vessels. To overcome this challenge, a self-supervised deep learning approach was recently proposed that combines geometric models and deep neural networks to learn vessel flow directions [6], but it also assumes a circular tube-like vessel shape and is thus limited in segmenting arbitrary-shaped small vessel structures.In this work, we propose a novel self-supervised network that focus on challenges in small vessel segmentation with the following contributions: (1) Instead of assuming ideal tube-shaped vessels like [6,8], we propose an adaptive scheme for shape-aware estimation of oriented fluxes to model the irregular (noncircular) cross-section profiles of small vessels. (2) We propose the Local Contrast Attention (LCA) module based on a novel local contrast measure to enhance the small vessel pattern and suppress the background clutter simultaneously. (3) We propose a novel unsupervised learning framework that considers the characteristics of small vessel structures in relatively limited resolution for the first time. Comprehensive experiments show promising improvements in 3D datasets of multiple modalities compared with previous unsupervised approaches."
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,2.0,Method,"Our proposed framework for small vessel detection is shown in Fig. 2, a Ushaped network augmented with multiple novel LCA and LCE modules to learn a general representation of irregular-shaped small vessels by optimizing a selfsupervised loss of shape-aware flux. Formally, let Ω ⊆ R 3 denote the image domain. To characterize the irregular shape of the small vessel at each point x ∈ Ω, we estimate a principal vessel direction -→ ρ x and a set of radii values R(x) = {r i (x)|i = 1, 2, ..., m} that represent the vessel radius along m sampling directions on the unit sphere S 2 . Based on the estimated R(x) and -→ ρ x , we can compute the vesselness score f vs at x, which represents the likelihood of x being a vascular structure. The proposed network is trained to maximize the vessel Shape-Aware Flux. Given an input image I, our proposed network will generate three outputs: P , R = {r 1 , r 2 , ..., r m } and the reconstructed image Î. We denote P as a vector field that represents the principal direction for every point x ∈ Ω and R as a collection of scalar fields, where r i ∈ R is a scalar field over Ω that estimates the vessel radius alongA projected flux response along a direction -→ ρ , which generalizes conventional flux measures for circular shaped tubes [8], can be defined at x as follows:after discretization and normalization, where v(•) is the image gradient, -→The vesselness score of x can then be computed aswhere -→ ρ 1 = P (x) is the estimated principal direction at x and -→ ρ 2 , -→ ρ 3 are two orthogonal vectors in the cross-sectional plane of the vascular structure. In contrast to conventional flux measures for vessel segmentation, where only isotropic radius value is estimated, radius in different sampling directions d i are estimated adaptively in this work to fit the irregular-shaped vascular boundaries as f vs is maximized only if the sampling vectors -→ h i fit the vessel edges. Since the radius values can be different from each other, our proposed network is designed to handle the small vessels with irregular and non-circular cross-sections.Local Contrast Guided Attention. As shown in Fig. 1, geometric features of large vessels are well-presented due to their high signal-to-noise ratio(SNR).On the other hand, the low contrast of small vessel region makes it hard to distinguish the vascular structures from the background clutters. To address this issue, Fig. 3(a) illustrates our novel spatial attention module to enhance the small vessels based on regional contrast measure. In a vascular image, we assume the pixels inside a vessel have similar intensities even for small structures. Consider the estimated radius R(x) for x ∈ Ω, we define a local contrast D aswhereWith s = 1, D measures the intensity differences between x and x + -→ h i which locates on vascular boundaries for every sampling direction d i . We can contract or expand the vascular regions along its edges with different s. To measure the contrast inside and outside the vascular regions, we design two measures D in and D out as follows:  3) is evaluated on two opposite directions, D in and D out will both give low measures if x is an one-sided edge. Based on these three scenarios, we design a novel local contrast measure aswhere is a small constant to prevent the numerical explosion. Since the ratio of D out and D in gives large value, D LC ≈ 1 for x inside a vascular structure, which means the D LC measure of small vessels is on a similar scale to large vessels.For the latter two situations, D LC ≈ σ(0) = 0.5 will stand since D in ≈ D out . Therefore, the proposed D LC measure can enhance the small vascular structures and suppresses backgrounds and edges simultaneously.Guided by the local contrast map of images, we propose to incorporate a novel spatial attention module in our DL network. Following CBAM [16], we use two pooling layers to abstract the previous features as two feature maps. A local contrast map is computed based on a coarsely estimated R, which is generated by previous features. Then, all three maps are sent to a convolutional block to compute the final spatial attention map. By simply extending the LCA module, LCE module scales the previous features with the attention map to refine it.To include low-level contrast information, we insert our LCA and LCE module in the skip connections between encoders and decoders as shown in Fig. 2. As shown in Fig. 3(b-d), with the local contrast attention modules, our model can enhance the vesselness measure of the small vessels with low contrasts.Self-supervised Losses. Based on (2), we propose our flux-based loss L flux as the negative average vesselness score over the whole image spatial space. This cost function takes advantage of the clear edge separation of flux-based filter and robustness to irregular-shaped vessels due to our adaptive shape-aware flux computation. To ensure the vessel structure's continuity, we adopt the path continuity loss L path from [6]. Let us denote P (x) as the vascular principal direction at x, and P (x + t) as the principal direction at the location x + t, which we obtain by walking for t from x along P (x). L path is designed to maximize the inner product of these two vectors to encourages P to have a consistent and smooth direction along the vessels. Formally, L flux and L path are computed aswhere N is the total number of voxels and R(x) is the average radius of R(x) so that the length of the walking path is relative to the vessel size at x. Mean square error loss is applied between the reconstructed image Î and original image I to make sure the network learns the semantic meaningful features based on the previous success of reconstruction-based self-supervised learning [9]. So, the overall objective of our network is expressed aswhere λ 1 , λ 2 and λ 3 correspond to the coefficients of each objective."
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,3.0,Experimental Results,"Datasets. We used two public datasets and two in-house datasets in our experiments for 3D vessel segmentation. The VESSEL12 [14] contains 23 CT lung images and 3 of them are provided with sparsely annotated vessel and nonvessel locations along 3 axial slices, which we used as a test set. The TubeTk [2] consists of 109 3D brain MRA images. We used the 42 images with ground truths as the test set and the rest of the dataset for training. To better demonstrate the small vessel detection ability of our method, we collected a new brain MRA dataset on a 7T Simens Terra scanner from 31 subjects with voxel size of 0.2 × 0.2 × 0.4mm. For this dataset, we sparsely annotated 2200 small vessels(within 3 × 3 grids) locations on axial slices of 7 images according to the definition of the small vessel in [12] (Fig. 4(i-j)). The remaining 24 images constitute the training set. In addition, a black-blood MRI dataset was collected using a Siemens 3T Prisma scanner with voxel size of 0.5 × 0.5 × 0.5mm and separated by left and right hemispheres for a total of 56 image volumes. Dense manual segmentation of the lenticulostriate arteries(LSAs) was carefully performed by two experts.The dataset was divided into a training set with 21 subjects (42 volumes) and a test set with 7 subjects (14 volumes).Implementation Details. We performed the vessel segmentation tasks for each dataset to evaluate our model's performance. For comparison, we selected 4 conventional filters, including the Frangi, Sato, Meijering and OOF filter and a flow-based DL method [6]. For fair comparison, the same pre-processing procedures were applied for all methods using FreeSurfer 6.0 [13] and the SimpleITK toolbox [1]. In addition, for the black-blood LSA dataset, the image intensities were inversed to make sure the vascular voxels are brighter than backgrounds.To tune the network's hyper-parameters, we used 15% of the training data as the validation set. Finally, we set m = 128, λ 1 = 5 and λ 2 = λ 3 = 1 for our model. For both DL models, patch size is set to 64 × 64 × 64 and the networks are trained for 100 epochs. All experiments were conducted with Pytorch on one NVIDIA A5000 GPU with the Adam optimizer and a learning rate of 0.001.For all methods, the output is a vessel enhanced image. The enhanced image is binarized through a hard threshold. We thus found the best threshold by optimizing the metrics based on the validation sets and then applied the final threshold to test sets. We reported five metrics in Table 1, namely, the areaunder-curve(AUC), accuracy, sensitivity, specificity and dice score. For VES- SEL12 and 7T MRA dataset, we treated the segmentation tasks as classification problems and dropped the dice score metric since they do not have dense labels.Results and Discussion. We can clearly observe from Table 1 that our model outperforms all other methods by a significant margin for all datasets. Since the flow-based DL outperformed the conventional filters, we will use it for visual comparisons with our method. From Fig. 4(a-d), we can observe that our model generates more accurate segmentation with more true positives and less false negatives as compared to the flow-based DL method [6]. These results show that our model can better detect general vessel structures including large vessels.Small Vessel Segmentation. From Fig. 4(e-h), we can see the main improvement of our model over the flow-based DL method occurs often at areas of small vessels, where our results have much lower false negatives for small vessels missed by the other. The results on the 7T MRA data in Table 1 further demonstrate that our model achieved better performance on the small vessel detection task. Figure 4(k-l) provides a visual comparison of the performance of small vessel detection by our method and the flow-based DL, which shows that our model can more successfully segment the vessels with very small radius and low contrasts. In  addition, Fig. 5 shows that our estimated principal directions are better aligned with the vessel directions over the flow-based DL. Furthermore, our estimated radius can better capture the vascular shapes by fitting the asymmetric vessel boundaries. Thus, comparing with the flow-based DL, our model can produce sharper and clearer vesselness maps(Fig. 5).Ablation Study. To investigate the contributions of each proposed module in our approach, we performed ablation studies by training the network with each proposed module using the 7T MRA dataset. The results are shown in Table 2, where we used the flow-based DL method as the baseline for comparison. Note that CS stands for circular sampling and AS stands for the proposed shapeaware adaptive sampling. As can be seen, with all the proposed modules, we increase the AUC metric of the baseline by 4.87% on the test set, showing significant improvements with only minimal extra computational cost. Furthermore, we observed that all the proposed modules improve both the sensitivity and specificity since complicated small vessels can be better modeled by these components."
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,4.0,Conclusions,"In this paper, we proposed a self-supervised network for the detection of small vessels from 3D imaging data. Our method is designed to address existing challenges in small vessel detection arising from their irregular appearance in relatively limited resolution and low-contrast conditions. In comparison to previous methods, we demonstrated that our method is able to achieve superior performance on small vessel detection. For future work, we will also apply it to various clinical datasets to examine its power for CSVD detection in brain images."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,1.0,Introduction,"The field of medical image segmentation has been increasingly drawn to semisupervised learning (SSL) due to the great difficulty and cost of data labeling.By utilizing both labeled and unlabeled data, SSL can significantly reduce the need for labeled training data and address inter-observer variability [1,8,14].Typical SSL approaches involve techniques such as self-training, uncertainty estimation, and consistency regularization. Self-training aims to expand labeled training set by selecting the most confident predictions from the unlabeled data to augment the labeled data [23]. To obtain high-quality pseudo-labels, uncertainty estimation is often employed in self-training models. Various uncertainty estimation methods have been proposed to reduce the influence of ambiguous unlabeled data, e.g., Monte Carlo dropout [27] and ensemble-based methods [15,22]. Also, some metrics have been defined to quantify the degree of uncertainty. The most widely-used one is information entropy [18,27], where a threshold or a percentage is set to determine whether an unlabeled sample is reliable, i.e., its predicted label can be used as pseudo-label during the training phase. Besides pseudolabeling, it is common to add a consistency regularization in loss function [5,11,28]. For instance, UA-MT [27] and CoraNet [15] impose consistency constraints on the teacher-student model for specific regions (certain/uncertain area or both). URPC [12] uses multilevel extraction of multi-scale uncertainty-corrected features to moderate the anomalous pixel of consistency loss.Despite current progress, the performance of pseudo-labeling and consistency constraints has been limited for two reasons. First, defining an appropriate quantification criterion for reliability across various tasks can be challenging due to the inherent complexity of uncertainty. Second, most of the consistency constraints are imposed at decision space with the assumption that the decision boundary must be located at the low-density area, while the latent feature space of unlabeled data has not been fully exploited, and the low-density assumption may be incapable to guide model learning in the correct way.Recently, prototype alignment has been introduced into SSL. Prototypebased methods have the potential of capturing underlying data structure including unlabeled information, and optimizing the distribution of feature embeddings across various categories [3,17]. Existing semi-supervised segmentation methods based on prototype learning aim to learn each class prototype from sample averaging and leverage consistency constraints to train the segmentation network. U 2 PL [18] distinguishes reliable samples among unlabeled data by uncertainty estimation, and constructs prototypes for the whole dataset by class averaging its features with labeled sample and reliable unlabeled features. CISC-R [19] queries a guiding labeled image that shares similar semantic information with an unlabeled image, then estimates pixel-level similarity between unlabeled features and labeled prototypes, thereby rectifying the pseudo labels with reliable pixellevel precision. CPCL [24] introduces a cyclic prototype consistency learning framework to exploit unlabeled data and enhance the prototype representation.Overall, prototype learning has much room for improvement in semisupervised segmentation. As voxel-level averaging is only reliable for labeled data, current prototype learning approaches rely on labeled data and a small amount of unlabeled data, or learn prototypes separately for labeled and unlabeled data. In this way, they may not fully represent the distribution of the embedding space. Here raises the question:"
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,"Can we capture the embedding distribution by considering all voxels, including both labeled and unlabeled, and exploit the knowledge of the entire dataset?","To answer it, we propose to learn fused prototypes through uncertainty-based attention pooling. The fused prototypes represent the most representative and informative examples from both the labeled and unlabeled data for each class. The main contributions of our work can be summarized as follows:1) We develop a novel uncertainty-informed prototype consistency learning framework, UPCoL, by considering voxel-level consistency in both latent feature space (i.e., prototype) and decision space. 2) Different from previous studies, we design a fused prototype learning scheme, which jointly learns from labeled and unlabeled data embeddings. 3) For stable prototype learning, we propose a new entropy measure to qualify the reliability of unlabeled voxel and an attention-weighted strategy for fusion. 4) We apply UPCoL to two-class and three-class segmentation tasks. UPCoL outperforms the SOTA SSL methods by large margins. The labeled images go through the student model for supervised learning, while the unlabeled images go through the teacher model for segmentation and uncertainty estimation. In Uncertainty-informed Prototype Fusion module, we utilize the reliability map to fuse prototypes learned from labeled and unlabeled embeddings. The similarity between fused prototypes and feature embeddings at each spatial location is then measured for consistency learning."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2.0,Methodology,"Given a dataset D = {D l , D u }, the labeled set D l = {x l i , y l i } N i=1 contains N samples, and the unlabeled setx l i , x u i ∈ R H×W ×D represent the input with height H, width W , depth D, and y u i ∈ {0, 1, ..., C-1} H×W ×D . The proposed framework UPCoL includes a student model and a self-ensembling teacher model, each consisting of a representation head h and a segmentation head f . Figure 1 shows the overview."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2.1,Uncertainty Assessment,"To assess the uncertainty at both voxel-level and geometry-level, we adopt the same ensemble of classifiers as [22] using different loss functions, i.e., crossentropy loss, focal loss [9], Dice loss, and IoU loss. Unlike [22], which simply differentiates certain and uncertain regions based on the result discrepancy of four classifiers, we use the average of four prediction results and define an entropy-based measure to quantify the reliability for each voxel. Specifically, let f∈ R C denote the softmax probability for voxel at position (x, y, z) in i -th unlabeled image yielded by the segmentation head of the teacher model, where the segmentation result is the average over multiple classifiers (AMC), and C is the number of classes. The entropy is formulated in Eq. ( 1),Intuitively, voxels with high entropy are ambiguous. Thus, a reliability map can be defined accordingly, denoted by Φ (x,y,z) i, which enables the model to assign varying degrees of importance to voxels,) ).(2)"
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2.2,Prototype Consistency Learning,"Uncertainty-Informed Prototype Fusion. The prototypes from labeled and unlabeled data are first extracted separately. Both of them originate from the feature maps of the 3rd-layer decoder, which are upsampled to the same size as segmentation labels by trilinear interpolation. Let h l s,i be the output feature by the representation head of the student model for the i-th labeled image, and h u t,i be the hidden feature by the teacher representation head for the i-th unlabeled image. B l and B u denote the batch sizes of labeled and unlabeled set respectively, and (x, y, z) denotes voxel coordinate. For labeled prototype, the feature maps are masked directly using ground truth labels, and the prototype of class c is computed via masked average pooling [17,29]:For unlabeled data, instead of simply averaging features from the same predicted class, UPCoL obtains the prototypes in an uncertainty-informed manner, i.e., using a masked attention pooling based on each voxel's reliability:x,y,z 1 ŷu(x,y,z)Temporal Ensembling technique in Mean-Teacher architecture enhances model performance and augments the predictive label quality [16], leading to a progressive improvement in the reliability of predictive labels and the refinement of unlabeled prototypes throughout the training process. Thus, we adopt a nonlinear updating strategy to adjust the proportion of unlabeled prototypes for fusing the labeled prototypes and unlabeled prototypes, i.e.,where λ lab = 1 1+λcon , λ unlab = λcon 1+λcon , and λ con is the widely-used timedependent Gaussian warming up function [16]. During the training process, the proportion of labeled prototypes decreases from 1 to 0.5, while the proportion of unlabeled prototypes increases from 0 to 0.5. This adjustment strategy ensures that labeled prototypes remain the primary source of information during training, even as the model gradually gives more attention to the unlabeled prototypes.Consistency Learning. We adopt non-parametric metric learning to obtain representative prototypes for each semantic class. The feature-to-prototype similarity is employed to approximate the probability of voxels in each class,where the value of is fixed to 1e -8 , and CosSim(•) denotes cosine similarity. To ensure the accuarcy of the prototype, prototype-based predictions for labeled voxels expect to close to ground truth. And the prototype-based predictions for unlabeled voxels expect to close to segmentor prediction since prototype predictions are considered to be reliable aid. Then the prototype consistency losses for labeled and unlabeled samples are defined respectively as:where ŷu(x,y,z) i is the student model prediction of the i-th unlabeled sample at (x, y, z). Equation ( 7) is a variant of expanding training set by pseudo labels of the unlabeled data commonly adopted in SSL, with the difference that we use the reliability-aware pseudo labels at the voxel level. Finally, the total loss of our UPCoL network is shown in Eq. ( 8),3 Experimental ResultsDatasets. We evaluate our approach on three datasets: the pancreas dataset (82 CTA scans), the left atrium dataset (100 MR images), and a multi-center dataset for type B aortic dissection (TBAD, 124 CTA scans). The pancreas and left atrium datasets are preprocessed following previous studies [7,10,21,22,27,28].The TBAD dataset is well-annotated by experienced radiologists, with 100 scans for training and 24 for test, including both public data [25] and data collected by our team. The dataset was resampled to 1mm 3 and resized to 128 × 128 × 128, in accordance with [2]. For all three datasets, we use only 20% of the training data with labels and normalize the voxel intensities to zero mean and unit variance. Implementation Details. We adopt V-Net [13] as the backbone, and use the results of V-Nets trained with 20% and 100% labeled data as the lower and upper bounds, respectively. For the Mean-Teacher framework, the student network is trained for 10k iterations using Adam optimizer and learning rate 0.001, while the teacher is updated with exponential moving average (EMA) of the student's parameters. The batch size is 3, including 1 labeled and 2 unlabeled samples. Following [7,10,22], we randomly crop cubes of size 96×96×96 and 112×112×80 for the pancreas and left atrium datasets, respectively. In addition, we use 3fold cross validation and apply data augmentation by rotation within the range (-10 • , 10 • ) and zoom factors within the range (0.9, 1.1) for TBAD dataset during training, as proposed in [2,4]. Four performance metrics are adopted, i.e., Dice coefficient (Dice), Jaccard Index (Jac), 95% Hausdorff Distance (95HD), and Average Symmetric Surface Distance (ASD).Results on the Left Atrium Dataset and Pancreas-CT Dataset. We compare UPCoL with nine SOTA SSL methods, including consistency-based (MT [16], MC-Net [21], ASE-Net [6], URPC [12]), uncertainty-based (UA-MT [27], MC-Net+ [20]), and divergence-based (DTC [10], SimCVD [26], CoraNet [15]). The results (Table 1) demonstrate a substantial performance gap between the lower and upper bounds due to the limited labeled data. Remarkably, our proposed framework outperforms the theoretical upper bound (the second row) in terms of Dice, Jac, 95HD on the left atrium dataset, and 95HD, ASD on the pancreas dataset, suggesting that the unlabeled knowledge extracted from deep features is reliable and well complements the information that is not captured in the fully supervised prediction phase.Results on the Aortic Dissection Dataset. We compare with two SOTA SSL methods, the uncertainty-based method FUSSNet [22] and embedding-based method URPC [12], as well as two common SSL approaches, MT [16] and UA-MT [27]. As shown in Table 2, the proposed UPCoL obtains the best segmentation results and outperforms fully-supervised V-Net over 6% on Dice score, but only requires 20% labels. The accuracy of FUSSNet [22], URPC [12], and UPCoL surpassing the upper bound demonstrates the effectiveness of uncertainty and embedding-based approaches in exploiting the latent information of the data, particularly in challenging classification tasks. We further visualize the segmentation results on test data of different methods in Fig. 2. As can be seen, UPCoL achieves superior segmentation performance with fewer false positives and superior capability in capturing intricate geometric features, such as the vessel walls between True Lumen (TL) and False Lumen (FL), and effectively smoothing out rough portions of the manual annotation.Ablation Study. Here we investigate the contribution of key components, including the mean-teacher architecture (MT), the average multi-classifier (AMC) (to yield segmentation results), and the prototype learning (PL) strategy. As shown in Table 3, the MT model, which enforces a consistency cost between the predictions of student model and teacher model, outperforms vanilla V-Net by a large margin (over 5% on Dice), and AMC can also enhance the MT's performance (over 2% on Dice). Compared to the consistency cost in original MT model [16] (used by MT and MT+AMC), the prototype consistency leads to better performance. Especially, we compare different prototype learning strategies in three methods, MT+PL, CPCL*, and UPCoL, which have the same backbone and AMC module. MT+PL performs prototype learning only for labeled data, CPCL* learns prototypes for labeled and unlabeled data separately using the same strategy proposed in [24], and UPCoL learns fused prototypes. As can be seen, prototype learning using unlabeled data is beneficial for performance improvement, but it requires a well-designed mechanism. Here CPCL* is slightly worse than using only labeled data for prototype learning, potentially due to the isolation of updating labeled and unlabeled prototypes, which may hinder their interaction and prevent the full utilization of knowledge. This highlights the importance of fusing labeled and unlabeled prototypes. UPCoL, on the other hand, successfully merges labeled and unlabeled prototypes through the use of reliability maps, resulting in SOTA performance. This demonstrates the effectiveness of uncertainty-based reliability assessment and prototype fusion in fully leveraging both labeled and unlabeled information. In the Supplementary Materials, visualized results showcase improved predicted labels and unlabeled prototypes as training progresses."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"This paper presents a novel framework, UPCoL, for semi-supervised segmentation that effectively addresses the issue of label sparsity through uncertaintybased prototype consistency learning. To better utilize unlabeled data, UPCoL employs a quantitative uncertainty measure at the voxel level to assign degrees of attention. UPCoL achieves a careful and effective fusion of unlabeled data with labeled data in the prototype learning process, which leads to exceptional performance on both 2-class and 3-class medical image segmentation tasks. As future work, a possible extension is to allow multiple prototypes for a class with diversified semantic concepts, and a memory-bank-like mechanism could be introduced to learn prototypes from large sample pools more efficiently."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Table 3 .,"'Lb', 'Unlb' and 'Fuse' denote the labeled, unlabeled, and fused prototypes, respectively."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_63.
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is always a critical task as it can be used for disease diagnosis, treatment planning, and anomaly monitoring. Weakly supervised semantic segmentation attracts significant attention from medical image community since it greatly reduces the cost of dense pixel-wise labeling to get segmentation mask. In WSSS, the training labels are usually easier and faster to obtain, like image-level tags, bounding boxes, scribbles, or point annotations. This work only focuses on WSSS with image-level tags, like whether a tumor presents or not. In this field, previous WSSS works [10,14] are dominated by class activation map (CAM) [25] and its variants [4,18,21], which was firstly introduced as a tool to visualize saliency maps when making a class prediction.Meanwhile, denoising diffusion models [6,9] demonstrate superior performance in image synthesis than other generative models. Also, there are several works exploring the application of diffusion models to semantic segmentation in natural images [2,8]and medical images [15,[22][23][24]. To the best of our knowledge, Wolleb et al. [22] is the only work that introduces diffusion models to pixel-wise anomaly detection with only classification labels. They achieve this by utilizing an external classifier trained with image-level annotations to guide the reverse Markov chain. By passing the gradient of the classifier, the diffusion model gradually removes the anomaly areas during the denoising process and then obtains the anomaly map by comparing the reconstructed and original images However, this approach is based on the hypothesis that the classifier can accurately locate the target objects and that the background is not changed when removing the noise. This assumption does not always hold, especially when the distribution of positive images is diverse, and the reconstruction error can also be accumulated after hundreds of steps. As shown in Fig. 1, the reconstructed images guided by the gradient of non-kidney not only remove the kidney area but also change the content in the background. Another limitation of this method is the long inference time required for a single image, as hundreds of iterations are needed to restore the image to its original noise level. In contrast, CAM approaches need only one inference to get the saliency maps. Therefore, there is ample room for improvement in using diffusion models for WSSS task. In this work, we propose a novel WSSS framework with conditional diffusion models (CDM) as we observe that the predicted noises on different condition show difference. Instead of completely removing the noises from images, we calculate the derivative of the predicted noise after a few stages with respect to conditions so that the related objects are highlighted in the gradient map with less background misidentified. As the output of diffusion model is not differentiable with respect to the discrete condition input, we adopt the finite difference method, i.e., perturbing the condition embedding by a small amplitude and logging the change of the output with DDIM [19] generative process. In addition, our method does not require the full reverse denoising process for the noised images and may only need one or a few iterations. Thus the inference time of our method is comparable to that of CAM-based approaches. We evaluate our methods on two different tasks, brain tumor segmentation and kidney segmentation, and provide the quantitative results of both CAM based and diffusion model based methods as comparison. Our approach achieves state-of-the-art performance on both datasets, demonstrating the effectiveness of the proposed framework. We also conduct extensive ablation studies to analyze the impact of various components in our framework and provide reasoning for each design."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2.1,Training Conditional Denoising Diffusion Models,"Suppose that we have a sample x 0 from distribution D(x|y), and y is the condition. The condition y can be various, like different modality [20], inpainting [3] and low resolution images [17]. In this work, y ∈ {y 0 , y 1 } indicates the binary classification label, like brain CT scans without tumor vs. with tumor. We then gradually add Guassian noise to the original image sample with different level t ∈ {0, 1, ..., T } asWith fixed variance {β 1 , β 2 , ..., β t }, x t can be explicitly expressed by x 0 ,where α t := 1β t , ᾱt := t s=1 α s . Then a conditional U-Net [16] θ (x, t, y) is trained to approximate the reverse denoising process,The variance μ σ can be learnable parameters or a fixed set of scalars, and both settings achieve comparable results in [9]. As for the mean, after reparameterization with x t = √ ᾱt x 0 + √ 1ᾱt for ∼ N (0, I), the loss function can be simplified as:As for how to infuse binary condition y in the U-Net, we follow the strategy in [6], using a embedding projection function e = f (y), f ∈ R → R n , with n being the embedding dimension. Then the condition embedding is added to feature maps in different blocks. After training the denoising model, Tashiro et al. [3] proved that the network can yield the desired conditional distribution D(x|y) given condition y.Algorithm 1. Generation of WSSS prediction mask using differentiate conditional model with DDIM sampling Input: input image x with label y 1 , noise level Q, inference stage R, noise predictor θ , τ Output: prediction mask of label y 1 for all t from 1 to Q do) end for return a"
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2.2,Gradient Map w.r.t Condition,"Inspired by the finding in [2] that the denoising model extracts semantic information when performing reverse diffusion process, we aim to get segmentation mask from the sample generated by single or just several reverse Markov steps with DDIM [19]. The reason for using DDIM is that one can generate a sample x t-1 from x t deterministically if removing the random noise term via:When given the same images at noise level Q, but with different conditions, the noises predicted by the network θ are supposed to reflect the localization of target objects, that is equivalently x t-1 (x t , t, y 1 )x t-1 (x t , t, y 0 ) . This idea is quite similar to [22] if we keep sampling x t-1 until x 0 . However, it is not guaranteed that the condition y 0 does not change background areas besides the object needed to be localized. Therefore, in order to minimize the error caused by the generation process, we propose to visualize the sensitivity of x t-1 (x t , t, y 1 ) w.r.t condition y 1 , that is ∂xt-1 ∂y . Since the embedding projection function f (•) is not differentiable, we approximate the gradient using the finite difference method:) in which, τ is the moving weight from f (y 1 ) towards f (y 0 ). The weight τ can not be too close to 1, otherwise there is no noticeable gap between x t-1 and x t-1 , and we find τ = 0.95 gives the best performance. Algorithm 1 shows the detailed workflow of obtaining the segmentation mask of samples with label y 1 . Notice that we iterate the process (5) for R steps, and the default R in this work is set as 10, much smaller than Q = 400. The purpose of R is to amplify the change of x t-1 since the condition does not change the predicted noise a lot in one step.In addition, we find that the guidance of additional classifier can further boost the WSSS task, by passing the gradient ˆ ← θ (x t )s √ 1ᾱt ∇ xt logp φ (y|x t ), where p φ is the classifier and s is the gradient scale. Then, in Algorithm 1, ˆ 1 and ˆ 0 have additional terms guided by gradient of y 1 and y 0 , respectively. The ablation studies of related hyperparameters can be seen in Sect. 3.2."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.0,Experiments and Results,"Brain Tumor Segmentation. BraTS (Brain Tumor Segmentation challenge) [1] contains 2,000 cases of 3D brain scans, each of which includes four different MRI modalities as well as tumor segmentation ground truth. In this work, we only use the FLAIR channel and treat all types of tumor as one single class. We divide the official training set into 9:1 for training and validation purpose, and evaluate our method on the official validation set. For preprocessing, we slice each volume into 2D images, following the setting in [5]. Then the total number of slices in training set is 193,905, and we report metrics on the 5802 positive samples in the test set Kidney Segmentation. This task is conducted on dataset from ISBI 2019 CHAOS Challenge [12], which contains 20 volumes of T2-SPIR MR abdominal scans. CHAOS provides pixel-wise annotation for several organs, but we focus on the kidney. We split the 20 volumes into four folds for cross-validation, and then decompose 3D volumes to 2D slices in every fold. In the test stage, we remove slices with area of interest taking up less than 5% of the total area in the slice, in order to avoid the influence of extreme cases on the average results.Only classification labels are used during training the diffusion models, and segmentation masks are used for evaluation in the test stage. For both datasets, we repeat the evaluation protocols for four times and report the average metrics and their standard deviation on test set."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.1,Implementation Details,"As for model architecture, we use the same setting as in [8]. The diffusion model is based on U-Net with encoder and decoder consisting of resnet blocks. We implement two different versions of the proposed method, one without classifier guidance and one with it. To differentiate the two in the experiments, we continue to call them former CDM, and the latter CG-CDM. The classifier used in CG-CDM is the same as the encoder of the diffusion model. We stop training the diffusion model after 50,000 iterations or 7 days, and the classifier is trained for 20,000 iterations. We choose AdamW as the optimizer with learning rate being 1e-5 and 5e-5 for diffusion model and classifier. The batch sizes for both datasets are 2. The implementation of all methods in this work is based on PyTorch library, and all experiments are run on a single NVIDIA RTX 2080Ti. "
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Types Methods,Dice↑ mIoU↑ HD95↓ infer time CAM GradCAM [18] 0.235 ± 0.075 0.149 ± 0.051 44.4 ± 8.9 3.79 s GradCAM++ [4] 0.281 ± 0.084 0.187 ± 0.059 32.6 ± 6.0 3.59 s ScoreCAM [21] 0.303 ± 0.053 0.202 ± 0.039 32.7 ± 2.1 27.0 s LayerCAM [11]  Types Methods Dice↑ mIoU↑ HD95↓CAM GradCAM [18] 0.105 ± 0.017 0.059 ± 0.010 33.9 ± 5.1 GradCAM++ [4] 0.147 ± 0.016 0.085 ± 0.010 28.5 ± 4.5 ScoreCAM [21] 0.135 ± 0.024 0.078 ± 0.015 32.1 ± 6.7 LayerCAM [11]  FSL N/A 0.847 ± 0.011 0.765 ± 0.023 3.6 ± 1.7
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.2,Results,"Comparison with State of the Arts. We benchmark our methods against previous WSSS works on two datasets in Table 1 & 2, in terms of dice score, mean intersection over union (mIoU), and Hausdorff distance (HD95). For CAM based methods, we include the classical GradCAM [18] and GradCAM++ [4], as well as two more recent methods, ScoreCAM [21] and LayerCAM [11]. The implementation of these CAM approaches is based on the repository [7]. For diffusion based methods, we include the only diffusion model for medical image segmentation in the WSSS literature, namely CG-Diff [22]. We follow the default setting in [22], setting noise level Q = 400 and gradient scale s = 100. We also present the results under the fully supervised learning setting, which is the upper bond of all WSSS methods (Fig. 2).From the results, we can make several key observations. Firstly, our proposed method, even without classifier guidance, outperform all other WSSS methods including the classifier guided diffusion model CG-Diff on both datasets for all three metrics. When classifier guidance is provided, the improvement gets even bigger, and CG-CDM can beat other methods regarding segmentation accuracy. Secondly, all WSSS methods have performance drop on kidney dataset compared with BraTS dataset. This demonstrates that the kidney segmentation task is a more challenging task for WSSS than brain tumor task, which may be caused by the small training size and diverse appearance across slices in the CHAOS dataset. Time Efficiency. Regarding inference time for different methods, as shown in Table 1, both CDM and CG-CDM are much faster than CG-Diff. The default noise level Q is set as 400 for all diffusion model approaches, and our methods run 10 iterations during the denoising steps. For all CAM-based approaches, we add augmentation smooth and eigen smooth suggested in [7] to reduce noise in the prediction mask. This post-processing greatly increases the inference time. Without the two smooth methods, the inference time for GradCAM is 0.031 s, but the segmentation accuracy is significantly degraded. Therefore, considering both inference time and performance, our method is a better option than CAM for WSSS.Ablation Studies. There are several important hyperparameters in our framework, noise level Q, number of iterations R, moving weight τ , and gradient scale s. The default setting is CG-CDM on BraTS dataset with Q = 400, R = 10, τ = 0.95, and s = 10. We evaluate the influence of one hyperparameter at a time by keeping other parameters at their default values. As illustrated in Fig. 3, a few observations can be made: (1) Either too large or too small noise level can negatively influence the performance. When Q is small, most spatial information is still kept in x t and the predicted noise by diffusion model contains no semantic knowledge. When Q is large, most of the spatial information is lost and the predicted noise can be distracted from original structure. Meanwhile, larger number of iterations can lightly improve the dice score at the beginning. When R gets too high, the error in the background is also accumulated after too many iterations. (2) We try different τ in the range (0, 1.0). Small τ leads to more noises in the background when calculating the difference in different conditions. On the other hand, as τ gets close to 1, the difference between x t-1 and x t-1 becomes minor, and the gradient map mainly comes from the guidance of the classifier, making localization not so accurate. Thus, τ = 0.95 becomes the optimal choice for this task. (3) As for gradient scale, Fig. 3 shows that before s = 100, larger gradient scale can boost the CDM, because at this time, the gradient from the classifier is at the same magnitude as the difference caused by the changed condition embedding. When the guidance of the classifier becomes dominant, the dice score gets lower as the background is distorted by too large gradients."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,4.0,Conclusion,"In this paper, we present a novel weakly supervised semantic segmentation framework based on conditional diffusion models. Fundamentally, the essence of generative approaches on WSSS is maximizing the change in class-related areas while minimizing the noise in the background. Our methods are designed around this rule to enhance the state-of-the-art. First, existing work that utilizes a trained classifier to remove target objects leads to unpredictable distortion in other areas, thus we decide to iterate the reverse denoising process for as few steps as possible. Second, to amplify the difference caused by different conditions, we extract the semantic information from gradient of the noise predicted by the diffusion model. Finally, this rule also applies to all other designs and choice of hyperparameters in our framework. When compared with latest WSSS methods on two public medical image segmentation datasets, our method shows superior performance regarding both segmentation accuracy and inference efficiency."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 72.
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,1.0,Introduction,"Glioma is one of the most common malignant brain tumors with varying degrees of invasiveness [1]. Brain Tumor Semantic segmentation of gliomas based on 3D spatially aligned Magnetic Resonance Imaging (SAMM-BTS) is crucial for accurate diagnosis and treatment planning. Unfortunately, radiologists suffer from spending several hours manually performing the SAMM-BTS task in clinical practice, resulting in low diagnostic efficiency. In addition, manual delineation requires doctors to have high professionalism. Therefore, it is necessary to design an efficient and accurate glioma lesion segmentation algorithm to effectively alleviate this problem and relieve doctors' workload and improve radiotherapy quality.With the rise of deep learning, researchers have begun to study deep learning-based image analysis methods [2,37]. Specifically, many convolutional neural network-based (CNN-based) models have achieved promising results [3][4][5][6][7][8]. Compared with natural images, medical image segmentation often requires higher accuracy to make subsequent treatment plans for patients. U-Net reaches an outstanding performance on medical image segmentation by combining the features from shallow and deep layers using skipconnection [9][10][11]. Based on U-Net, Brugger. et al. [12] proposed a partially reversible U-Net to reduce memory consumption while maintaining acceptable segmentation results. Pei et al. [13] explored the efficiency of residual learning and designed a 3D ResUNet for multi-modal brain tumor segmentation. However, due to the lack of global understanding of images for convolution operation, CNN-based methods struggle to model the dependencies between distant features and make full use of the contextual information [14]. But for semantic segmentation tasks whose results need to be predicted at pixel-level or voxel-level, both local spatial details and global dependencies are extremely important.In recent years, models based on the self-attention mechanism, such as Transformer, have received widespread attention due to their excellent performance in Natural Language Processing (NLP) [15]. Compared with convolution operation, the self-attention mechanism is not restricted by local receptive fields and can capture long-range dependencies. Many works [16][17][18][19] have applied Transformers to computer vision tasks and achieved favorable results. For classification tasks, Vision Transformer (ViT) [19] was a groundbreaking innovation that first introduced pure Transformer layers directly across domains. And for semantic segmentation tasks, many methods, such as SETR [20] and Segformer [21], use ViT as the direct backbone network and combine it with a taskspecific segmentation head for prediction results, reaching excellent performance on some 2D natural image datasets. For 3D medical image segmentation, Vision Transformer has also been preferred by researchers. A lot of robust variants based on Transformer have been designed to endow U-Net with the ability to capture contextual information in long-distance dependencies, further improving the semantic segmentation results of medical images [22][23][24][25][26][27]. Wang et al. [25] proposed a novel framework named TransBTS that embeds the Transformer in the bottleneck part of a 3D U-Net structure. Peiris et al. [26] introduced a 3D Swin-Transformer [28] to segmentation tasks and first incorporated the attention mechanism into skip-connection.While Transformer-based models have shown effectiveness in capturing long-range dependencies, designing a Transformer architecture that performs well on the SAMM-BTS task remains challenging. First, modeling relationships between 3D voxel sequences is much more difficult than 2D pixel sequences. When applying 2D models, 3D images need to be sliced along one dimension. However, the data in each slice is related to three views, discarding any of them may lead to the loss of local information, which may cause the degradation of performance [29]. Second, most existing MRI segmentation methods still have difficulty capturing global interaction information while effectively encoding local information. Moreover, current methods just stack modalities and pass them through a network, which treats each modality equally along the channel dimension and may ignore the contribution of different modalities. To address the above limitations, we propose a novel encoder-decoder model, namely DBTrans, for multi-modal medical image segmentation. In the encoder, two types of window-based attention mechanisms, i.e., Shifted Window-based Multi-head Self Attention (Shifted-W-MSA) and Shuffle Window-based Multi-head Cross Attention (Shuffle-W-MCA), are introduced and applied in parallel to dual-branch encoder layers, while in the decoder, in addition to Shifted-W-MSA mechanism, Shifted Window-based Multi-head Cross Attention (Shifted-W-MCA) is designed for the dual-branch decoder layers. These mechanisms in the dual-branch architecture greatly enhance the ability of both local and global feature extraction. Notably, DBTrans is designed for 3D medical images, avoiding the information loss caused by data slicing.The contributions of our proposed method can be described as follows: 1) Based on Transformer, we construct dual-branch encoder and decoder layers that assemble two attention mechanisms, being able to model close-window and distant-window dependencies without any extra computational cost. 2) In addition to the traditional skipconnection structure, in the dual-branch decoder, we also establish an extra path to facilitate the decoding process. We design a Shifted-W-MCA-based global branch to build a bridge between the decoder and encoder, maintaining affluent information of the segmentation target during the decoding process. 3) For the multi-modal data adopt in the task of SAMM-BTS, we improve the channel attention mechanism in SE-Net by applying SE-weights to features from both branches in the encoder and decoder layers. By this means, we implicitly consider the importance of multiple MRI modalities and two window-based attention branches, thereby strengthening the fusion effect of the multi-modal information from a global perspective."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.0,Methodology,"Figure 1 shows the overall structure of the proposed DBTrans. It is an end-to-end framework that has a 3D patch embedding along with a U-shaped model containing an encoder and a decoder. The model takes MRI data of D×H ×W ×C with four modalities stacked along channel dimensions as the input. The 3D patch embedding converts the input data to feature embedding e 1 ∈ R D 1 ×H 1 ×W 1 ×C 1 which will be further processed by encoder layers. At the tail of the decoder, the segmentation head takes the output of the last layer and generates the final segmentation result of D × H × W × K."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.1,Dual-Branch in Encoder,"As shown in Fig. 1(b), the encoder consists of four dual-branch encoder layers (one bottleneck included). Each encoder layer contains two consecutive encoder blocks and a 3D patch merging to down-sample the feature embedding. Note that there is only one encoder block in the bottleneck. The encoder block includes a dual-branch architecture with the local feature extraction branch, the global feature extraction branch, and the channel-attention-based dual-branch fusion module. After acquiring the embedding 2] which are then separately fed into two branches."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Shifted-W-MSA-Based Local Branch. The image embedding e,"] is fed into the local branch in the encoder block. In the process of window partition (denoted as WP), e i 1 is split into non-overlapping windows after a layer normalization (LN) operation to obtain the window matrix m i 1 . Since we set M as 2, the input of 4×4×4 size is uniformly divided into 8 windows of 2 × 2 × 2. Following 3D Swin-Transformer [27], we introduce MSA based on the shifted-window partition to the second block in an encoder layer. During window partition of Shifted-W-MSA, the whole feature map is shifted by half of the window size, i.e., M  2 , M 2 , M 2 . After the window partition, W -MSA is applied to calculate multi-head self-attention within each window. Specifically, for window matrix m i 1 , we first apply projection matrices W i Q ,W i K , and andV i 1 matrices (the process is denoted as Proj). After projection, multi-head selfattention calculation is performed on Q i 1 ,K i 1 , and V i 1 to get the attention score of every window. Finally, we rebuild the feature map from the windows, which serves as the inverse process of the window partition. After calculating the attention score, other basic components in Transformer are also employed, that is, layer normalization (LN), as well as a multi-layer perceptron (MLP) with two fully connected layers and a Gaussian Error Linear Unit (GELU). Residual connection is applied after each module. The whole process can be expressed as follows:where ẑi 1 represents the attention score after W -MSA calculation, "" Shifted-"" represents that we use the shifted-window partition and restoration in the second block of every encoder layer. o i 1 is the final output of the local branch. Shuffle-W-MCA-Based Global Branch. Through the local branch, the network still cannot model the long-distance dependencies between non-adjacent windows in the same layer. Thus, Shuffle-W-MCA is designed to complete the complementary task. After the window-partition process that converts the embedding, inspired by ShuffleNet [35], instead of moving the channels, we propose to conduct shuffle operations on the patches in different windows. The patches at the same relative position in different windows are rearranged together in a window, and their position is decided by the position of the window they originally belong to. Then, this branch takes the query from the local branch, while generating keys and values from m i 2 to compute cross-attention scores. Such a design aims to enable the network to model the relationship between a window and other distant windows. Note that we adopt the projection function Proj from the local branch, indicating that the weights of the two branches are shared. Through the shuffle operation, the network can generate both local and global feature representations without setting additional weight parameters. The whole process can be formulated as follows:(In the second block, we get the final output o i 2 of the layer through the same process."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.2,Dual-Branch in Decoder,"As shown in Fig. 1(c), the decoder takes the output of the encoder as the input and generates the final prediction through three dual-branch decoder layers as well as a segmentation head. Each decoder layer consists of two consecutive decoder blocks and a 3D patch expanding layer. As for the j-th decoder layer (j ∈ [1, 3]), the embedding d j ∈ R D j ×H j ×W j ×C j is also divided into the feature maps 2] . We further process d j 1 , d j 2 using a dual-branch architecture similar to that of the encoder, but with an additional global branch based on Shifted-W-MCA mechanism. Finally, the segmentation head generates the final segmentation result of D × H × W × K, where K represents the number of classes. The local branch based on Shifted-W-MSA is the same as that in the encoder and will not be introduced in this section."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Shifted-W-MCA-Based Global Branch.,"Apart from employing Shifted-W-MSA to form the local branch of the decoder layer, we design a novel Shifted-W-MCA mechanism for the global branch to ease the information loss during the decoding process and take full advantage of the features from the encoder layers. The global branch receives the query matrix from the split feature map d j 2 ∈ R D j ×H j ×W j ×[C j /2] , while receiving key and value matrices from the encoder block in the corresponding stage, denoted as Q j 2 , K e i , and V e i . The process of Shifted-W-MCA can be formulated as follows:where ẑj 2 denotes the attention score after MCA calculation, o j 2 denotes the final output of the global branch."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.3,Channel-Attention-Based Dual-Branch Fusion,"As shown in Fig. 1(b) and Fig. 1(c), the dual-branch fusion module is based on the channel attention. For the block of the m -th (m ∈ [1,3]) encoder or decoder layer, the dual-branch fusion module combines the features 2]  from the two extraction branches, obtaining a feature map filled with abundant multiscale information among different modalities. Subsequently, the dependencies between the feature channels within the individual branches are implicitly modeled with the SE-Weight assignment first proposed in SE-Net [30]. Different from SE-Net, we dynamically assign weights for both dual-branch fusion and multi-modal fusion. The process of obtaining the attention weights can be represented as the formula (5) below:whereis the attention weight of a single branch. Then, the weight vectors of the two branches are re-calibrated using a Softmax function. Finally, the weighted channel attention is multiplied with the corresponding scale feature map to obtain the refined output feature map with richer multi-scale feature information:where "" "" represents the operation of element-wise multiplication and ""Cat"" represents the concatenation. The concatenated output O, serving as the dual-branch output of a block in the encoder or decoder, implicitly integrates attention interaction information within individual branches across different channels/modalities, as well as across different branches."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.4,Training Details,"For the loss function, the widely used cross entropy (CE) loss and Dice loss [32] are introduced to train our DBTrans. Here we use parameter γ to balance the two loss parts. Our network is implemented based on PyTorch, and trained for 300 epochs using a single RTX 3090 with 24G memory. The weights of the network were updated using the Adam optimizer, the batch size was set to 1, and the initial learning rate was set to 1 × 10 -4 . A cosine decay scheduler was used as the adjustment strategy for the learning rate during training. We set the embedding dimensions C 0 as 144. Following previous segmentation methods, the parameter γ is set to 0.5. "
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,3.0,Experiments and Results,"Datasets. We use the Multimodal Brain Tumor Segmentation Challenge (BraTS 2021 [33,34,38]) as the benchmark training set, validation set, and testing set. We divide the 1251 scans provided into 834, 208, and 209 (in a ratio of 2:1:1), respectively for training and testing. The ground truth labels of GBM segmentation necrotic/active tumor and edema are used to train the model. The BraTS 2021 dataset reflects real clinical diagnostic species and has four spatially aligned MRI modality data, namely T1, T1CE, T2, and Flair, which are obtained from different devices or according to different imaging protocols. The dataset contains three distinct sub-regions of brain tumors, namely peritumoral edema, enhancing tumor, and tumor core. The data augmentation includes random flipping, intensity scaling and intensity shifting on each axis with probabilities set to 0.5, 0.1 and 0.1, respectively.Comparative Experiments. To evaluate the effectiveness of the proposed DBTrans, we compare it with the state-of-the-art brain tumor segmentation methods including six Transformer-based networks Swin-Unet [27], TransBTS [25], UNETR [22], nnFormer [23], VT-Unet-B [26], NestedFormer [36] as well as the most basic CNN network 3D U-Net [31] as the baseline. During inference, for any size of 3D images, we utilize the overlapping sliding windows technique to generate multi-class prediction results and take average values for the voxels in the overlapping region. The evaluation strategy adopted in this work is consistent with that of VT-Unet [26]. For other methods, we used the corresponding hyperparameter configuration mentioned in the original papers and reported the average metrics over 3 runs.  Ablation Study. To further verify the contribution of each module, we establish the ablation models based on the modules introduced above. Note that, DP-E represents the dual-branch encoder layer, while DB-D represents the dual-branch decoder layer. When the dual-branch fusion is not included, we do not split the input, and simply fuse the features from the two branches using a convolution layer. In all, there are 5 models included in this ablation study:  2, After applying our proposed dual-branch encoder and decoder layers to the baseline model, the average Dice score notably increased by 2.13. Subsequently, applying the dual-branch fusion module also prominently contributes to the performance of the model by an improvement of 0.83 on the Dice score. Notably, our dual-branch designs achieve higher performance while also reducing the number of parameters required. This is because we split the original feature embedding into two parts, thus the channel dimensions of features in two branches are halved."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,4.0,Conclusion,"In this paper, we innovatively proposed an end-to-end model named DBTrans for multimodal medical image segmentation. In DBTrans, first, we well designed the dual-branch structures in encoder and decoder layers with Shifted-W-MSA, Shuffle-W-MCA, and Shifted-W-MCA mechanisms, facilitating feature extraction from both local and global views. Moreover, in the decoder, we establish a bridge between the query of the decoder and the key/value of the encoder to maintain the global context during the decoding process for the segmentation target. Finally, for the multi-modal superimposed data, we modify the channel attention mechanism in SE-Net, focusing on exploring the contribution of different modalities and branches to the effective information of feature maps. Experimental results demonstrate the superiority of DBTrans compared with the state-of-the-art medical image segmentation methods."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,1.0,Introduction,"Convolutional neural networks (CNNs) have been widely used for medical image segmentation because of their speed and accuracy [11,18]. Nevertheless, given the local receptive fields of convolutional layers, long-range spatial correlations are mainly captured through consecutive convolutions and pooling. For computationally demanding 3D segmentation, the receptive fields and abstract levels can be more limited than in 2D as fewer layers can be used. To balance between computational complexity and network capability, input size reductions by image downsampling and patch-wise training are common approaches. However, CNNs trained with downsampled images can be suboptimal when applied on the original resolution, and the receptive field of patch-wise training can be largely reduced depending on the patch size.With the introduction of Transformers [24] and their vision alternatives [6,19], the self-attention mechanism for long-range dependencies has been adopted to medical image segmentation with promising results [5,7,9,27]. These approaches form a sequence of samples by either using the pixel values of lowresolution features [7,27] or by dividing an image into smaller patches [5,9], and the multi-head attention is used to learn the dependencies among samples. Although these self-attention approaches allow capturing of long-range dependencies, as the computational requirements are proportional to sequence lengths and patch sizes which are proportional to image sizes, size-reduction approaches are needed for large images especially in 3D.As image size reduction is usually required for large images, it is desirable to have a model that is robust to training image resolution so that the trained model can be applied to higher-resolution images with decent accuracy. Furthermore, as self-attention of Transformers allows better expressiveness through high-order channel and sample mixing [15,23], incorporating self-attention in an efficient way can be beneficial. To gain these advantages, here we propose the HartleyMHA model which is a resolution-robust and parameter-efficient network architecture with frequency-domain self-attention for 3D image segmentation. This model is based on the Fourier neural operator (FNO) [17], which is a deep learning model that learns mappings between functions in partial differential equations (PDEs) and has the appealing properties of zero-shot super-resolution and global receptive field. Our contributions include: 1. To utilize the FNO for computationally expensive 3D segmentation, we modify it by using the Hartley transform with shared model parameters in the frequency domain. Residual connections [10] and deep supervision [14] are also introduced. These reduce the number of model parameters by orders of magnitude and improve accuracy. We call it the HNOSeg model. 2. As only low-frequency components are required for decent segmentation results, multi-head self-attention can be efficiently applied in the frequency domain. This allows high-order combination of features to improve the expressiveness of the model. We call it the HartleyMHA model. 3. We compare our proposed models with other models on different training image resolutions to study their robustness. This provides useful insights that are usually unavailable in other studies.Experimental results on the BraTS'19 dataset [2,3,22] show that the proposed models have superior robustness to training image resolution than other tested models with less than 1% of their model parameters."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.1,Fourier Neural Operator,"FNO is a deep learning model for learning mappings between functions in PDEs without the PDEs provided [17]. By formulating the solution in the continuous space based on the Green's function [16], FNO can learn a single set of model parameters for multiple resolutions. For computationally expensive 3D segmentation, such zero-shot super-resolution capability is advantageous as a model trained with lower-resolution images can be applied on higher-resolution images with decent accuracy. The neural operator is formulated as iterative updates:where u t (x) ∈ R du t is a function of x. W ∈ R du t+1 ×du t is a learnable linear transformation and σ accounts for normalization and activation. In our work, D ⊂ R 3 represents the 3D imaging space, and u t (x) are the outputs of hidden layers with d ut channels. K is the kernel integral operator with κ ∈ R du t+1 ×du t a learnable kernel function. As (Ku t ) is a convolution, it can be efficiently solved by the convolution theorem which states that the Fourier transform (F) of a convolution of two functions is the pointwise product of their Fourier transforms:Therefore, each pointwise product at k is realized as a matrix multiplication. When the fast Fourier transform is used in implementation, k ∈ N 3 are non-negative integer coordinates, and each k has a learnable R(k). As mainly low-frequency components are required for image segmentation, only k i ≤ k max,i corresponding to the lower frequencies in each dimension i are used to reduce model parameters and computation time."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.2,Hartley Neural Operator (HNO),"As the FNO requires complex number operations in the frequency domain, the computational requirements such as memory and floating point operations are higher than with real numbers. Therefore, we use the Hartley transform instead, which is an integral transform alternative to the Fourier transform [8]. The Hartley transform (H) converts real-valued functions to real-valued functions, which is related to the Fourier transform as (Hf ) = Real(Ff ) -Imag(Ff ). The convolution theorem of discrete Hartley transform is more complicated [4], and the kernel integration in (1) becomes: 3 is the size of the frequency domain. R and Û are N -periodic in each dimension 1 . Similar to using (2), the models built using (3) have tens of million parameters even with small k max (e.g., (14,14,10)). Therefore, instead of using a different R(k) at each k, we use the same (shared) R for all k and (3) becomes:This is equivalent to applying a convolution layer with the kernel size of one in the frequency domain. We find that using (4) simplifies the computation and largely reduces the number of parameters without affecting the accuracy."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.3,Hartley Multi-head Attention (MHA),"As real instead of complex numbers are used in (4), multi-head attention in [24] can be applied in the frequency domain for high-order feature combination.As k max can be much smaller than the image size for image segmentation, the sequence length (number of voxels) can be largely reduced. With (4), the query, key, and value matrices (Q, K, V ) of self-attention can be computed as:whereAlthough N f can be relatively small, the computation and memory requirements of computing QK T can still be demanding. For example, k max = (14, 14, 10) corresponds to an attention matrix with around 246M elements. To remedy this, for each Q, K, and V , we group the feature vectors with a patch size of 2 × 2 × 2 voxels in the frequency domain and their matrix sizes becomeThis reduces the number of elements in QK T by 64 times. The self-attention can then be computed as:where SELU represents the scaled exponential linear unit [13]. Similar to [21], we find that using softmax in self-attention results in suboptimal segmentations, thus the SELU was chosen after testing with multiple activations. Furthermore, we find that position encoding is unnecessary. The result of ( 6) can be rearranged back to the original shape in the frequency domain so that the inverse Hartley transform can be applied. The multi-head attention can be used with (6)."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.4,Network Architectures -HNOSeg and HartleyMHA,"Figure 1 shows the network architecture. We call it HNOSeg with the HNO blocks and HartleyMHA with the Hartley MHA blocks. Different from the FNO in [17], residual connections [10] and deep supervision [14] are used to improve the training stability, convergence, and accuracy. As the batch size is usually small for memory demanding 3D segmentation, layer normalization (LN) is used [1]. The SELU [13] is used as the activation function, and the softmax function is used to produce the final prediction scores. Similar to the Fourier transform, the Hartley transform provides a global receptive field as all voxels are used to compute the value at each k, thus pooling is not required. As using the original image resolution usually results in out-of-memory errors in 3D segmentation, downsampling the inputs and then upsampling the predictions may be required. Instead of using traditional image resampling methods, we use a convolutional layer with the kernel size and stride of two right after the input layer, and replace the output convolutional layer by a transposed convolutional layer with the kernel size and stride of two (red blocks in Fig. 1). In this way, the model can learn the optimal resampling approach. In the experiments, k max = (14, 14, 10) so that it can be used with the lowest tested training resolution of 60 × 60 × 39. Other hyperparameters such as d ut+1 , N h , and N B were obtained empirically for decent segmentations when training with the original image resolution. As each HNO block and Hartley MHA block can be implemented as a deep-learning layer in commonly used libraries, they can be easily adopted by other architectures."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.5,Training Strategy,"The images of different modalities are stacked along the channel axis to provide a multi-channel input. As the intensity ranges across modalities can be quite different, intensity normalization is performed on each image of each modality. Image augmentation with rotation (axial, ±30 • ), shifting (±20%), and scaling ([0.8, 1.2]) is used and each image has an 80% chance to be transformed. The Adamax optimizer [12] is used with the cosine annealing learning rate scheduler [20], with the maximum and minimum learning rates as 10 -2 and 10 -3 , respectively. The Pearson's correlation coefficient loss is used as it is robust to learning rate for image segmentation [25], and it consistently outperformed the Dice loss and weighted cross-entropy in our experiments. An NVIDIA Tesla P100 GPU with 16 GB memory is used with a batch size of one and 100 epochs, and Keras in TensorFlow 2.6.2 is used for implementation. Note that small batch sizes are common in 3D segmentation given the high memory requirement."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3.1,Data and Experimental Setups,"The dataset of BraTS'19 with 335 cases of gliomas was used, each with four modalities of T1, post-contrast T1, T2, and T2-FLAIR images with 240 × 240 × 155 voxels [3]. There is also an official validation dataset of 125 cases in the same format without given annotations. Models were trained with images downsampled by different factors (1, 2, 3, and 4) to study the robustness to image resolution. In training, we split the training dataset (335 cases) into 90% for training and 10% for validation. In testing, each model was tested on the official validation dataset (125 cases) with 240 × 240 × 155 voxels regardless of the downsampling factor. The predictions were uploaded to the CBICA Image Processing Portal3 for the results statistics of the ""whole tumor"" (WT), ""tumor core"" (TC), and ""enhancing tumor"" (ET) regions [3]. We compare our proposed HNOSeg and HartleyMHA models with three other models:1. V-Net-DS [26]: a V-Net with deep supervision representing the commonlyused encoding-decoding architectures. 2. UTNet [7]: a U-Net enhanced by the Transformer's attention mechanism. 3. FNO [17]: original FNO without shared parameters, residual connections, and deep supervision. The same hyperparameters as HNOSeg were used. The learnable resampling approach in Sect. 2.4 was applied to all models. Note that our goal is not competing for the best accuracy but studying the robustness to image resolution. Although only the results of a dataset are shown because of the page limit, the characteristics of the proposed models can be demonstrated through this challenging multi-modal brain tumor segmentation problem."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3.2,Results and Discussion,"Figure 2 and Table 1 show comparisons of resolution robustness among tested models, and Table 2 shows the computational costs during inference. At the original resolution, V-Net-DS and UTNet outperformed HNOSeg and HartleyMHA by less than 3% in the Dice coefficient on average, but HNOSeg and HartleyMHA only had less than 50k model parameters which were less than 1% of V-Net-DS and UTNet. FNO performed worst with the most parameters (144.5M). As the resolution decreased, the accuracies of V-Net-DS and UTNet decreased almost linearly with the downsampling factor, while HNOSeg and HartleyMHA were more robust. When the downsampling factor changed from 1 to 3, the average Dice coefficients of V-Net-DS and UTNet decreased by more than 14.8%, while those of HNOSeg and HartleyMHA only decreased by less than 2.5%. Similar trends can be observed for the 95% Hausdorff distance, except that FNO performed surprisingly well in this aspect. HartleyMHA performed better overall than HNOSeg. Note that we fixed k max for the consistency among models in the experiments, which can be adjusted for better results in other situations.For computation cost, Table 2 shows that V-Net-DS and UTNet had shorter inference times than HNOSeg and HartleyMHA, though all models used less  Figure 3 shows the visual comparisons of the segmentation results on an unseen case. Consistent to Fig. 2 With such superior robustness to image resolution, HNOSeg and Hart-leyMHA can be trained with lower-resolution images using fewer computational resources to provide decent segmentation results on the original resolution during inference. While HartleyMHA performed better than HNOSeg in general, their similar performance is consistent with the findings in [15,23] that self-attention is sufficient for good performance but is not crucial. On the other hand, as the use of efficient self-attention improves the expressiveness of the Hartley MHA block, fewer layers can be used to reduce the overall computational costs."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,4.0,Conclusion,"In this paper, based on the idea of FNO which has the properties of zeroshot super-resolution and global receptive field, we propose the HNOSeg and HartleyMHA models for resolution-robust and parameter-efficient 3D image segmentation. HNOSeg is FNO improved by the Hartley transform, residual connections, deep supervision, and shared parameters in the frequency domain. We further extend this concept for efficient multi-head attention in the frequency domain as HartleyMHA. Experimental results show that HNOSeg and HartleyMHA had similar accuracies as other tested segmentation models when trained with the original image resolution, but had superior performance when trained with images of much lower resolutions. HartleyMHA performed slightly better than HNOSeg and ran faster with less memory. With these advantages, HartleyMHA can be a promising alternative for 3D image segmentation especially when computational resources are limited."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,1.0,Introduction,"Stroke is the second leading cause of death worldwide, with over twelve million new cases reported annually. Projections estimate that one in four individuals will experience a stroke in their lifetime [3]. Ischemic stroke, caused by blood vessel occlusion, is the most common stroke, accounting for approximately 80% of all cases. These strokes carry a high risk of morbidity, emphasizing the need for timely and accurate diagnosis and treatment [4].Lesion delineation and quantification are critical components of stroke diagnosis, guiding clinical intervention and informing patient prognosis [7]. Diffusionweighted MRI studies are the primary tool for observing and characterizing stroke lesions. Among these sequences, the B-1000 (DWI) and the apparent diffusion coefficient (ADC) are biomarkers of cytotoxic injury that predicts edema formation and outcome after ischemic stroke [1]. Typically, the standard lesion analysis requires neuroradiologists to observe both modalities to derive a comprehensive lesion characterization. However, this observational task is challenging, time-consuming (taking approximately 15 min per case), and susceptible to biased errors [9,11].To address these challenges, computational approaches have supported stroke lesion segmentation, including multi-context approaches from convolutional architectures and standard training schemes to achieve a latent lesion representation [5]. Other strategies have tackled the class imbalance between lesion regions and healthy tissue by using mini-batches of image patches, ensuring stratified information but overlooking global lesion references [2]. More recent architectures have included attention modules that integrate local and contextual information to enhance stroke-related patterns [8,12]. These approaches in general are however adjusted from only one modality reference, losing complementary radiological findings. Furthermore, many of these approaches are calibrated to one-expert annotations, introducing the possibility of learning expert bias.To the best of our knowledge, this work is the first effort to explore and model complementary lesion annotations from DWI and ADC modalities, utilizing a novel multimodal cross-attentional autoencoder. The proposed architecture employs modality-specific encoders to extract stroke patterns and generate a cross-domain embedding that enhances lesion segmentations for both modalities. To mitigate noise propagation, cross-attention mechanisms were integrated with skip connections between encoder and decoder blocks with similar spatial dimensions. The approach was evaluated on 82 ADC and DWI sequences, annotated by two neuroradiologists, evidencing generalization capabilities and outperforming typical unimodal models. The proposed method makes significant contributions to the field, as outlined below.-A multi-context and multi-segmentation deep attentional architecture that retrieves lesion annotations from ADC and DWI, sequences. -An architecture generalization study by training the model with one reference radiologist but evaluating with respect to two radiologist annotations."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.0,Proposed Approach,"This work introduces a multi-path encoder-decoder representation with the capability to recover stroke segmentations from ADC and DWI parameters. Independent branches are encoded for each modality and fused into an embedding representation. Then, independent decoder paths are evolved to recover stroke lesions, supported by cross-attention modules from the respective encoders. A multi-segmentation task is herein implemented to update the proposed deep representation. The general description of the proposed approach is illustrated in Fig. 1.Fig. 1. Graphical representation of the proposed multimodal network for accurate segmentation of lesions in ADC and DWI modalities. The architecture comprises three main parts: A) the encoder, which characterizes the most relevant information in the input image and transforms it into a low-dimensional embedding; B) the decoder, which recovers high-dimensional information captured in the embedded representation; and C) the attention module, which filters and relates the most important features in both modalities."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.1,Multimodal Attentional Mechanisms,"In this work, a dual path encoder-decoder is introduced to recover stroke shape findings, observed from ADC and DWI. From the decoder section, two independent convolutional paths build a deep representation of each considered parameter. Both paths project studies into a low-dimensional embedding space that code stroke associated findings (see Fig. 1a).Then, a single and complementary latent representation is achieved by the concatenation of both encoders. We hypothesize that such late fusion from embedding space allows a more robust integration of salient features related to stroke findings in both modalities.Hence, independent decoder branches, for each modality, are implemented from the fused embedding representation. These decoder paths are updated at each level from skip connections, implemented as cross-attentional mechanisms. Particularly, these connections control the contribution of the encoder X i e into the decoder X i d representation, at each convolutional level i. The refined encoder features are computed as:where σ 1 and σ 2 are ReLU and Sigmoid activations, respectively (see in Fig. 1c). Consequently, the refined features X i re preserve highly correlated features between encoder and decoder branches, at the same level of processing. These cross-attention mechanisms enhance the quality of lesion estimations as a result of complementing the decoder representation with early representations of the encoder."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.2,Multi-segmentation Minimization,"A multi-segmentation loss function is here introduced to capture complementary textural patterns from both MRI parameters and improve lesion segmentation. This function induces the learning of highly correlated features present in ADC and DWI images. Specifically, the loss function is defined as:Here, γ is a focusing parameter, α is a weight balancing factor, and ŶADC and ŶDWI are the annotations for ADC and DWI, respectively. It should be noted, that ischemic stroke segmentation is highly imbalanced, promoting background reconstruction. To overcome this issue, each modality loss is weighted by a set of class weight maps (C ∈ N H×W ) that increase the contribution of lesion voxels and counteract the negative impact of class imbalance. From this multi-segmentation loss function, the model can more effectively capture complementary information from both ADC and DWI, leading to improved lesion segmentation."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.3,ADC-DWI Data and Experimental Setup,"A retrospective study was conducted to validate the capability of the proposed approach, estimating ischemic stroke lesion segmentation. The study collected 82 studies of patients with stroke symptoms at a clinical center between October 2021 and September 2022. Each study has between 20 to 26 slices with resolutions ranging in each axis as x, y = [0.83 -0.94] and z = [5.50 -7.20] mm. After reviewing imaging studies, the studies were categorized into control (n = 7) or ischemic stroke (n = 75) studies. Control patients with stroke symptoms were included to diversify tissue samples, potentially enhancing our models' ability to segment stroke lesion tissue. Each study included ADC and DWI sequences obtained using a Toshiba Vantage Titan MRI scanner. Two neuro-interventional radiologists, each with more than five years of experience, individually annotated each medical sequence based on clinical records using the MRIcroGL software [6]. To meet the inclusion criteria, patients had to be older than 18 years and show no signs of a cerebral hemorrhage. Patients who showed evidence of partial reperfusion were not excluded from the study.The dataset was stratified into two subsets for training and testing. The training set consisted of 51 studies with annotations from a single expert (R1), while the test set contained 31 studies with manual delineations from two experts. In addition, a validation set was created from 11 studies of the training set. This validation set was used to measure the model's capability to segment ischemic stroke lesions on unseen data during training and to save the best weights before predicting on the test set. The test set was specifically designed to evaluate the model's performance on quantifying differences between lesion annotations. It allowed for a comparison of annotations between modalities of the same radiologist and between radiologists with the same modality. Furthermore, the test set was used to assess the generalizability of the model when compared to annotations made by the two radiologists. Segmentation capability was measured from the standard Dice score (DSC), precision, and recall.Regarding the model, the proposed symmetric architecture consists of five convolutional levels (32, 64, 128, 256, and 512 filters for both networks and 1024 filters in the bottleneck) with residual convolutional blocks (3 × 3, batch normalization, and ReLU activation). For training, the architecture was updated over 300 epochs with a batch size of 16, and a Focal loss with an AdamW optimizer with γ = 2 and α = 0.25. A linear warm-up strategy was used in the first 90 epochs (learning rate from 5e-3 and decay 1e-5), followed by 210 epochs of cosine decay. The class weight map values were set to 1 for non-lesion voxels and 3 for lesion voxels. The whole studies were normalized and resized to 224 × 224 pixels. Registration and skull stripping were not considered. Furthermore, data augmentation such as random brightness and contrast, flips, rotations, random elastic transformations, and random grid and optical distortions were applied to the slices. As a baseline, the proposed architecture was trained with a single modality (ADC, DWI). The code for our work can be found in https://gitlab.com/bivl2ab/research/2023-cross-domain-stroke-segmentation."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,3.0,Evaluation and Results,"From a retrospective study, we conducted an analysis to establish radiologist agreement in both ADC and DWI, following the Kappa coefficient from lesion differences. In Fig. 2a and Fig. 2b are summarized the distribution agreement between radiologist annotations, observing the ADC and DWI studies, respectively. In Fig. 2c is reported the agreement distribution between modalities, observed for each radiologist.As expected, a substantial level of agreement among radiologists was found in both ADC and DWI sequences, with an average agreement of 65% and 70%, respectively. It should be noted however that in 25% of studies there exist between fair and moderate agreement. These discrepancies may be associated with acute strokes, where radiological findings are negligible and challenging even for expert radiologists. In such a sense, it should be noted that computational approaches may be biased for expert annotations, considering the substantial agreement. Also, among modalities is reported a kappa value of around 65% that evidences the importance to complement findings to achieve more precise stroke delineations.Hence, an ablation study was then carried out to measure the capabilities of the proposed approach to recover segmentation, regarding multi-segmentation segmentation and the contribution of cross-attention mechanisms. The capability of architecture was measured with respect to the two radiologist delineations and with respect to each modality. Figure 3 illustrates four different variations of the proposed approach. Firstly, a unimodal configuration was implemented from decoder cross-attention mechanisms but adjusted with only one modality. The Dual V1 configuration corresponds to multi-segmentation proposed strategy that uses cross-attention mechanisms at decoder paths. In this version, each cross-attention is learned from the encoder and previous decoder levels. The version (Dual V2) implements cross-attention mechanisms into the encoder path, fusing both modalities at each processing level. Conversely, the Dual V3 integrates encoder and decoder cross-attention modules to force multimodal fusion at different levels of architecture.Table 1 summarizes the achieved results for each architecture variation and with respect to each radiologist reference at ADC and DWI modalities. Interestingly, the Dual V1 architecture, following an embedding fusion, achieves consistent dice scores in both ADC (V1 = 0.58 vs V3 = 0.53) and DWI (V1 = 0.60 vs V3 = 0.62) for both radiologists, taking advantage of multi-segmentation segmentation learning. Notably, the Dual V1 outperforms most of the other Dual models by up to 10% and the typical approximation by up to 30%. It should be noted that the score achieved for unimodal architecture is coherent with the achieved by other strategies in public datasets [5,10]. The other dual versions, despite of multilevel fusion, have a trend to filter stroke features and reduce the quality of the estimated segmentations. Also, the proposed approach achieves remarkable results regarding expert radiologists, even with respect to R2 annotations, unseen during training. These results evidence the potential capabilities of the architecture to generalize segmentations, avoiding expert bias. Taking advantage of the substantial agreement between the annotations of both radiologists in our study, we carried out a t-test analysis between different versions of the proposed approach, finding statistically significant differences between Unimodal and Dual models, while the distributions among different Dual models have a similar statistical performance. This analysis was extended to compare the masks of the models with respect to expert radiologists. In this study, the masks of the Dual V1 model have the same distribution (p > 0.05) as the reference dice scores over both ADC and DWI. Similarly, the Dual V3 model masks follow the same distribution as the reference dice scores, but only on DWI. Complementary, from Bland-Altman plots in Fig. 4 it can be seen that Dual models exhibit small and clustered differences around the mean, with a mean difference closer to zero. In contrast, single models show more spread-out differences and a higher mean difference compared to Dual models. Figure 5 illustrated the segmentations generated in each of the experiments for the proposed approach and the unimodal baseline. In top, the 3D brain rendering illustrates the stroke segmentations achieved by the dual architecture (red contours), the unimodal baseline (blue areas) and the reference of a radiologist (green areas). At the bottom, the slices illustrate each estimated segmentation as red contours, while the radiologist delineation is represented as the blue area. As expected, the proposed approach reports a remarkable capability to correctly define the limits of the ischemic lesion. Additionally, the proposed approach achieves adequate localization without overestimating the lesion, which is a key issue in identifying treatments and estimating patient diagnosis. "
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,4.0,Conclusions and Perspectives,"The proposed approach demonstrates the potential of a multi-context and multisegmentation cross-attention encoder-decoder architecture for recovering stroke segmentations from DWI and ADC images. The proposed approach has the potential to improve the accuracy and reliability of stroke segmentation in clinical practice, nevertheless, it introduces potential challenges in scalability. Future research directions include the development of attention mechanisms that can fuse additional MRI modalities to improve the accuracy of stroke segmentation. Furthermore, the study could benefit from a more exhaustive preprocessing for the medical images. Finally, an extension of the agreement study to include a larger number of radiologists and studies could provide valuable insights into the reliability and consistency of lesion annotation in MRI studies."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,1.0,Introduction,"Medical image segmentation (MIS) is a crucial component in medical image analysis, which aims to partition an image into distinct regions (or segments) that are semantically related and/or visually similar. This process is essential for clinicians to, among others, perform qualitative and quantitative assessments of various anatomical structures or pathological conditions and perform imageguided treatments or treatment planning [2]. Vision transformers (ViTs), with their inherent ability to model long-range dependencies, have recently been considered a promising technique to tackle MIS. They process images as sequences of patches, with each patch having a global view of the entire image. This enables a ViT to achieve improved segmentation performance compared to traditional convolutional neural networks (CNNs) on plenty of segmentation tasks [16]. However, due to the lack of inductive biases, such as weight sharing and locality, ViTs are more data-hungry than CNNs, i.e., require more data to train [31]. Meanwhile, it is common to have access to multiple, diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for the same MIS task, e.g., PH2 [25] and ISIC 2018 [11] in dermatology, LiTS [6] and CHAOS [18] in liver CT, or OASIS [24] and ADNI [17] in brain MRI. As each dataset alone is too small to properly train a ViT, the challenge becomes how to effectively leverage the different datasets. Various strategies have been proposed to address ViTs' data-hunger (Table 1), mainly: Adding inductive bias by constructing a hybrid network that fuses a CNN with a ViT [39], imitating CNNs' shifted filters and convolutional operations [7], or enhancing spatial information learning [22]; sharing knowledge by transferring knowledge from a CNN [31] or pertaining ViTs on multiple related tasks and then fine-tuning on a down-stream task [37]; increasing data via augmentation [34]; and non-supervised pre-training [8]. Nevertheless, one notable limitation in these approaches is that they are not universal, i.e., they rely on separate training for each dataset rather than incorporate valuable knowledge from related domains. As a result, they can incur additional training, inference, and memory costs, which is especially challenging when dealing with multiple small datasets in the context of MIS tasks. Multi-domain learning, which trains a single universal model to tackle all the datasets simultaneously, has been found promising for reducing computational demands while still leveraging information from multiple domains [1,21]. To the best of our knowledge, multi-domain universal models have not yet been investigated for alleviating ViTs' data-hunger.Given the inter-domain heterogeneity resulting from variations in imaging protocols, scanner manufacturers, etc. [4,21], directly mixing all the datasets for training, i.e., joint training, may improve a model's performance on one dataset while degrading performance on other datasets with non-negligible unrelated domain-specific information, a phenomenon referred to as negative knowledge transfer (NKT) [1,38]. A common strategy to mitigate NKT in computer vision is to introduce adapters aiding the model to adapt to different domains, i.e., multi-domain adaptive training (MAT), such as domain-specific mechanisms [21,26,32], and squeeze-excitation layers [28,35] (Table 1). However, those MAT techniques are built based on CNN rather than ViT or are scalable, i.e., the models' size at the inference time increases linearly with the number of domains.To address ViTs' data-hunger, in this work, we propose MDViT, a novel fixedsize multi-domain ViT trained to adaptively aggregate valuable knowledge from multiple datasets (domains) for improved segmentation. In particular, we introduce a domain adapter that adapts the model to different domains to mitigate negative knowledge transfer caused by inter-domain heterogeneity. Besides, for better representation learning across domains, we propose a novel mutual knowledge distillation approach that transfers knowledge between a universal network (spanning all the domains) and additional domain-specific network branches.We summarize our contributions as follows: (1) To the best of our knowledge, we are the first to introduce multi-domain learning to alleviate ViTs' data-hunger when facing limited samples per dataset. (2) We propose a multi-domain ViT, MDViT, for medical image segmentation with a novel domain adapter to counteract negative knowledge transfer and with mutual knowledge distillation to enhance representation learning. (3) The experiments on 4 skin lesion segmentation datasets show that our multi-domain adaptive training outperforms separate and joint training (ST and JT), especially a 10.16% improvement in IOU on the skin cancer detection dataset compared to ST and that MDViT outperforms state-of-the-art data-efficient ViTs and multi-domain learning strategies."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2.0,Methodology,"Let X ∈ R H×W ×3 be an input RGB image and Y ∈ {0, 1} H×W be its groundtruth segmentation mask. Training samples {(X, Y )} come from M datasets, each representing a domain. We aim to build and train a single ViT that performs well on all domain data and addresses the insufficiency of samples in any of the datasets. We first introduce our baseline (BASE), a ViT with hierarchical transformer blocks (Fig. 1-a). Our proposed MDViT extends BASE with 1) a domain adapter (DA) module inside the factorized multi-head self-attention (MHSA) to adapt the model to different domains (Fig. 1-b,c), and 2) a mutual knowledge distillation (MKD) strategy to extract more robust representations across domains (Fig. 1-d). We present the details of MDViT in Sect. 2.1. BASE is a U-shaped ViT based on the architecture of U-Net [27] and pyramid ViTs [7,19]. It contains encoding (the first four) and decoding (the last four) transformer blocks, a two-layer CNN bridge, and skip connections. As described in [19], the ith transformer block involves a convolutional patch embedding layer with a patch size of 3 × 3 and L i transformer layers with factorized MHSA in linear complexity, the former of which converts a feature map X i-1 into a sequence of patch embeddings z i ∈ R Ni×Ci , whereis the number of patches and C i is the channel dimension. We use the same position embedding as [19] and skip connections as [27]. To reduce computational complexity, following [19], we add two and one CNN layer before and after transformer blocks, respectively, enabling the 1st transformer block to process features starting from a lower resolution: H 4 × W 4 . We do not employ integrated and hierarchical CNN backbones, e.g., ResNet, in BASE as data-efficient hybrid ViTs [33,39], to clearly evaluate the efficacy of multi-domain learning in mitigating ViTs' data-hunger."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2.1,MDViT,"MDViT consists of a universal network (spanning M domains) and M auxiliary network branches, i.e., peers, each associated with one of the M domains. The universal network is the same as BASE, except we insert a domain adapter (DA) in each factorized MHSA to tackle negative knowledge transfer. Further, we employ a mutual knowledge distillation (MKD) strategy to transfer domain-specific and shared knowledge between peers and the universal network to enhance representation learning. Next, we will introduce DA and MKD in detail."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Domain Adapter (DA):,"In multi-domain adaptive training, some methods build domain-specific layers in parallel with the main network [21,26,32]. Without adding domain-specific layers, we utilize the existing parallel structure in ViTs, i.e., MHSA, for domain adaptation. The H parallel heads of MHSA mimic how humans examine the same object from different perspectives [10]. Similarly, our intuition of inserting the DA into MHSA is to enable the different heads to have varied perspectives across domains. Rather than manually designate each head to one of the domains, guided by a domain label, MDViT learns to focus on the corresponding features from different heads when encountering a domain. DA contains two steps: Attention Generation and Information Selection (Fig. 1-c).Attention Generation generates attention for each head. We first pass a domain label vector m (we adopt one-hot encoding m ∈ R M but other encodings are possible) through one linear layer with a ReLU activation function to acquire a domain-aware vector d ∈ R K r . K is the channel dimension of features from the heads. We set the reduction ratio r to 2. After that, similar to [20], we calculate attention for each head:where ψ is a softmax operation across heads andInformation Selection adaptively selects information from different heads. After getting the featurefrom the hth head, we utilize a h to calibrate the information along the channel dimension: ũh"
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Mutual Knowledge Distillation (MKD):,"Distilling knowledge from domainspecific networks has been found beneficial for universal networks to learn more robust representations [21,40]. Moreover, mutual learning that transfers knowledge between teachers and students enables both to be optimized simultaneously [15]. To realize these benefits, we propose MKD that mutually transfers knowledge between auxiliary peers and the universal network. In Fig. 1-d, the mth auxiliary peer is only trained on the mth domain, producing output Ŷ m , whereas the universal network's output is Ŷ . Similar to [21], we utilize a symmetric Dice loss L am mkd = Dice( Ŷ , Ŷ m ) as the knowledge distillation loss. Each peer is an expert in a certain domain, guiding the universal network to learn domain-specific information. The universal network experiences all the domains and grasps the domain-shared knowledge, which is beneficial for peer learning.Each Auxiliary Peer is trained on a small, individual dataset specific to that peer (Fig. 1-d). To achieve a rapid training process and prevent overfitting, particularly when working with numerous training datasets, we adapt a lightweight multilayer perception (MLP) decoder designed for ViT encoders [36] to our peers' architecture. Specifically, multi-level features from the encoding transformer blocks (Fig. 1-a) go through an MLP layer and an up-sample operation to unify the channel dimension and resolution to H 4 × W 4 , which are then concatenated with the feature involving domain-shared information from the universal network's last transformer block. Finally, we pass the fused feature to an MLP layer and do an up-sample to obtain a segmentation map."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2.2,Objective Function,"Similar to Combo loss [29], BASE's segmentation loss combines Dice and binary cross entropy loss: L seg = L Dice +L bce . In MDViT, we use the same segmentation loss for the universal network and auxiliary peers, denoted as L u seg and L a seg , respectively. The overall loss is calculated as follows.We set both α and β to 0.5. L am seg does not optimize DA to avoid interfering with the domain adaptation learning. After training, we discard the auxiliary peers and only utilize the universal network for inference."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Datasets and Evaluation Metrics:,"We study 4 skin lesion segmentation datasets collected from varied sources: ISIC 2018 (ISIC) [11], Dermofit Image Library (DMF) [3], Skin Cancer Detection (SCD) [14], and PH2 [25], which contain 2594, 1300, 206, and 200 samples, respectively. To facilitate a fairer performance comparison across datasets, as in [4], we only use the 1212 images from DMF that exhibited similar lesion conditions as those in other datasets. We perform 5-fold cross-validation and utilize Dice and IOU metrics for evaluation as [33].Implementation Details: We conduct 3 training paradigms: separate (ST), joint (JT), and multi-domain adaptive training (MAT), described in Sect. 1, to train all the models from scratch on the skin datasets. Images are resized to 256 × 256 and then augmented through random scaling, shifting, rotation, flipping, Gaussian noise, and brightness and contrast changes. The encoding transformer blocks' channel dimensions are [64, 128, 320, 512] (Fig. 1-a). We use two transformer layers in each transformer block and set the number of heads in MHSA to 8. The hidden dimensions of the CNN bridge and auxiliary peers are 1024 and 512. We deploy models on a single TITAN V GPU and train them for 200 epochs with the AdamW [23] optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an initial learning rate of 1×10 -4 , which changes through a linear decay scheduler whose step size is 50 and decay factor γ = 0.5."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Comparing Against BASE:,"In Table 2-a,b, compared with BASE in ST, BASE in JT improves the segmentation performance on small datasets (PH2 and SCD) but at the expense of diminished performance on larger datasets (ISIC and DMF). This is expected given the non-negligible inter-domain heterogeneity between skin lesion datasets, as found by Bayasi et al. [5]. The above results demonstrate that shared knowledge in related domains facilitates training a ViT on small datasets while, without a well-designed multi-domain algorithm, causing negative knowledge transfer (NKT) due to inter-domain heterogeneity, i.e., the model's performance decreases on other datasets. Meanwhile, MDViT fits all the domains without NKT and outperforms BASE in ST by a large margin; significantly increasing Dice and IOU on SCD by 6.4% and 10.16%, showing that MDViT smartly selects valuable knowledge when given data from a certain domain. Additionally, MDViT outperforms BASE in JT across all the domains, with average improvements of 0.82% on Dice and 1.4% on IOU.  [7], UTNet [13], BAT [33], TransFuse [39], and Swin UNETR [30]. We implement ResNet-34 as the backbone of BAT for fair comparison (similar model size). As illustrated in Table 2-a,b,c, these SOTA models are superior to BASE in ST. This is expected since they are designed to reduce data requirements. Nevertheless, in JT, these models also suffer from NKT: They perform better than models in ST on some datasets, like SCD, and worse on others, like ISIC. Finally, MDViT achieves the best segmentation performance in average Dice and IOU without NKT and has the best results on SCD and PH2. Figure 2 shows MDViT's excellent performance on ISIC and DMF and that it achieves the closest results to ground truth on SCD and PH2. More segmentation results are presented in the supplementary material. Though BAT and TransFuse in ST have better results on some datasets like ISIC, they require extra compute resources to train M models as well as an M -fold increase in memory requirements. The above results indicate that domain-shared knowledge is especially beneficial for training relatively small datasets such as SCD.We employ the two fixed-size (i.e., independent of M ) multi-domain algorithms proposed by Rundo et al. [28] and Wang et al. [35] on BASE. We set the number of parallel SE adapters in [35] to 4. In Table 2-b,d, MDViT outperforms both of them on all the domains, showing the efficacy of MDViT and that multi-domain methods built on ViTs might not perform as well as on CNNs. We also apply the domain-specific normalization [21] to BASE and MDViT to get BASE † and MDViT † , respectively. In Table 2-d, BASE † confronts NKT, which lowers the performance on DMF compared with BASE in ST, whereas MDViT † not only addresses NKT but also outperforms BASE † on average Dice and IOU."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Ablation Studies and Plug-in Capability of DA:,"We conduct ablation studies to demonstrate the efficacy of DA, MKD, and auxiliary peers. Table 3b reveals that using one-direction knowledge distillation (KD) or either of the critical components in MDViT, i.e., DA or MKD, but not together, could not achieve the best results. Table 3-c exemplifies that, for building the auxiliary peers, our proposed MLP architecture is more effective and has fewer parameters (1.6M) than DeepLabv3's decoder [9] (4.7M) or BASE's decoding layers (10.8M). Finally, we incorporate DA into two ViTs: TransFuse and DosViT (the latter includes the earliest ViT encoder [12] and a DeepLabv3's decoder). As shown in Table 3-a,b, DA can be used in various ViTs but is more advantageous in MDViT with more transformer blocks in the encoding and decoding process."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,4.0,Conclusion,"We propose a new algorithm to alleviate vision transformers (ViTs)' datahunger in small datasets by aggregating valuable knowledge from multiple related domains. We constructed MDViT, a robust multi-domain ViT leveraging novel domain adapters (DAs) for negative knowledge transfer mitigation and mutual knowledge distillation (MKD) for better representation learning. MDViT is nonscalable, i.e., has a fixed model size at inference time even as more domains are added. The experiments on 4 skin lesion segmentation datasets show that MDViT outperformed SOTA data-efficient medical image segmentation ViTs and multi-domain learning methods. Our ablation studies and application of DA on other ViTs show the effectiveness of DA and MKD and DA's plug-in capability."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Table 2 .,† 28.6(.02×) MAT 90.24 90.71 93.38 95.90 92.56 ± 0.52 82.97 83.31 88.06 92.19 86.64 ± 0.76
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Table 3 .,"Comparing Against State-of-the-Art (SOTA) Methods: We conduct experiments on SOTA data-efficient MIS ViTs and multi-domain learning methods. Previous MIS ViTs mitigated the data-hunger in one dataset by adding inductive bias, e.g., SwinUnet"
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 43.
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.0,Methods,"Problem Setting. Given limited labeled data Overview. The overview of the proposed method is shown in Fig. 1. Our approach is built on top of the Mean-teacher [12] framework and adopts hybrid UNet (H-UNet) as backbone. Following [13], the networks learn through the student branch only, and the teacher model is update by using an Exponential Moving Average (EMA). The student model is trained with labeled pairs and pseudo labels generated by the teacher model. We train a linear-and a prototype-based classifier in parallel to alleviate class imbalance. Moreover, to guarantee the label quality, two multi-level tree filters (TFs) constructed based on the pixel affinity in terms of intensity and semantic features are employed to refine the teacher model's predictions from the prototype-based classifier."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.1,H-UNet for 3D Medical Image Segmentation,"The backbone model H-UNet is a variant of H-DenseUNet [14]. Given n samples in a training batch X ∈ R n×1×H×W ×D , the 3D UNet produces features F 3d ∈ R n×m×H×W ×D . Meanwhile, the 2D slices X 2d ∈ R nD×1×H×W obtained by performing the transformation operation T [14] on the original inputs are fed into the 2D UNet to generate features F 2d ∈ R nD×m×H×W . By conducting inverse transformation operation, T -1 , F 2d is aligned with F 3d before the two features are added and fed into the hybrid feature fusion (HFF) layer to yield the hybrid features F . We apply supervision on both 2D UNet and 3D UNet to train the H-UNet."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.2,Class Imbalance Alleviation via Prototype-Based Classifier,"Prototype-based classifier, leveraging prototypes instead of a parameterized predictor to make predictions, presents efficacy for semantic segmentation [15] and class imbalance alleviation [16]. Therefore, we introduce an extra prototypebased classifier (PC) into the student model in parallel to a linear-based classifier (LC). Specifically, PC makes dense predictions by matching the normalized hybrid feature F with the nearest prototypes. Each prototype is the feature aggregation of several training pixels belonging to the same class. We denoteas a set of prototypes associated with C classes for segmentation. Note that each class may have more than one prototype (i.e., K) due to the intra-class heterogeneity exhibited in medical images [17]. Given X i , PC produces the probability distribution of pixel a over the C classes:(1) where s a,c ∈ [-1, 1] denotes the pixel-class similarity between pixel a and its closest prototype of class c. S(, ) is the similarity measure (i.e., cosine similarity). F i [a] denotes the extracted features of pixel a, and Y i [a] denotes the predicted probability. • 2 stands for the 2 normalization.Meanwhile, the features F are also fed into LC parameterized byThe probability distribution of pixel a estimated by LC is defined as:These two classifiers complete each other during training. In the early training phase, LC dominates knowledge learning and provides PC with discriminative features to initialize and update the prototypes (See Sect. 2.4). With the addition of PC, the feature embedding space is further regularized, along with intraclass features being more compact and inter-class features being more separated, which in turn benefits the learning of LC."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.3,Pseudo Label Refinement via Multi-level Tree Filters,"Rather than directly using the teacher's high-confidence prediction to generate pseudo labels [13], we propose a cascaded refining strategy (CRS) to improve the label quality in a slice-by-slice manner on 3D volumes by using TFs [18], which proceeds with the following three steps as depicted in Fig. 2.Graph Construction: First, we respectively represent the topology of the low-level original unlabeled image and the high-level hybrid features as two 4connected planar graphs: G * = {V * , E * } where V * is the vertex set associated with each pixel and E * is the edge set, and * ∈ {low, high}. The weight of the edge connecting two adjacent nodes a and b indicates their dissimilarity, which is defined by: Multi-level Filter Construction: Based on the two MSTs, we build the lowlevel TF F low and the high-level TF F high . The filter weight F * a,b of any two nodes is obtained by aggregating the MST edge weights along the path between the two nodes [20]:Here, E * a,b is the edge set in the path from node a to node b. S G * mst (•) maps the distance of two vertices into a positive scalar which measures the pixel affinity. z a is a normalization factor, which is the summation of the similarity between node a and all other nodes in the MST.Cascaded Refinement: Lastly, we refine the teacher's prediction P with the two filters in a cascade manner to acquire high-quality pseudo labels Ŷ :where R(•, •) is the refinement process where each unlabeled pixel can contribute to the refinement for other pixels with a contribution proportional to their similarity. By exploiting the multi-level complementary features, i.e., the object boundary information encoded in F low and the semantic similarity encoded in F high , CRS shows superiority when a single TF fails in some cases. E.g., when two pixels of different classes have similar intensity values (pixel a and b in Fig. 2), F low will model them as affinity pairs, which is not expected. Fortunately, F high can suppress the mutual interference between them according to their distinct semantic features, thus ensuring refinement efficacy (See Supplementary Fig. 1)."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.4,Network Training and Prototype Update,"The framework is trained by utilizing both the labeled data D L and the unlabeled data D U . For the labeled data, the supervised loss is defined as:where Y 2D is the prediction generated by the 2D UNet in H-UNet, and Y 2D is the transformation of the 3D ground truth Y by conducting T (See Sect. 2.1). l is the weighted sum of cross entropy loss and dice loss.For the unlabeled data, the student model is trained with the pseudo labels Ŷ refined by the proposed CRS. The unsupervised loss is defined as:(7) Following [10], we use a time-dependent Gaussian warming up function to control the balance between the supervised and unsupervised loss.Prototype Initialize and Update: The initialization of prototypes determines the PC performance. To get better prototypes, we first pretrain the network with the LC solely. Then, we collect pixelwise deep features and use K-means [21] to generate K subclusters for each class. Finally, initial prototypes of each class are obtained by averaging the features in each subcluster. Following [15], after each training iteration, we first conduct online clustering by few steps of Sinkhorn-Knopp iteration [22] and let the prototypes evolve continuously with the clustering features of both the labeled and unlabeled samples:where α is a momentum coefficient, i.e., 0.999. 3 Experiments and Results"
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.1,Datasets and Implementation Details,"Datasets and Evaluation Metrics: The model is evaluated on two public cardiac MRI datasets: (1) The ACDC Implementation Details: We split the datasets into the training and validation set randomly at a ratio of 4:1 [11]. To assess the model performance on different label percentages, we respectively take 1.25%, 2.5%, and 10% data from the ACDC training set and 10%, 20%, and 40% data from the MMWHS training set as the labeled data and the rest as unlabeled. The training details for the two datasets are provided in Section A of the Supplementary material."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.2,Comparison with State-of-the-Arts,"Table 1. Comparison with state-of-art methods on the ACDC and MMWHS datasets Methods Avg DSC (ACDC) Avg DSC (MMWHS) L = 1.25% L = 2.5% L = 10% L = 10% L = 20% L = 40%Self train [7] 0 We compare our method with several state-of-the-art frameworks for semisupervised medical image segmentation. The results are reported in Table 1.Among these, the proposed method outperforms competing methods across all datasets and label percentages, except for a slightly lower DSC than method Class-wise Sampling [11] by using 20% labeled MMWHS data. However, method [11] fails to produce satisfactory results using very few annotations (L = 1.25% in ACDC and L = 10% in MMWHS). In contrast, our method exhibits quite consistent performance on different label percentages and has substantial improvement when only few annotations are available, with more than 10% increase by using 10% labeled MMWHS. This indicates the benefit of fully leveraging unlabeled data via the proposed PC and CRS, in which every pixel in the unlabeled data can properly contribute to the model training and the refinement for pseudo labels. Methods like Data Aug [25] and contrastive learning of global and local features [27] do not explicitly address the class imbalance problem, thus they have inferior performance on both datasets. Also, these two methods both require a complex pretraining process, which is inefficient and time-consuming. Mixmatch [26], mixing labeled and unlabeled data using MixUp in SSL, cannot work very well for the datasets with a severe class imbalance problem, mainly because simply mixing two images could oppositely increase difficulty in the less-dominant classes learning. We conducted ablation analyses on MMWHS by using 40% labeled data to investigate the contribution of the proposed key components (PC and CRS) and the choice of the backbone network. All the models are trained in the SSL way, and we use the thresholded pseudo-labeling [3] (threshold = 0.8) to generate pseudo labels for the models when CRS is absent. Table 2 presents the results of several variants by using different combinations of the proposed key components and two backbone networks. Compared with 3D UNet, H-UNet (model ①) shows more consistent results on different substructures and better performance regardless of the use of the proposed components, with a 1.2% and 4.0% increase in average DSC with and without PC and CRS respectively, indicating the merits of exploiting both intra-slice and inter-slice information for 3D medical image segmentation. When thresholded pseudo-labeling is replaced with CRS, model ② improves the average DSC by 1.7% with a smaller deviation for each heart substructure among all test samples, suggesting the effectiveness of CRS. PC alleviates the class imbalance problem by leveraging the prior knowledge of the entire sample set to assist individual learning. This is justified by the improvements of 1.5% on average DSC and better results on tail classes (e.g., RA, MYO) brought by substituting LC with PC (model ③). Moreover, the performance is further boosted when these two classifiers are simultaneously adopted (model ④), outperforming LC and PC by a margin of 4.3% and 2.8% respectively. This is because the mutual promotion between the two classifiers could provide a better regularized feature space for pixel predictions. With CRS integrated, PC and LC are trained with more reliable pseudo labels. The DSC is further improved by 1.5% (model ⑤), arriving at the highest value (0.840). Noticeably, when 3D UNet is equipped with all the components, the average DSC is improved by 3.0% with performance soar on tail classes. Such consistent efficacy indicates the potential of using the proposed PC and CRS to endow any segmentation backbones with the capability of addressing the class imbalance problem."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.3,Ablation Studies,Please refer to Section B in the Supplementary material for more results in terms of average symmetric surface distance (ASSD) in voxel and the qualitative analyses of multi-level tree filters.
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,4.0,Conclusion,"The scarcity of pixel-level annotations affects the performance of deep neural networks for medical image segmentation. Moreover, the class imbalanced problem existing in the medical data can further exacerbate the model degradation. To address the problems, we propose a novel semi-supervised class imbalanced learning approach by additionally introducing the prototype-based classifier into the student model and constructing two multi-level tree filters to refine the pseudo labels for more robust learning. Experiments conducted on two public cardiac MRI datasets demonstrate the superiority of the proposed method."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_44.
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,1.0,Introduction,"Age-related macular degeneration (AMD) is a leading cause of blindness worldwide, primarily attributable to choroidal neovascularization (CNV) [1]. Optical coherence tomography angiography (OCTA), a non-invasive imaging technique, has gained popularity in recent years due to its ability to visualize blood flow in the retina and choroid with micrometer depth resolution [2]. Thus, automated CNV segmentation based on OCTA images can facilitate quantitative analysis and enhance the diagnosis performance of AMD [3]. However, the accurate segmentation of CNV from OCTA images poses a significant challenge due to the complex morphology of CNVs and the presence of imaging artifacts [4], as illustrated in Fig. 1. Hence, reliable CNV segmentation is promptly needed to assist ophthalmologists in making informed clinical decisions.Several methods have been proposed to segment CNV regions from OCTA images, including handcraft feature descriptors [5,6] and deep learning-based techniques [7]. For example, a saliency-based method for automated segmentation of CNV regions in OCTA images was proposed by Liu et al. [5], which capitalizes on distinguishing features of CNV regions with higher intensity compared to background artifacts and noise. In [6], an unsupervised algorithm for CNV segmentation was proposed, which utilizes a density cell-like P system. However, their accuracy is restricted by weak saliency and ambiguous boundaries. With the recent advancements of deep learning, several methods have also been proposed for CNV segmentation in OCT images. U-shaped multiscale information fusion networks are proposed in [8] and [9] for segmenting CNVs with multiscale scenarios. Wang et al. [7] further proposed a two-stage CNN-based architecture based on OCTA images that is capable of extracting both CNV regions and vessel details. However, common issues including substantial scale variations of CNV regions and low-contrast microvascular boundaries were not fully deliberated in previous network designs. Thus, more dedicated modules with scale adaptivity and boundary refinement properties need to be explored to solve existing challenges.Previously, Gal et al. [10] proposed a theory to effectively model uncertainty with dropout NNs. Afterward, Bragman et al. [11] applied this method to the field of medical image analysis. Nair et al. [12] showed the success of using dropout for the detection of three-dimensional multiple sclerosis lesions. Motivated by these findings, we also consider taking advantage of the pixel-level uncertainty estimation and making it adaptive to the segmentation of ambiguous CNV boundaries.In this work, we propose a reliable boundary-guided network (RBGNet) to simultaneously segment both the CNV regions and vessels. Our proposed method is composed of a dual-branch encoder and a boundary uncertainty-guided multitask decoder. The dual-branch encoder is designed to capture both of the global long-range dependencies and the local context of CNVs with significant scale variations, while the proposed uncertainty-guided multi-task decoder is designed to strengthen the model to segment ambiguous boundaries. The uncertainty is achieved by approximating a Bayesian network through Monte Carlo dropout.The main contributions are summarized as follows:(a) We propose a multi-task joint optimization method to interactively learn shape patterns and boundary contours for more accurate segmentation of CNV regions and vessels. (b) We design a dual-stream encoder structure to take advantages of the CNN and transformer, which promote the network to effectively learn both local and global information. (c) We propose an uncertainty estimation-guided weight optimization strategy to provide reliable guidance for multi-task network training."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.0,Proposed Method,"The proposed RBGNet comprises three primary components: a dual-stream encoder, a multi-task decoder, and an uncertainty-guided weight optimization strategy, as depicted in Fig. 2. The OCTA image is firstly processed using a dual-stream encoder that combines Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) models [13] to produce high-dimensional semantic features. This approach enhances the local context and global dependencies of the image, thereby improving the CNV feature representation. These features are then fed into a multi-task decoder, which integrates information from multiple tasks to achieve better CNV segmentation. To further optimize the model's performance, we introduce a pixel-level uncertainty estimation approach that enhances the model's capacity to handle ambiguous region boundaries."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.1,Dual-Stream Encoder,"The CNVs in OCTA images are of various shapes and a wide range of scales, which pose challenges to the accurate segmentation of CNVs. To extract the long-range dependencies of cross-scale CNV information, we employ VIT [13] as an independent stream in the feature encoder of the proposed method. Moreover, the proposed dual-stream encoder is also embedded with a CNN-based multi-scale encoder to obtain the local context of CNV regions. Specifically, the VIT stream utilizes a stack of twelve transformer layers to extract features from flattened uniform non-overlapping patches, which seamlessly integrates with the CNN encoder stream through skip connections, similar to [14]. However, unlike [14], we divide the output representations of the transformer layers in VIT into four groups, each containing three feature representations. These groups correspond to the scales of the CNN stream, uniformly arranged from shallow to deep. Then, we perform element-wise summation for each group of representations, followed by reshaping them into non-overlapping patch sizes. The reshaped representation is further upsampled to the corresponding CNN feature resolution. The CNN branch is a U-shaped network [15] that extracts multiscale features using ReSidual U-block (RSU) proposed by [16] to preserve highresolution information locally. To integrate complementary information from the CNN and VIT features, we concatenate them in each of the first four feature extraction layers of the CNN branch. To enhance the features from the dualbranch encoder, we apply a bottleneck layer that consists of a feature extraction block, max-pooling, upsampling, and another feature extraction block."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.2,Multi-task Decoder,"The proposed multi-task decoder performs two main tasks: CNV region segmentation and vessel segmentation. The CNV region segmentation task contains two auxiliary subtasks including boundary prediction and shape regression. Each task is implemented at the end of the decoder using a 1 × 1 convolutional layer followed by a Sigmoid activation, as shown in Fig. 2.Region Segmentation: Region segmentation of CNV allows accurate assessment of lesion size. This task aims to accurately segment the entire CNV regions in OCTA images via boundary prediction and shape regression. Region segmentation is typically accomplished by categorizing individual pixels as either belonging to the CNV region or the background region. The purpose of region boundary prediction is to explicitly enhance the model's focus on ambiguous boundaries, allowing for more accurate region segmentation. The process of shape regression for region segmentation involves the transformation of boundary regression into a task of signed distance field regression. This is achieved by assigning a signed distance to each pixel, representing its distance from the boundary, with negative values inside the boundary, positive values outside the boundary, and zero values on the boundary. By converting the ground truth into a signed distance map (SDM), the network can learn CNV shape patterns from the rich shape pattern information contained in the SDMs.Vessel Segmentation: To improve the vessel segmentation performance, we propose to guide the model to focus on low-contrast vessel details by estimating their pixel uncertainty. Simultaneously, the complementary information from the CNV region segmentation task is further utilized to eliminate the interference of vessel pixels outside the region, thus better refining the vessel segmentation results. The proposed multi-task decoder improves the segmentation accuracy of regions and vessels by explicitly or implicitly using the information between individual tasks and optimizing each task itself."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.3,Uncertainty-Guided Multi-Task Optimization,"Uncertainty Estimation: In contrast to traditional deep learning models that produce deterministic predictions, Bayesian neural networks [17] can provide not only predictions but also uncertainty. It treats the network weights as random variables with a priori distribution and infers the posterior distribution of the weights. In this paper, we employ Monte Carlo dropout (MC-dropout) [10] to approximate Bayesian networks and capture the uncertainty of the model. Bayesian inference offers a rigorous method for making decisions in the presence of uncertainty. However, the computational complexity of computing the posterior distribution often renders it infeasible. This issue is usually solved by finding the best approximation in a finite space.To learn the weight distribution of the network, we minimize the Kullback-Leibler (KL) scatter between the true posterior distribution and its approximation. The probability distribution of each pixel is obtained based on Dropout to sample the posterior weight distribution M times. Then, the mean P i of each pixel is used to generate the prediction, while the variance V i is used to quantify the uncertainty of the pixel. This process can be described as follows.Pm , andUncertainty-Weighted Loss: In CNV region segmentation, the importance of each pixel may vary, especially for ambiguous boundaries, while assigning equal weights to all samples may not be optimal. To address this issue, uncertainty maps are utilized to assign increased weights to pixels with higher levels of uncertainty. This, in turn, results in a more substantial impact on the update of the model parameters. Moreover, the incorporation of multiple tasks can generate different uncertainty weights for a single image, enabling a more comprehensive exploration of CNV boundary features via joint optimization. We employ a combination of loss functions, including binary cross-entropy (BCE) loss, mean squared error (MSE) loss, and Dice loss, to optimize the model parameters across all tasks. However, for the region shape regression task, we restricted the loss functions to only BCE and MSE. We incorporate uncertainty weights into the BCE loss by weighting each pixel to guide uncertainty on model training, i.e.,where y i and ŷi are respective ground truth and prediction for pixel i. The total loss function can be expressed as L = T t λ t L t , where L t denotes the loss function for t th task. λ t denotes the loss weight, obtained by averaging the uncertainty map of the corresponding task and normalizing them to sum to 1."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Dataset:,"The proposed RBGNet was evaluated on a dataset consisting of 74 OCTA images obtained using the Heidelberg OCT2 system (Heidelberg, Germany). All images were from AMD patients with CNV progression, captured in a 3 × 3 mm 2 area centered at the fovea. The enface projected OCTA images of the avascular complex were used for our experiments. All the images were resized into a resolution of 384 × 384 for experiments. The CNV areas and vessels were manually annotated by one senior ophthalmologist, and then reviewed and refined by another senior ophthalmologist. All images were acquired with regulatory approvals and patient consents as appropriate, following the Declaration of Helsinki.Implementation Details: Our method is implemented based on the PyTorch framework with NVIDIA GeForce GTX 1080Ti. We train the model using an Adam optimizer with an initial learning rate of 0.0001 and a batch size of 4 for 300 epochs, without implementing a learning rate decay strategy. During training, the model inputs were subject to standard data augmentation pipelines, including random horizontal, vertical flips, random rotation, and random cropping. A 5-fold cross-validation approach is adopted to evaluate the performance."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Comparison with State-of-the-Arts:,"To benchmark our model's performance, we compared it with several state-of-the-art methods in the medical image segmentation field, including U-Net [15], CE-Net [18], CS-Net [19], Tran-sUNet [20], and the backbone method U 2 Net [16]. We use the Dice coefficient (Dice), intersection over union (IoU), false discovery rate (FDR), and area under the ROC curve (AUC) to evaluate the segmentation performance. The quantitative results are demonstrated in Table 1. Our results demonstrate that the  The proposed method exhibits superior performance in precisely segmenting ambiguous boundaries of CNV regions, as demonstrated in the first two rows of Fig. 3. In contrast, existing state-of-the-art methods such as U-Net [15], and CS-Net [19] exhibit limitations in accurately segmenting complex and variable structures, leading to the misidentification of background structures as CNV regions. The illustrated quantitative results and performance comparisons serve as evidence of the proposed method's ability to simultaneously segment CNV regions and vessels with state-of-the-art performance. Accurate segmentation of  "
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,3.1,Conclusion,"In summary, this study proposes a novel method to address the challenges of CNV segmentation in OCTA images. It incorporates a dual-branch encoder, multi-task optimization, and uncertainty-weighted loss to accurately segment CNV regions and vessels. The findings indicate that the utilization of crossscale information, multi-task optimization, and uncertainty maps improve CNV segmentations. The proposed method exhibits superior performance compared to state-of-the-art methods, which suggests potential clinical implications for the diagnosis of CNV-related diseases. Nevertheless, further research is needed to validate the effectiveness of the proposed approach in large-scale clinical studies."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.0,Methodology,"Overview. Figure 2 and Fig. 3 illustrate two choices of the proposed IFE. It is general for different DMIS tasks. 1) We introduce a feature quantization method based on either curvature (Fig. 2) or information entropy (Fig. 3), which characterizes the content abundance of each feature channel. The larger these parameters, the richer the texture and detail of the corresponding channel feature. 2) We select a certain proportion of channel features with high curvature or information entropy and combine them with raw features. IFE improves performance with minor modifications to the segmentation network architecture.  "
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.1,Curvature-Based Feature Selection,"For a two-dimensional surface embedded in Euclidean space R 3 , two curvatures exist: Gaussian curvature and mean curvature. Compared with the Gaussian curvature, the mean curvature can better reflect the unevenness of the surface. Gong [15] proposed a calculation formula that only requires a simple linear convolution to obtain an approximate mean curvature, as shown below:whereT , the values of α, β, and γ are -1/16, 5/16, -1. * denotes convolution, X represents the input image, and C is the mean curvature. Figure 2 illustrates the process, showing that the curvature image can effectively highlight the edge details in the features."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.2,Information Entropy-Based Feature Selection,"As a statistical form of feature, information entropy [1] reflects the spatial and aggregation characteristics of the intensity distribution. It can be formulated as:i n denotes the gray value of the center pixel in the n th 3 × 3 sliding window and j n denotes the average gray value of the remaining pixels in that window (teal blue box in Fig. 3). The probability of (i n , j n ) occurring in the entire image is denoted by P i,j , and E represents the information entropy.Each pixel on images corresponds to a gray or color value ranging from 0 to 255. However, each element in the feature map represents the activation level of the convolution filter at a particular position in the input image. Given an input feature F x , as shown in Fig. 3, the tuples (i, j) obtained by the sliding windows are transformed to a histogram, representing the magnitude of the activation level and distribution within the neighborhood. This involves rearranging the activation levels of the feature map. The histogram converting method histc and information entropy E are presented in Algorithm 1. Note that the probability P hist(i,j ) will be used to calculate the information entropy."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.3,Instructive Feature Enhancement,"Although IFE can be applied to various deep neural networks, this study mainly focuses on widely used segmentation networks. Figure 4 shows the framework of IFE embedded in representative networks, e.g., DeepLabV3+ [9], UNet [28], nnUNet [18], and SINetV2 [14]. The first three are classic segmentation networks. Because the MIS task is similar to the camouflage object detection, such as low contrast and blurred edges, we also consider SINetV2 [14]. According to [27], we implement IFE on the middle layers of UNet [28] and nnUNet [18], on the lowlevel features of DeepLabV3+ [9], and on the output of the TEM of SINetV2 [14].While the input images are encoded into the feature space, the different channel features retain textures in various directions and frequencies. Notably, the information contained by the same channel may vary across different images, which can be seen in Fig. 5. For instance, the 15 th channel of the lung CT feature map contains valuable texture and details, while the same channel in the aortic CT feature map may not provide significant informative content. However, Algorithm 1. Information entropy of features with histogram.Compute the histogram of (i, j). ext_k = kernel_size//2 P h i s t (i , j ) = f hist(i,j) / ((H + ext k ) × (W + ext k )) E = sum -P h i s t (i , j ) × log2 P h i s t (i , j ) their 2 nd channel features both focus on the edge details. By preserving the raw features, the channel features that contribute more to the segmentation of the current object can be enhanced by dynamically selecting from the input features. Naturally, it is possible to explicitly increase the sensitivity of the model to the channel information. Specifically, for input image X, the deep featurecan be obtained by an encoder with the weights θ x : F x = Encoder (X, θ x ), our IFE can be expressed as:F x is the selected feature map and S is the quantification method (see Fig. 2 or Fig. 3), and r is the selected proportion. As discussed in [6], enhancing the raw features through pixel-wise addition may introduce unwanted background noise and cause interference. In contrast, the concatenate operation directly joins the features, allowing the network to learn how to fuse them automatically, reducing the interference caused by useless background noises. Therefore, we used the concatenation and employed the concatenated features F = [F x , F x ] as the input to the next stage of the network. Only the initialization channel number of the corresponding network layer needs to be modified."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,3.0,Experimental Results,"Cosmos55k. To construct the large-scale Cosmos55k, 30 publicly available datasets [3,4,11,[19][20][21][22][23][24]29,30,32,35] were collected and processed with organizers' permission. The processing procedures included uniform conversion to PNG format, cropping, and removing mislabeled images. Cosmos55k (Fig. 1) offers 7 imaging modalities, including CT, MRI, X-ray, fundus, etc., covering 26 anatomical structures such as the liver, polyp, melanoma, and vertebra, among others. Images contain just one labeled object, reducing confusion from multiple objects with different structures.Implementation Details. Cosmos55k comprises 55,023 images, with 31,548 images used for training, 5,884 for validation, and 17,591 for testing. We conducted experiments using Pytorch for UNet [28], DeeplabV3+ [9], SINetV2 [14], and nnUNet [18]. The experiments were conducted for 100 epochs on an RTX 3090 GPU. The batch sizes for the first three networks were 32, 64, and 64, respectively, and the optimizer used was Adam with an initial learning rate of 10 -4 . Every 50 epochs, the learning rate decayed to 1/10 of the former. Considering the large scale span of the images in Cosmos55k, the images were randomly resized to one of seven sizes (224, 256, 288, 320, 352, or 384) before being fed into the network for training. During testing, the images were resized to a fixed size of 224. Notably, the model set the hyperparameters for nnUNet [18] automatically.  Quantitative and Qualitative Analysis. To demonstrate the efficacy of IFE, we employ the following metrics: Conformity (Con) [7], Dice Similarity Coefficient (DSC) [5], Jaccard Distance (JC) [12], F1 [2], Human Correction Efforts (HCE) [26], Mean Absolute Error (MAE) [25], Hausdorff Distance (HD) [34], Average Symmetric Surface Distance (ASD) [13], Relative Volume Difference(RVD) [13]. The quantitative results for UNet [28], DeeplabV3+ [9], SINetV2 [14], and nnUNet [18] are presented in Table 1. From the table, it can be concluded that IFE can improve the performance of networks on most segmentation metrics. Besides, Fig. 6 shows that IFE helps models perform better in most modalities and anatomical structures. Figure 7 presents a qualitative comparison. IFE aids in locating structures in an object that may be difficult to notice and enhances sensitivity to edge gray variations. IFE can substantially improve the segmentation accuracy of the base model in challenging scenes.Ablation Studies. Choosing a suitable selection ratio r is crucial when applying IFE to different networks. Different networks' encoders are not equally capable of extracting features, and the ratio of channel features more favorable to the segmentation result varies. To analyze the effect of r, we conducted experiments using UNet [28]. As shown in Table 2, either too large or too small r will lead to a decline in the model performance."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,4.0,Conclusion,"In order to benchmark the general DMIS, we build a large-scale dataset called Cosmos55k. To balance universality and accuracy, we proposed an approach (IFE) that can select instructive feature channels to further improve the segmentation over strong baselines against challenging tasks. Experiments showed that IFE can improve the performance of classic models with slight modifications in the network. It is simple, universal, and effective. Future research will focus on extending this approach to 3D tasks."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,1.0,Introduction,"Deep learning methods have demonstrated their tremendous potential when it comes to medical image segmentation. However, the success of most existing architectures relies on the availability of pixel-level annotations, which are difficult to produce [1]. Furthermore, these methods are known to be inadequately equipped for distribution shifts. Therefore, cross-modality generalization is needed when one imaging modality has insufficient training data. For instance, conditions such as Vestibular Schwannoma, where new hrT2 sequences are set to replace ceT1 for diagnosis to mitigate the use of contrast agents, is a sample use case [2]. Recently Billot et al. [3] proposed a domain randomisation strategy to segment images from a wide range of target contrasts without any fine-tuning. The method demonstrated great generalization capability for brain parcellation, but the model performance when exposed to tumors and pathologies was not quantified. This challenge could also be addressed through unsupervised domain-adaptive approaches, which transfer the knowledge available in the ""source"" modality S from pixel-level labels to the ""target"" imaging modality T lacking annotations [4].Several generative models attempt to generalize to a target modality by performing unsupervised domain adaptation through image-to-image translation and image reconstruction. In [5], by learning to translate between CT and MR cardiac images, the proposed method jointly disentangles the domain specific and domain invariant features between each modality and trains a segmenter from the domain invariant features. Other methods [6][7][8][9][10][11][12] also integrate this translation approach, but the segmenter is trained in an end-to-end manner on the synthetic target images generated from the source modality using a Cycle-GAN [13] model. These methods perform well but do not explicitly use the unannotated target modality data to further improve the segmentation.In this paper, we propose M-GenSeg, a novel training strategy for crossmodality domain adaptation, as illustrated in Fig. 1. This work leverages and extends GenSeg [14], a generative method that uses image-level ""diseased"" or ""healthy"" labels for semi-supervised segmentation. Given these labels, the model imposes an image-to-image translation objective between the image domain presenting tumor lesions and the domain corresponding to an absence of lesions. Therefore, like in low-rank atlas based methods [15][16][17] the model is taught to find and remove a lesion, which acts as a guide for the segmentation. We incorporate cross-modality image segmentation with an image-to-image translation objective between source and target modalities. We hypothesize both objectives are complementary since GenSeg helps localizing the tumors on unannotated target images, while modality translation enables fine-tuning the segmenter on the target modality by displaying annotated pseudo-target images. We evaluate M-GenSeg on a modified version of the BraTS 2020 dataset, in which each type of sequence (T1, T2, T1ce and FLAIR) is considered as a distinct modality. We demonstrate that our model can better generalize than other state-of-the-art methods to the target modality. Healthy-Diseased Translation. We propose to integrate image-level supervision to the cross-modality segmentation task with GenSeg, a model that introduces translation between domains with a presence (P) or absence (A) of tumor lesions. Leveraging this framework has a two-fold advantage here. Indeed, (i) training a GenSeg module on the source modality makes the model aware of the tumor appearances in the source images even with limited source pixel-level annotations. This helps to preserve tumor structures during the generation of pseudo-target samples (see Sect. 2.1). Furthermore, (ii) training a second GenSeg module on the target modality allows to further close the domain gap by extending the segmentation objective to unannotated target data."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.1,M-GenSeg: Semi-supervised Segmentation,"In order to disentangle the information common to A and P, and the information specific to P, we split the latent representation of each image into a common code c and a unique code u. Essentially, the common code contains information inherent to both domains, which represents organs and other structures, while the unique code stores features like tumor shapes and location. In the two fol-lowing paragraphs, we explain P→A and A→P translations for source images. The same process is applied for target images by replacing S notation with T .Presence to Absence Translation. Given an image S P of modality S in the presence domain P, we use an encoder E S to compute the latent representation [c S P , u S P ]. A common decoder G S com takes as input the common code c S P and generates a healthy version S PA of that image by removing the apparent tumor region. Simultaneously, both common and unique codes are used by a residual decoder G S res to output a residual image Δ S PP , which corresponds to the additive change necessary to shift the generated healthy image back to the presence domain. In other words, the residual is the disentangled tumor that can be added to the generated healthy image to create a reconstruction S PP of the initial diseased image: Like approaches in [18][19][20] we therefore generate diseased samples from healthy ones for data augmentation. However, M-GenSeg aims primarily at tackling cross-modality lesion segmentation tasks, which is not addressed in these studies. Furthermore, note that these methods are limited to data augmentation and do not incorporate any unannotated diseased samples when training the segmentation network, as achieved by our model with the P→A translation.Modality Translation. Our objective is to learn to segment tumor lesions in a target modality by reusing potentially scarce image annotations in a source modality. Note that for each modality m ∈ {S, T }, M-GenSeg holds a segmentation decoder G m seg that shares most of its weights with the residual decoder G m res , but has its own set of normalization parameters and a supplementary classifying layer. Thus, through the Absence and Presence translations, these segmenters have already learned how to disentangle the tumor from the background. However, supervised training on a few example annotations is still required to learn how to transform the resulting residual representation into appropriate segmentation maps. While this is a fairly straightforward task for the source modality using pixel-level annotations, achieving this for the target modality is more complex, justifying the second unsupervised translation objective between source and target modalities. Based on the CycleGan [13] approach, modality translations are performed via two distinct generators that share their encoder with the GenSeg task. More precisely, combined with the encoder E S a decoder G T enables performing S→T modality translation, while the encoder E T and a second decoder G S perform the T→S modality translation. To maintain the anatomical information, we ensure cycle-consistency by reconstructing the initial images after mapping them back to their original modality. We note for the T→S→T cycle. Note that to perform the domain adaptation, training the model to segment only the pseudo-target images generated by the S→T modality generator would suffice (in addition to the diseased/healthy target translation). However, training the segmentation on diseased source images also imposes additional constraints on encoder E S , ensuring the preservation of tumor structures. This constraint proves beneficial for the translation decoder G T as it generates pseudo-target tumoral samples that are more reliable. Segmentation is therefore trained on both diseased source images S P and their corresponding synthetic target images S T P , when provided with annotations y S . To such an extent, two segmentation masks are predicted ŷS = G S seg • E S (S P ) and ŷST = G T seg • E T (S T P )."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.2,Loss Functions,"Segmentation Loss. For the segmentation objective, we compute a soft Dice loss [21] on the predictions for both labelled source images and their translations:Reconstruction Losses. L mod cyc and L Gen rec respectively impose pixel-level image reconstruction constraints on modality translation and GenSeg tasks. Note that L 1 refers to the standard L1 norm:Moreover, like in [14] we compute a loss L Gen lat that ensures that the translation task holds the information relative to the initial image, by reconstructing their latent codes with the L1 norm. It also enforces the distribution of unique codes to match the prior N (0, I) by making u AP match u, where u AP is obtained by encoding the fake diseased sample x AP produced with random sample u.Adversarial Loss. For the healthy-diseased translation adversarial objective, we compute a hinge loss L Gen adv as in GenSeg, learning to discriminate between pairs of real/synthetic images of the same output domain and always in the same imaging modality, e.g. S A vs S PA . In the modality translation task, the L mod adv loss is computed between pairs of images of the same modality without distinction between domains A and P , e.g. {S A , S P } vs {T S A , T S P }.Overall Loss. The overall loss for M-GenSeg is a weighted sum of the aforementioned losses. These are tuned separately. All weights sum to 1. First, λ Gen adv , λ Gen rec , and λ Gen lat weights are tuned for successful translation between diseased and healthy images. Then, λ mod adv and λ mod cyc are tuned for successful modality translation. Finally, λ seg is tuned for segmentation performance."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.3,Implementation Details,"Training and Hyper-Parameters. All models are implemented using PyTorch and are trained on one NVIDIA A100 GPU with 40 GB memory. We used a batch size of 15, an AMSGrad optimizer (β 1 = 0.5 and β 2 = 0.999) and a learning rate of 10 -4 . Our models were trained for 300 epochs and weights of the segmentation model with the highest validation Dice score were saved for evaluation. The same on-the-fly data augmentation as in [14] was applied for all runs. Each training experiment was repeated three times with a different random seed for weight initialization. The performance reported is the mean of all test Dice scores, with standard deviation, across the three runs. The following parameters yielded both great modality and absence/presence translations: λ mod adv = 3, λ mod cyc = 20, λ Gen adv = 6, λ Gen rec = 20 and λ Gen lat = 2. Note that optimal λ seg varies depending on the fraction of pixel-level annotations provided to the network for training.Architecture. One distinct encoder, common decoder, residual/segmentation decoder, and modality translation decoder are used for each modality. The architecture used for encoders, decoders and discriminators is the same as in [14]. However, in order to give insight on the model's behaviour and properly choose the semantic information relevant for each objective, we introduced attention gates [22] in the skip connections. Figure 2a shows the attention maps generated for each type of decoder. As expected, residual decoders focus towards tumor areas. More interestingly, in order not to disturb the process of healthy image generation, common decoders avoid lesion locations. Finally, modality translators tend to focus on salient details of the brain tissue, which facilitates contrast redefinition needed for accurate translation."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.1,Datasets,"Experiments were performed on the BraTS 2020 challenge dataset [23][24][25], adapted for the cross-modality tumor segmentation problem where images are known to be diseased or healthy. Amongst the 369 brain volumes available in BraTS, 37 were allocated each for validation and test steps, while the 295 left were used for training. We split the 3D brain volumes into 2 hemispheres and extracted 2D axial slices. Any slices with at least 1% tumor by brain surface area were considered diseased. Those that didn't show any tumor lesion were labelled as healthy images. Datasets were then assembled from each distinct pair of the four MRI contrasts available (T1, T2, T1ce and FLAIR). To constitute unpaired training data, we used only one modality (source or target) per training volume. All the images are provided with healthy/diseased weak labels, distinct from the pixel-level annotations that we provide only to a subset of the data. Note that the interest for cross-sequence segmentation is limited if multi-parametric acquisitions are performed as is the case in BraTS. However, this modified version of the dataset provides an excellent study case for the evaluation of any modality adaptation method for tumor segmentation."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.2,Model Evaluation,"Domain Adaptation. We compared M-GenSeg with AccSegNet [10] and AttENT [6], two high performance models for domain-adaptative medical image segmentation. To that extent, we performed domain-adaptation experiments with source and target modalities drawn from T1, T2, FLAIR and T1ce. We used available GitHub code for the two baselines and performed fine-tuning on our data. For each possible source/target pair, pixel-level annotations were only retained for the source modality. We show in Fig. 2b several presence to absence translations and segmentation examples on different target modality images. Although no pixel-level annotations were provided for the target modality, tumors were well disentangled from the brain, resulting in a successful presence to absence translation, as well as segmentation. Note that for hypo-intense lesions (T1 and T1ce), M-GenSeg still manages to convert complex residuals into consistent segmentation maps. We plot in Fig. 3 the Dice performance on the target modality for (i) supervised segmentation on source data without domain adaptation, (ii) domain adaptation methods and (iii) UAGAN [26], a model designed for unpaired multi-modal datasets, trained on all source and target data. Over all modality pairs our model shows an absolute Dice score increase of 0.04 and 0.08, respectively, compared to AccSegNet and AttENT. Annotation Deficit. M-GenSeg introduces the ability to train with limited pixel-level annotations available in the source modality. We show in Fig. 4 the Dice scores for models trained when only 1%, 10%, 40%, or 70% of the source T1 modality and 0% of the T2 target modality annotations were available. While performance is severely dropping at 1% of annotations for the baselines, our model shows in comparison only a slight decrease. We thus claim that M-GenSeg can yield robust performance even when a small fraction of the source images is annotated.Reaching Supervised Performance. We report that, when the target modality is completely unannotated, M-GenSeg reaches 90% of UAGAN's performance (vs 81% and 85% for AttENT and AccSegNet). Further experiments showed that with a fully annotated source modality, it is sufficient to annotate 25% of the target modality to reach 99% of the performance of fully-supervised UAGAN (e.g. M-GenSeg: 0.861 ± 0.004 vs UAGAN: 0.872 ± 0.003 for T1 → T2 experiment). Thus, the annotation burden could be reduced with M-GenSeg."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.3,Ablation Experiments,"We conducted ablation tests to validate our methodological choices. We report in Table 1 the relative loss in Dice scores on target modality as compared to the proposed model. We assessed the value of doing image-level supervision by setting all the λ Gen loss weights to 0 . Also, we showed that training modality translation only on diseased data is sufficient . However, doing it for healthy data as well provides additional training examples for this task. Likewise, performing translation from absence to presence domain is not necessary but makes more efficient use of the data. Finally, we evaluated M-GenSeg with separate latent spaces for the image-level supervision and modality translation, and we contend that M-GenSeg efficiently combines both tasks when the latent representations share model updates. "
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,4.0,Conclusion,"We propose M-GenSeg, a new framework for unpaired cross-modality tumor segmentation. We show that M-GenSeg is an annotation-efficient framework that greatly reduces the performance gap due to domain shift in cross-modality tumor segmentation. We claim that healthy tissues, if adequately incorporated to the training process of neural networks like in M-GenSeg, can help to better delineate tumor lesions in segmentation tasks. However, top performing methods on BraTS are 3D models. Thus, future work will explore the use of full 3D images rather than 2D slices, along with more optimal architectures. Our code is available: https://github.com/MaloADBA/MGenSeg_2D."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,,"P ) and S PP = S PA + Δ S PP(1) Absence to Presence Translation. Concomitantly, a similar path is implemented for images in the healthy domain. Given an image S A of modality S in domain A, we generate a translated version in domain P. To do so, a synthetic tumor Δ S AP is generated by sampling a code from the normal distribution N (0, I) A , u S A ] as follows:"
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_14.
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"Simultaneous multi-index quantification (i.e., max diameter (MD), center point coordinates (X o , Y o ), and Area), segmentation, and uncertainty prediction of liver tumor have essential significance for the prognosis and treatment of patients [6,16]. In clinical settings, segmentation and quantitation are manually performed by the clinicians through visually analyzing the contrast-enhanced MRI images (CEMRI) [9,10,18]. However, as shown in Fig. 1(b), Contrast-enhanced Fig. 1. Our method integrates segmentation and quantification of liver tumor using multi-modality NCMRI, which has the advantages of avoiding contrast agent injection, mutual promotion of multi-task, and reliability and stability. MRI (CEMRI) has the drawbacks of being toxic, expensive, and time-consuming due to the need for contrast agents (CA) to be injected [2,4]. Moreover, manually annotating medical images is a laborious and tedious process that requires human expertise, making it manpower-intensive, subjective, and prone to variation [14]. Therefore, it is desirable to provide a reliable and stable tool for simultaneous segmentation, quantification, and uncertainty analysis, without requiring the use of contrast agents, as shown in Fig. 1(a).Recently, an increasing number of works have been attempted on liver tumor segmentation or quantification [25,26,28,30]. As shown in Fig. 1(c), the work [26] attempted to use the T2FS for liver tumor segmentation, while it ignored the complementary information between multi-modality NCMRI of T2FS and DWI. In particular, there is evidence that diffusion-weighted imaging (DWI) helps to improve the detection sensitivity of focal lesions as these lesions typically have higher cell density and microstructure heterogeneity [20]. The study in [25,30] attempted to quantify the multi-index of liver tumor, however, the approach is limited to using multi-phase CEMRI that requires the injection of CA. In addition, all these works are limited to a single task and ignore the constraints and mutual promotion between multi-tasks. Available evidence suggests that uncertainty information regarding segmentation results is important as it guides clinical decisions and helps understand the reliability of the provided segmentation. However, current research on liver tumors tends to overlook this vital task.To the best of our knowledge, although many works focus on the simultaneous quantization, segmentation, and uncertainty in medical images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). No attempt has been made to automatically liver tumor multi-task via integrating multi-modality NCMRI due to the following challenges: (1) The lack of an effective multi-modality MRI fusion mechanism. Because the imaging characteristics between T2FS and DWI have significant differences (i.e., T2FS is good at anatomy structure information while DWI is good at location information of lesions [29]). (2) The lack of strategy for capturing the accurate boundary information of liver tumors. Due to the lack of contrast agent injection, the boundary of the lesion may appear blurred or even invisible in a single NCMRI, making it challenging to accurately capture tumor boundaries [29]. (3) The lack of an associated multi-task framework. Because segmentation and uncertainty involve pixel-level classification, whereas quantification tasks involve image-level regression [11]. This makes it challenging to integrate and optimize the complementary information between multi-tasks.In this study, we propose an edge-aware multi-task network (EaMtNet) that integrates the multi-index quantification (i.e., center point, max-diameter (MD), and Area), segmentation, and uncertainty. Our basic assumption is that the model should capture the long-range dependency of features between multimodality and enhance the boundary information for quantification, segmentation, and uncertainty of liver tumors. The two parallel CNN encoders first extract local feature maps of multi-modality NCMRI. Meanwhile, to enhance the weight of tumor boundary information, the Sobel filters are employed to extract edge maps that are fed into edge-aware feature aggregation (EaFA) as prior knowledge. Then, the EaFA module is designed to select and fuse the information of multi-modality, making our EaMtNet edge-aware by capturing the long-range dependency of features maps and edge maps. Lastly, the proposed method estimates segmentation, uncertainty prediction, and multi-index quantification simultaneously by combining multi-task and cross-task joint loss.The contributions of this work mainly include: (1) For the first time, multiindex quantification, segmentation, and uncertainty of the liver tumor on multimodality NCMRI are achieved simultaneously, providing a time-saving, reliable, and stable clinical tool. (2) The edge information extracted by the Sobel filter enhances the weight of the tumor boundary by connecting the local feature as prior knowledge. (3) The novel EaFA module makes our EaMtNet edge-aware by capturing the long-range dependency of features maps and edge maps for feature fusion. The source code will be available on the author's website."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.0,Method,"The EaMtNet employs an innovative approach for simultaneous tumor multiindex quantification, segmentation, and uncertainty prediction on multimodality NCMRI. As shown in Fig. 2, the EaMtNet inputs multi-modality NCMRI of T2FS and DWI for capturing the feature and outputs the multiindex quantification, segmentation, and uncertainty. Specifically, the proposed approach mainly consists of three steps: 1) The CNN encoders for capturing feature maps and the Sobel filters for extracting edge maps (Sect. 2.1); 2) The edge-aware feature aggregation (EaFA) for multi-modality feature selection and fusion via capturing the long-distance dependence (Sect. 2.2); and 3) Multi-task prediction module (Sect. 2.3)."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.1,CNN Encoder for Feature Extraction,"In Step 1 of Fig. 2, the multi-modality NCMRI (i.e., χ i T 2 ∈ R H×W , χ i DW I ∈ R H×W ) are fed into two parallel encoders and the Sobel filter to extract the feature maps (i.e., g i T 2 ∈ R H×W ×N , g i DW I ∈ R H×W ×N ) and the corresponding edge maps (i.e., edge i T 2 ∈ R H×W , edge i DW I ∈ R H×W ) respectively. Specifically, EaMtNet employs UNet as the backbone for segmentation because the CNN encoder has excellent capabilities in low-range semantic information extraction [15]. The two parallel CNN encoders have the same architecture where each encoder contains three shallow convolutional network blocks to capture features of adjacent slices. Each conv block consists of a convolutional layer, batch normalization, ReLU, and non-overlapping subsampling. At the same time, EaMt-Net utilizes the boundary information extracted by the Sobel filter [19] as prior knowledge to enhance the weight of tumor edge information to increase the awareness of the boundary."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.2,Edge-Aware Feature Aggregation(EaFA) for Multi-modality Feature Selection and Fusion,"In Step 2 of the proposed model, the feature maps (i.e., g i T 2 , g i DW I ) and the edge maps (i.e., edge i T 2 , edge i DW I ) are fed into EaFA for multi-modality feature fusion with edge-aware. In particular, the EaFA makes the EaMtNet edge-aware by using the Transformer to capture the long-range dependency of feature maps and edge maps. Specifically, the feature maps and edge maps are first flattened to the 1D sequence corresponding to X 1D ∈ R N ×P 2 and E 1D ∈ R 2×Q 2 , respectively. Where N = 2 × C means the channel number C of the last convolutional layer from the two parallel encoders. (P, P ) and (Q, Q) represent the resolution of each feature map and each edge map, respectively. On the basis of the 1D sequence, to make the feature fusion with edge awareness, the operation of position encoding is performed not only on feature maps but also on edge maps. The yielded embeddings Z ∈ R N ×P 2 +2×Q 2 can serve as the input sequence length for the multi-head attention layer in Transformer. The following operations in our EaFA are similar to the traditional Transformer [22]. After the three cascade Transformer layers, the EaFA yields the fusion feature vector F for multi-task prediction. The specific computation of the self-attention matrix and multi-head attention are defined below [22]:where query Q, key K, and value V are all vectors of the flattened 1D sequences of X 1D and E 1D . W O i is the projection matrix, and 1is the scaling factor."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.3,Multi-task Prediction,"In Step 3 of Fig. 2, the EaMtNet outputs the multi-modality quantification ŷQ (i.e., MD, X o , Y o and Area), segmentation result ŷs and uncertainty map ûi . Specifically, for the quantification path, ŷQ is directly obtained by performing a linear layer to the feature F from EaFA. For the segmentation and uncertainty path, the output feature F from EaFA is first reshaped into a 2D feature map F out . Then, to scale up to higher-resolution images, a 1 × 1 convolution layer is employed to change the channel number of F out for feeding into the decoder. After upsampling by the CNN decoder, EaMtNet predicts the segmentation result ŷs with H × W and uncertainty map ûi with H × W . The CNN decoder contains three shallow deconv blocks, which consist of deconv layer, batch normalization, and ReLU. Inspired by [24], we select the entropy map as our uncertainty measure. Given the prediction probability after softmax, the entropy map is computed as follows:where z i is the probability of pixel x belonging to category i. When a pixel has high entropy, it means that the network is uncertain about its classification. Therefore, pixels with high entropy are more likely to be misclassified. In other words, its entropy will decrease when the network is confident in a pixel's label.Under the constraints of uncertainty, the EaMtNet can effectively rectify the errors in tumor segmentation because the uncertainty estimation can avoid overconfidence and erroneous quantification [23]. Moreover, the EaMtNet novelly make represent different tasks in a unified framework, leading to beneficial interactions. Thus, the quantification performance is improved through backpropagation by the joint loss function L multi-task . The function comprises segmentation loss L seg and quantification loss L qua , where the loss function L seg is utilized for optimizing tumor segmentation, and L qua is utilized for optimization of multi-index quantification. It can be defined as:where ŷs represents the prediction, and y i represents the ground truth label. The sum is performed on S pixels, ŷi task represents the predicted multi-index value, and y i task represents the ground truth of multi-index value, task ∈ {MD, X, Y , Area}."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,3.0,Experimental Results and Discussion,"For the first time, EaMtNet has achieved high performance with the dice similarity coefficient (DSC) up to 90.01 ± 1.23%, and the mean absolute error (MAE) of the MD, X o , Y o and Area are down to 2.72 ± 0.58 mm,1.87±0.76 mm, 2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively.Dataset and Configuration. An axial dataset includes 250 distinct subjects, each underwent initial standard clinical liver MRI protocol examinations with corresponding pre-contrast images (T2FS [4mm]) and DWI [4mm]) was collected. The ground truth was reviewed by two abdominal radiologists with 10 and 22 years of experience in liver imaging, respectively. If any interpretations demonstrated discrepancies between the reviewers, they would re-evaluate the examinations together and reach a consensus. To align the paired images of T2 and DWI produced at different times. We set the T2 as the target image and the DWI as the source image to perform the pre-processing of non-rigid registration between T2 and DWI by using the Demons non-rigid registration method. It has been widely used in the field of medical image registration since it was proposed by Thirion [21]. We perform the Demons non-rigid registration on an open-source toolbox DIRART using Matlab 2017b.Inspired by the work [22], we set the scaling factor d k to 64 in equation (1). All experiments were assessed with a 5-fold cross-validation test. To quantitatively evaluate the segmentation results, we calculated the dice coefficient scores (DSC) metric that measures the overlapping between the segmentation prediction and ground truth [12]. To quantitatively evaluate the quantification results,  we calculated the mean absolute error (MAE). Our EaMtNet was implemented using Ubuntu 18.04 platform, Python v3.6, PyTorch v0.4.0, and running on two NVIDIA GTX 3090Ti GPUs.Accurate Segmentation. The segmentation performance of EaMtNet has been validated and compared with three state-of-the-art (SOTA) segmentation methods (TransUNet [1], UNet [15], and UNet++ [31]). Furthermore, to ensure consistency in input modality, the channel number of the first convolution layer in the three comparison methods is set to 2. The visual examples of liver tumors are shown in Fig. 3, it is evident that our proposed EaMtNet outperforms the three SOTA methods. Some quantitative analysis results are shown in Table 1 and Table 2, our network achieves high performance with the DSC of 90.01 ± 1.23% (5.39% higher than the second-best). The results demonstrate that edge-aware, multi-modality fusion, and uncertainty prediction are essential for segmentation.Ablation Study. To verify the contributions of edge-aware feature aggregation (EaFA) and uncertainty, we performed ablation study and compared and performance of different networks. First, we removed the EaFA and used concatenate, meaning we removed fusion multi-modality (No-EaFA). Then, we removed the uncertainty task (No-Uncertainty). The quantitative analysis results of these ablation studies are shown in Table 1. Our method exhibits high performance in both segmentation and quantification, indicating that each component of the EaMtNet plays a vital role in liver tumor segmentation and quantification.Performance Comparison with State-of-the-Art. The EaMtNet has been validated and compared with three SOTA segmentation methods and two SOTA quantification methods (i.e., ResNet-50 [7] and DenseNet [8]). Furthermore, the channel number of the first convolution layer in the two quantification comparison methods is set to 2 to ensure the consistency of input modalities. The visual segmentation results are shown in Fig. 3. Moreover, the quantitative results (as shown in Table 2) corresponding to the visualization results (i.e., Fig. 3) obtained from the existing experiments further demonstrate that our method outperforms the three SOTA methods. Specifically, compared with the second-best approach, the DSC is boosted from 84.62 ± 1.45% to 90.01 ± 1.23%. The quantitative analysis results are shown in Table 3. It is evident that our method outperforms the two SOTA methods with a large margin in all metrics, owing to the proposed multi-modality fusing and multi-task association."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,4.0,Conclusion,"In this paper, we have proposed an EaMtNet for the simultaneous segmentation and multi-index quantification of liver tumors on multi-modality NCMRI. The new EaFA enhances edge awareness by utilizing boundary information as prior knowledge while capturing the long-range dependency of features to improve feature selection and fusion. Additionally, multi-task leverages the prediction discrepancy to estimate uncertainty, thereby improving segmentation and quantification performance. Extensive experiments have demonstrated the proposed model outperforms the SOTA methods in terms of DSC and MAE, with great potential to be a diagnostic tool for doctors."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,1.0,Introduction,"Automatic detection of brain tumors from Magnetic Resonance Imaging (MRI) is complex, tedious, and time-consuming because there are a lot of missed, misinterpreted, and misleading tumor-like lesions in the images of the brain tumors [8]. Most of the current work focuses on brain tumor classification and segmentation from MRI and detection tasks are less explored [1,13,22]. While existing studies showed that various Convolutional Neural Networks (CNNs) are efficient for brain tumor detection, the performance of using You Only Look Once (YOLO) networks is scarcely investigated [12,20,[23][24][25]27].With the rapid development of CNNs, the accuracies of different visual tasks are constantly improved. However, the increasingly complex network architecture in CNN-based models, such as ResNet [6], DenseNet [9], Inception [28], etc. renders the inference speed slower. Though many advanced CNNs deliver higher accuracy, the complicated multi-branch designs (e.g., residual-addition in ResNet and branch-concatenation in Inception) make the models difficult to implement and customize, slowing down the inference and reducing memory utilization. The depth-wise separable convolutions used in MobileNets [7] also reduce the upper limit of the GPU inference speed. In addition, 3 × 3 regular convolution is highly optimized by some modern computing libraries. Consequently, VGG [26] is still heavily used for real-world applications in both research and industries.RepVGG [2] is an extension of VGG via reparametrization to accelerate inference time. RepVGG uses a multi-branch topological architecture during the training phase, which is then reparameterized to a simplified single-branch architecture during the inference phase. In terms of the optimization strategy of network training, reparameterization was introduced in YOLOv6 [16], YOLOv7 [31], and YOLOv6 v3.0 [17]. YOLOv6 and YOLOv6 v3.0 employ reparameterization from RepVGG. RepConv, a RepVGG without an identity connection, is converted from RepVGG during inference time in YOLOv6, YOLOv6 v3.0, and YOLOv7 (named RepConvN in YOLOv7). Due to the removal of identity connections in RepConv, direct access to ResNet or the concatenation in DenseNet can provide more diversity of gradients for different feature maps. Grouped convolutions, which use a group of convolutions with multiple kernels per layer, like RepVGG, can also significantly reduce the computational complexity of the model, but there is no information communication between groups, which limits the ability of feature extraction of the convolution operator. In order to overcome the disadvantage of grouped convolutions, ShuffleNet V1 [34] and V2 [21] introduced the channel shuffle operation to facilitate information flows across different feature channels. In addition, when comparing Spatial Pyramid Pooling & Cross Stage Partial Network plus ConvBNSiLU (SPPCSPC) in YOLOv7 with Spatial Pyramid Pooling Fast (SPPF) in YOLOv5 [10] and YOLOv8 [11], it is found that more convolution layers in SPPCSPC architecture slow down the computation of the network. Nevertheless, SPP [4,5]   "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.0,Methods,The architecture of the proposed RCS-YOLO network is shown in Fig. 1. It incorporates a new module-RCS-OSA in the backbone and neck of the YOLObased object detector.
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.1,RepVGG/RepConv ShuffleNet,"Inspired by ShuffleNet, we design a structural reparameterized convolution based on channel shuffle. Figure 2 shows the structural schematic diagram of RCS.Given that the feature dimensions of an input tensor are C × H × W , after the channel split operator, it is divided into two different channel-wise tensors with equal dimensions of C×H ×W . For one of the tensors, we use the identity branch, 1 × 1 convolution, and 3 × 3 convolution to construct the training-time RCS. At the inference stage, the identity branch, 1 × 1 convolution, and 3 × 3 convolution are transformed to 3 × 3 RepConv by using structural reparameterization. The multi-branch topology architecture can learn abundant information about features during the training time, simplified single-branch architecture can save memory consumption during the inference time to achieve fast inference. After the multi-branch training of one of the tensors, it is concatenated to the other tensor in a channel-wise manner. The channel shuffle operator is also applied to enhance information fusion between two tensors so that the depth measurement between different channel features of the input can be realized with low computational complexity. When there is no channel shuffle, the output feature of each group only relates to the input feature within a group of grouped convolutions, and outputs from a certain group only relate to the input within the group. This blocks information flow between channel groups and weakens the ability of feature extraction. When channel shuffle is used, input and output features are fully related where one convolution group takes data from other groups, enabling more efficient feature information communication between different groups. The channel shuffle operates on stacked grouped convolutions and allows more informative feature representation. Moreover, assuming that the number of groups is g, for the same input feature, the computational complexity of channel shuffle is 1 g times that of a generic convolution.Compared with the popular 3 × 3 convolution, during the inference stage, RCS uses the operators including channel split and channel shuffle to reduce the computational complexity by a factor of 2, while keeping the inter-channel information exchange. Moreover, using structural reparameterization enables deep representation learning from input features during the training stage, and reduction of inference-time memory consumption to achieve fast inference."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.2,RCS-Based One-Shot Aggregation,"The One-Shot Aggregation (OSA) module has been proposed to overcome the inefficiency of dense connections in DenseNet, by representing diversified features with multi-receptive fields and aggregating all features only once in the last feature maps. VoVNet V1 [14] and V2 [15] used the OSA module within its architecture to construct both lightweight and large-scale object detectors, which outperform the widely-used ResNet backbone with faster speed and better energy efficiency.We develop an RCS-OSA module by incorporating RCS developed in Sect. 2.1 for OSA, as shown in Fig. 3. The RCS modules are stacked repeatedly to ensure the reuse of features and to enhance the information flow among different channels between features of adjacent layers. At different locations of the network, we set a different number of stacked modules. To reduce the level of network fragmentation, only three feature cascades are maintained on the one-shot aggregate path, which can mitigate the amount of network calculation burden and reduce the memory footprint. In terms of multi-scale feature fusion, inspired by the idea of Path Aggregation Network (PANet) [19], RCS-OSA + Upsampling and RCS-OSA + RepVGG/RepConv undersampling carry out the alignment of feature maps of different sizes to allow information exchange between the two prediction feature layers. This enables high-accuracy fast inference in object detection. Moreover, RCS-OSA maintains the same number of input channels and minimum output channels, thus reducing the memory access cost (MAC). For network building, we perpetuate max-pooling undersampling 32 times of YOLOv7 to construct a backbone network and adopt RepVGG/RepConv with a step of 2 to achieve undersampling. Due to the diversified feature representation of the RCS-OSA module and low-cost memory consumption, we use a different number of stacked RCS in RCS-OSA modules to achieve semantic information extraction during different stages of both backbone and neck networks. The common evaluation metric of computation efficiency (or time complexity) is floating-point operations (FLOPs). FLOPs are only the indirect indicator to measure the speed of inference. However, the object detector with a DenseNet backbone shows rather slow speed and low energy efficiency because the linearly increasing number of channels by dense connection leads to heavy MAC, which causes considerable computation overhead. Given input features of dimension M × M , the convolution kernel of size K × K, number of input channels C 1 , and the number of output channels C 2 , FLOPs and MAC can be calculated as:Assuming n to be 4, FLOPs of the proposed RCS-OSA and Efficient Layer Aggregation Networks (ELAN) [31,33] are 20.25C 2 M 2 and 40C 2 M 2 respectively. Compared with ELAN, FLOPs of RCS-OSA are reduced by nearly 50%. The MAC of RCS-OSA (i.e., 6CM 2 + 20.25C 2 ) is also reduced compared to that of ELAN (i.e., 17CM 2 + 40C 2 )."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.3,Detection Head,"To further reduce inference time, we decrease the number of detection heads comprised of RepVGG and IDetect from 3 to 2. The YOLOv5, YOLOv6, YOLOv7, and YOLOv8 have three detection heads. However, we use only two feature layers for prediction, reducing the number of original nine anchors with different scales to four and using the K-means unsupervised clustering method to regenerate anchors with different scales. The corresponding scales are (87, 90), (127, 139), (154, 171), (191,240). This not only reduces the number of convolution layers and computational complexity of RCS-YOLO but also reduces the overall computational requirements of the network during the inference stage and the computational time of postprocessing non-maximum suppression."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.1,Dataset Details,"To evaluate the proposed RCS-YOLO model, we used the brain tumor detection 2020 dataset (Br35H) [3], with a total of 701 images in the 'train' and 'val' two folders, 500 images of which are the 'train' folder were selected as the training set, while the other 201 images in the 'val' folder as the testing set. For the input size of 640×640 image, the actual corresponding size is 44×32. The small object is defined as the object whose pixel size is less than 32 × 32 defined by the MS COCO dataset [18], so there are no small objects in the brain tumor medical image data sets, and the scale change of the target boxes is smooth, almost square. The label boxes of the brain images were normalized (See supplementary material Sect. 1)."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.2,Implementation Details,"For model training and inference, we used Ubuntu 18.04 LTS, Intel R Xeon R Gold 5218 CPU processor, CUDA 12.0, and cuDNN 8.2. GPU is GeForce RTX 3090 with 24G memory size. The networking development framework is Pytorch 1.9.1. The Integrated Development Environment (IDE) is PyCharm. We uniformly set epoch 150, the batch size as 8, image size as 640 × 640. Stochastic Gradient Descent (SGD) optimizer was used with an initial learning rate of 0.01 and weight decay of 0.0005."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.3,Evaluation Metrics,"In this paper, we choose precision, recall, AP 50 , AP 50:95 , FLOPs, and Frames Per Second (FPS) as comparative metrics of detection effect to determine the advantages and disadvantages of the model. Taking IoU = 0.5 as the standard, precision, and recall can be calculated by the following equations:"
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,P recision = T P T P + F P,"(3)where T P represents the number of positive samples correctly identified as positive samples, F P represents the number of negative samples incorrectly identified as positive samples and F N represents the number of positive samples incorrectly identified as negative samples. AP 50 is the area under the precision-recall (PR) curve formed by precision and recall. For AP 50:95 , divide 10 IoU threshold of 0.5:0.05:0.95 to acquire the area under the PR curve, then average the results. FPS represents the number of images detected by the model per second."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.4,Results,"To highlight the accuracy and rapidity of the proposed model for the detection of brain tumor medical image data set, Table 1 shows the performance comparison between our proposed detector and other state-of-the-art object detectors. The time duration of FPS includes data preprocessing, forward model inference, and post-processing. The long border of the input images is set as 640 pixels. The short border adaptively scales without distortion, whilst keeping the grey filling with 32 times the pixels of the short border.It can be seen that RCS-YOLO with the advantages of incorporating the RCS-OSA module performs well. Compared with YOLOv7, the FLOPs of the object detectors of this paper decrease by 8.8G, and the inference speed improves by 43.4 FPS. In terms of detection rate, precision improves by 0.024; AP 50 increases by 0.01; AP 50:95 by 0.006. Also, RCS-YOLO is faster and more accurate than YOLOv6-L v3.0 and YOLOv8l. Although the AP 50:95 of RCS-YOLO equals that of YOLOv8l, it doesn't obscure the essential advantage of RCS-YOLO. The results clearly show the superior performance and efficiency of our method, compared to the state-of-the-art for brain tumor detection. As shown in supplementary material Fig. 2, brain tumor regions are accurately detected from MRI by using the proposed method. "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.5,Ablation Study,"We demonstrate the effectiveness of the proposed RCS-OSA module in YOLObased object detectors. The results of RepVGG-CSP in Table 2, where RCS-OSA in the RCS-YOLO is replaced with the Cross Stage Partial Network (CSP-Net) [32] used in existing YOLOv4-CSP [30] architecture, are decreased than RCS-YOLO except GFLOPs. Because the parameters of RepVGG-CSP (22.2M) are less than half those of RCS-YOLO (45.7M), the computation amount (i.e., GFLOPs) of RepVGG-CSP is accordingly smaller than RCS-YOLO. Nevertheless, RCS-YOLO still performs better in actual inference speed measured by FPS."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,4.0,Conclusion,"We developed an RCS-YOLO network for fast and accurate medical object detection, by leveraging the reparameterized convolution operator RCS based on channel shuffle in the YOLO architecture. We designed an efficient one-shot aggregation module RCS-OSA based on RCS, which serves as a computational unit in the backbone and neck of a new YOLO network. Evaluation of the brain MRI dataset shows superior performance for brain tumor detection in terms of both speed and precision, as compared to YOLOv6, YOLOv7, and YOLOv8 models."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,,YOLO-based model for fast brain tumor detection. Evaluation on a publicly available brain tumor detection annotated dataset shows superior detection accuracy and speed compared to other state-of-the-art YOLO architectures.
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_57.
Certification of Deep Learning Models for Medical Image Segmentation,1.0,Introduction,"For the past decade, deep neural networks have dominated the computer vision community and provided near human performance on many different tasks, including classification [18], segmentation [24], and image generation [16]. Given these impressive results, convolutional neural networks are now used on a daily basis in fields like healthcare, self-driving cars, and robotics, to cite a few. In medical imaging, convolutional neural networks are particularly used to segment organs or regions of interest on different modalities such as X-rays, CT scans, MRIs, or ultrasound [36]. Indeed, segmentation techniques and variations of 2D and 3D U-Nets are currently the state-of-the-art to identify and isolate tumors, blood vessels, organs, or other structures within an image and provide crucial help to physicians for medical diagnosis, screening, and prognosis [32].Nowadays, segmentation models are gaining widespread adoption in modern clinical practice and are being used with increasing frequency, making the results of these models critical for many patients. However, it is now commonly known that neural networks can be vulnerable to adversarial attacks [17,34], i.e., small input perturbations invisible to humans crafted specifically such that the network performs errors. Over the past few years, a large body of work has devised empirical defenses against adversarial attacks for classification tasks [3,17,25], as well as segmentation tasks [37], including applications on medical imaging [27]. Although state-of-the-art empirical defenses provide significant robustness, these defenses do not guarantee theoretical robustness and stronger attacks can be crafted to break them [5]. Recently, certified defenses, for classification [2,11,26] and segmentation [15,23], have been proposed to guarantee the accuracy and reliability of neural networks. However, certified defenses for segmentation in the context of medical imaging are still lacking, even if models are getting market approvals (e.g., FDA, CE) and are already adopted in clinical practice.In this paper, we provide the first method for certified robustness in the context of segmentation for medical imaging. We leverage the randomized smoothing strategy [11,15], and the recent work on diffusion models [7] to achieve stateof-the-art certified robustness for segmentation models. Randomized smoothing consists in convolving the neural network with a Gaussian distribution (i.e., by adding noise to the input) in order to obtain a smooth segmentation model. From the smoothness properties of the segmentation model, we can derive a robustness guarantee and compute a certified Dice score. We go even further by using diffusion models to first denoise the perturbed input and boost the certified robustness. By extension, we show that current diffusion models, trained on 'classical images' generalize well to medical datasets for denoising tasks. Extensive experiments on five public medical datasets of chest X-rays [21,31], skin lesions [10], and colonoscopies [6], and different popular segmentation models, prove the potential of our method. We hope that this study will provide the first step towards robustness guarantees for medical image segmentation."
Certification of Deep Learning Models for Medical Image Segmentation,2.0,Related Work,"Since the discovery of adversarial attacks [17,34], numerous defenses [8,17,25] and attacks have been devised [8,25], demonstrating that neural networks are sensitive to small input perturbation and vulnerable to attacks. Adversarial training, which has been acknowledged as one of the most successful empirical defenses, consists in training a network directly on adversarial examples [25]. However, it is now known that even strong defenses can be bypassed by adaptive attacks [12]. Paschali et al. [27] were among the first to study adversarial attacks in the context of medical imaging. They conducted experiments using several neural network architectures [20,33] (i.e., Inception V3, V4, MobileNet) and several attacks [17,25] to demonstrate that the vulnerability of neural networks is extended to medical images.More specifically, in the context of classification, a previous work [4] has analyzed the robustness of neural networks for chest X-ray images and showed that gradient-based attacks were successful in fooling both machines and humans. In a similar line of work, Yao et al. [38] proposed an add-on to known attacks that bypasses state-of-the-art adversarial detectors making current defenses even less robust. On the other hand, several works have been focused on crafting defense strategies specifically in the context of medical imaging. For example, Almalik et al. [1] proposed a self-ensembling method to enhance the robustness of Vision Transformers in the presence of adversarial attacks. In the context of segmentation in medical imaging, [30] introduced a vector quantization approach by learning a discrete representation in a low dimensional embedding space and improving the robustness of a segmentation model. Finally, Daza et al. [13] proposed a lattice architecture that segments organs and lesions on MRI and CT scans and leveraged an efficient approach of adversarial training to defend against adversarial examples.Although a large body of work has focused on constructing defenses for classification and segmentation tasks in the context of medical imaging, certified defenses are under-studied by the medical community. In this paper, we propose to leverage randomized smoothing and diffusion models for certified segmentation on medical datasets, setting the first baseline for this challenging problem and certifying popular segmentation architectures."
Certification of Deep Learning Models for Medical Image Segmentation,3.0,Randomized Smoothing,"Randomized smoothing is a model agnostic technique, proposed by Cohen et al. [11], used to improve and certify the robustness of neural networks against adversarial attacks. This method consists in adding random noise (e.g., noise generated from a Gaussian distribution) to the input data and then classifying the perturbed data using the neural network. Let D = X × Y denote the data distribution where X ⊂ R d and Y = {1, . . . , k} represent the input space and target space respectively and k is the number of classes. Let f : X → Y be a neural network such that for (x, y) ∈ D, the classifier correctly classifies ifRandomized smoothing is a procedure to construct a new smooth classifier g given any base classifier f . Let N (0, σ 2 I) be a Gaussian distribution of mean 0 and variance σ, then, the smooth classifier g is defined as follows:Cohen et al. [11] have shown that if R = σΦ -1 (g(x)) where Φ is the cumulative distribution function of the standard Gaussian distribution and R can be considered the certified radius, then, g(x + δ) = y for all δ satisfying δ 2 ≤ R.However, since it is not possible to compute g at x exactly, they proposed using Monte Carlo algorithms as an alternative approach for estimating g(x) using random sampling. In order to obtain a reliable estimate of the probability g(x), they also suggested a method that involves generating n samples of η from a normal distribution N (0, σ 2 I) and evaluating f (x + η) for each sample. The resulting counts for each class in Y are then used to estimate probability p y and the radius R with confidence 1 -α (where α is a value between 0 and 1). If the confidence level cannot be achieved (for example, due to insufficient samples), the method will abstain from providing an estimate. More recently, Fischer et al. [15] built upon the work of [11] by introducing SegCertify, the first certified approach for image segmentation. The segmentation process involves assigning a segmentation class to every pixel in the image, which can be viewed as a form of classification at the pixel level. In the segmentation settings, the output space consists of regions or categories to be segmented, such as cars, roads, pedestrians, etc. The classifier function f : X → Y d determines the class for each pixel and categorizes each component independently. In this context, the certification algorithm proposed by Cohen et al. [11] can be extended to accommodate the segmentation task.To obtain a smooth classifier, it is necessary to add random noise to the input of the classifier. However, this creates a trade-off between accuracy and robustness. If low variance noise is added, accuracy won't be impacted significantly, but the certified radius will remain low. Conversely, adding high variance noise can improve certificates but at the expense of accuracy. To address this issue, Cohen et al. proposed a simple trick of training the network with noise injection during the training phase. While this method may reduce accuracy when evaluating the classifier with noise during the certification process, it can also help mitigate the trade-off between accuracy and robustness. One can note that during training, the network's objective is to learn to ignore the noise and classify at the same time. To improve the natural as well as the certified accuracy, Salman et al. [29] proposed to separate the two tasks with two networks trained separately. First, a network, h : X → X , is trained to denoise the data such that for η ∼ N (0, σ 2 I), we have h(x + η) ≈ x, then, the output of the denoiser is given to the classifier.In this paper, we leverage randomized smoothing and diffusion probabilistic models to obtain state-of-the-art results on certified segmentation for medical imaging. To the best of our knowledge, we are the first to propose a comprehensive study on certified segmentation for medical imaging."
Certification of Deep Learning Models for Medical Image Segmentation,4.0,Diffusion Probabilistic Models for Certification,"The training of a Denoising Diffusion Probabilistic Model (DDPM) is an iterative process that involves adding a small amount of noise at every step of the diffusion process until random noise is reached. The reverse process then starts from random noise and generates a new image that conforms to the data distribution. Since DDPMs are inherently iterative denoising models, we can leverage this property for randomized smoothing. The idea would be to start the reverse process with a noisy image, rather than Gaussian noise, enabling the DDPM to output an image that resembles the original image.Similar to Carlini et al. [7], our proposed pipeline is composed of two main steps: we denoise, then we certify. In order to complete the denoising, we need to first map between the noise model utilized in diffusion models and the one used in randomized smoothing. Randomized smoothing needs a data point that is enhanced with Gaussian noise added to it, given by x rs = x + δ with δ ∼ N (x, σ 2 I). On the other hand, diffusion models suppose the noise model for Since randomized smoothing is applied to each pixel separately with a probability of 1 -α, considering the entire segmentation region would imply considering a union bound with significantly reduced confidence. Similar to Fischer et al. [15], we leverage the Holm-Bonferroni method [19] and perform multipletesting corrections. For each image, we repeat this process n = 100 times, identifying pixels on which the model abstains, and computing the certified scores. We extend the work of Fischer et al. [15] to also compute a certified Dice score that is calculated ignoring the abstain class ( ). Our approach has a significant advantage compared to SegCertify since it leverages off-the-shelf and stateof-the-art pre-trained denoisers and segmentation models. SegCertify, on the other hand, relies on models trained with Gaussian noise."
Certification of Deep Learning Models for Medical Image Segmentation,5.0,Experiments and Results,"Datasets: We perform experiments on 5 different publicly available datasets. All datasets were divided to 70% for training, 10% for validation, and 20% for testing. The testing set is the one used to compute certified results.Chest X-rays Datasets: JSRT dataset [31] with annotations of lung, heart, and clavicles provided by [35] is used. This dataset contains 247 images. For lung segmentation only, we use both the Montgomery and Shenzen datasets [21]. Montgomery consists of 138 and Shenzen of 662 annotated images.Skin Lesion: Skin images with their annotations provided by the ISIC 2018 boundary segmentation challenge [10] were used. This dataset consists of 2694 RGB dermatoscopy images. Colonoscopy Images: CVC-ClinicDB dataset [6] containing 612 colonoscopy images in RGB together with their annotations were utilized.Implementation Details: We train three different segmentation models namely, a UNet [28], a ResUNet++ [22], and a DeeplabV2 [9] with and without noise. The models trained without noise are used exclusively with our method. The models trained with a Gaussian noise of 0.25 are used to compute SegCertify scores. All 6 models use an image input size of 512 × 512 for X-ray images, 384×512 for skin lesions, and 288×384 for colonoscopy. As a denoiser, we use an off-the-shelf denoising diffusion probabilistic model provided by [14]. We perform our experiments with the 256 × 256 class unconditional denoiser with a linear scheduler and without timestep respacing. For each noise level, our method follows the steps described in the previous section and uses n 0 = 10, n=100 for each image, and α = 0.001, and τ = 0.75. Our code is made publicly available at: https://github.com/othmanela/medical_cert_seg.Results and Discussion: For all five datasets, we compute a certified Dice score and certified mean Intersection over Union (IoU). We also report the percentage of abstentions (% ) representing the mean number of pixels on which the model's prediction confidence was insufficient with respect to the radius R.The lower the percentage of abstentions the better the segmentation model is.In Table 1, we compare our method using 3 different and popular architectures (UNet, ResUNet++, and DeeplabV2) on the chest X-rays datasets. We  S2 of the supplementary material. Overall, for both methods, ResUNet++ is the most robust architecture followed by UNet and then DeeplabV2 for all σ and R combinations. Moreover, certified metrics for lungs and heart remain high for our method, even with high levels of noise. However, the increasing level of noise affects the clavicles since these are smaller structures. A comparison of our method and SegCertify using the ResUNet++ architecture is presented in Table 2 for the three chest X-ray datasets. We observe that we outperform SegCertify, especially for high sigma values. For σ = 0.25, SegCertify performs slightly better. This is due to the fact that the model used with SegCertify is trained with a noise level of 0.25. The main drawback however is that its Dice on unperturbed images drops considerably (e.g., from 0.96 to 0.91 on lung segmentation). On the other hand, our pipeline does not require training a segmentation model with noise or even a denoising model. Our methodology relies only on off-the-shelf models. For the highest noise level of σ = 1.0, we notice that the certified Dice and IoU with SegCertify both drop to 0 whereas our proposed method is able to maintain high certified scores.Qualitative results are provided in Fig. 1 for our proposed method and SegCertify for the different datasets and different levels of noise. Regarding the structures to segment, we notice that the abstentions around the clavicles (the smallest benchmarked region of interest on chest X-rays) get bigger. We also notice that the fine segmentation boundaries (e.g., area around the skin lesion) may not be as sharp after denoising. As we increase the noise, the decision boundary is harder to find for all models. This may be due to the fact that fine details on the image are lost after the denoising step. However, our method is still able to segment the large majority of pixels properly on the image, contrary to its competitor, especially for high noise levels (third row on chest X-rays).Table 3 reports certified segmentation results for skin lesions and colonoscopy on both techniques. We notice that our method is still performing better than  SegCertify. This supports our claim that DDPMs generalize quite well to medical images and that harnessing their potential boosts the state-of-the-art.Regarding the denoiser, we used a single-step denoising strategy, i.e., we perform a single call to the DDPM to compute the denoised image from t * to t = 0. Another strategy could be to iteratively denoise from t * , t * -1, ... until t = 0. However, this implies predicting a denoised image multiple times and in the end, may result in images with unwanted artifacts. We perform multi-step denoising experiments and report results in Table S1 of the supplementary material. We note that the single-step denoising performs best since it relies more on the denoising power of DDPMs rather than their generative capabilities, and is also faster than the multi-step approach. Finally, we perform a comparison with another denoiser architecture. We train three UNet models (one for each noise level) on the JSRT dataset. We report results in Table S3 and notice that even with custom-trained denoisers, the DDPM outperforms the UNet denoising architecture. A comparison of denoised images is provided in Figure S1. We notice that the DDPM is able to keep high-fidelity images compared to the UNet and is therefore more relevant for certified medical image segmentation."
Certification of Deep Learning Models for Medical Image Segmentation,6.0,Conclusion,"In this paper, we present the first work on certified segmentation for medical imaging, and extensively evaluate it on five different datasets and three deep learning segmentation models. Our technique leverages off-the-shelf denoising and segmentation models and provides the highest certified Dice and mIoU on multi-class and binary segmentation of five different datasets. With that, we are able to remove the overhead of having to train and fine-tune models specifically for robustness. This paradigm shift alleviates the dilemma of having to choose between highly accurate segmentation models or models robust to attacks. We hope that this work serves as a baseline for the unexplored yet critical topic of certified segmentation in medical imaging. Future work will involve extending our approach to 3D medical imaging modalities as well as exploring the realm of certified classification."
Certification of Deep Learning Models for Medical Image Segmentation,,,"* (x + δ), δ ∼ N (0, σ 2 I). * to t = 0. A multi-step denoising strategy implies iteratively predicting all images at t * , t * -1, . . . until t = 0. Both techniques are explored in the next section and supplementary material."
Certification of Deep Learning Models for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_58.
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Multi-site semi-supervised learning (MS-SSL),"The unlabeled image pool can be quickly enriched via the support from partner clinical centers with low barriers of entry (only unlabeled images are required) Data heterogeneity due to different scanners, scanning protocols and subject groups, which violate the typical SSL assumption of i.  "
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"Prostate segmentation from magnetic resonance imaging (MRI) is a crucial step for diagnosis and treatment planning of prostate cancer. Recently, deep learningbased approaches have greatly improved the accuracy and efficiency of automatic prostate MRI segmentation [7,8]. Yet, their success usually requires a large amount of labeled medical data, which is expensive and expertise-demanding in practice. In this regard, semi-supervised learning (SSL) has emerged as an attractive option as it can leverage both limited labeled data and abundant unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. Nevertheless, the effectiveness of SSL is heavily dependent on the quantity and quality of the unlabeled data.Regarding quantity , the abundance of unlabeled data serves as a way to regularize the model and alleviate overfitting to the limited labeled data. Unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the local unlabeled pool is also limited due to restricted image collection capabilities or scarce patient samples. As a specific case shown in Table 1, there are only limited prostate scans available per center. Taking C1 as a case study, if the amount of local unlabeled data is limited, existing SSL methods may still suffer from inferior performance when generalizing to unseen test data (Fig. 1). To efficiently enrich the unlabeled pool, seeking support from other centers is a viable solution, as illustrated in Fig. 1. Yet, due to differences in imaging protocols and variations in patient demographics, this solution usually introduces data heterogeneity, lead-ing to a quality problem. Such heterogeneity may impede the performance of SSL which typically assumes that the distributions of labeled data and unlabeled data are independent and identically distributed (i.i.d.) [16]. Thus, proper mechanisms are called for this practical but challenging SSL scenario.Here, we define this new SSL scenario as multi-site semi-supervised learning (MS-SSL), allowing to enrich the unlabeled pool with multi-site heterogeneous images. Being an under-explored scenario, few efforts have been made. To our best knowledge, the most relevant work is AHDC [2]. However, it only deals with additional unlabeled data from a specific source rather than multiple arbitrary sources. Thus, it intuitively utilizes image-level mapping to minimize dual-distribution discrepancy. Yet, their adversarial min-max optimization often leads to instability and it is difficult to align multiple external sources with the local source using a single image mapping network.In this work, we propose a more generalized framework called Categorylevel regularized Unlabeled-to-Labeled (CU2L) learning, as depicted in Fig. 2, to achieve robust MS-SSL for prostate MRI segmentation. Specifically, CU2L is built upon the teacher-student architecture with customized learning strategies for local and external unlabeled data: (i) recognizing the importance of supervised learning in data distribution fitting (which leads to the failure of CPS [3] in MS-SSL as elaborated in Sec. 3), the local unlabeled data is involved into pseudolabel supervised-like learning to reinforce fitting of the local data distribution; (ii) considering that intra-class variance hinders effective MS-SSL, we introduce a non-parametric unlabeled-to-labeled learning scheme, which takes advantage of the scarce expert labels to explicitly constrain the prototype-propagated predictions, to help the model exploit discriminative and domain-insensitive features from heterogeneous multi-site data to support the local center. Yet, observing that such scheme is challenging when significant shifts and various distributions are present, we further propose category-level regularization, which advocates prototype alignment, to regularize the distribution of intra-class features from arbitrary external data to be closer to the local distribution; (iii) based on the fact that perturbations (e.g., Gaussian noises [15]) can be regarded as a simulation of heterogeneity, perturbed stability learning is incorporated to enhance the robustness of the model. Our method is evaluated on prostate MRI data from six different clinical centers and shows promising performance on tackling MS-SSL compared to other semi-supervised methods."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.1,Problem Formulation and Basic Architecture,"In our scenario of MS-SSL, we have access to a local target dataset D local (consisted of a labeled sub-set D l local and an unlabeled sub-set D u local ) and the external unlabeled support datasets D u e = m j=1 D u,j e , where m is the number of support centers. Specifically,with n u unlabeled scans. with n j unlabeled samples. Considering the large variance on slice thickness among different centers [7,8], our experiments are performed in 2D. Thus, we refer to pixels in the subsequent content. As shown in Fig. 2, our framework is built upon the popular teacher-student framework. Specifically, the student f s θ is an in-training model optimized by loss back-propagation as usual while the teacher model f t θ is slowly updated with a momentum term that averages previous weights with the current weights, where θ denotes the student's weights and θ the teacher's weights. θ is updated by θt = α θt-1 + (1α)θ t at iteration t, where α is the exponential moving average (EMA) coefficient and empirically set to 0.99 [26]. Compared to the student, the teacher performs self-ensembling by nature which helps smooth out the noise and avoid sudden changes of predictions [15]. Thus, the teacher model is suitable for handling the heterogeneous external images and producing relatively stable pseudo labels (will be used later). As such, our task of MS-SSL can be formulated as optimizing the following loss:where L l sup is the supervised guidance from local labeled data and L u denotes the additional guidance from the unlabeled data. λ is a trade-off weight scheduled by the time-dependent ramp-up Gaussian function [15] where w max and t max are the maximal weight and iteration, respectively. The key challenge of MS-SSL is the proper design of L u for robustly exploiting multi-site unlabeled data {D u local , D u e } to support the local center."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.2,Pseudo Labeling for Local Distribution Fitting,"As mentioned above, supervised-like learning is advocated for local unlabeled data to help the model fit local distribution better. Owning the self-ensembling property, the teacher model provides relatively stable pseudo labels for the student model. Given the predicted probability map P u,t local of X u local from the teacher model, the pseudo label Ŷ u,t local corresponds to the class with the maximal posterior probability. Yet, with limited local labeled data for training, it is difficult to generate high-quality pseudo labels. Thus, for each pixel, if max c (p u,t local ) ≥ δ, where c denotes the c-th class and δ is a ramp-up threshold ranging from 0.75 to 0.9 as training goes, this pixel will be included in loss calculation. Considering that the cross-entropy loss has been found very sensitive to label noises [18], we adopt the partial Dice loss L Dice [27] to perform pseudo label learning, formulated as:, where P u,s local denotes the prediction of X u local from the student model. The Dice loss is calculated for each of the K equally-sized regions of the image, and the final loss is obtained by taking their mean. Such a regional form [6] can help the model better perceive the local discrepancies for fine-grained learning."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.3,Category-Level Regularized Unlabeled-to-Labeled Learning,"Unlabeled-to-Labeled Learning. Inherently, the challenge of MS-SSL stems from intra-class variation, which results from different imaging protocols, disease progress and patient demographics. Inspired by prototypical networks [13,19,25] that compare class prototypes with pixel features to perform segmentation, here, we introduce a non-parametric unlabeled-to-labeled (U2L) learning scheme that utilizes expert labels to explicitly constrain the prototype-propagated predictions. Such design is based on two considerations: (i) a good prototypepropagated prediction requires both compact feature and discriminative prototypes, thus enhancing this prediction can encourage the model to learn in a variation-insensitive manner and focus on the most informative clues; (ii) using expert labels as final guidance can prevent error propagation from pseudo labels. Specifically, we denote the feature map of the external unlabeled image X u e before the penultimate convolution in the teacher model as F u,t e . Note that F u,t e has been upsampled to the same size of X u e via bilinear interpolation but with L channels. With the argmax pseudo label Ŷ u,t e and the predicted probability map P u,t e , the object prototype from the external unlabeled data can be computed via confidence-weighted masked average pooling: c.Likewise, the background prototype c u(bg) e can also be obtained. Considering the possible unbalanced sampling of prostate-containing slices, EMA strategy across training steps (with a decay rate of 0.9) is applied for prototype update. Then, as shown in Fig. 2 , where we use cosine similarity for sim(•, •) and empirically set the temperature T to 0.05 [19]. Note that a similar procedure can also be applied to the local unlabeled data X u local , and thus we can obtain another prototype-propagated unlabeledto-labeled prediction P u2l local for X l local . As such, given the accurate expert label Y l local , the unlabeled-to-labeled supervision can be computed as:Category-Level Regularization. Being a challenging scheme itself, the above U2L learning can only handle minor intra-class variation. Thus, proper mechanisms are needed to alleviate the negative impact of significant shift and multiple distributions. Specifically, we introduce category-level regularization, which advocates class prototype alignment between local and external data, to regularize the distribution of intra-class features from arbitrary external data to be closer to the local one, thus reducing the difficulty of U2L learning. In U2L, we have obtained prototypes from local unlabeled data {c where mean squared error is adopted as the distance function d(•, •). The weight of background prototype alignment is smaller due to less relevant contexts."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Stability Under Perturbations.,"Although originally designed for typical SSL, encouraging stability under perturbations [26] can also benefit MS-SSL, considering that the perturbations can be regarded as a simulation of heterogeneity and enforcing such perturbed stability can regularize the model behavior for better generalizability. Specifically, for the same unlabeled input X u ∈ {D u local ∪ D u e } with different perturbations ξ and ξ (using the same Gaussian noises as in [26]), we encourage consistent pre-softmax predictions between the teacher and student models, formulated aswhere mean squared error is also adopted as the distance function d(•, •).Overall, the final loss for the multi-site unlabeled data is summarized as:"
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3.0,Experiments and Results,"Materials. We utilize prostate T2-weighted MR images from six different clinical centers (C1-6) [1,4,5] to perform a retrospective evaluation.  rizes the characteristics of the six data sources, following [7,8], where [7,8] also reveal the severity of inter-center heterogeneity here through extensive experiments. The heterogeneity comes from the differences in scanners, field strengths, coil types, disease and in-plane/through-plane resolution. Compared to C1 and C2, scans from C3 to C6 are taken from patients with prostate cancer, either for detection or staging purposes, which can cause inherent semantic differences in the prostate region to further aggravate heterogeneity. Following [7,8], we crop each scan to preserve the slices with the prostate region only and then resize and normalize it to 384 × 384 px in the axial plane with zero mean and unit variance. We take C1 or C2 as the local target center and randomly divide their 30 scans into 18, 3, and 9 samples as training, validation, and test sets, respectively.Implementation and Evaluation Metrics. The framework is implemented on PyTorch using an NVIDIA GeForce RTX 3090 GPU. Considering the large variance in slice thickness among different centers, we adopt the 2D architecture. Specifically, 2D U-Net [12] is adopted as our backbone.  consists of the cross-entropy loss and the K-regional Dice loss [6]. The maximum consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. K is empirically set to 2. The network is trained using the SGD optimizer and the learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t max ) 0.9 . Data augmentation is applied, including random flip and rotation. We adopt the Dice similarity coefficient (DSC) and Jaccard as the evaluation metrics and the results are the average over three runs with different seeds.Comparison Study. Table 2 presents the quantitative results with either C1 or C2 as the local target center, wherein only 6 or 8 local scans are annotated. Besides the supervised-only baselines, we include recent top-performing SSL methods [2,3,11,14,15,17,20,25,26] for comparison. All methods are implemented with the same backbone and training protocols to ensure fairness. As observed, compared to the supervised-only baselines, our CU2L with {6, 8} local labeled scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} DSC improvements in {C1, C2}, showing its effectiveness in leveraging multi-site unlabeled data. Despite the violation of the assumption of i.i.d. data, existing SSL methods can still benefit from the external unlabeled data to some extent compared to the results using local data only as shown in Fig. 1, revealing that the quantity of unlabeled data has a significant impact. However, due to the lack of proper mechanisms for learning from heterogeneous data, limited improvement can be achieved by them, especially for CPS [3] and FixMatch [14] in C2. Particularly, CPS relies on cross-modal pseudo labeling which exploits all the unlabeled data in a supervised-like fashion. We attribute its degradation to the fact that supervised learning is crucial for distribution fitting, which supports our motivation of performing pseudo-label learning on local unlabeled data only. As a result, its models struggle to determine which distribution to prioritize. Meanwhile, the most relevant AHDC [2] is mediocre in MS-SSL, mainly due to the instability of adversarial training and the difficulty of aligning multiple distributions to the local distribution via a single image-mapping network. In contrast, with specialized mechanisms for simultaneously learning informative representations from multi-site data and handling heterogeneity, our CU2L obtains the best performance over the recent SSL methods. Figure 3(a) further shows that the predictions of our method fit more accurately with the ground truth.Ablation Study. To evaluate the effectiveness of each component, we conduct an ablation study under the setting with 6 local labeled scans, as shown in Fig. 2(b). Firstly, when we remove L u P L (CU2L-1), the performance drops by {5.69% (C1), 3.05%(C2)} in DSC, showing that reinforcing confirmation on local distribution is critical. CU2L-2 represents the removal of both L u2l and L cr , and it can be observed that such an unlabeled-to-labeled learning approach combined with class-level regularization is crucial for exploring multi-site data. If we remove L cr which accompanies with L u2l (CU2L-3), the performance degrades, which justifies the necessity of this regularization to reduce the difficulty of unlabeled-to-labeled learning process. CU2L-4 denotes the removal of L u sta . As observed, such a typical stability loss [15] can further improve the performance by introducing hand-crafted noises to enhance the robustness to real-world heterogeneity."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,4.0,Conclusion,"In this work, we presented a novel Category-level regularized Unlabeled-to-Labeled (CU2L) learning framework for semi-supervised prostate segmentation with multi-site unlabeled MRI data. CU2L robustly exploits multi-site unlabeled data via three tailored schemes: local pseudo-label learning for better local distribution fitting, category-level regularized unlabeled-to-labeled learning for exploiting the external data in a distribution-insensitive manner and stability learning for further enhancing robustness to heterogeneity. We evaluated our method on prostate MRI data from six different clinical centers and demonstrated its superior performance compared to other semi-supervised methods."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Table 1 .,Center Source#Scans Field strength (T) Resolution (in-plane/through-plane in mm) Coil
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Table 1,summa-
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,1.0,Introduction,"Radiology plays an important role in the diagnosis of some pulmonary infectious diseases, such as the COVID-19 pneumonia outbreak in late 2019 [1]. With the development of deep learning, deep neural networks are more and more used to process radiological images for assisted diagnosis, such as disease classification, lesion detection and segmentation, etc. With the fast processing of radiological images by deep neural networks, some diagnoses can be obtained immediately, such as the classification of bacterial or viral pneumonia and the segmentation mask for pulmonary infections, which is important for quantifying the severity of the disease as well as its progression [2]. Besides, these diagnoses given by the AI allow doctors to predict risks and prognostics in a ""patient-specific"" way [3]. Radiologists usually take more time to complete lesion annotation than AI, and annotation results can be influenced by individual bias and clinical experience [4].Ariadne's thread, the name comes from ancient Greek myth, tells of Theseus walking out of the labyrinth with the help of Ariadne's golden thread.Therefore, it is of importance to design automatic medical image segmentation algorithms to assist clinicians in developing accurate and fast treatment plans.Most of the biomedical segmentation methods [5][6][7][8][9] are improved based on U-Net [10]. However, the performance of these image-only methods is constrained by the training data, which is also a dilemma in the medical image field. Radford et al. proposed CLIP [11] in 2021, where they used 4M image-text pairs for contrastive learning. With the rise of multi-modal learning in the recent years, there are also methods [12][13][14][15][16] that focus on vision-language pretraining/processing and applying them on local tasks. Li et al. proposed a language-driven medical image segmentation method LViT [16], using a hybrid CNN-Transformer structure to fuse text and image features. However, LViT uses an early fusion approach and the information contained in the text is not well represented. In this paper, we propose a multi-modal segmentation method that using independent text encoder and image encoder, and design a GuideDecoder to fuse the features of both modalities at decoding stage. Our main contributions are summarized as follow:-We propose a language-driven segmentation method for segmenting infected areas from lung x-ray images. Source code of our method see: https://github.com/Junelin2333/LanGuideMedSeg-MICCAI2023 -The designed GuideDecoder in our method can adaptively propagate sufficient semantic information of the text prompts into pixel-level visual features, promoting consistency between two modalities. -We have cleaned the errors contained in the text annotations of QaTa-COV19 [17] and contacted the authors of LViT to release a new version. -Our extended study reveals the impact of information granularity in text prompts on the segmentation performance of our method, and demonstrates the significant advantage of multi-modal method over image-only methods in terms of the size of training data required."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,2.0,Method,"The overview of our proposed method is shown in Fig. 1(a). The model consists of three main components: Image Encoder, Text Encoder and GuideDecoder that enables multi-modal information fusion. As you can see, our proposed method uses a modular design. Compared to early stage fusion in LViT, our proposed method in modular design is more flexible. For example, when our method is used for brain MRI images, thanks to the modular design, we could first load pre-trained weights trained on the corresponding data to separate visual and text encoders, and then only need to train GuideDecoders.Visual Encoder and Text Encoder. The Visual Encoder used in the model is ConvNeXt-Tiny [18]. For an input image I ∈ R H×W ×1 , we extract multiple visual features from the four stages of ConvNeXt-Tiny, which are defined as Note that C is the feature dimension, H and W are the height and width of the original image. For an input text prompt T ∈ R L , We adopt the CXR-BERT [19] to extract text features g t ∈ R L×C . Note that C is the feature dimension, L is the length of the text prompt.GuideDecoder. Due to our modular design, visual features and textual features are encoded independently by different encoders. Therefore, the design of the decoder is particularly important, as we can only fuse multi-modal features from different encoders in post stage. The structure of GuideDecoder is shown in Fig. 1(b). The GuideDecoder first processes the input textual features and visual features before performing multi-modal interaction.The input textual features first go through a projection module (i.e. Project in the figure) that aligns the dimensionality of the text token with that of the image token and reduces the number of text tokens. The projection process is shown in Eq. 1.where W T is a learnable matrix, Conv(•) denotes a 1 × 1 convolution layer, and σ(•) denotes the ReLU activation function. Given an input feature T ∈ R L×D , the output projected features is, where M is the number of tokens after projection and C 1 is the dimension of the projected features, consistent with the dimension of the image token.For the input visual features I ∈ R H×W ×C1 , after adding the position encoding we use self-attention to enhance the visual information in them to obtain the evolved visual features. The process is shown in Eq. 2.where MHSA(•) denotes Multi-Head Self-Attention layer, LN (•) denotes Layer Normalization, and finally the evolved visual features f i ∈ R H×W ×C1 with residuals could be obtained.After those, the multi-head cross-attention layer is adopted to propagate fine-grained semantic information into the evolved image features. To obtain the multi-modal feature f c ∈ R H×W ×C1 , the output further computed by layer normalization and residual connection:where MHCA(•) denotes multi-head cross-attention and α is a learnable parameter to control the weight of the residual connection.Then, the multi-modal feature f c ∈ R (H×W )×C1 would be reshaped and upsampling to obtain f c ∈ R H ×W ×C1 . Finally the f c is concatenated with f s ∈ R H ×W ×C2 on the channel dimension, where f s is the low-level visual feature obtained from visual encoder via skip connection. The concatenated features are processed through a convolution layer and a ReLU activation function to obtain the final decoded outputwhere [•, •] represents the concatenate operation on the channel dimension."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.1,Dataset,"The dataset used to evaluate our method performance is the QaTa-COV19 dataset [17], which is compiled by researchers from Qatar University and Tampere University. It consists of 9258 COVID-19 chest radiographs with pixel-level manual annotations of infected lung areas, of which 7145 are in the training set and 2113 in the test set. However, the original QaTa-COV19 dataset does not contain any matched text annotations. Li et al. [16] have made significant contributions by extending the text annotations of the dataset, their endeavors are worthy of commendation. We conducted a revisitation of the text annotations and found several notable features. Each sentence consists of three parts, containing position information at different granularity. However, these sentences cannot be considered as medical reports for lacking descriptions of the disease, we consider them as a kind of ""text prompt"" just as the title of the paper states.Besides, we found some obvious errors (e.g. misspelled words, grammatical errors and unclear referents) in the extended text annotations. We have fixed these identified errors and contacted the authors of LViT to release a new version of the dataset. Dataset see Github link: https://github.com/HUANGLIZI/LViT."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.2,Experiment Settings,"Following the file name of the subjects in the original train set, we split the training set and the validation set uniformly in the ratio of 80% and 20%. Therefore, the training set has a total of 5716 samples, the validation set has 1429 samples and the test set has 2113 samples. All images are cropped to 224 × 224 and the data is augmented using a random zoom with 10% probability.We used a number of open source libraries including but not limited to PyTorch, MONAI [20] and Transformers [21] to implement our method and baseline approach. We use PyTorch Lightning for the final training and inference wrapper. All the methods are training on one NVIDIA Tesla V100 SXM3 32 GB VRAM GPU. We use the Dice loss plus Cross-entropy loss as the loss function, and train the network using AdamW optimization with a batch size of 32. We utilize the cosine annealing learning rate policy, the initial learning rate is set to 3e-4 and the minimal learning rate is set to 1e-6.We used three metrics to evaluate the segmentation results objectively: Accuracy, Dice coefficient and Jaccard coefficient. Both Dice and Jaccard coefficient calculate the intersection regions over the union regions of the given predicted mask and ground truth, where the Dice coefficient is more indicative of the segmentation performance of small targets."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.3,Comparison Experiments,"We compared our method with common mono-modal medical image segmentation methods and with the LViT previously proposed by Li et al. The quantitative results of the experiment are shown in Table 1. UNet++ achieves the best performance of the mono-modal approach. Comparing to UNet++, our method improves accuracy by 1.44%, Dice score by 6.09% and Jaccard score by 9.49%. Our method improves accuracy by 1.28%, Dice score by 4.86% and Jaccard coefficient by 7.66% compared to the previous multi-modal method LViT. In general, using text prompts could significantly improve segmentation performance. The results of the qualitative experiment are shown in Fig. 2. The imageonly mono-modal methods tend to generate some over-segmentation, while the multi-modal approach refers to the specific location of the infected region through text prompts to make the segmentation results more accurate."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.4,Ablation Study,"Our proposed method introduces semantic information of text in the decoding process of image features and designs the GuideDecoder to let the semantic information in the text guide the generation of the final segmentation mask. We performed an ablation study on the number of GuideDecoder used in the model and the results are shown in the Table 2.Table 2. Ablation studies on QaTa-COV19 test set. We used different numbers (0-3) of GuideDecoders in the model to verify the effectiveness of the GuideDecoder. Note that the GuideDecoder in the model is replaced in turn by the Decoder in the UNet, 'w/o text' means without text and the model use UNet Decoders only."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Method Acc,"Dice Jaccard w/o text 0.9610 0.8414 0.7262 1 layer 0.9735 0.8920 0.8050 2 layers 0.9748 0.8963 0.8132 3 layers 0.9752 0.8978 0.8144As can be seen from the Table 2, the segmentation performance of the model improves as the number of GuideDecoders used in the model increases. The effectiveness of GuideDecoder could be proved by these results."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.5,Extended Study,"Considering the application of the algorithm in clinical scenarios, we conducted several interesting extension studies based on the QaTa-COV19 dataset with the text annotations. It is worth mentioning that the following extended studies were carried out on our proposed method."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Impact of Text Prompts at Different Granularity on Segmentation,"Performance. In Sect. 3.1 we mention that each sample is extended to a text annotation with three parts containing positional information at different granularity, as shown in the Fig. 3. Therefore we further explored the impact of text prompts at different granularity on segmentation performance of our method and the results are shown in Table 3. The results in the table show that the segmentation performance of our proposed method is driven by the granularity of the position information contained in the text prompt. Our proposed method achieved better segmentation performance when given a text prompt with more detailed position information. Meanwhile, we observed that the performance of our method is almost identical when using two types of text prompts, i.e. Stage3 alone and Stage1 + Stage2 + Stage3. It means the most detailed position information in the text prompt plays the most significant role in improving segmentation performance. But this does not mean that other granularity of position information in the text prompt does not contribute to the improvement in segmentation performance. Even when the input text prompts contain only the coarsest location information (Stage1 + Stage2 items in the Table 3), our proposed method yielded a 1.43% higher Dice score than the method without text prompt."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Impact of the Size of Training Data on Segmentation Performance.,"As shown in Table 4, our proposed method demonstrates highly competitive performance even with a reduced amount of training data. With only a quarter of the training data, our proposed method achieves a 2.69% higher Dice score than UNet++, which is the best performing mono-modal model trained on the full dataset. This provides sufficient evidence for the superiority of multimodal approaches and the fact that suitable text prompts could significantly help improve the segmentation performance. We observed that when the training data was reduced to 10%, our method only began to exhibit inferior performance compared to UNet++, which was trained with all available data. Similar experiments could be found in the LViT paper. Therefore, it can be argued that multi-modal approaches require only a small amount of data (less than 15% in the case of our method) to achieve performance equivalent to that of mono-modal methods."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,4.0,Conclusion,"In this paper, we propose a language-driven method for segmenting infected areas from lung x-ray images. The designed GuideDecoder in our method can adaptively propagate sufficient semantic information of the text prompts into pixel-level visual features, promoting consistency between two modalities. The experimental results on the QaTa-COV19 dataset indicate that the multi-modal segmentation method based on text-image could achieve better performance compared to the image-only segmentation methods. Besides, we have conducted several extended studies on the information granularity of the text prompts and the size of the training data, which reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,1.0,Introduction,"Segmenting the prostate anatomy and detecting tumors is essential for both diagnostic and treatment planning purposes. Hence, the task of developing domain generalisable prostate MRI segmentation models is essential for the safe translation of these models into clinical practice. Deep learning models are susceptible to textural shifts and artefacts which is often seen in MRI due to variations in the complex acquisition protocols across multiple sites [12].The most common approach to tackle domain shifts is with data augmentation [16,33,35] and adversarial training [11,30]. However, this increases training time and we propose to tackle the problem head on by learning shape only embedding features to build a shape dictionary using vector quantisation [31] which can be sampled to compose the segmentation output. We therefore hypothesise by limiting the search space to a set of shape components, we can improve generalisability of a segmentation model. We also propose to correctly sample and compose shape components with local and global topological constraints by tracking topological features as we compose the shape components in an ordered manner. This is achieved using a branch of algebraic topology called cellular sheaf theory [8,19]. We hypothesise this approach will produce more anatomically meaningful segmentation maps and improve tumour localisation.The contributions of this paper are summarized as follows: 1. This work considers shape compositionality to enhance the generalisability of deep learning models to segment the prostate on MRI. 2. We use cellular sheaves to aid compositionality for segmentation as well as improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,2.1,Persistent Homology,"Topological data analysis is a field which extracts topological features from complex data structures embedded in a topological space. One can describe a topological space through its connectivity which can be captured in many forms. One such form is the cubical complex. The cubical complex C is naturally equipped to deal with topological spaces represented as volumetric grid structured data such as images [32]. In a 3D image, a cubical complex consists of individual voxels serving as vertices, with information regarding their connections to neighboring voxels captured through edges, squares, and cubes. Matrix reduction algorithms enable us to represent the connectivity of C in terms of a series of mathematical groups, known as the homology groups. Each homology group encompasses a specific dimension, d of topological features, such as connected components (d = 0), holes (d = 1), and voids (d = 2). The number of topological features present in each group is quantified by the corresponding Betti number. Betti numbers provide useful topological descriptors of the binary label maps as it is a single scale topological descriptor. However, the output, Y from a segmentation model is continuous. Thus, the Betti number for a cubical complex where vertices are continuous will be a noisy topological descriptor. We therefore use persistent homology which tracks changes in the topological features at multiple scales [17]. A cubical complex can be constructed at some threshold, τ over the output defined as: C τ = {y ∈ Y|Y ≥ τ }. We can now create q cubical complexes over q ordered thresholds. This leads to a sequence of nested cubical complexes shown in Eq. 1 known as a sublevel set filtration. The persistent homology defines d dimensional topological features such as connected components which are born at τ i and dies at τ j where τ j > τ i . This creates tuples (τ i , τ j ) which are stored as points in a persistence diagram (Fig. 2b)."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,2.2,Cellular Sheaves,"The homology of segmentation maps provides a useful tool for analysing global topology but does not describe how local topology is related to construct global topological features. Sheaf theory provides a way of composing or 'gluing' local data together to build a global object (new data) that is consistent with the local information [8]. This lends well to modelling compositionality. Formally, a sheaf is a mathematical object which attaches to each open subset or subspace, U in a topological space, Y an algebraic object like a vector space or set (local data) such that it is well-behaved under restriction to smaller open sets [8].We can consider a topological space, Y such as a segmentation output divided into a finite number of subspaces, {∅, Y 1 , Y 2 ...Y n } which are the base spaces for Y or equivalently the patches in a segmentation map. If we sequentially glue base spaces together in a certain order to form increasingly larger subspaces of Y starting with the ∅, one can construct a filtration of Y such that;We neatly formalise the subspaces and how subspaces are glued together with a poset. A poset (P ) is a partially ordered set defined by a relation, ≤ between elements in P which is reflexive, antisymmetric, and transitive [19]. In our work, we define a poset by the inclusion relation; p i ≤ p j implies p i ⊆ p j for p i , p j ∈ P . Hence, we can map each element in P with a subspace in X which satisfies the inclusion relations in P like in X.A cellular sheaf, F over a poset is constructed by mapping, each element, p ∈ P to a vector space F(p) over a fixed field which preserves the ordering in P by linear transformations, ρ .,. which are inclusion maps in this case [19]. In our work each element in P maps to the vector space, R 2 which preserves the inclusion relations in P . Specifically, we compute a persistence diagram, D for the subspace in X associated (homeomorphic) with p ∈ P whereby (τ i , τ j ) in the persistence diagram are a set of vectors in the vector space, R 2 . A cellular sheaf naturally arises in modelling the connectivity of a segmentation map and provides a mathematically precise justification for using cellular sheaves in our method. We show by approaching the composition of segmentation maps through this lens, one can significantly improved the robustness of segmentation models."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,3.0,Related Work,"There have been various deep learning based architectures developed for prostate tumour segmentation [3,15,18]. There is however no work looking at developing models which generalise well to target domains after training on one source domain known as single domain generalisation (SDG). Effective data augmentation techniques, such as CutOut [16], MixUp [34] and BigAug [35] offer a straightforward approach to enhance the generalisability of segmentation models across different domains. Recent methods have utilized adversarial techniques, such as AdvBias [11], which trains the model to generate bias field deformations and enhance its robustness.RandConv [33] incorporates a randomized convolution layer to learn textural invariant features. Self-supervised strategies such as JiGen [9] can also improve generalisability. The principle of compositionality has been integrated into neural networks for tasks such as image classification [23], generation [2] and more recently, segmentation [26,28] to improve generalisability. The utilization of persistent homology in deep learning-based segmentation is restricted to either generating topologically accurate segmentations in the output space [21] or as a subsequent processing step [14]. The novel approach of topological auto-encoders [27] marks the first instance of incorporating persistent homology to maintain the topological structure of the data manifold within the latent representation. Cellular sheaves were used to provide a topological insight into the poor performance of graph neural networks in the heterophilic setting [7]. Recently, cellular sheaves were used as a method of detecting patch based merging relations in binary images [20]. Finally, [4] recently proposed using sheaf theory to construct a shape space which allows one to precisely define how to glue shapes together in this shape space."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.1,Shape Equivariant Learning,"Given spatial, T s and textural, T i transformations of the input space, X , the goal is to learn an encoder, Φ e to map X to lower dimensional embedding features, E which are shape equivariant and texture invariant as shown in Eq. 2.We assume T2 and ADC MRI images share the same spatial information and only have textural differences. We exploit this idea in Fig. 1, where firstly an ADC image under spatial transformation, T s is mapped with an encoder, Φ e to z 2 and the T 2 image is mapped with the same encoder to z 1 . Shape equivariance and texture invariance is enforced with the contrastive loss, L contr = T s (z 1 )z 2 2 2 . Specifically, we apply transformations from the dihedral group (D4) which consists of 90 • rotations in the z plane and 180 • rotations in the y plane. Note, a contrastive only learns equivariance as opposed to constraining the convolutional kernels to be equivariant. z 1 containing 128 channels is spatially quantised before passing into the composer. In the test phase, the ADC image is not required and only the T2 image is used as input. T2 segmentations are used as the label."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.2,Shape Component Learning,"We posit that there is limited shape variation in the low dimensional embedding space across subjects which can be fully captured in N discrete shapes. N discrete shapes form a shape dictionary, D shown in Fig. 1 which is learnt with vector quantisation [31]. Given we enforce a texture invariant continuous embedding space and hence only contains shape information, quantisation converts this continuous embedding space to discrete shape features, ẑ. The quantisation process involves minimising the Euclidean distance between the embedding space, z 1 divided into m features, z 1 i and its nearest shape component, e k ∈ D shown in Eq. 3 where k = argmin j z 1ie j 2 and m = 3048. Next, sampling D such that z 1 i is replaced by e k produces the spatially quantized embedding space ẑ. Straight-through gradient approximation is applied for backpropagation through the sampling process to update z 1 and D [31]. Gradient updates are applied to only the appropriate operands using stop gradients (sg) during optimization."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.3,Cellular Sheaves for Shape Composition,Shapes in D sampled with a uniform prior can lead to anatomically implausible segmentations after composition which we tackle through the language of cellular sheaves to model the connectivity of patches in an image which provides a connectivity-based loss function.
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Composition:,"The quantised embedding space, ẑ, is split into c groups. The composition of each group in ẑ to form each class segmentation, Y c in the output, Y involves two steps. Initially, a decoder with grouped convolutions equal to the number of classes followed by the softmax function maps, ẑ ∈ R 128×16×16×12 to C ∈ R p×c×256×256×24 where p is the number of patches for each class c. The second step of the composition uses a cellular sheaf to model the composition of Y c by gluing the patches together in an ordered manner defined by a poset while tracking its topology using persistent homology. This in turn enforces D to be sampled in a topological preserving manner as input into the decoder/composer to improve both the local and global topological correctness of each class segmentation output, Y c after composition."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Illustration:,"We illustrate our methodology of using cellular sheaves with a simple example in Fig. 2. Here, we show Y as perfectly matching the ground truth label (not one-hot encoded) divided into 2 patches. Y is a topological space with the subspaces,Each element in P is associated with a subspace in V such that the inclusion relationship is satisfied. Therefore, in Fig. 2a, P defines that Y 1 and Y 2 associated with (1, 0) and (0, 1) respectively are glued together to form Y = Y 1 ∪ Y 2 which maps with (1, 1). A cellular sheaf F over P is created by assigning a vector space to p ∈ P by deriving a persistence diagram, D for each element in V associated with p as shown in Fig. 2b. The arrows in Fig. 2b are inclusion maps defined as ρ .,. . Persistence diagrams are computed from the sequence of nested cubical complexes of each subspace in V. The persistence diagrams in Fig. 2b are formed by overlapping the persistence diagrams for each class segmentation. Note, persistence diagrams contain infinite points in the form (τ, τ ) (diagonal line in persistence diagrams) which always allows a bijection between two persistence diagrams. The main advantage of our approach is that in addition to ensuring correct local topology (patch level) and global topology (image level), we also force our network to produce topologically accurate patches correctly merged together in a topology preserving manner which matches the ground truth. For example in Fig. 2b, Y 2 contains 3 connected components glued onto Y 1 containing 2 connected components to form Y , which also has 3 connected components. This means an extra connected component is added by Y 2 due to tumour which therefore improves patch-wise tumour localisation. It also indicates the other 2 connected components in Y 2 are merged into the 2 connected components in Y 1 to form 2 larger connected components (peripheral and transitional zone) in Y . Hence, the same 2 vectors present in both F(1, 0) and F(0, 1) representing the peripheral and transitional zone are also in F (1,1). This is also known as a local section in F.  "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Implementation,"We construct cellular sheaves, F over P c and P c and minimise the distance between these cellular sheaves.We firstly plot persistence diagrams, D from the set of vectors (τ i , τ j ) in F(P c i ) and F( P c i ). Next, we minimise the total p th Wasserstein distance (topological loss) between the persistence diagrams D(F(P c i )) and D(F( P c i )) shown in Eq. 4 where η : D(F(P c i )) → D(F( P c i )) is a bijection between the persistence diagrams [27] and p = 2. This loss function is proven to be stable to noise [29] and differentiable [10]. We add a dice loss between Y and Ŷ . The total loss to train our entire framework is:"
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,5.2,Results,"In the task of anatomical segmentation, the first two columns of Table 1 show the results for the domain shift from RUNMC in the decathlon dataset to BMC.Here, we demonstrate that our method improves segmentation performance in all evaluation metrics compared to the baseline, nn-UNet and the other SDG methods. Similar findings are noted for the domain shift from the internal dataset to the RUNMC data in the ProstateX2 dataset (second two columns of Table 1).In Table 2, we note our method significantly improves tumour segmentation and localisation performance. We visualise our findings with an example in Fig. 3, where there is improved localisation of the tumour and the correct number of tumour components enforced by our topological loss. This significantly reduces the false positive rate highlighted in Table 2. Also, note the more anatomically plausible zonal segmentations. However, our method is restricted by the number of low dimensional shape components in the shape dictionary used to compose the high dimensional segmentation output. Therefore, our approach can fail to segment the finer details of prostate tumours due to its high shape variability which leads to coarser but better localised tumour segmentations."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,6.0,Conclusion,"In conclusion, we propose shape compositionality as a way to improve the generalisability of segmentation models for prostate MRI. We devise a method to learn texture invariant and shape equivariant features used to create a dictionary of shape components. We use cellular sheaf theory to help model the composition of sampled shape components from this dictionary in order to produce more anatomically meaningful segmentations and improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 24. Pre-processing: All images are resampled to 0.5 × 0.5 × 3 mm, centre cropped to 256 × 256 × 24 and normalised between 0 and 1."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Model:,"In order to address the anisotropic characteristics of Prostate MRI images, we have chosen a hybrid 2D/3D UNet as our baseline model. We use the same encoder and decoder architecture as the baseline model in our method. See supplementary material for further details."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Comparison:,"We compare our method with the nnUNet [22] and several approaches to tackle SDG segmentation namely, RandConv [33], AdvBias [11], Jigen [9] and BigAug [35] applied to the baseline model. We also compare to a compositionality driven segmentation method called the vMFNet [26].Training: In all our experiments, the models were trained using Adam optimization with a learning rate of 0.0001 and weight decay of 0.05. Training was run for up to 500 epochs on three NVIDIA RTX 2080 GPUs. The performance of the models was evaluated using the Dice score, Betti error [21] and Hausdorff distance. We evaluate tumour localisation by determining a true positive if the tumour segmentation overlaps by a minimum of one pixel with the ground truth.In our ablation studies, the minimum number of shape components required in D for the zonal and zonal + tumour segmentation experiments was 64 and 192 respectively before segmentation performance dropped. See supplementary material for ablation experiments analysing each component of our framework."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,1.0,Introduction,"Transformers [7,21,30] have seen wide-scale adoption in medical image segmentation as either components of hybrid architectures [2,3,8,9,31,33] or standalone techniques [15,25,34] for state-of-the-art performance. The ability to learn longrange spatial dependencies is one of the major advantages of the Transformer architecture in visual tasks. However, Transformers are plagued by the necessity of large annotated datasets to maximize performance benefits owing to their limited inductive bias. While such datasets are common to natural images (ImageNet-1k [6], ImageNet-21k [26]), medical image datasets usually suffer from the lack of abundant high quality annotations [19]. To retain the inherent inductive bias of convolutions while taking advantage of architectural improvements of Transformers, the ConvNeXt [22] was recently introduced to re-establish the competitive performance of convolutional networks for natural images. The Con-vNeXt architecture uses an inverted bottleneck mirroring that of Transformers, composed of a depthwise layer, an expansion layer and a contraction layer (Sect. 2.1), in addition to large depthwise kernels to replicate their scalability and long-range representation learning. The authors paired large kernel Con-vNeXt networks with enormous datasets to outperform erstwhile state-of-the-art Transformer-based networks. In contrast, the VGGNet [28] approach of stacking small kernels continues to be the predominant technique for designing ConvNets in medical image segmentation. Out-of-the-box data-efficient solutions such as nnUNet [13], using variants of a standard UNet [5], have still remained effective across a wide range of tasks.The ConvNeXt architecture marries the scalability and long-range spatial representation learning capabilities of Vision [7] and Swin Transformers [21] with the inherent inductive bias of ConvNets. Additionally, the inverted bottleneck design allows us to scale width (increase channels) while not being affected by kernel sizes. Effective usage in medical image segmentation would allow benefits from -1) learning long-range spatial dependencies via large kernels, 2) less intuitively, simultaneously scaling multiple network levels. To achieve this would require techniques to combat the tendency of large networks to overfit on limited training data. Despite this, there have been recent attempts to introduce large kernel techniques to the medical vision domain. In [18], a large kernel 3D-UNet [5] was used by decomposing the kernel into depthwise and depthwise dilated kernels for improved performance in organ and brain tumor segmentationexploring kernel scaling, while using constant number of layers and channels. The ConvNeXt architecture itself was utilized in 3D-UX-Net [17], where the Transformer of SwinUNETR [8] was replaced with ConvNeXt blocks for high performance on multiple segmentation tasks. However, 3D-UX-Net only uses these blocks partially in a standard convolutional encoder, limiting their possible benefits.In this work, we maximize the potential of a ConvNeXt design while uniquely addressing challenges of limited datasets in medical image segmentation. We present the first fully ConvNeXt 3D segmentation network, MedNeXt, which is a scalable Encoder-Decoder network, and make the following contributions: MedNeXt achieves state-of-the-art performance against baselines consisting of Transformer-based, convolutional and large kernel networks. We show performance benefits on 4 tasks of varying modality (CT, MRI) and sizes (ranging from 30 to 1251 samples), encompassing segmentation of organs and tumors. We propose MedNeXt as a strong and modernized alternative to standard ConvNets for building deep networks for medical image segmentation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.1,Fully ConvNeXt 3D Segmentation Architecture,"In prior work, ConvNeXt [22] distilled architectural insights from Vision Transformers [7] and Swin Transformers [21] into a convolutional architecture. The ConvNeXt block inherited a number of significant design choices from Transformers, designed to limit computation costs while scaling the network, which demonstrated performance improvements over standard ResNets [10]. In this work, we leverage these strengths by adopting the general design of ConvNeXt as the building block in a 3D-UNet-like [5] macro architecture to obtain the MedNeXt. We extend these blocks to up and downsampling layers as well (Sect. 2.2), resulting in the first fully ConvNeXt architecture for medical image segmentation. The macro architecture is illustrated in Fig. 1a. MedNeXt blocks (similar to ConvNeXt blocks) have 3-layers mirroring a Transformer block and are described for a C-channel input as follows:1. Depthwise Convolution Layer: This layer contains a Depthwise Convolution with kernel size k ×k ×k, followed by normalization, with C output channels. We use channel-wise GroupNorm [32] for stability with small batches [27], instead of the original LayerNorm. The depthwise nature of convolutions allow large kernels in this layer to replicate a large attention window of Swin-Transformers, while simultaneously limiting compute and thus delegating the ""heavy lifting"" to the Expansion Layer. "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.0,Expansion Layer:,"Corresponding to a similar design in Transformers, this layer contains an overcomplete Convolution Layer with CR output channels, where R is the expansion ratio, followed by a GELU [12] activation. Large values of R allow the network to scale width-wise while 1 × 1 × 1 kernel limits compute. It is important to note that this layer effectively decouples width scaling from receptive field (kernel size) scaling in the previous layer. 3. Compression Layer: Convolution layer with 1 × 1 × 1 kernel and C output channels performing channel-wise compression of the feature maps.MedNeXt is convolutional and retains the inductive bias inherent to Conv-Nets that allows easier training on sparse medical datasets. Our fully ConvNeXt architecture also enables width (more channels) and receptive field (larger kernels) scaling at both standard and up/downsampling layers. Alongside depth scaling (more layers), we explore these 3 orthogonal types of scaling to design a compound scalable MedNeXt for effective medical image segmentation (Sect. 2.4).  "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.2,Resampling with Residual Inverted Bottlenecks,"The original ConvNeXt design utilizes separate downsampling layers which consist of standard strided convolutions. An equivalent upsampling block would be standard strided transposed convolutions. However, this design does not implicitly take advantage of width or kernel-based ConvNeXt scaling while resampling. We improve upon this by extending the Inverted Bottleneck to resampling blocks in MedNeXt. This is done by inserting the strided convolution or transposed convolution in the first Depthwise Layer for Downsampling and Upsampling MedNeXt blocks respectively. The corresponding channel reduction or increase is inserted in the last compression layer of our MedNeXt 2× Up or Down block design as in Fig. 1a. Additionally, to enable easier gradient flow, we add a residual connection with 1 × 1 × 1 convolution or transposed convolution with stride of 2.In doing so, MedNeXt fully leverages the benefits from Transformer-like inverted bottlenecks to preserve rich semantic information in lower spatial resolutions in all its components, which should benefit dense medical image segmentation tasks."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.3,UpKern: Large Kernel Convolutions Without Saturation,"Large convolution kernels approximate the large attention windows in Transformers, but remain prone to performance saturation. ConvNeXt architectures in classification of natural images, despite the benefit of large datasets such as ImageNet-1k and ImageNet-21k, are seen to saturate at kernels of size 7 × 7 × 7 [22]. Medical image segmentation tasks have significantly less data and performance saturation can be a problem in large kernel networks. To propose a solution, we borrow inspiration from Swin Transformer V2 [20] where a largeattention-window network is initialized with another network trained with a smaller attention window. Specifically, Swin Transformers use a bias matrix B ∈ R (2M -1)×(2M -1) to store learnt relative positional embeddings, where M is the number of patches in an attention window. On increasing the window size, M increases and necessitates a larger B. The authors proposed spatially interpolating an existing bias matrix to the larger size as a pretraining step, instead of training from scratch, which demonstrated improved performance. We propose a similar approach but customized to convolutions kernels, as seen in Fig. 1b, to overcome performance saturation. UpKern allows us to iteratively increase kernel size by initializing a large kernel network with a compatible pretrained small kernel network by trilinearly upsampling convolutional kernels (represented as tensors) of incompatible size. All other layers with identical tensor sizes (including normalization layers) are initialized by copying the unchanged pretrained weights. This leads to a simple but effective initialization technique for Med-NeXt which helps large kernel networks overcome performance saturation in the comparatively limited data scenarios common to medical image segmentation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.4,"Compound Scaling of Depth, Width and Receptive Field","Compound scaling [29] is the idea that simultaneous scaling on multiple levels (depth, width, receptive field, resolution etc.) offers benefits beyond that of scaling at one single level. The computational requirements of indefinitely scaling kernel sizes in 3D networks quickly becomes prohibitive and leads us to investigate simultaneous scaling at different levels. Keeping with Fig. 1a, our scaling is tested for block count (B), expansion ratio (R) and kernel size (k) -corresponding to depth, width and receptive field size. We use 4 model configurations of the MedNeXt to do so, as detailed in Table 1 (Left). The basic functional design (MedNeXt-S) uses number of channels (C) as 32, R = 2 and B = 2. Further variants increase on just R (MedNeXt-B) or both R and B (MedNeXt-M).The largest 70-MedNext-block architecture uses high values of both R and B (MedNeXt-L) and is used to demonstrate the ability of MedNeXt to be significantly scaled depthwise (even at standard kernel sizes). We further explore large kernel sizes and experiment with k = {3, 5} for each configuration, to maximize performance via compound scaling of the MedNeXt architecture.3 Experimental Design"
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.1,"Configurations, Implementation and Baselines","We use PyTorch [24] for implementing our framework. We experiment with 4 configurations of the MedNeXt with 2 kernel sizes as detailed in Sect. 2.4. The GPU memory requirements of scaling are limited via -1) Mixed precision training with PyTorch AMP, 2) Gradient Checkpointing. [4]. Our experimental framework uses the nnUNet [13] as a backbone -where the training schedule (epochs = 1000, batches per epoch = 250), inference (50% patch overlap) and data augmentation remain unchanged. All networks, except nnUNet, are trained with AdamW [23] as optimizer. The data is resampled to 1.0 mm isotropic spacing during training and inference (with results on original spacing), using input patch size of 128 × 128 × 128 and 512 × 512, and batch size 2 and 14, for 3D and 2D networks respectively. The learning rate for all MedNeXt models is 0.001, except kernel:5 in KiTS19, which uses 0.0001 for stability. For baselines, all Swin models and 3D-UX-Net use 0.0025, while ViT models use 0.0001. We use Dice Similarity Coefficient (DSC) and Surface Dice Similarity (SDC) at 1.0 mm tolerance for volumetric and surface accuracy. 5-fold cross-validation (CV) mean performance for supervised training using 80:20 splits for all models are reported. We also provide test set DSC scores for a 5-fold ensemble of MedNeXt-L (kernel: 5 × 5 × 5) without postprocessing. Our extensive baselines consist of a high-performing convolutional network (nnUNet [13]), 4 convolution-transformer hybrid networks with transformers in the encoder (UNETR [9], SwinUNETR [8]) and in intermediate layers (TransBTS [31], TransUNet [3]), a fully transformer network (nnFormer [34]) as well as a partially ConvNeXt network (3D-UX-Net [17]). TransUNet is a 2D network while the rest are 3D networks. The uniform framework provides a common testbed for all networks, without incentivizing one over the other on aspects of patch size, spacing, augmentations, training and evaluation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.2,Datasets,"We use 4 popular tasks, encompassing organ as well as tumor segmentation tasks, to comprehensively demonstrate the benefits of the MedNeXt architecture -1) Beyond-the-Cranial-Vault (BTCV) Abdominal CT Organ Segmentation [16], 2) AMOS22 Abdominal CT Organ Segmentation [14] 3) Kidney Tumor Segmentation Challenge 2019 Dataset (KiTS19) [11], 4) Brain Tumor Segmentation Challenge 2021 (BraTS21) [1]. BTCV, AMOS22 and KiTS19 datasets contain 30, 200 and 210 CT volumes with 13, 15 and 2 classes respectively, while the BraTS21 dataset contains 1251 MRI volumes with 3 classes. This diversity shows the effectiveness of our methods across imaging modalities and training set sizes."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4.1,Performance Ablation of Architectural Improvements,"We ablate the MedNeXt-B configuration on AMOS22 and BTCV datasets to highlight the efficacy of our improvements and demonstrate that a vanilla ConvNeXt is unable to compete with existing segmentation baselines such as nnUNet. The following are observed in ablation tests in  3. The performance boost in large kernels is seen to be due to the combination of UpKern with a larger kernel and not merely a longer effective training schedule (Upkern vs Trained 2×), as a trained MedNeXt-B with kernel 3×3×3 retrained again is unable to match its large kernel counterpart.This highlights that the MedNeXt modifications successfully translate the ConvNeXt architecture to medical image segmentation. We further establish the performance of the MedNeXt architecture against our baselines -comprising of convolutional, transformer-based and large kernel baselines -on all 4 datasets. We discuss the effectiveness of the MedNeXt on multiple levels."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4.2,Performance Comparison to Baselines,"There are 2 levels at which MedNeXt successfully overcomes existing baselines -5 fold CV and public testset performance. In 5-fold CV scores in Table 2, Med-NeXt, with 3 × 3 × 3 kernels, takes advantage of depth and width scaling to provide state-of-the-art segmentation performance against every baseline on all 4 datasets with no additional training data. MedNeXt-L outperforms or is competitive with smaller variants despite task heterogeneity (brain and kidney tumors, organs), modality (CT, MRI) and training set size (BTCV: 18 samples vs BraTS21: 1000 samples), establishing itself as a powerful alternative to established methods such as nnUNet. With UpKern and 5 × 5 × 5 kernels, MedNeXt takes advantage of full compound scaling to improve further on its own small kernel networks, comprehensively on organ segmentation (BTCV, AMOS22) and in a more limited fashion on tumor segmentation (KiTS19, BraTS21).Furthermore, in leaderboard scores on official testsets (Fig. 1c), 5-fold ensembles for MedNeXt-L (kernel: 5 × 5 × 5) and nnUNet, its strongest competitor are compared -1) BTCV: MedNeXt beats nnUNet and, to the best of our knowledge, is one of the leading methods with only supervised training and no extra training data (DSC: 88.76, HD95: 15.34), 2) AMOS22: MedNeXt not only surpasses nnUNet, but is also Rank 1 (date: 09.03.23) currently on the leaderboard (DSC: 91.77, NSD: 84.00), 3) KITS19: MedNeXt exceeds nnUNet performance (DSC: 91.02), 4) BraTS21: MedNeXt surpasses nnUNet in both volumetric and surface accuracy (DSC: 88.01, HD95: 10.69). MedNeXt attributes its performance solely to its architecture without leveraging techniques like transfer learning (3D-UX-Net) or repeated 5-fold ensembling (UNETR, SwinUNETR), thus establishing itself as the state-of-the-art for medical image segmentation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,5.0,Conclusion,"In comparison to natural image analysis, medical image segmentation lacks architectures that benefit from scaling networks due to inherent domain challenges such as limited training data. In this work, MedNeXt is presented as a scalable Transformer-inspired fully-ConvNeXt 3D segmentation architecture customized for high performance on limited medical image datasets. We demonstrate Med-NeXt's state-of-the-art performance across 4 challenging tasks against 7 strong baselines. Additionally, similar to ConvNeXt for natural images [22], we offer the compound scalable MedNeXt design as an effective modernization of standard convolution blocks for building deep networks for medical image segmentation."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,1.0,Introduction,"Light-sheet microscopy is a powerful imaging modality that allows for fast and high-resolution imaging of large samples, such as the whole brain of the Supported by NIH R01NS110791, NIH R01MH121433, NIH P50HD103573, and Foundation of Hope.mouse [3,14]. Tissue-clearing techniques enable the removal of light-scattering molecules, thus improving the penetration of light through biological samples and allowing for better visualization of internal structures, including nuclei [2,16]. Together, light-sheet microscopy and tissue-clearing techniques have revolutionized the field of biomedical imaging and they have been widely used for studying the structure and function of tissues and organs at the cellular level.Accurate 3D nuclei instance segmentation plays a crucial role in identifying and delineating individual nuclei within three-dimensional space, which is essential for understanding the complex structure and function of biological tissues in the brain. Previous [10] and [5] have applied graph-based approaches that model links of voxel and neuron region, respectively, for 3D neuron segmentation from electron microscopy image stacks. However, accurate segmentation of nuclei from light-sheet microscopy images of cleared tissue can be a challenging task due to the presence of complex tissue structures, cell shapes, and variations in nuclei size and shape [1]. Due to the high cost of 3D manual nuclei annotations and the complexity of learning, current end-to-end NIS models are typically limited to training and testing on small image stacks (e.g., 128×128×64). Considering these limitations, one approach for achieving whole-brain NIS is dividing the whole stack into smaller stacks, so the existing NIS methods can handle each piece individually. In such a scenario, constructing the whole-brain nuclei instance segmentation in 3D from these smaller image stacks arises a new challenge. The gaps between these smaller stacks (intra-slice) and the slices (inter-slice) require a robust stitching method for accurate NIS. We show these gaps in Fig. 1. Note, the intra-slice gap, commonly referred to as the boundary gap, arises due to the existence of boundaries in the segmentation outcome of image stacks and poses a challenge in achieving smooth segmentation between neighboring image stacks. Current approaches may, however, undermine the overall quality of the wholebrain NIS results which leads to inaccurate reports of nuclei counts. Figure 1 (left) illustrates the typical examples of the boundary gap issues, where the red circle highlights the incidence of over-counting (both partial nuclei instances are recognized as a complete nucleus in the corresponding image stack), while the dashed blue box denotes the issue of under-counting (none of the partial nuclei instances has been detected in each image stack).It is a common practice to use overlapped image stacks to stitch the intensity image (continuous values) by weighted averaging from multiple estimations [12,15]. However, when nuclei are in close proximity and represent the same entity, it becomes crucial to accurately match the indexes of nuclei instances, which refer to the segmentation labels. We call this the nuclei stitching issue. This issue presents a significant challenge in the pursuit of achieving wholebrain NIS. To address this non-trivial challenge, we formulate this problem as a knowledge graph (KP) task that is built to characterize the nuclei-to-nuclei relationships based on the feature presentation of partial image appearance. By doing so, the primary objective of this learning problem is to determine whether to merge two partial nuclei instances that exist across different slices or stacks. Drawing inspiration from recent research on object tracking using graph models,  we construct a graph contextual model to assemble 3D nuclei in a graph neural network (GNN). In the overlapping area, the complete 2D nucleus instance is represented as a graph node, and the links between these nodes correspond to the nuclei-to-nuclei relationship. Conventional knowledge graph learning typically emphasizes the learning of the relationship function. In this context, it appears that the process of wholebrain NIS largely depends on the relationship function to link partial nuclei instances along the gap between slices. Nonetheless, a new challenge arises from the NIS backbone due to the anisotropic image resolution where inter-slice Z resolution (e.g., 2.5 µm) is often several times lower than the in-plane X-Y resolution (0.75 × 0.75 µm 2 ). That is, the (partial) nuclei instances located near the boundary across X-Y planes (along the inter-slice direction with poor image resolution) in the 3D image stack have a large chance of being misidentified, leading to the failure of NIS stitching due to the absence of nuclei instances that are represented as nodes in the contextual graph model. To alleviate this issue, we present a two-stage hierarchical whole-brain NIS framework that involves stitching 2D NIS results in the X-Y plane (stage 1) and then assembling these 2D instances into 3D nuclei using a graph contextual model (stage 2). The conjecture is that the high resolution in the X-Y plane minimizes the risk of missing 2D NIS instances, allowing the knowledge graph learning to be free of absent nodes in establishing correspondences between partial nuclei instances.Stitching 2D nuclei instances in each image slice is considerably easier than stitching across image slices due to the finer image resolution in the X-Y plane. However, as shown in Fig. 1 (right), the poor resolution in the Z-axis makes it challenging to identify partial nuclei instances along this axis. Thus, prestitching in the X-Y plane of the first stage can reduce the probability of having 2D nuclei instances missing along the Z axis at the second stage. Among this, we train the graph contextual model to predict the nuclei-to-nuclei correspondence across image slices, where each node is the 2D nuclei instance without the intraslice gap issue. Since stage 2 is on top of the existing NIS methods in stage 1, our stitching framework is agnostic and can support any state-of-the-art NIS methods to expand from small image stacks to the entire brain.In the experiments, we have comprehensively evaluated the segmentation and stitching accuracy (correspondence matching between 2D instances) and wholebrain NIS results (both visual inspection and quantitative counting results). Compared to no stitching, Our deep stitching model has shown a significant improvement in the whole-brain NIS results with different state-of-the-art models, indicating its potential for practical applications in the field of neuroscience."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.0,Methods,"Current state-of-the-art NIS methods, such as Mask-RCNN [6,7], 3D Unet [4,8] and Cellpose [9,11] are designed to segment nuclei instances in a predefined small image stack only. To scale up to whole-brain NIS, we propose a graph-based contextual model to establish nuclei-to-nuclei correspondences across image stacks. On top of this backbone, we present a hierarchical wholebrain NIS stitching framework that is agnostic to existing NIS methods."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.1,Graph Contextual Model,"Problem Formulation. Our graph contextual model takes a set of partial nuclei instances, sliced by the inter-slice gap, as input. These nuclei instances can be obtained using a 2D instance segmentation method, such as Mask-RCNN. The output of our model is a collection of nuclei-to-nuclei correspondences, which enable us to stitch together the NIS results from different image slices (by running NIS separately). We formulate this correspondence matching problem as a knowledge graph learning task where the links between nodes in the graph contextual model represent the probability of them belonging to the same nuclei instance. In this regard, the key component of NIS stitching becomes seeking for a relationship function that estimates the likelihood of correspondence based on the node features, i.e., image appearance of to-be-stitched 2D nuclei instances.Machine Learning Components in Graph Contextual Model. First, we construct an initial contextual graph G = {V, E} for each 2D nucleus instance x (i.e., image appearance vector). The set of nodes V = {x i |D(x, x i ) > δ} includes all neighboring 2D nuclei instances, where the distance between the centers of two instances is denoted by D, and δ is a predefined threshold. The matrix E ∈ R N ×N represents the edges between nodes, where N is the number of neighboring instances. Specifically, we compute the Intersection over Union (IoU) between the two instances and set the edge weight as e ij = IoU (x i , x j ).Second, we train the model on a set of contextual graphs G to recursively (1) find the mapping function γ to describe the local image appearance on each graph node and (2) learn the triplet similarity function ψ.-Graph feature representation learning. For the k th iteration, we enable two connected nodes to exchange their feature representations constrained by the current relationship topology e k ij by the k th layer of the deep stitching model. In this context, we define the message-passing function as:Following the popular learning scheme in knowledge graphs [13], we employ Multilayer Perceptron (MLP) to act functions γ, φ. -Learning the link-wise similarity function to predict nuclei-to-nuclei correspondence. Given the updated node feature representations {x (k+1) i }, we train another MLP to learn the similarity function ψ in a layer-by-layer manner. In the k th layer, we update each 2D-to-3D contextual correspondence e (k+1) j,i for the next layer by(2)"
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.2,Hierarchical Stitching Framework for Whole-Brain NIS,"Our graph contextual model is able to stitch the NIS results across the intraslice gap areas in X/Y-Z plane. As demonstrated in Fig. 1, the accuracy of no stitching is limited by the absence of 2D partial nuclei instances due to the poor inter-slice resolution. With the graph model, as shown in Fig. 2 and3, we propose a hierarchical stitching framework for whole-brain NIS in two stages.1. Resolve intra-slice gap in X-Y plane. Suppose that each within-stack NIS result overlaps with its neighboring image stack in the X-Y plane. Then, we can resolve the intra-slice gap problem in X-Y plane in three steps: (i) identify the duplicated 2D nuclei instances from multiple overlapped image stacks, (ii) find the representative NIS result from the ""gap-free"" image stack, and (iii) unify multiple NIS estimations by using the ""gap-free"" NIS estimation as the appearance of the underlying 2D nuclei. The effect of this solution is shown in Fig. 2 right, where the gray areas are spatially overlapped. We use the arrows to indicate that the 2D nuclei instances (red dash circles) have been merged to the counterpart from the ""gap-free"" image stack (yellow circles). 2. Inter-slice stitching using graph contextual model. At each gap area along Z-axis, we deploy the graph contextual model to stitch the sliced nuclei instances. Specifically, we follow the partition of the whole-brain microscopy image in stage 1, that is a set of overlapped 3D image stacks, all the way from the top to the bottom as shown in the left of Fig. 3. It is worth noting that each 2D nuclei instance in the X-Y plane is complete, as indicated by the red-highlighted portion extending beyond the image stack. Next, we assign a stack-specific local index to each 2D nuclei instance. After that, we apply the (trained) graph contextual model to each 2D nuclei instance. By tracking the correspondences among local indexes, we remove the duplicated inter-slice correspondence and assign the global index to the 3D nuclei instance."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.3,Implementation Details,"We empirically use 18.75 µm for the overlap size between two neighboring image stacks in X-Y plane. The conventional NIS method is trained using 128 × 128 patches. For the graph contextual model, the MLPs consist of 12 fully-connected layers. Annotated imaging data has been split into training, validation, and testing sets in a ratio of 6:1:1. Adam is used with lr = 5e -4 as the optimizer, Dropout = 0.5, and focal loss as the loss function.3 Experiments"
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.1,Experimental Settings,"Stitching Methods Under Comparison. We mainly compare our stitching method with the conventional analytic approach by IoU (Intersection over Union) matching, which is widely used in object detection with no stitching. We perform the experiments using three popular NIS deep models, that are Mask-RCNN-R50 (with ResNet50 backbone), Mark-RCNN-R101 (with ResNet101 backbone) and CellPose, with two stitching methods, i.e., our hierarchical stitching framework and IoU-based matching scheme.Data and Computing Environment. "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.2,Evaluation Metrics,"We use the common metrics precision, recall, and F1 score to evaluate the 3D NIS between annotated and predicted nuclei instances. Since the major challenge of NIS stitching is due to the large inter-slice gap, we also define the stitching accuracy for each 3D nuclei instance by counting the number of 2D NIS (in the X-Y plane) that both manual annotation and stitched nuclei share the same instance index."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.3,Evaluating the Accuracy of NIS Stitching Results,"Quantitative Evaluation. As shown in Fig. 4, there is a clear sign that NIS models with our hierarchical stitching method outperform IoU-based counterparts on NIS metrics, regardless of the NIS backbone models. In average, our hierarchical stitching method has improved 14.0%, 5.1%, 10.2%, and 3.4% in precision, recall, F1 score, and stitching accuracy, respectively compared with IoU-based results.Visual Inspection. We also show the visual improvement of whole-brain NIS results before and after stitching in Fig. 5. Through the comparison, it is apparent that (1) the inconsistent NIS results along the intra-slice gap area have been "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.4,Whole-Brain NIS in Neuroscience Applications,"One of the important steps in neuroscience studies is the measurement of regional variations in terms of nuclei counts. In light of this, we evaluate the counting accuracy. Since we only have 16 image stacks with manual annotations, we sim-ulate the stack-to-stack gap in the middle of each image stack and compare counting results between whole-brain NIS with or without hierarchical stitching. Compared to whole-brain NIS without stitching, our hierarchical stitching method has reduced the nuclei counting error from 48.8% down to 10.1%. The whole-brain NIS improvement has vividly appeared in Fig. 5. In addition, the running time of whole-brain NIS is around 26 h on average for the typical light-sheet microscopy images of the mouse brain. More visual results of the whole-brain NIS can be found in supplementary materials."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,4.0,Conclusion,"In this work, we introduce a learning-based stitching approach to achieve 3D instance segmentation of nuclei in whole-brain microscopy images. Our stitching framework is flexible enough to incorporate existing NIS methods, which are typically trained on small image stacks and may not be able to scale up to the whole-brain level. Our method shows great improvement by addressing interand intra-slice gap issues. The promising results in simulated whole-brain NIS, particularly in terms of counting accuracy, also demonstrate the potential of our approach for neuroscience research."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_5.
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,1.0,Introduction,"Fluorodeoxyglucose Positron Emission Tomography (PET) is widely recognized as an essential tool in oncology [10], playing an important role in the stag-ing, monitoring, and follow-up radiotherapy (RT) planning [2,19]. Delineation of Region of Interest (ROI) is a crucial step in RT planning. It enables the extraction of semi-quantitative metrics such as standardized uptake values (SUVs), which normalize pixel intensities based on patient weight and radiotracer dose [20]. Manual delineation is a time-consuming and laborious task that is prone to poor reproducibility in medical imaging, and this is particularly true for PET, due to its low signal-to-noise ratio and limited spatial resolution [10]. In addition, manual delineation depends heavily on the expert's prior knowledge, which often leads to large inter-observer and intra-observer variations [8]. Therefore, there is an urgent need for developing accurate automatic segmentation algorithms in PET images which will reduce expert workload, speed up RT planning while reducing intra-observer variability.In the last decade, CNNs have demonstrated remarkable achievements in medical image segmentation tasks. This is primarily due to their ability to learn informative hierarchical features directly from data. However, as illustrated in [9,23], it is rather difficult for CNNs to recognize the object boundary precisely due to the information loss in the successive downsampling layers. Despite the headway made in using CNNs, their applications have been restricted to the generation of pixel-wise segmentation maps instead of smooth contour. Although CNNs may yield satisfactory segmentation results, low values of the loss function may not always indicate a meaningful segmentation. For instance, a noisy result can create incorrect background contours and blurry object boundaries near the edge pixels [6]. To address this, a kernel smoothing-based probability contour (KsPC) approach was proposed in our previous work [22]. Instead of a pixel-wise analysis, we assume that the true SUVs come from a smooth underlying spatial process that can be modelled by kernel estimates. The KsPC provides a surface over images that naturally produces contour-based results rather than pixel-wise results, thus mimicking experts' hand segmentation. However, the performance of KsPC depends heavily on the tuning parameters of bandwidth and threshold in the model, and it lacks information from other patients.Beyond tumour delineation, another important use of functional images, such as PET images is their use for designing IMRT dose painting (DP). In particular, dose painting uses functional images to paint optimised dose prescriptions based on the spatially varying radiation sensitivities of tumours, thus enhancing the efficacy of tumour control [14,18]. One of the popular DP strategies is dose painting by contours (DPBC), which assigns a homogeneous boost dose to the subregions defined by SUV thresholds. However, there is an urgent need to develop image segmentation approaches that reproducibly and accurately identify the high recurrent-risk contours [18]. Our previously proposed KsPC provides a clear framework to calculate the probability contours of the SUV values and can readily be used to define an objective strategy for segmenting tumours into subregions based on metabolic activities, which in turn can be used to design the IMRT DP strategy.To address both tumour delineation and corresponding dose painting challenges, we propose to combine the expressiveness of deep CNNs with the versa-tility of KsPC in a unified framework, which we call KsPC-Net. In the proposed KsPC-Net, a CNN is employed to learn directly from the data to produce the pixel-wise bandwidth feature map and initial segmentation map, which are used to define the tuning parameters in the KsPC module. Our framework is completely automatic and differentiable. More specifically, we use the classic UNet [17] as the CNN backbone and evaluate our KsPC-Net on the publicly available MICCAI HECKTOR (HEad and neCK TumOR segmentation) challenge 2021 dataset. Our proposed KsPC-Net yields superior results in terms of both Dice similarity scores and Hausdorff distance compared to state-of-art models. Moreover, it can produce contour-based segmentation results which provide a more accurate delineation of object edges and provide probability contours as a byproduct, which can readily be used for DP planning."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.1,Kernel Smoothing Based Probability Contour,"Kernel-based method and follow up approach of modal clustering [13,16] have been used to cluster high-dimensional random variables and natural-scene image segmentation. In this work, we propose to model the pixel-specific SUV as a discretized version of the underlying unknown smooth process of ""metabolic activity"". The smooth process can then be estimated as kernel smoothed surface of the SUVs over the domain of the entire slice. In particular, let Y = (y 1 , y 2 , ..., y N ) denote N pixel's SUV in a 2D PET image sequentially, and x i = (x i1 , x i2 ), i = 1, ..., N denote position vector with x i1 and x i2 being the position in 2D respectively. Note that x i ∈ R d and d = 2 in our case. We assume that for each position vector x, the SUV represents the frequency of x appearing in the corresponding grid. The SUV surface can therefore be modelled as kernel density estimate (KDE) [3,15] of an estimated point x, which is defined generally aswhere K is a kernel function and H is a symmetric, positive definite, d×d matrix of smoothing tuning parameters, called bandwidth which controls the orientation and amount of smoothing via the scaled kernelOn the other hand, since x t is counted y i times at the same position, Eq. 1 can be further simplified asA scaled kernel is positioned so that its mode coincides with each data point x i which is expressed mathematically as K H (x-x i ). In this paper, we have used a Guassian kernel which is denoted as:which is a normal distribution with mean x i and variance-covariance matrix H. Therefore, we can interpret f in Eq. ( 2) as the probability mass of the data point x which is estimated by smoothing the SUVs of the local neighbourhood using the Gaussian kernel. The resulting surface built by the KDE process can be visualized in Fig. 1(c). By placing a threshold plane, a contour-based segmentation map can naturally be obtained. Note that one can obtain a pixel-based segmentation map, by thresholding the surface at the observed grid points. After delineating the gross tumour volume, a follow-up application of the kernel smoothed surface is to construct probability contours. Mathematically, a 100 ω% region of a density f is defined as the level set L(f ω ) = {f (x) ≥ f ω } with its corresponding contour level f ω such that P(x∈ L(f ω ) = 1ω, where x is a random variable and L(f ω ) has a minimal hypervolume [11]. In other words, for any ω ∈ (0, 1), the 100 ω% contour refers to the region with the smallest area which encompasses 100 ω% of the probability mass of the density function [11]. In practice, f ω can be estimated using the following result.Result. The estimated probability contour level f ω can be computed as the ω-th quantile of fω of f (x 1 ; H), ..., f (x n ; H) (Proof in supplementary materials).The primary advantage of utilizing probability contours is their ability to assign a clear probabilistic interpretation on the defined contours, which are scale-invariant [5]. This provides a robust definition of probability under the perturbation of the input data. In addition, these contours can be mapped to the IMRT dose painting contours, thus providing an alternative prescription strategy for IMRT. Examples of the application of probability contours will be demonstrated and explained in Sect. 4.2."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.2,The KsPC-Net Architecture,"In the KsPC module, the model performance heavily depends on the bandwidth matrix H and it is often assumed that each kernel shares the same scalar bandwidth parameter. However, one may want to use different amounts of smoothing in the kernel at different grid positions. The commonly used approach for bandwidth selection is cross-validation [4], which is rather time-consuming even in the simpler scalar situation. In this paper, we instead use the classic 2D-Unet [17] as our CNN backbone to compute the pixel-level bandwidth feature map, which informs the KsPc bandwidth. Additionally, we obtain the optimal threshold for constructing the KsPC contour from the initial segmentation map. As shown in Fig. 2 the proposed KsPC-Net integrates the KsPC approach with a CNN backbone (UNet) in an end-to-end differentiable manner. First, the initial segmentation map and pixel-level bandwidth parameter map h(x i1 , x i2 ) of KsPC are learned from data by the CNN backbone. Then the KsPC module obtains the quantile threshold value for each image by identifying the quantile corresponding to the minimum SUV of the tumour class in the initial segmentation map. The next step involves transmitting the bandwidth map, quantile threshold, and raw image to KsPC module to generate the segmentation map and its corresponding probability contours. The resulting output from KsPC is then compared to experts' labels using a Dice similarity loss function, referred to KsPC loss. Additionally, the initial Unet segmentation can produce another loss function, called CNN loss, which serves as an auxiliary supervision for the CNN backbone. The final loss can then be constructed as the weighted sum of CNN loss and KsPC loss. By minimizing the final loss, the error can be backpropagated through the entire KsPC architecture to guide the weights updating the CNN backbone. "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.3,Loss Function,"The Dice similarity coefficient is widely employed to evaluate segmentation models. We utilize the Dice loss function to optimize the model performance during training, which is defined as:, where y i is the label from experts and ŷi is the predicted label of i-th pixel. N is the total number of pixels and is a small constant in case of zero division.As shown in Fig. 2, we construct the weighted Dice loss to train the model as follows:where L f inal denotes the weighed dice loss while L CNN and L KsP C denotes the CNN loss and KsPC loss, respectively. In addition, α is a balancing parameter and is set to be 0.01 in this work."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.1,Dataset,"The dataset is from the HECKTOR challenge in MICCAI 2021 (HEad and neCK TumOR segmentation challenge). The HECKTOR training dataset consists of 224 patients diagnosed with oropharyngeal cancer [1]. For each patient, FDG-PET input images and corresponding labels in binary description (0 s and 1 s) for the primary gross tumour volume are provided and co-registered to a size of 144 × 144 × 144 using bounding box information encompassing the tumour. Five-fold cross-validation is used to generalize the performance of models."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.2,Implementation Details,We used Python and a trained network on a NVIDIA Dual Quadro RTX machine with 64 GB RAM using the PyTorch package. We applied a batch size of 12 and the Adam algorithm [12] with default parameters to minimize the dice loss function. All models were trained for 300 epochs. Each convolutional layer is followed by RELU activation and batch normalization.
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4.1,Results on HECKTOR 2021 Dataset,"To evaluate the performance of our KsPC-Net, we compared it with the results of 5-fold cross-validation against three widely-used models namely, the standard 2D Unet, the 2D residual Unet and the 3D Unet. Additionally, we compare our performance against newly developed approaches MSA-Net [7] and CCUT-Net [21] which were reported in the HECKTOR 2021 challenges [1]. To quantify the performance, we report several metrics including Dice similarity scores, Precision, Recall, and Hausdorff distance. Table 1 shows the quantitative comparison of different approaches on HECKTOR dataset. It is worth mentioning that since our KsPC-Net is in a 2D Unet structure, the Hausdorff distance here was calculated on slice averages to use a uniform metric across all 2D and 3D segmentation models. However, the results of 2D Hausdorff distances of MSA-Net and CCUT-Net are not available and therefore they are omitted in the table of comparison. The results clearly demonstrate that the proposed KsPC-Net is effective in segmenting H&N tumours, achieving a mean Dice score of 0.768. This represents a substantial improvement over alternative approaches, including 2D-UNet (0.740), 3D U-Net (0.764), Residual-Unet (0.680), MSA-Net (0.757) and CCUT-Net (0.750). While we acknowledge that there was no statistically significant improvement compared to other SOTA models, it is important to note that our main goal is to showcase the ability to obtain probability contours as a natural byproduct while preserving state-of-the-art accuracy levels. On the other hand, in comparison to the baseline 2D-Unet model, KsPC-Net yields a higher Recall (0.911) with a significant improvement (4.35%), indicating that KsPC-Net generates fewer false negatives (FN). Although the Precision of KsPC-Net is slightly lower than the best-performing method (3D Unet), it achieves a relatively high value of 0.793. In addition, the proposed KsPC-Net achieves the best performance on Hausdorff distance among the three commonly used Unet models (2D-Unet, Res-Unet and 3D-Unet), which indicates that KsPC-Net exhibits a stronger capacity for accurately localizing the boundaries of objects. This is consistent with the mechanisms of KsPC, which leverages neighbouring weights to yield outputs with enhanced smoothness."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4.2,Probability Contours,"One of the byproducts of using the kernel-smoothed densities to model the SUVs is the associated probability contours, which can be readily used to develop a comprehensive inferential framework and uncertainty quantification. For example, Fig. 3 provides two examples of PET image segmentation maps by KsPC-Net and their corresponding probability contours in the last column. There are 5 contours in each case which is linear in probability space, in the sense that each contour encloses 10%, 30%, 50%, 70% and 90% probability mass respectively (from inner to outer), thus dividing the density surface into subregions with attached probability mass. These probability contours can provide a rigorous framework for designing the number and magnitude of SUV thresholds for designing optimal DP strategies. Since the SUVs are smoothed by the kernel density heights, the inner 10% probability contour corresponds to the subregion with relatively higher SUVs. In other words, there is an inverse mapping between the probability contours and the amount of dose boost assigned to subvolumes."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,5.0,Conclusion,"In this paper, we present a novel network, KsPC-Net, for the segmentation in 2D PET images, which integrates KsPC into the UNet architecture in an end-toend differential manner. The KsPC-Net utilizes the benefits of KsPC to deliver both contour-based and grid-based segmentation outcomes, leading to improved precision in the segmentation of contours. Promising performance was achieved by our proposed KsPC-Net compared to the state-of-the-art approaches on the MICCAI 2021 challenge dataset (HECKTOR). It is worth mentioning that the architecture of our KsPC-Net is not limited to Head & Neck cancer type and can be broadcast to different cancer types. Additionally, a byproduct application of our KsPC-Net is to construct probability contours, which enables probabilistic interpretation of contours. The subregions created by probability contours allow for a strategy planning for the assigned dose boosts, which is a necessity for the treatment planning of radiation therapy for cancers."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 51.
NISF: Neural Implicit Segmentation Functions,1.0,Introduction,"Image segmentation is a core task in domains where the area, volume or surface of an object is of interest. The principle of segmentation involves assigning a class to every presented point in the input space. Typically, the input is presented in the form of images: aligned pixel (or voxel) grids, with the intention to obtain a class label for each. In this context, the application of deep learning to the medical imaging domain has shown great promise in recent years. With the advent of the U-Net [20], Convolutional Neural Networks (CNN) have been successfully applied to a multitude of imaging domains and achieved (or even surpassed) human performance [11]. The convolution operation make CNNs an obvious choice for dealing with inputs in the form of 2D pixel-or 3D voxel-grids.Despite their efficacy, CNNs suffer from a range of limitations that lead to incompatibilities for some imaging domains. CNNs are restricted to data in the form of grids, and cannot easily handle sparse or partial inputs. Moreover, due to the CNN's segmentation output also being confined to a grid, obtaining smooth object surfaces requires post-processing heuristics. Predicting a high resolution segmentations also has implications on the memory and compute requirements in high-dimensional domains. Finally, the learning of long-distance spatial correlations requires deep stacks of layers, which may pose too taxing in low resource domains.We introduce a novel approach to image segmentation that circumvents these shortcomings: Neural Implicit Segmentation Functions (NISF). Inspired by ongoing research in the field of neural implicit functions (NIF), a neural network is taught to learn a mapping from a coordinate space to any arbitrary real-valued space, such as segmentation, distance function, or image intensity. While CNNs employ the image's pixel or voxel intensities as an input, NISF's input is a real-valued vector c ∈ R N for a single N-dimensional coordinate, alongside a subject-specific latent representation vector h ∈ R d . Given c and h, the network is taught to predict image intensity and segmentation value pairs. The space H over all possible latent vectors h serves as a learnable prior over all possible subject representations.In this paper, we describe an auto-decoder process by which a previously unseen subject's pairs of coordinate-image intensity values (c, i) may be used to approximate that subject's latent representation h. Given a latent code, the intensity and segmentation predictions from any arbitrary coordinates in the volume may be sampled. We evaluate the proposed framework's segmentation scores and investigate its generalization properties on the UK-Biobank cardiac magnetic resonance imaging (MRI) short-axis dataset. We make the source code publicly available1 ."
NISF: Neural Implicit Segmentation Functions,2.0,Related Work,"Cardiac MRI. Cardiac magnetic resonance imaging (MRI) is often the preferred imaging modality for the assessment of function and structure of the cardiovascular system. This is in equal parts due to its non-invasive nature, and due to its high spatial and temporal resolution capabilities. The short-axis (SAX) view is a (3+t)-dimensional volume made up of stacked cross-sectional (2D+t) acquisitions which lay orthogonal to the ventricle's long axis (see Fig. 1). Spatial resolution is highest in-plane (typically <3 mm 2 ), with a much lower inter-slice resolution (10 mm), and a temporal resolution of ≤45 ms [15]. On the other hand, long-axis (LAX) views are (2D+t) acquisitions orthogonal to the SAX plane and provide high resolution along the ventricle's long axis.Image Segmentation. The capabilities of the CNN has caused it to become the predominant choice for image segmentation tasks [8,20]. However, a pitfall of these models is their poor generalization to certain input transformations. One such transformation is scaling. This drawback limits the use of CNNs on domains with large variations in pixel spacings. Past works have attempted to mitigate this issue by accounting for dataset characteristics [11], building resilience through augmentations [29], or using multi-scale feature extractors [5].Additionally, segmentation performed by fully convolutional model is restricted to predicting in pixel (or voxel) grids. This requires post-processing heuristics to extract smooth object surfaces. Works such as [12,19] try to mitigate this issue through point-wise decoders that operate on interpolated convolutional features. Alternatives to binarized segmentation have been recently proposed such as soft segmentations [7] and distance field predictions [6,24]. Smoothness can also be improved by predicting at higher resolutions. This is however limited by the exponential increase of memory that comes with high-dimensional data. Partitioning of the input can make memory requirements manageable [3,9], but doing so disallows the ability to learn long-distance spatial correlations.Neural Implicit Functions. In recent years, NIFs have achieved notable milestones in the field of shape representations [17,18]. NIFs have multiple advantages over classical voxelized approaches that makes them remarkably interesting for applications in the medical imaging domain [10,28]. First, NIFs can sample shapes at any points in space at arbitrary resolutions. This makes them particularly fit for working with sparse, partial, or non-uniform data. Implicit functions thus remove the need for traditional interpolation as high-resolution shapes are learnt implicitly by the network [1]. This is specially relevant to the medical imaging community, where scans may have complex sampling strategies, have missing or unusable regions, or have highly anisotropic voxel sizes. These properties may further vary across scanners and acquisition protocols, making generalization across datasets a challenge. Additionally, the ability to process each point independently allows implicit functions to have flexible optimization strategies, making entire volumes be optimizable holistically.Image Priors. The typical application of a NIF involves the training of a multilayer perceptron (MLP) on a single scene. Although generalization still occurs in generating novel views of the target scene, the introduction of prior knowledge and conditioning of the MLP is subject to ongoing research [1,14,16,18,22,23]. Approaches such as [1,18] opt for auto-decoder architectures where the network is modulated by latent code at the input level. At inference time, the latent code of the target scene is optimized by backpropagation. Works such as [16] choose to instead modulate the network at its activation functions. Other frameworks obtain the latent code in a single-shot fashion through the use of an encoder network [14,16,22,23]. This latent code is then used by a hyper-network [14,16,23] or a meta-learning approach [22] to generate the weights of a decoder network."
NISF: Neural Implicit Segmentation Functions,3.0,Methods,"Shared Prior. In order to generalize to unseen subjects, we attempt to build a shared prior H over all subjects. This is done by conditioning the classifier with a latent vector h ∈ R d at the input level. Each individual subject j in a population X, can be thought of having a distinct h j that serves as a latent code of their unique features. Following [1,18], we initialize a matrix H ∈ R Xd , where each row is a latent vector h j corresponding to a single subject j in the dataset. The latent vector h j of a subject is fed to the MLP alongside a point's coordinate and can be optimized through back-propagation. This allows H to be optimized to capture useful inter-patient features.Model Architecture. The architecture is composed of a segmentation function f θ and a reconstruction function f φ . At each continuous-valued coordinate c ∈ R N , function f θ models the shape's segmentation probability s c for all M classes, and function f φ models the image intensity i c . The functions are conditioned by a latent vector h at the input level as follows:In order to improve local agreement between the segmentation and reconstruction functions, we jointly model f θ and f φ by a unique multi-layer perceptron (MLP) with two output heads (Fig. 2). We employ Gabor wavelet activation functions [21] which are known to be more expressive than Fourier Features combined with ReLU [26] or sinusoidal activation functions [23].Prior Training. Following the setup described in [1], we randomly initialize the matrix H consisting of a trainable latent vector h j ∼ N 0, 10 -2 for each subject in the training set. On each training sample, the parameters of the MLP are jointly optimized with the subject's h j . We select a training batch by uniformly sampling a time frame t and using all points within that 3D volume. Each voxel in the sample is processed in parallel along the batch dimension. Coordinates are normalized to the range [0, 1] based on the voxel's relative position.The difference in image reconstruction from the ground-truth voxel intensities is supervised using binary cross-entropy (BCE). This is motivated by our data's voxel intensity distribution being heavily skewed towards the extremes. The segmentation loss is a sum of a BCE loss component and a Dice loss component. We found that adding a weighting factor of α = 10 to the image reconstruction loss component yielded inference-time improvements on both image reconstruction and segmentation metrics. Additionally, L2 regularization is applied to the latent vector h j and the MLP's parameters. The full loss is summarized as follows:Inference. Once the segmentation function f θ has learnt a mapping from the population prior H to the segmentation space S, inference becomes a task of finding a latent code h within H that correctly models the new subject's features. The ground-truth segmentation of a new subject is obviously not available at inference, and it is thus not possible to use f θ to optimize h. However, since both functions f φ (image reconstruction) and f θ (segmentation) have been jointly trained by consistently using the same latent vector h, we make the following assumption: A latent code h optimized for image reconstruction under f φ will also produce accurate segmentations under f θ . This assumption makes it possible to use the image reconstruction function f φ alone to find a latent code h for an unseen image in order to generalize segmentation predictions using f θ .For this task, a new h ∼ N 0, 10 -4 is initialized. The weights of the MLP are frozen, such that the only tuneable parameters are those of h. Optimization is performed exclusively on the image reconstruction loss (dashed green line in Fig. 2):Due to the loss being composed exclusively by the image reconstruction term, h is expected to eventually overfit to f φ . Special care should be taken to find a step-number hyperparameter that stops the optimization of h at the optimal segmentation performance. In our experiments, we chose this parameter based on the Dice score of the best validation run. "
NISF: Neural Implicit Segmentation Functions,4.0,Experiments and Results,"Data Overview. The dataset consists of a random subset of 1150 subjects from the UK Biobank's short-axis cardiac MRI acquisitions [25]. An overview of the UK Biobank cohort's baseline statistics can be found in their showcase website [27]. The dataset split included 1000 subjects for the prior training, 50 for validation, and 100 for testing. The (3D+t) short-axis volumes are anisotropic in nature and have a wide range of shapes and pixel spacings along the spatial dimensions. No form of preprocessing was performed on the images except for an intensity normalization to the range [0, 1] as performed in similar literature [2]. The high dimensionality of (3D+t) volumes makes manual annotation prohibitively time consuming. Due to this, we make use of synthetic segmentation as ground truth shapes created using a trained state of the art segmentation CNN provided by [2]. The object of interest in each scan is composed of three distinct, mutually exclusive sub-regions: The left ventricle (LV) blood pool, LV myocardium, and right ventricle (RV) blood pool (see Fig. 1).Implementation Details. The architecture consists of 8 residual layers, each with 128 hidden units. The subject latent codes had 128 learnable parameters. The model was implemented using Pytorch and trained on an NVIDIA A40 GPU for 1000 epochs, lasting approximately 9 days. Inference optimization lasted 3-7 minutes per subject depending on volume dimensions. Losses are minimized using the ADAM optimizer [13] using a learning rate of 10 -4 during the prior training and 10 -4 during inference."
NISF: Neural Implicit Segmentation Functions,,Results.,"As the latent code is optimized during inference, segmentation metrics follow an overfitting pattern (see Fig. 3). This is an expected consequence of the inference process optimizing solely on the image reconstruction loss. Early stopping should be employed to obtain the best performing latent code state.The benefits of training a prior over the population is investigated by tracking inference-time Dice scores obtained from spaced-out validation runs. Training of the prior is shown to significantly improve performance of segmentation and image reconstruction at inference-time as seen in Fig. 4.Validation results showed the average optimal number of latent code optmimization steps at inference to be 672. Thus, the test set per-class Dice scores (Table 1) were obtained after 672 optimization steps on h for each test subject.     Further investigation is performed on the generalization capabilities of the subject prior by producing segmentations for held-out sections of the image volume. First, the subject's latent code is optimized using the inference process. Then, the model's output is sampled at the held-out region's coordinates.Right ventricle segmentation in basal slices is notoriously challenging to manually annotate due to the delineation of the atrial and ventricular cavity combined with the sparsity of the resolution along the long axis [4]. Nonetheless, as seen in Fig. 5, our approach is capable of capturing smooth and plausible morphology of these regions despite not having access to the image information.We go on to show NISF's ability to generate high-resolution segmentation for out-of-plane views. We optimize on a short-axis volume at inference and subsequently sample coordinates corresponding to long-axis views. Despite never presenting a ground-truth long-axis image, the model reconstructs an interpolated view and provides an accurate segmentation along its plane (Fig. 6)."
NISF: Neural Implicit Segmentation Functions,5.0,Conclusion,"We present a novel family of image segmentation models that can model shapes at arbitrary resolutions. The approach is able to leverage priors to make predictions for regions not present in the original image data. Working directly on the coordinate space has the benefit of accepting high-dimensional sparse data, as well as not being affected by variations in image shapes and resolutions. We implement a simple version of this framework and evaluate it on a short-axis cardiac MRI segmentation task using the UK Biobank. Reported Dice scores on 100 unseen subjects average 0.87 ± 0.045. We also perform a qualitative analysis on the framework's ability to predict held-out sections of image volumes."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,1.0,Introduction,"Extracting brain tumors from medical image scans plays an important role in further analysis and clinical diagnosis. Typically, a brain tumor includes peritumoral edema, enhancing tumor, and non-enhancing tumor core. Since different modalities present different clarity of brain tumor components, we often use multi-modal image scans, such as T1, T1c, T2, and Flair, in the task of brain tumor segmentation [12]. Works have been done to handle brain tumor segmentation using image scans collected from all four modalities [11,15]. However, in practice, we face the challenge of collecting all modalities at the same time, with often one or more missing. Therefore, in this paper, we consider the problem of segmenting brain tumors with missing image modalities.Current image segmentation methods for handling missing modalities can be divided into three categories, including: 1) brute-force methods: designing individual segmentation networks for each possible modality combination [18], 2) completion methods: synthesizing the missing modalities to complete all modalities required for conventional image segmentation methods [16], and 3) fusionbased methods: mapping images from different modalities into the same feature space for fusion and then segmenting brain tumors based on the fused features [10]. Methods in the first category have good segmentation performance; however, they are resource intensive and often require more training time. The performance of methods in the second category is limited by the synthesis quality of the missing modality. The third category often has one single network to take care of different scenarios of missing modalities, which is the most commonly used one in practice.To handle various numbers of modal inputs, HeMIS [5] projects the image features of different modalities into the same feature space, by computing the mean and variance of the feature maps extracted from different modalities as the fused features. To improve the representation of feature fusion, HVED [3] treats the input of each modality as a Gaussian distribution, and fuses feature maps from different modalities through a Gaussian mixture model. RobustSeg [1], on the other hand, decomposes the modality features into modality-invariant content code and modality-specific appearance code, for more accurate fusion and segmentation. Considering the different clarity of brain tumor regions observed in different modalities, RFNet [2] introduces an attention mechanism to model the relations of modalities and tumor regions adaptively. Based on graph structure and attention mechanism, MFI [21] is proposed to learn adaptive complementary information between modalities in different missing situations.Due to the complexity of current models, we tend to develop a simple model, which adopts a simple average fusion and attention mechanism. These two techniques are demonstrated to be effective in handling missing modalities and multimodal fusion [17]. Inspired by MAML [20], we propose a model called A2FSeg (Average and Adaptive Fusion Segmentation network, see Fig. 1), which has two fusion steps, i.e., an average fusion and an attention-based adaptive fusion, to integrate features from different modalities for segmentation. Although our fusion idea is quite simple, A2FSeg achieves state-of-the-art (SOTA) performance in the incomplete multimodal brain tumor image segmentation task on the BraTS2020 dataset. Our contributions in this paper are summarized below:-We propose a simple multi-modal fusion network, A2FSeg, for brain tumor segmentation, which is general and can be extended to any number of modalities for incomplete image segmentation. -We conduct experiments on the BraTS 2020 dataset and achieve the SOTA segmentation performance, having a mean Dice core of 89.79% for the whole tumor, 82.72% for the tumor core, and 66.71% for the enhancing tumor."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,2.0,Method,"Figure 1 presents the network architecture of our A2FSeg. It consists of four modality-specific sub-networks to extract features from each modality, an average fusion module to simply fuse features from available modalities at the first stage, and an adaptive fusion module based on an attention mechanism to adaptively fuse those features again at the second stage.Modality-Specific Feature Extraction (MSFE) Module. Before fusion, we first extract features for every single modality, using the nnUNet model [7] as shown in Fig. 1. In particular, this MSFE model takes a 3D image scan from a specific modality m, i.e., I m ∈ R H×W ×D and m ∈ {T1, T2, T1c, Flair}, and outputs the corresponding image featuresHere, the number of channels is C = 32; H f , W f , and D f are the height, width, and depth of feature maps F m , which share the same size as the input image. For every single modality, each MSFE module is supervised by the image segmentation mask to fasten its convergence and provide a good feature extraction for fusion later. All four MSFEs have the same architecture but with different weights.Average Fusion Module. To aggregate image features from different modalities and handle the possibility of missing one or more modalities, we use the average of the available features from different modalities as the first fusion result. That is, we obtain a fused average featureHere, N m is the number of available modalities. For example, as shown in Fig. 1, if only the first two modalities are available at an iteration, then N m = 2, and we will take the average of these two modalities, ignoring those missing ones.Adaptive Fusion Module. Since each modality contributes differently to the final tumor segmentation, similar to MAML [20], we adopt the attention mechanism to measure the voxel-level contributions of each modality to the final segmentation. As shown in Fig. 1, to generate the attention map for a specific modality m, we take the concatenation of its feature extracted by the MSFE module F m and the mean feature after the average fusion F, which is passed through a convolutional layer to generate the initial attention weights:Here, F m is a convolutional layer for this specific modality m, and θ m represents the parameters of this layer, and σ is a Sigmoid function. That is, we have an individual convolution layer F m for each modality to generate different weights.Due to the possibility of missing modalities, we will have different numbers of feature maps for fusion. To address this issue, we normalize the different attention weights by using a Softmax function:That is, we only consider feature maps from those available modalities but normalize their contribution to the final fusion result, so that, the fused one has a consistent value range, no matter how many modalities are missing. Then, we perform voxel-wise multiplication of the attention weight with the corresponding modal feature maps. As a result, the adaptively fused feature maps F is calculated by the weighted sum of each modal feature:Here, ⊗ indicates the voxel-wise multiplication.Loss Function. We have multiple segmentation heads, which are distributed in each module of A2FSeg. For each segmentation head, we use the combination of the cross-entropy and the soft dice score as the basic loss function, which is defined aswhere ŷ and y represent the segmentation prediction and the ground truth, respectively. Based on this basic one, we have the overall loss function defined aswhere the first term is the basic segmentation loss for each modality m after feature extraction; the second term is the loss for the segmentation output of the average fusion module; and the last term is the segmentation loss for the final output from the adaptive fusion module. 3 Experiments"
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.1,Dataset,"Our experiments are conducted on BraTS2020, which contains 369 multicontrast MRI scans with four modalities: T1, T1c, T2, and Flair. These images went through a sequence of preprocessing steps, including co-registration to the same anatomical template, resampling to the same resolution (1 mm 3 ), and skullstripping. The segmentation masks have three labels, including the whole tumor (abbreviated as Complete), tumor core (abbreviated as Core), and enhancing tumor (abbreviated as Enhancing). These annotations are manually provided by one to four radiologists according to the same annotation protocol."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.2,Experimental Settings and Implementation Details,"We implement our model with PyTorch [13] and perform experiments on an Nvidia RTX3090 GPU. We use the Adam optimizer [8], with an initial learning rate of 0.01. Since we use the method of exponential decay of learning rate, the initial learning rate is then multiplied by (1 -#epoch #max_epoch ) 0.9 . Due to the limitation of GPU memory, each volume is randomly cropped into multiple patches with the size of 128 × 128 × 128 for training. The network is trained for 400 epochs. In the inference stage, we use a sliding window to produce the final segmentation prediction of the input image."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.3,Experimental Results and Comparison to Baseline Methods,"To evaluate the performance of our model, we compare it with four recent models, HeMIS [5], U-HVED [3], mmFormer [19], and MFI [21]. The dataset is randomly split into 70% for training, 10% for validation, and 20% for testing, and all methods are evaluated on the same dataset and data splitting. We use the Dice score as the metric. As shown in Table 1, our method achieves the best result. For example, our method outperforms the current SOTA method MFI [21] in most missing-modality cases, including all cases for the whole/complete tumor, 8 out of 15 cases for the tumor core, 12 out of 15 cases for the enhancing tumor. Compared to MFI, for the whole tumor, tumor core, and enhancing tumor regions, we improve the average Dice scores by 0.99%, 0.41%, and 0.77%, respectively. Although the design of our model is quite simple, these results demonstrate its effectiveness for the incomplete multimodel segmentation task of brain tumors. Figure 2 visualizes the segmentation results of samples from the BraTS2020 dataset. With only one Flair image available, the segmentation results of the tumor core and enhancing tumor are poor, because little information on these two regions is observed in the Flair image. With an additional T1c image, the segmentation results of these two regions are significantly improved and quite close to the ground truth. Although adding T1 and T2 images does not greatly improve the segmentation of the tumor core and the enhancing tumor, the boundary of the whole tumor is refined with their help.Figure 3 visualizes the contribution to each tumor region from each modality. The numbers are the mean values of the attention maps computed for images in the test set. Overall, in our model, each modality has its contribution to the final segmentation, and no one dominates the result. This is because we have supervision on the segmentation branch of each modality, so that, each modality has the ability to segment each region to some extent. However, we still observe that Flair and T2 modalities have relatively larger contributions to the segmentation of all tumor regions, followed by T1c and then T1. This is probably because the whole tumor area is much clear in Flair and T2 compared to the other two modalities. Each modality shows its preference when segmenting different regions. Flair and T2 are more useful for extracting the peritumoral edema (ED) than the enhancing tumor (ET) and the non-enhancing tumor and  necrosis (NCR/NET); while T1c and T1 are on the opposite and more helpful for extracting ET and NCR/NET."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.4,Ablation Study,"In this part, we investigate the effectiveness of the average fusion module and the adaptive fusion module, which are two important components of our method. Firstly, we set a baseline model without any modal interaction, that is, with the average fusion module only. Then, we add the adaptive fusion module to the baseline model. Table 2 reports this ablation study. With only adding the average fusion module, our method already obtains comparable performance with the current SOTA method MFI. By adding the adaptive fusion module, the dice scores of the three regions further increase by 0.50%, 0.72%, and 0.71%, respectively. This shows that both the average fusion module and the adaptive fusion module are effective in this brain tumor segmentation task."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,4.0,Discussion and Conclusion,"In this paper, we propose an average and adaptive fusion segmentation network (A2FSeg) for the incomplete multi-model brain tumor segmentation task. The essential components of our A2FSeg network are the two stages of feature fusion, including an average fusion and an adaptive fusion. Compare to existing complicated models, our model is much simpler and more effective, which is demonstrated by the best performance on the BraTS 2020 brain tumor segmentation task. The experimental results demonstrate the effectiveness of two techniques, i.e., the average fusion and the attention-based adaptive one, for incomplete modal segmentation tasks.Our study brings up the question of whether having complicated models is necessary. If there is no huge gap between different modalities, like in our case where all four modalities are images, the image feature maps are similar and a simple fusion like ours can work. Otherwise, we perhaps need an adaptor or an alignment strategy to fuse different types of features, such as images and audio.Also, we observe that a good feature extractor is essential for improving the segmentation results. In this paper, we only explore a reduced UNet for feature extraction. In future work, we will explore other feature extractors, such as Vision Transformer (ViT) or other pre-trained visual foundation models [4,6,14]. Recently, the segment anything model (SAM) [9] demonstrates its general ability to extract different regions of interest, which is promising to be adopted as a good starting point for brain tumor segmentation. Besides, our model is general for multi-modal segmentation and we will apply it to other multi-model segmentation tasks to evaluate its generalization on other applications."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,1.0,Introduction,"Diffuse glioma is a common malignant tumor with highly variable prognosis across individuals. To improve survival outcomes, many pre-operative survival prediction methods have been proposed with success. Based on the prediction results, personalized treatment can be achieved. For instance, Isensee et al. [1] proposed a random forest model [2], which adopts the radiomics features [3] of the brain tumor images, to predict the overall survival (OS) time of diffuse glioma patients. Nie et al. [4] developed a multi-channel 3D convolutional neural network (CNN) [5] to learn features from multimodal MR brain images and classify OS time as long or short using a support vector machine (SVM) model [6]. In [7], an end-to-end CNN-based method was presented that uses multimodal MR brain images and clinical features such as Karnofsky performance score [8] to predict OS time. In [9], an imaging phenotype and genotype based survival prediction method (PGSP) was proposed, which integrates tumor genotype information to enhance prediction accuracy.Despite the promising results of existing pre-operative survival prediction methods, they often overlook clinical knowledge that could aid in improving the prediction accuracy. Notably, tumor types have been found to be strongly correlated with the prognosis of diffuse glioma [10]. Unfortunately, tumor type information is unavailable before craniotomy. To address this limitation, we propose a new pre-operative survival prediction method that integrates a tumor subtyping network into the survival prediction backbone. The subtyping network is responsible for learning tumor-type-related features from pre-operative multimodal MR brain images. Concerning the inherent issue of imbalanced tumor types in the training data collected in clinic, a novel ordinal manifold mixup based feature augmentation is presented and applied in the training stage of the tumor subtyping network. Unlike the original manifold mixup [11], which ignores the feature distribution of different classes, in the proposed ordinal manifold mixup, feature distribution of different tumor types is encouraged to be in the order of risk grade, and the augmented features are produced between neighboring risk grades. In this way, inconsistency between the augmented features and the corresponding labels can be effectively reduced.Our method is evaluated using pre-operative multimodal MR brain images of 1726 diffuse glioma patients collected from cooperation hospitals and a public dataset BraTS2019 [12] containing multimodal MR brain images of 210 patients. Our method achieves the highest prediction accuracy of all state-of-the-art methods under evaluation. In addition, ablation study further confirms the effectiveness of the proposed tumor subtyping network and the ordinal manifold mixup."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.0,Methods,"Diffuse glioma can be classified into three histological types: the oligodendroglioma, the astrocytoma, and the glioblastoma [10]. The median survival times (in months) are 119 (oligodendroglioma), 36 (astrocytoma), and 8 (glioblastoma) [13]. So the tumor types have strong correlation with the prognosis of diffused glioma. Based on this observation, we propose a new pre-operative survival prediction method (see Fig. 1). Our network is composed of two parts: the survival prediction backbone and the tumor subtyping network. The survival prediction backbone is a deep Cox proportional hazard model [14] which takes the multimodal MR brain images of diffuse glioma patients as inputs and predicts the corresponding risks. The tumor subtyping network is a classification network, which classifies the patient tumor types and feeds the learned tumor-type-related features to the backbone to enhance the survival prediction performance.The tumor subtyping network is trained independently before being integrated into the backbone. To solve the inherent issue of imbalanced tumor type in the training data collected in clinic, a novel ordinal manifold mixup based feature augmentation is applied in the training of the tumor subtyping network. It is worth noting that the ground truth of tumor types, which is determined after craniotomy, is available in the training data, while for the testing data, tumor types are not required, because tumor-type-related features can be learned from the pre-operative multimodal MR brain images."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.1,The Survival Prediction Backbone,"The architecture of the survival prediction backbone, depicted in Fig. 1 top, consists of an encoder E cox with four ResBlocks [15], a global average pooling layer (GAP), and three fully connected (FC) layers. Assume that D = {x 1 , ..., x N } is the dataset containing pre-operative multimodal MR brain images of diffuse glioma patients, and N is the number of patients. The backbone is responsible for deriving features from x i to predict the risk of the patient. Moreover, after the GAP of the backbone, the learned featurefrom the tumor subtyping network (discussed later), and M is the vector dimension which is set to 128. As f type i has strong correlation with prognosis, the performance of the backbone can be improved. In addition, information of patient age and tumor position is also used. To encode the tumor position, the brain is divided into 3 × 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or 1) with each value for one block. If a block contains tumors, then the corresponding binary value is 1, otherwise is 0. The backbone is based on the deep Cox proportional hazard model, and the loss function is defined as:where h θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ stands for the parameters of the backbone, x i is the input multimodal MR brain images of the i-th patient, R(t i ) is the risk group at time t i , which contains all patients who are still alive before time t i , t i is the observed time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored patient."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.2,The Tumor Subtyping Network,"The tumor subtyping network has almost the same structure as the backbone.It is responsible for learning tumor-type-related features from each input preoperative multimodal MR brain image x i and classifying the tumor into oligodendroglioma, astrocytoma, or glioblastoma. The cross entropy is adopted as the loss function of the tumor subtyping network, which is defined as:where y k i and p k i are the ground truth (0 or 1) and the prediction (probability) of the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. The learned tumor-type-related feature f type i ∈ R M is fed to the survival prediction backbone and concatenated with f cox i learned in the backbone to predict the risk. In the in-house dataset, the proportions of the three tumor types are 20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which is consistent with the statistical report in [13]. To solve the imbalance issue of tumor types in the training of the tumor subtyping network, a novel ordinal manifold mixup based feature augmentation is presented."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.3,The Ordinal Manifold Mixup,"In the original manifold mixup [11], features and the corresponding labels are augmented using linear interpolation on two randomly selected features (e.g., f i and f j ). Specifically, the augmented feature fi∼j and label ȳi∼j is defined as:where y k i and y k j stand for the labels of the k-th tumor type of the i-th and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. For binary classification, the original manifold mixup can effectively enhance the network performance, however, for the classification of more than two classes, e.g., tumor types, there exists a big issue.As shown in Fig. 2 left, assume that f i and f j are features of oligodendroglioma (green) and astrocytoma (yellow) learned in the tumor subtyping network, respectively. The augmented feature fi∼j (red) produced from the linear interpolation between f i and f j has the corresponding label ȳi∼j with high probabilities for the tumor types of oligodendroglioma and astrocytoma. However, since these is no constraint imposed on the feature distribution of different tumor types, fi∼j could fall into the distribution of glioblastoma (blue) as shown in Fig. 2 left. In this case, fi∼j and ȳi∼j are inconsistent, which could influence the training and degrade the performance of the tumor subtyping network. As aforementioned, the survival time of patients with different tumor types varies largely (oligodendroglioma > astrocytoma > glioblastoma), so the tumor types can be regarded as risk grade, which are ordered rather than categorical. Based on this assertion and inspired by [16], we impose an ordinal constraint on the tumor-type-related features to make the feature distribution of different tumor types in the order of risk grade. In this way, the manifold mixup strategy can be applied between each two neighboring tumor types to produce augmented features with consistent labels that reflect reasonable risk (see Fig. 2 "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,right).,"Normally, the feature distribution of each tumor type is assumed to be independent normal distribution, so their joint distribution is given by:where F k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma, and glioblastoma, respectively, μ k and σ 2 k are mean and variance of F k . To impose the ordinal constraint, we define the desired feature distribution of each tumor type as N (μ 1 , σ21 ) for k = 1, and N (μ k-1 + Δ k , σ2 k ) for k = 2 and 3. In this way, the feature distribution of each tumor type depends on its predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to the mean feature of its predecessor μk-1 shifted by Δ k . Note that Δ k is set to be larger than 3 × σk to ensure the desired ordering [16]. In this way, the conditional distribution under the ordinal constraint is defined as:which can be represented as:where μ1 and σk , k = 1, 2, 3 can be learned by the tumor subtyping network. Finally, the ordinal loss, which is in the form of KL divergence, is defined as:In our method, μ k and σ 2 k are calculated bywhere Φ θ and G are the encoder and GAP of the tumor subtyping network, respectively, θ is the parameter set of the encoder,stands for the subset containing the pre-operative multimodal MR brain images of the patients with the k-th tumor type, N k is the patient number in D k . So we impose the ordinal loss L KL to the features after the GAP of the tumor subtyping network as shown in Fig. 1. Since the ordinal constraint, feature distribution of different tumor types learned in the subtyping network is encouraged to be in the ordered of risk grade, and features can be augmented between neighboring tumor type. In this way, inconsistency between the resulting augmented features and labels can be effectively reduced.The tumor subtyping network is first trained before being integrated into the survival prediction backbone. In the training stage of the tumor subtyping network, each input batch contains pre-operative multimodal MR brain images of N patients and can be divided into K = 3 subsets according to their corresponding tumor types, i.e., D k , k = 1, 2, 3. With the ordinal constrained feature distribution, high consistent features can be augmented between neighboring tumor types. Based on the original and augmented features, the performance of the tumor subtyping network can be enhanced.Once the tumor subtyping network has been trained, it is then integrated into the survival prediction backbone, which is trained under the constraint of the cox proportional hazard loss L Cox ."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.0,Results,"In our experiment, both in-house and public datasets are used to evaluate our method. Specifically, the in-house dataset collected in cooperation hospitals contains pre-operative multimodal MR images, including T1, T1 contrast enhanced (T1c), T2, and FLAIR, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse glioma types. The patient number of each tumor type is 361 (oligodendroglioma), 495 (astrocytoma), and 870 (glioblastoma), respectively. In the 1726 patients, 743 have the corresponding overall survival time (dead, non-censored), and 983 patients have the last visiting time (alive, censored). Besides the inhouse dataset, a public dataset BraTS2019, including pre-operative multimodal MR images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the external independent testing dataset. All images of the in-house and BraTS2019 datasets go through the same pre-processing stage, including image normalization and affine transformation to MNI152 [17]. Based on the tumor mask of each image, tumor bounding boxes can be calculated. According to the bounding boxes of all 1936 patients, the size of input 3D image patch is set to 96 × 96 × 64 voxels, which can cover the entire tumor of every patient.Besides our method, four state-of-the-art methods, including random forest based method (RF) [18], deep convolutional survival model (deepConvSurv) [19], multi-channel survival prediction method (MCSP) [20], and imaging phenotype and genotype based survival prediction method (PGSP) [9], are evaluated. It is worth noting that in the RF method, 100 decision trees and 390 handcrafted radiomics features are used. The output of RF, MCSP, and PGSP is the overall survival (OS) times in days, while for deepConvSurv and our method, the output is the risk (deep Cox proportional hazard models). Concordance index (C-index) is adopted to quantify the prediction accuracy:where D = {x 1 , ..., x N } is the dataset containing all patients, T i and T j are ground truth of survival times of the i-th and j-th patients, R i and R j are the days predicted by RF, MCSP, and PGSP or risks predicted by the deep Cox proportional hazard models (i.e., deepConvSurv and our method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is censored or non-censored. As RF, MCSP, and PGSP cannot use the censored data in the in-house dataset, 80% of the non-censored data (594 patients) are randomly selected as the training data, and the rest 20% non-censored data (149 patients) are for testing. While deepConvSurv and our method are deep Cox models, both censored and non-censored patients can be utilized. So besides the 80% non-censored patients, all censored data (983 patients) are also included in the training data.Table 1 shows the evaluation results of the in-house and the external independent (BraTS2019) testing datasets using all methods under evaluation. Our method achieves the highest C-index of all the methods under evaluation. Moreover, comparing with deepConvSurv, our method can improve the prediction accuracy up to 10% (in-house) and 8% (BraTS2019)."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.1,Ablation Study of Survival Prediction,"To show the effect of the tumor subtyping network and the ordinal manifold mixup in survival prediction, our method without the tumor subtyping network (Baseline-1) and our method with the tumor subtyping network (using original manifold mixup instead, Baseline-2) are evaluated. For the in-house dataset, the resulting C-indices are 0.744 (Baseline-1) and 0.735 (Baseline-2). So our method make the improvement of C-index more than 8% comparing with Baseline-2. For the external independent testing dataset BraTS2019, the resulting C-indices are 0.738 (Baseline-1) and 0.714 (Baseline-2), and our method still has more than 6% improvement comparing with Baseline-2.Figure 3 shows the distributions of tumor-type-related features (after the GAP) of the in-house testing data in Baseline-2, and our method. Principal component analysis [21] is used to project features to a 2D plane. Since the ordinal constraint, the feature distribution of different tumor types is in the order of risk grade using our method, which cannot be observed in Baseline-2. "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,4.0,Conclusions,"We proposed a new method for pre-operative survival prediction of diffuse glioma patients, where a tumor subtyping network is integrated into the prediction backbone. Based on the tumor subtyping network, tumor type information, which are only available after craniotomy, can be derived from the pre-operative multimodal MR images to boost the survival prediction performance. Moreover, a novel ordinal manifold mixup was presented, where ordinal constraint is imposed to make feature distribution of different tumor types in the order of risk grade, and feature augmentation only takes place between neighboring tumor types. In this way, inconsistency between the augmented features and corresponding labels can be effectively reduced. Both in-house and public datasets containing 1936 patients were used in the experiment. Our method outperformed the state-of-the-art methods in terms of the concordance-index."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,1.0,Introduction,"Segmentation of skin lesions from dermoscopy images is a critical task in disease diagnosis and treatment planning of skin cancers [17]. Manual lesion segmentation is time-consuming and prone to inter-and intra-observer variability. To improve the efficiency and accuracy of clinical workflows, numerous automated skin lesion segmentation models have been developed over the years [1,2,7,10,18,19,21]. These models have focused on enhancing feature representations using various techniques such as multi-scale feature fusion [10], attention mechanisms [1,7], self-attention mechanisms [18,19], and boundary-aware attention [2,18,19], resulting in significant improvements in skin lesion segmentation performance. Despite these advances, the segmentation of skin lesions with ambiguous boundaries, particularly at extremely challenging scales, remains a bottleneck issue that needs to be addressed. In such cases, even state-of-the-art segmentation models struggle to achieve accurate and consistent results."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,,Boundary Evolution Image,"Small Large Fig. 1. The boundary evolution process. It could be seen that various lesions can be accurately segmented by splitting the segmentation into sequential timesteps (t), named as boundary evolution in this work.Two representative boundaries are visualized in Fig. 1, where one extremely small lesion and one particularly large lesion are presented. The small one covers 1.03% in the image space and the large one covers 72.96%. As studied prior, solving the segmentation problems of such two types of lesions have different strategies. (1) For the small lesions, translating the features at a lower depth to the convolutional layers at a higher depth can avoid losing local contexts [10]. (2) For the large lesions, enlarging the receptive field by dilated convolution [1], and even global attention [18] can capture the long-range dependencies to improve the boundary decision. Besides the challenge of how to yield stable representations for various scales, multi-scale lesions will cause training fluctuation, that is, small lesions usually lead to large Dice loss. Feeding more boundary-aware supervision can reduce these negative effects to some degree [2,19]. The latest transformer, Xbound-Former, comprehensively addresses the multi-scale boundary problem through cross-scale boundary learning and exactly reaches higher performance on whatever small or large lesions.However, current models for skin lesion segmentation are still struggling with extremely challenging cases, which are often encountered in clinical practice. While some approaches aim to optimize the model architecture by incorporating local and global contexts and multi-task supervision, and others seek to improve performance by collecting more labeled data and building larger models, both strategies are costly and can be limited by the inherent complexity of skin lesion boundaries. Therefore, we propose a novel approach that shifts the focus from merely segmenting lesion boundaries to predicting their evolution. Our approach is inspired by recent advances in image synthesis achieved by diffusion probabilistic models [6,9,14,15], which generate synthetic samples from a randomly sampled Gaussian distribution in a series of finite steps. We adapt this process to model the evolution of skin lesion boundaries as a parameterized chain process, starting from Gaussian noise and progressing through a series of denoising steps to yield a clear segmentation map with well-defined lesion boundaries. By predicting the next step in the chain process rather than the final segmentation map, our approach enables the more accurate segmentation of challenging lesions than previous models. We illustrate the process of boundary evolution in Fig. 1, where each row corresponds to a different step in the evolution process, culminating in a clear segmentation map with well-defined boundaries.In this paper, we propose a Medical Boundary Diff usion model (MB-Diff ) to improve the skin lesion segmentation, particularly in cases where the lesion boundaries are ambiguous and have extremely large or small sizes. The MB-Diff model follows the basic design of the plain diffusion model, using a sequential denoising process to generate the lesion mask. However, it also includes two key innovations: Firstly, we have developed an efficient multi-scale image guidance module, which uses a pretrained transformer encoder to extract multi-scale features from prior images. These features are then fused with the evolution features to constrain the direction of evolution. Secondly, we have implemented an evolution uncertainty-based fusion strategy, which takes into account the uncertainty of different initializations to refine the evolution results and obtain more precise lesion boundaries. We evaluate our model on two popular skin lesion segmentation datasets, ISIC-2016 and PH 2 datasets, and find that it performs significantly better than existing models."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.0,Method,"The key objective of MB-Diff is to improve the representation of ambiguous boundaries by learning boundary evolution through a cascaded series of steps, rather than a single step. In this section, we present the details of our cascaded boundary evolution learning process and the parameterized architecture of the evolution process. We also introduce our evolution-based uncertainty estimation and boundary ensemble techniques, which have significant potential for enhancing the precision and reliability of the evolved boundaries."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.1,Boundary Evolution Process,"We adopt a step-by-step denoising process to model boundary evolution in MB-Diff, drawing inspiration from recent diffusion probabilistic models (DPMs). Specifically, given the image and boundary mask distributions as (X , Y), assuming that the evolution consists of T steps in total, the boundary at T -th step (y T ) is the randomly initialized noise and the boundary at 0-th (y 0 ) step denotes the accurate result. We formulate the boundary evolution process as follows:where p(y T ) = N (y T ; 0, I) is the initialized Gaussian distribution and p θ (y t-1 |y t ) is each learnable evolution step, formulated as the Gaussian transition, denoted as:Note that the prediction function takes the input image as a condition, enabling the evolving boundary to fit the corresponding lesion accurately. By modeling boundary evolution as a step-by-step denoising process, MB-Diff can effectively capture the complex structures of skin lesions with ambiguous boundaries, leading to superior performance in lesion segmentation.To optimize the model parameters θ, we use the evolution target as an approximation of the posterior at each evolution step. Given the segmentation label y as y 0 , the label is gradually added by a Gaussian noise as:where {β t } T t=1 is a set of constants ranging from 0 to 1. After that, we compute the posterior q(y t-1 |y t , y 0 ) using Bayes' rule. The MSE loss function is utilized to measure the distance between the predicted mean and covariance of the Gaussian transition distribution and the evolution target q(y t-1 |y t , y 0 )."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.2,Paramterized Architecture with Image Prior,"The proposed model is a parameterized chain process that predicts the μ * t-1 and * t-1 at each evolution step t under the prior conditions of the image x and the prior evolution y * t . To capture the deep semantics of these conditions and perform efficient fusion, we adopt a basic U-Net [16] architecture inspired by the plain DPM and introduce novel designs for condition fusion, that is the efficient multi-scale image guidance module.The architecture consists of a multi-level convolutional encoder and a symmetric decoder with short connection layers between them. To incorporate the variable t into the model, we first embed it into the latent space. Then, the prior evolution y * t is added to the latent t before each convolution. At the bottleneck layer, we fuse the evolution features with the image guidance to constrain the evolution and ensure that the final boundary suits the conditional image.To achieve this, priors train a segmentation model concurrently with the evolution model and use an attention-based parser to translate the image features in the segmentation branch into the evolution branch [22]. Since the segmentation model is trained much faster than the evolution model, we adopt a pretrained pyramid vision transformer (PVT) [20] as the image feature extractor to obtain the multi-scale image features. Let {f l } 4 l=1 denote the extracted features at four levels, with a 2x, 4x, 8x, 16x, smaller size of the original input. Each feature at the three lower levels is resized to match the scale of f 4 using Adaptive Averaging Pooling layers. After that, the four features are concatenated and fed into a fullconnection layer to map the image feature space into the evolution space. We then perform a simple yet effective addition of the mapped image feature and the encoded prior evolution feature, similar to the fusion of time embeddings, to avoid redundant computation."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.3,Evolution Uncertainty,"Similar to typical evolutionary algorithms, the final results of boundary evolution are heavily influenced by the initialized population. As a stochastic chain process, the boundary evolution process may result in different endpoints due to the random Gaussian samples at each evolution step. This difference is particularly evident when dealing with larger ambiguity in boundary regions. The reason is that the image features in such ambiguous regions may not provide discriminative guidance for the evolution, resulting in significant variations in different evolution times. Instead of reducing the differences, we surprisingly find that these differences can represent segmentation uncertainty. Based on the evolution-based uncertainty estimation, the segmentation results become more accurate and trustworthy in practice [4,5,12].Uncertainty Estimation: To estimate uncertainty, the model parameters θ are fixed, and the evolution starts with a randomly sampled Gaussian noise y * T ∼ N (0, I). Let {y * ,i T } n i=1 denote a total of n initializations. Once the evolution is complete, the obtained {μ * ,i } n i=1 , { * ,i } n i=1 are used to the sample final lesion maps as: y * ,i = μ * ,i + exp( 1 2 * ,i )N (0, I). Unlike traditional segmentation models that typically scale the prediction into the range of 0 to 1, the evolved maps generated by MB-Diff have unfixed distributions due to random sampling. Since the final result is primarily determined by the mean value μ, and the predicted has a limited range [6], we calculate the uncertainty as:Evolution Ensemble: Instead of training multiple networks or parameters to make the ensemble, MB-Diff allows running the inference multiple times and fusing the obtained evolutions. However, simply averaging the predicted identities from multiple evolutions is not effective, as the used MSE loss without activation constrains the predicted identities to be around 0 or 1, unlike the Sigmoid function which would limit the identities to a range between 0 and 1. Therefore, we employ the max vote algorithm to obtain the final segmentation map. In this algorithm, each pixel is classified as a lesion only if its identity sum across all n evolutions is greater than a threshold value τ . Finally, the segmentation map is generated as3 Experiment"
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.1,Datasets and Evaluation Metrics,"Datasets: We use two publicly available skin lesion segmentation datasets from different institutions in our experiments: the ISIC-2016 dataset and the PH 2 dataset. The ISIC-2016 dataset [8] is provided by the International Skin Imaging Collaboration (ISIC) archive and consists of 900 samples in the public training set and 379 samples in the public validation set. As the annotation for its public test set is not currently available, we additionally collect the PH 2 dataset [13], which contains 200 labeled samples and is used to evaluate the generalization performance of our methods.Evaluation Metrics: To comprehensively compare the segmentation results, particularly the boundary delineations, we employ four commonly used metrics to quantitatively evaluate the performance of our segmentation methods. These metrics include the Dice score, the IoU score, Average Symmetric Surface Distance (ASSD), and Hausdorff Distance of boundaries (95-th percentile; HD95).To ensure fair comparison, all labels and predictions are resized to (512×512) before computing these scores, following the approach of a previous study [18]."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.2,Implementation Details,"For the diffusion model hyper-parameters, we use the default settings of the plain diffusion model, which can be found in the supplementary materials. Regarding dataset. We highlight the small lesions using dotted boxes in the third row.the training parameters, we resize all images to (256 × 256) for efficient memory utilization and computation. We use a set of random augmentations, including vertical flipping, horizontal flipping, and random scale change (limited to 0.9 ∼ 1.1), to augment the training data. We set the batch size to 4 and train our model for a total of 200,000 iterations. During training, we use the AdamW optimizer with an initial learning rate of 1e-4. For the inference, we set n = 4 and τ = 2 considering the speeds."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.3,Comparison with State-of-the-Arts,"We majorly compare our method to the latest skin lesion segmentation models, including the CNN-based and transformer-based models, i.e., U-Net++ [24], CA-Net [7], TransFuse [23], TransUNet [3], and especially the boundary-enhanced method, X-BoundFormer [18]. Additionally, we evaluate our method against MedSegDiff [22], a recently released diffusion-based model, which we re-trained for 200,000 steps to ensure a fair comparison. The quantitative results are shown in Table 1, which reports four evaluation scores for two datasets. Though the parameters of CNNs and Transformers are selected with the best performance on ISIC-2016 validation set and the parameters of our method are selected by completing the 200,000 iterations, MB-Diff still achieves the 1.18% IoU improvement and 0.7% Dice improvement. Additionally, our predicted boundaries are closer to the annotations, as evidenced by the ASSD and HD95 metrics, which reduce by 1.02 and 1.93 pixels, respectively. When compared to MedSegDiff, MB-Diff significantly outperforms it in all metrics. Moreover, our method shows a larger improvement in generalization performance on the PH 2 dataset, indicating its better ability to handle new data. We present a visual comparison of challenging samples in Fig. 2, including three samples from the ISIC-2016 validation set and three from the PH 2 dataset. These samples represent edge cases that are currently being studied in the community, including size variation, boundary ambiguity, and neighbor confusion. Our visual comparison reveals several key findings: (1) MB-Diff consistently achieves better segmentation performance on small and large lesions due to its thorough learning of boundary evolution, as seen in rows 3, 5, and 6. (2) MB-Diff is able to produce correct boundaries even in cases where they are nearly indistinguishable in human perception, eliminating the need for further manual adjustments and demonstrating significant practical value. (3) MB-Diff generates fewer false positive segmentation, resulting in cleaner predictions that enhance the user experience.Furthermore, we provide a visualization of evolution uncertainties in Fig. 2, where deeper oranges indicate larger uncertainties. It is evident that most regions with high uncertainties correspond to false predictions. This information can be used to guide human refinement of the segmentation in practical applications, ultimately increasing the AI's trustworthiness."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.4,Detailed Analysis of the Evolution,"In this subsection, we make a comprehensive analysis to investigate the performance of each component in our method and compare it to the diffusion-based model, MedSegDiff. The results of our ablation study are presented in Fig. 3(a), where ""w/o Evo"" refers to using image features to directly train a segmentation model with FPN [11] architecture and ""w/o Fusion"" means no evolution fusion is used. To ensure a fair comparison, we average the scores of multiple evolutions to represent the performance of ""w/o Fusion"". The results demonstrate that our evolutionary approach can significantly improve performance, and the evolution uncertainty-based fusion strategy further enhances performance. Comparing our method to MedSegDiff, the training loss curve in Fig. 3(b) shows that our method converges faster and achieves smaller losses, indicating that our multi-scale image guidance is more effective than that of MedSegDiff. Furthermore, we evaluate our method's performance using parameters saved at different iterations, as shown in Fig. 3(c). Our results demonstrate that our method has competitive performance at 50k iterations versus MedSegDiff at 200k iterations and our method at 100k iterations has already outperformed well-trained MedSegDiff."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,4.0,Conclusion,"In this paper, we introduced the medical boundary diffusion (MB-Diff) model, which is a novel approach to segment skin lesions. Our proposed method formulates lesion segmentation as a boundary evolution process with finite timesteps, which allows for efficient and accurate segmentation of skin lesions. To guide the boundary evolution towards the lesions, we introduce an efficient multi-scale image guidance module. Additionally, we propose an evolution uncertainty-based fusion strategy to yield more accurate segmentation. Our method is evaluated on two well-known skin lesion segmentation datasets, and the results demonstrate superior performance and generalization ability in unseen domains. Through a detailed analysis of our training program, we find that our model has faster convergence and better performance compared to other diffusion-based models. Overall, our proposed MB-Diff model offers a promising solution to accurately segment skin lesions, and has the potential to be applied in a clinical setting."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,1.0,Introduction,"Accurate tumor segmentation from medical images is essential for quantitative assessment of cancer progression and preoperative treatment planning [3]. Tumor tissues usually present different features in different imaging modalities. For example, Computed Tomography (CT) and Positron Emission Tomography (PET) are beneficial to represent morphological and metabolic information of tumors, respectively. In clinical practice, multimodal registered images, such as PET-CT images and Magnetic Resonance (MR) images with different sequences, are often utilized to delineate tumors to improve accuracy. However, manual delineation is time-consuming and error-prone, with a low inter-professional agreement [12]. These have prompted the demand for intelligent applications that can automatically segment tumors from multimodal images to optimize clinical procedures.Recently, multimodal tumor segmentation has attracted the interest of many researchers. With the emergence of multimodal datasets (e.g., BRATS [25] and HECKTOR [1]), various deep-learning-based multimodal image segmentation methods have been proposed [3,10,13,27,29,31]. Overall, large efforts have been made on effectively fusing image features of different modalities to improve segmentation accuracy. According to the way of feature fusion, the existing methods can be roughly divided into three categories [15,36]: input-level fusion, decisionlevel fusion, and layer-level fusion. As a typical approach, input-level fusion [8,20,26,31,34] refers to concatenating multimodal images in the channel dimension as network input during the data processing or augmentation stage. This approach is suitable for most existing end-to-end models [6,32], such as U-Net [28] and U-Net++ [37]. However, the shallow fusion entangles the low-level features from different modalities, preventing the effective extraction of high-level semantics and resulting in limited performance gains. In contrast, [35] and [21] propose a solution based on decision-level fusion. The core idea is to train an independent segmentation network for each data modality and fuse the results in a specific way. These approaches can bring much extra computation at the same time, as the number of networks is positively correlated with the number of modalities. As a compromise alternative, layer-level fusion methods such as HyperDense-Net [10] advocate the cross-fusion of the multimodal features in the middle layer of the network.In addition to the progress on the fusion of multimodal features, improving the model representation ability is also an effective way to boost segmentation performance. In the past few years, Transformer structure [11,24,30], centered on the multi-head attention mechanism, has been introduced to multimodal image segmentation tasks. Extensive studies [2,4,14,16] have shown that the Transformer can effectively model global context to enhance semantic representations and facilitate pixel-level prediction. Wang et al. [31] proposed TransBTS, a form of input-level fusion with a U-like structure, to segment brain tumors from multimodal MR images. TransBTS employs the Transformer as a bottleneck layer to wrap the features generated by the encoder, outperforming the traditional end-to-end models. Saeed et al. [29] adopted a similar structure in which the Transformer serves as the encoder rather than a wrapper, also achieving promising performance. Other works like [9] and [33], which combine the Transformer with the multimodal feature fusion approaches mentioned above, further demonstrate the potential of this idea for multimodal tumor segmentation.Although remarkable performance has been accomplished with these efforts, there still exist several challenges to be resolved. Most existing methods are either limited to specific modality numbers due to the design of asymmetric connections or suffer from large computational complexity because of the huge amount of model parameters. Therefore, how to improve model ability while ensuring computational efficiency is the main focus of this paper.To this end, we propose an efficient multimodal tumor segmentation solution named Hybrid Densely Connected Network (H-DenseFormer). First, our method leverages Transformer to enhance the global contextual information of different modalities. Second, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module, which can extract and fuse multimodal image features as a complement to naive input-level fusion structure. Specifically, MPE assigns an independent encoding path to each modality, then merges the semantic features of all paths and feeds them to the encoder of the segmentation network. This decouples the feature representations of different modalities while relaxing the input constraint on the specific number of modalities. Finally, we design a lightweight, Densely Connected Transformer (DCT) module to replace the standard Transformer to ensure performance and computational efficiency. Extensive experimental results on two publicly available datasets demonstrate the effectiveness of our proposed method. as the auxiliary extractor of multimodal fusion features, while the latter is used to generate predictions. Specifically, given a multimodal image input X 3D ∈ R C×H×W ×D or X 2D ∈ R C×H×W with a spatial resolution of H × W , the depth dimension of D (number of slices) and C channels (number of modalities), we first utilize MPE to extract and fuse multimodal image features. Then, the obtained features are progressively upsampled and delivered to the encoder of the segmentation network to enhance the semantic representation. Finally, the segmentation network generates multi-scale outputs, which are used to calculate deep supervision loss as the optimization target."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.2,Multi-path Parallel Embedding,"Many methods [5,10,15] have proved that decoupling the feature representation of different modalities facilitates the extraction of high-quality multimodal features. Inspired by this, we design a Multip-path Parallel Embedding (MPE) module to enhance the representational ability of the network. As shown in Fig. 1, each modality has an independent encoding path consisting of a patch embedding module, stacked Densely Connected Transformer (DCT) modules, and a reshape operation. The independence of the different paths allows MPE to handle an arbitrary number of input modalities. Besides, the introduction of the Transformer provides the ability to model global contextual information. Given the input X 3D , after convolutional embedding and tokenization, the obtained feature of the i-th path is, where i ∈ [1, 2, ..., C], p = 16 and l = 128 denote the path size and embedding feature length respectively. First, we concatenate the features of all modalities and entangle them using a convolution operation. Then, interpolation upsampling is performed to obtain the multimodal fusion featurewhere k = 128 refers to the channel dimension. Finally, F out is progressively upsampled to multiple scales and delivered to different encoder stages to enhance the learned representation."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.3,Densely Connected Transformer,"Standard Transformer structures [11] typically consist of dense linear layers with a computational complexity proportional to the feature dimension. Therefore, integrating the Transformer could lead to a mass of additional computation and memory requirements. Shortening the feature length can effectively reduce computation, but it also weakens the representation capability meanwhile. To address this problem, we propose the Densely Connected Transformer (DCT) module inspired by DenseNet [17] to balance computational cost and representation capability. Figure 1 details the DCT module, which consists of four Transformer layers and a feedforward layer. Each Transformer layer has a linear projection layer that reduces the input feature dimension to g = 32 to save computation. Different Transformer layers are connected densely to preserve representational power with lower feature dimensions. The feedforward layer at the end generates the fusion features of the different layers. Specifically, the output z j of the j-th (j ∈ [1,2,3,4]) Transformer layer can be calculated by:where z 0 represents the original input, cat(•) and p(•) denote the concatenation operator and the linear layer, respectively. The norm(•), att(•), f (•) are the regular layer normalization, multi-head self-attention mechanism, and feedforward layer. The output of DCT is z out = f (cat([z 0 ; z 1 ; ...; z 4 ])). Table 1 shows that the stacked DCT has lower parameters and computational complexity than a standard Transformer structure with the same number of layers. "
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.4,Segmentation Backbone Network,"The H-DenseFormer adopts a U-shaped encoder-decoder structure as its backbone. As shown in Fig. 1, the encoder extracts features and reduces their resolution progressively. To preserve more details, we set the maximum downsampling factor to 8. The multi-level multimodal features from MPE are fused in a bitwise addition way to enrich the semantic information. The decoder is used to restore the resolution of the features, consisting of deconvolutional and convolutional layers with skip connections to the encoder. In particular, we employ Deep Supervision (DS) loss to improve convergence, which means that the multiscale output of the decoder is involved in the final loss computation. Deep Supervision Loss. During training, the decoder has four outputs; for example, the i-th output of 2D H-DenseFormer is, where i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of segmentation classes. To mitigate the pixel imbalance problem, we use a combined loss of Focal loss [23] and Dice loss as the optimization target, defined as follows:where N refers to the total number of pixels, p t and q t denote the predicted probability and ground truth of the t-th pixel, respectively, and r = 2 is the modulation factor. Thus, DS loss can be calculated as follows:where G i represents the ground truth after resizing and has the same size as O i . α is a weighting factor to control the proportion of loss corresponding to the output at different scales. This approach can improve the convergence speed and performance of the network."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.1,Dataset and Metrics,"To validate the effectiveness of our proposed method, we performed extensive experiments on HECKTOR21 [1] and PI-CAI221 . HECKTOR21 is a dualmodality dataset for head and neck tumor segmentation, containing 224 PET-CT image pairs. Each PET-CT pair is registered and cropped to a fixed size of (144,144,144). PI-CAI22 provides multimodal MR images of 220 patients with prostate cancer, including T2-Weighted imaging (T2W), high b-value Diffusion-Weighted imaging (DWI), and Apparent Diffusion Coefficient (ADC) maps. After standard resampling and center cropping, all images have a size of (24,384,384). We randomly select 180 samples for each dataset as the training set and the rest as the independent test set (44 cases for HECKTOR21 and 40 cases for PI-CAI22). Specifically, the training set is further randomly divided into five folds for cross-validation. For quantitative analysis, we use the Dice Similarity Coefficient (DSC), the Jaccard Index (JI), and the 95% Hausdorff Distance (HD95) as evaluation metrics for segmentation performance. A better segmentation will have a smaller HD95 and larger values for DSC and JI. We also conduct holistic t-tests of the overall performance for our method and all baseline models with the two-tailed p < 0.05."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.2,Implementation Details,"We use Pytorch to implement our proposed method and the baselines. For a fair comparison, all models are trained from scratch using two NVIDIA A100 GPUs and all comparison methods are implemented with open-source codes, following their original configurations. In particular, we evaluate the 3D and 2D H-DenseFormer on HECKTOR21 and PI-CAI22, respectively. During the training phase, the Adam optimizer is employed to minimize the loss with an initial learning rate of 10 -3 and a weight decay of 10 -4 . We use the PolyLR strategy [19] to control the learning rate change. We also use an early stopping strategy with a tolerance of 30 epochs to find the best model within 100 epochs. Online data augmentation, including random rotation and flipping, is performed to alleviate the overfitting problem. Table 2 compares the performance and computational complexity of our proposed method with the existing state-of-the-art methods on the independent test sets. For HECKTOR21, 3D H-DenseFormer achieves a DSC of 73.9%, HD95 of 8.1mm, and JI of 62.5%, which is a significant improvement (p < 0.01) over 3D U-Net [7], UNETR [16], and TransBTS [31]. It is worth noting that the performance of hybrid models such as UNETR is not as good as expected, even worse than 3D U-Net, perhaps due to the small size of the dataset. Moreover, compared to the champion solution of HECKTOR20 proposed by Iantsen et al. [18], our method has higher accuracy and about 10 and 5 times lower amount of network parameters and computational cost, respectively. For PI-CAI22, the 2D variant of H-DenseFormer also outperforms existing methods (p < 0.05), achieving a DSC of 49.9%, HD95 of 35.9 mm, and JI of 37.1%. Overall, H-DenseFormer reaches an effective balance of performance and computational cost compared to existing CNNs and hybrid structures. For qualitative analysis, we show a visual comparison of the different methods. It is evident from Fig. 2 that our approach can describe tumor contours more accurately while providing better segmentation accuracy for small-volume targets. These results further demonstrate the effectiveness of our proposed method in multimodal tumor segmentation tasks."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.4,Parameter Sensitivity and Ablation Study,"Impact of DCT Depth. As illustrated in Table 3, the network performance varies with the change in DCT depth. H-DenseFormer achieves the best performance at the DCT depth of 6. An interesting finding is that although the depth  of the DCT has increased from 3 to 9, the performance does not improve or even worsen. We suspect that the reason is over-fitting due to over-parameterization. Therefore, choosing a proper DCT depth is crucial to improve accuracy. Impact of Different Modules. The above results demonstrate the superiority of our method, but it is unclear which module plays a more critical role in performance improvement. Therefore, we perform ablation experiments on MPE, DCT and DS loss. Specifically, w/o MPE refers to keeping one embedding path, w/o DCT signifies using a standard 12-layer Transformer, and w/o DS loss denotes removing the deep supervision mechanism. As shown in Table 4, the performance decreases with varying degrees when removing them separately, which means all the modules are critical for H-DenseFormer. We can observe that DCT has a greater impact on overall performance than the others, further demonstrating its effectiveness. In particular, the degradation after removing the MPE also con- firms that decoupling the feature expression of different modalities helps obtain higher-quality multimodal features and improve segmentation performance."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,4.0,Conclusion,"In this paper, we proposed an efficient hybrid model (H-DenseFormer) that combines Transformer and CNN for multimodal tumor segmentation. Concretely, a Multi-path Parallel Embedding module and a Densely Connected Transformer block were developed and integrated to balance accuracy and computational complexity. Extensive experimental results demonstrated the effectiveness and superiority of our proposed H-DenseFormer. In future work, we will extend our method to more tasks and explore more efficient multimodal feature fusion methods to further improve computational efficiency and segmentation performance."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 3 .,DCT Depth Params↓ GFLOPs↓ DSC (%) ↑ HD95 (mm) ↓ JI (%) ↑ 7 ± 1.2 8.7 ± 0.6 61.2 ± 1.3
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 4 .,3D H-DenseFormer w/o DS loss 72.2 ± 0.9 10.2 ± 1.0 60.1 ± 1.2 3D H-DenseFormer 73.9 ± 0.5 8.1 ± 0.6 62.5 ± 0.5
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,1.0,Introduction,"Accurate cancer diagnosis, grading, and treatment decisions from medical images heavily rely on the analysis of underlying complex nuclei structures [7]. Yet, due to the numerous nuclei contained in a digitized whole-slide image (WSI), or even in an image patch of deep learning input, dense annotation of nuclei contouring is extremely time-consuming and labor-expensive [11]. Consequently, automated nuclei segmentation approaches have emerged to satisfy a broad range of computer-aided diagnostic systems, where the deep learning methods, particularly the convolutional neural networks [5,12,14,19,21] have received notable attention due to their simplicity and generalization ability.In the literature work, the sole-decoder design in these UNet variants (Fig. 1(a)) is susceptible to failures in splitting densely clustered nuclei when precise edge information is absent. Hence, deep contour-aware neural network (DCAN) [3] with bi-decoder structure achieves improved instance segmentation performance by adopting multi-task learning, in which one decoder learns to segment the nuclei and the other recognizes edges as described in Fig. 1(b). Similarly, CIA-Net [20] extends DCAN with an extra information aggregator to fuse the features from two decoders for more precise segmentation. Much recently, CA 2.5 -Net [6] shows identifying the clustered edges in a multiple-task learning manner can achieve higher performance, and thereby proposes an extra output path to learn the segmentation of clustered edges explicitly. A significant drawback of the aforementioned multi-decoder networks is the ignorance of the prediction consistency between branches, resulting in sub-optimal performance and missing correlations between the learned branches. Specifically, a prediction mismatch between the nuclei and edge branches is observed in previous work [8], implying a direction for performance improvement. To narrow this gap, we propose a consistency distillation between the branches, as shown by the dashed line in Fig. 1(c). Furthermore, to resolve the cost of involving more decoders, we propose an attention sharing scheme, along with an efficient token MLP bottleneck [16], which can both reduce the number of parameters.Additionally, existing methods are CNN-based, and their intrinsic convolution operation fails to capture global spatial information or the correlation amongst nuclei [18], which domain experts rely heavily on for accurate nuclei allocation. It suggests the presence of long-range correlation in practical nuclei segmentation tasks. Inspired by the capability in long-range global context capturing by Transformers [17], we make the first attempt to construct a tri-decoder based Transformer model to segment nuclei. In short, our major contributions are three-fold: (1) We propose a novel multi-task framework for nuclei segmentation, namely TransNuSeg, as the first attempt at a fully Swin-Transformer driven architecture for nuclei segmentation. (2) To alleviate the prediction inconsistency between branches, we propose a novel self distillation loss that regulates the consistency between the nuclei decoder and normal edge decoder. (3) We propose an innovative attention sharing scheme that shares attention heads amongst all decoders. By leveraging the high correlation between tasks, it can communicate the learned features efficiently across decoders and sharply reduce the number of parameters. Furthermore, the incorporation of a light-weighted MLP bottleneck leads to a sharp reduction of parameters at no cost of performance decline. Fig. 2. The overall framework of the proposed TransNuSeg of three output branches to separate the nuclei, normal edges, and cluster edges, respectively. In the novel design, a pre-defined proportion of the attention heads are shared between the decoders via the proposed sharing scheme, which considerably reduces the number of parameters and enables more efficient information communication."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,2.0,Methodology,"Network Architecture Overview. Figure 2 illustrates the overall architecture of the proposed multi-task tri-decoder Transformer network, named TransNuSeg. Both the encoder and decoders utilize the Swin Transformer [13] as the building blocks to capture the long-range feature correlations in the nuclei segmentation context. Our network consists of three individual output decoder paths for nuclei segmentation, normal edges segmentation, and clustered edges segmentation. Given the high dependency between edge and clustered edge, we are inspired to propose a novel attention sharing scheme, which can communicate the information and share learned features across decoders while also reducing the number of parameters. Additionally, a token MLP bottleneck is incorporated to further increase the model efficiency.Attention Sharing Scheme. To capture the strong correlation between nuclei segmentation and contour segmentation between multiple decoders [15], we introduce a novel attention sharing scheme that is designed as an enhancement to the multi-headed self-attention (MSA) module in the plain Transformer [17]. Based on the attention sharing scheme, we design a shared MSA module, which is similar in structure to vanilla MSA. Specifically, it consists of a LayerNorm layer [1], residual connection, and feed-forward layer. Innovatively, it differs from the vanilla MSA by sharing a proportion of globally-shared self-attention (SA) heads amongst all the parallel Transformer blocks in decoders, while keeping the remaining SA heads unshared i. e. learn the weights separately. A schematic illustration of the shared MSA module in the Swin Transformer block is demonstrated in Fig. 3, as is formally formulated as follows:[•] writes for the concatenation, SA(•) denotes the self-attention head whose output dimension is D h , and U u MSA ∈ R (m+n)•D h ×D is a learnable matrix. The superscript s and u refer to the globally-shared and unshared weights across all decoders, respectively.Token MLP Bottleneck. To reduce the complexity of the model, we leverage a token MLP bottleneck as a light-weight alternative for the Swin Transformer bottleneck. Specifically, this approach involves shifting the latent features extracted by the encoder via two MLP blocks across the width and height channels, respectively [16]. The objective of this process is to attend to specific areas, which mimics the shifted window attention mechanism in Swin Transformer [13]. The shifted features are then projected by a learnable MLP and normalized through a LayerNorm [1] before being fed to a reprojection MLP layer.Consistency Self Distillation. To alleviate the inconsistency between the contour generated from the nuclei segmentation prediction and the predicted edge, we propose a novel consistency self distillation loss, denoted as L SD . Formally, this regularization is defined as the dice loss between the contour generated from the nuclei branch prediction (y n ) using the Sobel operation (sobel(y n )) and the predicted edges y e from the normal edge decoder. Specifically, the self distillation loss L D is formulated by L sd = Dice(sobel(y n ), y e ).Multi-task Learning Objective. We employ a multi-task learning paradigm to train the tri-decoder network, aiming to improve model performance by leveraging the additional supervision signal from edges. Particularly, the nuclei semantic segmentation is considered the primary task, while the normal edge and clustered edge semantic segmentation are viewed as auxiliary tasks. All decoder branches follow a uniform scheme that combines the cross-entropy loss and the dice loss, with the balancing coefficients set to 0.60 and 0.40 respectively, as previous work [6]. Subsequently, the overall loss L is calculated as a weighted summation of semantic nuclei mask loss (L n ), normal edge loss (L e ), and clustered edge loss (L c ), and the self distillation loss (L SD ) i. e., where coefficients γ n , γ e and γ c are set to 0.30, 0.35, 0.35 respectively, and γ sd is initially set to 1 with a 0.3 decrease for every 10 epochs until it reaches 0.4."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,3.0,Experiments,"Dataset. We evaluated the applicability of our approach across multiple modalities by conducting evaluations on microscopy and histology datasets.   The private dataset contains 300 images sized at 512 × 512 tessellated from 50 WSIs scanned at 20×, and meticulously labeled by five pathologists according to the labeling guidelines of the MoNuSeg [10]. For both datasets, we randomly split 80% of the samples on the patient level as the training set and the remaining 20% as the test set.  TransNuSeg demonstrates superior segmentation performance compared to its counterparts, which can successfully distinguish severely clustered nuclei from normal edges.Implementations. All experiments are performed on one NVIDIA RTX 3090 GPU with 24 GB memory. We use Adam optimizer with an initial learning rate of 1 × 10 -4 . We compare TransNuSeg with UNet [14], UNet++ [21], Tran-sUNet [4], SwinUNet [2], and CA 2.5 -Net [6]. We evaluate the results by using Dice Score (DSC), Intersection over Union (IoU), pixel-level accuracy (Acc), and F1-score(F1) as metrics, and ErCnt [8]. To ensure statistical significance, we run all methods five times with different fixed seeds and report the results as mean ± standard deviation.Results. Table 1 shows the quantitative comparisons for the nuclei segmentation. The large margin between the SwinUNet and the other CNN-based or hybrid networks also confirms the superiority of the Transformer in fine-grained nuclei segmentation. More importantly, our method can outperform SwinUNet and the previous methods on both datasets. For example, in the histology image dataset, TransNuSeg improves the dice score, F1 score, accuracy, and IoU by 2.08%, 3.41%, 1.25%, and 2.70% respectively, over the second-best models. Similarly, in the fluorescence microscopy image dataset, our proposed model improves DSC by 0.96%, while also leading to 1.65%, 1.03% and 1.91% increment in F1 score, accuracy, and IoU to the second-best performance. For better visualization, representative samples and their segmentation results using different methods are demonstrated in Fig. 4. Furthermore, Table 2 compares the model complexity in terms of the number of parameters, floating point operations per second (FLOPs), and the training computational cost, where our approach can significantly reduce around 28% of the training time compared to the state-ofthe-art CNN multi-task method CA 2.5 -Net, while also boosting performance. Ablation. Our ablation study yields that token MLP bottleneck and attention sharing schemes can complementarily reduce the training cost while increasing efficiency, as shown in Table 2 (the last 4 rows). To further show the effectiveness of these schemes, as well as consistency self distillation, we conduct a comprehensive ablation study on both datasets. As described in Table 3, each component proportionally contributes to the improvement to reach the overall performance boost. Moreover, self distillation can enhance the intrinsic consistency between two branches, as visualized in Fig. 5."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,4.0,Conclusion,"In this paper, we make the first attempt at an efficient but effective multi-task Transformer framework for modality-agnostic nuclei segmentation. Specifically, our tri-decoder framework TransNuSeg leverages an innovative self distillation regularization to impose consistency between the different branches. Experimental results on two datasets demonstrate the excellence of our TransNuSeg against state-of-the-art counterparts for potential real-world clinical deployment. Additionally, our work opens a new architecture to perform nuclei segmentation tasks with Swin Transformer, where further investigations can be performed to explore the generalizability to the top of our methods with different modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1.0,Introduction,"Multi-modal learning has become a popular research area in computer vision and medical image analysis, with modalities spanning across various media types, including texts, audio, images, videos and multiple sensor data. This approach has been utilised in Robot Control [15,17], Visual Question Answering [12] and Audio-Visual Speech Recognition [10], as well as in the medical field to improve diagnostic system performance [7,18]. For instance, Magnetic Resonance Imaging (MRI) is a common tool for brain tumour detection that relies on multiple modalities (Flair, T1, T1 contrast-enhanced known as T1c, and T2) rather than a single type of MRI images. However, most existing multi-modal methods require complete modalities during training and testing, which limits their applicability in real-world scenarios, where subsets of modalities may be missing during training and testing.The missing modality issue is a significant challenge in the multi-modal domain, and it has motivated the community to develop approaches that attempt to address this problem. Havaei et al. [8] developed HeMIS, a model that handles missing modalities using statistical features as embeddings for the model decoding process. Taking one step ahead, Dorent et al. [6] proposed an extension to HeMIS via a multi-modal variational auto-encoder (MVAE) to make predictions based on learned statistical features. In fact, variational auto-encoder (VAE) has been adopted to generate data from other modalities in the image or feature domains [3,11]. Yin et al. [20] aimed to learn a unified subspace for incomplete and unlabelled multi-view data. Chen et al. [4] proposed a feature disentanglement and gated fusion framework to separate modality-robust and modalitysensitive features. Ding et al. [5] proposed an RFM module to fuse the modal features based on the sensitivity of each modality to different tumor regions and a segmentation-based regularizer to address the imbalanced training problem. Zhang et al. [22] proposed an MA module to ensure that modality-specific models are interconnected and calibrated with attention weights for adaptive information exchange. Recently, Zhang et al. [21] introduced a vision transformer architecture, MMFormer, that fuses features from all modalities into a set of comprehensive features. There are several existing works [9,16,19] proposed to approximate the features from full modalities when one or more modalities are absent. But none work performs cross-modal knowledge distillation. From an other point of view, Wang et al. [19] introduced a dedicated training strategy that separately trains a series of models specifically for each missing situation, which requires significantly more computation resources compared with a nondedicated training strategy. An interesting fact about multi-modal problems is that there is always one modality that contributes much more than other modalities for a certain task. For instance, for brain tumour segmentation, it is known from domain knowledge that T1c scans clearly display the enhanced tumour, but not edema [4]. If the knowledge of these modalities can be successfully preserved, the model can produce promising results even when these best performing modalities are not available. However, the aforementioned methods neglect the contribution biases of different modalities and failed to consider keeping that knowledge.Aiming at this issue, we propose the non-dedicated training model1 Learnable Cross-modal Knowledge Distillation (LCKD) for tackling the missing modality issue. LCKD is able to handle missing modalities in both training and testing by automatically identifying important modalities and distilling knowledge from them to learn the parameters that are beneficial for all tasks while training for other modalities (e.g., there are four modalities and three tasks for the three types of tumours in BraTS2018). Our main contributions are:-We propose the Learnable Cross-modal Knowledge Distillation (LCKD) model to address missing modality problem in multi-modal learning. It is a simple yet effective model designed from the viewpoint of distilling crossmodal knowledge to maximise the performance for all tasks; -The LCKD approach is designed to automatically identify the important modalities per task, which helps the cross-modal knowledge distillation process. It also can handle missing modality during both training and testing.The experiments are conducted on the Brain Tumour Segmentation benchmark BraTS2018 [1,14], showing that our LCKD model achieves state-of-theart performance. In comparison to recently proposed competing methods on BraTS2018, our model demonstrates better performance in segmentation Dice score by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour, on average."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,Overall Architecture,"Let us represent the N -modality data with M l = {x∈ X denotes the l th data sample and the superscript (i) indexes the modality. To simplify the notation, we omit the subscript l when that information is clear from the context. The label for each set M is represented by y ∈ Y, where Y represents the ground-truth annotation space. The framework of LCKD is shown in Fig. 1.Multi-modal segmentation is composed not only of multiple modalities, but also of multiple tasks, such as the three types of tumours in BraTS2018 dataset that represent the three tasks. Take one of the tasks for example. Our model undergoes an external Teacher Election Procedure prior to processing all modalities {x (i) } N i=1 ∈ M in order to select the modalities that exhibit promising performance as teachers. This is illustrated in Fig. 1, where one of the modalities, x (2) , is selected as a teacher, {x (1) , x (3) , ..., x (N ) } are the students, and x (n)  (with n = 2) is assumed to be absent. Subsequently, the modalities are encoded to output features {f (i) } N i=1 , individually. For the modalities that are available, namely x (1) , ..., x (n-1) , x (n+1) , ..., x (N ) , knowledge distillation is carried out between each pair of teacher and student modalities. However, for the absent  i=1 } are processed by the encoder to produce the features {f (i) N i=1 }, which are concatenated and used by the decoder to produce the segmentation. The teacher is elected using a validation process that selects the top-performing modalities as teachers. Cross-modal distillation is performed by approximating the available students' features to the available teachers' features. Features from missing modalities are generated by averaging the other modalities' features. modality x (n) , its features f (n) are produced through a missing modality feature generation process from the available features f (1) , ..., f (n-1) , f (n+1) , ..., f (N ) .In the next sections, we explain each module of the proposed Learnable Crossmodal Knowledge Distillation model training and testing with full and missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Teacher Election Procedure,"Usually, one of the modalities is more useful than others for a certain task, e.g. for brain tumour segmentation, T1c scan clearly displays the enhanced tumour, but it does not clearly show edema [4]. Following knowledge distillation (KD) [9], we propose to transfer the knowledge from modalities with promising performance (known as teachers) to other modalities (known as students). The teacher election procedure is further introduced to automatically elect proper teachers for different tasks.More specifically, in the teacher election procedure, a validation process is applied: for each task k (for k ∈ {1, ..., K}), the modality with the best performance is selected as the teacher t (k) . Formally, we have:where i indexes different modalities, F (•; Θ) is the LCKD segmentation model parameterised by Θ, including the encoder and decoder parameters {θ enc , θ dec } ∈ Θ, and d(•, •) is the function to calculate the Dice score. Based on the elected teachers for different tasks, a list of unique teachers (i.e., repetitions are not allowed in the list, so for BraTS, {T1c, T1c, Flair} would be reduced to {T1c, Flair}) are generated with: T = φ(t (1) , t (2) , ..., t (k) , ..., t (K) ), (2) where φ is the function that returns the unique elements from a given list, and T ⊆ {1, ..., N } is the teacher set."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.3,Cross-Modal Knowledge Distillation,"As shown in Fig. 1, after each modality x (i) is inputted into the encoder parameterised by θ enc , the features f (i) for each modality is fetched, as in:(3)The cross-modal knowledge distillation (CKD) is defined by a loss function that approximates all available modalities' features to the available teacher modalities in a pairwise manner for all tasks, as follows:where • p presents the p-norm operation, and here we expended the notation of missing modalities to make it more general by assuming a set of modalities m is missing. The minimisation of this loss pushes the model parameter values to a point in the parameter space that can maximise the performance of all tasks for all modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.4,Missing Modality Feature Generation,"Because of the knowledge distillation between each pair of teachers and students, the features of modalities in the feature space ought to be close to the ""genuine"" features that can uniformly perform well for different tasks. Still assuming that modality set m is missing, the missing features f (n) can thus be generated from the available features:where |m| denotes the number of missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.5,Training and Testing,"All features encoded from Eq. 3 or generated from Eq. 5 are then concatenated to be fed into the decoder parameterised by θ dec for predictingwhere ỹ ∈ Y is the prediction of the task.The training of the whole model is achieved by minimising the following objective function: tot (D, Θ) = task (D, θ enc , θ dec ) + α ckd (D; θ enc ), (7) where task (D, θ enc , θ dec ) is the objective function for the whole task (e.g., Cross-Entropy and Dice losses are adopted for brain tumour segmentation), and α is the trade-off factor between the task objective and cross-modal KD objective.Testing is based on taking all image modalities available in the input to produce the features from Eq. 3, and generating the features from the missing modalities with Eq. 5, which are then provided to the decoder to predict the segmentation with Eq. 6."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.1,Data and Implementation Details,"Our model and competing methods are evaluated on the BraTS2018 Segmentation Challenge dataset [1,14]. The task involves segmentation of three subregions of brain tumours, namely enhancing tumour (ET), tumour core (TC), and whole tumour (WT). The dataset consists of 3D multi-modal brain MRIs, including Flair, T1, T1 contrast-enhanced (T1c), and T2, with ground-truth annotations. The dataset comprises 285 cases for training, and 66 cases for evaluation. The ground-truth annotations for the training set are publicly available, while the validation set annotations are hidden 2 .3D UNet architecture (with 3D convolution and normalisation) is adopted as our backbone network, where the CKD process occurs at the bottom stage of the UNet structure. To optimise our model, we adopt a stochastic gradient descent optimiser with Nesterov momentum [2] set to 0.99. L1 loss is adopted for ckd (.) in Eq. 4. Batch-size is set to 2. The learning rate is initially set to 10 -2 and gradually decreased via the cosine annealing [13] strategy. We trained the LCKD model for 115,000 iterations and use 20% of the training data as the validation task for teacher election. To simulate modality-missing situations with non-dedicated training of models, we randomly dropped 0 to 3 modalities for each iteration. Our training time is 70.12 h and testing time is 6.43 s per case on one Nvidia 3090 GPU. 19795 MiB GPU memory is used for model training with batch-size 2 and 3789 MiB GPU memory is consumed for model testing with batch-size 1."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.2,Overall Performance,"Table 1 shows the overall performance on all 15 possible combinations of missing modalities for three sub-regions of brain tumours. Our models are compared with several strong baseline models: U-HeMIS (abbreviated as HMIS in the figure) [8], U-HVED (HVED) [6], Robust-MSeg (RSeg) [4] and mmFormer (mmFm) [21]. We can clearly observe that with T1c, the model performs considerably better than other modalities for ET. Similarly, T1c for TC and Flair for WT contribute the most, which confirm our motivation.The LCKD model significantly outperforms (as shown by the one-tailed paired t-test for each task between models in the last row of Table 1) U-HeMIS, U-HVED, Robust-MSeg and mmFormer in terms of the segmentation Dice for enhancing tumour and whole tumour on all 15 combinations and the tumour core on 14 out of 15. It is observed that, on average, the proposed LCKD model improves the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of the segmentation Dice score. Especially in some combinations without the best modality, e.g. ET/TC without T1c and WT without Flair, LCKD has a 6.15% improvement with only Flair and 10.69% with only T1 over the second best model for ET segmentation; 10.8% and 10.03% improvement with only Flair and T1 for TC; 8.96% and 5.01% improvement with only T1 and T1c for WT, respectively. These results demonstrate that useful knowledge of the best modality has been successfully distilled into the model by LCKD for multimodal learning with missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.3,Analyses,"Single Teacher vs. Multi-teacher. To analyse the effectiveness of knowledge distillation from multiple teachers of all tasks in the proposed LCKD model, we perform a study to compare the model performance of adopting single teacher and multi-teachers for knowledge distillation. We enable multi-teachers for LCKD by default to encourage the model parameters to move to a point that can perform well for all tasks. However, for single teacher, we modify the  2, compared with multi-teacher model LCKD-m, we found that the single teacher model LCKD-s receives comparable results for ET and TC segmentation (it even has better average performance on ET), but it cannot outperform LCKD-m on WT. This phenomenon, also shown in Fig. 2, demonstrates that LCKD-m has better overall segmentation performance. This resonates with our expectations because there are 3 tasks in BraTS, and the best teachers for ET and TC are the same, which is T1c, but for WT, Flair is the best one. Therefore, for LCKD-s, the knowledge of the best teacher for ET and TC can be distilled into the model, but not for WT. The LCKD-m model can overcome this issue since it attempts to find a point in the parameter space that is beneficial for all tasks. Empirically, we observed that both models found the correct teacher(s) quickly: the best teacher of the single teacher model alternated between T1c and Flair for a few validation rounds and stabilised at T1c; while the multi-teacher model found the best teachers (T1c and Flair) from the first validation round.  Role of α and CKD Loss Function. As shown in Fig. 3, we set α in Eq. 7 to {0, 0.1, 0.5, 1} using T1 input only and L1 loss for ckd (.) in (4). If α = 0, the model performance drops greatly, but when α > 0, results improve, where α = 0.1 produces the best result. This shows the importance of the cross-modal knowledge distillation loss in (7). To study the effect of a different CKD loss, we show Dice score with L2 loss for ckd (.) in ( 4), with α = 0.1. Compared with the L1 loss, we note that Dice decreases slightly with the L2 loss, especially for TC and WT."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.0,Conclusion,"In this paper we introduced the Learnable Cross-modal Knowledge Distillation (LCKD), which is the first method that can handle missing modality during training and testing by distilling knowledge from automatically selected important modalities for all training tasks to train other modalities. Experiments on BraTS2018 [1,14] show that LCKD reaches state-of-the-art performance in missing modality segmentation problems. We believe that our proposed LCKD has the potential to allow the use of multimodal data for training and missingmodality data per testing. One point to improve about LCKD is the greedy teacher selection per task. We plan to improve this point by transforming this problem into a meta-learning strategy, where the meta parameter is the weight for each modality, which will be optimised per task."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,1.0,Introduction,"Cortical surface reconstruction aims to extract 3D meshes of inner (white matter) and outer (pial) surfaces of the cerebral cortex from brain magnetic resonance images (MRI). These surfaces provide both 3D visualization and estimation of morphological features for the cortex [7,11,12]. In addition to accurately representing the highly folded cortex, the cortical surfaces of each hemisphere are   [20]. Right: neonatal brain MRI from the dHCP dataset [9].required to be closed manifolds and topologically homeomorphic to a sphere [7]. Traditional neuroimage analysis pipelines [10,13,24,29] such as FreeSurfer [10] comprise a series of processing steps to extract cortical surfaces from brain MRI. These pipelines provide an invaluable service to the research community. However, the current implementations have limited accuracy and require several hours to process a single MRI scan.With the recent advances of geometric deep leaning [32,33], a growing number of fast learning-based approaches have been proposed to learn either implicit surface representation [6,15] or explicit mesh deformation [4,17,[21][22][23]28] for cortical surface reconstruction. These approaches enhance the accuracy and reduce the processing time to a few seconds for a single subject. Recent studies focus on generating manifold cortical meshes [21,22,28] and preventing mesh self-intersections by learning diffeomorphic deformations, which have been widely adopted in medical image registration [1][2][3]. The basic idea is to learn stationary velocity fields (SVF) to deform an initial mesh template to target surfaces. Since a single SVF has limited representation capacity, several approaches [16,21,28,30] have proposed to train multiple neural networks to predict a sequence of SVFs for coarse-to-fine surface deformation. This improves the geometric accuracy but increases the computational burden for both training and inference.Cortical surface reconstruction plays an essential role in modeling and quantifying the brain development in fetal and neonatal neuroimaging studies such as the Developing Human Connectome Project (dHCP) [9,24]. However, most learning-based approaches so far rely on adult MRI as training data [13,20,25]. Compared to adult data, neonatal brain MR images have lower resolution and contrast due to a smaller region of interest and the use of, e.g., fast imaging sequences with sparse acquisition to minimize head motion artifacts of unsedated infants [5,19]. Besides, the rapid growth and continuously increasing complexity of the cortex during the perinatal period lead to considerable variation in shape and scale between neonatal cortical surfaces at different post-menstrual ages (PMA) (see Fig. 1). Moreover, since the neonatal head is still small, the cortical sulci of term-born neonates are much narrower than adults as shown in Fig. 2. Hence, the neonatal pial surfaces are prone to be affected by partial volume effects and more likely to produce surface self-intersections."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Contribution.,"In this work, we present Conditional Temporal Attention Network (CoTAN). CoTAN adopts attention mechanism [18,27] to learn a conditional time-varying velocity field (CTVF) for neonatal cortical surface reconstruction. Given an input brain MR image, CoTAN first predicts multiple SVFs at different resolutions. Rather than integrating all SVFs as [16,21,28], CoTAN learns conditional temporal attention maps to attend to specific SVFs for different time steps of integration and PMA of neonates. The CTVF is represented by the weighted sum of learned SVFs, and thus a single CoTAN model is sufficient to model the large deformation and variation of neonatal cortical surfaces. The evaluation on the dHCP neonatal dataset [9] verifies that CoTAN performs better in geometric accuracy, mesh quality and computational efficiency than state-of-theart methods. The visualization of attention maps indicates that CoTAN learns coarse-to-fine deformations automatically without intermediate constraints. Our source code is released publicly at https://github.com/m-qiang/CoTAN."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,2.0,Method,"Diffeomorphic Surface Deformation. We define the diffeomorphic surface deformation φ t : R 3 × R → R 3 as a flow ordinary differential equation (ODE) following previous work [1][2][3]16,21,22,28]:where v t is a time-varying velocity field (TVF) and Id is the identity mapping. Given an initial surface S 0 ⊂ R 3 with points x 0 ∈ S 0 , we define x t := φ t (x 0 ) as the trajectories of the points on the deformable surface S t = φ t (S 0 ) for t ∈ [0, T ].Then the flow equation (1) can be rewritten as d dt x t = v t (x t ) with initial value x 0 . By the existence and uniqueness theorem for ODE solutions [16], if v t (x) is Lipschitz continuous with respect to x, the trajectories x t will not intersect with each other, so that the surface self-intersections can be prevented effectively. By integrating the ODE (1), we obtain a diffeomorphism φ T that deforms S 0 to a manifold surface S T , on which the points areConditional Temporal Attention Network (CoTAN). An overview of the CoTAN architecture is shown in Fig. 3. CoTAN first predicts multiple SVFs given a 3D brain MRI volume. A 3D U-Net [26] is used to extract feature maps with R resolution levels, each of which scales the input size by the factor of 2 r-R for r = 1, ..., R. Then we upsample the multi-scale feature maps and learn M volumetric SVFs for each resolution. Let V denote all R × M discrete SVFs. The continuous multi-resolution SVFs v : R 3 → R R×M ×3 can be obtained by v(x) = Lerp(x, V), where Lerp(•) is the trilinear interpolation function. Each element v r,m : R 3 → R 3 is an SVF for r = 1, ..., R and m = 1, ..., M . Note that v(x) is Lipschitz continuous since Lerp(•) is continuous and piece-wise linear.CoTAN adopts a channel-wise attention mechanism [18,27] to focus on specific SVFs since it is time-consuming to integrate all R × M SVFs [16,21,28]. The attention is conditioned on both integration time t ∈ [0, T ] and information about the subject. To model the high variation between infant brains, we consider the post-menstrual ages (PMA) a ∈ R of the neonates at scan time as the conditioning variable in this work. Note that we do not use a self-attention module [8,31] to learn key and query pairs. Instead, we learn a probability attention map to measure the importance of each SVF. More precisely, as shown in Fig. 3, we use a fully connected network (FCN) to encode the input time t and PMA a into a (R • M ) × 1 feature vector. After reshaping and softmax activation, the FCN learns conditional temporal attention maps p(t, a) ∈ R R×M which satisfy R r=1 M m=1 p r,m (t, a) = 1 for any t and a. Then a conditional time-varying velocity field (CTVF) is predicted by computing the weighted sum of all SVFs:The CTVF is adaptive to the integration time and the age of subjects, which can handle the large deformation and variation for neonatal cortical surfaces. Such an attention mechanism encourages CoTAN to learn coarse-to-fine surface deformation by attending to SVFs at different resolutions.To deform the initial surface S 0 to the target surface, we integrate the flow ODE (1) with the CTVF through the forward Euler method. For k = 0, ..., K -1, the surface points are updated by x k+1 = x k + hv k (x k ; a), where K is the total integration steps and h = T /K is the step size with T = 1. For each step k, we only need to recompute the attention maps p(hk, a) and update the CTVF v k (x k ; a) accordingly by Eq. ( 2). CoTAN only integrates a single CTVF which saves considerable runtime compared to integrating multiple SVFs directly as [16,21,28]. Neonatal Cortical Surface Reconstruction. We train two CoTAN models on the dHCP neonatal dataset [9] to 1) deform an initial surface into a white matter surface and to 2) expand the white matter surface into a pial surface as shown in Fig. 4. We use the same initial surface (leftmost in Fig. 4) for all subjects, which is created by iteratively applying Laplacian smoothing to a Conte-69 surface atlas [14]. We generate pseudo ground truth (GT) surfaces by the dHCP structural neonatal pipeline [24], which has been fully validated through quality control performed by clinical experts.For white matter surface reconstruction, we consider loss functions that have been widely used in previous work [4,32,33]: the Chamfer distance loss L cd computes the distance between two point clouds, the mesh Laplacian loss L lap regularizes the smoothness of the mesh, and the normal consistency loss L nc constrains the cosine similarity between the normals of two adjacent faces. The final loss is weighted by L = L cd + λ lap L lap + λ nc L nc . We train CoTAN in two steps for white matter surface extraction. First, we pre-train the model using relatively large weights λ lap and λ nc for regularization. The Chamfer distance is computed between the vertices of the predicted and pseudo-GT surfaces. These ensure that the initial surface can be deformed robustly during training. Then, we fine-tune CoTAN using small weights to increase geometric accuracy. The distances are computed between 150k uniformly sampled points on the surfaces.For pial surface reconstruction, we follow [22,23] and use the pseudo-GT white matter surfaces as the input for training. Then the MSE loss L = i xix * i 2 can be computed between the vertices of predicted and pseudo-GT pial meshes. No point matching is required since the pseudo-GT white matter and pial surfaces have the same mesh connectivity. Therefore, the MSE loss provides stronger supervision than the Chamfer distance, while the latter is prone to mismatching the points in narrow sulci, resulting in mesh self-intersections."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,3.0,Experiments,"Implementation Details. We evaluate CoTAN on the third release of dHCP neonatal dataset [9] (https://biomedia.github.io/dHCP-release-notes/), which includes 877 T2-weighted (T2w) brain MRI scanned from newborn infants at PMA between 27 to 45 weeks. The MRI images are affinely aligned to the MNI152 space and clipped to the size of 112 × 224 × 160 for each brain hemisphere. The dataset is split into 60/10/30% for training/validation/testing.For the CoTAN model, we set the resolution R=3 and the number of SVFs M =4 for each resolution. For integration, we set the total number of steps to  K=50 with step size h=0.02. We re-mesh the initial mesh to 140k vertices, of which the coordinates are normalized to [-1, 1]. We use the Adam optimizer for training. For the white matter surface, we first pre-train CoTAN for 100 epochs using a learning rate of γ=10 -4 and weights λ lap =0.5, λ nc =5×10 -4 for the loss function. Then we fine-tune for 100 epochs using smaller weights λ lap =0.1 and λ nc =10 -4 with γ=2×10 -5 . For the pial surface, we set the maximum channel size of CoTAN as 32 to avoid overfitting and train for 200 epochs with γ=10 -4 . We only consider left brain hemisphere in the experiments. All experiments are conducted on a Nvidia RTX3080 GPU with 12GB memory.Comparative Results. We compare the performance of CoTAN with existing learning-based cortical surface extraction approaches including Cortex-ODE [22], CorticalFlow++ (CFPP) [28], CorticalFlow [21], Vox2Cortex [4] and DeepCSR [6]. We employ the fast topology correction [22] for DeepCSR. CoTAN can guarantee spherical topology and the Euler number is 2 for all predicted surfaces.-Geometric Accuracy: We measure the geometric accuracy by commonly used metrics [4,6,22]: average symmetric surface distance (ASSD) and 90th percentile of Hausdorff distance (HD90). The distances are computed between uniformly sampled 100k points on the predicted and pseudo-GT surfaces. For fair com- parison, the predicted cortical meshes have around 140k vertices for all baseline approaches. Note that CoTAN and [6,21,28] can generalize on high-resolution meshes with up to 600k vertices (see Appendix). We conduct a paired t-test to examine the statistical significance. As reported in Table 1, CoTAN achieves significantly superior geometric accuracy (p-value<0.05) compared to all stateof-the-art baselines, except for the white matter surfaces of CortexODE.-Mesh Quality: We evaluate the mesh quality by the ratio of self-intersecting faces (SIF) as shown in Table 1. Note that due to the narrower sulcal gaps of neonatal cortex compared to adult [20,25] (see Fig. 2), all baseline methods produce more SIFs than reported in their original papers. Except that DeepCSR produces no SIFs since it uses the Marching Cubes algorithm, CoTAN achieves the best mesh quality with only 0.001% SIFs in white matter surfaces and 0.071% SIFs in pial surfaces. These remaining SIFs are likely introduced by the discretization of triangular mesh representation and ODE integration. CoTAN produces fewer SIFs for three reasons. Firstly, CoTAN employs diffeomorphic deformation, while the non-diffeomorphic Vox2Cortex creates 14% SIFs. We further set the integration steps K=5 for CoTAN so that the deformation is no longer diffeomorphic. The SIFs of pial surfaces are increased to 2.99 ± 1.19%. Secondly, CoTAN reconstructs the pial surface by expanding the input white matter surface. It is difficult to avoid collisions during the deformation from a smooth template into deep sulci, e.g., in Vox2Cortex and CorticalFlow. Lastly, CoTAN uses MSE loss rather than Chamfer distance for pial surface extraction, which alleviates the mismatch between points in the narrow sulci.-Computational Efficiency: We report the runtime and GPU memory cost for both training and testing, as well as the number of learnable parameters for CoTAN and all baseline approaches in Table 2. The runtime includes both model inference and post-processing. CoTAN only requires 0.21 s to extract cortical surfaces for each hemisphere, which is 2× faster than the best baseline. CoTAN can be trained end-to-end and reduces training time by 46% compared to CorticalFlow and CFPP, which have to train three U-Nets consecutively to parameterize three SVFs for a single surface. Although CoTAN uses relatively Ablation Study. We conduct ablation studies on CoTAN and evaluate the geometric accuracy on the white matter surface reconstruction. First, without fine-tuning, the geometric errors increase by 8% as reported in Table 3. Then we consider CoTAN with single resolution (R=1) or only predict a single SVF (M =1) for each resolution. The geometric distances increase in both cases.Next, we examine the effectiveness of the CTVF v t (x; a) by fixing the input time t=0 or age a=0. We train CoTAN models to predict the TVF (a=0), CVF (t=0) and SVF (a=t=0), which are degraded from the CTVF. Table 3 shows that the SVF increases geometric errors by 30% due to its limited representation capacity. The CVF increases accuracy slightly by learning conditional deformations adaptive to the age of neonates. The TVF exploits temporal information to model a wider range of deformations and effectively improves the performance.Lastly, we train a U-Net to predict R×M SVFs and integrate them directly without attention. Since the gradients are backpropagated through all SVFs, it requires >140 h training time which is 2.4× slower than our attention-based CoTAN. The model is also sensitive to small updates, which can affect all SVFs. This results in exploding gradients which we have observed in the training, whereas CoTAN can be trained robustly by integrating a single CTVF.Attention Maps. We explore the attention maps p r,m (t, a) learned by CoTAN. We define p r = M m=1 p r,m to reflect the importance of the SVFs at each resolution level r=1, ..., R, where R = 3 and larger r means higher resolution. Figure 6 visualizes the attention maps p r (t, a) for white matter surface reconstruction for integration time t ∈ [0, 1] and age a ∈ {28, 35, 42}. It shows that the attention maps focus on low resolution (r=1) at the beginning of the integration, and then attends to high resolution (r=3) when t→1. Furthermore, Fig. 6 (Right) shows that an initial white matter surface deforms into a coarse shape for t≤0.3 and learns fine details for t≥0.6. This matches the attention maps and demonstrates that CoTAN learns coarse-to-fine deformations automatically without any supervision on the intermediate deformations. Additionally, Fig. 6 shows that CoTAN pays more attention to low resolution for younger subjects (28 week) whose brains have not fully developed yet. More deformations at higher resolutions (r≥2) are required for older neonates (≥35 week) with highly folded cortex.Discussion. One limitation of our experiments is that we only train and evaluate CoTAN based on the pseudo-GT generated by the dHCP structural pipeline [24]. Previous approaches [4,6,22] have been validated by the test-retest experiments. However, this is infeasible for neonates, whose brain develops rapidly even within a short period. To verify the superior anatomical accuracy of CoTAN, we provide qualitative comparison between the pseudo-GT and CoTAN as visualized in Fig. 5. It shows that CoTAN can effectively mitigate corruptions introduced by the dHCP pipeline for neonatal subjects at different ages. In addition, the dHCP pipeline requires 4 h to process a single subject [24], while CoTAN extracts cortical surfaces in only 0.21 s for each brain hemisphere."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,4.0,Conclusion,"In this work, we propose CoTAN for diffeomorphic neonatal cortical surface reconstruction. CoTAN employs an attention mechanism to combine multiple SVFs to a CTVF, which outperforms existing baselines in geometric accuracy and mesh quality. CoTAN can also be extended and applied to extract adult cortical surfaces conditioned on the age, gender or pathological information of the subjects. Our future work will integrate CoTAN into a learning-based pipeline for universal cortical surface analysis across all age groups."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_30.
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,1.0,Introduction,"Gliomas are the most commonly seen central nervous system malignancies with aggressive growth and low survival rates [19]. Accurate multi-class segmentation of gliomas in multimodal magnetic resonance imaging (MRI) plays an indispensable role in quantitative analysis, treatment planning, and monitoring of progression and treatment. Although deep learning-based methods have achieved state-of-the-art performance in automated brain tumor segmentation [6][7][8]14], their performance often drops when tasked with segmenting out-of-distribution samples and poor-quality artifactual images. However, segmentations of desired quality are required to reliably drive treatment decisions and facilitate clinical management of gliomas. Therefore, tools for automated quality control (QC) are essential for the clinical translation of automated segmentation methods. Such tools can enable a streamlined clinical workflow by identifying catastrophic segmentation failures, informing clinical experts where the segmentations need to be refined, and providing a quantitative measure of quality that can be taken into account in downstream analyses.Most previous studies of segmentation QC only provide subject-level quality assessment by either directly predicting segmentation-quality metrics or their surrogates. Specifically, Wang et al. [18] leveraged a variational autoencoder to learn the latent representation of good-quality image-segmentation pairs in the context of cardiac MRI segmentation. During the inference, an iterative search scheme was performed in the latent space to find a surrogate segmentation. This segmentation is assumed to be a good proxy of the (unknown) ground-truth segmentation of the query image, and can thus be compared to the at-hand predicted segmentation to estimate its quality. Another approach that takes advantage of the pairs of images and ground-truth segmentation is the reverse classification accuracy (RCA) framework [13,17]. In this framework, the test image is registered to a preselected reference dataset with known ground-truth segmentation. The quality of a query segmentation is assessed by warping the query image to the reference dataset. However, these methods primarily targeted QC of cardiac MRI segmentation, which involves a single imaging modality and a single tissue type with a welch-characterized location and appearance. In contrast, brain tumor segmentation involves the delineation of heterogeneous tumor regions, which are manifested through intensity changes relative to the surrounding healthy tissue across multiple modalities. Importantly, there is significant variability in brain tumor appearances, including multifocal masses and complex shapes with heterogeneous textures. Consequently, adapting approaches for automated QC of cardiac segmentation to brain tumor segmentation is challenging. Additionally, iterative search or registration during inference makes the existing methods computationally expensive and time-consuming, which limits their applicability in large-scale segmentation QC.Multiple studies have also explored regression-based methods to directly predict segmentation-quality metrics, e.g., Dice Similarity Coefficient (DSC). For example, Kohlberger et al. [10] used Support Vector Machine (SVM) with handcrafted features to detect cardiac MRI segmentation failures. Robinson et al. [12]  proposed a convolutional neural network (CNN) to automatically extract features from segmentations generated by a series of Random Forest segmenters to predict DSC for cardiac MRI segmentation. Kofler et al. [9] proposed a CNN to predict holistic ratings of segmentations, which were annotated by neuroradiologists, with the goal of better emulating how human experts. Though these regression-based methods are advantageous for fast inference, they do not provide voxel-level localization of segmentation failures, which can be crucial for both auditing purposes and guiding manual refinements.In summary, while numerous efforts have been devoted to segmentation QC, most works were in the context of cardiac MRI segmentation with few works tackling segmentation QC of brain tumors, which have more complex and heterogeneous appearances than the heart. Furthermore, most of the existing methods do not localize segmentation errors, which is meaningful for both auditing purposes and guiding manual refinement. To address these challenges, we propose a novel framework for joint subject-level and voxel-level prediction of segmentation quality from multimodal MRI. The contribution of this work is four-fold. First, we proposed a predictive model (QCResUNet) that simultaneously predicts DSC and localizes segmentation errors at the voxel level. Second, we devised a datageneration approach, called SegGen, that generates a wide range of segmentations of varying quality, ensuring unbiased model training and testing. Third, our end-to-end predictive model yields fast inference. Fourth, the proposed method achieved a good performance in predicting subject-level segmentation quality and identifying voxel-level segmentation failures."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,2.0,Method,"Given four imaging modalities denoted as [X 1 , X 2 , X 3 , X 4 ] and a predicted multiclass brain tumor segmentation mask (S pred ), the goal of our approach is to automatically assess the tumor segmentation quality by simultaneously predicting DSC and identifying segmentation errors as a binary mask (S err ). Toward this end, we proposed a 3D encoder-decoder architecture termed QCResUNet (see Fig. 1(a)) for simultaneously predicting DSC and localizing segmentation errors. QCResUNet has two parts trained in an end-to-end fashion: i) a ResNet-34 [4] encoder for DSC prediction; and ii) a decoder architecture for segmentation error map prediction (i.e., the difference between predicted segmentation and ground-truth segmentation).The ResNet-34 encoder enables the extraction of semantically rich features that are useful for characterizing the quality of the segmentation. We maintained the main structure of the vanilla 2D ResNet-34 [4] but made the following modifications, which were necessary to account for the 3D nature of the input data (see Fig. 1(b)). First, all the 2D convolutional layers and pooling layers in the vanilla ResNet were changed to 3D. Second, the batch normalization [5] was replaced by instance normalization [16] to accommodate the small batch size in 3D model training. Third, spatial dropout [15] with a probability of 0.3 was added to each residual block to prevent overfitting.The building block of the decoder consisted of an upsampling by a factor of two, which was implemented by a nearest neighbor interpolation in the feature map, followed by two convolutional blocks that halve the number of feature maps. Each convolutional block comprised a 3 × 3 × 3 convolutional layer followed by an instance normalization layer and a leaky ReLU activation [11] (see Fig. 1(c)). The output of each decoder block was concatenated with features from the corresponding encoder level to facilitate information flow from the encoder to the decoder. Compared to the encoder, we used a shallower decoder with fewer parameters to prevent overfitting and reduce computational complexity.The objective function for training QCResUNet consists of two parts. The first part corresponds to the DSC regression task. It consists of a mean absolute error (MAE) loss (L MAE ) term that penalizes differences between ground truth (DSC gt ) and predicted DSC (DSC pred ):where N denotes the number of samples in a batch. The second part of the objective function corresponds to the segmentation error prediction. It consists of a dice loss [3] and a binary cross-entropy loss, given by:where S errgt , S err pred denote the binary ground-truth segmentation error map and the predicted error segmentation map from the sigmoid output of the decoder, respectively. The dice loss and cross-entropy loss were averaged across the number of pixels I in a batch. The two parts are combined using a weight parameter λ to balance the different loss components:3 ExperimentsFor this study, pre-operative multimodal MRI scans of varying grades of glioma were obtained from the 2021 Brain Tumor Segmentation (BraTS) challenge [1] training dataset (n = 1251). For each subject, four modalities viz. pre-contrast T1-weighted (T1), T2-weighted (T2), post-contrast T1-weighted (T1c), and Fluid attenuated inversion recovery (FLAIR) are included in the dataset. It also included expert-annotated multi-class tumor segmentation masks comprising enhancing tumor (ET), necrotic tumor core (NCR), and edema (ED) classes. All data were already registered to a standard anatomical atlas and skull-stripped. The skull-stripped scans were then z-scored to zero mean and unit variance. All the data was first cropped to non-zero value regions, and then zero-padded to a size of 160 × 192 × 160 to be fed into the network."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,3.1,Data Generation,"The initial dataset was expanded by producing segmentation results at different levels of quality to provide an unbiased estimation of segmentation quality. To this end, we adopted a three-step approach. First, a nnUNet framework [6] was adopted and trained five times separately using different modalities as input (i.e., T1-only, T1c-only, T2-only, FLAIR-only, and all four modalities). As only certain tissue-types are captured in each modality (e.g., enhancing tumor is captured well in T1c but not in FLAIR), this allowed us to generate segmentations of a wide range of qualities. nnUNet was selected for this purpose due to its wide success in brain tumor segmentation tasks. Second, to further enrich our dataset with segmentations of diverse quality, we sampled segmentations along the training routines at different iterations. A small learning rate (1 × 10 -6 ) was chosen in training all the models to slower their convergence in order to sample segmentations gradually sweeping from poor quality to high quality. Third, we devised a method called SegGen that applied image transformations, including random rotation (angle = [-15 • , 15 • ]), random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]), and random elastic deformation (displacement = [0, 20]), to the ground-truth segmentations with a probability of 0.5, resulting in three segmentations for each subject.The original BraTS 2021 training dataset was split into training (n = 800), validation (n = 200), and testing (n = 251) sets. After applying the three-step approach, it resulted in 48000, 12000, and 15060 samples for the three sets, respectively. However, this generated dataset suffered from imbalance (Fig. 2(a),  (b), and(c)) because the CNN models could segment most of the cases correctly. Training using such an imbalanced dataset is prone to producing biased models that do not generalize well. To mitigate this issue, we proposed a resampling strategy during the training to make the DSC more uniformly distributed. Specifically, we used the Quantile transform to map the distribution of a variable to a target distribution by randomly smoothing out the samples unrelated to the target distribution. Using the Quantile transform, the data generator first transformed the distribution of the generated DSC to a uniform distribution. Next, the generated samples closest to the transformed uniform distribution in terms of Euclidean distance were chosen to form the resampled dataset. After applying our proposed resampling strategy, the DSC in the training and validation set approached a uniform distribution (Fig. 2(a), (b), and (c)). The total number of samples before and after resampling remained the same with repeating samples. We kept the resampling stochastic at each iteration during training to make all the generated samples seen by the model. The generated testing set was also resampled to perform an unbiased estimation of the quality at different levels resulting in 4895 samples.In addition to the segmentations generated by the nnUNet framework and the SegGen method, we also generated out-of-distribution segmentation samples for the testing set to validate the generalizability of our proposed model. For this purpose, five models were trained on the training set using the DeepMedic framework [8] with different input modalities (i.e., T1-only, T1c-only, T2-only, FLAIR-only, and all four modalities). This resulted in 251 × 5 = 1255 out-ofdistribution samples in the testing set."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,3.2,Experimental Design,"Baseline Methods: In this study, we compared the performance of the proposed model to three baseline models: (i) a UNet model [14], (ii) a ResNet-34 [4], and (iii) the ReNet-50 model used by Robinson et al. [12]. For a fair comparison, the residual blocks in the ResNet-34 and ResNet-50 were the same as that in the QCResUNet. We added an average pooling followed by a fully-connected layer to the last feature map of the UNet to predict a single DSC value. The evaluation was conducted on in-sample (nnUNet and SegGen) and out-of-sample segmentations generated by DeepMedic.Table 1. The QC performance of three baseline methods and the proposed method was evaluated on in-sample (nnUNet and SegGen) and out-of-sample (DeepMedic) segmentations. The best metrics in each column are highlighted in bold. DSCerr denotes the median DSC between Serr gt and Serr pred across all samples. Training Procedure: All models were trained for 150 epochs using an Adam optimizer with a L 2 weight decay of 5 × 10 -4 . The batch size was set to 4. Data augmentation, including random rotation, random scaling, random mirroring, random Gaussian noise, and Gamma intensity correction, was applied to prevent overfitting during training. We performed a random search [2] to determine the optimal hyperparameters (i.e., initial learning rate and loss weight balance parameter λ) on the training and validation set. The hyperparameters that yielded the best results were λ = 1 and an initial learning rate of 1 × 10 -4 . The learning rate was exponentially decayed by a factor of 0.9 at each epoch until 1 × 10 -6 . Model training was performed on four NVIDIA Tesla A100 and V100S GPUs. The proposed method was implemented in PyTorch v1.12.1."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Statistical Analysis:,We assessed the performance of the subject-level segmentation quality prediction in terms of Pearson coefficient r and MAE between the predicted DSC and the ground-truth DSC. The performance of the segmentation error localization was assessed by the DSC err between the predicted segmenta- tion error map and the ground-truth segmentation error map. P-values were computed using a paired t-test between DSC predicted by QCResUNet versus ones predicted by corresponding baselines.
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,4.0,Results,"The proposed QCResUNet achieved good performance in predicting subjectlevel segmentation quality for in-sample (MAE = 0.0570, r = 0.964) and out-ofsample (MAE = 0.0606, r = 0.966) segmentations. The proposed method also showed statistically significant improvement against all three baselines (Table 1 and Fig. 3). We found that the DSC prediction error (MAE) of the proposed method was distributed more evenly across different levels of quality than all baselines (see Fig. 3) with a smaller standard deviation of 0.050 for in-sample segmentations and 0.049 for out-of-sample segmentations. A possible explanation is that the joint training of predicting subject-level and voxel-level quality enabled the QCResUNet to learn deep features that better characterize the segmentation quality. For the voxel-level segmentation error localization task, the model achieved a median DSC of 0.834 for in-sample segmentations and 0.867 for out-of-sample segmentations. This error localization is not provided by any of the baselines and enables QCResUnet to track segmentation failures at different levels of segmentation quality (Fig. 4)."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,5.0,Conclusion,"In this work, we proposed a novel CNN architecture called QCResUNet to perform automatic brain tumor segmentation QC in multimodal MRI scans. QCRe-sUNet simultaneously provides subject-level segmentation-quality prediction and localizes segmentation failures at the voxel level. It achieved superior DSC prediction performance compared to all baselines. In addition, the ability to localize segmentation errors has the potential to guide the refinement of predicted segmentations in a clinical setting. This can significantly expedite clinical workflows, thus improving the overall clinical management of gliomas."
Boundary Difference over Union Loss for Medical Image Segmentation,1.0,Introduction,"Medical image segmentation is a vital branch of image segmentation [4,5,9,15,16,23], and can be used clinically for segmenting human organs, tissues, and lesions. Deep learning-based methods have made great progress in medical image segmentation tasks and achieved good performance, including early CNN-based methods [10,11,18,25], as well as more recent approaches utilizing Transformers [7,8,21,22,24].From CNN to Transformer, many different model architectures have been proposed, as well as a number of training loss functions. These losses can be mainly divided into three categories. The first class is represented by the Cross-Entropy Loss, which calculates the difference between the predicted probability distribution and the ground truth. Focal Loss [14] is proposed for addressing hard-to-learn samples. The second category is Dice Loss and other improvements. Dice Loss [17] is based on the intersection and union between the prediction and ground truth. Tversky Loss [19] improves the Dice Loss by balancing precision and recall. The Generalized Dice Loss [20] extends the Dice Loss to multi-category segmentation. The third category focuses on boundary segmentation. Hausdorff Distance Loss [12] is proposed to optimize the Hausdorff distance, and the Boundary Loss [13] calculates the distance between each point in the prediction and the corresponding ground truth point on the contour as the weight to sum the predicted probability of each point. However, the current loss for optimizing segmented boundaries dependent on combining different losses or training instability. To address these issues, we propose a simple boundary loss inspired by the Boundary IoU metrics [6], i.e., Boundary DoU Loss.Our proposed Boundary DoU Loss improves the focus on regions close to the boundary through a region-like calculation similar to Dice Loss. The error region near the boundary is obtained by calculating the difference set of ground truth and prediction, which is then reduced by decreasing its ratio to the union of the difference set and a partial intersection set. To evaluate the performance of our proposed Boundary DoU loss, we conduct experiments on the ACDC [1] and Synapse datasets by using the UNet [18], TransUNet [3] and Swin-UNet [2] models. Experimental results show the superior performance of our loss when compared with others."
Boundary Difference over Union Loss for Medical Image Segmentation,2.0,Method,"This section first revisit the Boundary IoU metric [6]. Then, we describe the details of our Boundary DoU loss function and adaptive size strategy. Next, we discuss the connection between our Boundary DoU loss with the Dice loss."
Boundary Difference over Union Loss for Medical Image Segmentation,2.1,Boundary IoU Metric,"The Boundary IoU is a segmentation evaluation metric which mainly focused on boundary quality. Given the ground truth binary mask G, the G d denotes the inner boundary region within the pixel width of d. The P is the predicted binary mask, and P d denotes the corresponding inner boundary region, whose size is determined as a fixed fraction of 0.5% relative to the diagonal length of the image. Then, we can compute the Boundary IoU metric by using following equation, as shown in the left of Fig. 1,A large Boundary IoU value indicates that the G d and P d are perfectly matched, which means G and P are with a similar shape and their boundary are well aligned. In practice, the G d and P d is computed by the erode operation [6]. However, the erode operation is non-differentiable, and we can not directly leverage the Boundary IoU as a loss function for training for increasing the consistency between two boundary areas."
Boundary Difference over Union Loss for Medical Image Segmentation,2.2,Boundary DoU Loss,"As shown in the left of Fig. 1, we can find that the union of the two boundaries Based on the above analysis, we design a Boundary DoU loss based on the difference region to facilitate computation and backpropagation. First, we directly treat the difference set as the miss-matched boundary between G and P . Besides, we consider removing the middle part of the intersection area as the inner boundary, which is computed by α * G∩P (α < 1) for simplicity. Then, we joint compute the G ∪ Pα * G ∩ P as the partial union. Finally, as shown in the right of Fig. 1, our Boundary DoU Loss can be computed by,where α is a hyper-parameter controlling the influence of the partial union area.Adaptive Adjusting α Based-on Target Size: On the other aspect, the proportion of the boundary area relative to the whole target varies for different sizes. When the target is large, the boundary area only accounts for a small proportion, and the internal regions can be easily segmented, so we are encouraged to focus more on the boundary area. In such a case, using a large α is preferred. However, when the target is small, neither the interior nor the boundary areas are easily distinguishable, so we need to focus simultaneously on the interior and boundary, and a small α is preferred. To achieve this goal, we future adaptively compute α based on the proportion,where C denotes the boundary length of the target, and S denotes its size. "
Boundary Difference over Union Loss for Medical Image Segmentation,2.3,Discussion,"In this part, we compare Boundary DoU Loss with Dice Loss. Firstly, we can re-write our Boundary DoU Loss as:where S D denotes the area of the difference set between ground truth and prediction, S I denotes the intersection area of them, and α = 1α. Meanwhile, the Dice Loss can be expressed by the following:where TP, FP and FN denote True Positive, False Positive, and False Negative, respectively. It can be seen that Boundary DoU Loss and Dice loss differ only in the proportion of the intersection area. Dice is concerned with the whole intersection area, while Boundary DoU Loss is concerned with the boundary since α < 1. Similar to the Dice loss function, minimizing the L DoU will encourage an increase of the intersection area (S I ↑) and a decrease of the different set (S D ↓). Meanwhile, the L DoU will penalize more over the ratio of S D /S I . To corroborate its effectiveness more clearly, we compare the values of L Dice and L DoU in different cases in Fig. 2. The L Dice decreases linearly with the difference set, whereas L DoU will decrease faster when S I is higher enough.  Evaluation Metrics: We use the most widely used Dice Similarity Coefficient (DSC) and Hausdorff Distances (HD) as evaluation metrics. Besides, the Boundary IoU [6] (B-IoU) is adopted as another evaluation metric for the boundary."
Boundary Difference over Union Loss for Medical Image Segmentation,3.2,Implementation Details,"We conduct experiments on three advanced models to evaluate the performance of our proposed Boundary DoU Loss, i.e., UNet, TransUNet, and Swin-UNet.The models are implemented with the PyTorch toolbox and run on an NVIDIA GTX A4000 GPU. The input resolution is set as 224 × 224 for both datasets.For the Swin-UNet [2] and TransUNet [3], we used the same training and testing parameters provided by the source code, i.e., the learning rate is set to 0.01, with a weight decay of 0.0001. The batch size is 24, and the optimizer uses SGD with a momentum of 0.9. For the UNet, we choose ResNet50 as the backbone and initialize the encoder with the ImageNet pre-trained weights following the setting in TransUNet [3]. The other configurations are the same as TransUNet. We train all models by 150 epochs on both Synapse and ACDC datasets. We further train the three models by different loss functions for comparison, including Dice Loss, Cross-Entropy Loss (CE), Dice+CE, Tversky Loss [19], and Boundary Loss [13]. The training settings of different loss functions are as follows. For the λ 1 Dice + λ 2 CE, we set (λ 1 , λ 2 ) as (0.5, 0.5) for the UNet and TransUNet, and (0.6, 0.4) for Swin-UNet. For the Tversky Loss, we set α = 0.7 and β = 0.3 by referring to the best performance in [19]. Following the Boundary Loss [13], we use L = α * (Dice + CE) + (1α) * Boundary for training . The α is initially set to 1 and decreases by 0.01 at each epoch until it equals 0.01."
Boundary Difference over Union Loss for Medical Image Segmentation,3.3,Results,"Quantitative Results: Table 1 shows the results of different losses on the Synapse dataset. From the table, we can have the following findings: 1) The original Dice Loss achieves the overall best performance among other losses. The CE loss function obtains a significantly lower performance than the Dice. Besides, the Dice+CE, Tversky, and Boundary do not perform better than Dice.2) Compared with the Dice Loss, our Loss improves the DSC by 2.30%, 1.20%, and 1.89% on UNet, TransUNet, and Swin-UNet models, respectively. The Hausdorff Distance also shows a significant decrease. Meanwhile, we achieved the best performance on the Boundary IoU, which verified that our loss could improve the segmentation performance of the boundary regions.Table 2 reports the results on the ACDC dataset. We can find that our Boundary DOU Loss effectively improves DSC on all three models. Compared with Dice Loss, the DSC of UNet, TransUNet, and Swin-UNet improved by 0.62%, 0.6%, and 0.85%, respectively. Although our Loss did not get all optimal performance for the Hausdorff Distance, we substantially outperformed all other losses on the Boundary IoU. These results indicate that our method can better segment the boundary regions. This capability can assist doctors in better identifying challenging object boundaries in clinical settings. Qualitative Results: Figures 3 and4 show the qualitative visualization results of our loss and other losses. Overall, our method has a clear advantage for segmenting the boundary regions. In the Synapse dataset (Fig. 3), we can achieve more accurate localization and segmentation for complicated organs such as the stomach and pancreas. Our results from the 3rd and 5th rows substantially outperform the other losses when the target is small. Based on the 2nd and last rows, we can obtain more stable segmentation on the hard-to-segment objects. As for the ACDC dataset (Fig. 4), due to the large variation in the shape of the RV region as shown in Row 1, 3, 4 and 6, it is easy to cause under-or missegmentation. Our Loss resolves this problem better compared with other Losses. Whereas the MYO is annular and the finer regions are difficult to segment, as shown in the 2nd and 5th row, the other losses all result in different degrees of under-segmentation, while our loss ensures its completeness. Reducing the misand under-classification will allow for better clinical guidance.Results of Target with Different Sizes: We further evaluate the influence of the proposed loss function for segmenting targets with different sizes. Based on the observation of C/S values for different targets, we consider a target to be a large one when C/S < 0.2 and otherwise as a small target. As shown in Table 3, our Boundary DoU Loss function can improve the performance for both large and small targets."
Boundary Difference over Union Loss for Medical Image Segmentation,4.0,Conclusion,"In this study, we propose a simple and effective loss (Boundary DoU) for medical image segmentation. It adaptively adjusts the penalty to regions close to the boundary based on the size of the different targets, thus allowing for better optimization of the targets. Experimental results on ACDC and Synapse datasets validate the effectiveness of our proposed loss function."
Boundary Difference over Union Loss for Medical Image Segmentation,,Table 2 .,"Ours 90.84 1.54 ± 0.33 76.44 91.29 2.16 ± 0.02 78.45 91.02 1.28 ± 0.00 77.00 3 Experiments 3.1 Datasets and Evaluation Metrics Synapse: 1 The Synapse dataset contains 30 abdominal 3D CT scans from the MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge. Each CT volume contains 86 ∼ 198 slices of 512 × 512 pixels. The slice thicknesses range from 2.5 mm to 5.0 mm, and in-plane resolutions vary from 0.54×0.54 mm 2 to 0.98×0.98 mm 2 . Following the settings in TransUNet [3], we randomly select 18 scans for training and the remaining 12 cases for testing."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,1.0,Introduction,"Deep neural networks (DNNs) have achieved phenomenal success in image analysis and comparable human performance in many semantic segmentation tasks.However, based on the assumption of DNNs, the training and testing data of the network should come from the same probability distribution [23]. The generalization ability of DNNs on unseen domains is limited. The lack of generalizability hinders the further implementation of DNNs in real-world scenarios.Ultrasound (US), as one of the most popular means of medical imaging, is widely used in daily medical practice to diagnose internal organs, such as vascular structures. Compared to other imaging methods, e.g., computed tomography (CT) and magnetic resonance imaging (MRI), US shows its advantages in terms of being radiation-free and portable. To accurately and robustly extract the vascular lumen for diagnosis, the Doppler signal [9] and artery pulsation signal [6] were employed to facilitate vessel segmentation. However, the US image quality is operator-dependent and sensitive to inter-machine and inter-patient variations. Therefore, the performance of the US segmentation is often decayed due to the domain shift caused by the inconsistency between the training and test data [8].Data Augmentation. One of the most common ways of improving the generalization ability of DNNs is to increase the variability of the dataset [26]. However, in most clinical cases, the number of data is limited. Therefore, data augmentation is often used as a feasible method to increase diversity. Zhang et al. proposed BigAug [27], a deep stacked transformation method for 3D medical image augmentation. By applying a wide variety of augmentation methods to the single source training data, they showed the trained network is able to increase its performance on unseen domains. In order to take the physics of US into consideration, Tirindelli et al. proposed a physics-inspired augmentation method to generate realistic US images [21].Image-Level Domain Adaptation. To make the network generalizable to target domains that are different from the source domain, the most intuitive way is to transfer the image style to the same domain. The work from Chen et al. achieved impressive segmentation results in MRI to CT adaptation by applying both image and feature level alignment [4]. To increase the robustness of segmentation networks for US images, Yang et al. utilized a rendering network to unify the image styles of training and test data so that the model is able to perform equally well on different domains [25]. Velikova et al. extended this idea by defining a common anatomical CT-US space so that the labeled CT data can be exploited to train a segmentation network for US images [24].Feature Disentanglement. Instead of solving the domain adaptation problem directly at the image-level, many researchers focused on disentangling the features in latent space, forcing the network to learn the shared statistical shape model across different domains [2]. One way of realizing this is through adversarial learning [7,11,15,28]. However, adversarial learning optimization remains difficult and unstable in practice [12]. A promising solution for decoupling latent representations is to minimize a metric that can explicitly measure the shared information between different features. Mutual information (MI), which measures the amount of shared information between two random variables [10], suits this demand. Previous researches have exploited its usage in increasing the generalizability for classification networks when solving the vision recognition [3,13,16] and US image classification [14] problems. In this study, we investigate the effective way to integrate MI into a segmentation network in order to improve the adaptiveness on unseen images.To solve the performance drop caused by the domain shift in segmentation networks, the aforementioned methods require a known target domain, e.g., CT [4], MRI [15], contrast enhanced US [28]. However, compared to MRI and CT, the image quality of US is more unstable and unpredictable. It is frequently observed that the performance of a segmentation network decreases dramatically for the US images acquired from a different machine or even with a different set of acquisition parameters. In such cases, it is impractical to define a so-called target US domain. Here we introduce MI-SegNet, an MI-based segmentation network, to address the domain shift problem in US image segmentation. Specifically, the proposed network extracts the disentangled domain (image style) and anatomical (shape) features from US images. The segmentation mask is generated based on the anatomical features, while the domain features are explicitly excluded. Thereby, the segmentation network is able to understand the statistical shape model of the target anatomy and generalize to different unseen scenarios. The ablation study shows that the proposed MI-SegNet is able to increase the generalization ability of the segmentation network in unseen domains."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.0,Method,"Our goal is to train a segmentation network that can generalize to unseen domains and serve as a good pre-trained model for downstream tasks, while the training dataset only contains images from a single domain. To this end, the training framework should be designed to focus on the shape of the segmentation target rather than the background or appearance of the images. Following this concept of design, we propose MI-SegNet. During the training phase, a parameterised data transformation procedure is undertaken for each training image (x). Two sets of parameters are generated for spatial (a 1 , a 2 ) and domain (d 1 , d 2 ) transformation respectively. For individual input, four transformed images (x a1d1 , x a2d2 , x a1d2 , x a2d1 ) are created according to the four possible combinations of the spatial and domain configuration parameters. Two encoders (E a , E d ) are applied to extract the anatomical features (f a1 , f a2 ) and domain features (f d1 , f d2 ) separately. The mutual information between the extracted anatomical features and the domain features from the same image is computed using mutual information neural estimator (MINE) [1] and minimized during training. Only the anatomical features are used to compute segmentation masks (m 1 , m 2 ). The extracted anatomical and domain features are then combined and fed into the generator network (G) to reconstruct the images ( x a1d1 , x a1d2 , x a2d1 , x a2d2 ) accordingly. Since the images are transformed explicitly, it is possible to provide direct supervision to the reconstructed images.Notably, only two of the transformed images (x a1d1 , x a2d2 ) are fed into the network, while the other two (x a1d2 , x a2d1 ) are used as ground truth for reconstructions (Fig. 1). "
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.1,Mutual Information,"In order to decouple the anatomical and domain features intuitively, a metric that can evaluate the dependencies between two variables is needed. Mutual information, by definition, is a metric that measures the amount of information obtained from a random variable by observing another random variable. The MI is defined as the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginal distributions of random variables f a and f d :where p(f a , f d ) is the joint distribution and p(f a ) ⊗ p(f d ) is the product of the marginal distributions. Based on the Donsker-Varadhan representation [5], the lower bound of MI can be represented as:where T is any arbitrary given continuous function. By replacing T with a neural network T θMINE and applying Monte Carlo method [16], the lower bound can be calculated as:where (f a , f d ) are drawn from the joint distribution and (f a , f d ) are drawn from the product of marginal distributions. By updating the parameters θ MINE to maximize the lower bound expression in Eq. 3, a loose estimation of MI is achieved, also known as MINE [1].To force the anatomical and domain encoders to extract decoupled features, the MI is served as a loss to update the weights of these two encoder networks. The loss is defined as:"
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.2,Image Segmentation and Reconstruction,"To make the segmentation network independent of the domain information, the domain features are excluded when generating the segmentation mask. Here, the segmentation loss L seg is defined in the combined form of dice loss L dice and binary cross-entropy loss L bce .where l is the ground truth label, m represents the predicted mask, s is added to ensure the numerical stability, and N is the mini batch size.To ensure that the extracted anatomical and domain features can contain all the information of the input image, a generator network is used to reconstruct the image based on both features. The reconstruction loss is then defined as:where x n is the ground truth image, x n is the reconstructed image, w and h are the width and height of the image in pixel accordingly."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.3,Data Transformation,"Since the training dataset only contains images from one single domain, it is necessary to enrich the diversity of the training data so that overfitting can be prevented and the generalization ability is increased. The transformation methods are divided into two categories, domain and spatial transformations. Each transformation (T ) is controlled by two parameters, probability (p) and magnitude (λ).Domain Transformations aim to transfer the single domain images to different domain styles. Five types of transformation methods are involved in this aspect, i.e., blurriness, sharpness, noise level, brightness, and contrast. The implementations are identical to [27], except the Gaussian noise is replaced by Rayleigh noise. The possibility of all the domain transformations are empirically set to 10%.Spatial Transformations mainly consist of two parts, crop and flip. For cropping, a window with configurable sizes ([0.7, 0.9] of the original image size) is randomly masked on the original image. Then the cropped area is resized to the original size to introduce varying shapes of anatomy. Here λ controls the size and the position of the cropping window. Besides cropping, horizontal flipping is also involved. Unlike domain transformations, the labels are also transformed accordingly by the same spatial transformation. The probability (p) of flipping is 5%, while the p for cropping is 50% to introduce varying anatomy sizes.The images are then transformed in a stacked way:where n = 7 represents the seven different transformation methods involved in our workrepresents the magnitude parameter, andcontains all the probability parameters for each transformations. In our setup, Λ and P can be further separated into a = [Λ a ; P a ] and d = [Λ d ; P d ] for spatial and domain transformations respectively."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.4,Cross Reconstruction,"According to experimental findings, the MI loss indeed forces the two representations to have minimal shared information. However, the minimization of MI between the anatomical and domain features cannot necessarily make both features contain the respective information. The network goes into local optimums frequently, where the domain features are kept constant, and all the information is stored in the anatomical features. Because there is no information in the domain features, the MI between two representations is thus approaching zero. However, this is not our original intention. As a result, cross reconstruction strategy is introduced to tackle this problem. The cross reconstruction loss will punish the behavior of summarizing all the information into one representation. Thus, it can force each encoder to extract informative features accordingly and prevent the whole network from going into the local optimums."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,3.1,Implementation Details,"The training dataset consists of 2107 carotid US images of one adult acquired using Siemens Juniper US Machine (ACUSON Juniper, SIEMENS AG, Germany) with a system-predefined ""Carotid"" acquisition parameter. The test dataset consists of (1) ValS: 200 carotid US images which are left out from the training dataset, (2) TS1: 538 carotid US images of 15 adults from Ultrasonix device, (3) TS2: 433 US images of 2 adults and one child from Toshiba device, and (4) TS3: 540 US images of 6 adults from Cephasonics device (Cephasonics, California, USA). TS1 and TS2 are from a public database of carotid artery [17]. Notably, due to the absence of annotations, the publicly accessed images were also annotated by ourselves under the supervision of US experts. The acquisition was performed within the Institutional Review Board Approval by the Ethical Commission of the Technical University of Munich (reference number 244/19 S).All the images are resized to 256 × 256 for training and testing. We use Adam optimizer with a learning rate of 1 × 10 -4 to optimize all the parameters. The training is carried out on a single GPU (Nvidia TITAN Xp) with 12 GB memory."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,3.2,Performance Comparison on Unseen Datasets,"In this section, we compare the performance of the proposed MI-SegNet with other state-of-art segmentation networks. All the networks are trained on the same dataset described in Sect. 3.1 with 200 episodes.Without Adaptation: The trained models are then tested directly on 4 different datasets described in Sect. 3.1 without further training or adaptation on the unseen domains. The dice score (DSC) is applied as the evaluation metrics. The results are shown in Table 1. Compared to the performance on ValS, all networks demonstrate a performance degradation on unseen datasets (TS1, TS2, and TS3). In order to validate the effectiveness of the MI loss as well as the cross reconstruction design, two ablation networks (MI-SegNet w/o L MI and MI-SegNet w/o cross rec.) are introduced here for comparison. The visual comparisons are shown in Fig. 2. The results on TS1 are the best among all three unseen datasets while the scores on TS3 are the worst for most networks, which indicates that the domain similarity between the source and target domain decreases accordingly from TS1 to TS3. The MI-SegNet performs the best among others on all three unseen datasets, which showcases the high generalization ability of the proposed framework. After Adaptation: Although the proposed network achieves the best scores when applied directly to unseen domains, performance decay still occurs. Using it directly to unseen dataset with degraded performance is not practical. As a result, adaptation on the target domain is needed. The trained models in Sect. 3.2 are further trained with 5% data of each unseen test dataset. The adapted models are then tested on the rest 95% of each dataset. Notably, for the MI-SegNet only the anatomical encoder and segmentor are involved in this adaptation process, which means the network is updated solely based on L seg . The intention of this experiment is to validate whether the proposed network can serve as a good pre-trained model for the downstream task. A well-trained pre-trained model, which can achieve good results when only a limited amount of annotations is provided, has the potential to release the burden of manual labeling and adapts to different domains with few annotations. Table 2 shows that the MI-SegNet performs the best on all test datasets. However, the difference is not that significant as in Table 1 when no data is provided for the target domain. This is partially due to the fact that carotid artery is a relatively easy anatomy for segmentation. It is observed that when more data (10%) is involved in the adaptation process GLFR and Att-UNet tend to outperform the others and it can be therefore expected when the data size further increases all the networks will perform equally well on each test set."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,4.0,Discussion and Conclusion,"In this paper, we discuss the particular importance of domain adaptation for US images. Due to the low speed of sound compared to light and X-ray, the complexity of US imaging and its dependency on many parameters are more remarkable than optical imaging, X-ray, and CT. Therefore, the performance decay caused by the domain shift is a prevalent issue when applying DNNs in US images. To address this problem, a MI-based disentanglement method is applied to increase the generalization ability of the segmentation networks for US image segmentation. The ultimate goal of increasing the generalizability of the segmentation network is to apply the network to different unseen domains directly without any adaptation process. However, from the authors' point of view, training a good pre-trained model that can be adapted to an unseen dataset with minimal annotated data is still meaningful. As demonstrated in Sect. 3.2, the proposed model also shows the best performance in the downstream adaptation tasks. Currently, only the conventional image transformation methods are involved. In the future work, more realistic and US specific image transformations could be implemented to strengthen the feature disentanglement."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_13.
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,1.0,Introduction,"Malignant melanoma is one of the most rapidly growing cancers in the world. As estimated by the American Cancer Society, there were approximately 100,350 new cases and over 6,500 deaths in 2020 [14]. Thus, an automated skin lesion segmentation system is imperative, as it can assist medical professionals in swiftly identifying lesion areas and facilitating subsequent treatment processes. To enhance the segmentation performance, recent studies tend to employ modules with larger parameter and computational complexity, such as incorporating self-attention mechanisms of Vision Transformer (ViT) [7]. For example, Swin-UNet [4], based on the Swin Transformer [11], leverages the feature extraction ability of self-attention mechanisms to improve segmentation performance.  b) respectively show the visualization of comparative experimental results on the ISIC2017 and ISIC2018 datasets. The X-axis represents the number of parameters (lower is better), while Y-axis represents mIoU (higher is better). The color depth represents computational complexity (GFLOPs, lighter is better). (Color figure online)TransUNet [5] has pioneered a serial fusion of CNN and ViT for medical image segmentation. TransFuse [26] employs a dual-path structure, utilizing CNN and ViT to capture local and global information, respectively. UTNetV2 [8] utilizes a hybrid hierarchical architecture, efficient bidirectional attention, and semantic maps to achieve global multi-scale feature fusion, combining the strengths of CNN and ViT. TransBTS [23] introduces self-attention into brain tumor segmentation tasks and uses it to aggregate high-level information.Prior works have enhanced performance by introducing intricate modules, but neglected the constraint of computational resources in real medical settings. Hence, there is an urgent need to design a low-parameter and low-computational load model for segmentation tasks in mobile healthcare. Recently, UNeXt [22] has combined UNet [18] and MLP [21] to develop a lightweight model that attains superior performance, while diminishing parameter and computation. Furthermore, MALUNet [19] has reduced the model size by declining the number of model channels and introducing multiple attention modules, resulting in better performance for skin lesion segmentation than UNeXt. However, while MALUNet greatly reduces the number of parameter and computation, its segmentation performance is still lower than some large models, such as Trans-Fuse. Therefore, in this study, we propose EGE-UNet, a lightweight skin lesion segmentation model that achieves state-of-the-art while significantly reducing parameter and computation costs. Additionally, to our best knowledge, this is the first work to reduce parameter to approximately 50KB.To be specific, EGE-UNet leverages two key modules: the Group multi-axis Hadamard Product Attention module (GHPA) and Group Aggregation Bridge module (GAB). On the one hand, recent models based on ViT [7] have shown promise, owing to the multi-head self-attention mechanism (MHSA). MHSA divides the input into multiple heads and calculates self-attention in each head, which allows the model to obtain information from diverse perspectives, integrate different knowledge, and improve performance. Nonetheless, the quadratic complexity of MHSA enormously increases the model's size. Therefore, we present the Hadamard Product Attention mechanism (HPA) with linear complexity. HPA employs a learnable weight and performs a hadamard product operation with the input to obtain the output. Subsequently, inspired by the multi-head mode in MHSA, we propose GHPA, which divides the input into different groups and performs HPA in each group. However, it is worth noting that we perform HPA on different axes in different groups, which helps to further obtain information from diverse perspectives. On the other hand, for GAB, since the size and shape of segmentation targets in medical images are inconsistent, it is essential to obtain multi-scale information [19]. Therefore, GAB integrates high-level and low-level features with different sizes based on group aggregation, and additionally introduce mask information to assist feature fusion. Via combining the above two modules with UNet, we propose EGE-UNet, which achieves excellent segmentation performance with extremely low parameter and computation. Unlike previous approaches that focus solely on improving performance, our model also prioritizes usability in real-world environments. A clear comparison of EGE-UNet with others is shown in Fig. 1.In summary, our contributions are threefold: (1) GHPA and GAB are proposed, with the former efficiently acquiring and integrating multi-perspective information and the latter accepting features at different scales, along with an auxiliary mask for efficient multi-scale feature fusion. (2) We propose EGE-UNet, an extremely lightweight model designed for skin lesion segmentation.(3) We conduct extensive experiments, which demonstrate the effectiveness of our methods in achieving state-of-the-art performance with significantly lower resource requirements."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,2.0,EGE-UNet,"The Overall Architecture. EGE-UNet is illustrated in Fig. 2, which is built upon the U-Shape architecture consisting of symmetric encoder-decoder parts. We take encoder part as an example. The encoder is composed of six stages, each with channel numbers of {8, 16, 24, 32, 48, 64}. While the first three stages employ plain convolutions with a kernel size of 3, the last three stages utilize the proposed GHPA to extract representation information from diverse perspectives. In contrast to the simple skip connections in UNet, EGE-UNet incorporates GAB for each stage between the encoder and decoder. Furthermore, our model leverages deep supervision [27] to generate mask predictions of varying scales, which are utilized for loss function and serve as one of the inputs to GAB. Via the integration of these advanced modules, EGE-UNet significantly reduces the parameter and computational load while enhancing the segmentation performance compared to prior approaches. Group Multi-axis Hadamard Product Attention Module. To overcome the quadratic complexity issue posed by MHSA, we propose HPA with linear complexity. Given an input x and a randomly initialized learnable tensor p, bilinear interpolation is first utilized to resize p to match the size of x. Then, we employ depth-wise separable convolution (DW) [10,20] on p, followed by a hadamard product operation between x and p to obtain the output. However, utilizing simple HPA alone is insufficient to extract information from multiple perspectives, resulting in unsatisfactory results. Motivated by the multi-head mode in MHSA, we introduce GHPA based on HPA, as illustrated in Algorithm 1. We divide the input into four groups equally along the channel dimension and perform HPA on the height-width, channel-height, and channel-width axes for the first three groups, respectively. For the last group, we only use DW on the feature map. Finally, we concatenate the four groups along the channel dimension and apply another DW to integrate the information from different perspectives. Note that all kernel size employed in DW are 3.Group Aggregation Bridge Module. The acquisition of multi-scale information is deemed pivotal for dense prediction tasks, such as medical image segmentation. Hence, as shown in Fig. 3, we introduce GAB, which takes three inputs: low-level features, high-level features, and a mask. Firstly, depthwise separable convolution (DW) and bilinear interpolation are employed to adjust the size of high-level features, so as to match the size of low-level features. Secondly, we partition both feature maps into four groups along the channel dimension, and concatenate one group from the low-level features with one from the high-level features to obtain four groups of fused features. For each group of fused features, the mask is concatenated. Next, dilated convolutions [25] with kernel size of 3 and different dilated rates of {1, 2, 5, 7} are applied to the different groups, in order to extract information at different scales. Finally, the four groups are concatenated along the channel dimension, followed by the application of a plain convolution with the kernel size of 1 to enable interaction among features at different scales.Loss Function. In this study, since different GAB require different scales of mask information, deep supervision [27] is employed to calculate the loss function for different stages, in order to generate more accurate mask information. Our loss function can be expressed as Eqs. (1) and (2). l i = Bce(y, ŷ) + Dice(y, ŷ)(1)where Bce and Dice represent binary cross entropy and dice loss. λ i is the weight for different stage. In this paper, we set λ i to 1, 0.5, 0.4, 0.3, 0.2, 0.1 from i = 0 to i = 5 by default."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,3.0,Experiments,"Datasets and Implementation Details. To assess the efficacy of our model, we select two public skin lesion segmentation datasets, namely ISIC2017 [1,3] and ISIC2018 [2,6], containing 2150 and 2694 dermoscopy images, respectively. Consistent with prior research [19], we randomly partition the datasets into training and testing sets at a 7:3 ratio. EGE-UNet is developed by Pytorch [17] framework. All experiments are performed on a single NVIDIA RTX A6000 GPU. The images are normalized and resized to 256 × 256. We apply various data augmentation, including horizontal flipping, vertical flipping, and random rotation. AdamW [13] is utilized as the optimizer, initialized with a learning rate of 0.001 and the CosineAnnealingLR [12] is employed as the scheduler with a maximum number of iterations of 50 and a minimum learning rate of 1e-5. A total of 300 epochs are trained with a batch size of 8. To evaluate our method, we employ Mean Intersection over Union (mIoU), Dice similarity score (DSC) as metrics, and we conduct 5 times and report the mean and standard deviation of the results for each dataset."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Comparative Results. The comparative experimental results presented in,"Table 1 reveal that our EGE-UNet exhibits a comprehensive state-of-the-art performance on the ISIC2017 dataset. Specifically, in contrast to larger models, such as TransFuse, our model not only demonstrates superior performance, but also significantly curtails the number of parameter and computation by 494x and 160x, respectively. In comparison to other lightweight models, EGE-UNet surpasses UNeXt-S with a mIoU improvement of 1.55% and a DSC improvement of 0.97%, while exhibiting parameter and computation reductions of 17% and 72% of UNeXt-S. Furthermore, EGE-UNet outperforms MALUNet with a mIoU improvement of 1.03% and a DSC improvement of 0.64%, while reducing parameter and computation to 30% and 85% of MALUNet. For the ISIC2018 dataset, the performance of our model also outperforms that of the best-performing model. Besides, it is noteworthy that EGE-UNet is the first lightweight model reducing parameter to about 50KB with excellent segmentation performance.Figure 1 presents a more clear visualization of the experimental findings and Fig. 4 shows some segmentation results.Ablation Results. We conduct extensive ablation experiments to demonstrate the effectiveness of our proposed modules. The baseline utilized in our work is referenced from MALUNet [19], which employs a six-stage U-shaped architecture with symmetric encoder and decoder components. Each stage includes a plain convolution operation with a kernel size of 3, and the number of channels at each stage is set to {8, 16, 24, 32, 48, 64}. In Table 2(a), we conduct macro ablations on GHPA and GAB. Firstly, we replace the plain convolutions in the last three layers of baseline with GHPA. Due to the efficient multi-perspective feature acquisition of GHPA, it not only outperforms the baseline, but also greatly reduces the parameter and computation. Secondly, we substitute the skip-connection operation in baseline with GAB, resulting in further improved performance. Table 2(b) presents the ablations for GHPA. We replace the multiaxis grouping with single-branch and initialize the learnable tensors with only random values. It is evident that the removal of these two key designs leads to a marked drop. Table 2(c) illustrates the ablations for GAB. Initially, we omit the mask information, and mIoU metric even drops below 79%, thereby confirming once again the critical role of mask information in guiding feature fusion. Furthermore, we substitute the dilated convolutions in GAB with plain convolutions, which also leads to a reduction in performance."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,4.0,Conclusions and Future Works,"In this paper, we propose two advanced modules. Our GHPA uses a novel HPA mechanism to simplify the quadratic complexity of the self-attention to linear complexity. It also leverages grouping to fully capture information from different perspectives. Our GAB fuses low-level and high-level features and introduces a mask to integrate multi-scale information. Based on these modules, we propose EGE-UNet for skin lesion segmentation tasks. Experimental results demonstrate the effectiveness of our approach in achieving state-of-the-art performance with significantly lower resource requirements. We hope that our work can inspire further research on lightweight models for the medical image community.Regarding limitations and future works, on the one hand, we mainly focus on how to greatly reduce the parameter and computation complexity while improving performance in this paper. Thus, we plan to deploy EGE-UNet in a real-world environment in the future work. On the other hand, EGE-UNet is currently designed only for the skin lesion segmentation task. Therefore, we will extend our lightweight design to other tasks."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,1.0,Introduction,"Quantitative analysis of structural MRI of the human brain is essential in various anatomical investigations in brain development, aging, and degradation. Accurate segmentation of brain structures is a prerequisite for quantitative and particularly morphometric analysis [5,8,23]. However, whole brain segmentation is challenging due to the low soft-tissue contrast, the high anatomical variability, and the limited labeled data, especially for fine-grained structures.During the past decade, MRI based whole brain segmentation approaches have been explored. Multi-atlas based methods [19,24] are shown to be simple yet effective and serve as the de facto standard whole brain segmentation methods. With a limited number of labeled data, multi-atlas based methods propagate labels from the atlas images to the target images through registration (mostly non-linear or even diffeomorphic registration), joint label fusion, and possibly corrective learning. With diffeomorphic registration, multi-atlas based methods are empowered of enhanced segmentation accuracy and topology-preserving capabilities that well accommodate the potential need of incorporating shape prior [4]. Nevertheless, multi-atlas based methods suffer from low computational efficiency and fail to leverage sophisticated contextual information when the tissue contrast in the structural MRI data is low.Recently, deep learning has demonstrated state-of-the-art (SOTA) performance on various medical image segmentation tasks [6,12,16], also serving as an alternative solution to the whole brain segmentation task. QuickNAT [21] trains three 2D U-Nets respectively on the axial, coronal and sagittal views, and then aggregates them to infer the final segmentation. Pre-training with auxiliary labels derived from Freesurfer is conducted to alleviate QuickNAT's reliance on manually annotated data. SLANT [11] applies multiple independent 3D U-Nets to segment brain structures in overlapped MNI subspace, followed by label fusion. Wu et al. [26] propose a multi-atlas and diffeomorphism based encoding block to determine the most similar atlas patches to a target patch and propagate them into 3D fully convolutional networks. These deep learning methods improve over conventional multi-atlas based methods by considerable margins in terms of both computational efficiency and segmentation accuracy.However, existing methods tend to ignore the ontology-based hierarchical structural relationship (OHSR) of the human brain's anatomy. Most of them assume all brain structures are disjoint and use multiple U-Nets to separately perform voxel-wise predictions for each of multiple structures of interest. It has been suggested that neuroanatomical experts recognize and delineate brain anatomy in a coarse-to-fine manner (Fig. 1) [17,18]. Concretely, at the highest level of the most coarse granularity (Level 1 in Fig. 1), the brain can be simply decomposed into telencephalon, diencephalon, mesencephalon, metencephalon, and myelencephalon. At a lower level, the cerebral cortex, cerebral nuclei and white matter can be identified within the telencephalon. At the lowest level of the most fine granularity, namely Level 5 in Fig. 1, the most fine-grained structures such as the hippocampus and the amygdala in the limbic system are characterized. It shall be most desirable if a neural network can learn brain anatomy in a similar fashion. Inspired by OHSR of brain anatomy, in the manifold space of a neural network, the distance between the feature vectors of the hippocampus and the amygdala should be smaller than that between the feature vectors of the hippocampus and any other structure that does not also belong to the limbic system. In other words, embeddings of fine-grained structures labeled as a same class at a higher level are supposed to be more similar than those of finegrained structures labeled as different classes at the same higher level. Such prior knowledge on brain anatomy has been ignored or only implicitly learned in both multi-atlas based and deep learning based whole brain segmentation methods. Moreover, image contrast is not the only anatomical clue to discriminate structure boundary, especially for fine-grained structures. For instance, the anterior limb and posterior limb of the internal capsule are both part of white matter, which cannot be separated based on intensities and contrasts but can be differentiated by the sharp bend and feature-rich neighboring gray matter anatomy. This further suggests the importance of exploring and capturing OHSR.To mimic experts' hierarchical perception of the brain anatomy, we here propose a novel approach to learn brain's hierarchy based on ontology, for a purpose of whole brain segmentation. Specifically, we encode the multi-level ontology knowledge into a voxel-wise embedding space. Deep metric learning is conducted to cluster contextually similar voxels and separate contextually dissimilar ones using a triplet loss with dynamic violate margin. By formatting the brain hierarchy into a directed acyclic graph, the violate margin can be easily induced by the height of the tree rooted at triplet's least common subsumer. As a result, the network is able to exploit the hierarchical relationship across different brain structures. The feature prototypes in the latent space are hierarchically organized following the brain hierarchy. To the best of our knowledge, this is the first work to incorporate ontology-based brain hierarchy into deep learning segmentation models. We evaluate our method on two whole brain segmentation datasets with different granularity and successfully establish SOTA performance."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.0,Methodology,"The proposed approach builds upon standard 3D U-Net and makes use of multilevel ontology knowledge from the brain hierarchy to enhance the whole brain segmentation performance. In subsequent subsections, we first revisit the standard triplet loss [9,14] and its dilemma in learning brain hierarchy. Then we describe how we construct the brain hierarchy graph and how we measure the semantic similarity between brain structures based on the constructed graph. After that, we introduce dynamic violate margin to the triplet loss. An intuitive illustration of hierarchy-based triplet loss and its corresponding toy example graph of brain hierarchy. The loss tends to group structures labeled as a same class at a higher ontology level."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.1,Triplet Loss,"The goal of the triplet loss is to learn a feature embedding space wherein distances between features correspond to semantic dissimilarities between objects. Given a triplet T i = {f i , f + i , f - i } comprising an anchor voxel-wise feature vector f i , a positive voxel-wise feature vector f + i which is semantically similar to the anchor vector, and a negative voxel-wise feature vector f - i which is semantically dissimilar to the anchor vector, the triplet loss is formulated aswhere •, • is a distance function to evaluate the semantic dissimilarity between two feature vectors. The violate margin M is a hyperparameter that defines the minimum distance between positive and negative samples. It forces the gap between f, f + and f, f -to be larger than M and ensures the model does not learn trivial solutions. [•] + = max{0, •} is the hinge loss, which prevents the model from being updated when the triplet is already fulfilled. During training, the overall objective of the voxel-wise triplet loss is to minimize the sum of the loss over all triplets in a mini-batch, namelywhere N is the total number of triplets in a mini-batch. Note that a triplet is allowed to consist of voxel-wise feature vectors from different subjects when the batch size is larger than 1. This strategy of sampling an anchor's neighbor enables the model to learn the global context in the brain instead of the local context in a subspace of the brain, since it is infeasible to train a 3D U-Net with whole brain MRI data.However, it is challenging to apply the standard triplet loss to learn brain hierarchy: postive or negative is ill-defined with a fixed violate margin. For instance, the violate margin between f hippo , f amyg and f hippo , f fimb is certainly different from that between f hippo , f amyg and f hippo , f IIIvent : the hippocampus, the amygdala, and the fimbria all belong to the limbic system while the third ventricle belongs to cerebrospinal fluid (CSF). As such, a distance function d G (•, •) is required to measure the semantic dissimilarity between two brain structures, and can be then used to determine the corresponding violate margin."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.2,Measuring Semantic Dissimilarity,"Let G = (V, E) be a directed acyclic graph with vertices V and edges E ⊆ V 2 . It specifies the hyponymy relationship between structures at different ontology levels. An edge (u, v) ∈ E indicates u is an ancestor vertex of v. Specifically, v belongs to u at a higher ontology level. The structures of interest S = {s 1 , ..., s n } ⊆ V are of the lowest ontology level. An example is shown in Fig. 2.A common measure for the dissimilarity d G : S 2 → R between two structures is the height of the tree rooted at the least common subsumer (LCS) divided by the height of the whole brain hierarchy tree, namelywhere the height of a tree h(•) = max v∈V ψ(•, v) is defined as the length of the longest path from the root to a leaf. ψ(•, •) is defined as the number of edges in the shortest path between two vertices. lcs(•, •) refers to the ancestor shared by two vertices that do not have any child also being an ancestor of the same two vertices. With respect to the example hierarchy in Fig. 2, the LCS of the hippocampus and the amygdala is the limbic and the LCS of the hippocampus and the third ventricle is the brain. Given the height of the example hierarchy is 4, we can easily derive that d G (hippo, amyg) = 1 4 and d G (hippo, IIIvent) = 1. Non-negativity, symmetry, identity of indiscernibles, and triangle inequality always hold for d G (•, •) since the brain hierarchy is a tree and all structures of interest are leaf vertices, thus being a proper metric [2]."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.3,Dynamic Violate Margin,"With d G (•, •), we can define positive and negative samples and their violate margin in the triplet loss. We sample triplet, where f, f + i , f - i are the feature vectors of the voxels respectively labeled as v, v + i , v - i . Then the violate margin M can be determined dynamically M = 0.5(M τ + M ),M τ ∈ (0, 1] is the hierarchy-induced margin required between negative pairs and positive pairs in terms of d G (•, •). M is the tolerance of the intra-class variance, which is computed as the average distance between samples in v. In this work, we adopt the cosine distance as our distance function in latent space:. The triplet loss can thus be reformulated asCollectively, the overall training objective to learn OHSR iswhere λ is a hyperparameter."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,3.1,Datasets and Implementation,"We evaluate our method on two public-accessible datasets with manually labeled fine-grained or coarse-grained brain structures. The first one is JHU Adult Atlas [25], containing 18 subjects with an age range of 27-55 years. T1-weighted MRI images are acquired in the MPRAGE sequence at Johns Hopkins University using 3T Philips scanners. All images are normalized to the MNI152 1mm space. The images are initially segmented into 289 structures using a single-subject atlas followed by substantial manual corrections. The optic tract, skull and bone marrow are excluded from our experiment, ending up with 282 structures. Two types of five-level OHSR are provided. Type I is based on classical definitions of the brain ontology and Type II is more commonly used in clinical descriptions [1,7,10]. The second one is Child and Adolescent Neuro Development Initiative (CANDI) [13], consisting of 103 1.5T T1-weighted MRI scans with 32 labeled structures. The subjects are aged 4-17 and come from both healthy and neurological disorder groups. A two-level hierarchical relationship is built by grouping the 32 structures into white matter, gray matter or CSF. For JHU Adult Atlas, 8, 2 and 8 images are randomly selected and used for training, validation and testing. For CANDI, we split the dataset into training (60%), validation (10%), and testing (30%) sets following a previous study [15]. Foreground image patches of size 96 × 112 × 96 are randomly cropped as the input of our model. To enlarge the training set, random scaling, gamma correction and rotation are applied. Image intensities are normalized using Z-score normalization. AdamW with a batch size of 4 is used to optimize the training objective, wherein L seg = L dice + L ce . L dice and L ce are respectively the Dice loss and the cross entropy loss. The hyperparameter λ ∈ [0, 0.5] is scheduled following a cosine annealing policy. The initial learning rate is 5 × 10 -4 and decays following a polynomial function. The model is trained for 30,000 steps, with the best model saved based on the validation Dice. All experiments are conducted using PyTorch 1.13.1 with NVIDIA Tesla V100 GPUs. More dataset and implementation details are provided in the supplementary material.Table 1. Comparisons with SOTA for segmenting 282 fine-grained brain structures on JHU Adult Atlas, in terms of DSC. ""S"" denotes small structures with a size smaller than 1000 mm 3 . ""M"" denotes medium structures with a size between 1000 mm 3 and 5000 mm 3 . ""L"" denotes large structures with a size larger than 5000 mm  "
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,3.2,Evaluation Results,"We now report quantitative and qualitative evaluation results. The Dice similarity coefficient (DSC) is used to quantitatively measure the segmentation accuracy. We compare our proposed method with SOTA methods and conduct several ablation studies to demonstrate the effectiveness of our method.Comparisons with SOTA. To fairly compare with SOTA methods, we use the identical 3D U-Net and data augmentation hyperparameters as the ""3d fullres nnUNetTrainerV2 noMirroring"" configuration in nnU-Net. As summarized in Table 1, our method obtains the highest overall mean DSC of 83.67% on JHU Adult Atlas, with an improvement of 0.70% over previous best method, i.e., nnU-Net. The improvements are consistent over small, medium and large brain structures, demonstrating the robustness of our method. The boxplot comparison results are illustrated in Fig. 3. Detailed improvement of each structure is presented in the supplementary material. Table 2 shows the results on CANDI. Please note all compared results except for nnU-Net in that table are directly copied from ACEnet paper [15] since CANDI has been explored more than JHU  Adult Atlas. Our method achieves an average DSC of 88.23%, performing not only better than the hyperparameter-tuning method nnU-Net, but also better than ACEnet encoding anatomical context via an attention mechanism. These results also suggest that our method can leverage both simple (two-level) and sophisticated (five-level) hierarchies to enhance whole brain segmentation.Ablation Studies. We first evaluate the effectiveness of incorporating OHSR into U-Net. The experiments are conducted on JHU Adult Atlas. As shown in Table 3, learning OHSR using a triplet loss with graph-based dynamic margin improves U-Net by 3.52% in DSC. This indicates our method can empower a relatively small network to segment fine-grained brain structures with considerable accuracy. Our method even outperforms nnU-Net by 0.70% in DSC, clearly demonstrating its superiority. We further compare the performance between two types of OHSR. As tabulated in Table 3, Type II ontology achieves bet-ter performance, indicating mimicking clinicians to understand brain hierarchy is of greater help.Qualitative Results. Qualitative comparisons are demonstrated in Fig. 4.From the axial view, we can clearly see the external capsule is well-segmented by our proposed method, while other methods can hardly differentiate its boundary. From the coronal and sagittal views, we observe that our method can better capture and preserve the overall shape of the lateral frontal-orbital gyrus."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,4.0,Conclusion,"In this paper, we propose a novel approach to learn brain hierarchy based on ontology for whole brain segmentation. By introducing graph-based dynamic violate margin into the triplet loss, we encode multi-level ontology knowledge into a voxel-wise embedding space and mimic experts' hierarchical perception of the brain anatomy. We successfully demonstrate that our proposed method outperforms SOTA methods both quantitatively and qualitatively. We consider introducing hierarchical information into the output space as part of our future efforts."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) revealing tumor hemodynamics information is often applied to early diagnosis and treatment of breast cancer [1]. In particular, automatically and accurately segmenting tumor regions in DCE-MRI is vital for computer-aided diagnosis (CAD) and various clinical tasks such as surgical planning. For the sake of promoting segmentation performance, recent methods utilize the dynamic MR sequence and exploit its temporal correlations to acquire powerful representations [2][3][4]. More recently, a handful of approaches take advantage of hemodynamic knowledge and time intensity curve (TIC) to improve segmentation accuracy [5,6]. However, the aforementioned methods require the complete DCE-MRI sequences and overlook the difficulty in assessing complete temporal sequences and the missing time point problem, especially post-contrast phase, due to the privacy protection and patient conditions. Hence, these breast cancer segmentation models cannot be deployed directly in clinical practice.Recently, denoising diffusion probabilistic model (DDPM) [7,8] has produced a tremendous impact on image generation field due to its impressive performance. Diffusion model is composed of a forward diffusion process that add noise to images, along with a reverse generation process that generates realistic images from the noisy input [8]. Based on this, several methods investigate the potential of DDPM for natural image segmentation [9] and medical image segmentation [10][11][12]. Specifically, Baranchuk et al. [9] explores the intermediate activations from the networks that perform the markov step of the reverse diffusion process and find these activations can capture semantic information for segmentation. However, the applicability of DDPM to medical image segmentation are still limited. In addition, existing DDPM-based segmentation networks are generic and are not optimized for specific applications. In particular, a core question for DCE-MRI segmentation is how to optimally exploit hemodynamic priors.Based on the above observations, we innovatively consider the underlying relation between hemodynamic response function (HRF) and denoising diffusion process (DDP). As shown in Fig. 1, during HRF process, only tumor lesions are enhanced and other non-tumor regions remain unchanged. By designing a network architecture to effectively transmute pre-contrast images into post-contrast images, the network should acquire hemodynamic inherent in HRF that can be used to improve segmentation performance. Inspired by the fact that DDPM generates images from noisy input provided by the parameterized Gaussian process, this work aims to exploit implicit hemodynamic information by a diffusion process that predict post-contrast images from noisy pre-contrast images. Specifically, given the pre-contrast and post-contrast images, the latent kinetic code is learned using a score function of DDPM, which contains sufficient hemodynamic characteristics to facilitate segmentation performance.Once the diffusion module is pretrained, the latent kinetic code can be easily generated with only pre-contrast images, which is fed into a segmentation module to annotate cancers. To verify the effectiveness of the latent kinetic code, the SM adopts a simple U-Net-like structure, with an encoder to simultaneously conduct semantic feature encoding and kinetic code fusion, along with a decoder to obtain voxel-level classification. In this manner, our latent kinetic code can be interpreted to provide TIC information and hemodynamic characteristics for accurate cancer segmentation.We verify the effectiveness of our proposed diffusion kinetic model (DKM) on DCE-MRI-based breast cancer segmentation using Breast-MRI-NACT-Pilot dataset [13]. Compared to the existing state-of-the-art approaches with complete sequences, our method yields higher segmentation performance even with precontrast images. In summary, the main contributions of this work are listed as follows:• We propose a diffusion kinetic model that implicitly exploits hemodynamic priors in DCE-MRI and effectively generates high-quality segmentation maps only requiring pre-contrast images. • We first consider the underlying relation between hemodynamic response function and denoising diffusion process and provide a DDPM-based solution to capture a latent kinetic code for hemodynamic knowledge. • Compared to the existing approaches with complete sequences, the proposed method yields higher cancer segmentation performance even with pre-contrast images."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.0,Methodology,"The overall framework of the proposed diffusion kinetic model is illustrated in Fig. 2. It can be observed that the devised model consists of a diffusion module (DM) and a segmentation module (SM). Let {x K , K = 0, 1, ..., k} be a sequence of images representing the DCE-MRI protocol, in which x 0 represents the precontrast image and x k represents the late post-contrast image. The DM takes a noisy pre-contrast image x t as input and generates post-contrast images to estimate the latent kinetic code. Once the DM is trained, the learned kinetic code is incorporated into the SM as hemodynamic priors to guide the segmentation process. Model details are shown as follows."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.1,Diffusion Module,"The diffusion module is following the denoising diffusion probabilistic model [8,14]. Based on the consideration from nonequilibrium thermodynamics, DDPM approximates the data distribution by learning a Markov chain process which originates from the Gaussian distribution. The forward diffusion process gradually adds Gaussian noise to the data x 0 according to a variance schedule β 1 , ..., β T [8]:Particularly, a noisy image x t can be directly obtained from the data x 0 :where α t := 1β t and ᾱt := t s=1 α s . Afterwards, DDPM approximates the reverse diffusion process by the following parameterized Gaussian transitions:where μ θ (x t , t) is the learned posterior mean and θ (x t ; t) is a fixed set of scalar covariances. In particular, we employ a noise predictor network ( θ (x t , t)) to predict the noise component at the step t (As shown in Fig. 2(a)).Inspired by the property of DDPM [8], we devise the diffusion module by considering the pre-contrast images x 0 as source and regarding the post-contrast images x k as target. Formally, a noisy sample can be acquired by:where α t := 1-β t and ᾱt := t s=1 α s . Next, we employ the reverse diffusion process to transform the noisy sample x t to the post-contrast data x k . As thus, the DM gradually exploits the latent kinetic code by comparing the pre-contrast and post-contrast images, which contains hemodynamic knowledge for segmentation."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.2,Segmentation Module,"Once pretrained, the DM outputs multi-scale latent kinetic code f dm from intermediate layers, which is fed into the SM to guide cancer segmentation. As shown in Fig. 2(b), the SM consists of four kinetic blocks and four up blocks. Each kinetic block is composed of a fusion layer, two convolutional layers, two batch normalization layers, two ReLU activation functions, a max pooling layer and a residual addition. Specifically, to obtain sufficient expressive power to transform the learned kinetic code into higher-level features, at least one learnable linear transformation is required. To this end, a linear transformation, parametrized by a weight matrix W , is applied to the latent code f dm , followed by a batch normalization, ReLU activation layer and concatenation, which can be represented as follows:where * represents 1 × 1 based convolution operation, W is the weight matrix, BN represents batch normalization, φ represents ReLU activation function and C is concatenation operation. In this way, the hemodynamic knowledge can be incorporated into the SM to capture more expressive representations to improve segmentation performance."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.3,Model Training,"To maintain training stability, the proposed DKM adopts a two-step training procedure for cancer annotation. In the first step, the DM is trained to transform pre-contrast images into post-contrast images for a latent space where hemodynamic priors are exploited. In particular, the diffusion loss for the reverse diffusion process can be formulated as follows:where θ represents the denoising model that employs an U-Net structure, x 0 and x k are the pre-contrast and post-contrast images, respectively, is Gaussian distribution data ∼ N (0, I), and t is a timestep. For a second step, we train the SM that integrates the previously learned latent kinetic code to provide tumor hemodynamic information for voxel-level prediction. Considering the varying sizes, shapes and appearances of tumors that results from intratumor heterogeneity and results in difficulties of accurate cancer annotation, we design the segmentation loss as follows:where L SSIM is used to evaluate tumor structural characteristics, S and G represents segmentation map and ground truth, respectively; μ S is the mean of S and μ G is the mean of G; ϕ S represents the variance of S and ϕ G represents the variance of G; C 1 and C 2 denote the constant to hold training stable [15], and ϕ SG is the covariance between S and G. The λ is set as 0.5 empirically. Following [16],where k 1 is set as 0.01, k 2 is set as 0.03 and L is set as the range of voxel values."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"Dataset: To demonstrate the effectiveness of our proposed DKM, we evaluate our method on 4D DCE-MRI breast cancer segmentation using the Breast-MRI-NACT-Pilot dataset [13], which contains a total of 64 patients with the contrastenhanced MRI protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time points (As shown in Fig. 3). Each MR volume consists of 60 slices and the size of each slice is 256 × 256. Regarding preprocessing, we conduct zeromean unit-variance intensity normalization for the whole volume. We divided the original dataset into training (70%) and test set (30%) based on the scans. Ground truth segmentations of the data are provided in the dataset for tumor annotation. No data augmentation techniques are used to ensure fairness."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Competing Methods and Evaluation Metrics:,"To comprehensively evaluate the proposed method, We compare it with 3D segmentation methods, including Dual Attention Net (DANet) [17], MultiResUNet [18] and multi-task learning network (MTLN) [19], and 4D segmentation methods, including LNet [20], 3D patch U-Net [21], and HybridNet [5]. All approaches are evaluated using 1) Dice Similarity Coefficient (DSC) and 2) Jaccard Index (JI).Implementation Details: We implement our proposed framework with PyTorch using two NVIDIA RTX 2080Ti GPUs to accelerate model training.Following DDPM [8], we set 128, 256, 256, 256 channels for each stage in the DM and set the noise level from 10 -4 to 10 -2 using a linear schedule with T = 1000.Once the DM is trained, we extract intermediate feature maps from four resolutions for further segmentation task. Similar to DM, the SM also consists of four resolution blocks. However, unlike channel settings of DM, we set 128, 256,  512, 1024 channels for each stage in the SM to capture expressive and sufficient semantic information. The SM is optimized by Adam with a learning rate 2 × 10 -5 and a weight decay 10 -6 . The model is trained for 500 epochs with the batch size to 1. No data augmentation techniques are used to ensure fairness.Comparison with SOTA Methods: The quantitative comparison of the proposed method to recent state-of-the-art methdos is reported in Table 1. Experimental results demonstrate that the proposed method comprehensively other models with less scans (i.e., pre-contrast) in testing. We attribute it to the ability of diffusion module to exploit hemodynamic priors to guide the segmentation task. Specifically, in comparison with 3D segmentation models (e.g. MTLN), our method yields higher segmentation scores. The possible reason is that our method is able to exploit the time intensity curve, which contains richer information compared to post-contrast scan. Besides, we can observe that our method achieves improvements when compared to 4D segmentation models using complete sequence. Our method outperform the HybridNet by 7.1% and 7.0% in DSC and JI, respectively. It probably due to two aspects: 1) The hemodynamic knowledge is implicitly exploited by diffusion module from pre-contrast images, which is useful for cancer segmentation.2) The intermediate activations from  diffusion models effectively capture the semantic information and are excellent pixel-level representations for the segmentation problem [9]. Thus, combining the intermediate features can further promote the segmentation performance. In a word, the proposed framework can produce accurate prediction masks only requiring pre-contrast images. This is useful when post-contrast data is limited.Ablation Study: To explore the effectiveness of the latent kinetic code, we first conduct ablation studies to select the optimal setting. We denote the intermediate features extracted from each stage in the DM as f 1 , f 2 , f 3 , and f 4 , respectively, where f i represents the feature map of i-th stage. Table 2 reports the segmentation performance with different incorporations of intermediate kinetic codes. It can be observed that the latent kinetic code is able to guide the network training for better segmentation results. Specifically, we note that the incorporation of f 3 and f 4 achieves the highest scores among these combinations, and outperforms the integration of all features by 2.0% and 2.6% in DSC and JI, respectively. We attribute it to the denoising diffusion model that receives the noisy input, leading to the noise of shallow features. In contrast, the deep features capture essential characteristics to reveal the structural information and hemodynamic changes of tumors. Figure 4 shows visual comparison of segmentation performance. The above results reveal that incorporation of kinetic code comfortably outperform the baseline without hemodynamic information."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,4.0,Conclusion,We propose a diffusion kinetic model by exploiting hemodynamic priors in DCE-MRI to effectively generate high-quality segmentation results only requiring precontrast images. Our models learns the hemodynamic response function based on the denoising diffusion process and estimates the latent kinetic code to guide the segmentation task. Experiments demonstrate that our proposed framework has the potential to be a promising tool in clinical applications to annotate cancers.
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Table 2 .,f1 f2 f3 f4 Dice (%) ↑ JI (%) ↑ 67.9 ± 2.4 54.4 ± 2.4 68.6 ± 2.3 55.0 ± 2.2 68.3 ± 2.4 54.8 ± 2.5 69.3 ± 2.0 55.5 ± 2.1
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,1.0,Introduction,"Mitochondria segmentation from electron microscopy (EM) images is pivotal to mitochondria morphological analysis [15]. With pixel-wise annotations, existing supervised mitochondria segmentation methods [3,9,10,14] have achieved extraordinary advances in the test data, when the training data and test data come from the same distribution. However, the data distribution of EM images varies in real scenarios due to the diversity in imaging devices, collected organisms and tissues. The different distributions between training and test data, i.e., domain shift [13], lead to drastic performance drops on the test data. Manual annotations and model finetuning can ameliorate this problem but at a huge cost. Instead, unsupervised domain adaptive (UDA) mitochondria segmentation methods [2,4,7,11,16,17,21], aiming to transfer the knowledge learned in the labeled dataset (source domain) to unlabeled data (target domain) without any annotations, have gained great popularity in the community.The mainstream of previous works focus on aligning the distributions of the source and target domains with supervision directly on the output segmentation maps. One line of these works use the model pretrained on source domain to obtain pseudo labels for target domain [1,21], the performance of which highly relies on the quality of the pseudo labels. Other methods are mainly based on GAN [7,16], where an additional domain adversarial learning task, designed to reduce the domain gap between source and target domains, is jointly optimized with segmentation task. The feature space, having higher dimension than the predictions, can express more adequate class-aware knowledge. However, these methods perform domain alignment on the output space, which has insufficient information compared with the feature space, hindering effective alignment.In this work, to take full advantage of the sufficient class related information in the feature space, we propose a class-aware domain alignment in the feature space for mitochondria segmentation, which relies on the prototype representation [8,22] to achieve fine-grained feature alignment. Specifically, 1) we first extract the source feature centroid of each class as prototype. To make the prototypes represent source class knowledge better, we minimize the distance between prototype and its within-class source features, as well as push different prototypes away from each other. Also, we select partial target features close enough to source prototypes and minimize their distance to align domains at class level. 2) To further supervise all target features, we derive the closest prototype as predicted result for each target feature vector based on its distance to prototypes, resulting in a segmentation result directly from the feature space without the segmentation head. A pseudo label is utilized to supervise these predictions for further cross-domain alignment with class knowledge. 3) Though cross-domain alignment can introduce knowledge from source domain to the target domain, there still exists additional potential information useful to segmentation in the target domain [18]. Taking this into consideration, we further propose an intra-domain consistency constraint for target samples, where two input images perturbed differently from the same image are enforced to generate the same features and predictions.Our contributions can be summarized as follows: 1) We propose a class-aware feature alignment method for domain adaptive mitochondria segmentation. To our best knowledge, it is the first attempt to align source and target domains on the feature level in UDA for EM mitochondria segmentation. 2) Our class-aware feature alignment relies on the source prototypes, which represent class knowledge from the feature space. With these prototypes, an innovative distance-based alignment and pseudo-labeling are incorporated to achieve class-aware feature alignment. 3) We propose an intra-domain consistency constraint in the target domain to tap into the potential target domain information. 4) We conduct thorough experiments on various EM dataset benchmarks and our proposed method achieves state-of-the-art performance for mitochondria segmentation."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,2.0,Class-Aware Feature Alignment,"Problem Formulation. Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain. In our work, we denote the source domain aswith M samples, where y s i is the groundtruth binary segmentation map of the input image x s i . The unlabeled target domain is denoted aswith N samples. The overall framework of our proposed method is shown in Fig. 1."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Prototype Extraction.,"Considering there exists more plentiful class-aware information in the feature space than the predictions, we propose the class-aware alignment for better adaptation in the feature space. To achieve class-aware alignment, we first derive the class-aware source prototypes from the source features with the corresponding labels. The prototypes can be calculated as the centroid of each class in the feature space:where f s b,h,w ∈ R is the source feature vectors, B s is the batch size, and H s , W s is the height and width of the features. c is the index of class number C. The maximum of C is 1. The prototypes can represent the class knowledge in the source domain."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Inter-and Intra-class Constraints.,"To make the source prototypes represent the class-discriminative source knowledge more accurately, we incorporate inter-and intra-class constraints on the prototypes, which can further help better class-aware alignment across domains. The inter-class loss L s inter intends to push the prototypes of different classes far away from each other, which can be implemented by minimizing the average cosine distance of different prototype pairs:In contrast, the intra-class loss L s intra is designed to pull the feature instance point closer to its corresponding prototype, i.e., making the feature distribution of the same class more concentrated/compact. The intra-class loss can be formulated as maximizing the average cosine distance between the prototype and the features belonging to the same class:It is not straightforward to align target domain to source domain in the class level, considering the lack of groundtruth labels in target domain. To achieve more reliable class-aware alignment for target samples, we only perform alignment on the instances with higher confidence. Specifically, we first calculate the cosine distance between each target feature and all the source prototypes, and only select instances { f t } the distance of which is closer than a preset threshold τ . The intra-class alignment loss enforces f t c to be closer to its corresponding prototype p t c :The class-aware alignment loss L align is the combination of these three losses, i.e., L align = L s intra + L s inter + L t intra . It is noteworthy that the alignment loss is optimized directly on the feature space instead of the final output predictions, considering there is more abundant information in the feature space.Pseudo Supervision. The above mentioned alignment loss L align only affects partial target features with higher confidence. To further force the alignment across domains in the feature space, we incorporate a pseudo supervision on the feature space. Specifically, based on the cosine distance between the feature of each location and the source prototypes, we can attain a distance map P g , which can be regarded as a segmentation prediction directly from feature space instead of the prediction head. We utilize a pseudo label map P t2 as groundtruth to supervise the learning of P g , leading to alignment directly on feature space. The formulation of P t2 will be discussed in the later section. The supervision is the standard cross entropy loss:Intra-domain Consistency. The alignment cross domains will borrow the knowledge from source domain to target domain. However, there exists abundant knowledge and information in the target domain itself [18]. To further exploit the sufficient knowledge existed in target domain, we propose an intra-domain consistency constraint in target domain. Specifically, for each target input image I t , we first augment it with two different random augmentation strategies, resulting in I t1 and I t2 , which are then fed into the network for segmentation prediction.We incorporate two consistency losses on the feature level L cf and the final prediction level L cp , respectively:where MSE denotes the standard mean squared error loss.Training and Inference. During the training phase, the total training objective L total is formulated as :where L s seg denotes the supervised segmentation loss with the cross-entropy loss and λ {align,p,cf,cp} are the hyperparameters for balancing different terms. Note Table 1. Quantitative comparisons on the Lucchi and MitoEM datasets. Oracle denotes the model is trained on target with groundtruth labels, while NoAdapt represents the model pretrained on source is directly applied in target for inference without any adaptation strategy. The results of Oracle, NoAdapt, UALR, DAMT-Net, DA-VSN and DA-ISC are adopted from [7]."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Methods,"VNC III → Lucchi (Subset1) VNC III → Lucchi (Subset2) mAP(%) F1(%) MCC(%) IoU(%) mAP(%) F1(%) MCC(%) IoU(%) that the feature extractor and the segmentation head are shared weights in the training phase. Their detailed structures can be found in the supplementary material. During the inference phase, we only adopt the trained feature extractor and segmentation head to predict the target images."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,3.0,Experiments,"Datasets. Following the previous work [7], our experiments involve four challenging EM datasets for domain adaptive mitochondria segmentation tasks, i.e., VNC III [5]→ Lucchi (Subset1) [12], VNC III→ Lucchi (Subset2) [12], MitoEM-H [20] → MitoEM-R [20] and MitoEM-R → MitoEM-H. VNC III [5] is imaged from the Drosophila ventral nerve cord by ssTEM. The physical resolution of the pixel is 50 × 5× 5 nm 3 . The dataset consists of 20 images, and their resolution is 1024 × 1024. Lucchi [12] is imaged from the hippocampus of mice collected by FIB-SEM. The physical resolution of the pixel is 5×5×5 nm 3 , the training (Sub-set1) and test (Subset2) sets both have 165 images with 1024 × 768 resolution. MitoEM [20] contains two image volumes imaged by mbSEM, one is from the  Evaluation Metrics. To thoroughly evaluate the performance of models, we conduct comparisons both on semantic-level and instance-level predictions. 1) Following [7], we compare different methods with mAP, F1, MCC and IoU scores on the 2D binary segmentation. 2) Considering that the quantity, size and morphology of mitochondria are pivotal to related studies, we further evaluate on the 3D instance segmentation task. Following [20], we take AP 50 and AP 75 as the metrics to quantitatively compare the performance of different methods.Implementation Details. Our network architecture is following [7]. We crop each image into 512 × 512 as patch to input feature extractor. All models are trained using the Adam optimizer with β 1 = 0.9 and β 2 = 0.999. The learning rate is set at 1e -4 and is polynomially decayed with a power of 0.9. We train models for 200k iterations in total. The balancing weights λ align , λ proto , λ cf , and λ cp in Eq. 7 are set as 0.1, 0.1, 0.1, and 0.1, respectively. The preset threshold τ is set as 0.7. To obtain 3D instance segmentation results, we adopt the markercontrolled watershed algorithm [20] on the predicted binary predictions.Comparisons with Baselines. The binary segmentation result comparisons of our proposed method with previous works on the Lucchi and MitoEM datasets are shown in Table 1. The competitors include UALR [21], DAMT-Net [16], Advent [19], DA-VSN [6], and DA-ISC [7]. Our method achieves the new stateof-the-art results in all cases, which corroborates the superiority of the proposed class-aware alignment in the feature space. Especially, compared with the previous state-of-the-art DA-ISC [7], our method surpasses it by a large margin on the benchmarks of VNC III→ Lucchi (Subset1) (3.1% IoU). The mitochondria in MitoEM-H distribute more densely and are more complex than those in MitoEM-R, leading to the result of MitoEM-R → MitoEM-H is lower than that of MitoEM-H → MitoEM-R. However, our result on MitoEM-R → MitoEM-H has remarkable improvement, owing to that our method not only aligns domain in a fine-grained way but also explore the full potential of target. As shown in Table 2, we also evaluate the effectiveness of our proposed method on 3D instance segmentation results. We only conduct experiments on the MitoEM dataset due to the lack of groundtruth for 3D instance segmentation in Lucchi. Our method not only deals with the domain adaptation for binary segmentation but also behaves well for the harder 3D instance segmentation, where the latter has rarely been studied in the literature. Furthermore, to further evaluate the effectiveness of our method, we visualize the predicted segmentation of our method and baselines in Fig. 2. Credited to the proposed class-aware feature alignment, our method estimates more fine-grained results on the target domain, substantiating that our method can alleviate the domain shift between source and target domains. In Fig. 2, the yellow and orange boxes represent mitochondria and background, respectively. In R2H, only our method segments the mitochondria in the yellow box. This is because our method is able to extract more representative features of mitochondria and background, and can separate these two types of features more effectively.Ablation Study for Loss Functions. We conduct thorough ablation experiments to validate the contribution of each loss term in Eq. 7, where the results are shown in Table 3. The experiment ① with only supervised segmentation loss L s seg is the Source-Only method. All other variants with our proposed losses have superior performance than ①. Specifically, with the proposed class-aware feature alignment (L align and L p ), ② improves ① by 6.6% IoU and ③ further enlarges the gain to 7.8%. To take advantage of the potential information in target domain with the intra-domain losses L cf and L cp , the final ⑤ improves by 3.6% IoU, leading to the 11.4% IoU improvement in total. To further explore the impact of different components in L align , i.e., L s intra , L s inter and L t intra , we conduct experiments and the results are shown in the supplementary material. We find that the cross-domain alignment term L t intra plays the core role. It is noteworthy that L s intra and L s inter help to learn better class-aware prototypes."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,4.0,Conclusion,"In this paper, for the first time, we propose the class-aware alignment for domain adaptation on mitochondria segmentation in the feature space. Based on the extracted source prototypes representing class knowledge, we design intradomain and inter-domain alignment constraint for fine-grained alignment cross domains. Furthermore, we incorporate an intra-domain consistency loss to take full advantage of the potential information existed in target domain. Comprehensive experiments demonstrate the effectiveness of our proposed method."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 23.
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,1.0,Introduction,"Accurate gland segmentation from whole slide images (WSIs) plays a crucial role in the diagnosis and prognosis of cancer, as the morphological features of glands can provide valuable information regarding tumor aggressiveness [11]. With the Prior USS methods in medical image research [2] and natural image research [6] vs. Our MSSG. Green and orange regions denote the glands and the background respectively. (Color figure online) emergence of deep learning (DL), there has been a growing interest in developing DL-based methods for semantic-level [9,12,36] and instance-level [5,13,25,27,32,35] gland segmentation. However, such methods typically rely on large-scale annotated image datasets, which usually require significant effort and expertise from pathologists and can be prohibitively expensive [28].To reduce the annotation cost, developing annotation-efficient methods for semantic-level gland segmentation has attracted much attention [10,18,23,37]. Recently, some researchers have explored weakly supervised semantic segmentation methods which use weak annotations (e.g., Bound Box [37] and Patch Tag [18]) instead of pixel-level annotations to train a gland segmentation network. However, these weak annotations are still laborious and require expert knowledge [37]. To address this issue, previous works have exploited conventional clustering [8,22,23] and metric learning [10,29] to design annotation-free methods for gland segmentation. However, the performance of these methods can vary widely, especially in cases of malignancy. This paper focuses on unsupervised gland segmentation, where no annotations are required during training and inference.One potential solution is to adopt unsupervised semantic segmentation (USS) methods which have been successfully applied to medical image research and natural image research. On the one hand, existing USS methods have shown promising results in various medical modalities, e.g., magnetic resonance images [19],x-ray images [1,15] and dermoscopic images [2]. However, directly utilizing these methods to segment glands could lead to over-segment results where a gland is segmented into many fractions rather than being considered as one target (see Fig. 1(b)). This is because these methods are usually designed to be extremely sensitive to color [2], while gland images present a unique challenge due to their highly dense and complex tissues with intricate color distribution [18]. On the other hand, prior USS methods for natural images can be broadly categorized into coarse-to-fine-grained [4,14,16,21,31] and end-to-end (E2E) cluster-ing [3,6,17]. The former ones typically rely on pre-generated coarse masks (e.g., super-pixel proposals [16], salience masks [31], and self-attention maps [4,14,21]) as prior, which is not always feasible on gland images. The E2E clustering methods, however, produce under-segment results on gland images by confusing many gland regions with the background; see Fig. 1(b). This is due to the fact that E2E clustering relies on the inherent connections between pixels of the same class [33], and essentially, grouping similar pixels and separate dissimilar ones. Nevertheless, the glands are composed of different parts (gland border and interior epithelial tissues, see Fig. 1(a)) with significant variations in appearance. Gland borders typically consist of dark-colored cells, whereas the interior epithelial tissues contain cells with various color distributions that may closely resemble those non-glandular tissues in the background. As such, the E2E clustering methods tend to blindly cluster pixels with similar properties and confuse many gland regions with the background, leading to under-segment results.To tackle the above challenges, our solution is to incorporate an empirical cue about gland morphology as additional knowledge to guide gland segmentation. The cue can be described as: Each gland is comprised of a border region with high gray levels that surrounds the interior epithelial tissues. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping, abbreviated as MSSG. To begin, we leverage the empirical cue to selectively mine out proposals for the two gland sub-regions with variant appearances. Then, considering that our segmentation target is the gland, we employ a Morphology-aware Semantic Grouping module to summarize the semantic information about glands by explicitly grouping the semantics of the sub-region proposals. In this way, we not only prioritize and dedicate extra attention to the target gland regions, thus avoiding under-segmentation; but also exploit the valuable morphology information hidden in the empirical cue, and force the segmentation network to recognize entire glands despite the excessive variance among the sub-regions, thus preventing over-segmentation. Ultimately, our method produces well-delineated and complete predictions; see Fig. 1(b).Our contributions are as follows: (1) We identify the major challenge encountered by prior unsupervised semantic segmentation (USS) methods when dealing with gland images, and propose a novel MSSG for unsupervised gland segmentation. (2) We propose to leverage an empirical cue to select gland sub-regions and explicitly group their semantics into a complete gland region, thus avoiding over-segmentation and under-segmentation in the segmentation results. (3) We validate the efficacy of our MSSG on two public glandular datasets (i.e., the GlaS dataset [27] and the CRAG dataset [13]), and the experiment results demonstrate the effectiveness of our MSSG in unsupervised gland segmentation."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2.0,Methodology,"The overall pipeline of MSSG is illustrated in Fig. 2. The proposed method begins with a Selective Proposal Mining (SPM) module which generates a proposal map that highlights the gland sub-regions. The proposal map is then used to train a segmentation network. Meantime, a Morphology-aware Semantic Grouping (MSG) module is used to summarize the overall information about glands from their sub-region proposals. More details follow in the subsequent sections."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2.1,Selective Proposal Mining,"Instead of generating pseudo-labels for the gland region directly from all the pixels of the gland images as previous works typically do, which could lead to over-segmentation and under-segmentation results, we propose using the empirical cue as extra hints to guide the proposal generation process.Specifically, let the i th input image be denoted as X i ∈ R C×H×W , where H, W , and C refer to the height, width, and number of channels respectively. We first obtain a normalized feature map F i for X i from a shallow encoder f with 3 convolutional layers, which can be expressed as F i = f (X i ) 2 . We train the encoder in a self-supervised manner, and the loss function L consists of a typical self-supervised loss L SS , which is the cross-entropy loss between the feature map F i and the one-hot cluster label C i = arg max (F i ), and a spatial continuity loss L SC , which regularizes the vertical and horizontal variance among pixels within a certain area S to assure the continuity and completeness of the gland border regions (see Fig. 1 in the Supplementary Material). The expressions for L SS and L SC are given below:(1)(2)Then we employ K-means [24] to cluster the feature map F i into 5 candidate regions, denoted as Y i = y i,1 ∈ R D×n0 , y i,2 ∈ R D×n2 , ..., y i,5 ∈ R D×n5 , where n 1 + n 2 + ... + n 5 equals the total number of pixels in the input image (H × W ). Sub-region Proposal Selection via the Empirical Cue. The aforementioned empirical cue is used to select proposals for the gland border and interior epithelial tissues from the candidate regions Y i . Particularly, we select the region with the highest average gray level as the proposal for the gland border. Then, we fill the areas surrounded by the gland border proposal and consider them as the proposal for the interior epithelial tissues, while the rest areas of the gland image are regarded as the background (i.e., non-glandular region). Finally, we obtain the proposal map P i ∈ R 3×H×W , which contains the two proposals for two gland sub-regions and one background proposal."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2.2,Morphology-Aware Semantic Grouping,"A direct merge of the two sub-region proposals to train a fully-supervised segmentation network may not be optimal for our case. Firstly, the two gland subregions exhibit significant variation in appearance, which can impede the segmentation network's ability to recognize them as integral parts of the same object. Secondly, the SPM module may produce proposals with inadequate highlighting of many gland regions, particularly the interior epithelial tissues, as shown in Fig. 2 where regions marked with × are omitted. Consequently, applying pixel-level cross-entropy loss between the gland image and the merged proposal map could introduce undesired noise into the segmentation network, thus leading to under-segment predictions with confusion between the glands and the background. As such, we propose two types of Morphology-aware Semantic Grouping (MSG) modules (i.e., MSG for Variation and MSG for Omission) to respectively reduce the confusion caused by the two challenges mentioned above and improve the overall accuracy and comprehensiveness of the segmentation results. The details of the two MSG modules are described as follows.Here, we first slice the gland image and its proposal map into patches as inputs. Let the input patch and its corresponding sliced proposal map be denoted as X ∈ R C× Ĥ× Ŵ and P ∈ R 3× Ĥ× Ŵ . We can obtain the feature embedding map F which is derived as F = f feat ( X) and the prediction map X as X = f cls ( F ), where f feat and f cls refers to the feature extractor and pixel-wise classifier of the segmentation network respectively.MSG for Variation is designed to mitigate the adverse impact of appearance variation between the gland sub-regions. It regulates the pixel-level feature embeddings of the two sub-regions by explicitly reducing the distance between them in the embedding space. Specifically, according to the proposal map P , we divide the pixel embeddings in F ∈ R D× Ĥ× Ŵ into gland border set G = g 0 , g 1 , ..., g kg , interior epithelial tissue set I = {i 0 , i 1 , ..., i ki } and non-glandular (i.e., background) set N = {n 0 , n 1 , ..., n kn }, where k g + k i + k n = Ĥ × Ŵ . Then, we use the average of the pixel embeddings in gland border set G as the alignment anchor and pull all pixels of I towards the anchor:(MSG for Omission is designed to overcome the problem of partial omission in the proposals. It identifies and relabels the overlooked gland regions in the proposal map and groups them back into the gland semantic category. To achieve this, for each pixel n in the non-glandular (i.e., background) set N , two similarities are computed with the gland sub-regions G and I respectively:S G n (or S I n ) represents the similarity between the background pixel n and gland borders (or interior epithelial tissues). If either of them is higher than a preset threshold β (set to 0.7), we consider n as an overlooked pixel of gland borders (or interior epithelial tissues), and relabel n to G (or I). In this way, we could obtain a refined proposal map RP . Finally, we impose a pixel-level cross-entropy loss on the prediction and refined proposal RP to train the segmentation network:The total objective function L for training the segmentation network can be summarized as follows: (6) where λ v (set to 1) is the coefficient."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.1,Datasets,"We evaluate our MSSG on The Gland Segmentation Challenge (GlaS) dataset [27] and The Colorectal Adenocarcinoma Gland (CRAG) dataset [13]. The GlaS dataset contains 165 H&E-stained histopathology patches extracted from 16 WSIs. The CRAG dataset owns 213 H&E-stained histopathology patches extracted from 38 WSIs. The CRAG dataset has more irregular malignant glands, which makes it more difficult than GlaS, and we would like to emphasize that the results on CRAG are from the model trained on GlaS without retraining.  "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.2,Implementation Details,"The experiments are conducted on four RTX 3090 GPUs. For the SPM, a 3layer encoder is trained for each training sample. Each convolutional layer uses a 3 × 3 convolution with a stride of 1 and a padding size of 1. The encoder is trained for 50 iterations using an SGD optimizer with a polynomial decay policy and an initial learning rate of 1e-2. For the MSG, MMSegmentation [7] is used to construct a PSPNet [38] with a ResNet-50 backbone as the segmentation network. The network is trained for 20 epochs with an SGD optimizer, a learning rate of 5e-3, and a batch size of 16. For a fair comparison, the results of all  unsupervised methods in Table 1 are obtained using the same backbone trained with the corresponding pseudo-labels. The code is available at https://github. com/xmed-lab/MSSG."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.3,Comparison with State-of-the-art Methods,"We compare our MSSG with multiple approaches with different supervision settings in Table 1. On the GlaS dataset, the end-to-end clustering methods (denoted by "" * "") end up with limited improvement over a randomly initialized network. Our MSSG, on the contrary, achieves significant advances. Moreover, MSSG surpasses all other unsupervised counterparts, with a huge margin of 10.56% at mIOU, compared with the second-best unsupervised counterpart. On CRAG dataset, even in the absence of any hints, MSSG still outperforms all unsupervised methods and even some of the fully-supervised methods. Additionally, we visualize the segmentation results of MSSG and its counterpart (i.e., SGSCN [2]) in Fig. 3. On both datasets, MSSG obtains more accurate and complete results."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.4,Ablation Study,"Table 2 presents the ablation test results of the two MSG modules. It can be observed that the segmentation performance without the MSG modules is not satisfactory due to the significant sub-region variation and gland omission. With the gradual inclusion of the MSG for Variation and Omission, the mIOU is improved by 6.42% and 2.57%, respectively. Moreover, with both MSG modules incorporated, the performance significantly improves to 62.72% (+14.30%). we also visualize the results with and without MSG modules in Fig. 4. It is apparent that the model without MSG ignores most of the interior epithelial tissues.With the incorporation of MSG for Variation, the latent distance between gland borders and interior epithelial tissues is becoming closer, while both of these two sub-regions are further away from the background. As a result, the model can highlight most of the gland borders and interior epithelial tissues. "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,4.0,Conclusion,"This paper explores a DL method for unsupervised gland segmentation, which aims to address the issues of over/under segmentation commonly observed in previous USS methods. The proposed method, termed MSSG, takes advantage of an empirical cue to select gland sub-region proposals with varying appearances. Then, a Morphology-aware Semantic Grouping is deployed to integrate the gland information by explicitly grouping the semantics of the selected proposals. By doing so, the final network is able to obtain comprehensive knowledge about glands and produce well-delineated and complete predictions. Experimental results prove the superiority of our method qualitatively and quantitatively."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,1.0,Introduction,"Reliable and robust segmentation of medical images plays a significant role in clinical diagnosis [12]. In recent years, deep learning has led to significant progress in image segmentation tasks [6,26]. However, training these models [23] requires large-scale image data with precise pixel-wise annotations, which are usually time-consuming and costly to obtain. To address this challenge, recent studies have explored semi-supervised learning approaches for medical image segmentation, leveraging unlabeled data to enhance performance [1,21]. Semi-supervised learning (SSL) commonly uses two methods: pseudo labeling and consistency regularization. Pseudo labeling uses a model's predictions on unlabeled data to create ""pseudo labels"" and augment the original labeled data set, allowing for improved accuracy and reduced cost in training [2,7,10]. Despite its potential benefits, the pseudo labeling approach may pose a risk to the accuracy of the final model by introducing inaccuracies into the training data. Consistency regularization, on the other hand, aims to encourage the model's predictions to be consistent across different versions of the same input. UA-MT [22] and DTC [11] are examples of consistency regularization-based methods that use teacher-student models for segmentation and emphasize dual consistency between different representations, respectively.Combining both methods can potentially lead to improved performance, as pseudo labeling leverages unlabeled data and consistency regularization improves the model's robustness [3,20]. However, pseudo labeling still inevitably introduces inaccuracies that can hinder the model's performance. To overcome this challenge, we propose a novel consistency-guided meta-learning framework called Meta-Learning for Bootstrapping Medical Image Segmentation (MLB-Seg). Our approach uses pixel-wise weights to adjust the importance of each pixel in the initialized labels and pseudo labels during training. We learn these weights through a meta-process that prioritizes pixels with loss gradient direction closer to those of clean data, using a small set of clean labeled images. To further improve the quality of the pseudo labels, we introduce a consistency-based Pseudo Label Enhancement (PLE) scheme that ensembles predictions from augmented versions of the same input. To address the instability issue arising from using data augmentation, we incorporate a mean-teacher model to stabilize the weight map generation from the student meta-learning model, which leads to improved performance and network robustness. Our proposed approach has been extensively evaluated on two benchmark datasets, LA [4] and PROMISE12 [9], and has demonstrated superior performance compared to existing methods."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.0,Method,"We present MLB-Seg, a novel consistency-guided meta-learning framework for semi-supervised medical image segmentation. Assume that we are given a training dataset consisting of clean data D c = {(x c i , y c i )} N i=1 , and unlabeled data, where the input image x c i , x u k are of size H × W with the corresponding clean ground-truth mask y c i . "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.1,Meta-learning for Bootstrapping,"We first estimate labels for all unlabeled data using the baseline model which is trained on the clean data, denoted as followswhere f c (:; θ c ) denotes the trained model parameterized by θ c and k = 1, 2, ..., K.We further denote them as D n = {(x ñ j , y ñ j )} K j=1 . We then develop a novel meta-learning model for medical image segmentation, which learns from the clean set D c to bootstrap itself up by leveraging the learner's own predictions (i.e., pseudo labels), called Meta-Learning for Bootstrapping (MLB). As shown in Fig. 1, by adaptively adjusting the contribution between the initialized and pseudo labels commensurately in the loss function, our method effectively alleviates the negative effects from the erroneous pixels. Specifically, at training step t, given a training batch fromare the batch size respectively. Our objective is:where y p j is the pseudo label generated by f (x ñ j ; θ), L(•) is the cross-entropy loss function, C is the number of classes (we set C = 2 throughout this paper), w n j , w p j ∈ R H×W are the weight maps used for adjusting the contribution between the initialized and the pseudo labels in two different loss terms. • denotes the Hadamard product. We aim to solve for Eq. 2 following 3 steps:-Step 1: Update θt+1 based on S n and current weight map set. Following [15], during training step t, we calculate θt+1 to approach the optimal θ * (w n , w p ) as follows:where α represents the step size. -Step 2: Generate the meta-learned weight maps w n * , w p * based on S c and θt+1 by minimizing the standard cross-entropy loss in the meta-objective function over the clean training data:Note that here we restrict every element in w n/p to be non-negative to prevent potentially unstable training [15]. Such a meta-learned process yields weight maps which can better balance the contribution of the initialized and the pseudo labels, thus reducing the negative effects brought by the erroneous pixels. Following [15,25], we only apply one-step gradient descent of w n/p j based on a small clean-label data set S c , to reduce the computational expense. To be specific, at training step t, w n/p j is first initialized as 0, then we estimate w n * , w p * as: where β stands for the step size and w n/pr,s j,tindicates the value at r th row, s th column of w n/p j at time t. Equation 7 is used to enforce all weights to be strictly non-negative. Then Eq. 8 is introduced to normalize the weights in a single training batch so that they sum up to one. Here, we add a small number to keep the denominator greater than 0.-Step 3: The meta-learned weight maps are used to spatially modulate the pixel-wise loss to update θ t+1 :"
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.2,Consistency-Based Pseudo Label Enhancement,"To generate more reliable pseudo labels, we propose Pseudo Label Enhancement (PLE) scheme based on consistency, which enforces consistency across augmented versions of the same input. Specifically, we perform Q augmentations on the same input image and enhance the pseudo label by averaging the outputs of the Q augmented versions and the original input:where f (x ñq j ; θ) is the output of q-th augmented sample, f (x ñ0 N +j ; θ) is the output of the original input, and τ -1 q means the corresponding inverse transformation of the q-th augmented sample. Meanwhile, to further increase the output consistency among all the augmented samples and original input, we introduce an additional consistency loss L Aug c to the learning objective:where (r, s) denotes the pixel index. τ q is the corresponding transformation to generate the q-th augmented sample. (q, v) denotes the pairwise combination among all augmented samples and the original input. The final loss is the mean square distance among all (Q+1)Q 2 pairs of combinations."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.3,Mean Teacher for Stabilizing Meta-learned Weights,"Using PLE alone can result in performance degradation with increasing numbers of augmentations due to increased noise in weight maps. This instability can compound during subsequent training iterations. To address this issue during meta-learning, we propose using a mean teacher model [17] with consistency loss (Eq. 13). The teacher network guides the student meta-learning model, stabilizing weight updates and resulting in more reliable weight maps. Combining PLE with the mean teacher model improves output robustness. The student model is used for meta-learning, while the teacher model is used for weight ensemble with Exponential Moving Average (EMA) [17] applied to update it. The consistency loss maximizes the similarity between the teacher and student model outputs, adding reliability to the student model and stabilizing the teacher model. For each input x ñ j in the batch S n , we apply disturbance to the student model input to become the teacher model input. Then a consistency loss L ST c is used to maximize the similarity between the outputs from the teacher model and student model, further increasing the student model's reliability while stabilizing the teacher model. Specifically, for each input x ñ j in the batch S n , then corresponding input of teacher model iswhere N (μ, σ) ∈ R H×W denotes the Gaussian distribution with μ as mean and σ as standard deviation. And γ is used to control the noise level. The consistency loss is implemented based on pixel-wise mean squared error (MSE) loss:3 Experiments"
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.1,Experimental Setup,"Datasets. We evaluate our proposed method on two different datasets including 1) the left atrial (LA) dataset from the 2018 Atrial Segmentation Challenge [4] as well as 2) the Prostate MR Image Segmentation 2012 (PROMISE2012) dataset [9]. Specifically, LA dataset is split into 80 scans for training and 20 scans for evaluation following [22]. From the training set, 8 scans are randomly selected as the meta set. All 2D input images were resized to 144 × 144. For PROMISE2012, we randomly split 40/4/6 cases for training and 10 for evaluation (4 for validation, 6 for test) following [13]. We randomly pick 3 cases from the training set as the meta set and resize all images to 144 × 144. We evaluate our segmentation performances using four metrics: the Dice coefficient, Jaccard Index (JI), Hausdorff Distance (HD), and Average Surface Distance (ASD).Implementation Details. All of our experiments are based on 2D images. We adopt UNet++ as our baseline. Network parameters are optimized by SGD setting the learning rate at 0.005, momentum to be 0.9 and weight decay as 0.0005. The exponential moving average (EMA) decay rate is set as 0.99 following [17].For the label generation process, we first train with all clean labeled data for 30 epochs with batch size set as 16. We then use the latest model to generate labels for unlabeled data. Next, we train our MLB-Seg for 100 epochs."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.2,Results Under Semi-supervision,"To illustrate the effectiveness of MLB-Seg under semi-supervision. We compare our method with the baseline (UNet++ [26]) and previous semi-supervised methods on LA dataset (Table 1) and PROMISE12 (Table 2), including an adversarial learning method [24], consistency based methods [5,11,18,20,22], an uncertaintybased strategy [1], and contrastive learning based methods [13,19]. For a fair comparison in Table 2, we also use UNet [16] as the backbone and resize images to 256 × 256, strictly following the settings in Self-Paced [13]. We split the evaluation set (10 cases) into 4 cases as the validation set, and 6 as the test set.We then select the best checkpoint based on the validation set and report the results on the test set. As shown in Table 1 and 2, our MLB-Seg outperforms recent state-of-the-art methods on both PROMISE12 (under different combinations of backbones and image sizes) and the LA dataset across almost all evaluation measures. "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.3,Ablation Study,"To explore how different components of our MLB-Seg contribute to the final result, we conduct the following experiments under semi-supervision on PROMISE12: 1) the bootstrapping method [14] (using fixed weights without applying meta-learning); 2) MLB, which only reweights the initialized labels and pseudo labels without applying PLE and mean teacher; 3) MLB + mean teacher which combines MLB with mean teacher scheme; 4) MLB + PLE which applies PLE strategy with MLB. When applying multiple data augmentations (i.e., for Q = 2, 4), we find the best performing combinations are 2 × PLE (using one zoom in and one zoom out), 4 × PLE (using one zoom in and two zoom out and one flip for each input); 5) MLB + PLE + mean teacher which combines PLE, mean teacher with MLB simultaneously to help better understand how mean teacher will contribute to PLE. Our results are summarized in Table 3, which shows the effectiveness of our proposed components. The best results are achieved when all components are used.To demonstrate how PLE combined with the mean teacher model help stabilize the meta-weight update, we compare the performance of MLB + PLE (w/mean teacher) with MLB + PLE + mean teacher under different augmentations (Q) on PROMISE12 dataset (See supplementary materials for details). We find out that for MLB + PLE (w/o mean teacher), performance improves from 74.34% to 74.99% when Q is increased from 1 to 2, but decreases significantly when Q ≥ 4. Specifically, when Q reaches 4 and 6, the performance significant drops from 74.99% to 72.07% (Q = 4) and from 74.99% to 70.91% (Q = 6) respectively. We hypothesize that this is due to increased noise from initialized labels in some training samples, which can lead to instability in weight updates. To address this issue, we introduce the mean-teacher model [17] into PLE to stabilize weight map generation from the student meta-learning model. And MLB + PLE + mean teacher turns out to consistently improve the stability of metalearning compared with its counterpart without using mean teacher, further validating the effectiveness of our method (see Supplementary for more examples). Specifically, for MLB + PLE + mean teacher, the performance reaches 76.63% (from 72.07%) when Q = 4, 75.84% (from 70.91%) when Q = 6.Qualitative Analysis. To illustrate the benefits of MLB-Seg for medical image segmentation, we provide a set of qualitative examples in Fig. 2. In the visualization of weight maps of Fig. 2, the blue/purple represents for the initialized label in y ñ/y p , while the red indicates pixels in w p * have higher values. We observe that MLB-Seg places greater emphasis on edge information. It is evident that higher weights are allotted to accurately predicted pseudo-labeled pixels that were initially mislabeled, which effectively alleviates the negative effects from erroneously initialized labels. "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a novel meta-learning based segmentation method for medical image segmentation under semi-supervision. With few expert-level labels as guidance, our model bootstraps itself up by dynamically reweighting the contributions from initialized labels and its own outputs, thereby alleviating the negative effects of the erroneous voxels. In addition, we address an instability issue arising from the use of data augmentation by introducing a mean teacher model to stabilize the weights. Extensive experiments demonstrate the effectiveness and robustness of our method under semi-supervision. Notably, our approach achieves state-of-the-art results on both the LA and PROMISE12 benchmarks."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,1.0,Introduction,"Robust and accurate elongated physiological structure segmentation is crucial for computer-aided diagnosis and quantification of clinical parameters [26,27]. Manual delineation is tedious and laborious. Recently, deep learning-based methods [14,16,19] have been proposed to delineate targets automatically. However, they are not able to outline correctly in ambiguous regions where exist uneven illumination, artifacts, or interference from the disease.Many researchers have tried to use uncertainty information to concentrate on the ambiguous region, and to evaluate the reliability of model's prediction. According to the source of prediction errors [3], uncertainty is categorized into two types: epistemic and aleatoric. The main methods for uncertainty estimation are as follows. Bayesian neural networks [15] place a probability distribution over model weights, but are hard to optimize. Monte Carlo dropout [10] approximates the Gaussian process by embedding the dropout operation into the neural network layers and calculating the variance of several times inference. Deep Ensembles [8] combine the outputs from a group of independent models to estimate uncertainty. Softmax uncertainty [13,17,18] performs well in distinguishing examples that are easy or fallible to classify. Once the uncertainty information has been estimated, we are able to pay more attention to the ambiguous region. Xie et al. [24] used the cross-attention module to extract influential features for ambiguous regions based on pixel-level uncertainty. Yang et al. [25] achieved uncertainty awareness by training with a multi-confidence mask, and further used self-attention block with feature aware filter together to highlight uncertain areas. Wang et al. [23] annotated alpha matte for medical images and used it as a soft label to intuitively promote the network to focus on uncertain areas. Kohl et al. [7] proposed a generative model to produce multiple reasonable hypotheses for clinical experts to select from, which improved the diagnosis reliability. However, existing works applied the 'hard' attention to utilize uncertainty, which lacks the ability of adaptive adjustment and ignores neighboring uncertain regions. In addition, features at different scales contain rich structural and semantic contexts, which are essential for elongated physiological structure segmentation, such as cobweb corneal endothelial cells and retinal vessels.This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation, which fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we use a gated soft uncertainty-aware (GSUA) module to adaptively highlight ambiguous areas based on spatial uncertainty maps. Second, we extract the uncertainty under different scales and propose the multi-scale uncertainty-aware (MSUA) fusion module to integrate hierarchical predictions for enhancing the final segmentation. Experiment results on segmentation tasks of the cornea endothelium and retinal vessel show the effectiveness of SSU-Net.   . The Bayesian approximate network has two outputs: segmentation prediction ŷ and the estimation of aleatoric uncertainty v. We can calculate the epistemic and aleatoric uncertainty maps, u e and u a , after multiple inferences. Furthermore, we consider the sigmoid probabilities of predictions under different scales as the second uncertainty source, and fuse the predictions {ŷ 1 , ŷ2 , ŷ3 } from multiple scales using the multi-scale uncertainty-aware (MSUA) module. ŷF is the final target output."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.1,Spatial Uncertainty and Gated Soft Uncertainty-Aware Module,"Spatial Uncertainty. Since the epistemic and aleatoric uncertainty maps are used to find the hard-to-classify spatial areas in this work, we regard them as spatial uncertainty. Referring to [6], we add dropout after each UNet block to approximate the Bayesian network, which learns the segmentation ỹ and aleatoric uncertainty v simultaneously. During inference, we sample a group of predictions {ỹ i } N i=1 and {v i } N i=1 by N stochastic forward pass. In this work, we set N = 16. The epistemic u e and aleatoric u a uncertainty are formulated by Eq. ( 1), where y is the ground truth.Gated Soft Uncertainty-Aware Module. To endow the uncertainty-aware module with adaptive adjustment ability, we propose the gated soft uncertaintyaware (GSUA) module, as illustrated in Fig. 1. We extract salient descriptions from uncertainty maps by two parallel pooling [ψ avg , ψ max ] and a 1 × 1 convolution f (•) operation. The relu operation is set as a switch to filter out areas with small uncertainty values, further strengthening our attention on areas with high uncertainty. Since it is also usually difficult to classify the area adjacent to the high-uncertainty regions, we use the Gaussian kernel to soften the boundary in such regions. The GSUA module is formulated by:where x i , x o ∈ R N ×c×h×w are the input and output features respectively; u = [u a , u e ] ∈ R N ×2×H×W is a tensor of uncertainty maps; ψ avg and ψ max represent average and max pooling; σ is the sigmoid function; g s denotes a convolution operation with Gaussian kernel and resizes the attention maps to the size of input features; is element-wise multiplication."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.2,Scale Uncertainty and Multi-scale Uncertainty-Aware Module,"Scale Uncertainty. To integrate the predictions from hierarchical layers during model training, we capture the uncertainty under multiple scales. The sigmoid function is a simple and effective way to estimate uncertainty for the binary classification task. We extract the multi-scale uncertainty by Equation (3), where u s is the uncertainty map of prediction ỹs under scale s ∈ {1, 2, 3}.Multi-scale Uncertainty-Aware Module. With the uncertainty maps from different scales, all the hierarchical predictions {ỹ 1 , ỹ2 , ỹ3 } are fused by the MSUA module to generate the enhanced prediction ỹF , as illustrate in Fig. 1.The uncertainty map u s provides the classification confidence for each pixel. Therefore, we use u s to highlight the confident region of ỹs and further extract the max value across the different scales. The process is formulated by:where ỹF (i, j) denotes the pixel value at location (i, j) of enhanced prediction; y s (i, j) is the value of prediction under scale s ∈ {1, 2, 3}; and σ denotes the sigmoid operation of Eq. (3)."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.3,Objective Function,"As shown in Fig. 1, we optimize the model with supervision on four segmentation branches simultaneously, including supervision for predicting three scales and the final enhanced output. The loss function is summarized as follows:where α 1 , α 2 , α 3 , and α F are the weight parameters for sub loss L s1 , L s2 , L s3 , and L F . In this experiment, we set all the weight parameters as 1. For these sub-losses, we adopt binary cross-entropy loss, as shown in Eq. ( 6).where y is the ground truth; the positive class value of each pixel is 1; the negative class value is 0; and ỹ ∈ (0, 1) is the predicted probability value.3 Experiment"
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.1,Dataset,"Two cornea endothelium microscope image datasets, TM-EM3000 and Rodrep, and one retinal fundus image dataset, FIVES, are used in this work. The private dataset TM-EM3000 contains 183 images measured by EM3000 specular microscope (Tomey Corporation, Japan). Following Ruggeri et al. [20], we cropped a 192 × 192 pixels sub-region from its 260 × 480 pixels whole image. We used 155 images for model training, ten images for validation, and 18 images for testing. Rodrep [21] contains 52 in-vivo confocal corneal microscope images, from 23 Fuchs patients with endothelial corneal dystrophy. We used 40 for training, five images for validation, and seven for testing. FIVES [5] is the largest known high-resolution fundus image dataset: It covers normal eyes and three different eye diseases with a balanced distribution. There are 800 high-resolution images and the corresponding manual annotations, with 550 for training, 50 for validation, and 200 for testing."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.2,Evaluation Metrics and Implementation Details,"There is a class imbalance between foreground and background pixels. To better evaluate the segmentation performance, we choose Dice score [22], mIoU , and mAcc as evaluation metrics. We optimized the models using the RMSprop strategy with momentum = 0.9 and weight decay = 1e-8 for 100 epochs. The initial learning rate was 2e-4, and the input size of all networks was uniformly set to 256 × 256. Random shift and rescaling within a range of [-0.3, +0.3] were used for data augmentation. We set the batch size to 1 based on our empirical observations. For uncertainty-based models, we set the dropout rate as 0.5 and no data augmentation. During testing, we inferred N = 16 times and obtained the finalỹi F and the epistemic uncertainty"
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.3,Ablation Study,"We investigated the influence of GSUA and MSUA modules on TM-EM3000, as shown in Table 1. The MSUA increased performance by 0.69% on the Dice score, and GSUA increased by 0.19%. The MSUA module brought more improvement than GSUA, which indicated that multi-scale context is crucial for cornea endothelium cell segmentation. When using both GSUA and MSUA modules simultaneously, we achieved the best performance. "
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.4,Comparison with State-of-the-Art Methods,"To study the effectiveness of the proposed SSU-Net, we compared it with a series of state-of-the-art methods. On TM-EM3000 and Rodrep, we implemented several popular networks for comparison: UNet [19], D-LinkNet [28], AttentionUNet [16], TransUNet [1], and uncertainty-based counterparts, Monte Carlo (MC) BayesianNet [2], Lee's method [9]. On the fundus image dataset FIVES, we additionally implemented several recent retinal vessel segmentation algorithms: FR-UNet [12], SA-UNet [4], and IterNet [11].As shown in Table 2, the proposed SSU-Net achieved the best performance. On the TM-EM3000 dataset, the uncertainty-based methods outperformed the typical convolution and attention methods, which proves that introducing uncertainty is beneficial. On the Rodrep dataset, SSU-Net performed considerably better than Lee's uncertainty method, improving the Dice score by 4.98%, mIoU by 3.48%, and mAcc by 3.14%. The results further suggest that the multi-scale predictions fusion module is crucial to elevate the robustness. According to the indication of uncertainty map u e , we cropped and zoomed in two ambiguous regions of each image, as shown in Fig. 2. The visualization results suggested that the proposed SSU-Net effectively improved the segmentation performance in ambiguous regions. On the FIVES dataset, the performance of the specialized network for retinal blood vessel segmentation in the fundus was similar to that of UNet, TransUNet, and AttettionUNet. The uncertainty-based methods are uniformly significantly superior to the above methods. The proposed SSU-Net achieved the best performance, increasing the Dice score by 10.08%, mIoU by 7.29%, and mAcc by 7.51% compared with UNet. Qualitative analysis is shown in Fig. 3, further supporting the conclusions of quantitative analysis.  "
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,4.0,Conclusion,"This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation. The ablation study shows the effectiveness of core components: the soft gated uncertainty-aware (GSUA) and the multi-scale uncertainty-aware (MSUA) fusion modules. Compared with some SOTA methods on cornea endothelial cell and retinal vessel image segmentation tasks, the proposed SSU-Net achieved the best segmentation performance and is more robust than other uncertainty-based methods. It is noteworthy that the SSU-Net performed considerably better than specialized retinal vessel segmentation networks. In the future, we plan to conduct experiments on various challenging situations to further explore the characteristics of SSU-Net."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,1.0,Introduction,"Left Ventricular Hypertrophy (LVH), one of the leading predictors of adverse cardiovascular outcomes, is the condition where heart's mass abnormally increases secondary to anatomical changes in the Left Ventricle (LV) [10]. These anatomical changes include an increase in the septal and LV wall thickness, and the enlargement of the LV chamber. More specifically, Inter-Ventricular Septal (IVS), LV Posterior Wall (LVPW) and LV Internal Diameter (LVID) are assessed to investigate LVH and the risk of heart failure [21]. As shown in Fig. 1(a), four landmarks on a parasternal long axis (PLAX) echo frame can characterize IVS, LVPW and LVID, and allow cardiac function assessment. To automate this, machine learning-based (ML) landmark detection methods have gained traction.It is difficult for such ML models to achieve high accuracy due to the sparsity of positive training signals (four or six) pertaining to the correct pixel locations. In an attempt to address this, previous works use 2D Gaussian distributions to smooth the ground truth landmarks of the LV [9,13,18]. However, as shown in Fig. 1(b), for LV landmark detection where landmarks are located at the wall boundaries (as illustrated by the dashed line), we argue that an isotropic Gaussian label smoothing approach confuses the model by being agnostic to the structural information of the echo frame and penalizing the model similarly whether the predictions are perpendicular or along the LV walls.In this work, to address the challenge brought by sparse annotations and label smoothing, we propose a hierarchical framework based on Graph Neural Networks (GNNs) [25] to detect LV landmarks in ultrasound images. As shown in Fig. 2, our framework learns useful representations on a hierarchical grid graph built from the input echo image and performs multi-level prediction tasks.Our contributions are summarized below.• We propose a novel GNN framework for LV landmark detection, performing message passing over hierarchical graphs constructed from an input echo; • We introduce a hierarchical supervision that is automatically induced from sparse annotations to alleviate the issue of label smoothing;• We evaluate our model on two LV landmark datasets and show that it not only achieves state-of-the-art mean absolute errors (MAEs) (1.46 mm and 1.86 mm across three LV measurements) but also outperforms other methods in out-of-distribution (OOD) testing (achieving 4.3 mm). "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,2.0,Related Work,"Various convolution-based LV landmark detection works have been proposed. Sofka et al. [26] use Fully Convolutional Networks to generate prediction heatmaps followed by a center of mass layer to produce the coordinates of the landmark locations. Another work [18] uses a modified U-Net [24] model to produce a segmentation map followed by a focal loss to penalize pixel predictions in close proximity of the ground truth landmark locations modulated by a Gaussian distribution. Jafari et al. [13] use a similar U-Net model with Bayesian neural networks [8] to estimate the uncertainty in model predictions and reject samples that exhibit high uncertainties. Gilbert et al. [6] smooth ground truth labels by placing 2D Gaussian heatmaps around landmark locations at angles that are statistically obtained from training data. Lastly, Duffy et al. [4] use atrous convolutions [1] to make predictions for LVID, IVS and LVPW measurements.Other related works focus on the detection of cephalometric landmarks from X-ray images. These works are highly transferable to the task of LV landmark detection as they must also detect a sparse number of landmarks. McCouat et al. [20] is one of these works that abstains from using Gaussian label smoothing, but still relies on one-hot labels and treats landmark detection as a pixel-wise classification task. Chen et al. [2] is another cephalometric landmark detection work that creates a feature pyramid from the intermediate layers of a ResNet [11].Our approach is different from prior works in that it aims to avoid the issue shown in Fig. 1(b) and the sparse annotations problem by the introduction of simpler auxiliary tasks to guide the main pixel-level task, so that the ML model learns the location of the landmarks without relying on Gaussian label smoothing. It further improves the representation learning via efficient messagepassing [7,25] of GNNs among pixels and patches at different levels without having as high a computational complexity as transformers [3,19]. Lastly, while GNNs have never been applied to the task of LV landmark detection, they have been used for landmark detection in other domains. Li et al. [16] and Lin et al. [17] perform face landmark detection via modeling the landmarks with a graph and performing a cascaded regression of the locations. These methods, however, do not leverage hierarchical graphs and hierarchical supervision and instead rely on initial average landmark locations, which is not an applicable approach to echo, where the anatomy of the depicted heart can vary significantly. Additionally, Mokhtari et al. [22] use GNNs for the task of EF prediction from echo cine series. However, their work focuses on regression tasks."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.1,Problem Setup,"We consider the following supervised setting for LV wall landmark detection. We have a dataset D = {X, Y }, where |D| = n is the number of {x i , y i } pairs such that x i ∈ X, y i ∈ Y , and i ∈ [1, n]. Each x i ∈ R H×W is an echo image of the heart, where H and W are height and width of the image, respectively, and each y i is the set of four point coordinates [(h i 1 , w i 1 ), (h i 2 , w i 2 ), (h i 3 , w i 3 ), (h i 4 , w i 4 )] indicating the landmark locations in x i . Our goal is to learn a function f : R H×W → R 4×2 that predicts the four landmark coordinates for each input image. A figure in the supp. material further clarifies how the model generates landmark location heatmaps on different scales (Fig. 2)."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.2,Model Overview,"As shown in Fig. 2, each input echo frame is represented by a hierarchical grid graph where each sub-graph corresponds to the input echo frame at a different resolution. The model produces heatmaps over both the main pixel-level task as well as the coarse auxiliary tasks. While the pixel-level heatmap prediction is of main interest, we use a hierarchical multi-level loss approach where the model's prediction over auxiliary tasks is used during training to optimize the model through comparisons to coarser versions of the ground truth. The intuition behind such an approach is that the model learns nuances in the data by performing landmark detection on the easier auxiliary tasks and uses this established reasoning when performing the difficult pixel-level task."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.3,Hierarchical Graph Construction,"To learn representations that better capture the dependencies among pixels and patches, we introduce a hierarchical grid graph along with multi-level prediction tasks. As an example, the simplest task consists of a grid graph with only four nodes, where each node corresponds to four equally-sized patches in the original echo image. In the main task (the one that is at the bottom in Fig. 2 and is the most difficult), the number of nodes is equal to the total number of pixels.More formally, let us denote a graph as G = (V, E), where V is the set of nodes, and E is the set of edges in the graph such that if v i , v j ∈ V and there is an edge from v i to v j , then e i,j ∈ E. To build hierarchical task representations, for each image x ∈ X and the ground truth y ∈ Y , K different auxiliary graphs G k (V k , E k ) are constructed using the following steps for each k ∈ [1, K]:that the larger values of k correspond to graphs of finer resolution, while the smaller values of k correspond to coarser graphs. 2. Grid-like, undirected edges are added such that e m-1,q , e m+1,q , e m,q-1 , e m,q+1 ∈ E k for each m, q ∈ [1 . . . 2 k ] if these neighbouring nodes exist in the graph (border nodes will not have four neighbouring nodes). 3. A patch feature embedding z k j , where j ∈ [1 . . . 4 k ] is generated and associated with that patch (node) v j ∈ V k . The patch feature construction technique is described in Sect. 3.4. 4. Binary node labels ŷk ∈ {0, 1} 4 k ×4 are generated such that ŷkj = 1 if at least one of the ground truth landmarks in y is contained in the patch associated with node v j ∈ V k . Note that for each auxiliary graph, four different one-hot labels are predicted, which correspond to each of the four landmarks required to characterize LV measurements.The main graph, G main , has a grid structure and contains H × W nodes regardless of the value of K, where each node corresponds to a pixel in the image. Additionally, to allow the model to propagate information across levels, we add inter-graph edges such that each node in a graph is connected to four nodes in the corresponding region in the next finer graph as depicted in Fig. 2."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.4,Node Feature Construction,"The graph representation described in Sect. 3.3 is not complete without proper node features, denoted by z ∈ R |V |×d , characterizing patches or pixels of the image. To achieve this, the grey-scale image is initially expanded in the channel dimension using a CNN. The features are then fed into a U-Net where the decoder part is used to obtain node features such that deeper layer embeddings correspond to the node features for the finer graphs. This means that the main pixel-level graph would have the features of the last layer of the network. A figure clarifying node feature construction is provided in the supp. material (Fig. 1)."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.5,Hierarchical Message Passing,"We now introduce how we perform message passing on our constructed hierarchical graph using GNNs to learn node representations for predicting landmarks. The whole hierarchical graph created for each sample, i.e., the main graph, auxiliary graphs, and cross-level edges, are collectively denoted as G i , where i ∈ [1, . . . , n]. Each G i is fed into GNN layers followed by an MLP:where σ is the Sigmoid function, h l nodes ∈ R |V G i |×d is the set of d-dimensional embeddings for all nodes in the graph at layer l, and h out ∈ [0, 1] |V G i |×4 is the four-channel prediction for each node with each channel corresponding to a heatmap for each of the pixel landmarks. The initial node features h 1 nodes are set to the features z described in Sects. 3.3 and 3.4. The coordinates (x p out , y p out ) for each landmark location p ∈ [1,2,3,4] are obtained by taking the expected value of individual heatmaps h p out along the x and y directions such that:where similar operations are performed in the y direction for y p out . Here, we vectorize the 2D heatmap into a single vector and then feed it to the softmax. loc x and loc y return the x and y positions of a node in the image. It must be noted that unlike some prior works such as Duffy et al. [4] that use postprocessing steps such as imposing thresholds on the heatmap values, our work directly uses the output heatmaps to find the final predictions."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.6,Training and Objective Functions,"To train the network, we leverage two types of objective functions. 1) Weighted Binary Cross Entropy (BCE): Since the number of landmark locations is much smaller than non-landmark locations, we use a weighted BCE loss; 2) L2 regression of landmark coordinates: We add a regression objective which is the L2 loss between the predicted coordinates and the ground truth labels."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.1,Datasets,"Internal Dataset: Our private dataset contains 29,867 PLAX echo frames, split in a patient-exclusive manner with 23824, 3004, and 3039 frames for training, validation, and testing, respectively. External Dataset: The public Unity Imaging Collaborative (UIC) [12] LV landmark dataset consists of a combination of 3822 end-systolic and end-diastolic PLAX echo frames acquired from seven British echocardiography labs. The provided splits contain 1613, 298, and 1911 training, validation, and testing samples, respectively. For both datasets, we down-sample the frames to a fixed size of 224 × 224."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.2,Implementation Details,"Our model creates K = 7 auxiliary graphs. For the node features, the initial single-layer CNN uses a kernel size of 3 and zero-padding to output features with a dimension of 224 × 224 × 4 (C = 4). The U-Net's encoder contains 7 layers with 128 × 128, 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, and 2 × 2 spatial dimensions, and 8, 16, 32, 64, 128, 256, and 512 number of channels, respectively. Three Graph Convolutional Network (GCN) [15] layers (L = 3) with a hidden node dimension of 128 are used. To optimize the model, we use the Adam optimizer [14] with an initial learning rate of 0.001, β of (0.9, 0.999) and a weight decay of 0.0001, and for the weighted BCE loss, we use a weight of 9000. The model is implemented using PyTorch [23] and Pytorch Geometric [5] and is trained on two 32-GB Nvidia Titan GPUs. Our code-base is publicly available at https://github.com/ MasoudMo/echoglad."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.3,Results,"We evaluate models using Mean Absolute Error (MAE) in mm, and Mean Percent Error (MPE) in percents, which is formulated as MPE = 100×|L pred -Ltrue| Ltrue , where L pred and L true are the prediction and ground truth values for every measurement. We also report the Success Detection Rate (SDR) for LVID for 2 and 6 mm thresholds. This rate shows the percentage of samples where the absolute error between ground truth and LVID predictions is below the specific threshold. These thresholds are chosen based on the healthy ranges for IVS (0.6-1.1cm), LVID (2.0-5.6cm), and LVPW (0.6-0.1cm). Hence, the 2 mm threshold provides a stringent evaluation of the models, while the 6 mm threshold facilitates the assessment of out-of-distribution performance.In-Distribution (ID) Quantitative Results. In Table 1, we compare the performance of our model with previous works in the ID setting where the training and test sets come from the same distribution (e.g., the same clinical setting), we separately train and test the models on the private and the public dataset. The results for the public dataset are provided in the supp. material (Table 1).Out-of-Distribution (OOD) Quantitative Results. To investigate the generalization ability of our model compared to previous works, we train all models on the private dataset (which consists of a larger number of samples compared to UIC), and test the trained models on the public UIC dataset as shown in Table 2. Based on our visual assessment, the UIC dataset looks very different compared to the private dataset, thus serving as an OOD test-bed. Qualitative Results. Failure cases are shown in supp. material (Fig. 3).Ablation Studies. In Table 3, we show the benefits of a hierarchical graph representation with a multi-scale objective for the task of LV landmark detection.We provide a qualitative view of the ablation study in supp. material (Fig. 4)."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,5.0,Conclusion and Future Work,"In this work, we introduce a novel hierarchical GNN for LV landmark detection. The model performs better than the state-of-the-art on most measurements without relying on label smoothing. We attribute this gain in performance to two main contributions. First, our choice of representing each frame with a hierarchical graph has facilitated direct interaction between pixels at differing scales. This approach is effective in capturing the nuanced dependencies amongst the landmarks, bolstering the model's performance. Secondly, the implementation of a multi-scale objective function as a supervisory mechanism has enabled the model to construct a superior inductive bias. This approach allows the model to leverage simpler tasks to optimize its performance in the more challenging pixel-level landmark detection task.For future work, we believe that the scalability of the framework for higherresolution images must be studied. Additionally, extension of the model to video data can be considered since the concept of intra-scale and inter-scale edges connecting nodes could be extrapolated to include temporal edges linking similar spatial locations across frames. Such an approach could greatly enhance the model's performance in unlabeled frames, mainly through the enforcement of consistency in predictions from frame to frame."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 22.
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,1.0,Introduction,"Accurate segmentation of the cardiac structure from echocardiogram videos is integral to several analysis tasks [11] and has a significant impact on clinical Z. Zheng and J. Yang-Two authors contributed equally to this work. Z. Zheng-Work completed during the internship at HKUST. practice [26]. For example, segmentation of the left ventricle (LV) enables quantifiable functional analysis of the heart, facilitating the detection and diagnosis of heart diseases [3,20,21]. Compared with the single view segmentation, multi-view information is crucial to diagnose heart disease, e.g., the diagnosis of congenital heart disease requires the analysis of four views: parasternal longaxis view (PSLAX), parasternal short-axis view (PSSAX), subxiphoid long-axis view (SXLAX), and suprasternal long-axis view (SSLAX) [22]. Consequently, to assist clinicians in diagnostic decision-making, there is a high demand for developing automated multi-view cardiac structure segmentation methods from echocardiogram videos in clinical practice. Existing echocardiogram segmentation approaches are primarily designed for single-view images or videos. For instance, Li et al. [26] proposed a dynamic neural network capable of segmenting the LV from a long-axis fetal echocardiogram. In comparison, Leclerc et al. [12] evaluated an encoder-decoder deep convolutional neural network that independently segments two and four-chamber images. However, these approaches have not addressed multi-view segmentation, where multi-view segmentation methods already exist in other medical domains, such as the CT-MRI [9,17,18], multiview cardiac MRI [4,13,15,16], multi-view mammogram [2], and longitudinal multiple sclerosis [1]. Applying the proposed methods to multi-view echocardiogram segmentation presents several limitations: (1) Some methods are built for specific datasets and cannot adapt to our task. For instance, UMCT [25] designated supervised training in one view by generating pseudo segmentation labels from other views, but has limitations in our task due to the significant gaps between views. In contrast, InfoTrans [13] is designed for transmitting information between views instead of fusion them. While VCN [6] employs contrastive learning to predict volume but may not be suitable for our task since defining positive and negative pairs is challenging due to the significant gap between views and labels. (2) Methods such as JOIN [2], ROI-based fine-grained CNN [14],MIMTP [1], MV U-Net [4], MV-CNN [23], and Type-I, II, III [9] concatenate the features or predicted probability maps of different views and then apply a fully-connected layer. However, these naive fusion strategies have shown limited performance and may even lead to worse results; see results in Table 1. (3) Existing multi-view segmentation methods such as TransFusion [15] and rDLA [16] mainly apply multi-view fusion with only global features. However, using global features for multi-view fusion may result in tangling the foreground/background pixels [10] or leads to high levels of background noise in echocardiograms.To address this limitation, as shown in Fig. 1, we first collect a multi-view echocardiogram video dataset, including three views: parasternal left ventricle long axis (PLVLA view), left ventricular short axis (LVSA) view, and apical 4 chamber (A4C) view. Different views of echocardiograms contain annotations for different chambers, such as, the PLVLA view contains the left ventricle (LV) and right ventricle (RV), the LVSA view contains the LV and RV, and the A4C view contains the LV, left atrium (LA), right atrium (RA), and RV. Furthermore, we propose a novel global-local fusion (GL-Fusion) network for multiview echocardiogram video segmentation, where GL-Fusion includes a multi-view local-global fusion module designed to aggregate information from different views and improve the representation of each view. The GL-Fusion comprises two components. First, a multi-view global fusion module (MGFM) interacts with the global semantics between different views and thus enhances the representation of each view. Second, since the global semantics may contain a significant amount of noisy information, a multi-view local fusion module (MLFM) is introduced to encourage the model to focus on foreground information.In addition to capturing multi-view information, we propose a novel dense cycle loss designed to utilize unlabelled video data for improved representation learning. Our motivation is based on the idea that standard multi-view data is obtained from the same patient and under the same stable conditions, without abnormal behaviours such as suffocating or exercising, ensuring consistent cardiac cycles. Previous work [7] proposed an unsupervised method called cycle loss, which trains the model with unlabelled frames based on the heartbeat cycle's characteristics. Nevertheless, the proposed cycle loss only focuses on a pair in two different cycles but ignores possibly similar images that may appear simultaneously in a systolic or diastolic period, resulting in features from similar frames being considered distant. To address this issue, our dense cycle loss examines all possible pairings throughout the heartbeat cycle. In summary, our contributions are as follows:-To the best of our knowledge, this is the first study to examine multi-view echocardiogram video segmentation. -Our proposed GL-Fusion uses a multi-view local-global fusion module to combine information from different views and improve the representation of each view. -We further design a dense cycle loss that utilizes unlabelled data to enforce feature similarity based on temporal cyclicality.  -Extensive experiments demonstrate our method improved performance over existing methods, achieving an average dice score of 0.81. We plan to make our code publicly available upon paper acceptance. "
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.1,The Overall Framework,", where X i ∈ R C×H×W ×T is the i-th view video and V is the number of views, and, C, H, W and T indicate the channels, height, width, and length of input images. Each video consists of T frames, i.e., X i = {x i t } T t=1 , where T remain the same for different view and x i t ∈ R C×H×W indicate t-th frame of i-th view video.Since only sparse frames are provided segmentation annotation for training in a video, thus we denote the annotation frame pair as {x i tn , y i tn } N n=1 , where t n is the index of the annotation and N is the number of labelled frames that N << T .During the training, We feed the videos V into the view-based encoder to extract the corresponding feature maps {F i } V i=1 of each view, where F i ∈ R D×h×w×T , and, D, h and w indicate the channel number, height and width of feature maps. Then the multi-view global-local fusion module aims to obtain the multi-view fused features {F i } V i=1 , which extract global and local semantics information from other views to enhance the representation of each view (See Sect. 2.2). Following is the view-based decoder that generates the predicted segmentation result y i from fused features, and maps the results to corresponding segmentation annotation, i.e., ŷi tn to the segmentation masks y i tn . For the annotated frames, we use the segmentation loss to supervise them, formulated as follows:where L bce is the Binary Cross Entropy. The sparse annotations are only a few frames in the whole video; thus can not obtain a robust model. To leverage a large number of unlabelled frames, we design the dense cycle loss L cyc to enforce temporal feature similarity of videos based on cyclicality; See Sect. 2.3.The overall loss function of our model is as follows:where α is the hyper-parameter to control the weight between two losses. In the following, we will illustrate the multi-view global-local fusion module and the dense cycle loss in detail."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.2,Multi-view Global-Local Fusion Module,"In this section, we describe the multi-view global-local fusion module that aggregates the information from different views to enhance their feature representation. To this end, we first concatenate extracted feature {F i } V i=1 from different views in a view-wise manner to obtain F = {f t } T t=1 , where f t is the t-th feature vector in F, and f t ∈ R D×V ×w×h . Then, we describe the multi-view global and local fusion with F global and F local , respectively.Multi-view Global Fusion. In order to enhance the representation of each view, we propose the global-based fusion module (MGFM) to interact with the global semantics between different views. To this end, we introduce a view-wise non-local block, which extracts the context information across views. Similarly to the previous research [8,24] that applied attention to fuse the information, we here introduce the view-wise attention module to aggregate the cross-view information (see Fig. 2). Then fused feature F global will be sent to both compute the dense cycle loss and cooperate with the local fused feature for segmentation prediction.Multi-view Local Fusion. Since each view represents different morphological information of the heart and may contain the same cardiac structure as others, for example, the view PLVLA and LVSA both contain left ventricle(LA) and right ventricle(RV). Hence, extracting the local feature that represents the cardiac structure can contribute to feature fusion more efficiently. In this module, the extracted feature F local will first pass to both the view-based decoder and a center block, where the decoder and center block has the same components with different output. The decoder provides the pseudo label {ŷ i } V i=1 of different cardiac structures. A center block is introduced to acquire the weight {w i } V i=1 of {ŷ i } V i=1 and compute the local feature masks {M i } V i=1 as Eq. 3,where weight w has the greatest volume in the central area of the segmented regions and attenuation with distance, σ denotes the sigmoid function and M ∈ R 1×H×W ×T . These masks highlight features with a stronger intensity that are closer to the object center, while discarding background information that is farther away from the center. This selection is based on the understanding that morphological information should remain consistent closer to the center. In the final, similar to the process of MLFM, the view-wise local feature will be conducted view-wise concatenation operation and multiplied with local feature mask {M i } V i=1 . Then sent to the view-wise attention module to acquire the local fused feature F local ."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.3,Dense Cycle Loss,"In echocardiogram videos, since only sparse annotation is available for the supervised training, involving the unlabelled data for our training and enhancing the performance is a challenge. The previous research [7] proposes an unsupervised method named cycle loss, which jointly trains the model with the unlabelled frames according to the characteristic of the heartbeat cycle. However, the proposed cycle loss considers only one clip in an iteration, which has the possibility to match frames that are morphologically identical but not in the same state, such as the search region being end-diastole while the template region is endsystole.Thus, we propose the dense cycle loss, which considers all the possible matching across all template and search regions in each view independently. For the multi-view fused feature F global of each video will be separated to template region P i and search region Q i with a ratio in 2:3 according to total frame length T . Then we densely sample all feature intervals {p i 1 , ..., p i n } from P i and {q i 1 , ..., q i m } from Q i , respectively, both sampling use the same chunk size s and in our experiment, n and m is 2 5 × T s and 3 5 × T s . Then we compute the similarity between candidate interval p i k and target intervals q i j of Q i . where W(•) is the computation of the similarity matrix. The similarity will be used as the weight to reconstruct the feature interval pi k . Then we back to template region P i and compute the similarity between pi k and all feature intervals{p i 1 , ..., p i n } in P i . Then we consider the index of p i k as one-hot label g of the most similar interval of pi k and compute view-wise cycle loss L cyc with label g as shown in the following equation:"
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3.0,Experiment,"Datasets. We collect a large multi-view echocardiogram video dataset named MvEVD from one medical institution, with a total of 254 sparsely annotated videos and 10 fully annotated videos with 800 × 600 resolution across three cardiac views (PLVLA, LVSA and A4C view). Each video includes 5 annotated frames. The average length of each video is larger than 100 frames that are able to cover more than one cardiac cycle.Implementation Details. We use the model DeeplabV3 [5]     Validation and Testing. We use all fully annotated videos and split them into validation and testing with a ratio of 2:8. In this stage, we resize each frame in 144 × 144 size and conduct center cropping to them with the size of 112 × 112. Selecting the best model based on validation performance and report results in the testing set with Dice score."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3.1,Comparison with the State-of-the-Art Methods,"To evaluate the performance of our method, we do the comparison with two types of methods: single-view methods and fusion-based methods in Table 1. To be specific, single-view methods independently train segmentation networks for each view without using any strategy across views or simply conducting semi-supervised approaches [7]. Fusion-based methods use feature-fusion modules to aggregate features and predict the segmentation masks. Our GL-Fusion method can reach 83.84%, 81.76% and 81.28% performance in Dice score across three different views, with 10.49%, 4.19% and 4.68% boosts when compared with the best single-view method [19], and 4.75%, 2.06%, 3.57% enhancement when compared to the best single-view with semi-supervised method CSS [7]. Also, compared with the different global fusion methods, our global and local fusion methods conduct significant improvements compared with the early-fusion approach. The visualization in Fig. 3 compares the segmentation quality with our GL-fusion method and others across three different views."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3.2,Ablation Study,"In this section, we analyze the contribution to the performance of the proposed modules Multi-view Global Fusion Module (MGFM) and Multi-view Global Fusion Module (MGFM) of our framework. All results are illustrated in Table 2. a-b, the baseline without adapting any fusion strategy presents the lowest average dice, while using only MGFM or MLFM module can boost the result to 80.20% and 78.41%, respectively. The combination of these two modules can reach 82.29% dice score with a 2.09% increase in Dice score. In contrast, using the fusion method and cycle loss will lead to worse performance, while our proposed dense cycle loss can boost the result from 80.36% to 82.29%."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,4.0,Conclusion,"In this paper, we propose a novel fusion framework called GL-Fusion, which jointly uses global and local information to enhance the segmentation performance of echocardiogram videos. Additionally, to ensure fair evaluation of the multi-view segmentation results, we introduce a multi-view echocardiogram video dataset called MvEVD, which provides full annotation for validating and testing performance. Our results demonstrate that the proposed GL-Fusion framework significantly outperforms other methods. In the future, we aim to further improve our method and make it more efficient."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,MGFM MLFM Global Fusion Module View * N,Sample All points from the video clip
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,1.0,Introduction,"Accurate segmentation of brain tumors from MRI images is of great significance as it enables more accurate assessment of tumor morphology, size, location, and distribution range, thereby providing clinicians with a reliable basis for diagnosis and treatment [16]. Physicians manually delineate the tumor regions based on the varying signal intensities between diseased and normal tissues. This signal disparity constitutes the edge information in the images, making it essential for accurate tumor segmentation.CNN-based networks, such as UNet [2], SegResNet [15], and nnUNet [8], have made significant progress in the field of medical image segmentation, including brain tumor segmentation. With the emergence of Transformer [19], which is capable of modeling long-range dependencies that CNNs struggle with, a number of CNN-Transformer hybrid networks have been proposed, such as TransBTS [21], UNETR [7], Swin-UNETR [6] and NestedFormer [23], leading to further improvements in brain tumor segmentation. However, the performance of existing brain tumor segmentation methods are still unsatisfactory, especially for the segmentation of edges between tumor lesion and normal tissues.Considerable advancement has been achieved in the field of natural image segmentation by focusing on the edge information [3,11,18,25], and this idea is also being applied to medical image segmentation. Some methods utilize the distance-dependent objective functions to generate more accurate edge predictions. Karimi et al. [9] design a Hausdorff-based metric loss function to minimize Hausdorff distance (HD), which is used to measure the edge distance between two point sets. Other methods [1,12,20,22] involve post-processing uncertain regions to more accurately segment pixels near edges. For example, BAT [20] considers global context to coarsely locate lesion area and paying special attention to the ambiguous area to specify the exact edges of the skin cancers. Similarly, Xie et al. [22] use the confidence map to evaluate the uncertainty of each pixel to enhance the segmentation of the ambiguous edges of ultrasound images. However, the methods mentioned above are not suitable for brain tumor segmentation for two main reasons. (1) Efficiency. For instance, Karimi et al. [9] require the calculation of the HD at each iteration, which is both time-consuming and computationally demanding. Moreover, processing every slice of large volumes of MRI images at the pixel-level is impractical. (2) Task Complexity. Unlike many other medical image segmentation tasks that involve the segmentation of a single ROI, brain tumor segmentation requires the simultaneous segmentation of three regions: the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET) regions. Therefore, in addition to focusing on the edge between the tumor lesion and normal tissue to segment the WT, it is also necessary to consider the edges within the tumor in order to segment the TC and ET regions.In this paper, we propose an Edge-oriented transFormer (EoFormer), for efficient and accurate brain tumor segmentation. We design a CNN-Transformer based encoder for more effective feature representation, called Efficient Hybrid Encoder (EHE). Specifically, the input image is first processed by the CNN blocks to extract low-level local features. Then, the extracted features are fed into the transformer blocks to create long-range dependencies, resulting in the formation of high-level semantic features. In addition, to provide more accurate edge predictions, we design two edge sharpening modules in the decoder, called Edgeoriented Sobel (EoS) and Laplacian (EoL) modules. By implicitly embedding Sobel and Laplacian filters into the convolution layers to extract 1st-order and 2nd-order differential features, the two modules could enhance the edge information contained in the feature maps. In order to reduce the computational and memory complexity of the model, we replace the vanilla attention module with our extended efficient attention module [17]. To simplify the model architecture and reduce inference time, we also introduce the re-parameterization technique [4,5]. Our model has been evaluated on both the publicly BraTS 2020 dataset and a private medulloblastoma segmentation dataset. The results demonstrate that EoFormer clearly outperforms the state-of-the-art methods with limited model parameters and lower FLOPs (see more in supplementary material). (2) a decoder which incorporates edge-oriented modules to enhance the edge information in features."
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,2.1,Efficient Hybrid Encoder,"The EHE, shown in Fig. 1(a), comprises four stages, each of which consists of a feature extraction module and a downsampling module. All four feature extraction modules follow the same paradigm of the general transformer architecture (see Fig. 1(b)), which regards the attention module in the transformer as a token mixer [24]. In the first two stages of EHE, we use depth-wise convolution (DWConv) to instantiate the token mixer, called the ConvFormer block. In the third stage and bottleneck, we use the multi-head self-attention (MSA) to instantiate the token mixer, which is the typical transformer block. For each stage i, given an input feature map X, the output of the i th block X is computed as follows:where the TokenMixer i (•) corresponds to DWConv (i ∈ {0, 1}) and MSA (i ∈ {2, 3}), Norm( • ) represents layer normalization, and MLP(•) denotes the Multilayer Perceptron. Our approach combines the strengths of CNN and transformer to create a more powerful encoder that can extract both local and global information from input data.We address the computational and memory complexity issues that arise from 3D input by replacing the vanilla attention with our extended 3D efficient attention. Assuming the size of the input feature is n and the dimensionality is d, the input feature X ∈ R n×d pass through three linear layers to generate the queriesand the efficient attention E(•) are computed as follows:where ρ(•) is the softmax activation function, T represents the matrix transpose operator. The efficient attention reduces the memory complexity and computational complexity of vanilla attention from O(n 2 ) and O(dn 2 ) to O(dn + d 2 ) and O(nd 2 ), where"
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,2.2,Edge-Oriented Transformer Decoder,"We design the EoFormer block (see Fig. 1(c)) in the decoder, which instantiates the token mixer with our proposed Edge-oriented Sobel module (EoS) and Edgeoriented Laplacian module (EoL). Each edge-oriented module includes a normal 3 × 3 × 3 convolution and an edge detection path to extract the 1st-order or the 2nd-order spatial derivatives from intermediate features. This design allows the edge-oriented module to efficiently extract the edges and textures of the features. Moreover, to boost the segmentation performance without sacrificing efficiency, we incorporate the re-parameterization technique in the decoder.Edge-Oriented Sobel Module. We use a dual-branch structure, where the input feature X is simultaneously processed by two different branches. The first branch contains a 3 × 3 × 3 convolution that extracts basic features from the input. The second branch, which is responsible for edge extraction, first uses a C × C × 1 × 1 × 1 convolution to enhance the interaction between channel features of X, then utilizes a learnable scaled Sobel filter to extract the 1storder differentiation edge information from X. This filter is capable of detecting edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so it comprises three filters M x , M y , and M z , each of which is represented by a 3 × 3 × 3 array. Take M x as an example, which is described as:We then apply a learnable scaling matrix S ∈ R C×1×1×1 to M x , which allows for dynamic adjustment of the scaling factor in each channel. The resulting feature extracted from the scaled Sobel-x filter is denoted as:where the '•' denotes channel-wise multiplication; the DWConv S•Mx indicates that DWConv(•) applies a S • M x learnable scaled filter as its kernel weight. Similarly, F y and F z are processed in the same way. The final output of the EoS module, denoted as F sob , is given by:Edge-Oriented Laplacian Module. Different from the Sobel filter that only extracts edges in the horizontal, vertical, and orthogonal directions, the Laplacian filter can extract edges in all directions. After extracting the 1st-order differentiation edge information, the intermediate features are then fed into the EoL module for extracting the 2nd-order differentiation edge information. Similarly, the feature F , obtained from the learnable scaling Laplacian filter, and the final output of the EoL module, denoted as F lap , are defined as:Re-parameterization of the Edge-Oriented Modules. We introduce the re-parameterization [4,5] into the edge-oriented modules to boost the segmentation performance while maintaining high efficiency. Specifically, we explain the re-parameterization of the EoL module as follows:where ' * ' represents the convolution operation, W conv means the weights of the convolution and B conv denotes the bias, and up(•) is the spatial broadcasting operation ,which upgrades the bias B ∈ R 1×C×1×1×1 into up(B) ∈ R 1×C×3×3×3 . In the inference stage, the output feature F is produced by a normal 3 × 3 × 3 convolution as follows: "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.1,Dataset and Evaluation Metric,"In order to validate the performance of EoFormer, we conduct extensive experiments on both the publicly available BraTS 2020 dataset and a private medulloblastoma segmentation dataset (MedSeg). The BraTS 2020 dataset [14] consists of MRI image data from 369 patients, with each patient having four modalities (T1, T1ce, T2 and T2-FLAIR) of skull-striped MRI, which are aligned to a standard brain template. The training/validation/test split follows 315/16/37 according to recent works [10,23].The MedSeg dataset includes MRI images of T1, T1ce, T2, and T2 FLAIR modalities from 255 patients with medulloblastoma. The dataset includes manual annotations of the WT and ET regions. These annotated masks are reviewed by two experienced physicians to ensure the accuracy of the annotated results. The images are registered to the size of 24 × 256 × 256. The training/validation/test split ratio is 3:1:1. Four-fold cross-validation is performed on this dataset. "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.2,Implementation Details,"We implement EoFormer in Pytorch 1.11. Our model is trained from scratch for 300 epochs using two NVIDIA GTX 3090 GPUs. We select a combination of soft dice loss and cross-entropy as the loss function and utilize the AdamW optimizer [13] with a weight decay of 1×10 -5 . The initial learning rate is 2×10 -4 . For data augmentation, we apply image croping, flipping, identity scaling and shifting."
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.3,Results,"We compare EoFormer with seven methods, including CNN-based methods (3D-UNet [2], SegResNet [15] and nnUNet [8]) and Transformer-based methods (TransBTS [21], UNETR [7], Swin-UNETR [6], NestedFormer [23]). The results are reproduced on our data split.Table 1 displays the performance comparison of EoFormer against other methods on the BraTS 2020 dataset. EoFormer achieves the highest Dice scores on TC, ET, and the average. In addition, EoFormer attains the best HD95 scores on TC, ET, and the average. HD95 measures the edge distance between prediction and annotation, which is more sensitive to boundaries. Table 2 illustrates the performance of EoFormer and other methods on MedSeg. EoFormer outperforms the second-ranked NestedFormer by an average of 1.59% on Dice and achieves the top performance for both WT and ET on HD95. Furthermore, compared to the second-ranked SegResNet, EoFormer demonstrates an "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.4,Ablation,We evaluate the effectiveness of our proposed EoFormer framework by conducting ablation experiments on the BraTS 2020. In 
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,4.0,Conclusion,"In this paper, we propose the EoFormer, a novel approach for brain tumor segmentation. Our method comprises the Efficient Hybrid Encoder and the Edgeoriented Transformer Decoder. The encoder effectively extracts features from images by striking a balance between CNN and Transformer architectures. The decoder integrates the Sobel and Laplacian edge detection filters into our edgeoriented modules that enhance the extraction capability of edge and texture information. Besides, we introduce the efficient attention mechanism and the re-parameterization technology to improve the model efficiency. Our EoFormer outperforms other state-of-the-art methods on both BraTS 2020 and MedSeg. Our model is computationally efficient and can be readily applied to other 3D medical image segmentation tasks."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1.0,Introduction,"Medical Image Analysis has greatly benefited from advances in AI [1] yet some improvements still remain to be addressed, importantly in areas that allow both algorithmic performance and fairness [2], and in certain medical applications that promise to significantly lessen morbidity and mortality. Early detection of skin lesions is such an endeavor as it can aid in identifying infectious diseases with cutaneous manifestations. Lyme disease is an example of that with a potentially diagnostic skin lesion [3]-which is caused by the bacterium Borrelia burgdorferi and leads to nearly 476,000 cases per annum during 2010-2018 [4]. The earliest and most treatable phase of Lyme disease is manifested via a red concentric lesion at the site of a tick bite, called erythema migrans (EM) [5]. While the EM pattern may appear simple to recognize, its diagnosis can be challenging for those with or without a medical background alike, as only 20% of United States patients have the stereotypical bull's eye lesion [6]. When skin lesions are atypical they can be mistaken for other diseases such as tinea corporis (TC) or herpes zoster (HZ), two other diseases acting as confusers for Lyme, considered herein. This has increased interest in medical applications of deep learning (DL), and using deep convolutional neural networks (CNNs), to assist clinicians in timely and accurate diagnosis of conditions including Lyme disease, TC and HZ [7][8][9].One important diagnosis task is to segment Lyme lesion, particularly the EM pattern, from benign skins. Such DL-assisted segmentation not only helps clinicians in pre-screening patients but also improves downstream tasks such as lesion classification. However, while Lyme disease lesion segmentation is intuitively simple, it is challenging due to the following reasons. First, there lacks of a well-segmented dataset with manual labels on Lyme disease. On one hand, some datasets-such as HAM10000 [10] and ISBI Challenges [11]-have manual annotated segmentations for diseases like melanoma, but they do not have Lyme disease lesions. On the other hand, some datasets-such as Groh et al. [12]-have Lyme disease and skin tone and classification labels, but not segmentation.Second, the segmentation of Lyme lesion is itself challenging due to the nature of EM pattern. Specifically, a typical Lyme lesion exhibits a bull's eye pattern with one central redness and one outer circle, which is different from darkness lesion in cancer-related skin disease like melanoma. Furthermore, clinical data collected for training is usually imbalanced in some properties, e.g., more samples with light skins compared with dark skins. Therefore, existing skin disease segmentation [13] as well as existing general segmentation works, such as U-Net [14], polar training [15], ViT-Adapter [16], and MFSNet [17], usually suffer from relatively low performance and reduced fairness [2,18,19].In this paper, we present the first Lyme disease dataset that contains labeled segmentation and skin tones. Our Lyme disease dataset contains two parts: (i) a classification dataset, composed of more than 3,000 diseased skin images that are either obtained from public resources or clinicians with patient-informed consent, and (ii) a segmentation dataset containing 185 samples that are manually annotated for three regions-i.e., background, skin (light vs. dark), and lesionconducted under clinician supervision and Institutional Review Boards (IRB) approval. Our dataset with manual labels is available at this URL [20].Secondly, we design a simple yet novel data preprocessing and alternation method, called EdgeMixup, to improve Lyme disease segmentation and diagnosis fairness on samples with different skin-tones. The key insight is to alter a skin image with a linear combination of the source image and a detected lesion boundary so that the lesion structure is preserved while minimizing skin tone information. Such an improvement is an iterative process that gradually improves lesion edge detection and segmentation fairness until convergence. Then, the detected, converged edge in the first step also helps classification of Lyme diseases via mixup with improved fairness. Our source code is available at this URL [20].We evaluate EdgeMixup for skin disease segmentation and classification tasks. Our results show that EdgeMixup is able to increase segmentation utility and improve fairness. We also show that the improved segmentation further improves classification fairness as well as joint fairness-utility metrics compared to existing debiasing methods, e.g., AD [21] and ST-Debias [22]."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,2.0,Motivation,"In this section, we motivate the design of EdgeMixup by showing that added lesion boundary helps a DL model focus more on the lesion part instead of other features such as skin or background. Note that not all skin disease datasets are carefully processed either due to the large amount of work required or the scarcity of data samples collected, e.g., SD-198 [23] contains samples that are taken under variant environments. Specifically, we train two ResNet-34 models using the same dataset with and without EdgeMixup for a classification task of skin disease. We keep all hyper-parameters exactly the same for two models, and only augment the same image with and without mixing lesion boundary up with the original image. We generate initial lesion edges using EdgeMixup, which we will elaborate in following sections. Figure 1 shows the original image (Fig. 1a) as well as two models' attention as heat-maps where red color represents the highest attention, yellow a higher attention, and purple the least attention. EdgeMixup helps the model to focus more on the lesion area comparing Fig. 1b and1c. The reason is that a legacy diagnosis has no information about lesion and does not know where to locate its focus, thus easily gets distracted by fingers instead of the lesion pattern."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,3.0,Method,"In this section, we first give the definition for model fairness, and we then describe the design of EdgeMixup for the purpose of de-biasing in Fig. 2  EdgeMixup improves model fairness on light and dark skin samples in both segmentation and classification tasks, and it has two major components: (i) edge detection using mixup, and (ii) data preprocessing and alteration for downstream tasks. More specifically, our proposed edge detection has two parts: initial edge detection and iterative improvement."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Initial Edge Detection:,"The purpose of initial detection, which is documented in the Initial_edge_detection function of Algorithm 1, is to provide a starting point, i.e., a rough boundary, for the next step of iterative improvement. The high-level idea is that EdgeMixup detects several edge candidates using the color range of ground-truth lesions in both Red-Green-Blue (RGB) and Hue-Saturation-Value (HSV) color space and then selects the target edge using a learning model based on the output confidence score. First, EdgeMixup trains a classification model based on a mixup of the ground-truth segmentation under clinician supervision and the original image (Line 7). Second, EdgeMixup generates many edge candidates. For example, EdgeMixup collects the mean range of lesion color from the training set and use the range as threshold to filter out any given sample for a candidate mask (Line 9). Lastly, EdgeMixup selects an edge candidate with the highest confidence score output by the learning model (Line 11) and returns it as the edge for this given sample. Note that the initial edge detection is irrelevant to the sample size of a particular subpopulation, thus improving the fairness. That is, even if the original dataset is imbalanced, as long as one sample from a subpopulation exists, the color range of the sample's lesion is considered in the initial detection. "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,9:,"Get all edge candidates {edge1, edge2, .., edgen} for each sample x 10:Mixup each edge candidate with x 11:Query mclass using all mixed-up {xedge 1 , ...xedge n } and choose the optimal edge edge opt"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,12:,"Generate edged sample xedge = Mixup(x, edge opt , α) "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,21:,while current_Jaccard > best_Jaccard do
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,22:,best_Jaccard = current_Jaccard
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,23:,"Predict lesion masks using miter, convert them to lesion edge edge"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,24:,"Generate new training set for next model Mixup(Dtrain, edge, α)"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,25:,Train a model for next iteration miter+1
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,26:,Evaluate miter+1 using edged D test edge and get current_Jaccard
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,27:,"iter += 1  affected area, further detection will refine and constrain the detected boundary. Besides, EdgeMixup calculates a linear combination of original image and lesion boundary, i.e., by assigning the weight of original image as α and lesion boundary as 1α. Figure 3 shows the edge-mixed-up images for different iterations. EdgeMixup removes more skin areas after each iteration and gradually gets close to the real lesion at the third iteration."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,4.0,Datasets,"We present two datasets: (i) a dataset collected and annotated by us (called Skin), and (ii) a subset of SD-198 [23] with our annotation (called SD-sub). First, We collect and annotate a dataset with 3,027 images containing three types of disease/lesions, i.e., Tinea Corporis (TC), Herpes Zoster (HZ), and Erythema Migrans (EM). All skin images are either collected from publicly available sources or from clinicians with patient informed consent. Then, a medical technician and a clinician in our team manually annotate each image. For the segmentation task, we annotate skin images into three classes: background, skin, and lesion; then, for the classification task, we annotate skin images by classifying them into four classes: No Disease (NO), TC, HZ, and EM. We name it as Skin-class for later reference. Second, we select five classes from SD-198 [23], a benchmark dataset for skin disease classification, as another dataset for both segmentation and classification tasks. Note that due to the amount of manual work involved in annotation, we select those classes based on the number of samples in each class. The selected classes are Dermatofibroma (DF), Keratoacanthoma (KA), Pyogenic Granuloma (PG), Tinea Corporis (TC), and Tinea Faciale (TF). We choose 30 samples in each class for segmentation task, and we split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing, respectively.Table 1 show the characteristics of these two datasets for both classification and segmentation tasks broken down by the disease type and skin tone, as calculated by the Individual Typology Angle (ITA) [24]. Specifically, we consider tan2, tan1, and dark as dark skin (ds) and others as light skin (ls). Compared to other skin tone classification schemas such as Fitzpartick scale [25], we divide ITA scores into more detailed categories (eight). One prominent observation is that ls images are more abundant than ds images due to a disparity in the availability of ds imagery found from either public sources or from clinicians with patient consent. "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,5.0,Evaluation,"We implement EdgeMixup using python 3.8 and Pytorch, and all experiments are performed using one GeForce RTX 3090 graphics card (NVIDIA).Segmentation Evaluation. Our segmentation evaluation adopts four baselines, (i) a U-Net trained to segment skin lesions, (ii) a polar training [15] transforming images from Cartesian coordinates to polar coordinates, (iii) ViT-Adapter [16], a state-of-the-art semantic segmentation using a fine-tuned ViT model, (iv) MFSNet [17], a segmentation model with differently scaled feature maps to compute the final segmentation mask. We follow the default setting from each paper for evaluation. Our evaluation metrics include (i) Jaccard index (IoU score), which measures the similarity between a predicted mask and the manually annotated ground truth, and (ii) the gap between Jaccard values (J gap ) to measure fairness. Table 2 shows the performance and fairness of EdgeMixup and different baselines. We compare predicted masks with the manually-annotated ground truth by calculating the Jaccard index, and computing the gap for subpopulations with ls and ds (based on ITA). EdgeMixup, a data preprocessing method, improves the utility of lesion segmentation in terms of Jaccard index compared with all existing baselines. One reason is that EdgeMixup preserves skin lesion information, thus improving the segmentation quality, while attenuating markers for protected factors. Note that EdgeMixup iteratively improves the segmentation results. Take our Skin-seg dataset for example. We trained our baseline Unet model for three iterations, and the model utility is increased by 0.0468 on Jaccard index while the J gap between subpopulations is reduced by 0.0193. Classification Evaluation. Our classification evaluation involves: (i) Adversarial Debiasing (AD) [21], (ii) DexiNed-avg, the average version of DexiNed [26] as an boundary detector used by EdgeMixup, and (iii) ST-Debias [22], a debiasing method augmenting data with conflicting shape and texture information. Our evaluation metrics include accuracy gap, the (Rawlsian) minimum accuracy across subpopulations, area under the receiver operating characteristic curve (AUC), and joint metrics (CAI α and CAUCI α ).Table 3 shows utility performance (acc and AUC) and fairness results (gaps of acc and AUC between ls and ds subpopulations). We here list two variants of EdgeMixup, and one of which, ""Unet"", uses the lesion edge generated by "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,6.0,Related Work,"Skin Disease Classification and Segmentation: Previous researches mainly work on improving model utility for both medical image [27] and skin lesion [28] classification. As for skin lesion segmentation tasks, few works has been proposed due to the lack of datasets with ground-truth segmentation masks. International Skin Imaging Collaboration (ISIC) hosts challenges of International Symposium on Biomedical Imaging (ISBI) [11] to encourage researches studying lesion segmentation, feature detection, and image classification. However, official datasets released, e.g., HAM10000 [10] only contains melanoma samples and all of the samples are with light skins according to our inspection using ITA scores.Bias Mitigation: Researchers have addressed bias and heterogeneity in deep learning models [18,29]. First, masking sensitive factors in imagery is shown to improve fairness in object detection and action recognition [30]. Second, adversarial debiasing operates on the principle of simultaneously training two networks with different objectives [31]. The competing two-player optimization paradigm is applied to maximizing equality of opportunity [32]. As a comparison, EdgeMixup is an effective preprocessing approach to debiasing when applied to skin disease particularly for Lyme-focused classification and segmentation tasks."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,7.0,Conclusion,"We present a simple yet novel approach to segment Lyme disease lesion, which can be further used for disease classification. The key insight is a novel data preprocessing method that utilizes edge detection and mixup to isolate and highlight skin lesions and reduce bias. EdgeMixup outperforms SOTAs in terms of Jaccord index for segmentation and CAI α and CAUCI α for disease classification."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,1.0,Introduction,"With the introduction of Vision Transformers (ViTs), CNNs have been greatly challenged as seen with the leading performance in multiple volumetric data benchmarks, especially for medical image segmentation [7,8,21,23]. The key contribution of ViTs is largely credited to the large Effective Receptive Field (ERF) with a multi-head self-attention mechanism [6]. Note the attention mechanism is computationally unscalable with respect to the input resolutions [17,18]. Therefore, the concept of depth-wise convolution is revisited to provide a scalable and efficient feature computation with large ERF using large kernel sizes (e.g., 7 × 7 × 7) [14,18]. However, either from prior works or our experiments, the model performance becomes saturated or even degraded when the kernel size is scaled up in encoder blocks [4,16]. We hypothesize that scaling up the kernel size in convolution may limit the optimal learning convergences across local to global scales. Recently, the feasibility of leveraging large kernel convolutions (e.g., 31 × 31 [4], 51 × 51 [16]) has been shown with natural image domain with Structural Re-parameterization (SR), which adapts Constant-Scale Linear Addition (CSLA) block (Fig. 2b) and re-parameterizes the large kernel weights during inference [4]. As convolutions with small kernel sizes converge more easily, the convergence of small kernel regions enhances in the re-parameterized weight, as shown in Fig. 1a. With such observation, we further ask: Can we adapt variable convergence across elements of the convolution kernel during training, instead of regional locality only?In this work, we first derive and extend the theoretical equivalency of the weight optimization in the CSLA block. We observe that the kernel weight of each branch can be optimized with variable convergence using branch-specific learning rates. Furthermore, the ERF with SR is visualized to be more widely distributed from the center element to the global surroundings [4], demonstrating a similar behavior to the spatial frequency in the human visual system [13]. Inspired by the reciprocal characteristics of spatial frequency, we model the spatial frequency as a Bayesian prior to adapt variable convergence of each kernel element with stochastic gradient descent (Fig. 1b). Specifically, we compute a scaling factor with respect to the distance from the kernel center and multiply the corresponding element for re-parameterization during training. Furthermore, we simplify the encoder block design into a plain convolution block only to minimize the computation burden in training and achieve State-Of-The-Art (SOTA) performance. We propose RepUX-Net, a pure 3D CNN with the large kernel size (e.g., 21 × 21 × 21) in encoder blocks, to compete favorably with current SOTA segmentation networks. We evaluate RepUX-Net on supervised multi-organ segmentation with 6 different public volumetric datasets. RepUX-Net demonstrates significant improvement consistently across all datasets compared to all SOTA networks. We summarize our contributions as below:-We propose RepUX-Net with better adaptation in large kernel convolution than 3D UX-Net, achieving SOTA performance in 3D segmentation. To our best knowledge, this is the first network that effectively leverages large kernel convolution with plain design in the encoder for 3D segmentation. -We propose a novel theory-inspired re-parameterization strategy to scale the element-wise learning convergence in large kernels with Bayesian prior knowledge. To our best knowledge, this is the first re-parameterization strategy to adapt 3D large kernels in the medical domain. -We leverage six challenging public datasets to evaluate RepUX-Net in 1)direct training and 2) transfer learning scenarios with 3D multi-organ seg-mentation. RepUX-Net achieves significant improvement consistently in both scenarios across all SOTA networks."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,2.0,Related Works,"Weights Re-parameterization: SR is a methodology of equivalently converting model structures via transforming the parameters in kernel weights. For example, RepVGG demonstrates to construct one extra ResNet-style shortcut as a 1 × 1 convolution, parallel to 3 × 3 convolution during training [5]. Such parallel branch design is claimed to enhance the learning efficiency during training, in which the 1 × 1 branch is then merged into the parallel 3 × 3 kernel via a series of linear transformation in the inference stage. OREPA further adds more parallel branches with linear scaling modules to enhance training efficiency [10]. Inspired by the parallel branches design, RepLKNet is proposed to scale up the 2D kernel size (e.g., 31 × 31) with a 3 × 3 convolution as the parallel branch [4]. SLaK further extends the kernel size to 51 × 51 by decomposing the large kernel into two rectangular parallel kernels with sparse groups and training the model with dynamic sparsity [16]. However, the proposed models' FLOPs remain at a high-level with the parallel branch design and demonstrates to have a trade-off between model performance and training efficiency. To tackle the tradeoff, RepOptimizer provides an alternative to re-parameterize the back-propagate gradient, instead of the structural parameters of kernel weights, to enhance the training efficiency with plain convolution block design [3]. Significant efforts have been demonstrated to enlarge the 2D kernel size in the natural image domain, while limited studies have been proposed for 3D kernels in medical domain. As 3D kernels have a larger number of parameters than 2D, it is challenging to directly leverage the parallel branch design and maintain an optimal convergence of learning large kernel convolution without trading off the computation efficiency significantly."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.0,Methods,"Instead of changing the gradient dynamics during training [3], we introduce RepUX-Net, a pure 3D CNN architecture that performs element-wise scaling in large kernel weights to enhance the learning convergence and effectively adapts large receptive field for volumetric segmentation. To design such behavior, we adapt a two-step pipeline: 1) we define the theoretical equivalency of variable learning convergence in convolution branches; 2) we simulate the behavior of spatial frequency to re-weight the learning importance of each element in kernels for stochastic gradient descent. Note the theoretical derivation depends on the optimization with first-order gradient-driven optimizer (e.g., SGD, AdamW) [3]."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.1,Variable Learning Convergence in Multi-Branch Design,"From Fig. 2b & 2c, previous re-parameterization strategies only demonstrate the benefits of the parallel branch design by either adding up the encoded outputs from both small and large kernels with SR (RepLKNet [4]) or performing Gradient Re-parameterization (GR) by multiplying with constant values (RepOptimizer [3]) in a Single Operator (SO) to enhance the locality learning in large kernels. Inspired by the concepts of SR and GR, we extend the theoretical equivalency proof in RepOptimizer to adapt variable learning convergence in branches.Here, we only showcase the conclusion with two convolutions and two constant scalars as the scaling factors for simplicity. The complete proof of equivalency is demonstrated in Supplementary 1.1. Let {α L , α S } and {W L , W S } be the two constant scalars and two convolution kernels (Large & Small) respectively. Let X and Y be the input and output features, the CSLA block is formulated as, where denotes as convolution. For SO blocks, we train the plain structure parameterized by W and Y SO = X W . Let i be the number of training iterations, we ensure thatSO , ∀i ≥ 0 and derive the stochastic gradient descent of parallel branches as follows:where L is the objective function; λ L and λ S are the Learning Rate (LR) of each branch respectively. We observe that the optimization of each branch can be different, which is feasible to control by adjusting the branch-specific LR.The locality convergence in large kernels enhance with the quick convergence in small kernels. Additionally from our experiments, a significant improvement is demonstrated with different branch-wise LR using SGD (Table 2). Building upon this insight, we further hypothesize that the convergence of each large kernel element can be optimized differently by linear scaling with prior knowledge."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.2,Bayesian Frequency Re-parameterization (BFR),"With the visualization of ERF in RepLKNet [4], the diffused distribution (from local to global) in ERF demonstrates similar behavior with the spatial frequency in the human visual system [13]. High spatial frequency (small ERF) allows to refine and sharpen details with high acuity, while global details are demonstrated with low spatial frequency. Inspired by the reciprocal characteristics in spatial frequency, we first generate a Bayesian prior distribution to model the spatial frequency by computing a reciprocal distance function between each element and the central point of the kernel weight as follows:where k and c are the element and central index of the kernel weight, α is the hyperparameter to control the shape of the generated frequency distribution. Instead of adjusting the LR in parallel branches, we propose to re-parameterize the convolution weights by multiplying the scaling factor δ to each kernel element and apply a static LR λ for stochastic gradient descent in single operator setting as follows:With the multiplication with δ, each element in the kernel weight is rescaled with respect to the frequency level and allow to converge differently with a static LR in stochastic gradient descent. Such design demonstrates to influence the weighted convergence diffused from local to global in theory, thus tackling the limitation of enhancing the local convergence only in branch-wise setting."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.3,Model Architecture,"The backbone of RepUX-Net is based on 3D UX-Net [14], which comprises multiple volumetric convolution blocks that directly utilize 3D patches and leverage skip connections to transfer hierarchical multi-resolution features for end-to-end optimization. Inspired by [15], we choose a kernel size of 21 × 21 × 21 for Depth-Wise Convolution (DWC-21) as the optimal choice without significant trade-off between model performance and computational efficiency in 3D. We further simplify the block design as a plain convolution block design to minimize the computational burden from additional modules. The encoder blocks in layers l and l + 1 are defined as follows:where ẑl and ẑl+1 are the outputs from the DWC layer in each depth level; BN denotes as the batch normalization layer."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,4.0,Experimental Setup,"Datasets. We perform experiments on six public datasets for volumetric segmentation, which comprise with 1) Medical Segmentation Decathlon (MSD) spleen dataset [1], 2) MICCAI 2017 LiTS Challenge dataset (LiTS) [2], 3) MIC-CAI 2019 KiTS Challenge dataset (KiTS) [9], 4) NIH TCIA Pancreas-CT dataset (TCIA) [20], 5) MICCAI 2021 FLARE Challenge dataset (FLARE) [19], and 6) MICCAI 2022 AMOS challenge dataset (AMOS) [12]. More details of each dataset (including data split for training and inference) are described in Supplementary Material (SM) Table 1.Implementation. We evaluate RepUX-Net with three different scenarios: 1) internal validation with direct supervised learning, 2) external validation with the unseen datasets, and 3) transfer learning with pretrained weights. All preprocessing and training details including baselines, are followed with [14] for benchmarking. For external validations, we leverage the AMOS-pretrained weights to evaluate 4 unseen datasets. In summary, we evaluate the segmentation performance of RepUX-Net by comparing current SOTA networks in a fully-supervised setting. Furthermore, we perform ablation studies to investigate the effect on Bayesian frequency distribution with different scales generated by α and the variability of branch-wise learning rates with first-order gradient optimizers (e.g., SGD, AdamW) for volumetric segmentation. Dice similarity coefficient is leveraged as an evaluation metric to measure the overlapping regions between the model predictions and the manual ground-truth labels."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,5.0,Results,"Different Scenarios Evaluations. Table 1 shows the result comparison of current SOTA networks on medical image segmentation in a volumetric setting.With our designed convolutional blocks as the encoder backbone, RepUX-Net demonstrates the best performance across all segmentation task with significant improvement in Dice score (FLARE: 0.934 to 0.944, AMOS: 0.891 to 0.902). Furthermore, RepUX-Net demonstrates the best generalizability consistently with a significant boost in performance across 4 different external datasets (MSD: 0.926 to 0.932, KiTS: 0.836 to 0.847, LiTS: 0.939 to 0.949, TCIA: 0.750 to 0.779). For transfer learning scenario, the performance of RepUX-Net significantly outper-   2. RepUX-Net demonstrates its capabilities across the generalizability of unseen datasets and transfer learning ability. The qualitative representations (in Fig. 3) further provides additional confidence of the quality improvement in segmentation predictions with RepUX-Net (Table 3)."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Ablation Studies with Block Designs & Optimizers.,"With the plain convolution design, a mean dice score of 0.906 is demonstrated with AdamW optimizer and perform slightly better than that with SGD. With the additional design of a parallel small kernel branch, the segmentation performance significantly improved (SGD: 0.898 to 0.917, AdamW: 0.906 to 0.929) with the optimized parallel branch LR using SR. The performance is further enhanced (SGD: 0.917 to 0.930, AdamW: 0.929 to 0.937) without being saturated with the increase of the training steps. By adapting BFR, the segmentation performance outperforms the parallel branch design significantly with a Dice score of 0.944.Effectiveness on Different Frequency Distribution. From Fig. 1 in SM, RepUX-Net demonstrates the best performance when α = 1, while comparable performance is demonstrated in both α = 0.5 and α = 8. A possible family of Bayesian distributions (different shapes) may need to further optimize the learning convergence of kernels across each channel.Limitations. The shape of the generated Bayesian distribution is fixed across all kernel weights with an unlearnable distance function. Each channel in kernels is expected to extract variable features with different distributions. Exploring different families of distributions to rescale the element-wise convergence in kernels will be our potential future direction."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,6.0,Conclusion,"We introduce RepUX-Net, the first 3D CNN adapting extreme large kernel convolution in encoder network for medical image segmentation. We propose to model the spatial frequency in the human visual system as a reciprocal function, which generates a Bayesian prior to rescale the learning convergence of each element in kernel weights. By introducing the frequency-guided importance during training, RepUX-Net outperforms current SOTA networks on six challenging public datasets via both direct training and transfer learning scenarios."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,1.0,Introduction,"Semantic segmentation, i.e., assigning a semantic label to each pixel in an image, is a common task in medical computer vision nowadays typically performed by fully convolutional encoder-decoder deep neural networks (DNNs). These DNNs usually incorporate some kind of normalization layers which are thought to reduce the impact of the internal covariate shift (ICS) [6]. This effect describes the adaption to small changes in the feature maps of deeper layers rather than learning the real representation of the target structures [21]. The understanding of Batch Normalization (BN) is very controversial, namely that its actual success is to use higher learning rates by smoothing the objective function instead of reducing the ICS [2]. Instance Normalization (IN), Layer Normalization (LN) and Group Normalization (GN) are examples of developments of BN to overcome its shortcomings, like reduced performance using smaller batch sizes [2]. As an alternative, Scaled Exponential Linear Units (SELUs) can act as selfnormalization activation functions [9]. All normalization methods have different strengths and weaknesses, which influence the performance and generalizability of the network. Commonly, only a single normalization method is used throughout the network, and studies involving multiple normalization methods are rare.Neural architecture search (NAS) is a strategy to tweak a neural architecture as such to discover efficient combinations of architectural building blocks for optimal performance on given datasets and tasks [12]. NAS strategies involve, for example, evolutionary algorithms to optimize an objective function by evaluating a set of candidate architectures and selecting the ""fittest"" architectures for breeding [12]. After several generations of training, selection, and breeding, the objective function of the evolutionary algorithm should be maximized.In this study, we propose a novel evolutionary NAS approach to increase semantic segmentation performance by optimizing the spatiotemporal usage of normalization methods in a baseline U-Net [17]. Our study provides a uniquely and thorough analysis of the most effective layer-wise normalization configuration across medical datasets, rather than proposing a new normalization method. In the following, we refer to our proposed methodology as evoNMS (evolutionary Normalization Method Search).We evaluated the performance of evoNMS on eleven biomedical segmentation datasets and compared it with a state-of-the-art semantic segmentation method (nnU-Net [7]) and U-Nets with constant normalization such as BN, IN, and no normalization (NoN). Our analysis demonstrates that evoNMS discovers very effective network architectures for semantic segmentation, achieves better or similar performance to state-of-the-art architectures, and guides the selection of the best normalization method for a specific semantic segmentation task. In addition, we analyze the normalization pattern across datasets and modalities and compare the normalization methods regarding their layer-specific contribution."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,2.0,Related Works,"To gain optimal performance in semantic segmentation tasks, it is important to optimize data preprocessing and architectural design. Popat et al. [15] and Wei et al. [19] concurrently developed an evolutionary approach to determine the best-performing U-Net architecture variant, considering the depth, filter size, pooling type, kernel type, and optimizer as hyperparameter-genes coding for a specific U-Net phenotype. When applied to retinal vessel segmentation, both showed that their approach finds a smaller U-Net configuration while achieving competitive performance with state-of-the-art architectures. Liu et al. [10] proved that not only the architecture has a huge impact on the generalizability of a neural network, but also the combination of normalization.Various studies show that neural networks (NNs) benefit from normalization to enhance task performance, generalizability, and convergence behavior. Zhou et al. [22] showed the benefit of batch normalization, which focuses on the data bias in the latent space by introducing a dual normalization for better domain generalization. Dual normalization estimates the distribution from source-similar and source-dissimilar domains and achieves a more robust model for domain generalization. Domain-independent normalization also helps to improve unsupervised adversarial domain adaptation for improved generalization capability as shown in [16]. In [21], the authors analyzed the influence of different normalization methods, such as BN, IN, LN, and GN. Although many segmentation networks rely on BN, they recommend using normalization by dividing feature maps, such as GN (with a higher number of groups) or IN [21]. Normalization methods have been well discussed in the literature [3,5,9,18,20]. In a systematic review, Huang et al. [5] concluded that normalizing the activations is more efficient than normalizing the weights. In addition to the efficiency of normalizing the activations, Luo et al. [13] demonstrated a synergistic effect of their advantages by introducing switchable normalization (SN). SN alternates between the normalization strategies IN, LN, and BN according to their respective importance weights [13]. With an evolutionary approach, Liu et al. [11] also showed that the combination of normalization and activation functions improves the performance of the NN."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,3.0,Methods,"We investigated the impact of normalization on semantic segmentation using eleven different medical image datasets. Eight datasets were derived from the Medical Segmentation Decathlon (MSD) [1]. In this study, we selected subsets for segmenting the hippocampus, heart, liver, lung, colon, spleen, pancreas, and hepatic vessels. We only considered the datasets with 3D volumes of the MSD. In addition, we used the BAGLS [4] dataset (segmentation of glottal area), the Kvasir-SEG [8] dataset (gastrointestinal polyp images), and an in-house dataset for bolus segmentation in videofluoroscopic swallowing studies (VFSS). Table 1 lists the datasets used regarding their region of interest/organ, modality, number of segmented classes, and number of images. To minimize the differences between the datasets and to gain comparable results, all datasets were analyzed in 2D. The images were converted to grayscale images, resized, and cropped to a uniform size of 224 × 224 px. Their pixel intensity was normalized to a range from 0 to 1. The datasets were divided into training, validation, and test subsets, with percentages of 70%, 10%, and 20% for the BAGLS, Kvasir-SEG, and bolus datasets. If a test set was explicitly given, the split for training and validation was 80% and 20%, respectively.We implemented our evolutionary optimization algorithm and the NN architectural design in TensorFlow 2.9.1/Keras 2.9.0 and executed our code on NVIDIA A100 GPUs. Each individual U-Net variant was trained for 20 epochs using the Adam optimizer, a constant learning rate of 1 × 10 -3 , and a batch size of 64. All segmentation tasks were optimized using the Dice Loss (DL). To evaluate the performance of the trained network, we calculated the Dice Coefficient (DC), Intersection over Union (IoU) of the fitted bounding boxes (BBIoU), and Hausdorff Distance with a percentile of 95% (HD95) of the validation set after  20 epochs. In addition, we included an early stopping criterion that is activated when the validation loss changes less than 0.1% to avoid unnecessary training time without further information gain. To compare our approach to state-of-theart segmentation networks, we considered nnU-Net [7] as a baseline which was similarly trained as the above-mentioned U-Net variants.Our proposed evoNMS approach is based on evolutionary optimization with leaderboard selection and is executed for 20 generations. Each generation's population consists of 20 individuals, i.e., U-Net variants, meaning that we train 400 variants for one evoNMS execution (duration 4 h (polyp) to 5 days (glottal area) on one A100 GPU). The first generation contains individuals with random sequences drawn from our gene pool containing either a BN, IN, FRN, GN2, GN4 layer or skips normalization (no normalization, NoN). Other U-Netspecific hyperparameters, such as initial filter size and activation functions were set across datasets to a fixed value (initial filter size of 16, and ReLU as activa-tion function). In general, we kept all other hyperparameters fixed to focus only on the influence of normalization and infer whether it exhibits decoder/encoder dependence or even dependence on the underlying modality. After training each architecture, the fitness F i (Eq. ( 1)) is evaluated for each individual iwhere we compute the mean validation DC, validation IoU of the Bounding Box, and reciprocal of the validation HD95. We use the reciprocal of HD95 to balance the influence of each metric on the fitness value by a value ranging from 0 to 1.After each generation, the top ten individuals with the highest fitness F were bred. To breed a new individual, we selected two random individuals from the top ten candidates and combined them with a randomly selected crossing point across the normalization layer arrays of the two parental gene pools. Next, we applied mutations at a rate of 10%, which basically changes the normalization method of a random position to any normalization technique available in the gene pool. "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,4.0,Results,"We first evaluated the influence of different normalization methods on medicalimage segmentation. We report the performance of neural architectures defined by our proposed evoNMS, which followed an evolutionary approach to determine the potentially best normalization method for each bottleneck layer, at generations 1 and 20. For each dataset, we evaluated the DC across different baselines of our default U-Net implementation with a fixed normalization method across layers (BN, IN, or NoN) and a state-of-the-art semantic segmentation network (nnU-Net) against our proposed evoNMS approach. Table 2 provides an overview of the mean validation DC as the main performance metric. Overall, the architecture configurations with solely IN (six datasets), nnU-Net (two datasets), or our proposed evoNMS (one first generation, four in the last generation) achieved the highest mean validation DC across all datasets. Noteworthy, our evoNMS approach achieved competitive performance across all datasets, which is clearly shown at the ranking in Table 2, where the last generation of our approach achieved the best grade of 1.72. The results of our evoNMS demonstrate that our approach achieves superior performance in terms of the average DC and BBIoU scores on the validation dataset and yields comparable results in terms of HD95 to the U-Net trained with IN. In contrast, architectures that rely on BN or NoN consistently produced poor results due to exploding gradients across multiple datasets ( [14]), questioning the broad application of BN. When comparing qualitative results of all baselines and evoNMS, we find that our approach accurately reflects the ground truth across datasets, especially after several generations of optimization (Fig. 2). We found that an evolutionary search without prior knowledge of the required hyperparameters and properties of the medical data can perform as well as, or in some cases, even better than, the best baseline of a U-net trained with IN or nnU-Net, showing the importance of normalization in semantic segmentation architectures.We next were interested in the optimization behavior of evoNMS. We found that random initialization of normalization methods yielded poorly and highly variant converging behavior overall (Supplementary Fig. 1). However, after evolutionary optimization, the converging behavior is clearly improved in terms of convergence stability and lower variability. In Supplementary Fig. 2, we can exemplary show that evoNMS also improves all fitness-related metrics across generations. This highlights the ability of evoNMS to converge on multiple objectives. These findings suggest that our approach can also be used as a hyperparameter optimization problem to improve convergence behavior.As we initialize the first population randomly, we determined whether the evoNMS algorithm converges to the same set of normalization methods for each dataset. We found for three independent evoNMS runs, that evoNMS converges for four out of eleven datasets on very similar patterns (Supplementary Fig. 3) across runs, with an overall average correlation of 36.3%, indicating that our algorithm is able to find relatively quickly a decent solution for a given dataset. In the case of the polyp dataset, top evoNMS-performing networks correlate with a striking 61.1% (Supplementary Table 1).We next identified the final distribution of normalization methods across the encoder and decoder U-Net layers to determine dataset-specific normalization. In Fig. 3, we show the distribution of the top 10% performers in the last generation of evoNMS. We found consistent patterns across individuals especially in the last layer: in four out of eleven datasets, no normalization was preferred. In the colon dataset, the encoder was mainly dominated by IN, especially in the first encoding layer. In contrast, the decoder in the polyp, liver, and hippocampus datasets showed more consistency in the normalization methods suggesting that normalization methods could be encoder-and decoder-specific and are dataset dependent.  To understand good normalization design across datasets in semantic segmentation tasks, we determined which normalization methods are more abundant at specific U-Net layers. IN is across layers a preferred choice by evoNMS except for the last two decoding layers (Fig. 4 A andB). Our results suggest that the latent space embedding heavily relies on IN (Fig. 4 A). Other normalization methods are less abundant in top-performing architectures, such as FRN, BN, and LN. However, NoN is mainly used in the last layer. FRN and especially BN seem to be inferior choices in semantic segmentation architectures.Finally, we investigated if any set of normalization methods can be derived by the imaging modality, such as endoscopy, CT and MRI. In Fig. 4, we show the cross-correlation of all evoNMS top performers across datasets. In general, there are only weak correlations at the global level; a stronger correlation can be seen by correlating encoder and decoder separately (Supplementary Fig. 3). These results provide evidence, that a priori knowledge of the imaging modality does not hint towards a specific normalization pattern but rather has to be optimized for any given dataset."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,5.0,Discussion and Conclusion,"Our approach shows that using normalization methods wisely has a powerful impact on biomedical image segmentation across a variety of datasets acquired using different imaging modalities. Due to its inherent property of always finding an optimal solution, evoNMS is potentially capable of providing the bestperforming set of normalization patterns for any given data set. For feasibility reasons, we considered only 20 epochs for each training set and 20 generations. However, we show that evoNMS with these constrained settings provides competitive results and outperforms or performs on par compared to all baselines.Our results suggest the superior performance of IN and GN (Fig. 4) as overall very useful normalization strategies when used at their preferred location, in contrast to FRN, BN, and LN. State-of-the-art architectures, such as nnU-Net also rely on IN throughout the network [7], as well as evolutionary optimized U-Net architectures [19]. This established use of normalization confirms our findings for high-performing evoNMS networks and can be extrapolated to other semantic segmentation architectures that incorporate normalization methods. The advantage of evoNMS is its ability to also include non-learnable objectives, such as architectural scaling and reducing inference time, crucial for point-of-care solutions. In this study, we constrained the search space, but we will incorporate multiple hyperparameters in the future. On the other hand, approaches that use main building blocks and optimize mainly convolutional layer parameters and activation functions [15,19] would benefit from incorporating normalization strategies as well."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Table 2 .,"For multiclass segmentation, we define the highest validation DC as the mean across the segmentation labels. Each value is the mean value of five individual runs. For evoNMS, we selected the best normalization pattern w.r.t. the fitness value and trained this configuration five times. The ranking represents the average behavior of each network across the datasets (1-best, 6-worst). The bottom rows show the average behavior of each network across the eleven datasets regarding DC, BBIoU, and HD95 on the validation dataset."
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 67.
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,1.0,Introduction,"Medical Hyperspectral Imaging (MHSI) is an emerging imaging modality which acquires two-dimensional medical images across a wide range of electromagnetic spectrum. It brings opportunities for disease diagnosis, and computational pathology [16]. Typically, an MHSI is presented as a hypercube, with hundreds of narrow and contiguous spectral bands in spectral dimension, and thousands of pixels in spatial dimension (Fig. 1(a)). Due to the success of 2-Dimensional (2D) deep neural network in natural images, the simplest way to classify/segment an MHSI is to treat its two spatial dimensions as input spatial dimension, and treat its spectral dimension as input channel dimension [25] (Fig. 1(c)). Dimensionality reduction [12] and recurrent approaches [1] are usually adopted to aggregate spectral information before feeding the HSI into 2D networks (Fig. 1(d)). These methods are not suitable for high spatial resolution MHSI, and they may bring noises in spatial features while reducing spectral dimension. The 2D networks are computationally efficient, usually much faster than 3D networks. But, they mix spectral information after the first convolutional layer, making the interband correlations of MHSIs underutilized. Building a 3D network usually suffers from high computational complexity, but it is the most straightforward way to learn interpixel and interband correlations of MHSIs [23] (Fig. 1(e)). Since spatiospectral orientations are not equally likely, there is no need to treat space and spectrum symmetrically, as is implicit in 3D networks. We might instead design a dual-stream strategy to ""factor"" the architecture. A few HSI classification backbones try to design dual-stream architectures that treat spatial structures and spectral intensities separately [2,20,30] (Fig. 1(f)). But, these methods simply adopt convolutional or MLP layers to extract spectral features. SpecTr [28], learning spectral and spatial features alternatively, utilizes Transformer to capture the global spectral feature. They overlook the low rankness in the spectral domain, which contains discriminative information for differentiating targets from the background.High spatiospectral dimensions make it difficult to perform a thorough analysis of MHSI. In MHSIs, there exist two types of correlation. One is a spectral correlation in adjacent pixels. As shown in Fig. 1(b), the intensity values vs. spectral bands for the local positive (cancer) area and negative (normal) area are highly correlated. The other is spatial correlation between adjacent bands. Figure 1(b) plots the spatial similarity among all bands, and shows large cosine similarity scores among nearby bands (error band of line chart in the light color area) and small scores between bands in a long distance. The correlation implies spectral redundancy when representing spatial features, and spatial redundancy when learning spectral features. The low-rank structure in MHSIs holds significant discriminatory and characterizing information [11]. Exploring MHSI's low-rank prior can promote the segmentation performance.In this paper, we consider treating spatiospectral dimensions separately and propose an effective and efficient dual-stream strategy to ""factor"" the architecture, by exploiting the correlation information of MHSIs. Our dual-stream strategy is designed based on 2D CNNs with U-shaped [16] architecture. For the spatial feature extraction stream, inspired from spatial redundancy between adjacent bands, we group adjacent bands into a spectral agent. Different spectral agents are fed into a 2D CNN backbone as a batch. For the spectral feature extraction stream, inspired by the low-rank prior on the spectral space, we propose a matrix factorization-based method to capture global spectral information. To remove the redundancy in the spatiospectral features and promote the capability of representing the low-rank prior of MHSI, we further design Lowrank Decomposition modules, and employ the Canonical-Polyadic decomposition method [9,32]. Our space and spectrum factorization strategy is plug-and-play. The effectiveness of the proposed strategy is compared and verified by plugging in different 2D architectures. We also show that with our proposed strategy, U-Net model using ResNet-34 can achieve state-of-the-art MHSI segmentation with 3-13 faster than other 3D architectures."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2.0,Methodology,"Mathematically, let Z ∈ R S×H×W denote the 3D volume of a pathology MHSI, where H × W is the spatial resolution, and S is the number of spectral bands. The goal of MHSI segmentation is to predict the per-pixel annotation mask, where Y i denotes the per-pixel groundtruth for MHSI Z i .The overall architecture of our proposed method is shown in Fig. 2, where the 2D CNN in the figure is a proxy which may represent all widely-used 2D architectures. It represents a spatial stream, which focuses on extracting spatial features from spectral agents (Sect. 2.1). The lightweight spectral stream learns multi-granular spectral features, and it consists of three key modules: Depthwise Convolution (DwConv), Spectral Matrix Decomposition (SMD) and Feed Forward Network, where SMD module effectively leverages low-rank prior from spectral features (Sect. 2.1). Besides, the Low-rank Decomposition module (LD) represents high-level low-rank spatiospectral features (Sect. 2.2). The input MHSI Z is decomposed into a spatial input Z spa ∈ R G×(S/G)×H×W and a spectral input Z spe ∈ R S×C spe 0 ×H×W , where G indicates evenly dividing spectral bands into G groups, i.e., spectral agents. S/G and C spe 0 = 1 are the input feature dimensions for two streams respectively."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2.1,Dual-Stream Architecture with SpatioSpectral Representation,"As mentioned above, for the spatial stream, we first reshape MHSI Z ∈ R S×H×W into Z spa ∈ R G×(S/G)×H×W , which has G spectral agents. Each spectral agent is treated as one sample. One sample contains highly correlated spectral bands, so that the spatial stream can focus on spatial feature extraction. For the spectral stream, to deal with problems of spatiospectral redundancy and the inefficiency of global spectral feature representation, we propose a novel and concise hierarchical structure shown in Fig. 2. We employ a basic transformer paradigm [21] but design it tailored for capturing global low-rank spectral features. Our spectral encoder block can be formulated by:where Z in ∈ R S×Cspe×H×W indicates the input spectral token tensor, and C spe is the spectral feature dimension. We introduce Depth-wise Conv (DwConv) for dynamically integrating redundant spatial information into spectral features to reduce spatial redundant noises, achieved by setting different strides of the convolutional kernel. Then, we represent long-distance dependencies among spectral inter-bands as a low-rank completion problem. SM D(•) indicates the spectral matrix decomposition operation. Concretely, we flatten feature map X to spectral sequence tokens X spe ∈ R H•W ×S×Cspe , which has S spectral tokens. We map X spe to a feature space using a linear transform W l ∈ R Cspe×C spe . We then apply a matrix decomposition method NMF (Non-negative Matrix Factorization) [10], denoted by M(•), to identify and solve for a low-rank signal subspace and use iterative optimization algorithms backpropagate gradients [4]: SM D(X spe ) = M(W l X spe ). Finally, to enhance the individual component of spectral tokens, we utilize a Feedforward Neural Network (FFN) in Transformer consisting of two linear layers and an activation layer.In the framework shown in Fig. 2, spectral information is integrated from channel dimensions by performing concatenation, after the second and fourth encoder blocks, to aggregate the spatiospectral features. The reason for this design is that spectral features are simpler and lack hierarchical structures compared to spatial features, we will discuss more in the experimental section."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2.2,Low-Rank Decomposition and Skip Connection Ensemble,"The MHSI has low-rank priority due to redundancy, so we propose a lowrank decomposition module using Canonical-Polyadic (CP) decomposition [9] to set constraints on the latent representation. For a three-order tensor U ∈ R C ×H ×W , where H × W is the spatial resolution and C is the channel number. It can be decomposed into a linear combination of N rank-1 tensors. The mathematical formulation of CP decomposition can be expressed asr is the tensor rank and λ i is a scaling factor. Recent research [3,32] has proposed new methods based on DNNs to address this problem of representing MHSIs as low-rank tensors. As shown in Fig. 2, rank-1 generators are used to create rank-1 tensors in different directions, which are then aggregated by Kronecker Product to synthesize a sub-attention map A 1 . The residual part between the input of features and the generated rank-1 tensor is used to generate second rank-1 tensors A 2 . It can obtain r rank-1 tensors by repeating r times. Mathematically, this process can be expressed as:where G c (•), G h (•) and G w (•) are the channel, height and width generators. Finally, we aggregate all rank-1 tensors (from A 1 to A r ) into the attention map along the channel dimension, followed by a linear layer used to reduce the feature dimension to obtain the low-rank feature U low :where is the element-wise product, and U low ∈ R C ×H ×W . We employ a straightforward non-parametric ensemble approach for grouping spectral agents. This approach involves multiple agents combining their features by averaging the vote. The encoders in the spatial stream produce 2D feature maps with G spectral agents, defined as F i ∈ R G×Ci×H/2 i ×W/2 i for the ith encoder, where G, C i , H/2 i , and W/2 i represent the spectral, channel, and two spatial dimensions, respectively. The ensemble is computed byrepresents the 2D feature map of the Gth agent. The ensemble operation aggregates spectral agents to produce a 2D feature map Table 1. Ablation study (in ""mean (std)"") on MDC dataset using RegNetX40 [26] as the backbone. SA denote the spectral agent. L1 to L4 represent the locations where output spectral features from the spectral flow module are inserted into the spatial flow. Tr and Conv mean we replace the SMD module in the spectral stream with self-attention and convolutional blocks. Best results are highlighted. with enhanced information interactions learned from the multi-spectral agents."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,SA Spectral,The feature maps obtained from the ensemble can be decoded using lightweight 2D decoders to generate segmentation masks.3 Experimental Results
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,3.1,Experimental Setup,"We conducted experiments on the public Multi-Dimensional Choledoch (MDC) Dataset [31] with 538 scenes and Hyperspectral Gastric Carcinoma (HGC) Dataset [33] (data provided by the author) with 414 scenes, both with highquality labels for binary MHSI segmentation tasks. These MHSIs are collected by hyperspectral system with an objective lens of 20x, and wavelengths from 550 nm to 1000 nm for MDC and 450 nm to 750 nm for HGC, resulting in 60 and 40 spectral bands for each scene. The size of a single band image in MDC and HGC are both resized to 256 × 320. Following [23,27], we partition the datasets into training, validation, and test sets using a patient-centric hard split approach with a ratio of 3:1:1. Specifically, each patient's data is allocated entirely to one of the three sets, ensuring that the same patient's data do not appear in multiple sets.We use data augmentation techniques such as rotation and flipping, and train with an Adam optimizer using a combination of dice loss and cross-entropy loss for 8 batch size and 100 epochs. The segmentation performance is evaluated using Dice-Sørensen coefficient (DSC), Intersection of Union (IoU), and Hausdorff Distance (HD) metrics, and Throughput (images/s) is reported for comparison. Pytorch framework and four NVIDIA GeForce RTX 3090 are used for implementation."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,3.2,Evaluation of the Proposed Dual-Stream Strategy,"Ablation Study. Our dual-stream strategy is plug-and-play. We first conduct an ablation study to show the effectiveness of each component. We use a dualstream strategy with RegNetX40 and U-Net architecture. As shown in Table 1, Our ablation study shows that spectral agent strategy improves segmentation performance by more than 2.5% (63.11 vs. 66.05). If we utilize spectral information from the spectral stream to assist in the spatial stream, we find that inserting spectral information at L2 and L4 yields a significant improvement of 3.7% (69.73 vs. 66.05), while inserting at L4 alone also results in a significant increase of 1.9% in DSC (67.95 vs. 66.05). A slight improvement is observed when inserting at L2, possibly due to the coarse features of shallow spectral information. Inserting spectral information at all spatial layers (i.e., L1 to L4) and only at L2 and L4 produce similar results, indicating that spectral features do not possess complex multilevel characteristics relative to spatial features. Therefore, we adopt a simple and efficient two-layer spectral flow design. Replacing the spectral stream with transformer layers results in a 0.96% (70.88 vs. 69.89) lower DSC, possibly because transformers are difficult to optimize on small datasets. Our proposed LD module is crucial, resulting in a 1.12% performance drop in terms of DSC without it.It is known that high feature redundancy limits the generalization of neural networks [29]. Here we show our low-rank representation effectively reduces the redundancy of features. Our quantitative and qualitative analysis demonstrated that the proposed MDC and LD modules effectively reduces the redundancy of output features. Following [8], we define the dominant features for the feature embedding of i-th MHSI h i ∈ R C d as L i = j : h ij > μ + σ, where μ is mean of h i and σ is stand deviation of h i . As shown in the left part of Fig. 3, our designed modules effectively reduce the number of dominant features and maintain sparsity in the entire spatiospectral feature space. Inspired by [24], we evaluate the degree of feature redundancy by computing the Pearson correlation coefficient between different feature channels. As shown in the right part of Fig. 3, the  Comparisons with State-of-the-Art MHSI Segmentation Methods. Table 3 shows comparisons on MDC and HGC datasets. We use a lightweight and efficient ResNet34 as the backbone of our dual-stream method. Experimental results show that 2D methods are generally faster than 3D methods in inference speed, but 3D methods have an advantage in segmentation performance (DSC & HD). However, our approach outperforms other methods in both inference speed and segmentation accuracy. It is also plug-and-play, with the potential to achieve better segmentation performance by selecting more powerful backbones. The complete table (including IoU and variance) and qualitative results are shown in the supplementary material."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,4.0,Conclusion,"In this paper, we present to factor space and spectrum for accurate and fast medical hyperspectral image segmentation. Our dual-stream strategy, leveraging low-rank prior of MHSIs, is computationally efficient and plug-and-play, which can be easily plugged into any 2D architecture. We evaluate our approach on two MHSI datasets. Experiments show significant performance improvements on different evaluation metrics, e.g., with our proposed strategy, we can obtain over 7.7% improvement in DSC compared with its 2D counterpart. After plugging our strategy into ResNet-34 backbone, we can achieve state-of-the-art MHSI segmentation accuracy with 3-13 times faster in terms of inference speed than existing 3D networks."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_15.
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,1.0,Introduction,"Charting brain development during the first postnatal year is crucial for identifying typical and atypical changes of brain tissues, i.e., grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF), which can be utilized for diagnosis of autism and other brain disorders [16]. Early identification of autism is very important for effective intervention, and currently, an accurate diagnosis can be achieved as early as 12 months old [21]. Advancing diagnosis at an earlier stage could provide more time for greatly improving intervention [17]. Unfortunately, it remains challenging in obtaining a precise tissue map from the early infancy structural MRI (sMRI) data, such as T1 or (and) T2.The acquisition of a mass of tissue maps relies on automatic segmentation techniques. However, the accuracy cannot be guaranteed, and the most difficult case is the segmentation of the isointense phase (6-9 months) data [18], as shown in Fig. 1, where the intensity distributions of GM and WM are highly overlapped compared with the adult-like phase (≥ 9 months) [18] (clear boundaries between GM and WM). Such a peculiarity incurs two challenges to conventional segmentation methods: i ) lack of well-annotated data for training automatic segmentation algorithms; and ii ) GM and WM demonstrate a low contrast result in limited anatomical information for accurately distinguishing GM and WM, even with enough high-quality annotations as training data.Conventional methods can mainly be classified into two categories: 1) the registration-based methods and 2) the learning-based methods, as introduced below. The registration-based methods usually utilize a single previous-defined atlas, for cross-sectional data, or a sequence of atlases, for longitudinal dataguided methods, to indirectly obtain the tissue maps [13,14,19,20]. Those methods require a substantial number of atlases and follow-up adult-like images to guide the segmentation process, which is known for low accuracy in segmenting infant tissues due to the rapid developmental changes in early life. The learning-based methods have gained significant prominence for individualized segmentation [8,22], which is also exploited to segment isointense brain tissues. For example, Nie et al. [9] used fully convolutional networks (FCNs) to directly segment tissue from MR images, and Wang et al. [23] employed an attention mechanism for better isointense infant brain segmentation. Also, Bui et al. proposed a 3D CycleGAN [2] to utilize the isointense data to synthesize adult-like data, which can be employed to make the segmentation of isointense data more accurate.In this work, we propose a novel Transformer-based framework for isointense tissue segmentation via T1-weighted MR images, which is composed of two stages: i.e., i ) the semantics-preserved GAN (SPGAN), and ii ) Transformerbased multi-scale segmentation network (TMSN). Specifically, SPGAN is a bidirectional synthesis model that enables both the isointense data synthesis using adult-like data and vice verse. The isointense structural MRI data is paired with the segmented tissue maps for extending the training dataset. Additionally, the synthesized adult-like data from isointense infant data are adopted to assist segmentation in TMSN. TMSN incorporates a Transformer-based cross-branch fusing (TCF) module which exploits supplementary tissue information from a patch with a larger receptive field to guide the local segmentation. Extensive experiments are conducted on the public dataset, National Database for Autism Research (NDAR) [5,12], and the results demonstrate that our proposed framework outperforms other state-of-the-art methods, particularly in accurately segmenting ambiguous tissue boundaries."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2.0,Methods,"We propose a Transformer-based framework for isointense tissue segmentation, which includes two main modules: i ) the semantic-preserved GAN (SPGAN), and ii ) the Transformer-based multi-scale segmentation network (TMSN). An overview of the framework is provided in Fig. 2, SPGAN is designed to synthesize isointense or adult-like data to augment the training dataset and guide segmentation with anatomical information (Sect. 2.1). TMSN is designed for isointense tissue segmentation by utilizing the tissue information from the synthesized adult-like data (Sect. 2.2)."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2.1,Semantics-Preserved Multi-phase Synthesis,"Motivated by the effectiveness of classical unpaired image-to-image translation models [24,25], we propose a novel bi-directional generative model that synthesizes both isointense and adult-like data while introducing essential anatomical constraints to maintain structural consistency between the input images and the corresponding synthesized images. As shown in Fig. 2 (b.1), the SPGAN consists of two generators, G A2I and G I2A , along with their corresponding discriminators D A2I and D I2A . The adult-like infant images are aligned to the isointense infant images using affine registration methods [1], before training SPGAN. Isointense Phase Synthesis. The generator G A2I is designed to synthesize isointense data from adult-like ones, which is constructed based on the 3D UNet [3] model, consisting of 4 encoder layers and 4 decoder layers. Inspired by GauGAN [10], we incorporate the spatial adaptive denormalization (SPADE) into G A2I to maintain structural consistency between the synthesized isointense brain images and corresponding adult-like brain images, as shown in Fig. 2 (b.2). SPADE module takes the adult-like tissue maps to perform feature normalization during synthesis stage of isointense phase data. Thus, the generated isointense brain images can obtain consistent tissue structures with adult-like brain images. Specifically, X in with a spatial resolution of (H × W × D) represents the input feature map to each layer, consisting of a batch of N samples and C channels. For an arbitrary n-th sample at the c-th channel, the activation value associated with location (i,j,k ) is denoted as x n,c,i,j,k . The SPADE operator adopts two convolution blocks to learn the modulation maps α(M ) and β(M ) for a given tissue map M , where the first convolution block makes the number of channels in the tissue map consistent with the number of channels in the feature map. The modulated value at (i,j,k ) in each feature map is formulated as:where μ c and σ c denote the mean and standard deviation, respectively. Additionally, we adopt PatchGAN [25] as the discriminator D A2I to provide adversary loss to train the generator G A2I .Adult-Like Phase Synthesis. The generator G I2A is designed to synthesize adult-like infant brain images from isointense infant brain images, which is employed to provide clear structural information for identifying the ambiguous tissue boundaries in isointense brain images. To ensure the synthesized adult-like infant brain images can provide tissue information as realistic and accurate as the real images, we utilize a pre-trained adult-like brain tissue segmentation network S A (3D UNet) to preserve the structural similarity between the synthesized adult-like data and the real adult-like data and promote more reasonable anatomical structures in the synthesized images. To achieve that goal, during the training of SPGAN, we freeze the parameters of S A and adopt the mean square error (MSE) to penalize the dissimilarity between the tissue probability maps of the real and synthesized brain images (extracted by the pre-trained segmentation model S A ). The MSE loss is formulated aswhere I RAB and I CAB denote the real adult-like brain images and synthesized adult-like brain images synthesized by G I2A . The overall loss function of SPGAN is defined as:where L cycle denotes the cycle consistency loss between two generators."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2.2,Transformer-Based Multi-scale Segmentation,"The overview of our proposed segmentation network (TMSN) is shown in Fig. 2 (c). In order to guide the network with anatomical prior, TMSN takes the pair of isointense and the corresponding adult-like images as input. The isointense and adult-like images are concatenated and cropped to image patches and then, they are fed into the top branch of TMSN. Due to the fact that neighboring tissues can provide additional information and greatly improve segmentation performance, hence, we also employ a multi-scale strategy by taking an image pair of the larger receptive field as the input of the bottom branch. It is important to note that the image pairs of the top and bottom branches should have the same sizes. Moreover, we fuse the features of the two branches at the same block using a novel Transformer-based cross-branch fusion (TCF) module, where the Transformer can better capture relationships across two different branches (with local and global features). Specifically, F m s and F m b denote the learned features of the two branches at the m-th stage, respectively. The TCF module treats the F m s and F m b as the input of the encoder and decoder in the multi-head Transformer to learn the relationships between the centroid tissues in the top branch and the corresponding surrounding tissues in the bottom branch and then fuses the bottom branch features with the ones of the top branch by utilizing the learned relationships. The proposed TCF module can be formulated as:where Q, K, and V denote the query, key, and value in the Transformer. We take a hybrid Dice and focal loss to supervise the two segmentation branches as follows:where I s and GT s , respectively, denote the input and ground truth (GT) of the top branch, and I b and GT b represent the ones of the bottom branch. The final tissue segmentation results are obtained from the top branch."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.1,Dataset and Evaluation Metrics,"We evaluated our proposed framework for isointense infant brain tissue segmentation on the public dataset NDAR [5,12]. The NDAR dataset comprises T1weighted brain images of 331 cases at the isointense phase and 368 cases at the adult-like phase (12-month-old), where 180 cases contain both time points. The GT annotations for GM, WM, and CSF were manually labeled by experienced radiologists. The data was aligned to MNI space and normalized to standard distribution using z-score normalization with dimensions of 182, 218, and 182 in the x, y, and z axis, respectively. The dataset was randomly divided into three subsets, i.e., 70% for training, 10% for validation, and 20% for testing. For quantitative evaluation, we employed three assessment metrics including Dice score, Hausdorff distance (HD), and average surface distance (ASD) [15]."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.2,Implementation Details,Our proposed framework is implemented based on PyTorch 1.7.1 [11] and trained on a workstation equipped with two NVIDIA V100s GPUs. We employ the Adam optimizer [7] with momentum of 0.99 and weight decay of 0.001. The learning 
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.3,Evaluation and Discussion,"We conduct extensive experiments to evaluate the effectiveness of our infant brain tissue segmentation framework. Table 1 summarizes the results of the ablation study and demonstrates the contribution of each component in our framework. Particularly, SegNet represents the baseline model using a single scale, based on which, SegNetMS involves an additional branch that captures a larger receptive field and employs channel concatenation to fuse the features of these two branches. SegNetMSAtt replaces feature concatenation with the Transformer-based TCF module. Each of the above three configurations contains four different settings, as listed in Table 1. The results demonstrate the benefits of using both isointense phase data augmentation and adult-like phase structural enhancement."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Effectiveness of Multi-phase Synthesis.,"In Table 1, we demonstrate the effectiveness of the synthesis of isointense and adult-like phase images by the proposed synthesis network SPGAN. When comparing with the methods based on limited samples and augmented samples, we witness that data augmentation (DA) effectively improves segmentation performance. Furthermore, by comparing with the methods with or without structural enhancement (SE), we observe that the generated adult-like infant brain images provide richer and more reliable structural information to guide the identification of ambiguous tissue boundaries at the isointense phase. When combining DA and SE, an improvement of the segmentation performance on all evaluating metrics emerge, which demonstrates that the DA and SE can be effectively integrated to bring significant benefits for improving tissue segmentation performance.Effectiveness of Multi-scale Assistance. In order to demonstrate the effectiveness of the additional branch in TMSN, which contains a larger receptive field, we show the corresponding results as one part of the ablation study in Table 1. Comparing the multi-scale SegNetMS with the single-scale variant Seg-Net, we can see that SegNetMS improves the Dice score from 94.32% to 96.01% and achieves a decrease in HD from 7.95 mm to 6.99 mm on average. This indicates that the multi-scale strategy enables the segmentation model to make use of the intrinsic relations between the centroid tissues and the corresponding counterparts with a larger receptive field, which leads to great improvement in the segmentation performance. In our proposed model SegNetMSAtt, we replace the channel concatenation in SegNetMS with TCF. We can find that the TCF module can more effectively build up the correspondence between top and bottom branches such that the surrounding tissues can well guide the segmentation.Comparison with State-of-the-Arts (SOTAs). We conduct a comparative evaluation of our framework against several SOTA learning-based tissue segmentation networks, including 1) the 3D UNet segmentation network [3], 2) the DualAtt network [4] utilizing both spatial and channel attention, and 3) the SwinUNETR network [6] employing a 3D Swin Transformer. From the results listed in Table 2, we can find that our framework achieves the highest average Dice score and lower HD and ASD compared with the SOTA methods.To further illustrate the advanced performance of our framework, we provide a visual comparison of two typical cases in Fig. 3. It can be seen that the predicted tissue maps (the last column) obtained by our method are much closer to the GT (the second column), especially in the subcortical regions. We can draw the same conclusion from the error maps (the second and the fourth rows in Fig. 3) between the segmented tissue maps obtained by each method and GT. Both the quantitative improvement and the superiority from the qualitative results performance consistently show the effectiveness of our proposed segmentation framework, as well as the two main components, i.e., SPGAN, and TMSN."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,4.0,Conclusion,"In this study, we have presented a novel Transformer-based framework to segment tissues from isointense T1-weighted MR images. We designed two modules, i.e., i ) semantic-preserved GAN (SPGAN), and ii ) Transformer-based multiscale segmentation network (TMSN). SPGAN is designed to synthesize both isointense and adult-like data, which augments the dataset and provides supplementary tissue constraint for assisting isointense tissue segmentation. TMSN is used to segment tissues from isointense data under the guidance of the synthesized adult-like data. Their advantages are distinctive. For example, SPGAN overcomes the lack of training samples and synthesizes adult-like infant brain images with clear structural information for enhancing ambiguous tissue boundaries. TMSN exploits the pair of isointense and adult-like phase images, as well as the multi-scale scheme, to provide more information for achieving accurate segmentation performance. Extensive experiments demonstrate that our proposed framework outperforms the SOTA approaches, which shows the potential in early brain development or abnormal brain development studies."
