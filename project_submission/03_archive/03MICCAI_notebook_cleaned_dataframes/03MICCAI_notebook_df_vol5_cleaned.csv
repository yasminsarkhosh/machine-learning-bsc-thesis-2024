Paper Title,Header Number,Header Title,Text
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"The periodic acquisition and analysis of volumetric CT and MRI scans of oncology patients is essential for the evaluation of the disease status, the selection of the treatment, and the response to treatment. Currently, scans are acquired every 2-12 months according to the patient's characteristics, disease stage, and treatment regime. The scan interpretation consists of identifying lesions (primary tumors, metastases) in the affected organs and characterizing their changes over time. Lesion changes include changes in the size of existing lesions, the appearance of new lesions, the disappearance of existing lesions, and complex lesion changes, e.g., the formation of conglomerate lesions. As treatments improve and patients live longer, the number of scans in longitudinal studies increases and their interpretation is more challenging and time-consuming.Radiological follow-up requires the quantitative analysis of lesions and patterns of lesion changes in subsequent scans. It differs from diagnostic reading since the goal is to find and quantify the differences between the scans, rather than to find abnormalities in a single scan. In current practice, quantification of lesion changes is partial and approximate. The RECIST 1.1 guidelines call for finding new lesions (if any), identifying up to the five largest lesions in each scan in the CT slice where they appear largest, manually measuring their diameters, and comparing their difference [1]. While volumetric measures of individual lesions and of all lesions (tumor burden) have long been established as more accurate and reliable than partial linear measurements, they are not used clinically because they require manual lesion delineation and lesion matching in unregistered scans, which is usually time-consuming and subject to variability [2].In a previous paper, we presented an automatic pipeline for the detection and quantification of lesion changes in pairs of CT liver scans [3]. This paper describes a graph-based lesion tracking method for the comprehensive analysis of lesion changes and their patterns at the lesion level. The tasks are formalized as graph-theoretic problems (Fig. 1). Complex lesion changes include merged lesions, which occurs when at least two lesions grow and merge into one (possible disease progression), split lesions, which occurs when a lesion shrinks and cleaves into several parts (possible response to treatment) and conglomeration of lesions, which occurs when clusters of lesions coalesce. While some of these lesion changes have been observed [4], they have been poorly studied. Comprehensive quantitative analysis of lesion changes and patterns is of clinical importance, since response to treatment may vary among lesions, so the analysis of a few lesions may not be representative.The novelties of this paper are: 1) identification and formalization of longitudinal lesion matching and patterns of lesion changes in CT in a graph-theoretic framework; 2) new classification and detection of changes of individual lesions and lesion patterns based on the properties of the lesion changes graph and its connected components; 3) a simultaneous lesion matching method with more than two scans; 4) graph-based methods for the detection of changes in individual lesions and patterns of lesion changes. Experimental results on lung (83 CTs, 19 patients) and liver (77 CECTs, 18 patients) datasets show that our method yields high classification accuracy.To the best of our knowledge, ours is the first method to perform longitudinal lesion matching and lesion changes pattern detection. Only a few papers address lesion matching in pairs of CT/MRI scans [5][6][7][8][9][10][11][12][13] -none performs simultaneous matching of all lesions in more than two scans. Also, very few methods [3,14] handle matching of split/merged lesions. Although many methods exist for object tracking in optical images and videos [15][16][17], they are unsuited for analyzing lesion changes since they assume many consecutive 2D images where objects have very similar appearance and undergo small changes between images. Overlap-based methods pair two lesions in registered scans when their segmentations overlap, with a reported accuracy of 66-98% [3,[5][6][7][8][9][10][11]18]. These methods assume that organs and lesions undergo minor changes, are very sensitive to registration errors, and cannot handle complex lesion changes. Similarity-based methods pair two lesions with similar features, e.g., intensity, shape, location [13][14][15][16] with an 84-96% accuracy on the DeepLesion dataset [14]. They are susceptible to major changes in the lesion appearance and do not handle complex lesion changes. Split-andmerge matching methods are used for cell tracking in fluorescence microscopy [19]. They are limited to 2D images, assume registration between images, and do not handle conglomerate changes. "
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.0,Method,"We present a new generic model-based method for the automatic detection and classification of changes in individual lesions and patterns of lesion changes in consecutive CT scans. The tasks are formalized in a graph-theoretic framework in which nodes represent lesions, edges represent lesion matchings, and paths and connected components represent patterns of lesion changes. Lesion matchings are computed with an overlap-based lesion pairing method after establishing a common reference frame by deformable registration of the scans and organ segmentations. Changes in individual lesions and patterns of lesion changes are computed from the graph's properties and its connected components. We define seven individual lesion change classes and five lesion change patterns that fully describe the evolution of lesions over time.The method inputs the scans and the organ and lesion segmentations in each scan. Its outputs are the lesion matchings, the labels of the changes in individual lesions, and the patterns of the lesion changes. The method is a pipeline of four steps: 1) pairwise deformable registration of each prior scan, organ and lesion segmentations, with the most recent (current) scan as in [3]; 2) overlap-based lesion matching; 3) construction of the lesion change graph from the individual lesion segmentations and lesion matches; 4) detection of changes in individual lesions and patterns of lesion changes from the graph properties and from analysis of its connected components."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"Let S = S 1 , . . . , S N be a series of N ≥ 2 consecutive patient scans acquired at timesis a set of vertices v i j corresponding to the lesions associated with the lesion segmentation masks L i = l i 1 , l i 2 , . . . , l i n i , where n i ≥ 0 is the number of lesions in scan S i at time t i . By definition, any two lesionsj,l indicates that the lesions corresponding to vertices v i j , v k l are the same lesion, i.e., that the lesion appears in scans S i , S k in the same location. Edges of consecutive scans S i , S i+1 are called consecutive edges; edges of non-consecutive scans, S i , S k , i < k -1, are called non-consecutive edges. The in-and out-degree of a vertex v i j , d in (v i j ) and d out (v i j ), are the number of incoming and outcoming edges, respectively.Let CC = {cc m } M m=1 be the set of connected components of the undirected graph version of G, where M is the number of connected components andBy definition, for each 1 ≤ m ≤ M , the sets V m , E m are mutually disjoint and their unions are V , E, respectively. In a connected component cc m , there is an undirected path between any two vertices v i j , v k l consisting of a sequence of undirected edges in E m . . In this setup, connected components correspond to matched lesions and their pattern of evolution over time (Fig. 1d).We define seven mutually exclusive individual lesion change labels for lesion v i j in scan S i based on the vertex in-and out-degrees (Fig. 2). In the following definitions we refer to the indices: 1 ≤ k < i < l ≤ N ; 1) Lone: a lesion present in scan S i and absent in all previous scans S k and subsequent scans S l ; 2) New: a lesion present in scan S i and absent in all previous scans S k ; 3) Disappeared: a lesion present in scan S i and absent in all subsequent scans S l ; 4) Unique: a lesion present in scan S i and present as a single lesion in a previous scan S k and/or in a subsequent scan S l ; 5) Merged: a lesion present in scan S i and present as two or more lesions in a previous scan S k ; 6) Split: a lesion present in scan S i and present as two or more lesions in a subsequent scan S l ; 7) Complex: a lesion present as two or more lesions in at least one previous scan S k and at least one subsequent scan S l . We also define as Existing a lesion present in scan S i and present in at least one previous scan S k and one subsequent scan S l , (d in (v i j ) ≥ 1, d out (v i j ) ≥ 1). For the first and current scans S 1 and S N , we set d in (v 1 j ) = 1, d out (v N j ) = 1, i.e., the lesion existed before the first scan or remains after the last scan. Thus, lesions in the first (last) scan can only be Unique, Disappeared or Split (Unique, New or Merged). Finally, when lesion v i j is Merged and d out (v i j ) = 0, i < N , it is also labeled Disappeared; when it is Split and d in (v i j ) = 0, i > 1, it is also labeled New. We define five patterns of lesion changes based on the properties of the connected components cc m of G and on the labels of lesion changes: 1) Single_P: a connected component cc m = v i j consisting of a single lesion labeled as Lone, New, Disappeared; 2) Linear_P: a connected component consisting of a single earliest vertex v  The changes in individual lesions and the detection and classification of patterns of lesion changes consist of constructing a graph whose vertices are the corresponding lesion in the scans, computing the graph consecutive and non-consecutive edges that correspond to lesion matchings, computing the connected components of the resulting graph, and assigning an individual lesion change label to each vertex and a lesion change pattern label to each connected component according to the categories above."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.3,Classification of Changes in Lesions and in Patterns of Lesion Changes,"The changes in individual lesions are directly computed for each lesion from the resulting graph with the in-and out-degree of each vertex (Fig. 2a). The connected components CC = {cc m } of G are computed by graph Depth First Search (DFS). The patterns of lesion changes (Fig. 2b) are computed with path and tree graph algorithms.The changes in individual lesions, patterns of lesion changes, and lesion changes graph serve as the basis for individual lesion tracking, which consists of following the path from the lesion in the most recent scan backwards to its origins in earlier scans and recording the merged, split and complex lesion changes labels. Summarizing longitudinal studies and queries can also be performed with graph-based algorithms."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"We evaluated our method with two studies on retrospectively collected patient datasets that were manually annotated by an expert radiologist.Dataset: Lung and liver CT studies were retrospectively obtained from two medical centers (Hadassah Univ Hosp Jerusalem Israel) during the routine clinical examination of patients with metastatic disease. Each patient study consists of at least 3 scans.DLUNG consists of 83 chest CT scans from 19 patients with a mean 4.4 ± 2.0 scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3 days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . DLIVER consists of 77 abdominal CECT scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .Lesions in both datasets were annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with <20 voxels were excluded). Ground-truth lesion matching graphs and lesion changes labeling were produced by running the method on the datasets and then having the radiologist review and correct the resulting node labels and edges.Study 1: Lesion changes labeling, lesion matching, evaluation of patterns of lesion changes. We ran our method on the DLUNGS and DLIVER lesion segmentations. The settings of the parameters were: dilation distance d = 1 mm, overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid maximum distance δ = 17 and 23 mm for the lungs and liver lesions, respectively.We compared the computed and ground-truth lesion changes graphs with two metrics: 1) lesion changes classification accuracy, which is the % of correct computed labels from the ground truth labels; 2) lesion matching precision and recall based on the presence/absence of computed vs. ground truth edges. The precision and recall definitions were adapted so that wrong or missed non-consecutive edges are counted as True Positive when there is a path between their vertices in either the ground-truth or the computed graph. Table 1 summarizes the results. The distribution of lesion changes labels for DLUNGS (1,178 lesions) is Unique 785 (67%), New 215 (18%), Lone 109 (9%), Disappeared 51 (4%), Merged 12 (1%), Split 6 (1%), Complex 0 (0%) with class accuracy ≥ 96% for all except Split (66%). For DLIVER (800 lesions) it is Unique 450 (56%), New 185 (23%), Lone 45 (6%), Disappeared 77 (10%), Merged 27 (3%), Split 18 (2%), Complex 1 (0.05%) with class accuracy ≥ 81% for all except Disappeared (71%) and Split (67%).For the patterns of lesion changes, we compared the computed and ground truth patterns of lesion changes. The accuracy is the % of identical connected components in each category. Table 1 summarizes the results. Note that the Split_P, Merged_P and Complex_P patterns jointly account for 3% and 8% of the cases. These patterns are hard to detect manually but their correct classification and tracking are crucial for the proper application of the RECIST 1.1 follow-up protocol [1]. Study 2: Detection of missed lesions in the ground truth. The expert radiologist was asked to examine non-consecutive edges and lesions labeled as Lone in the lesion changes graph and determine if lesions were unseen or undetected (actual or presumed false negative) in the skipped or contiguous scans (Fig. 1d). For each non-consecutive edge connecting lesions v i j , v k l , he analyzed the corresponding region in the skipped scans S j at t j ∈ ]t i , t k [ for possible missed lesions. For the DLUNGS dataset, 25 visible and 5 faintly visible or surmised to be present unmarked lesions were found for 27 nonconsecutive edges. For the DLIVER dataset, 20 visible and 21 faintly visible or surmised to be present unmarked lesions were found for 25 non-consecutive edges.After reviewing the 42 and 37 lesions labeled as Lone in DLUNGS and DLIVER with > 5mm diameter, the radiologist determined that 1 and 8 of them had been wrongly identified as a cancerous lesion. Moreover, he found that 14 and 16 lesions initially labeled as Lone, had been wrongly classified: for these lesions he found 15 and 21 previously unmarked matching lesions in the next or previous scans. In total, 45 and 62 missing lesions were added to the ground truth DLUNGS and DLIVER datasets, respectively. These hard-to-find ground-truth False Negatives (3.7%, 7.2% of all lesions) may change the radiological interpretation and the disease status. See the Supplemental Material for examples of these scenarios."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"The use of graph-based methods for lesion tracking and detection of patterns of lesion changes was shown to achieve high accuracy in classifying changes in individual lesion and identifying patterns of lesion changes in liver and lung longitudinal CT studies of patients with metastatic disease. This approach has proven to be useful in detecting missed, faint, and surmised to be present lesions, otherwise hardly detectable by examining the scans separately or in pairs, leveraging the added information provided by evaluating all patient's scans simultaneously using the labels from the lesion changes graph and non-consecutive edges."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.2,Lesion Matching Computation,"Lesion matchings are determined by the location and relative proximity of the lesions in two or more registered scans. The lesion matching rule is lesion voxel overlap: when the lesion segmentation voxels l i j , l k l of vertices v i j , v k l overlap, 1 ≤ i < k ≤ N , they are matched and the edge e i,k j,l = v i j , v k l is added to E. Lesion matchings are computed first on consecutive pairs and then on non-consecutive pairs of scans.Consecutive lesion matching on scans (S i , S i+1 ) is performed with an iterative greedy strategy whose aim is to compensate for registration errors: 1) the lesion segmentations in L i and L i+1 are isotropically dilated in 3D by d millimeters; 2) for all pairs of lesions v i j , v i+1 l , compute the intersection % of their corresponding lesion a segmentationsl is added to E c ; ; 4) remove the lesion segmentations l i j , l i+1 l from L i , L i+1 , respectively. Steps 1-4 are repeated r times. This yields the consecutive edges graph). The values of d , r are pre-defined empirically. Lesion matching on non-consecutive scans searches for lesion matchings that were not found previously due to missing lesions (unseen or undetected). It is performed by examining the pairs of connected components of G C and finding possible edges (lesion pairings) between them. Formally, let CC = {cc m } M m=1 be the set of undirected connected components of G C . Let τ m = t first m , t last m be the time interval between the first and last scans of cc m , and let centroid (cc m ) be the center of mass of all lesions in cc m at all times. Let G CC = (V cc , E cc ) be the graph of connected components of G C such that each vertex cc m of V cc corresponds to a connected component cc m and edges e CC i,j = cc i , cc j of E CC satisfy three conditions: 1) the time interval of cc i is disjoint and precedes by at least one time point that of cc j , i.e., t first i > t last i + 1; 2) the connected components are not too far from each other, i.e., the distance between the connected components centroids is smaller than a fixed distance δ, centroid (cc i )centroid cc j ≤ δ; 3) there is at least one pairwise matching between a lesion in t last i and a lesion in t first j computed with the consecutive lesion matching method described above. When the consecutive lesion matching between the lesions in t last i in cc i and the lesions in t first j in cc j yields a nonempty set of edges, these edges are added as non-consecutive edges to E c . . Iterating over all non-ordered pairs (cc i , cc j ) yields the set of consecutive and non-consecutive edges of E.We illustrate this process with the graph of Fig. 1. First, the consecutive edges (straight lines) of the graph are computed by consecutive lesion matching. This yields a graph with four connected components: "
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"Neuropsychiatric systemic lupus erythematosus (NPSLE) refers to a complex autoimmune disease that damages the brain nervous system of patients. The clinical symptoms of NPSLE include cognitive disorder, epilepsy, mental illness, etc., and patients with NPSLE have a nine-fold increased mortality compared to the general population [11]. Since the pathogenesis and mature treatment of NPSLE have not yet been found, it is extremely important to detect NPSLE at its early stage and put better clinical interventions and treatments to prevent its progression. However, the high overlap of clinical symptoms with other psychiatric disorders and the absence of early non-invasive biomarkers make accurate diagnosis difficult and time-consuming [3].Although conventional magnetic resonance imaging (MRI) tools are widely used to detect brain injuries and neuronal lesions, around 50% of patients with NPSLE present no brain abnormalities in structural MRI [17]. In fact, metabolic changes in many brain diseases precede pathomorphological changes, which indicates proton magnetic resonance spectroscopy ( 1 H-MRS) to be a more effective way to reflect the early appearance of NPSLE. 1 H-MRS is a non-invasive neuroimaging technology that can quantitatively analyze the concentration of metabolites and detect abnormal metabolism of the nervous system to reveal brain lesions. However, the complex noise caused by overlapping metabolite peaks, incomplete information on background components, and low signal-tonoise ratio (SNR) disturb the analysis results of this spectroscopic method [15]. Meanwhile, the individual differences in metabolism and the interaction between metabolites under low sample size make it difficult for traditional learning methods to distinguish NPSLE. Figure 1 shows spectra images of four participants including healthy controls (HC) and patients with NPSLE. It can be seen that the visual differences between patients with NPSLE and HCs in the spectra of the volumes are subtle. Therefore, it is crucial to develop effective learning algorithms to discover metabolic biomarkers and accurately diagnose NPSLE. The machine learning application for biomarker analysis and early diagnosis of NPSLE is at a nascent stage [4]. Most studies focus on the analysis of MR images using statistical or machine learning algorithms, such as Mann-Whitney U test [8], support vector machine (SVM) [7,24], ensemble model [16,22], etc. Generally, Machine learning algorithms based on the minimum mean square error (MMSE) criterion heavily rely on the assumption that noise is of Gaussian distribution. However, measurement-induced non-Gaussian noise in 1 H-MRS data undoubtedly limits the performance of MMSE-based machine learning methods.On the other hand, for the discovery task of potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm, etc.) force row elements to zero that remove some valuable features [12,21]. More importantly, different brain regions have different functions and metabolite concentrations, which implies that the metabolic features for each brain region have different sparsity levels. Therefore, applying the same sparsity constraint to the metabolic features of all brain regions may not contribute to the improvement of the diagnostic performance of NPSLE.In light of this, we propose a robust exclusive adaptive sparse feature selection (REASFS) algorithm to jointly address the aforementioned problems in biomarker discovery and early diagnosis of NPSLE. Specifically, we first extend our feature learning through generalized correntropic loss to handle data with complex non-Gaussian noise and outliers. We also present the mathematical analysis of the adaptive weighting mechanism of generalized correntropy. Then, we propose a novel regularization called generalized correntropy-induced exclusive 2,1 to adaptively accommodate various sparsity levels and preserve informative features. The experimental results on a benchmark NPSLE dataset demonstrate the proposed method outperforms comparing methods in terms of early noninvasive biomarker discovery and early diagnosis."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"Dataset and Preprocessing: The T2-weighted MR images of 39 participants including 23 patients with NPSLE and 16 HCs were gathered from our affiliated hospital. All images were acquired at an average age of 30.6 years on a SIGNA 3.0T scanner with an eight-channel standard head coil. Then, the MR images were transformed into spectroscopy by multi-voxel 1 H-MRS based on a point-resolved spectral sequence (PRESS) with a two-dimensional multi-voxel technique. The collected spectroscopy data were preprocessed by a SAGE software package to correct the phase and frequency. An LCModel software was used to fit the spectra, correct the baseline, relaxation, and partial-volume effects, and quantify the concentration of metabolites. Finally, we used the absolute NAA concentration in single-voxel MRS as the standard to gain the absolute concentration of metabolites, and the NAA concentration of the corresponding voxel of multi-voxel 1 H-MRS was collected consistently. The spectra would be accepted if the SNR is greater than or equal to 10 and the metabolite concentration with standard deviations (SD) is less than or equal to 20%. The absolute metabolic concentrations, the corresponding ratio, and the linear combination of the spectra were extracted from different brain regions: RPCG, LPCG, RDT, LDT, RLN, LLN, RI, RPWM, and LPWM. A total of 117 metabolic features were extracted, and each brain region contained 13 metabolic features: Cr, phosphocreatine (PCr), Cr+PCr, NAA, NAAG, NAA+NAAG, NAA+NAAG/Cr+PCr, mI, mI/Cr+PCr, Cho+phosphocholine (PCh), Cho+PCh/Cr+PCr, Glu+Gln, and Glu+Gln/Cr+PCr."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.1,Sparse Coding Framework,"Given a data matrix X = [x 1 ; • • • ; x n ] ∈ R n×d with n sample, the i-th row is represented by x i , and the corresponding label matrix is denoted aswhere y i is one-hot vector. The Frobenius norm ofFor sparse codingbased methods, the general problem can be formulated aswhere L(W), R(W), and λ are the loss function, the regularization term, and the hyperparameter, respectively. For least absolute shrinkage and selection operator (LASSO) [20],For multi-task feature learning [6], 2,1 norm is the most widely used regularization to select classshared features via row sparsity, which is defined as. Due to the row sparsity of 2,1 norm, the features selected for all different classes are enforced to be exactly the same. Thus, the inflexibility of 2,1 norm may lead to the deletion of meaningful features."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.2,Generalized Correntropic Loss,"Originating from information theoretic learning (ITL), the correntropy [2] is a local similarity measure between two random variables A and B, given bywhere E, k σ (•, •), and F AB (a, b) denote the mathematical expectation, the Gaussian kernel, and the joint probability density function of (A, B), respectively. When applying correntropy to the error criterion, the boundedness of the Gaussian kernel limits the disturbance of large errors caused by outliers on estimated parameters. However, the kernelized second-order statistic of correntropy is not suitable for all situations. Therefore, the generalized correntropy [1], a more flexible and powerful form of correntropy, was proposed by substituting the generalized Gaussian density (GGD) function for the Gaussian kernel in correntropy, and the GGD function is defined aswhere Γ (•) is the gamma function, α, β > 0 are the shape and bandwidth parameters, respectively, s = 1/β α is the kernel parameter, γ = α/(2βΓ (1/α)) is the normalization factor. Specifically, when α = 2 and α = 1, GGD degenerates to Gaussian distribution and Laplacian distribution, respectively. As an adaptive similarity measure, generalized correntropy can be applied to machine learning and adaptive systems [14]. The generalized correntropic loss function between A and B can be defined asTo analyze the adaptive weighting mechanism of generalized correntropy, we consider an alternative problem of (1), whereThe optimal projection matrix W should satisfy (∂J(W)/∂W) = 0, and we havewhere Λ is a diagonal matrix with error-based diagonal elementsfor adaptive sample weight."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.3,"Generalized Correntropy-Induced Exclusive 2,1","To overcome the drawback of 2,1 norm and achieve adaptive sparsity regularization on metabolic features of different brain regions, we propose a novel GCIE 2,1 . A flexible feature learning algorithm exclusive 2,1 [9] is defined aswhereBased on the exclusive 2,1 , we can not only removes the redundant features shared by all classes through row sparsity of 2,1 norm but also selects different discriminative features for each class through exclusive sparsity of 1,2 norm. Then we propose to introduce generalized correntropy to measure the sparsity penalty in the feature learning algorithm. We apply generalized correntropy to the row vector w i of W to achieve the adaptive weighted sparse constraint, and the problem (6) can be rewritten aswhereSince minimizing w i 2 is equivalent to maximizing exp(-s w i 2 ), we add a negative sign to this term. Through the GCIE 2,1 regularization term in (7), each feature is expected to be enforced with a sparsity constraint of a different weight according to its sparsity level of metabolic information in different brain regions.Optimization: Since the GCIE 2,1 is a non-smooth regularization term, the final problem (7) can be optimized by the stochastic gradient method with appropriate initialization of W. To this end, We use the closed-form solution (5) to initialize W reasonably, and the error-driven sample weight has reached the optimum based on half-quadratic analysis [13]. Once we obtained the solution to the problem (7), the importance of feature i is proportional to w i 2 . We then rank the importance of features according to ( w i 2 , • • • , w d 2 ), and select the top m ranked features from the sorted order for further classification."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"Experimental Settings: The parameters α and λ 1 are are set to 1, while β and λ 2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. We use Adam optimizer and the learning rate is 0.001. To evaluate the performance of classification, we employ a support vector machine as the basic classifier, where the kernel is set as the radial basis function (RBF) and parameter C is set to 1. We average the 3-fold cross-validation results.Results and Discussion: We compare the classification accuracy of the proposed REASFS with several SOTA baselines, including two filter methods: maximal information coefficient (MIC) [5], Gini [23], and four sparse coding-based methods: multi-task feature learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm [21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. The proposed REASFS is expected to have better robustness and flexibility. It can be seen from Fig. 2 that the sparse coding-based methods achieve better performance than filter methods under most conditions, where ""0%"" represents no noise contamination. The highest accuracy of our REASFS demonstrates the effectiveness and flexibility of the proposed GCIE 2,1 . Generally speaking, the probability of samples being contaminated by random noise is equal. Therefore, we randomly select features from the training set and replace the selected features with pulse noise. The number of noisy attributes is denoted by the ratio between the numbers of selected features and total features, such as 15% and 30%. The classification performances of the NPSLE dataset contaminated by attribute noise are shown in Fig. 3(a) and Fig. 3(b), where one clearly perceives that our REASFS achieves the highest accuracy under all conditions. Besides, it is unreasonable to apply the same level of sparse regularization to noise features and uncontaminated features, and our GCIE 2,1 can adaptively increase the sparse level of noise features to remove redundant information, and vice versa. For label noise, we randomly select samples from the training set and replace classification labels of the selected samples with opposite values, i.e., 0 → 1 and 1 → 0. The results are shown in Fig. 3(c) and Fig. 3(d), where the proposed REASFS is superior to other baselines. It can be seen from Fig. 3 that our REASFS achieves the highest accuracy in different noisy environments, which demonstrates the robustness of generalized correntropic loss. For non-invasive biomarkers, our method shows that some metabolic features contribute greatly to the early diagnosis of NPSLE, i.e., NAAG, mI/Cr+PCr, and Glu+Gln/Cr+PCr in RPCG; Cr+PCr, NAA+NAAG, NAA+NAAG/Cr+PCr, mI/Cr+PCr and Glu+Gln in LPCG; NAA, NAAG, and Cho+PCh in LDT; PCr, Cr+PCr, Cho+PCh, Cho+PCh/Cr+PCr and Glu+Gln/Cr+PCr in RLN; MI/Cr+PCr, Cho+PCh and Cho+PCh/Cr+PCr in LLN; NAA+NAAG/Cr+PCr and Cho+PCh in RI; Cho+PCh/Cr+PCr and Glu+Gln/Cr+PCr in RPWM; And PCr, NAAG and NAA+NAAG/Cr+PCr in LPWM. Moreover, we use isometric feature mapping (ISOMAP) [19] to analyze these metabolic features and find that this feature subset is essentially a low-dimensional manifold. Meanwhile, by combining the proposed REASFS and ISOMAP, we can achieve 99% accuracy in the early diagnosis of NPSLE. In metabolite analysis, some studies have shown that the decrease in NAA concentration is related to chronic inflammation, damage, and tumors in the brain [18]. In the normal white matter area, different degrees of NPSLE disease is accompanied by different degrees of NAA decline, but structural MRI is not abnormal, suggesting that NAA may indicate the progress of NPSLE. We also found that Glu+Gln/Cr+PCr in RI decreased, which indicates that the excitatory neurotransmitter Glu in the brain of patients with NPSLE may have lower activity. To sum up, the proposed method provides a shortcut for revealing the pathological mechanism of NPSLE and early detection."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,,Conclusion:,"In this paper, we develop REASFS, a robust flexible feature selection that can identify metabolic biomarkers and detect NPSLE at its early stage from noisy 1 H-MRS data. The main advantage of our approach is its robust generalized correntropic loss and a novel GCIE 2,1 regularization, which jointly utilizes the row sparsity and exclusive sparsity to adaptively accommodate various sparsity levels and preserve informative features. The experimental results show that compared with previous methods, REASFS plays a very important role in the biomarker discovery and early diagnosis of NPSLE. Finally, we analyze metabolic features and point out their clinical significance."
Multi-view Vertebra Localization and Identification from CT Images,1.0,Introduction,"Automatic Localization and identification of vertebra from CT images are crucial in clinical practice, particularly for surgical planning, pathological diagnosis, and post-operative evaluation [1,9,10]. However, the process is challenging due to the significant shape variations of vertebrae with different categories, such as lumbar and thoracic, and also the close shape resemblance of neighboring vertebrae. Apart from these intrinsic challenges, the arbitrary field-of-view (FOV) of different CT scans and the presence of metal implant artifacts also introduce additional difficulties to this task.With the advance of deep learning, many methods are devoted to tackling these challenges. For example, Lessmann et al. [11] employed a one-stage segmentation method to segment vertebrae with different labels for localization and identification. It is intuitive but usually involves many segmentation artifacts. Building upon this method, Masuzawa et al. [12] proposed an instance memory module to capture the neighboring information, but the long-term sequential information is not well studied. Recently, two or multi-stage methods [4,5,[13][14][15], that first localize the vertebra and further classify the detected vertebra patches, are proposed to achieve the state-of-the-art performance. And some additional modules, such as attention mechanism [5], graph optimization [13], and LSTM [15], are integrated to capture the sequential information of adjacent vertebrae. However, all these methods are performed on 3D patches, where the global information of the CT scan is destroyed and cannot be well-captured. Moreover, due to the lack of pre-trained models in 3D medical imaging, networks trained from scratch using a small dataset often lead to severe overfitting problems with inferior performance.In this paper, to tackle the aforementioned challenges, we present a novel framework that converts the 3D vertebra labeling problem into a multi-view 2D vertebra localization and identification task. Without the 3D patch limitation, our network can learn 2D global information naturally from different view perspectives, as well as leverage the pre-trained models from ImageNet [6]. Specifically, given a 3D CT image, we first generate multi-view 2D Digitally Reconstructed Radiograph (DRR) projection images. Then, a multi-view contrastive learning strategy is designed to further pre-train the network on this specific task. For vertebra localization, we predict the centroid of each vertebra in all DRR images and map the 2D detected centroids of different views back into the 3D CT scan using a least-squares algorithm. As for vertebra identification, we formulate it as a 2D segmentation task that generates vertebra labels around vertebra centroids. Particularly, a Sequence Loss, based on dynamic programming, is introduced to maintain the sequential information along the spine vertebrae in the training stage, which also serves as a weight to vote the multiview 2D identification results into the 3D CT image for more reliable results. Our proposed method is validated on a public challenging dataset [17] and achieved the state-of-the-art performance both in vertebra localization and identification. Moreover, more evaluation results on a large-scale in-house dataset collected in real-world clinics (with 500 CT images) are provided in the supplementary materials, further demonstrating the effectiveness and robustness of our framework."
Multi-view Vertebra Localization and Identification from CT Images,2.0,Methodology,"An overview of our proposed method for vertebra localization and identification using multi-view DRR from CT scans is shown in Fig. 1, which mainly consists of three steps. Step 1 is to generate DRR images, followed by a multi-view contrastive learning strategy to pre-train the backbone. Step 2 aims to finish 2D single-view vertebra localization and identification, and step 3 is to map the 2D results back to 3D with a multi-view fusion strategy. We will elaborate our framework in this section."
Multi-view Vertebra Localization and Identification from CT Images,2.1,DRR Multi-view Contrastive Learning,"DRR Generation. To accurately localize and identify each vertebra in CT images, we convert the 3D task into 2D, where global information can be naturally captured from different views, avoiding the large computation of 3D models. To achieve this, DRR (Digitally Reconstructed Radiograph) technique, a simulation procedure for generating a radiograph similar to conventional X-ray image, is performed by projecting a CT image onto a virtual detector plane with a virtual X-ray source. In this way, we can generate K DRR projection images of a CT image for every 360/K degree. The 3D labeling problem can then be formulated as a multi-view localization and identification task in a 2D manner. Specifically, the 2D ground-truth are generated by projecting the 3D centroids and labels onto the 2D image following the DRR projection settings.DRR Multi-view Contrastive Learning. After DRR generation, our goal is to localize and identify the vertebra on DRR images. However, as the dataset for the vertebra task is relatively small due to time-consuming manual annotation, we design a new multi-view contrastive learning strategy to better learn the vertebrae representation from various views. Unlike previous contrastive learning methods, where the pretext is learned from numerous augmented negative and positive samples [2,3,7,8], e.g., random crop, image flip, rotation and resize, the multi-view DRR images generated from the same CT image share consistent anatomical information, which are natural positive samples. Based on this insight, we pre-train our network backbone using the Simsiam [3] approach to encode two random views from the same CT image as a key and query, as shown in Fig. 1 (b), in the aims of learning the invariant vertebrae representation from different views."
Multi-view Vertebra Localization and Identification from CT Images,2.2,Single-view Vertebra Localization,"With multi-view DRR images, the 3D vertebra localization problem is converted into a 2D vertebra centroid detection task, followed by a multi-view fusion strategy (as introduced in Sect. 2.4) that transforms the 2D results to 3D. To achieve this, we utilize the commonly-used heatmap regression strategy for 2D vertebra centroid detection. Specifically, for each vertebra in a DRR image, our model is trained to learn the contextual heatmap defined on the ground-truth 2D centroid using a Gaussian kernel. During inference, we apply a fast peak search clustering method [16] to localize the density peaks on the regressed heatmap as the predicted centroid. Benefiting from the pre-trained models from multiview contrastive learning, our method can capture more representative features from different views. Further, compared to existing 3D methods, our approach performs vertebra localization on several DRR images with a fusion strategy, making it more robust to the situation of missing detection in certain views."
Multi-view Vertebra Localization and Identification from CT Images,2.3,Single-View Vertebra Identification,"After the vertebrae localization, we further predict the label of each vertebra using an identification network on multi-view DRR images. Unlike other 3D methods that require cropping vertebra patches for classification, our identification network performs on 2D, allowing us to feed the entire DRR image into the network, which can naturally capture the global information. Specifically, we use a segmentation model to predict the vertebra labels around the detected vertebra centroids, i.e., a 22 mm × 22 mm square centered at the centroid. During the inference of single-view, we analyze the pixel-wise labels in each square and identify the corresponding vertebra with the majority number of labels.Sequence Loss. In the identification task, we observe that the vertebra labels are always in a monotonically increasing order along the spine, which implies the presence of sequential information. To better exploit this property and enhance our model to capture such sequential information, we propose a Sequence Loss as an additional network supervision, ensuring the probability distribution along the spine follows a good sequential order. Specifically, as shown in Fig. 1, we compute a probability map P ∈ R n×c for each DRR image by averaging the predicted pixel-wise possibilities in each square around the vertebra centroid from the identification network. Here, n is the number of vertebrae contained in this DRR image, and c indicates the number of vertebra categories (i.e., from C1 to L6). Due to the sequential nature of the vertebra identification problem, the optimal distribution of P is that the index of the largest probability in each row is in ascending order (green line in Fig. 1). To formalize this notion, we compute the largest accumulated probability in ascending order, starting from each category in the first row and ending at the last row, using dynamic programming. The higher accumulated probability, the better sequential structure presented by this distribution. We set this accumulated probability as target profit, and aim to maximize it to enable our model to better capture the sequential structure in this DRR image. The optimal solution (OP T ) based on the dynamic programming algorithm is as:where i ∈ [1, n] and j ∈ [1, c]. Here, α and β are two parameters that are designed to alleviate the influence of wrong-identified vertebra. Sequence Loss (L s ) is then defined as:The overall loss function L id for our identification network is:where L ce and L s refer to the Cross-Entropy loss and Sequence Loss, respectively. γ is a parameter to control the relative weights of the two losses."
Multi-view Vertebra Localization and Identification from CT Images,2.4,Multi-view Fusion,"Localization Multi-view Fusion. After locating all the vertebrae in each DRR image, we fuse and map the 2D centroids back to 3D space by a leastsquares algorithm, as illustrated in Fig. 1 (d). For a vertebra located in K views, we can track K target lines from the source points in DRR technique to the detected centroid on the DRR images. Ideally, the K lines should intersect at a unique point in the 3D space, but due to localization errors, this is always unachievable in practice. Hence, instead of finding a unique intersection point, we employ the least-squares algorithm to minimize the sum of perpendicular distances from the optimal intersection point to all the K lines, given by:where p denotes the 3D coordinate of the optimal intersection point, a k and n k represent the point on the k th target line and the corresponding direction vector. By taking derivatives with respect to p, we get a linear equation of p as shown in Eq. ( 5), where the optimal intersection point can be obtained by achieving the minimum distance to the K lines.(Identification Multi-view Voting. The Sequence Loss evaluates the quality of the predicted vertebra labels in terms of their sequential property. During inference, we further use this Sequence Loss of each view as weights to fuse the probability maps obtained from different views. We obtain the final voted identification map V of K views as:For each vertebra, the naive solution for obtaining vertebra labels is to extract the largest probability from each row in voted identification map V . Despite the promising performance of the identification network, we still find some erroneous predictions. To address this issue, we leverage the dynamic programming (described in Eq. ( 1)) again to correct the predicted vertebra labels in this voted identification map V . Specifically, we identify the index of the largest accumulated probability in the last row as the last vertebra category and utilize it as a reference to correct any inconsistencies in the prediction."
Multi-view Vertebra Localization and Identification from CT Images,3.1,Dataset and Evaluation Metric,"We extensively evaluate our method on the publicly available MICCAI VerSe19 Challenge dataset [17], which consists of 160 spinal CT with ground truth annotations. Specifically, following the public challenge settings, we utilize 80 scans for training, 40 scans for testing, and 40 scans as hidden data. To evaluate the performance of our method, we use the mean localization error (L-Error) and identification rate (Id-Rate) as the evaluation metrics, which are also adopted in the challenge. The L-Error is calculated as the average Euclidean distance between the ground-truth and predicted vertebral centers. The Id-Rate is defined as the ratio of correctly identified vertebrae to the total number of vertebrae."
Multi-view Vertebra Localization and Identification from CT Images,3.2,Implementation Details,"All CT scans are resampled to an isotropic resolution of 1 mm. For DRR Multi-View Contrastive Learning, we use ResNet50 as encoder and apply the SGD optimizer with an initial learning rate of 0.0125, which follows the cosine decay schedule. The weight decay, SGD momentum, batch size and loss function are set to 0.0001, 0.9, 64, and cosine similarity respectively. We employ U-Net for both the localization and identification networks, using the pre-trained ResNet50 from our contrastive learning as backbone. Adam optimizer is set with an initial learning rate of 0.001, which is divided by 10 every 4000 iterations. Both networks are trained for 15k iterations. We empirically set α = 0.1, β = 0.8, γ = 1. All methods were implemented in Python using PyTorch framework and trained on an Nvidia Tesla A100 GPU with 40 GB memory."
Multi-view Vertebra Localization and Identification from CT Images,3.3,Comparison with SOTA Methods,"We train our method on 70 CT images and tune the hyperparameter on the rest 10 CT images from the training data. We then evaluate it on both testing and hidden datasets, following the same setting as the challenge. In the comparison, our method is compared with four methods which are the first four positions in the benchmark of this challenge [17]. The experimental results are presented in Table 1. Our method achieves Id-Rate of 98.12% and L-Error of 1.79 mm on the test dataset, and Id-Rate of 96.45% and L-Error of 2.17 mm on the hidden dataset, which achieves the leading performance both in localization and identification tasks with just two 2D networks. Compared to these methods performed on 3D with random cropping or patch-wise method (Payer C. [17], Lessmann N.[17] and Chen M. [17]), our 2D strategy can capture more reliable global and sequential information in all 2D projection images which can improve the labeling performance, especially the localization error. Compared to those using 2D MIP (Sekuboyina A. [18]), our DRR multi-view projection and fusion strategy can provide superior performance by analyzing more views and introducing the geometry information carried by varied DRR projections naturally."
Multi-view Vertebra Localization and Identification from CT Images,3.4,Ablation Study,"Ablation Study of Key Components. We conduct an ablation study on the VerSe19 dataset to demonstrate the effectiveness of each component. As presented in Table 2, we build the basic network for the vertebra localization and  identification with a bagging strategy, where for each vertebra, we opt for the ID that is predicted by the majority of views, when not using weighted voting, and K = 10, denoted as Baseline. Pre-train, Sequence Loss, and voting in Table 2 represent the addition of the multi-view contrastive learning, Sequence Loss, and multi-view voting one by one. Pre-trained from ImageNet is used when not utilizing our contrastive learning pre-trained parameters. Specifically, the Baseline achieves Id-Rate of 84.00% and 83.54% on two datasets. With the contrastive learning pre-trained parameters, we achieve 1.88% and 2.98% improvements over the ImageNet pre-trained, respectively. This shows the pre-trained parameters of the backbone obtained from our contrastive learning can effectively facilitate the network to learn more discriminative features for identification than the model learning from scratch. Sequence Loss provides extra supervision for sequential information, and results in 3.53% and 4.02% increase, illustrating the significance of capturing the sequential information in the identification task. Finally, multi-view weighted voting yields the best results with 98.12% and 96.45% on the two datasets, indicating the robustness of our multi-view voting when the identification errors occurred in a small number of DRR images can be corrected by other DRR prediction results.Ablation Study of Projection Number. We also conduct an ablation study on the same dataset to further evaluate the impact of the projection number K. The results are presented in Fig. 2, indicating a clear trend of performance improvements as the number of projections K increases from 5 to 10. However, when K increases to 20, the performance is just comparable to that of 10. We analyze that using too few views may result in inadequate and unreliable anatomical structure representation, leading to unsatisfactory results. On the other hand, too many views may provide redundant information, resulting in comparable results but with higher computation cost. Therefore, K is set to 10 as a trade-off between accuracy and efficiency."
Multi-view Vertebra Localization and Identification from CT Images,4.0,Conclusion,"In this paper, we propose a novel multi-view method for vertebra localization and identification in CT images. The 3D labeling problem is converted into a multi-view 2D localization and identification task, followed by a fusion strategy.In particular, we propose a multi-view contrastive learning strategy to better learn the invariant anatomical structure information from different views. And a Sequence Loss is further introduced to enhance the framework to better capture sequential structure embedded in vertebrae both in training and inference. Evaluation results on a public dataset demonstrate the advantage of our method."
Multi-view Vertebra Localization and Identification from CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_14.
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,1.0,Introduction,"Lung cancer is currently the foremost cause of cancer-related mortalities globally, with non-small cell lung cancer (NSCLC) being responsible for 85% of reported cases [25]. Within NSCLC, squamous cell carcinoma (SCC) and adenocarcinoma (ADC) are recognized as the two principal histological subtypes. Since SCC and ADC differ in the effectiveness of chemotherapy and the risk of complications, accurate identification of different subtypes is crucial for clinical treatment options [15]. Although pathological diagnosis via lung biopsy can provide a reliable result of subtype identification, it is highly invasive with potential clinical implications [19]. Therefore, non-invasive methods utilizing computed tomography (CT) images have garnered significant attention over the last decade [15,16].Recently, several deep-learning methods have been put forward to differentiate between the NSCLC histological subtypes using CT images [4,11,13,22]. Chaunzwa et al. [4] and Marentakis et al. [13] both employ a convolutional neural network (CNN) model with axial view CT images to classify the tumor histology into SCC and ADC. Albeit the good performance, the above 2D CNN-based models only take CT images from a single view as the input, limiting their ability to describe rich spatial properties of CT volumes [20]. Multi-view deep learning, a 2.5D method, represents a promising solution to this issue, as it focuses on obtaining a unified joint representation from different views of lung nodules to capture abundant spatial information [16,20]. For example, Wu et al. [22] aggregate features from axial, coronal, and sagittal view CT images via a multi-view fusion model. Similarly, Li et al. [11] also extract patches from three orthogonal views of a lung nodule and present a multi-view ResNet for feature fusion and classification. By integrating multi-view representations, these methods efficiently preserve the spatial information of CT volumes while significantly reducing the required computational resource compared to 3D CNNs [9,20,23].Despite the promising results of previous multi-view methods, they still confront a severe challenge for accurate NSCLC histological subtype prediction. In fact, due to the limitation of scan time and hardware capacity in clinical practice, different views of CT volumes are anisotropic in terms of in-plane and inter-plane resolution [21]. Additionally, images from certain views may inevitably contain some unique background information, e.g., the spine in the sagittal view [17]. Such anisotropy and background dissimilarity both reveal the existence of significant variations between different views, which lead to markedly various representations in feature space. Consequently, the discrepancies of distinct views will hamper the fusion of multi-view information, limiting further improvements in the classification performance.To overcome the challenge mentioned above, we propose a novel cross-aligned representation learning (CARL) method for the multi-view histologic subtype classification of NSCLC. CARL offers a holistic and disentangled perspective of multi-view CT images by generating both view-invariant and -specific representations. Specifically, CARL incorporates a cross-view representation alignment learning network which targets the reduction of multi-view discrepancies by obtaining discriminative view-invariant representations. A shared encoder with a novel discriminability-enforcing similarity constraint is utilized to map all representations learned from multi-view CT images to a common subspace, enabling cross-view representation alignment. Such aligned projections help to capture view-invariant features of cross-view CT images and meanwhile make full use of the discriminative information obtained from each view. Additionally, CARL learns view-specific representations as well which complement the view-invariant ones, providing a comprehensive picture of the CT volume data for histological subtype prediction. We validate our approach by using a publicly available NSCLC dataset from The Cancer Imaging Archive (TCIA). Detailed experimental results demonstrate the effectiveness of CARL in reducing multi-view discrepancies and improving NSCLC histological subtype classification performance. Our contributions can be summarized as follows:-A novel cross-aligned representation learning method called CARL is proposed for NSCLC histological subtype classification. To reduce the discrepancies of multiview CT images, CARL incorporates a cross-view representation alignment learning network for discriminative view-invariant representations. -We employ a view-specific representation learning network to learn view-specific representations as a complement to the view-invariant representations. -We conduct experiments on a publicly available dataset and achieve superior performance compared to the most advanced methods currently available. "
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.1,Architecture Overview,"Figure 1 shows the overall architecture of CARL. The cross-view representation alignment learning network includes a shared encoder which projects patches of axial, coronal, and sagittal views into a common subspace with a discriminability-enforcing similarity constraint to obtain discriminative view-invariant representations for multi-view discrepancy reduction. In addition, CARL introduces a view-specific representation learning network consisting of three unique encoders which focus on learning view-specific representations in respective private subspaces to yield complementary information to view-invariant representations. Finally, we introduce a histological subtype classification module to fuse the view-invariant and -specific representations and make accurate NSCLC histological subtype classification."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.2,Cross-View Representation Alignment Learning,"Since the discrepancies of different views may result in divergent statistical properties in feature space, e.g., huge distributional disparities, aligning representations of different views is essential for multi-view fusion. With the aim to reduce multi-view discrepancies, CARL introduces a cross-view representation alignment learning network for mapping the representations from distinct views into a common subspace, where view-invariant representations can be obtained by cross-view alignment. Specifically, inspired by [12,14,24], we exert a discriminability-enforcing similarity constraint to align all sub-view representations with those of the main view, significantly mitigating the distributional disparities of multi-view representations.Technically speaking, given the axial view image I av , coronal view image I cv , and sagittal view image I sv , the cross-view representation alignment learning network tries to generate view-invariant representations h c v , v ∈ {av, cv, sv} via a shared encoder based on a residual neural network [10]. This can be formulated as below:where E c (•) indicates the shared encoder, and av, cv, sv represent axial, coronal, and sagittal views, respectively. In the common subspace, we hope that through optimizing the shared encoder E c (•), the view-invariant representations can be matched to some extent. However, the distributions of h c av , h c cv and h c sv are very complex due to the significant variations between different views, which puts a burden on obtaining well-aligned view-invariant representations with merely an encoder.To address this issue, we design a discriminability-enforcing similarity loss L dsim to further enhance the alignment of cross-view representations in the common subspace. Importantly, considering that the axial view has a higher resolution than other views and are commonly used in clinical diagnosis, we choose axial view as the main view and force the sub-views (e.g., the coronal and sagittal views) to seek distributional similarity with the main view. Mathematically, we introduce a cross-view similarity loss L sim which calculates the central moment discrepancy (CMD) metric [24] between all sub-views and the main view as shown below:where CMD(•) denotes the distance metric which measures the distribution disparities between the representations of i-th sub-view h sub i and the main view h main . N is the number of sub-views. Despite the fact that minimizing the L sim can efficiently mitigate the issue of distributional disparities, it may not guarantee that the alignment network will learn informative and discriminative representations. Inspired by recent work on multimodal feature extraction [12,14], we impose a direct supervision by inputting h main into a classifier f (•) to obtain the prediction of histological subtype, and use a crossentropy loss to enforce the discriminability of the main-view representations. Finally, the discriminability-enforcing similarity loss L dsim is as follows:where y denotes the ground-truth subtype labels, λ controls the weight of L CE . We observed that L CE is hundred times smaller than L sim , so this study uses an empirical value of λ = 110 to balance the magnitude of two terms. By minimizing L dsim , the cross-view representation alignment learning network pushes the representations of each sub-view to align with those of the main view in a discriminability-enforcing manner.Notably, the benefits of such cross-alignment are twofold. Firstly, it greatly reduces the discrepancies between the sub-views and the main view, leading to consistent viewinvariant representations. Secondly, since the alignment between distinct views compels the representation distribution of the sub-views to match that of the discriminative main view, it can also enhance the discriminative power of the sub-view representations.In other words, the cross-alignment procedure spontaneously promotes the transfer of discriminative information learned by the representations of the main view to those of the sub-views. As a result, the introduced cross-view representation alignment learning network is able to generate consistent and discriminative view-invariant representations cross all views to effectively narrow the multi-view discrepancies."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.3,View-Specific Representation Learning,"On the basis of learning view-invariant representations, CARL additionally learns view-specific representations in respective private subspaces, which provides supplementary information for the view-invariant representations and contribute to subtype classification as well. To be specific, a view-specific representation learning network containing three unique encoders is proposed to learn view-specific representations h p v , v ∈ {av, cv, sv}, enabling effective exploitation of the specific information from each view. We formulate the unique encoders as follows:whereis the encoder function dedicated to capture single-view characteristics. To induce the view-invariant and -specific representations to learn unique characteristics of each view, we draw inspiration from [14] and adopt an orthogonality loss L orth with the squared Frobenius norm between the representations in the common and private subspaces of each view, which is denoted byA reconstruction module is also employed to calculate a reconstruction loss L rec between original image I v and reconstructed image I r v using the L 1 -norm, which ensures the hidden representations to capture details of the respective view."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.4,Histologic Subtype Classification,"After obtaining view-invariant and -specific representations from each view, we integrate them together to perform NSCLC subtype classification. Specifically, we apply a residual block [10] to fuse view-invariant and -specific representations into a unified multi-view representation h. Then, h is sent to a multilayer perceptron neural network (MLP) to make the precise NSCLC subtype prediction. The NSCLC histological subtype classification loss L cls can be calculated by using cross-entropy loss."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.5,Network Optimization,"The optimization of CARL is achieved through a linear combination of several loss terms, including discriminability-enforcing similarity loss L dsim , orthogonality loss L orth , reconstruction loss L rec and the classification loss L cls . Accordingly, the total loss function can be formulated as a weighted sum of these separate loss terms:where α, β and γ denote the weights of L dsim , L rec and L orth . To normalize the scale of L dsim which is much larger than the other terms, we introduce a scaling factor S = 0.001, and perform a grid search for α, β and γ in the range of 0.1S-S, 0.1-1, and 0.1-1, respectively.Throughout the experiments, we set the values of α, β and γ to 0.6S, 0.4 and 0.6, respectively."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"Our dataset NSCLC-TCIA for lung cancer histological subtype classification is sourced from two online resources of The Cancer Imaging Archive (TCIA) [5]: NSCLC Radiomics [1] and NSCLC Radiogenomics [2]. Exclusion criteria involves patients diagnosed with large cell carcinoma or not otherwise specified, along with cases that have contouring inaccuracies or lacked tumor delineation [9,13]. Finally, a total of 325 available cases (146 ADC cases and 179 SCC cases) are used for our study. We evaluate the performance of NSCLC classification in five-fold cross validation on the NSCLC-TCIA dataset, and measure accuracy (Acc), sensitivity (Sen), specificity (Spe), and the area under the receiver operating characteristic (ROC) curve (AUC) as evaluation metrics. We also conduct analysis including standard deviations and 95% CI, and DeLong statistical test for further AUC comparison.For preprocessing, given that the CT data from NSCLC-TCIA has an in-plane resolution of 1 mm × 1 mm and a slice thickness of 0.7-3.0 mm, we resample the CT images using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm. Then one 128 × 128 pixel slice is cropped from each view as input based on the center of the tumor. Finally following [7], we clip the intensities of the input patches to the interval (-1000, 400 Hounsfield Unit) and normalize them to the range of [0, 1]."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.2,Implementation Details,"The implementation of CARL is carried out using PyTorch and run on a workstation equipped with Nvidia GeForce RTX 2080Ti GPUs and Intel Xeon CPU 4110 @ 2.10GHz. Adam optimizer is used with an initial learning rate of 0.00002, and the batch size is set to 8."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.3,Results,"Comparison with Existing Methods. Several subtype classification methods have been employed for comparison including: two conventional methods, four single-view and 3D deep learning methods, and four representative multi-view methods. We use publicly available codes of these comparison methods and implement models for methods without code. The experimental results are reported in Table 1. The multi-view methods are generally superior to the single-view and 3D deep learning methods. It illustrates that the multi-view methods can exploit richer spatial properties of CT volumes than the single-view methods while greatly reducing the model parameters to avoid overfitting compared to the 3D methods. The floating point operations (FLOPs) comparison between CARL (0.9 GFLOPs) and the 3D method [9] (48.4 GFLOPs) also proves the computational efficiency of our multi-view method. Among all multi-view methods, our proposed CARL achieves the best results, outperforming Wu et al. by 3.2%, 3.2%, 1.5% and 4.1% in terms of AUC, Acc, Sen and Spe, respectively. Not surprisingly, the ROC curve of CARL in Fig. 2(a) is also closer to the upper-left corner, further indicating its superior performance. These results demonstrate that CARL can effectively narrow the discrepancies of different views by obtaining view-invariant representations in a discriminative way, thus leading to excellent classification accuracy compared to other methods.   Ablation Analysis. We evaluate the efficacy of different losses in our method. The results are reported in Table 2, where CARL-B0 refers to CARL only using the classification loss, +L * indicates the loss superimposed on CARL-B0 and L all denotes that we utilize all the losses in Eq. 5. We can observe that CARL-B2 performs better than CARL-B0 by employing the discriminability-enforcing similarity loss to align crossview representations. Besides, CARL-B3 and CARL-B4 show better performance than CARL-B0, illustrating view-specific representations as a complement which can also contribute to subtype classification. Though single loss already contributes to performance improvement, CARL-B5 to CARL-B7 demonstrate that the combinations of different losses can further enhance classification results. More importantly, CARL with all losses achieves the best performance among all methods, demonstrating that our proposed method effectively reduces multi-view discrepancies and significantly improves the performance of histological subtype classification by providing a holistic and disentangled perspective of the multi-view CT images. The ROC curve of CARL in Fig. 2(b) is generally above its variants, which is also consistent with the quantitative results."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,4.0,Conclusion,"In summary, we propose a novel multi-view method called cross-aligned representation learning (CARL) for accurately distinguishing between ADC and SCC using multi-view CT images of NSCLC patients. It is designed with a cross-view representation alignment learning network which effectively generates discriminative view-invariant representations in the common subspace to reduce the discrepancies among multi-view images. In addition, we leverage a view-specific representation learning network to acquire viewspecific representations as a necessary complement. The generated view-invariant and -specific representations together offer a holistic and disentangled perspective of the multi-view CT images for histological subtype classification of NSCLC. The experimental result on NSCLC-TCIA demonstrates that CARL reaches 0.817 AUC, 76.8% Acc, 73.2% Sen, and 79.7% Spe and surpasses other relative approaches, confirming the effectiveness of the proposed CARL method."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible and progressive neurodegenerative disorder, and it is the most common cause of dementia. Early diagnosis and appropriate interventions play a vital role in managing AD [22]. Existing studies suggest that AD is a disconnection syndrome manifesting the brain network alterations caused by neuropathological processes before the onset of clinical symptoms [14,15]. Therefore, identifying the propagation patterns of neuropathological burdens provides a new window to comprehend the pathophysiological mechanism of AD and predict the early stage of AD. Recent advances in neuroimaging techniques, such as diffusion-weighted imaging (DWI), allow us to observe the fiber bundles between two anatomical regions in-vivo, thereby encoding the structural brain network into a graph data structure [13,17].In recent years, brain network analyses via graph neural networks have been widely used in brain disease diagnosis [10,12,21]. The advantage of graph convolution is that it takes the topological information of the graph into account, so that the information of neighbor nodes can be integrated into the target nodes via a message-passing scheme to upgrade their discriminative performance. However, these GNN-based approaches assume that the relationship between nodes is pairwise, which ignores the high-order relationships widely existing in brain networks (e.g., functional interactions between multiple brain regions) [8]. To address this limitation, hypergraph, a special graph structure where one hyperedge can contain multiple nodes, is proposed to capture the high-order information in graph [24]. Moreover, Feng et al. [2] extended the hypergraph learning method to hypergraph neural network (HGNN) to learn an optimal data representation. In neuroscience, Ji et al. [7] proposed a hypergraph attention network (FC-HAT) consisting of a dynamic hypergraph generation phase and a hypergraph attention aggregation phase to classify the functional brain networks.Although HGNN methods have achieved promising progress in many fields, such approaches have three major limitations for brain network analyses. (1)Lack of consideration of the unique topological properties in the brain network. The brain networks are cost-efficient small-world networks, which contain a series of organization patterns such as hub nodes and hierarchical modularity. However, current hypergraph constructed using K-nearest neighbors (KNN) exhibit a lack of power to extract specific high-order topological information from the brain network. (2)Lack of a robust ROI-aware encoding to overcome the anonymity of nodes in HGNN. Current GNN or HGNN methods are permutation invariant, indicating that the node order in a graph is insensitive to the performance of the graph neural network models. However, every brain region in the brain network has its specific brain function and location; such permutation invariance and anonymity are problematic for brain network analyses. (3)Lack of an appropriate mechanism to identify brain network dysfunction. Compared to identifying the disease-related brain regions, less attention has been paid to detecting the propagation patterns of neuropathological burdens using the HGNN methods, despite the brain network alterations with strong interpretation being more attractive to neuroimaging researchers.To overcome these limitations, we propose a hypergraph neural network explicitly tailored for brain networks to perform (1) identification of propagation patterns of neuropathology and (2) early diagnosis of AD. In this study, we adopt a second-order random walk on a brain network reference to generate two groups of hyperedges to depict the topology of the brain network. Additionally, an ROI identity encoding module is incorporated into our model to avoid the influence of anonymity of nodes during HGNN convolution on brain network analysis. Furthermore, we design a self-learned weighted hypergraph convolution to extract the discriminative hyperedges, which can be used to characterize the spreading pathways of neuropathological events in AD. The framework of our BrainHGNN is shown in Fig. 1. We have evaluated the effectiveness and robustness of our proposed BrainHGNN on neuroimaging data from the ADNI database. Compared to other methods, the BrainHGNN achieves enhanced discriminative capacity for CN, EMCI, and LMCI classification, as well as illustrates its potential for discovering the putative propagation patterns of neuropathology in AD. "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.1,Hypergraph Construction,"A brain network is often encapsulated into an adjacency matrix A s ∈ R N ×N (s = 1, • • • , S), where N stands for the number of brain regions in the brain network and S denotes the number of samples. To characterize the spreading pathway of neuropathological events in AD, we first calculate a group-mean adjacency matrix Ā = 1 S S s=1 A s as a brain network reference from a population of brain networks. Since complex brain functions are usually carried out by the interaction of multiple brain regions, a hypergraph neural network is introduced as the backbone of our proposed model to capture high-order relationships. A hyperedge in hypergraph connects more than two nodes, characterizing the high-order relationships. Let G = {V, E, W} denote a hypergraph with node set V and hyperedge set E, where each hyperedge is actually a subset of V randomly sampled based on Ā. Each diagonal element in the diagonal matrix W represents a weight of hyperedge. The hypergraph structure is represented by an incident matrix H ∈ R |V|×|E| , where H(v, e) = 1 means v node is in the e hyperedge. Moreover, the diagonal matrices of the hyperedge degrees D e and the vertex degrees D v can be calculated by δ(e) = v∈V H(v, e) and d(v) = e∈E w(e)H(v, e), respectively.Node Representation. Empirical biomarkers (such as deposition of amyloid plaques) calculated on PET neuroimaging data are the hallmarks of AD. We obtain the standard uptake value ratio for each brain region and represent them as a column vector f s ∈ R N , with each element corresponding to the feature of the node. However, the structural brain network has specific regions with distinct functions and messages, making the permutation invariance of hypergraph neural networks less suitable. We address this by generating a region-of-interest (ROI) identity encoding p ∈ R N ×m for each brain region and concatenating it with the empirical biomarkers, thereby reducing permutation invariance and anonymity of nodes in graph convolution. The parameter m denotes the dimension of identity encoding and is set to 2 in our experiments. The final node representation is formulated as,where f 0 s is a vector of the initial input biomarkers and p is a learnable matrix to encode the regional information.Hyperedge Generation. The brain, a complex network, exhibits information processing with maximum efficiency and minimum costs. Such brain organization requires the brain network to be equipped with both high local clustering (hierarchical modularity) and global efficiency (shortest path length) [18]. In this context, inspired by the random walk sampling in [6], we propose a second-order biased random walk sampling strategy to each node of the weighted graph Ā to generate a group of local hyperedges H local ∈ R N ×N and a group of global hyperedges H global ∈ R N ×N , respectively. Specifically, the transition probability π ux on edge (u, x) with source node u is decided by edge weight Ā(u, x) and a bias term α p (v, x) related to source node v of previous step, that is,where α p (v, x) = p dvx-1 , d vx ∈ {0, 1, 2} and d vx is the shortest path distance between node v and candidate target x. In Eq. ( 2), we can observe that when p sets to a small value (0 < p < 1), node u tends to transfer to the neighbors of node v, thus forming a local network community. However, when p takes a large value (p > 1), node u has a high probability of moving to a node far away from node v, thereby generating a global network community. Therefore, the hyperedge generation can be described mainly in three steps. (1) Calculate the transition probability matrix with the original edge weight matrix and parameter p according to Eq. ( 2). ( 2 "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.2,Hypergraph Convolution,"As stated in [4], the hypergraph convolution can be decoupled as a two-stage message passing procedure, which can be formulated as:where X t ∈ R |V|×Mt is the node feature matrix in t th layer and Z t ∈ R |E|×Mt is the feature matrix of hyperedges in t th layer. Θ t+1 ∈ R Mt×Mt+1 is the learning parameter in (t + 1) th layer. The intuition behind Eq. ( 3) is that we first use the incidence matrix H to guide each node to aggregate and generate the hyperedge feature matrix Z t , then the updated node feature matrix X t+1 can be obtained by aggregating node-relevant hyperedge features with a learning parameter Θ t+1 and a nonlinear activation function σ(•).In most cases, the hyperedge weight matrix W is pre-defined as either an identical matrix I or a diagonal matrix with specific elements determined by prior knowledge, for ease of use. Nevertheless, these two types of hyperedge weight matrices pose difficulties in identifying the discriminant hyperedges associated with AD-related brain network dysfunction. To address this issue, we propose using a learnable weight matrix mask to guide the learning of hyperedge weights, enabling us to identify propagation patterns of neuropathological burden in AD. The hyperedge convolution can be rewritten as,where W = sigmoid( M • W) is the learned hyperedge weight matrix, M is the learnable weight mask and • means Hadamard product."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.3,Optimization,"In this study, we use the cross entropy loss to train our model and predict disease status. To avoid overfitting and discover AD-related propagation patterns, we first impose an l 1 -norm constraint to the learnable weight mask M to overshadow the irrelevant hyperedges and preserve the discriminative hyperedges that are highly related to the prediction of labels. Then, we require Θ to be smooth with an l 2 -norm constraint to void its divergence. Finally, the loss function can be formulated as follows:where z i is the predictive label of i th sample output from the fully connected layer by our BrainHGNN and y i is the ground truth of i th sample."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.1,Dataset Description and Experimental Settings,"Data Processing. We collect 94 structural brain networks from the ADNI database (https://adni.loni.usc.edu) to calculate a group-mean adjacency matrix Ā to construct the incident matrix H of the common hypergraph. For each subject, we construct the structural brain network in the following steps. First, we apply Destrieux atlas [1] to the T1-weighted MR to obtain 148 ROIs. Then we apply surface seed-based probabilistic fiber tractography [3] to the DWI image to generate a adjacency matrix A s . Regarding the feature representation of each ROI, we adopt a similar image processing pipeline to pracellate the ROIs and calculate the standard update value ratio (SUVR) score of each ROI for the amyloid-PET and FDG-PET data and represent them as a column vector f s of whole-brain pathological events. The detailed information on multi-modal neuroimaging data is shown in Table 1.Parameter Settings. In our experiments, we set parameters num layer = 1, num epoch = 250, batchsize = 256, learning rate = 0.05. LeakyReLu activation function σ(•) is used for node convolution. The l 1 -norm is adopted on the learnable weight mask M with a hyper-parameter λ 1 = 1e -3 to extract the discriminative hyperedges. The l 2 -norm is applied on the learning parameter matrix Θ with a hyper-parameter λ 2 = 1e -5.Baselines. The comparison baselines incorporate two traditional methods, including support vector machines (SVM) and random forest (RF), as well as five graph-based methods, including graph convolution neural network (GCN) [9], graph attention network (GAT) [19], hypergraph neural network (HGNN) [4], Dynamic Graph CNN (DGCNN) [20], and BrainGNN [11]. "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.2,Evaluating Diagnostic Capability on Amyloid-PET and FDG-PET Data,"In this section, our objective is to evaluate the diagnostic performance of our BrainHGNN model. We first construct three group comparisons, including CN/ EMCI, EMCI/LMCI, and CN/LMCI, for amyloid-PET or FDG-PET data. For each comparison, we apply our BrainHGNN on empirical biomarkers to evaluate the classification metrics, including accuracy, F1-score, sensitivity, and specificity, according to 10-fold cross validation. From Table 2, it is obvious that (1) our proposed method achieves the highest classification performance in most of the metrics on both amyloid-PET and FDG-PET data, compared to the other methods; (2) the graph-based deep learning methods (GCN, GAT, DGCNN, BrainGNN, HGNN, and BrainHGNN) are generally superior to traditional machine learning methods (SVM and RF), mainly because graph-based methods fully incorporate the topology of the brain network, thus enhancing the classification performance; (3) the BrainHGNN constructs the hypergraph customized to the brain network's unique topology, which is more suitable for brain network analyses than the traditional hypergraph neural networks."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.3,Evaluating the Statistical Power of Identifying Brain Network Dysfunction in AD,"In this section, we investigate the capability of identifying AD-related discriminative hyperedges by our BrainHGNN. Benefiting from the sparse weight mask, we can extract four discriminative hyperedges that most frequently occurred in the top-10 highest weight hyperedge list. Here, we take CN/LMCI group comparison as an example and map the discriminative hyperedges on the cortical surface for amyloid-PET data (first row in Fig. 2(a)) and FDG-PET data (second row in Fig. 2(a)). We can observe that most of nodes in these discriminative hyperedges are located in the Default Mode network, Cingulo-Opercular network, FrontoParietal network, DorsalAttention network, and VentralAttention network. Numerous studies have demonstrated a significant association between these subnetworks and the progress of AD [5,16,23]. Discussion. Here, we design an ablation experiment to evaluate the power of each module used in BrainHGNN. We divide the models into five different counterparts, including the HGNN, model without random walk sampling, model without self-learned edge weight, model without ROI encoding, and our Brain-HGNN. We apply these five methods on three group comparisons using empirical amyloid SUVR and FDG SUVR to calculate the classification accuracy, as shown "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,4.0,Conclusion,"In this paper, we propose a random-walk-based hypergraph neural network by integrating the topological nature of the brain network to predict the early stage of AD and discover the propagation patterns of neuropathological events in AD.Compared with other methods, our proposed BrainHGNN achieve enhanced classification performance and statistical power in group comparisons on ADNI neuroimaging dataset. In the future, we plan to apply our BrainHGNN to other neurodegenerative disorders that manifest brain network dysfunction."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,,Acknowledgements,". This work was supported in part by the National Key Research and Development Program of China (2022YFE0112200), the National Natural Science Foundation of China (U21A20520,62102153), the Science and Technology Project of Guangdong Province (2022A0505050014), the Guangdong Key Laboratory of Human Digital Twin Technology (2022B1212010004), Natural Science Foundation of Guangdong Province of China (2022A1515011162), Key-Area Research and Development Program of Guangzhou City (202206030009), and the China Postdoctoral Science Foundation (2021M691062, 2023T160226). The neuroimaging datasets used in this study were supported by the Alzheimer's Disease Neuroimaging Initiative (ADNI)."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,1.0,Introduction,"Gastroscopic Lesion Detection (GLD) plays a key role in computer-assisted diagnostic procedures. Although deep neural network-based object detectors achieve tremendous success within the domain of natural images, directly training generic object detectors on GLD datasets performs below expectations for two reasons: 1) The scale of labeled data in GLD datasets is limited in comparison to natural images due to the annotation costs. Though gastroscopic images are abundant, those containing lesions are rare, which necessitates extensive image review for lesion annotation. 2) The characteristic of gastroscopic images exhibits distinct differences from the natural images [18,19,21] and is often of high similarity in global but high diversity in local. Specifically, each type of lesion may have diverse appearances though gastroscopic images look quite similar. Some appearances of lesions are quite rare and can only be observed in a few patients. Generic self-supervised backbone pre-training or semi-supervised detector training methods can solve the first challenge for natural images but its effectiveness is undermined for gastroscopic images due to the second challenge.Self-Supervised Backbone Pre-training methods enhance object detection performance by learning high-quality feature representations from massive unlabelled data for the backbone. The mainstream self-supervised backbone pretraining methods adopt self-supervised contrast learning [3,4,7,9,10] or masked Fig. 1. Pipeline of Self-and Semi-Supervised Learning (SSL) for GLD. SSL consists of a Hybrid Self-Supervised Learning (HSL) method and a Prototype-based Pseudo-label Generation (PPG) method. HSL combines patch reconstruction with dense contrastive learning. PPG generates pseudo-labels for potential lesions based on the similarity to the prototype feature vectors.image modeling [8,15]. Self-supervised contrastive learning methods [3,4,7,9] can learn discriminative global feature representations, and [10] can further learn discriminative local feature representations by extending contrastive learning to dense paradigm. However, these methods usually cannot grasp enough local detailed information. On the other hand, masked image modeling is expert in extracting local detailed information but is weak in preserving the discriminability of feature representation. Therefore, both types of methods have their own weakness for GLD tasks.Semi-Supervised object detection methods [12,14,16,17,20,22,23] first use detectors trained with labeled data to generate pseudo-labels for unlabeled data and then enhance object detection performance by regarding these unlabeled data with pseudo-labels as labeled data to train the detector. Current pseudolabel generation methods rely on the objectiveness score threshold to generate pseudo-labels, which makes them perform below expectations on GLD, because the characteristic of gastroscopic lesions makes it difficult to set a suitable threshold to discover potential lesions meanwhile avoiding introducing much noise.The motivation of this paper is to explore how to enhance GLD performance using massive unlabeled gastroscopic images to overcome the labeled data shortage problem. The main challenge for this goal is the characteristic of gastroscopic lesions. Intuitively, such a challenge requires local feature representations to contain enough detailed information, meanwhile preserving discriminability. Enlightened by this, we propose the Self-and Semi-Supervised Learning (SSL) framework tailored to address challenges in daily clinical practice and use massive unlabeled data to enhance GLD performance. SSL overcomes the challenges of GLD by leveraging a large volume of unlabeled gastroscopic images using self-supervised learning for improved feature representations and semi-supervised learning to discover and utilize potential lesions to enhance performance. Specifically, it consists of a Hybrid Self-Supervised Learning (HSL) method for self-supervised backbone pre-training and a Prototype-based Pseudo-label Generation (PPG) method for semi-supervised detector training. The HSL combines the dense contrastive learning [10] with the patch reconstruction to inherit the advantages of discriminative feature learning and grasp the detailed information that is important for GLD tasks. The PPG generates pseudo-labels based on the similarity to the prototype feature vectors (formulated from the feature vectors in its Memory Module) to discover potential lesions from unlabeled data, and avoid introducing much noise at the same time. Moreover, we propose the first Large-scale GLD Datasets (LGLDD), which contains 10,083 gastroscopic images with 12,292 well-annotated lesion bounding boxes of four categories of lesions (polyp, ulcer, cancer, and sub-mucosal tumor). We evaluate SSL with multiple detectors on LGLDD and SSL brings significant improvement compared with baseline methods (CenterNet [6]: +2.7AP, Faster RCNN [13]: +2.0AP). In summary, our contributions include:-A Self-and Semi-supervise Learning (SSL) framework to leverage massive unlabeled data to enhance GLD performance. -A Large-scale Gastroscopic Lesion Detection datasets (LGLDD) -Experiments on LGLDD demonstrate that SSL can bring significant enhancement compared with baseline methods."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.0,Methodology,"In this section, we introduce the main ideas of the proposed SSL for GLD. The proposed approach includes 2 main components and is illustrated in Fig. 1."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.1,Hybrid Self-supervised Learning,"The motivation of Hybrid Self-Supervised Learning (HSL) is to learn the local feature representations of high discriminability meanwhile contain detailed information for the backbone from massive unlabeled gastroscopic images. Among existing backbone pre-training methods, dense contrastive learning can preserve local discriminability and masked image modeling can grasp local detailed information. Therefore, to leverage the advantages of both types of methods, we propose Hybrid Self-Supervised Learning (HSL), which combines patch reconstruction with dense contrastive learning to achieve the goal.Structure. HSL heritages the structure of the DenseCL [10] but adds an extra reconstruction projection head to reconstruct patches. Specifically, HSL consists of a backbone network and 3 parallel sub-heads. The global projection head and the dense projection head heritages from the DenseCL [10], and the proposed reconstruction projection head is inspired by the Masked Image Modeling.Enlightened by the SimMIM [15], we adopt a lightweight design for the reconstruction projection head, which only contains 2 convolution layers.Learning Pipeline. Like other self-supervised contrastive learning methods, HSL randomly generates 2 different ""views"" of the input image, uses the backbone to extract the dense feature maps F 1 , F 2 ∈ R H×W ×C , and then feeds them to the following projection heads. The global projection head of HSL uses F 1 , F 2 to obtain the global feature vector f g1 , f g2 like MoCo [9]. The dense projection head and the reconstruction projection head crop the dense feature maps F 1 , F 2 into S×S patches and obtain the local feature vector sets F 1 and F 2 of each viewThe dense projection head use F 1 and F 2 to obtain local feature vector sets F l1 and F l2 (F l = {f l1 , f l2 , ..., f lS 2 }) like DenseCL [10]. The reconstruction projection head uses each feature vector in F 1 , F 2 to reconstruct corresponding patches and obtains the patch set P 1 , P 2 (P = {p i1 , p i2 , ..., p iS 2 }.Training Objective. The HSL formulates the two contrastive learning as dictionary look-up tasks like DenseCL [10] while the reconstruction learning as a regression task. The global contrastive learning uses the global feature vector f g of an image as query q and feature vectors from the alternate view of the query image and the other images within the batch as keys K = {k 1 , k 2 , ..., }. For each query q, the only positive key k + is the different views of the same images and the others are all negative keys (k -) like MoCo [9]. We adopt the InfoNCE loss function for it:The dense contrastive learning uses the local feature vector in F li as query r and keys T l = {t 1 , t 2 , ..., }. The negative keys t -here are the feature vectors of different images while the positive key t + is the correspondence feature vector of r in another view of the images. Specifically, we adopt the correspondence methods in DenseCL [10] to obtain the positive key t + , which first conducts the matching process based on vector-wise cosine similarity between r and feature vectors in T and then selects the t j of highest similarity as the t + . The loss function is also the InfoNCE loss but in a dense paradigm:The reconstruction task uses the feature vector in F to reconstruct each patch and obtain P i . The ground truth is the corresponding patchesof the input view. We adopt the MSE loss function for it:The overall loss function is the weighted sum of these losses:where λ D and λ R are the weights of L D and L R and are set to 1 and 2."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.2,Prototype-Based Pseudo-label Generation Method,"We propose the Prototype-based Pseudo-label Generation method (PPG) to discover potential lesions from unlabeled gastroscopic data meanwhile avoid introducing much noise to further enhance GLD performance. Specifically, PPG adopts a Memory Module to remember feature vectors of the representative lesions as memory and generates prototype feature vectors for each class based on the memories stored. To preserve the representativeness of the memory and further the prototype feature vectors, PPG designs a novel Memory Update Strategy. In semi-supervised learning, PPG generates pseudo-labels for unlabeled data relying on the similarity to the prototype feature vectors, which achieves a better balance between lesion discovery and noise avoidance.Memory Module. Memory Module stores a set of lesion feature vectors as memory. For a C-class GLD task, the Memory Module stores C × N feature vectors as memory. Specifically, for each lesion, we denote the feature vector used to classify the lesion in the detector as f c . PPG stores N feature vectors for each class c to formulate the class memory m c = {f c1 , f c2 , ..., f cN }, and the memory M of PPG can be expressed as M = {m 1 , m 2 , ..., m C }. Then, PPG obtains the prototype feature vector p c by calculating the center of each class memory m c , and the prototype feature vector set can be expressed as P t = {p 1 , p 2 , ..., p C }. Moreover, the prototype feature vectors further serve as supervision for detector training under a contrastive clustering formulation and adopt a contrastive loss:If the detector training loss is L Det , the overall loss L can be expressed as:where the λ cc is the weight of the contrastive learning loss and is set to 0.5.Memory Update Strategy. Memory Update Strategy directly influences the representativeness of the class memory m c and further the prototype feature vector p c . Therefore, PPG adopts a novel Memory Update Strategy, which follows the idea that ""The Memory Module should preserve the more representative feature vector among similar feature vectors"". The pipeline of the strategy is as follows: 1) Acquisition the lesion feature vector f c . 2) Identification of the most similar f s to f c from corresponding class memory m c based on similarity:3) Updating the memory by selecting more unique features f s of F = {f s , f c } compared to the class prototype feature vector p c based upon similarity:The similarity function sim(u, v) can be expressed as sim(u, v) = u T v/ u v .To initialize the memories, we empirically select 50 lesions randomly for each class. To maintain stability, we start updating the memory and calculating its loss after fixed epochs, and only the positive sample feature vector can be selected to update the memory.Pseudo-label Generation. PPG proposes to generate pseudo-labels based on the similarity between the prototype feature vectors and the feature vector of potential lesions. To be specific, PPG first detects a large number of potential lesions with a low objectiveness score threshold τ u and then matches them with all the prototype feature vectors P to find the most similar one:PPG assigns the pseudo-label c for similarity value sim(p c , f u ) greater than the similarity threshold τ s otherwise omits it. We set τ u = 0.5 and τ s = 0.5"
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,3.0,Datasets,"We contribute the first Large-scale Gstroscopic Lesion Detection Datasets (LGLDD) in the literature. Collection : LGMDD collects about 1M+ gastroscopic images from 2 hospitals of about 500 patients and their diagnosis reports. After consulting some senior doctors and surveying gastroscopic diagnosis papers [1], we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can) and sub-mucosal tumor(smt). We invite 10 senior doctors to annotate them from the unlabeled endoscopic images. To preserve the annotation quality, doctors can refer to the diagnosis reports, and each lesion is annotated by a doctor and checked by another. Finally, they annotates 12,292 lesion boxes in 10,083 images after going through about 120,000 images. The polyp, ulcer, cancer, and sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. The train/val split of LGMDD is 8,076/2,007. The other data serves as unlabeled data.Evaluation Metrics : We use standard object detection metrics to evaluate the GLD performance, which computes the average precision (AP) under multiple intersection-of-union (IoU) thresholds and then evaluate the performance using the mean of APs (mAP) and the AP of some specific IoU threshold. For mAP, we follow the popular object detection datasets COCO [11] and calculate the mean of 11 APs of IoU from 0.5 to 0.95 with stepsize 0.05 (mAP @[.5:.05:.95]).We also report AP under some specific IoU threshold (AP 50 for .5, AP 75 for .75) and AP of different scale lesions (AP S , AP M , AP L ) like COCO [11]."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,4.0,Experiments,"Please kindly refer to the Supplemental Materials for implementation details and training setups. The Objectiveness score threshold controls the quality of pseudo-labels. a) A low threshold generates noisy pseudo-labels, leading to reduced performance (-0.6/-0.2 AP at thresholds 0.5/0.6). b) A high threshold produces high-quality pseudo-labels but may miss potential lesions, resulting in only slight performance improvement (+0.3 AP at threshold 0.7). c) PPG approach uses a low threshold (0.5) to identify potential lesions, which are then filtered using prototype feature vectors, resulting in the most significant performance enhancement (+0.9 AP).3) Memory Update Strategy influences the representativeness of memory and the prototype feature vectors. We compare our Memory Update Strategy with a queue-like ('Q-like') memory update strategy (first in & first out). Experiment results (Table 2.c) show our Memory Update Strategy performs better. 4) Endo21: To further evaluate the effectiveness of SSL, we conduct experiments on Endo21 [2] Sub-task 2 (Endo21 challenge consists of 4 sub-tasks and only the Sub-task 2 train/test split is available according to the [2]). Experimental results in Table 2.d show that SSL can bring significant improvements to publicly available datasets. Moreover, SSL overperforms current SOTA (YOLO v5 [2])."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,5.0,Conclusion,"In this work, we propose Self-and Semi-Supervised Learning (SSL) for GLD tailored for using massive unlabeled gastroscopic to enhance GLD performance.The key novelties of the proposed method include a Hybrid Contrastive Learning method for backbone pre-training and a Prototype-based Pseudo-Label Generation method for semi-supervised learning. Moreover, we contribute the first Large-scale GLD Datasets (LGLDD). Experiments on LGLDD prove that SSL can bring significant improvements to GLD performance. Since annotation cost always limits of datasets scale of such tasks, we hope SSL and LGLDD could fully realize its potential, as well as kindle further research in this direction."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,,Table 2 .,2) Objectiveness score threshold τ u : We compare PPG with objectiveness score-based pseudo-label generation methods with different τ u (Table 2.b).
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,1.0,Introduction,"Precision medicine permits more informed treatment decisions to be made based on individual patient characteristics (e.g. age, sex), with the goal of improving patient outcomes. Deep causal models based on medical images can significantly improve personalization by learning individual, data-driven features to predict the effect of treatments. 1 As a result, they could significantly improve patient outcomes, particularly in the context of chronic, heterogeneous diseases [18], potentially non-invasively.However, despite significant advances, predictive deep learning models for medical image analysis are not immune to error, and severe consequences for the patient can occur if a clinician trusts erroneous predictions. A provided measure of uncertainty for each prediction is therefore essential to trust the model [26]. Although uncertainty is now commonly embedded in predictive medical image analysis (e.g. [1,15,22]), it is not well-studied for precision medicine.Image-based precision-medicine is highly relevant in multiple sclerosis (MS), a chronic disease characterized by the appearance over time of new or enlarging T2 lesions (NE-T2) on MRI [9,17]. Several treatment options exist to suppress future NE-T2 lesions, but their level of efficacy and side effects are heterogeneous across the population [12]. Although one other model has been proposed for estimating the individual treatment effect (ITE) based on MR images [2], it does not incorporate uncertainty. Figure 1 illustrates how knowledge of the model's uncertainty could improve treatment recommendations.To integrate uncertainty into clinical decision making, new validation measures must be defined. The usual strategy for validating uncertainty estimates, discarding uncertain predictions [8,14] and examining performance on the remaining predictions, is not always appropriate when predicting treatment effects. For example, discarding uncertain predictions could result in discarding predictions for the most responsive of individuals. A better strategy for this individual would be to consider the level of response and uncertainty jointly when making a treatment decision.In this work, we present the first uncertainty-aware causal model for precision medicine based on medical images. We validate our model on a large, multi-center dataset of MR images from four different randomized clinical trials (RCTs) for MS. Specifically, we develop a multi-headed, Bayesian deep learning probabilistic model [13] which regresses future lesion counts, a more challenging task than classification, but which provides more fine-grained estimates of treatment effect. We evaluate the model's uncertainty by showing correlation of predictive uncertainty on factual and counterfactual error, and demonstrate how to bound the treatment effect error using group-level ground truth data to evaluate its correlation with the predicted personalized treatment effect. We then show the use of incorporating predictive uncertainty to improve disease outcomes by better treatment recommendations. Lastly, we demonstrate how uncertainty can be used to enrich clinical trials and increase their statistical power [21]."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.1,Background on Individual Treatment Effect Estimation,"We frame precision medicine as a causal inference problem. Specifically, we wish to predict factual outcomes (on the treatment a patient received), counterfactual outcomes (on treatments a patient did not receive), as well as the individual treatment effect (ITE, the difference between the outcomes on two treatments). Let X ∈ R d be the input features, Y ∈ R be the outcome of interest, and T ∈ {0, 1, ..., m} be the treatment allocation with t = 0 as a control (e.g. placebo) and the remaining are m treatment options. Given a dataset containing triples D = {(x i , y i , t i )} n i=1 , the ITE for patient i and a drug T = t can be defined using the Neyman/Rubin Potential Outcome Framework [16] as ITE t = y ty 0 , where y t and y 0 represents potential outcomes on treatment and control, respectively. The ITE t is an unobservable causal quantity because only one of the two potential outcomes is observed. The average treatment effect (ATE t ) is defined asand is an observable quantity. Treatment effect estimation in machine learning therefore relies on a related causal estimand, τ t :τ t (x)2 can be identified from RCT data (as in our case), where (y 0 , y t ) ⊥ ⊥ T |X [5]. Individual treatment outcomes y t and y 0 , and ITE t , can therefore be estimated using machine learning models such that ITE t (x) = ŷt (x) -ŷ0 (x) [11]."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.2,Probabilistic Model of Individual Treatment Effects,"In this work, we seek to learn the probability distribution of individual potential outcome predictions ŷt (x) and the effect estimates ITE t (x). Let ŷt (x) ∼ N ( μt (x), σt 2 (x)) be a normal distribution for potential outcome predictions whose parameters are outputs of a neural network. This probabilistic framework conveniently allows for propagating the uncertainty estimates for each potential outcome to an uncertainty estimate for personalized treatment effects.Assuming independence between the two Gaussian distributions, ITE t (x) ∼ N ( μt (x) -μ0 (x), σt (x) 2 + σ0 (x) 2 ).For our specific context, the input x to our model consists of multi-sequence patient MRI, lesion maps, and clinical and demographic features at baseline. The model is based on a multi-headed network for treatment response estimation [2,19]. Each head predicts μt (x) and σt 2 (x) for a particular treatment. For the case of MS, the model maximizes the log likelihood of the observed number of log NE-T2 lesions formed between 1 year and 2 years in the future (Fig. 2). Fig. 2. Multi-head ResNet architecture for treatment effect prediction (based on [2]). It is modified to generate probabilistic estimates of individual outcomes. Specific architecture details can be found in the Appendix."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.3,Evaluating Probabilistic Predictions,"Bounds for the ITE Error We can validate the quality of the estimated uncertainty for factual outcome predictions through the correlation between predictive uncertainty and Mean Squared Error (MSE) error. However, given that ground truth for the individual treatment effects are not available, we cannot compute MSE between ITE t and ITE t (x). In this work, we choose to compute the upper and lower bounds for this MSE. We validate our uncertainty estimates by showing that selecting patients with the highest confidence in their predictions reduces the bounds on the ITE error. The bounds serve as an approximation to the true ITE error, and can validate models even if the ground truth ITE is not available. We use the upper bound for the MSE as in [19]. Jensen's inequality can be used to obtain a lower bound on the MSE as follows:Evaluating Individual Treatment Recommendations. Predictive uncertainty can be used to improve treatment recommendations for the individual. Let π(x i , t i ) ∈ {0, 1} be a treatment recommendation policy taking as input a patient's features x i and their factual treatment assignment t i . The binary output of π(x i , t i ) denotes whether t i is recommended under π. In this work, we set π to be a function of the model's predictions, μt (x) and σt (x). For example, π can be defined such that a treatment is recommended if the number of predicted NE-T2 lesions on a particular treatment are less than 2 [4]. An uncertainty aware policy could instead recommend a drug according to P ( ŷt (x) < 2). The expected response under proposed treatments (ERUPT) [27] can then be used to quantify the effectiveness of that policy:For the example of NE-T2 lesions, a lower value for ERUPT is better because there were fewer lesions on average for patients on the recommended treatment.Uncertainty for Clinical Trial Enrichment. Enriching a trial with predicted responders has been shown to increase statistical power in the context of MS [3]. We measure the statistical power by the z-score: ATE t / Var(y t ) + Var(y 0 ).Where Var(y t ) is the variance of factual outcomes on treatment t. The approach taken by [3] achieves higher statistical power by selecting a subset of the population with larger ATE t . Our proposed uncertainty-based enrichment selects patients with lower ITE uncertainty ( σt (x) 2 + σ0 (x) 2 ), with the goal of reducing the population variance (Var(y t )+Var(y 0 )). The benefit of this approach is most apparent if we inspect a specific population (defined by a particular value for ATE in the numerator).3 Experiments and Results"
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.1,Dataset,"The dataset is composed of patients from four randomized clinical trials: BRAVO [25], OPERA 1 [6], OPERA 2 [6], and DEFINE [7]. Each trial enrolled patients with relapsing-remitting MS. Each patient sample consists of multi-sequence patient MRI (T1 weighted pre-contrast, T1 weighted post-contrast, FLAIR, T2-weighted, and proton density weighted), lesion maps (T2 hyperintense and gadolinium-enhancing lesions), as well as relevant clinical and demographic features (age, sex, expanded disability status scale scores [10]) at baseline. The number of NE-T2 lesions between 1 and 2 years after trial initiation were provided for each patient. Next we evaluate the correlation between the model error and the predicted variance. An accurate uncertainty estimate should be positively correlated with prediction accuracy [14]. This relationship is shown in Fig. 3a, where the MSE for the factual predictions decreases as we select a sub-group of patients with lower predictive uncertainty.Next, we examine the results for the ITE error. Figure 3b and Fig. 3c show the upper and lower bounds (Eq. 2). Similarly to the factual error, the lower bound and upper bound on the ITE error decrease with decreasing ITE uncertainty. "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.3,Uncertainty for Individual Treatment Recommendations,"The effect of integrating uncertainty into treatment recommendations is evaluated by defining a policy using this uncertainty (Eq. 3). Here, we report outcomes on the lesion values (as opposed to log-lesions) for interpretability. In Fig. 4a, a treatment, laquinimod 3 , is recommended if the predicted probability of having fewer than 2 NE-T2 lesions is greater than a threshold k: P ( ŷt (x) < 2) > k. A policy requiring greater confidence indeed selects patients who more often have fewer than 2 lesions. It is worth noting that laquinimod was not found to be efficacious at the whole group level in clinical trials [25] and is therefore not approved, but this analysis shows that using personalized recommendations based on uncertainty can identify a sub-group of individuals that can benefit.In Fig. 4b, a treatment effect-based policy is used such that laquinimod is recommended if the probability of any treatment response is greater than a threshold k: P ( ITE t (x) ≤ 0) > k. As certainty in response grows, the difference between the treated and placebo groups grows suggesting an uncertainty aware policy better identifies patients for which the drug will have an effect. Uncertainty can be useful when we wish to attribute a cost, or risk, to a certain range of outcomes. In our case, we assume a hypothetical non-linear cost c for having more NE-T2 lesions, where c = (NE-T2 lesions + 1) 2 . Figure 5a describes a case where the recommended treatment (in terms of the mean) changes if this cost transformation is applied. In this case, the shape of the distribution over possible outcomes (which informs our uncertainty about this outcome) affects how much the mean of the distribution shifts under this transformation. This analysis is extended to the entire laquinimod cohort in Fig. 5b. We compute the average cost (Eq. 3) rather than the number of future NE-T2 lesions (as in Fig. 4a) for two types of policies. In the uncertainty-aware policy, the predicted distribution is used to make the treatment decision, whereas for the mean policy, the decision is based on only on μt (x). As expected, uncertaintyaware recommendations incur a lower expected cost across the entire cohort compared to the mean policy. The advantage is most visible for intermediate values on the x-axis, because at the far right all patients are recommended laquinimod, and at the far left patients have closer to 0 NE-T2 lesions on average and the magnitude of the improvement due to the uncertainty-aware policy lessens. "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.4,Uncertainty for Clinical Trial Enrichment,"Uncertainty estimation can also be useful when selecting a sub-population of to enroll in a clinical trial, in a technique called predictive enrichment [20]. Figure 1b, shows an example where two patients have similar estimated future lesions but different ITE uncertainties. For trial enrichment, the second patient is more likely to experience a significant effect from this drug, and therefore enriching the trial with such patients could increase it's statistical power to detect an effect if done appropriately (see Sect. 2.3). In Fig. 6 we show the effect of uncertainty-aware trial enrichment. For a population with a particular effect size, we remove patients (right to left) with high ITE uncertainty and compute the z-score between the untreated and treated populations for the remaining Fig. 6. To isolate the effect of uncertainty on enrichment, we fixed the ATE to be equal to 0 to -2 NE-T2 lesions by including patients with fixed placebo (2 < ŷ0(x) < 3) and treatment (1 < ŷt(x) < 2) outcomes. The z-score then decreases for patient groups with smaller predicted ITE uncertainty.groups. As expected, groups with smaller average ITE uncertainty have greater statistical differences (lower z scores)."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,4.0,Conclusion,"In this work, we present a novel, causal, probabilistic, deep learning framework for image-based precision medicine. Our multi-headed architecture produces distributions over potential outcomes on multiple treatment options and a distribution over personalized treatment effects. We evaluate our model on a real-world, multi-trial MS dataset, where we demonstrate quantitatively that integrating the uncertainties associated with each prediction can improve treatment-related outcomes in several real clinical scenarios compared to a simple mean prediction. The evaluation methods used in this work are agnostic to the method of uncertainty quantification which permits flexibility in the choice of measure. Overall, this work has the potential to greatly increase trust in the predictions of causal models for image-based precision medicine in the clinic."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_46.
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,1.0,Introduction,"Segmentation is a critical task in medical image analysis. Known approaches mainly utilize discrete data representations (e.g., rasterized label masks) with convolutional neural networks (CNNs) [6,8,12,26] or Transformers [9,10] to classify image entities in a bottom-up manner. While undeniably effective, this paradigm suffers from two primary limitations. (1) These approaches have limited spatial flexibility and poor computational scaling. Retrieving predictions at higher resolutions would require either increasing the input size, which decreases performance and incurs quadratic or cubic memory increases, or interpolating output predictions, which introduces discretization artifacts. (2) Per-pixel or voxel learning inadequately models object shapes/boundaries, which are central to both robust computer vision methods and our own visual cortical pathways [23]. This often results in predictions with unrealistic object shapes and locations [24], especially in settings with limited annotations and out-ofdistribution data.Instead of segmenting structures with discrete grids, we explore the use of Implicit Neural Representations (INRs) which employ continuous representations to compactly capture coordinate-based signals (e.g., objects in images). INRs represent object shapes with a parameterized function f θ : (p, z) → [0, 1] that maps continuous spatial coordinates p = (x, y, z), x, y, z ∈ [-1, 1] and a shape embedding vector z to occupancy scores. This formulation enables direct modeling of object contours as the decision boundary of f θ , superior memory efficiency [5], and smooth predictions at arbitrary resolutions that are invariant to input size. INRs have been adopted in the vision community for shape reconstruction [3,4,19,22], texture synthesis [21], novel view synthesis [20], and segmentation [11]. Medical imaging studies have also used INRs to learn organ templates [31], synthesize cell shapes [29], and reconstruct radiology images [27].The adoption of INRs for medical image segmentation, however, has been limited where most existing approaches directly apply pipelines designed for 3D reconstruction to images. These works emphasize either global embeddings z or point-wise ones. OSSNet [25] encodes a global embedding from an entire volume and an auxiliary local image patch to guide voxel-wise occupancy prediction. Although global shape embeddings facilitate overall shape coherence, they neglect the fine-grained details needed to delineate local boundaries. The local patches partially address this issue but lack contextual understanding beyond the patches and neglect mid-scale information. In an effort to enhance local acuity and contextual modeling, IFA [11], IOSNet [16], and NUDF [28] each extract a separate embedding for every input coordinate by concatenating point-wise features from multi-scale CNN feature maps. Although more expressive, point-wise features still lack sufficient global contextual understanding and suffer from the same unconstrained prediction issues observed in discrete segmentation methods. Moreover, these methods use components designed for shape reconstruction-a domain where synthetic data is abundant and the modeling of texture, multiclass discrimination, and multi-scale contexts are less crucial.To address these limitations, we propose SwIPE (Segmentation with Implicit Patch Embeddings) which learns continuous representations of foreground shapes at the patch level. By decomposing objects into parts (i.e., patches), we aim to enable both accurate local boundary delineation and global shape coherence. This also improves model generalizability and training efficiency since local curvatures often reoccur across classes or images. SwIPE first encodes an image into descriptive patch embeddings and then decodes the point-wise occupancies using these embeddings. "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.0,Methodology,"The core idea of SwIPE (overviewed in Fig. 1) is to use patch-wise INRs for semantic segmentation. To formulate this, we first discuss the shift from discrete to implicit segmentation, then delineate the intermediate representations needed for such segmentation, and overview the major components involved in obtaining these representations. Note that for the remainder of the paper, we present formulations for 2D data but the descriptions are conceptually congruous in 3D.In a typical discrete segmentation setting with C classes, an input image X is mapped to class probabilities with the same resolution f : X ∈ R H×W ×3 → Ŷ ∈ R H×W ×C . Segmentation with INRs, on the other hand, maps an image X and a continuous image coordinate p i = (x, y), x, y ∈ [-1, 1], to the coordinate's class-wise occupancy probability ôi ∈ R C , yielding f θ : (p i , X) → ôi , where f θ is parameterized by a neural network with weights θ. As a result, predictions of arbitrary resolutions can be obtained by modulating the spatial granularity of the input coordinates. This formulation also enables the direct use of discrete pixel-wise losses like Cross Entropy or Dice with the added benefit of boundary modeling. Object boundaries are represented as the zero-isosurface in f θ 's prediction space or, more elegantly, f θ 's decision boundary.SwIPE builds on the INR segmentation setting (e.g., in [16]), but operates on patches rather than on points or global embeddings (see Table 1 & left of Table 3 for empirical justifications) to better enable both local boundary details and global shape coherence. This involves two main steps: (1) encode shape embeddings from an image, and (2) decode occupancies for each point while conditioning on its corresponding embedding(s). In our case, f θ includes an encoder E b (or backbone) that extracts multi-scale feature maps from an input image, a context aggregation module E n (or neck) that aggregates the feature maps into vector embeddings for each patch, and MLP decoders D P (decoder for local patches where P is for patch) & D I (decoder for entire images where I is for image) that output smoothly-varying occupancy predictions given embedding & coordinate pairs. To encode patch embeddings in step ( 1), E b and E n map an input image X to a global image embedding z I and a matrix Z P containing a local patch embedding z P at each planar position. For occupancy decoding in step (2), D P decodes the patch-wise class occupancies o P i using relevant local and global inputs while D I predicts occupancies o I i for the entire image using only image coordinates p I i and the image embedding z I . Below, we detail the encoding of image & patch embeddings (Sect. §2.1), the point-wise decoding process (Sec. §2.2), and the training procedure for SwIPE (Sect. §2.3)."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.1,Image Encoding and Patch Embeddings,"The encoding process utilizes the backbone E b and neck E n to obtain a global image embedding z I and a matrix Z P of patch embeddings. We define an image patch as an isotropic grid cell (i.e., a square in 2D or a cube with identical spacing in 3D) of length S from non-overlapping grid cells over an image. Thus, an image X ∈ R H×W ×3 with a patch size S will produce H S • W S patches. For simplicity, we assume that the image dimensions are evenly divisible by S.A fully convolutional encoder backbone E b (e.g., Res2Net-50 [7]) is employed to generate multi-scale features from image X. The entire image is processed as opposed to individual crops [3,15,25] to leverage larger receptive fields and integrate intra-patch information. Transformers [9] also model cross-patch relations and naturally operate on patch embeddings, but are data-hungry and lack helpful spatial inductive biases (we affirm this in Sec. §3.5). E b outputs four multi-scale feature maps from the last four stages,n=2 to produce z I (the shape embedding for the entire image) and Z P (the grid of shape embeddings for patches). The feature maps are initially fed into a modified Receptive Field Block [17] (dubbed RFB-L or RFB-Lite) that replaces symmetric convolutions with a series of efficient asymmetric convolutions (e.g.,The context-enriched feature maps are then fed through multiple cascaded aggregation and downsampling operations (see E n in Fig. 1) to obtain four multi-stage intermediate embeddings with identical shapes,n=2 to patch embeddings Z P , we first resize them to Z P 's final shape via linear interpolation to produce {F n } 5 n=2 , which contain low-level (F 2 ) to high-level (F 5 ) information. Resizing enables flexibility in designing appropriate patch coverage, which may differ across tasks due to varying structure sizes and shape complexities. Note that this is different from the interpolative sampling in [16] and more similar to [11], except the embeddings' spatial coverage in SwIPE are larger and adjustable. To prevent the polarization of embeddings toward either local or global scopes, we propose a multi-stage embedding attention (MEA) module to enhance representational power and enable dynamic focus on the most relevant abstraction level for each patch. Given four intermediate embedding vectors {e n } 5 n=2 from corresponding positions in {F n } 5 n=2 , we compute the attention weights via W = Sof tmax(MLP 1 (cat(MLP 0 (e 2 ), MLP 0 (e 3 ), MLP 0 (e 4 ), MLP 0 (e 5 )))), where W ∈ R 4 is a weight vector, cat indicates concatenation, and MLP 0 is followed by a ReLU activation. The final patch embedding is obtained by, where w i is the ith weight of W.Compared to other spatial attention mechanisms like CBAM [30], our module separately aggregates features at each position across multiple inputs and predicts a proper probability distribution in W instead of an unconstrained score. The output patch embedding matrix Z P is populated with z P at each position and models shape information centered at the corresponding patch in the input image (e.g., if S = 32, Z P [0, 0] encodes shape information of the top left patch of size 32 × 32 in X). Finally, z I is obtained by average-pooling F 5 into a vector."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.2,Implicit Patch Decoding,"Given an image coordinate p I i and its corresponding patch embedding z P i , the patch-wise occupancy can be decoded with decoder D P : (p P i , z P i ) → ôP i , where D P is a small MLP and p P i is the patch coordinate with respect to the patch center c i associated with z P i and is obtained by p P i = p I ic i . But, this design leads to poor global shape predictions and discontinuities around patch borders.To encourage better global shape coherence, we also incorporate a global image-level decoder D I . This image decoder, D I : (p I i , z I ) → ôI i , predicts occupancies for the entire input image. To distill higher-level shape information into patch-based predictions, we also condition D P 's predictions on p I i and z I . Furthermore, we find that providing the source coordinate gives additional spatial context for making location-coherent predictions. In a typical segmentation pipeline, the input image X is a resized crop from a source image and we find that giving the coordinate p S i (S for source) from the original uncropped image improves performance on 3D tasks since the additional positional information may be useful for predicting recurring structures. Our enhanced formulation for patch decoding can be described as D P : (p P i , z P i , p I i , z I , p S i ) → ôP i . To address discontinuities at patch boundaries, we propose a training technique called Stochastic Patch Overreach (SPO) which forces patch embeddings to make predictions for coordinates in neighboring patches. For each patch point and embedding pair (p P i , z P i ), we create a new pair (p P i , z P i ) by randomly selecting a neighboring patch embedding and updating the local point to be relative to the new patch center. This regularization is modulated by the set of valid choices to select a neighboring patch (connectivity, or con) and the number of perturbed points to sample per batch point (occurrence, or N SPO ). con = 4 means all adjoining patches are neighbors while con = 8 includes corner patches as well. Note that SPO differs from the regularization in [3] since no construction of a KD-Tree is required and we introduce a tunable stochastic component which further helps with regularization under limited-data settings."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.3,Training SwIPE,"To optimize the parameters of f θ , we first sample a set of point and occupancy pairs {p S i , o i } i∈I for each source image, where I is the index set for the selected points. We obtain an equal number of points for each foreground class using Latin Hypercube sampling with 50% of each class's points sampled within 10 pixels of the class object boundaries. The point-wise occupancy loss, written as, where ôc i is the predicted probability for the target occupancy with class label c. Note that in practice, these losses are computed in their vectorized forms; for example, the Dice loss is applied with multiple points per image instead of an individual point (similar to computing the Dice loss between a flattened image and its flattened mask). The loss for patch and image decoder predictions iswhere α is a local-global balancing coefficient. Similarly, the loss for the SPO occupancy predictionFinally, the overall loss for a coordinate is formulated as, where β scales SPO and the last term (scaled by λ) regularizes the patch & image embeddings, respectively."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.0,Experiments and Results,"This section presents quantitative results from four main studies, analyzing overall performance, robustness to data shifts, model & data efficiency, and ablation & component studies. For more implementation details, experimental settings, and qualitative results, we refer readers to the Supplementary Material. The losses for both tasks are optimized with AdamW [18] and use α = 0.5, β = 0.1, and λ = 0.0001. For inference, we adopt MISE like prior works [16,19,25] and evaluate on a reconstructed prediction mask equal in size to the input image. D P segments boundaries better than D I , and is used to produce final predictions.For fair comparisons, all the methods are trained using the same equallyweighted Dice and Cross Entropy loss for 30,000 and 50,000 iterations on 2D Sessile and 3D BCV, resp. The test score at the best validation epoch is reported. Image input sizes were 384 × 384 for Sessile and 96 × 96 × 96 for BCV. All the implicit methods utilize the same pre-sampled points for each image. For IOSNet [16], both 2D and 3D backbones were upgraded from three downsampling stages to five for fair comparisons and empirically confirmed to outperform the original. We omit comparisons against IFA [11] to focus on medical imaging approaches; plus, IFA did not outperform IOSNet [16] on either task."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.2,Study 1: Performance Comparisons,"The results for 2D Polyp Sessile and 3D CT BCV organ segmentation are presented in Table 1. FLOPs are reported from the forward pass on a single image during training.On the smaller polyp dataset, we observe notable improvements over the bestknown implicit approaches (+6.7% Dice) and discrete methods (+2.5% Dice) with much fewer parameters (9% of PraNet [6] and 66% of IOSNet [16]). For BCV, the performance gains are more muted; however, we still marginally outperform UNETR [10] with over 20x fewer parameters and comparable FLOPs."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.3,Study 2: Robustness to Data Shifts,"In this study, we explore the robustness of various methods to specified target resolutions and dataset shifts. The left-most table in Table 2 contains results for the former study conducted on 2D Sessile, where we first analyze the effect of directly resizing outputs (Table 2 left, rows 1 to 6) when given an input image that is standard during training (384 × 384). The discrete method, PraNet, outputs 384 × 384 predictions which are interpolated to the target size (Table 2 left, rows 1 & 4). This causes more performance drop-offs than the implicit methods which can naturally vary the output size by changing the resolution of the input coordinates. We also vary the input size so that no manipulations of predictions are required (Table 2 left, rows 7 & 8), which results in steep accuracy drops.The results for the dataset shift study are given in the middle of Table 2, where CVC is another binary poly segmentation task and the liver class is evaluated on all CT scans in AMOS. Both discrete methods outperform IOSNet, which may indicate that point-based features are more prone to overfitting due to a lack of contextual regularization. Also, we highlight our method's consistent outperformance over both discrete methods and IOSNet in all of the settings."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.4,Study 3: Model Efficiency and Data Efficiency,"To analyze the model efficiency (the right-most column of charts in Table 2), we report on 2D Sessile and vary the backbone size in terms of depth and width. For data efficiency, we train using 10%, 25%, 50%, and 100% of annotations. Not only do we observe outperformance across the board in model sizes & annotation amounts, but the performance drop-off is more tapered with our method. "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.5,Study 4: Component Studies and Ablations,"The left side of Table 3 presents our ablation studies, showing the benefits enabled by context aggregation within E n , global information conditioning, and adoption of MEA & SPO. We also explore alternative designs on the right side of Table 3 for our three key components, and affirm their contributions on achieving superior performance."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,4.0,Conclusions,"SwIPE represents a notable departure from conventional discrete segmentation approaches and directly models object shapes instead of pixels and utilizes continuous rather than discrete representations. By adopting both patch and image embeddings, our approach enables accurate local geometric descriptions and improved shape coherence. Experimental results show the superiority of SwIPE over state-of-the-art approaches in terms of segmentation accuracy, efficiency, and robustness. The use of local INRs represents a new direction for medical image segmentation, and we hope to inspire further research in this direction."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_31.
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,1.0,Introduction,"Colorectal cancer is a life-threatening disease that results in the loss of millions of lives each year. In order to improve survival rates, it is essential to identify colorectal polyps early. Hence, regular bowel screenings are recommended, where endoscopy is the gold standard. However, the accuracy of endoscopic screening can heavily rely on the individual skill and expertise of the domain experts involved, which are prone to incorrect diagnoses and missed cases. To reduce the workload on physicians and enhance diagnostic accuracy, computer vision technologies, such as deep neural networks, are involved to assist in the presegmentation of endoscopic images. The UNet-like model uses skip connections that transmit only single-stage features. In contrast, our approach utilizes FPE to propagate features from all stages, incorporating a gate mechanism to regulate the flow of valuable information.Deep learning-based image segmentation methods have gained popularity in recent years, dominated by UNet [11] in the field of medical image segmentation. UNet's success has led to the development of several other methods that use a similar encoder-decoder architecture to tackle polyp segmentation, including ResUNet++ [7], PraNet [3], CaraNet [10] and UACANet [8]. However, these methods are prone to inefficient feature fusion at the decoder due to the transmission of multi-stage features without filtering out irrelevant information.To address these limitations, we propose a novel feature enhancement network for polyp segmentation that employs Feature Propagation Enhancement (FPE) modules to transmit multi-scale features from all stages to the decoder. Figure 1 illustrates a semantic comparison of our feature propagation scheme with the UNet-like model. While the existing UNet-like models use skip connections to propagate a single-scale feature, our method utilizes FPE to propagate multi-scale features from all stages in encoder. More importantly, this research highlights the usage of FPE can effectively replace skip connections by providing more comprehensive multi-scale characteristics from full stages in encoder. To further address the issue of high-level semantics being overwhelmed in the progressive feature fusion process, we also integrate a Feature Aggregation Enhancement (FAE) module that aggregates the outputs of FPE from previous stages at decoder. Moreover, we introduce gate mechanisms in both FPE and FAE to filter out redundant information, prioritizing informative features for efficient feature fusion. Finally, we propose a Multi-Scale Aggregation (MSA) module appended to the output of the encoder to capture multi-scale features and provide the decoder with rich multi-scale semantic information. The MSA incorporates a cross-stage multi-scale feature aggregation scheme to facilitate the aggregation of multi-scale features. Overall, our proposed method improves upon existing UNet-like encoder-decoder architectures by addressing the limitations in feature propagation and feature aggregation, leading to improved polyp segmentation performance.Our major contributions to accurate polyp segmentation are summarized as follows.(1) The method addresses the limitations of the UNet-like encoder-decoder architecture by introducing three modules: Feature Propagation Enhancement (FPE), Feature Aggregation Enhancement (FAE), and Multi-Scale Aggregation (MSA). ( 2) FPE transmits all encoder-extracted feature maps to the decoder, and FAE combines the output of the last stage at the decoder and multiple outputs from FPE. MSA aggregates multi-scale high-level features from FPEs to provide rich multi-scale information. (3) The proposed method achieves state-of-the-art results in five polyp segmentation datasets and outperforms the previous cutting-edge approach by a large margin (3%) on CVC-ColonDB and ETIS datasets."
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,2.0,Method,"Overview. Our proposed feature enhancement network illustrated in Fig. 2(a), is also a standard encoder-decoder architecture. Following Polyp-PVT [2], we adopt PVT [16] pretrained on ImageNet as the encoder. The decoder consists of three feature aggregation enhancement modules (FAE) and a multi-scale aggregation module (MSA). Given an input image I, we first extract the pyramidal features using the encoder, which is defined as follows,where, {P 1 , P 2 , P 3 , P 4 } is the set of pyramidal features from four stages with the spatial size of 1/4, 1/8, 1/16, 1/32 of the input respectively. Features with lower spatial resolution usually contain richer high-level semantics. Then, these features are transmitted by the feature propagation enhancement module (FPE) to yield the feature set {C 1 , C 2 , C 3 , C 4 }, which provides multi-scale information from all the stages. This is different from the skip connection which only transmits the single-scale features at the present stage. Referring to [3,10], three highest-level features generated by FPEs are subsequently fed into a MSA module for aggregating rich multi-scale information. Afterwards, feature fusion is performed by FAE in the decoder, whereby it progressively integrates the outputs from FPE and previous stages. The higher-level semantic features of the FPE output are capable of effectively compensating for the semantics that may have been overwhelmed during the upsampling process. This process is formulated as,A noteworthy observation is that the gating mechanism has been widely utilized in both FPE and FAE to modulate the transmission and integration of features. By selectively controlling the flow of relevant information, this technique has shown promise in enhancing the overall quality of feature representations [4,9]. The final features O 1 are passed through the classifier (i.e., a 1 × 1 convolutional layer) to get the final prediction result in O. Further details on FPE, FAE, and MSA will be provided in the following sections.Feature Propagation Enhancement Module. In contrast to the traditional encoder-decoder architecture with skip connections, the FPE aims to transmit multi-scale information from full stage at the encoder to the decoder, rather than single-scale features at the current stage. The FPE architecture is illustrated in Fig. 2(b). The input of the FPE includes the features from the other three stages, in addition to the feature of the current stage, which delivers richer spatial and semantic information to the decoder. However, these multi-stage inputs need to be downsampled or upsampled to match the spatial resolution of the features at the present stage. To achieve this, FPE employs a stepwise downsampling strategy, with each step only downsampling by a factor of 2, and each downsampling step is followed by a Convolutional Unit (CU) to perform feature transformation. The number of downsamplings is denoted as N = log T 2 , where T stands for the scale factor for downsampling. The CU consists of a 3 × 3 convolutional layer, a Batch Normalization layer, and an activation layer (i.e., ReLU). This strategy can be a highly effective means of mitigating the potential loss of intricate details during the process of large-scale interpolation. Similarly, this same strategy is employed in FAE.The features from the other three stages, denoted as P 1 , P 2 , and P 3 , are downsampled or upsampled to generate P 1 , P 2 , and P 3 . Instead of directly combining the four inputs, FPE applies gate mechanisms to emphasize informative features. The gate mechanism takes the form of Y = G(X ) * Y, where G (in this work, Sigmoid is used) measures the importance of each feature vector in the reference feature X ∈ R H×W . By selectively enhancing useful information and filtering out irrelevant information, the reference features X assist in identifying optimal features Y at the current level. The output of G is in [0, 1] H×W , which controls the transmission of informative features from Y or helps filter useless information. Notably, X can be Y itself, serving as a reference feature. FPE leverages such gate mechanism to obtain informative features in P and passes them through a CU respectively. After that, FPE concatenates features from the four branches to accomplish feature aggregation. A CU is followed to boost the feature fusion.Feature Aggregation Enhancement Module. The FAE is a novel approach that integrates the outputs of the last stages at the decoder with the FPE's outputs at both the current and deeper stages to compensate for the high-level semantics that may be lost in the process of progressive feature fusion. In contrast to the traditional encoder-decoder architecture with skip connections, the FAE assimilates the output of the present and higher-stage FPEs, delivering richer spatial and semantic information to the decoder.The FAE, depicted in Fig. 2(c), integrates the outputs of the current and deeper FPE stages with high-level semantics. As an example, the last FAE takes as inputs O 2 (output of the penultimate FAE), C 1 (output of the current FPE stage), and C 2 and C 3 (outputs of FPE from deeper stages). Multiple outputs from deeper FPE stages are introduced to compensate for high-level semantics. Furthermore, gate mechanisms are utilized to filter out valueless features for fusion, and the resulting enhanced feature is generated by a CU after concatenating the filtered features. Finally, O 2 is merged with the output feature through element-wise summation, followed by a CU to produce the final output feature O 1 .Multi-Scale Aggregation Module. The MSA module in our proposed framework, inspired by the Parallel Partial Decoder in PraNet [3], integrates three highest-level features C 2 , C 3 , and C 4 from the FPE output. This provides rich multi-scale information for subsequent feature aggregation in the FAE and also helps to form a coarse localization of polyps under supervision. It benefits from an additional supervision signal, as observed in PraNet [3], CaraNet [8], and etc. by aiding in forming a coarse location of the polyp and contributing to improved accuracy and performance. As depicted in Fig. 2(d), the MSA module first processes these three features separately. C 2 , which has the highest feature resolution, is processed with multiple dilated convolutions to capture its multiscale information while keeping its spatial resolution unchanged. C 3 is processed with only one dilated convolution due to its higher spatial resolution, while C 4 is not processed since it already contains the richest contextual information. The output features of the three branches are then upsampled to the size of C 2 . To better integrate these three multi-scale high-level features for subsequent fusion, additional cross-feature fusion operations (i.e., 2 CU layer) are performed in the MSA module."
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,3.0,Experiments,"Datasets. We conduct extensive experiments on five polyp segmentation datasets, including Kvasir [6], CVC-ClinicDB [1], CVC-ColonDB [13], ETIS [12] and CVC-T [14]. Following the setting in [2,3,5,10,10,17,19], the model is trained using a fraction of the images from CVC-ClinicDB and Kvasir, and its performance is evaluated by the remaining images as well as those from CVC-T, CVC-ColonDB, and ETIS. In particular, there are 1450 images in the training set, of which 900 are from Kvasir and 550 from CVC-ClinicDB. The test set contains all of the images from CVC-T, CVC-ColonDB, and ETIS, which have 60, 380, and 196 images, respectively, along with the remaining 100 images from Kvasir and the remaining 62 images from CVC-ClinicDB.Implementations. We utilize PyTorch 1.10 to run experiments on an NVIDIA RTX3090 GPU. We set an initial learning rate to 1e-4 and halve it after 80 epochs. We train the model for 120 epochs. The same multi-scale input and gradient clip strategies used in [3,8,10] are employed in the training phase. The batch size is 16 by default. AdamW is selected as the optimizer with a weight decay of 1e-4. We adopt the same data augmentation techniques as UACANet [8], including random flip, random rotation, and color jittering. In evaluation phase, we mainly focus on mDice, mIoU, the two most common metrics in medical image segmentation, to evaluate the performance of the model. Referring to [3,8,10], we use a combination loss consisting of weighted Dice loss and weighted IoU loss to supervise network optimization. Comparison with State-of-the-Art Methods. We compared our proposed method with previous state-of-the-art methods. According to the experimental settings, the results on CVC-ClinicDB and Kvasir demonstrate the learning ability of the proposed model, while the results on CVC-T, CVC-ColonDB, and ETIS demonstrate the model's ability for cross-dataset generalization. The experimental results are listed in Table .1. It can be seen that our model is slightly inferior to Polyp-PVT on the CVC-ColonDB, but the gap is quite small, e.g., 0.6% in mDice and 0.4% in mIoU. On Kvasir, we are ahead of the previous best model by 1.1% in mDice and 1.6% in mIoU. This shows that our model is second to none in terms of learning ability, which demonstrates the effectiveness of our model. Furthermore, our proposed method demonstrates strong cross-dataset generalization capability on CVC-T, CVC-ColonDB, and ETIS datasets, with particularly good performance on the latter two due to their larger and more representative datasets. Our model outperforms state-of-the-art models by 2.9% mDice and 3.2% mIoU on CVC-ColonDB and 3.5% mDice and 4.0% mIoU on ETIS. These results validate the effectiveness of feature-level enhancement and highlight the superior performance of our method. We also provide visual results in Fig. 3, where our predictions are shown to be closer to the ground truth.Ablation Study. We carried out ablation experiments to verify the effectiveness of the proposed FPE, FAE, and MSA. For our baseline, we use the simple encoder-decoder structure with skip connections for feature fusion and perform element-wise summation at the decoder. Table 2 presents the results of our ablation experiments. Following the ablation study conducted on our proposed approach, it is with confidence that we assert the significant contribution of each module to the overall performance enhancement compared to the baseline. Our results indicate that the impact of each module on the final performance is considerable, and their combination yields the optimal overall performance. Specifically, across the five datasets, our proposed model improves the mDice score by at least 1.4% and up to 3.4% on CVC-T, compared to the baseline. For mIoU, the improvements are 1.5% and 3.5% on the corresponding datasets. In summary, our ablation study underscores the crucial role played by each component of our approach, and establishes its potential as a promising framework for future research in this domain.  "
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,4.0,Conclusion,"We introduce a new approach to polyp segmentation that addresses inefficient feature propagation in existing UNet-like encoder-decoder networks. Specifically, a feature propagation enhancement module is introduced to propagate multiscale information over full stages in the encoder, while a feature aggregation enhancement module is attended at the decoder side to prevent the loss of high-level semantics during progressive feature fusion. Furthermore, a multiscale aggregation module is used to aggregate multi-scale features to provide rich information for the decoder. Experimental results on five popular polyp datasets demonstrate the effectiveness and superiority of our proposed method. Specifically, it outperforms the previous cutting-edge approach by a large margin (3%) on CVC-ColonDB and ETIS datasets. To extend our work, our future direction focuses on exploring more effective approaches to feature utilization, such that efficient feature integration and propagation can be achieved even on lightweight networks."
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,,Table 2 .,90.2/83.5 93.0/88.4 92.1/87.2 81.5/73.4 80.9/72.9 90.0/83.6 92.7/88.0 92.6/87.8 81.2/73.1 81.6/73.9 90.5/83.9 93.1/88.5 92.8/88.0 83.7/75.9 82.2/74.6
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,1.0,Introduction,"Nowadays, brain imaging genetics has gained increasing attention in the neuroscience area. This interdisciplinary field refers to integrates genetic variations (single nucleotide polymorphisms, SNPs) and structural or functional neuroimaging quantitative traits (QTs). Different imaging technologies can capture different knowledge of the brain and thus are a better choice in brain imaging genetics [12,17]. Over the past decade, genome-wide association studies (GWAS) have proven to be a powerful tool in finding the genetic effects on imaging phenotypes in single SNP level [7,10,14]. However, GWAS can only investigate the single-SNP-single-QT relationship, and thus may lose the information among multiple SNPs and/or multiple QTs due to the polygenic inheritance of brain disorders [9].To leverage the multi-modal brain imaging QTs and identify the joint effect of multiple SNPs, many learning methods were proposed for multi-modal brain imaging genetics [5,6,15]. The dirty multi-task sparse canonical correlation analysis (DMTSCCA) is a bi-multivariate learning method for multi-modal brain imaging genetics [4]. DMTSCCA can disentangle the specific patterns of multimodal imaging QTs from shared ones and thus is state-of-the-art. However, DMTSCCA depends on individual-level imaging and genetic data, and cannot work when the original imaging and genetic data are unavailable.Since GWAS studies usually release their summary statistics results for academic use, we here developed a novel DMTSCCA method using GWAS summary statistics rather than individual data. The method, named S-DMTSCCA, has the same ability as DMTSCCA in modeling the association between multimodal imaging QTs and SNPs and does not require raw imaging and genetic data. We investigated the performance of S-DMTSCCA based on two kinds of experiments. Firstly, we applied S-DMTSCCA to GWAS summary statistics from Alzheimer's Disease Neuroimaging Initiative (ADNI) and compared it to DMTSCCA which directly ran on the original imaging genetic data of ADNI. Results suggested that S-DMTSCCA and DMTSCCA obtained equivalent results. Secondly, we applied S-DMTSCCA to a GWAS summary statistics from the UK Biobank. The experiment results showed that S-DMTSCCA can identify meaningful genetic markers for brain imaging QTs. More importantly, the structure information of SNPs was also captured which was usually missed by GWAS. It is worth noting that all these results were obtained without assessing the original neuroimaging genetic data. This demonstrates that our method is a powerful tool and provides a novel method for brain imaging genetics."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.0,Method,"In this article, we represent scalars with italicized letters, column vectors with boldface lowercase letters, and matrices with boldface capitals. For X = (x ij ), the ith row is denoted as x i , jth column as x j , and the ith matrix as X i . X 2 denotes the Euclidean norm, X 2,1 denotes the sum of the Euclidean norms of the rows of X. Suppose X ∈ R n×p load the genetic data with n subjects and p biomarkers, and Y c ∈ R n×q (c = 1, ..., C) load the cth modality of phenotype data, where q and C is the number of imaging QTs and imaging modalities (tasks) respectively."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.1,DMTSCCA,"DMTSCCA identifies the genotype-phenotype associations between SNPs and multi-modal imaging QTs using the following model [4],(In this model,) is a weight vector to balance among multiple sub-tasks. In this paper, κ ensures an equal optimization for each imaging modality. v c is the cth vector in V, where V = [v 1 , ..., v C ] ∈ R q×C denotes the canonical weight for phenotypic data. S ∈ R p×C and W ∈ R p×C are the canonical weights for genotypes, where S is the task-consistent component being shared by all tasks and W is the task-dependent component being associated with a single task. λ v , λ s and λ w are nonnegative tuning parameters."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.2,Summary-DMTSCCA (S-DMTSCCA),"Now we propose S-DMTSCCA only using summary statistics from GWAS. It does not need individual-level imaging and genetic data.For ease of presentation, we derive our method by first introducing GWAS. GWAS uses linear regression to study the effect of a single SNP on a single imaging QT. Let x d denotes the genotype data with p SNPs and y l denotes the phenotype data with q imaging QTs, a typical GWAS model can be defined aswhere b dl is the effect size of the d-th SNP on the l-th imaging QT. α is the y-intercept, and is the error term which is independent of x d . When the SNPs and imaging QTs were normalized to have zero mean and unit variance, b dl will equal to the covariance between x d and y l , i.e., b dl =x T d y l n-1 . On this account, we can construct B ∈ R p×q by loading p×q summary statistics of GWAS. Obviously, B will be the covariance between multiple SNPs and multiple imaging QTs since its element is the covariance of a single SNP and a single imaging QT. Let B c denotes covariance of the c-th modality from GWAS, we haveFurther, we use ΣXX denote an estimated covariance of genetic data, i.e., ΣXX = X T X n-1 . X can be obtained from n subjects of the same or similar population since we do not have the original data.According the phenotype correlation [8], the covariance of phenotype data of the c-th modality can be calculated byLet s * c , w * c and v * c denote the final results, we will present how to solve them only using GWAS results (B) and several subjects of a public reference database.Solving S and W: Since our method is bi-convex, we can solve one variable by fixing the remaining variables as constants. The model of S-DMTSCCA and DMTSCCA are the same [4], and thus we can solve each ŝc and ŵc by substituting Σ XX , Σ Y Y , Σ XY and Σ Y X . Specifically, we have the following closed-form solution, ŝc =In both equations, D and D are diagonal matrices, and their i-th diagonal element are 1 2 s i 2 and . (7) Solving V: If S and W are solved, we can fix them to solve for V. In line with Eqs.(5-6), substituting Eq. (3) and Eq. ( 4) into the equation of vc in [4], we can get the solution formulas for V, i.e.,Q is a diagonal matrix with the j-th element being 1 2(n-1)|vjc| (j = 1, ..., q). Now we have obtained all the solutions to DMTSCCA without using original imaging and genetic data. In contrast, we use the GWAS summary statistics to obtain the covariance between imaging QTs and SNPs. The in-set covariance Σ Y Y can also be calculated based on the results of GWAS. The in-set covariance Σ XX can be approximated using subjects of the same population. In this paper, we used the public 1000 genome project (1kGP) database to generate ΣXX . In practice, using ΣXX of the same population could yield acceptable results [1,13]. Therefore, our S-DMTSCCA is quite meaningful since it does not depend on raw neuroimaging genetic data."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,3.0,Experiment and Results,"We conducted two kinds of experiments to evaluate S-DMTSCCA. First, we used the ADNI data set where the original brain imaging phenotypes and genotypes are available. Specifically, our method cannot access individual-level imaging and genetic data. Instead, S-DMTSCCA can only work on the GWAS summary statistics obtained from this ADNI data set. At the same time, DMTSCCA directly ran on the original imaging and genetic data. By comparison, we can observe the performance difference between S-DMTSCCA and DMTSCCA. This can help evaluate the usefulness of our method. Second, we ran our method on a public GWAS result which studied the associations of imaging phenotypes and SNPs from the UK Biobank.To find the best parameters for λ s , λ w and λ v in DMTSCCA, we employed the grid search strategy with a moderate candidate parameter range 10 i (i = -5, -4, ..., 0, ..., 4, 5). Since S-DMTSCCA takes summary statistics as the input data without using the individual data, the conventional regularization parameters procedure is impracticable. Therefore, we used the grid search method with the same range based on the data set whose individual-level data was accessible to find the optimal parameters for S-DMTSCCA. Besides, to ensure equal optimization for each imaging modality, we used the same constants for the task weight parameters κ c in different sub-tasks.We used the 1kGP data sets as the reference samples. By conducting wholegenome sequencing on individuals from a range of ethnicity, the 1kGP institute obtains an extensive collection of human prevalent genetic variants [3]. We used individuals of British in England and Scotland (GBR) from 1kGP (release 20130502) to compute ΣXX in S-DMTSCCA since UK Biobank GWAS results from the European ancestors. All experiments ran on the same platform, and all methods employed the same stopping condition, i.e. both max c |("
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,3.1,Study on the ADNI Dataset,"Data Source. The individual-level brain genotype and imaging data we used were downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). One goal of ADNI is to investigate the feasibility of utilizing a multi-modal approach that combines serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment to measure the progression of Alzheimer's disease (AD). For the latest information, see https://www.adniinfo.org. We used three modalities, i.e. the 18-F florbetapir PET (AV45) scans, fluorodeoxyglucose positron emission tomography (FDG) scans, and structural MRI (sMRI) scans. These data had been aligned to the same visit of each subject. The sMRI data were analyzed with voxel-based morphometry (VBM) by SPM. All scans were aligned to a T1-weighted template image, segmented into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) maps, normalized to the standard MNI space, and smoothed with an 8 mm FWHM kernel. Additionally, the FDG and AV45 scans were registered into the same MNI space. Then 116 regions of interest (ROIs) level measurements were extracted based on the MarsBaR automated anatomical labeling (AAL) atlas. These 116 imaging QTs were pre-adjusted to remove the effects of the baseline age, gender, education, and handedness by the regression weights generated from healthy controls. The genotype data were also from the ADNI database. Specifically, we studied 5000 SNPs of chromosome 19: 46670909 -46167652 including the well-known AD risk genes such as APOE [11]. As a methodology paper, we aimed to develop a GWAS summary statistics-based imaging genetics method that could handle GWAS summary statistics rather than individual-level imaging and genetic data. Thus, although using a large number of SNPs could be more interesting, it might go beyond the key research topic of this paper. The goal of the method was to explore the relationships between the multiple modalities of QTs (GM densities for VBM scans, amyloid values for AV45 scans and glucose utilization for FDG scans) and SNPs. For clarity, we respectively denoted the canonical weights of imaging QTs for AV45-PET, FDG-PET, and VBM-MRI as v 1 , v 2 and v 3 .Biomarkers Identification. We presented the identified SNPs and imaging QTs for each imaging modality based on the estimated canonical weights in Fig. 1 (a, b, andc). DMTSCCA and S-DMTSCCA decomposed the canonical weight of SNPs into two components, i.e., the multi-modal shared component S and modality-specific component W. Thus, we presented both of them here. Both S and W of S-DMTSCCA identified the famous AD-related SNP rs429358, and the top SNPs marked in this figure were all related to AD, demonstrating the effectiveness of our method. In addition, S-DMTSCCA presented a task-consistent pattern, indicating that SNPs such as rs12721051 (APOC1 ), rs56131196 (APOC1 ) and rs44203638 (APOC1 ) could contribute to all three imaging modalities. S-DMTSCCA also presented a task-specific pattern, including rs7256200 (APOC1 ), rs73052335 (APOC1 ) and rs10414043 (APOC1 ) which were associated with Av45 and FDG scans, rs483082 (APOC1 ) which was associated with Av45 and VBM-MRI scans, rs12721046 (APOC1 ) and rs5117 (APOC1 ) which were only associated with VBM imaging scans. Importantly, our method identified the same top SNPs as the conventional DMTSCCA, suggesting that S-DMTSCCA possesses an equivalent feature selection capacity to DMTSCCA. In the heat maps of imaging QTs, S-DMTSCCA was able to identify different biomarkers for different scans as the conventional one. For example, the Frontal-Med-Orb-Right and Frontal-Med-Orb-Left of AV45-PET scans, Cingulum-Post-Right of FDG-PET scans and Hippocampus-Left of VBM-MRI scans. All in all, S-DMTSCCA presented a good agreement with the conventional method in feature selection. These results demonstrated that our method could be a very promising and meaningful method in multi-modal brain imaging genetics since it did not use individual-level brain imaging genetic data. Bi-multivariate Associations. We summarized the CCCs of conventional DMTSCCA and our method in Table 1. The values here represented the strength of the identified correlations. Since we have 3 imaging modalities in this section, 3 groups of CCCs were shown. Firstly, we can see that all CCCs of S-DMTSCCA were comparable to those of DMTSCCA, and all CCCs were relatively high (>0.2). These results indicated that our method can identify equivalent bimultivariate associations between genetic variants and multi-modal phenotypes without using individual-level data.  [7]. We used three modalities of imaging QTs, including two structural volumetric measurements obtained by FreeSurfer, i.e., the Desikan-Killiany-Tourville atlas and the Destrieux atlas, and one from resting-state functional MRI (rfMRI). We used summary statistics of these three modalities of the whole brain and denoted their canonical weights as v 1 , v 2 and v 3 respectively. In particular, there were 148,64 and 76 imaging QTs for three imaging modalities. Genotype data for these imaging QTs were obtained from the UKB database [2]. We used 5000 SNPs in chromosome 10: 95952196 -96758136 and chromosome 14: 59072418 -59830880. We aimed to evaluate our method with the expectation to gain a comprehensive understanding of the genetic basis of multi-modal brain imaging phenotypes.Biomarkers Identification. Figure 1 (d ande) presented the heat maps of the identified SNPs and imaging QTs for all three modalities, with modalityconsistent and modality-specific SNPs separately shown. In this figure, S identified rs9419788 (PLCE1 ) in chromosome 10, and rs1252916 (DAAM1 ) and rs4258526 (LINC01500 ) in chromosome 14. This implied that these loci are shared by all three modalities. In addition, in heat maps of W, rs7080472 (PLCE1 ) only can be identified by rfMRI scans, and rs74826997 (LINC01500 ) and rs73313052 only can be identified by the other two scans. All these identified biomarkers shared by three modalities or related to a specific modality were consistent with the results of GWAS. This suggested that S-DMTSCCA can select leading genetic variations contributing to related imaging QTs. From the heat maps of imaging QTs, our method simultaneously identified the specific related imaging QTs for a specific task while GWAS cannot. These specific patterns included the a2009s-lh-G-cuneus-area for a2009s imaging QTs, the DKTatlas-rh-lingual-area for DKT IDPs, and rfMRI-NODEampes25-0005 for rfMRI scan. In summary, these results demonstrated that S-DMTSCCA can simultaneously identify the important genetic variants and imaging phenotypes of multiple modalities only depending on summary statistics data.Bi-multivariate Associations. In addition to feature selection, we calculated CCCs of S-DMTSCCA. The values for the three modalities were 0.1455, 0.1354, and 0.1474 respectively. This indicated that S-DMTSCCA could identify substantial bi-multivariate associations for each modality which might be attributed to its good modeling capability. These results again suggested that S-DMTSCCA can work well with summary statistics."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,4.0,Conclusion,"In brain imaging genetics, DMTSCCA can identify the genetic basis of multimodal phenotypes. However, DMTSCCA depends on individual-level genetic and imaging data and thus was infeasible when the raw data cannot be obtained. In this paper, we developed a source-free S-DMTSCCA method using GWAS summary statistics rather than the original imaging and genetic data. Our method had the same modeling ability as the conventional methods. When applied to multi-modal phenotypes from ADNI, S-DMTSCCA showed an agreement in feature selection and canonical correlation coefficient results with DMTSCCA. When applied to multiple modalities of GWAS summary statistics, our method can identify important SNPs and their related imaging QTs simultaneously. In the future, it is essential to consider the pathway and brain network information in our method to identify higher-level biomarkers of biological significance."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,1.0,Introduction,"Myopia, resulting in blurred distance vision, is one of the most common eye diseases, with a rising prevalence around the world, particularly among schoolchildren and young adults [11,19,22]. Common wisdom attributes the causes of myopia to excessive elongation of the eyeball, the development of which can continue throughout childhood, and especially in patients with high myopia, throughout adulthood [1,5]. In the coming decades, the prognosis for patients with high myopia will continue to deteriorate, with some developing pathological myopia, leading to irreversible vision damage involving posterior scleral staphyloma, macular retinoschisis, macular hole, retinal detachment, choroidal neovascularization, dome-shaped macula, etc. [3,7,12,23].As a crucial tool in the study of high myopia, fundus images demonstrate the retinal changes affected by myopia. Myopic maculopathy in color fundus photographs (CFP) can be important evidence in the evaluation of high myopia and myopic fundus diseases [21]. However, some myopic macular lesions such as myopic traction maculopathy and domeshaped macula are usually not observed in CFP. Optical coherence tomography (OCT), characterized by non-invasive and high-resolution three-dimensional retinal structures, has more advantages in the examination of high myopia [9,15]. Several studies have used convolutional neural networks to automatically diagnose high myopia and pathological myopia [6,16]. Choi et al. employed two ResNet-50 [10] networks to inference vertical and horizontal OCT images simultaneously. Li et al. introduced focal loss into Inception-Resnet-V2 [24] to enhance its identification ability. However, the different classes of images in these tasks have significant differences, and the performance of the model, when used for more complex screening scenarios, is not discussed. Hence, this work aims to design a highly generalizable screening model for high myopia on OCT images.There exist challenges to developing an automatic model that meets certain clinical conditions. For the inclusion criteria for high myopia, different studies will expect different outputs under different thresholds. High myopia is defined by a spherical equivalent (SE) ≤ -6.0 dioptres (D) or an axial length (AL) ≥ 26.0mm in most cases, but some researchers set the threshold of SE to -5.0D [4] or -8.0D [20]. Moreover, some scenarios where screening risk needs to be controlled may modify the thresholds appropriately. To remedy this issue, the screening scheme should ideally output compliant results for different thresholds. For the accuracy of supervision, although a worse SE has a higher chance of causing structural changes in the retina (and vice versa), the two are not the single cause and effect, i.e., there are natural label noises when constructing datasets. One direct piece of evidence is that both measures of SE and AL can be used as inclusion criteria for high myopia, but there is disagreement in some samples. We illustrate the distribution of samples with both SE and AL in our dataset in Fig. 1(a). Clinical experience considers that AL and SE can be roughly estimated from each other using a linear relationship, which is indicated by the red line in Fig. 1(a). Most of the samples are located in the area (purple) that satisfies both conditions, but the remaining samples can only satisfy one. In more detail, Fig. 1(b) shows three samples with a low deviation of the experience trend (the top row) and three samples with a high deviation (the bottom row), where the worse SE does not always correspond to longer AL or more retinal structural changes. To mitigate degenerate representation caused by noisy data, the screening scheme should avoid over-fitting of extremely confusing samples. Besides, rich interpretable decisions support enhancing confidence in screening results. To this end, the screening scheme should evaluate the uncertainty of the results.The contributions of our work are summarized as: (1) we propose a novel adjustable robust transformer (ARTran) for high myopia screening to adapt variable inclusion criteria. We design an anisotropic patch embedding (APE) to encode more myopia-related information in OCT images, and an adjustable class embedding (ACE) to obtain adjustable inferences. (2) We propose shifted subspace transition matrix (SST) to mitigate the negative impacts of label noise, which maps the class-posteriors to the corresponding distribution range according to the variable inclusion criteria. (3) We implement our ARTran on a high myopia dataset and verify the effectiveness of screening, and jointly use the proposed modules to generate multi-perspective uncertainty evaluation results."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.0,Method,"In this section, we propose a novel framework for high myopia screening in OCT called adjustable robust transformer (ARTran). This model can obtain the corresponding decisions based on the given threshold of inclusion criteria for high myopia and is trained end-to-end for all thresholds at once. During the testing phase, the screening results can be predicted interactively for a given condition."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.1,Modified Vision Transformer for Screening,"Transformers have shown promising performance in visual tasks attributed to long-range dependencies. Inspired by this, we use ViT [8] as the backbone of our  framework and make improvements for the task of screening high myopia OCT images.Patients with high myopia are usually accompanied by directional structural changes, such as increased retinal curvature and choroidal thinning. On the other hand, due to the direction of light incidence perpendicular to the macula, OCT images have unbalanced information in the horizontal and vertical directions. Therefore, we propose a novel non-square strategy called anisotropic patch embedding (APE) to replace vanilla patch embedding for perceiving finer structural information, where an example is shown in Fig. 2. First, we reduced the height size of the patches. This strategy increases the number of patches per column, i.e., the number of patches within the retinal height range, which enhances the information perception of overall and individual layer thickness. In order not to introduce extra operations, we reduce the number of patches per row. We also use an overlapping sampling strategy for sliding windows in the horizontal direction, where a wider patch captures more information about the peripheral structure of the layer in which it is located. Specifically, based on the 16×16 size of ViT, our APE changed the height to 8 pixels and the width to 56 pixels with an overlap of 28 pixels, which keeps the number of embedded patches.Considering that different researchers may have different requirements for inclusion criteria or risk control, we propose adjustable class embedding (ACE) to adapt to variable conditions. Take the -6.0D as the benchmark of inclusion criteria, we define the relationship of the biased label, SE, and the adjustment coefficient Δ:where 1 indicates the positive label and 0 indicates the negative label. Our ACE introduces two parameterized vectors v 1 and v 2 , and constructs a linear combination with the given δ to obtain v AP E (δ) to replace the fixed class token:where v AP E (0) is the equal combination and others are biased combinations.Several studies have demonstrated impressive performance in multi-label tasks using transformers with multiple class tokens [13,27]. Inspired by this, we set v AP E (0) as the benchmark class embedding and v AP E (δ) as the biased class embedding. In the training stage, the inclusion threshold is varied around the benchmark, affecting the supervision consequently. The ACE adaptively changes the input state according to the scale of the adjustment coefficient to obtain the corresponding output. The scheme for constructing loss functions using two outputs is described in Sect. 2.2. In the testing stage, the ACE interactively makes the model output different results depending on the given conditions. Furthermore, we did not add the position encoding to ACE, so both tokens are position-independently involved to multi-head self-attention and are only distinguished by adjustment coefficient."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.2,Shifted Subspace Transition Matrix,"To enhance the robustness of the model, we follow the common assumptions of some impressive label noise learning methods [26,28]. Conventional wisdom is that the clean class-posterior P (Y |X = x) can be inferred by utilizing the noisy class-posterior P ( Ỹ |X = x) and the transition matrix T (x):where the transition matrix guarantees statistical consistency. Li et al. [14] have proved that identifying the transition matrix can be treated as the problem of recovering simplex for any x, i.e., T (x) = T . Based on this theory, in this work, we further propose a class-dependent and adjustment-dependent transition matrix T (x, δ) = T (δ) called shifted subspace transition matrix (SST) to adapt to the variable class-posteriors distribution space. Simplistically, this work only discusses the binary classification approach applicable to screening. For each specific inclusion threshold δ ∈ [-1, 1], the range of the noisy class-posterior is determined jointly by the threshold and the SST:where T i,i (δ) > 0.5 is the i th diagonal element of SST, and the sum of each column of the matrix is 1. Thus any class-posterior of x is inside the simplex form columns of T (δ) [2]. As shown in Fig. 2, the benchmark simplex is formed by T (0), where the orange arrows indicate the two sides of the simplex. When adjusting the inclusion criteria for high myopia, we expect the adjusted classposterior to prefer same the adjustment direction compared to the benchmark. One solution is to offset both the upper and lower bounds of the class-posterior according to the scales of the adjustment coefficient:where the S(•) is the Sigmoid function, θ i is the parameter used only for column, and θ 0 is the parameter shared by both columns. Adjusting the δ is equivalent to shifting the geometric space of the simplex, where the spaces with a closer adjustment coefficient share a more common area. This ensures that the distribution of the noise class-posterior has a strong consistency with the adjustment coefficient. Furthermore, the range distribution of any T (δ) is one subspace of an extended transition matrix T Σ , whose edges is defined as the edges of T (-1) and T (1):To train the proposed ARTran, we organize a group of loss functions to jointly optimize the proposed modules. The benchmark noise posteriors and the adjusted noise posteriors are optimized with classification loss with benchmark and adjusted labels respectively. Following the instance-independent scheme, we optimize the SST by minimizing the volume of the extended SST [14]. The total loss function L we give is as follows:where L cls (•) indicates the cross entropy function, and L vol (•) indicates the volume minimization function."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.1,Dataset,"We conduct experiments on an OCT dataset including 509 volumes of 290 subjects from the same OCT system (RTVue-XR, Optovue, CA) with high diversity in SE range and retinal shape. Each OCT volume has a size of 400 (frames) × 400 (width) × 640 (height) corresponding to a 6mm × 6mm × 2mm volume centered at the retinal macular region. The exclusion criteria were as follows: eyes with the opacity of refractive media that interfered with the retinal image quality, and eyes that have undergone myopia correction surgery. Our dataset contains 234 low (or non) myopia volumes, and 275 high myopia volumes, where labels are determined according to a threshold spherical equivalent -6.0D. We divide the dataset evenly into 5 folds for cross-validation according to the principle of subject independence for all experiments. For data selection, we select the center 100 frames of each volume for training and testing, so a total of 50,900 images were added to the experiment. And the final decision outcome of one model for each volume is determined by the classification results of the majority of frames. For data augmentation, due to the special appearance and location characteristics of high myopia in OCT, we only adopt random horizontal flipping and random vertical translation with a range of [0, 0.1]. "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.2,Comparison Experiments and Ablations,"To evaluate the proposed ARTran in predicting under a benchmark inclusion criteria (-6.0D), we compare it with two convolution-based baselines: ResNet-50 [10] and EfficientNet-B5 [25]; three transformer-based baselines: ViT-Small [8], Swin-Tiny [18] and Swin-V2-Tiny [17]; and two state-of-the-art high myopia screening methods: Choi's method and Li's method [16]. Table 1 presents the classification accuracy, precision, and recall by our ARTran and comparison methods. The proposed ARTran already outperforms baseline methods remarkably with a 2.9% to 5.1% higher accuracy and a lower variance. This is because we design modules to capture the features of high myopia, which brings effectiveness and robustness. Although consisting of fewer parameters, our ARTran obtains higher accuracy than two state-of-the-art screening methods. Moreover, clinical practice requirements generally advocate the model that predicts smaller false negative samples, i.e., a higher recall. It is observed that the recall of our approach is the best performance, which means that our model has minimal risk of missing highly myopic samples.We further perform ablations in order to better understand the effectiveness of the proposed modules. Table 2 presents quantitative comparisons between different combinations of our APE, ACE, and SST. As can be seen, ablation of APE leads to a rapid decrease in recall, which means that this patch embedding approach captures better features about the propensity for high myopia. The ablation of ACE represents the ablation of the proposed adjustment scheme, which leads to lower accuracy. The ACE drives the network to learn more discriminative features for images in the adjustment range during the training process. The ablation of SST leads to a rapid decrease in precision. The possible reason is that the label noise may be more from negative samples, leading to increased false positive samples. "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,4.0,Conclusion,"In this work, we proposed ARTran to screen high myopia using OCT images. Experimental results demonstrated that our approach outperforms baseline classification methods and other screening methods. The ablation results also demonstrated that our modules helps the network to capture the features associated with high myopia and to mitigate the noise of labels. We organized the evaluation of the adjustable and interpretable ability. Experimental results showed that our method exhibits robustness under variable inclusion criteria of high myopia. We evaluated uncertainty and found that confusing samples had higher uncertainty scores, which could increase the interpretability of the screening task."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.3,Adjustable Evaluation and Uncertainty Evaluation,"To evaluate the effectiveness of the adjustment module, we change the adjustment coefficient several times during the testing phase to obtain screening results at different thresholds. Figure 3(a) depicts the PR curve when adjusting the adjustment coefficient. The performance of the two endpoints (Δ = -1 and Δ = 1) is marked. Even with a high recall rate, the precision is not low. Figure 3(b) shows the performance of the biased labels for different adjustment coefficients. As can be seen, the accuracy improves when offsetting the inclusion criteria, which on the one hand may be due to the difficulty of classification near"
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,1.0,Introduction,"Diabetic Retinopathy (DR) is a leading cause of blindness, affecting millions of people worldwide, and early severity grading is vital for disease management [1]. Although deep learning (DL) has shown promising results in automatic DR grading [2][3][4], its real-world deployment is still challenging. For instance, Google's DR grading system performed ideally in controlled lab settings [4], but failed to generalize well to complex scenarios which suffer from data shifts [5]. It is a common problem known as domain generalization (DG) [6], where the model performance significantly drops when applied to unseen domains different from the training data. Such an issue hinders the wide adoption and success of DLbased diagnostic tools in clinical practice [7].Recently, several studies have explored the DG problem and reported significant performance drops in the retinal vessel segmentation [8,9]. Similarly, in DR grading, previous works showed a significant decrease in performance when presented with unseen domains and attempted to solve this problem through the perspective of feature disentanglement [10] and domain-invariant feature learning [11]. Although these methods have improved performance towards unseen domains, they may not be effective in more complex real-world scenarios because they attribute the generalization issue only to limited domain shifts, such as simple visual discrepancies. However, the generalization issues across domains cannot be solely attributed to visual discrepancies [12].In contrast to previous works, we argue that three factors contribute to poor generalization in DGDR: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. Specifically, as shown in Fig. 1, we first conduct a preliminary analysis of three public datasets/domains. First, style shifts arise due to various factors, not only limited to visual style discrepancies, but also factors such as variations in lighting conditions [13], image resolution [14], or the presence of artifacts or noise [15]. These factors have been neglected in previous works yet are essential for building a generalizable model. Second, domains may contain diverse diagnostic patterns, such as variations in lesion types, distribution, combination and severity in certain categories [16,17]. This diversity makes learning a generalizable model towards unseen domains challenging because they may contain partially-overlapped or even unknown diagnostic patterns. Finally, data imbalance across categories and domains causes samples from specific datasets and minority classes to be underrepresented. Moreover, this imbalance can exacerbate the issue of omitting rare diagnostic patterns, leading to shortcuts in learning and poor generalization [18].In this paper, we propose a novel framework, Generalizable Diabetic Retinopathy Grading Network (GDRNet) to address the DGDR problem. Our framework consists of three critical components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domainclass-aware re-balancing (DCR). By simulating visual transformations and image degradations, FundusAug enables the model to learn robust features that are less sensitive to style shifts caused by factors such as lighting conditions or artifacts and noise. DahLoss employs a hybrid-supervised learning paradigm to handle diagnostic pattern diversity and dynamically balances the influence of supervised and unsupervised learning. Jointly functioning with FundusAug, it enables the model to preserve pixel-level diagnostic information and learn generalizable features with sufficient intra-class variations. Furthermore, DCR assigns soft-balanced weights to each domain-class pair to prevent underrepresentation caused by data imbalance while avoiding undesired over-emphasis introduced by hard weighting. Finally, to evaluate generalization ability, we design a publicly available benchmark named Generalizable Diabetic Retinopathy Grading Benchmark (GDRBench), comprising eight popular datasets and two evaluation settings."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,2.0,Methodology,"An overview of GDRNet is shown in Fig. 2. It addresses the mentioned generalization issues, including style shifts, diagnostic pattern diversity, and domain-class data imbalance, by the proposed FundusAug, DahLoss, and DCR, respectively. Overall, GDRNet provides a unified solution to improve the generalization ability in unseen domains. This section will introduce each component in detail.Fundus Visual-Artifact Augmentation. The external machine and internal retinal illumination conditions can cause differences in visual attributes [13], such as contrast and brightness, while image degradations, like spot artifacts and blurring, are also very common in fundus imaging [15,19], as depicted in Fig. 1. To bridge the style shifts caused by visual discrepancies and image degradations, we developed FundusAug, which is parameter-free and plug-anduse, designed to generate diverse and realistic augmented views. It employs five basic image transformations, including brightness, contrast, saturation, hue, and sharpness adjustments, to fill visual discrepancy gaps. Moreover, it uses extra four degradation-based image transformations, including halo simulation, hole generation, spot addition, and image blur, to address image degradation gaps. Specifically, given training data x with label y, the augmented view x of original data can be derived through the following equation:where π n pn,mn denotes transformation n with probability p n and random intensity m n . To reduce the parameter space of FundusAug while still ensuring image diversity, we implemented FundusAug by applying each operation with a parameter-free procedure that uniformly selects a process with a probability of 0.5. FundusAug can generate realistic augmented views while preserving their diagnostic semantics, as well as providing a robust foundation for subsequent generalizable feature learning by increasing image diversity. A detailed description and visualization of operations can be found in the appendix.Dynamic Hybrid-Supervised Loss. While the supervised loss (SupLoss), e.g., the cross-entropy loss (CE), effectively guides the model to learn effective feature representations [20], it has two disadvantages in DGDR. First, SupLoss leads to dense features within categories while sufficient intra-class variations are crucial for effectively generalizing to unseen domains [21]. Second, the potential variety of diagnostic patterns in unseen domains requires the models to learn pixel-level lesion semantics as much as possible, while SupLoss lacks such functionality [22]. To tackle these issues, we proposed DahLoss to encourage models to learn features with sufficient intra-class variations and preserve diagnostic patterns, by introducing a hybrid-supervised paradigm to jointly leverage imagelevel severity supervision and pixel-level semantics consistency. A basic form of DahLoss is aswhere α decreasing within range [0, 1] is a hyper-parameter to dynamically control the task focus, and L sup and L scon could be any supervised and selfsupervised contrastive loss functions. In this paper, we adopt CE and instance discrimination loss [23] as L sup and L scon . Specifically, within a multiviewed batch, let i ∈ I ≡ {1...N } be the index of arbitrary augmented samples by Fun-dusAug and j(i) ∈ J ≡ {1 ...N } be the index of weakly-augmented samples originating from the same source sample, we denote L i dhs for sample i aswhere p • t denotes the predicted probability of the true class under one-hot encoding label, f • denotes the l 2 normalized feature, the • symbol denotes the inner product, τ is the temperature parameter and A(i) ≡ (J ∪ I)/i.As illustrated in Fig. 2 (b), the use of L sup alone tends to force samples within the same class to cluster tightly together, resulting in a highly concentrated feature representation that may not be beneficial for generalization to unseen domains. However, by incorporating L scon , DahLoss can achieve a balance between the intra-class variation and inter-class distance in learned feature representations. Specifically, while L sup encourages samples within the same class to cluster together, L scon simultaneously pulls the augmented views of these samples closer to each other and pushes them far away from those of other samples. It results in a feature representation with both sufficient intraclass variation and clear inter-class separation. Moreover, DahLoss also leverages pixel-level consistency to preserve crucial diagnostic patterns. It is achieved by enforcing the model to maintain feature representations with semantics similarity between strong-weak augmented views originating from the same sample, which ensures that critical details and structures are learned. By jointly considering both image-level supervision and pixel-level consistency, DahLoss encourages the model to learn features with sufficient intra-class variations and preserve crucial diagnostic information, improving generalization performance in unseen domains. Finally, gradually decreasing the value of α during training would guide the model to focus more on the grading task, leading to a balance between representation learning and grading performance. In this paper, we simply set α decay from 1 to 0 linearly across training epochs to verify the effectiveness of DahLoss.Domain-Class-Aware Re-balancing. The domain-class data imbalance can result in certain categories and diagnostic patterns in specific datasets being underrepresented, leading to biased and inaccurate model predictions [12]. To complicate the situation, adopting a hard balancing style, such as weighting domain-class pairs based on the reciprocal of occurrence frequency, could potentially cause performance degradation [24]. Such decay is owing to the fact that underrepresented domain-class pairs often have a small ratio, resulting in excessively high weights to dominate gradients. Inspired by [25], we designed the DCR method to address this issue. It assigns weights to each sample based on the occurrence probability q d c of its category c and domain d. Specifically, DCR calculates the weight w d c for category c in domain d aswhere D denotes the set of domains, N is the amount of classes, and β with a range of [0, 1] is a hyperparameter that adjusts the balancing intensity. When β approaches to 0, DCR assigns weight more equally, and when β closes to 1, it acts more like the naive hard balancing method. By introducing β, DCR enables more nuanced weighting of samples based on their domain and class, reducing the risk of over-emphasizing underrepresented samples in the loss function. By considering the occurrence probabilities of all categories across all domains, DCR mitigates the data imbalance problem of underrepresented class-domain pairs. These two make DCR an effective solution for handling domain-class data imbalance and improving the generalizability of DR grading models."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,3.0,Experiments,"Experimental Settings, Implementation Details and Evaluation Metrics.To comprehensively analyze and evaluate our framework, we designed the GDRBench involving two generalization ability evaluation settings and eight popular public datasets. First, GDRBench preserves the classic leave-onedomain-out protocol (DG test), which requires leaving one domain for evaluation and training models on the rest. It involves six datasets, including DeepDR [16], Messidor [17], IDRID [31], APTOS [32], FGADR [33], and RLDR [34]. Further, we designed an extreme single-domain generalization setting (ESDG test), which follows the train-on-single-domain protocol with datasets mentioned above but adds two extra large-scale datasets, DDR [35] and EyePACS [36] for evaluation. It simulates real-world generalization issues, in which models are trained only on thousands of samples but are required to generalize well on one hundred thousand images from multiple hospitals and areas. We used ResNet50 pre-trained on ImageNet as the backbone and a fully connected layer as the linear classifier. For evaluation, we report three critical metrics, namely accuracy (ACC), the area under the ROC curve (AUC), and macro F1-score (F1). We used bold and underline to indicate the first-and second-highest scores. A detailed illustration of the datasets and implementation settings can be found in the appendix.Comparison with Other Methods. We conducted a comprehensive experiment to evaluate our framework, comparing it with a vanilla baseline (ERM) and other state-of-the-art methods from various categories, including ophthalmic disease diagnosis (OSD) [2,3], domain generalization techniques (DGT) [11,[26][27][28][29], and feature representation learning (FRL) [12,30]. These chosen methods can be adopted in DGDR with minimal or no modifications, and their brief descriptions are in the appendix. We employed a standard DG augmentation pipeline [12] for all methods except DRGen with a default augmentation strategy [11]. Table 1 shows the quantitative results of the DG test, where the row of ""target"" indicates the test domain. GDRNet performs better than other methods and significantly improves at least two of AUC, ACC, and F1 on all sub-tests except the test on IDRiD, whose small scale leads to unobvious diagnostic pattern diversity. Typically, ODS methods do not consider domain shifts, and they thus fail to generalize as well as other methods. As expected, DGT and FRL methods consistently improve the performance compared with ERM due to specific designs towards limited domain shifts. However, GDRNet still outperforms these methods markedly due to it handles three-fold generalization issues via increasing training diversity via realistic augmentation, learning generalizable features with sufficient intra-class variations, preserving diagnostic patterns, and rebalancing the minority of samples. Overall, the result shows the effectiveness of GDRNet.Ablation Study of Proposed Components. To evaluate the effectiveness of our proposed components, we conducted an extensive ablation study under the DG test and presented the AUC score achieved by different models in Table 2.We first examined the individual effects of FundusAug with only visual transformation (VT), FundusAug with only image degradation (ID), DCR and DahLoss  3, where the row of ""source"" indicates the training dataset. As expected, the average performance of all methods decreases significantly, indicating the difficulty of ESDG. Although some methods outperform others in the DG test, such as MixStyle, due to their specific design to leverage the discrepancy of domains, they fail to generalize in the strict ESDG test. Despite more strict requirements, GDRNet still outperforms other methods in average performance and at least one metric in all sub-tests, owing to its effective designs towards three-fold generalization issues."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,4.0,Conclusion,"In this paper, we tackled the three-fold generalization issues that hinder the generalizability of DR grading, including style shifts, diagnostic pattern diversity, and data imbalance. To overcome these challenges, we proposed a novel and unified framework called GDRNet, incorporating three effective components: FundusAug, DahLoss, and DCR. FundusAug enables the generation of realistic augmented views, while DahLoss leverages supervised and unsupervised learning to preserve diagnostic patterns and increase intra-class variations of features. Finally, DCR softly handles the data imbalance across categories and domains to avoid potential performance decay. Together, these three components work synergistically to improve the generalization performance of the model. GDR-Net achieved superior performance on both DG and ESDG tests of the proposed publicly available GDRBench, demonstrating its effectiveness and robustness in addressing the three-fold generalization issues in DR grading. Overall, our work provides valuable insights and practical solutions for improving the generalization capability of deep learning in medical image analysis, and has the potential to benefit real-world clinical applications."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_42.
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,1.0,Introduction,"Chest radiography is currently the most common imaging examination, playing a crucial role in epidemiological studies [3] and clinical diagnosis [10]. Nowadays, the automated analysis of chest X-rays using deep learning algorithms has attracted increasing attention due to its capability of significantly reducing the workload of radiologists and expediting clinical practice. However, training deep learning models to achieve expert-level performance on various medical imaging tasks requires large, labeled datasets, which are typically difficult to obtain due to data privacy and workload concerns. Developing generative models for high-fidelity X-rays that faithfully represent various medical concepts in radiology reports presents a possible remedy for the lack of datasets in the medical domain [5,15]. This approach may substantially improve traditional supervised downstream tasks such as disease diagnosis [10] and medical image retrieval [8].Generating chest radiographs based on radiology reports can be thought of as transforming textual input into visual output, while current methods typically rely on text-to-image generation in computer vision. The fidelity and diversity of synthesized images are two major qualities of generative models [1], where fidelity means the generated images should be close to the underlying real data distribution, while diversity means the output images should ideally cover a large variability of real-world images. Recently, several works have been extensively proposed to generate high-fidelity images using GANs according to text prompts. StackGAN [25] stacked multiple generators and discriminators to gradually increase the resolution of the generated images. AttnGAN [23] synthesized images with fine-grained details by introducing a cross-modal attention mechanism between subregions of images and relevant words. DM-GAN [27] introduced a memory module to refine fuzzy image contents caused by inferior initial images in stacked architecture. MirrorGAN [16] reconstructed the text from the synthesized images to preserve cross-domain semantic consistency. Despite the progress of text-to-image generation in the general domain, generating X-rays from radiology reports remains challenging in terms of word embedding, handling the linguistic structure in reports, cross-modal feature fusion, etc.The first work to explore generating chest X-rays conditioned on clinical text prompts is XrayGAN [24], which synthesized high-fidelity images in a progressive way. Additionally, XrayGAN [24] proposed a hierarchical attentional text encoder to handle the linguistic structure of radiology reports, as well as a pretrained view-consistency network to constrain the generators. Although impressive results have been presented, three problems still exist: 1) The progressive generation stacks multiple generators of different scales trained separately in an adversarial manner (see Fig. 1), where visual features of different scales are difficult to be fused uniformly and smoothly, making the final refined X-rays look like a simple combination of blurred contours and some details (see Fig. 2). 2) A high proportion of reconstruction loss and a pre-trained view-consistency network are used at each layer for the convergence of training stacked generators, which severely limits the diversity of generated X-rays (only one chest radiograph can be generated from one report). 3) The word vectors in [24] are based on the order in which words appear in the vocabulary, ignoring the information presented in medical-specific structures and the internal structure of words. More recently, another line of works [1,2] investigated adapting pre-trained visuallanguage foundational models to generate chest X-rays. However, transferring the diffusion models [18] trained with large multi-modal datasets to the medical imaging domain typically has high computational requirements.In this paper, we propose a new report-to-X-ray generation method called DivXGAN to address the above issues. As illustrated in Fig. 1, DivXGAN allows for the synthesis of various X-rays containing relevant clinical concepts from a single report. The following contributions are made: 1) Inspired by the one-stage architecture [21], we propose to directly synthesize high-fidelity X-rays without entangling different generators. 2) We discard the pixel-level reconstruction losses and introduce noise vectors in the latent space of the generator to provide the variability, thus allowing the diversity of generated chest radiographs. 3) Lastly, we design a domain-specific hierarchical text encoder to represent semantic information in reports and perform multiple cross-modal feature fusions during X-ray generation. We demonstrate the superiority of our method on two benchmark datasets and a downstream task of multi-label classification."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.0,Method,"Let X and Z denote the image space and the low-dimensional latent space, respectively. Given a training set {x i , r i } N i=1 of N X-ray images, each of which x i is associated with a radiology report r i . The task of report-to-X-ray generation aims to synthesize multiple high-fidelity chest radiographsfrom the corresponding report r i and latent noises {z j ∈ Z} j=1,2,..., . The generative models are expected to produce X-rays with high fidelity and diversity, so as to be used for data augmentation of downstream applications."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.1,Fidelity of Generated X-Rays,"One-Stage Generation. Existing generative method [24] uses a stacked structure to progressively synthesize high-fidelity X-rays. The stacked structure stabilizes the training of GANs but induces entanglements between generators trained separately in an adversarial way at different scales, resulting in fuzzy or discontinuous images. We draw inspiration from the one-stage architecture [21] and propose to directly generate high-fidelity X-rays using a single pair of generator G and discriminator D. The network architecture of our method is illustrated in Fig. 1. The generator contains many up-sampling layers to increase the resolution of the synthesized X-ray xi , while the corresponding discriminator also requires many down-sampling operations to compute the adversarial loss. To stabilize the training of deep networks in this design, we introduce residual connections [6] in both the generator and the discriminator.Distill and Incorporate Report Knowledge. Semantic information and medical concepts in radiology reports should be fully interpreted and incorporated into visual features to reduce the distance between the generated data distribution and the real data distribution, thereby improving fidelity. We design a medical domain-specific text encoder with hierarchical structure to extract the embeddings of the free-text reports. At the word level, each sentence is represented as a sequence of T word tokens, plus a special token [SEN T ]. We embed each word token w t with an embedding matric W e , i.e., e t = W e w t . Unlike previous work [24] that use one-hot embedding, we initialize our word embeddings with the pre-trained biomedical embeddings BioWordVec [26], which can capture the semantic information in the medical domain and the internal structure of words. Then, we use a Transformer encoder with positional embedding to capture the contextual information of each word and aggregate the holistic representations of the sentence into the embedding of the special token e [SENT ] :At the sentence level, a report consists of a sequence of S sentences, each of which is represented as e (i)[SENT ] using the word-level encoder described above. We also utilize a Transformer to learn the contextual importance of each sentence and encode them into a special token embedding e [REP O] , which serves as the holistic representation of the report:[SENT ] , e (2) [SENT ] , ..., e (S) [SENT ] (2) Moreover, we perform cross-modal feature fusion after each up-sampling module of the generator (see Fig. 1), to make the synthesized X-rays more faithful to the report. The fusion block contains two channel-wise Affine transformations and two ReLU layers. The scaling and shifting parameters of each Affine transformation are predicted by two MLPs (Multilayer Perceptron), using the vector e [REP O] as input. The Affine transformation expands the representation space of the generator G, allowing for better fusion of features from different modalities."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.2,Diversity of Generated X-Rays,"A radiology report is a medical interpretation of the corresponding chest radiograph, describing the clinical information included and assessing the patient's physical condition. Reports that describe chest radiographs of different patients with similar physical conditions are often consistent. Ideally, multiple X-ray images with the same health conditions could be generated from a single report, only with some differences in irrelevant factors such as body size, etc.To this end, we omit the pixel-wise reconstruction loss and introduce noise vectors z in the latent space Z as one of the inputs to our one-stage generator, thereby providing the model with the necessary variability to ensure the diversity of synthesized X-rays. In this case, the generator G maps the low-dimensional latent space Z into a specific X-ray image space X r , conditioned on the report vector e i [REP O] :where x(j) i denotes the j-th synthesized X-ray from the i-th report r i . The noise vector z j ∈ Z follows a standard multivariate normal distribution N (0, I). In this way, given a radiology report, noise vectors can be sampled to generate various chest X-rays matching the medical description in the report."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.3,Learning Objectives and Training Process,"Since DivXGAN uses a one-stage generator to directly generate high-fidelity chest radiographs, only one level of generator and discriminator needs to be alternately trained. The discriminator D outputs a scalar representing the probability that the input X-ray came from the real dataset and is faithful to the input report. There are three kinds of inputs that the discriminator can observe: real X-ray with matching report, synthesized X-ray with matching report, and real X-ray with mismatched report. The discriminator D x, e [REP O] ; θ d is trained to maximize the probability of assigning the report vector e [REP O] to the corresponding real X-ray x i , while minimizing the probability of the other two kinds of inputs. Due to multiple down-sampling blocks and residual connections, we employ the hinge loss [13] to stabilize the training of D:where p data , p g and p mis denote the data distribution, implicit generative distribution (represented by G) and mismatched data distribution, respectively.The generator G z, e [REP O] ; θ g builds a mapping from the latent noise distribution to the X-ray image distribution based on the correlated reports, fooling the discriminator to obtain high scores:It is worth noting that the parameters θ t of the text encoder in Eqs. ( 1) and ( 2) are learned simultaneously during the training of the generator G."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,3.1,Datasets and Experimental Settings,"We use two public datasets, namely Open-i [4] and MIMIC-CXR [9], to evaluate our generative model. The public subset of Open-i [4] consists of 7,470 chest X-rays with 3,955 associated reports. Following previous works [14,24], we select studies with two-view X-rays and a report, then end up with 2,585 such studies. As for MIMIC-CXR [9], which contains 377,110 chest X-ray images and 227,827 radiology reports, for a fair comparison with alternative methods, we also conduct experiments on the p10 subset with 6,654 cases to verify the effectiveness of our approach. Moreover, we adopt the same data split protocol as used in XRayGAN [24] for these two datasets, where the ratio of the train, validation, and test sets are 70%, 10%, and 20%. For consistency, we follow the set-up of XRayGAN [24] to focus on two major sections in each free-text radiology report, namely the ""findings"" section and the ""impression"" section.Our network is trained from scratch using the Adam [11] optimizer with β 1 =0.0 and β 2 =0.9. The learning rates for G and D are set to 0.0001 and 0.0004, respectively, according to the Two Timescale Update Rule [7]. The hidden dimension of the Transformer in the text encoder is 512. The noise vector z in the latent space is sampled from a standard multivariate normal distribution with a dimension of 100. The resolution of synthesized X-rays is 512 × 512. We implemented our method using PyTorch 1.7 and two GeForce RTX 3090 GPUs. We use Inception Score (IS) [19] and Fréchet Inception Distance (FID) [7] to assess the fidelity and diversity of the synthesized X-rays. Typically, IS and FID are calculated using an Inception-V3 model [20] pre-trained on ImageNet, which might fail in capturing relevant features of the chest X-ray modality [12]. Therefore, we calculate these metrics from the intermediate layer of a pre-trained CheXNet [17]. Higher IS and lower FID indicate that the generated X-rays are more similar to the original X-rays. In addition, we also calculate the pairwise Structural Similarity Index Metric (SSIM) [22] to evaluate the diversity of Xrays generated by different methods. A lower SSIM indicates a smaller structural similarity between images, which combined with a low FID, can be interpreted as higher generative diversity [1]."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,3.2,Results and Analysis,"We compare our approach with several state-of-the-art methods based on generative adversarial networks, including text-to-image generation: StackGAN [25], AttnGAN [23], and report-to-X-ray generation: XRayGAN [24]. The performance of different approaches on the test sets of Open-i and MIMIC-CXR is shown in Table 1. We can observe that XRayGAN [24] achieves better IS and FID than other text-to-image generation baselines. This is because the pixelwise reconstruction loss imposes strong constraints on the stacked generators at different scales to help avoid generating insensible images. However, this strong constraint severely reduces the diversity of generated X-rays, resulting in the worst SSIM. Our method consistently outperforms other alternatives by achieving both the lowest FID and the lowest SSIM, which means the generated X-rays have better fidelity and diversity. The reason lies in that the latent noise vectors impose the necessary variation factor, and the one-stage generation process and multiple cross-modal feature fusions improve the image quality. We conduct ablation experiments to quantify the impact of different components in DivXGAN. The results in Table 2 show that our one-stage architecture definitely improves performance due to better cross-modal feature fusion. The domain-specific encoder outperforms the hierarchical attentional encoder [24], regardless of the backbone structure, indicating the advantage of the domainspecific embedding matrix, especially for medical reports with very rare and specific vocabulary. The comparison of SSIM for different components demonstrates that the latent vector input preserves the diversity of synthesized X-rays.Visualization of chest X-rays synthesized from a report using different methods is shown in Fig. 2. As we can see, the X-rays generated by text-to-image baselines are very coarse, and even the outline of the hearts can be barely recognized. XRayGAN [24] alleviates the blur and generates X-rays with relatively obvious chest contours, because of the strong constraints that prevent the generative model from producing abnormal samples. However, this strongly constrained approach still fails to preserve a clear outline of the heart and ribs, due to the entanglements between generators introduced by the stacked architecture. In particular, the lack of variability in the strong constraints results in only one X-ray being generated per report. Our method prevails over other alternatives as the generated X-rays are obviously clearer and more realistic, and even generate annotation information in the top right corner (seen in almost all samples). This phenomenon indicates the efficacy of the one-stage generation and multiple cross-modal feature fusion in generating high-fidelity X-rays. Furthermore, our method can generate various X-rays from one report, each of which manifests the relevant clinical findings. For example, the regions marked by red arrows in Fig. 2 show that our method can synthesize various different X-rays matching the clinical finding ""Cardiomegaly"" described in the report. Although our model is capable of generating X-rays based on radiology reports, due to the complexity of medical images, the synthesized X-rays have a limited range of gray-scale values and may not effectively capture high-frequency information such as subtle lung markings.  Furthermore, the ethical implications associated with the misuse of generated X-rays are significant. They should not be solely relied upon for clinical decisionmaking or used to train inexperienced medical students as radiologists. While the direct use of generated X-rays in clinical studies or medical training programs may not be appropriate, they can still serve valuable purposes in research, data augmentation, and other potential applications within the medical field. Here, we train a DenseNet-121 from scratch using various splits of real data (from MIMIC-CXR) and synthesized data (from DivXGAN) to demonstrate that the generated chest radiographs can be used for data augmentation when training downstream tasks. The task is a multi-label classification of four findings (""Cardiomegaly"", ""Consolidation"", ""Pleural Effusion"" and ""No Findings""). We randomly sample 5k real images with corresponding reports from the test set. These 5k real reports are input into our generative model, generating one image per report using a single latent vector, resulting in 5k generated images. When training the multilabel classifier, both real and generated images undergo the same general data augmentation techniques, such as rotation and scaling. As shown in Table 3, compared to the baseline trained exclusively on 5k real X-rays, the AUROC of the classifier trained exclusively on 5k synthesized X-rays drops by 0.019. However, training the classifier with 5k real X-rays and 5k generated X-rays improves AUROC by 0.031, suggesting that the synthesized X-rays can augment real data for supervised downstream applications."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,4.0,Conclusion,"In this paper, we have devised a diversity-preserving method for high-fidelity chest radiographs generation from the radiology report. Different from state-ofthe-art alternatives, we propose to directly synthesize high-fidelity X-rays using a single pair of generator and discriminator. A domain-specific text encoder and latent noise vectors are introduced to distill medical concepts and incorporate necessary variability into the generation process, thus generating X-rays with high fidelity and diversity. We show the capability of our generative model in data augmentation for supervised downstream applications. Investigation of capturing high-frequency information of X-rays in generative models can be an interesting and challenging direction of future work."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,1.0,Introduction,"Ischemic stroke caused by large vessel occlusion (LVO) is one of the leading causes of death and disability worldwide [1]. These occlusions restrict cerebral blood supply, resulting in rapid and irreversible damage to brain tissues. As such, prompt reperfusion treatment is critical to restore blood flow and salvage brain tissues from permanent damage. Endovascular thrombectomy (EVT) is the standard treatment recommended for LVO patients [2]. However, this treatment is costly and invasive and does not improve prognoses for all LVO patients [3]. Therefore, it is vital to identify those most likely to benefit from EVT.There have been a number of models proposed for the prognostication of LVO in recent years [4][5][6]. The majority are traditional statistical models using nonimaging data, with the identification of clinical predictors for prognostication having shown limited capability [7]. A few models incorporate raw image data using deep-learning techniques, while most of these [4,5] utilize a single image modality, such as Non-Contrast Computed Tomography (NCCT).CT Perfusion (CTP) maps are increasingly utilized for the prediction of LVO outcomes because they offer quantitative information on blood flow and volume in the brain, as well as the arrival time of blood bolus to brain tissues [8]. This information can show the brain perfusion status and identify the brain tissues that are irreversibly damaged (ischemic core), and those that are potentially salvageable (penumbra) [8]. In contrast, NCCT images can highlight the ischemic core as these tissues normally appear hypodense and CT angiography (CTA) can illustrate the collateral supply via highlighting vessel structure using a low dose contrast injection [9]. These two modalities are routinely collected at patients' admission because of their rapid acquisition and high tolerance [2]. Although the information estimated by NCCT and CTA can reflect the status of blood perfusion to some extent [9], these images are not as clinically useful as CTP maps since they do not directly show the most prominent neuropathological changes of ischemic stroke-ischemic core and salvageable penumbra. An example of ischemic changes shown by NCCT, CTA and CTP maps is presented in Fig. 1, where it can be seen that CTP maps provide clearer visual information, and hence are likely to lead to more accurate and clinically trusted prediction models. As such, it is important to investigate the benefits of incorporating CTP maps into deep learning prediction models. Despite the advantages of CTP maps, several factors have limited their utilization: (i) it is time-consuming to acquire, process and interpret the images [2]; (ii) poor quality maps are more likely due to motion artefacts and inadequate contrast agent, potentially making them uninterpretable [10]; and (iii) they require substantial investment to purchase advanced scanners, hire professional staff to run and maintain the scanner, which limits usage in hospitals with limited funding, such as in many rural and remote areas [11]. Therefore, it will be helpful if the flexibility of models incorporating CTP maps can be improved through image synthesis from commonly available image modalities (e.g., NCCT and CTA). In this way, patients without access to a CTP scanner, or with uninterpretable CTP maps, can still benefit from the prognostic prediction that uses clinically-relevant features of CTP maps to inform treatment selection.In recent years, techniques of image synthesis have shown promising potential in medicine [12][13][14]. However, most of these models have focused on imageto-image conversion tasks. In this paper, we propose a two-stage model that incorporates clinical knowledge for image synthesis and multimodal prognostic prediction. Specifically, in the image synthesis stage, the model was assigned to optimize a joint task, including a generative task, a discriminative task and a clinically-focused task. In the multimodal prognostic prediction stage, the model utilized imaging and non-imaging clinical information for predicting the dichotomised modified Rankin Scale (mRS) score 3 months post-stroke -the main outcome in stroke prognosis."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,2.1,Dataset and Pre-processing,"Data utilized in this research was collected from the Royal Adelaide Hospital, which provides the sole EVT service to all stroke patients in South Australia and Northern Territory. There were 460 LVO patients included in the study, admitted between 01 Dec 2016 and 01 Dec 2021, and treated with EVT with full image modalities (NCCT, CTA and CTP maps). Of these, 256 achieved functional independency (mRS ≤ 2) 3 months post-stroke. The non-imaging data (i.e., age, stroke severity, blood glucose, pre-admission functional status, use of intravenous thrombolysis, onset-to-groin puncture time, stroke hemisphere, and 3-month mRS score) was collected by experienced neurologists and nurses adhering to the standard admission procedure. The study was approved by The Central Adelaide Local Health Network Human Research Ethics Committee.The NCCT and CTA images were skull-stripped with the attenuation clipped between 0 and 100 Hounsfield Units (HU) for the NCCT images and 0 and 750 HU for the CTA images. Multimodal CT imaging data, including NCCT, CTA and CTP maps, were acquired using Canon Aquilion ONE scanners. The NCCT and CTA acquisitions have isotropic voxel sizes ranging from 0.4-0.6 mm and 0.4-0.7 mm, respectively. The acquisition voxel size of the CTP maps is 0.4×0.4×4.9 mm 3 . Four CTP maps, including cerebral blood volume (CBV), cerebral blood flow (CBF), mean transit time (MTT), and relative arrival time of contrast (Delay), were selected for their clinical utility, based on consultations with two senior neurologists. To rule out the impact of different brain sizes, affine registration to a CT template was performed for each modality [15,16]. The CTP maps were linearly scaled versions of the ""true"" (quantitative) maps using the within-scan relative values for prognostic prediction, rather than absolute values that may be influenced by a range of nuisance factors (e.g. head size, blood pressure) [17]. The image intensities for each modality were normalised to the interval of [0,1] by rescaling the min-max range for prognostication. Images were resampled to a 1mm isotropic resolution."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,2.2,Models,"Problem Statement. We takei=1 to be a set of data for N patients, where X i NCCT , X i CTA , and X i Cli var are the NCCT, CTA and clinical non-imaging data (i.e., age, stroke severity, blood glucose, pre-admission functional status, use of intravenous thrombolysis, and onset-togroin puncture time) for the i th patient. Four CTP maps for the i th patient are denoted by the set X i CTP which is defined asDelay . The dichotomized prognostic outcome for the i th patient is denoted byWe aim to: (i) evaluate the performance of CTP maps in predicting the dichotomized mRS score; and (ii) synthesize the CTP maps using two commonly used image modalities (NCCT and CTA) at admission for prognostic prediction. For the first aim, the model can be written as:where ŷ i acq is the predicted outcome and F acq is the mapping function from the acquired CTP maps and clinical information to the dichotomized mRS score.For the second aim, there are two tasks, including (i) learning a mapping function G for CTP map generation, and (ii) learning a function F syn to map synthetic CTP maps and clinical information to the dichotomized mRS score. That is:where Xi CBF , Xi CBV , Xi MTT , Xi Delay are the predicted CBF, CBV, MTT, and Delay maps and ŷ i syn is the predicted outcome from synthetic CTP maps for the i th patient. To fulfil the second aim, we propose a two-stage deep learning framework, including a clinical-guided synthesis and a multimodal prognostic prediction. The network architecture and loss function are detailed below. Stage 1: Clinical-Guided Synthesis. The method for synthesizing CTP maps utilizes a 3D generative adversarial network (GAN) model, which is illustrated in Fig. 2 -stage 1. The encoder-decoder is a U-net architecture [18]. The encoder contains one convolutional layer (32 filters) and five 3D-residual blocks (32, 64, 128, 256, 256 filters) with 3 × 3 × 3 kernels. Each convolutional layer in the residual block is followed by instance normalization and LeakyReLU activation. The decoder contains five 3D-residual blocks (128, 64, 32, 32, 4 filters) with 3 × 3 × 3 kernels. After each residual block, features were upsampled and combined with encoder outputs, as usual. NCCT and CTA inputs were concatenated at the channel level. The discriminator contains one convolutional layer (16 filters) and four 3D-residual blocks (32, 64, 128, 256 filters). Real CTP maps and synthetic CTP maps (e.g., X i CBF and Xi CBF ) were input to the discriminator, where two classification heads were designed for: a discriminative task with four fully connected (FC) layers (filters from 128 to 2) to distinguish real from synthetic maps; and a clinical task with a FC layer (filters from 128 to 2) to identify the cerebral hemisphere of the occlusion. The loss function is:where Loss mse and Loss bce calculate the mean square error and the binary cross entropy, respectively. D is a mapping function for discriminative and clinical tasks. y i hemi is the label of the clinical task {0 : occlusion in the left hemishpere; 1 : occlusion in the right hemishpere}. We used the total loss for the set of synthetic CTP maps for backpropagation. Step 1 :Step 2 :where F img is a mapping function of Xi CTP to a binary mRS score and F logistic is a mapping function of step 1 outputs and X i Cli var to a binary mRS score. XiDelay , concatenated as channels."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,3.0,Experiments and Results,"We performed two sets of experiments in the current study. In the first set of experiments, we compared prognostic prediction performance between models using different modalities, including (i) NCCT and CTA, (ii) CTP maps, (iii) NCCT, CTA and CTP maps, (iv) non-imaging data, (v) NCCT, CTA and nonimaging data, (vi) CTP maps and non-imaging data, and (vii) NCCT, CTA, CTP maps and non-imaging data. Images were input into the models with the architecture described in Sect. 2.2 stage 2, where inputs were replaced with corresponding imaging modalities concatenated at the channel level. In the second set of experiments, we evaluated the quality of the synthetic images and the performance when using them for prognostic prediction. We initially compared our model to four synthesis models: UNET, WGAN, CycleGAN and L2GAN. The L2GAN has the same architecture as our model but is not assigned the additional clinical task in the discriminator. To evaluate the quality of the synthetic images, we compared the structural similarity index measure (SSIM) and peak signalto-noise ratio (PSNR) between synthesis models. Area under the ROC curve (AUC), accuracy (ACC), and F1-Score were used to assess the performance of prognostic prediction. We also compared our model to three state-of-the-art models that used raw images and clinical non-imaging data [4,5]. We randomly split the data into a training and a testing dataset. We used 4-fold cross-validation for training. For image synthesis, the models were trained for 200 epochs with a batch size of 4 using the Adam optimizer with learning rates of 2 × 10 -4 and 2 × 10 -5 for the generator and discriminator, respectively. For prognostic prediction, models with image inputs were trained for 100 epochs with a batch size of 4 using the Adam optimizer with a learning rate of 1 × 10 -5 . All of the models were trained independently. The experiments above were implemented using PyTorch on the NVIDIA 3090 24GB GPU. Logistic regression models were trained with hyperparameters using grid search (Supplementary Table S1) based on Scikit-learn.   "
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,3.1,Results of Image Synthesis and Prognostic Prediction,"Data Modalities for Prognostic Prediction. The performance of models using different combinations of data modalities is shown in Figs. 3 and4 and supplementary Table S2. Models that included CTP maps clearly had better performance (Fig. 3). When non-imaging data were incorporated into the models (Fig. 4), those with CTP maps outperformed those without. This demonstrates that the inclusion of CTP maps can increase the performance of prognostic prediction. The validation results are similar to these test data results (Table S2). Synthetic CTP Maps for Prognostic Prediction. The model incorporating the CTP maps generated by our proposed method shows the best performance of prognostic prediction compared to other synthesis methods (Table 2). This indicates that the inclusion of another clinical task can improve the outcome prediction. Also, the predictive performance of our synthetic maps is considerably closer to that of the acquired maps (bottom row) (ROC curves shown in Supplementary Fig. S2), indicating that the proposed method can recover most of its predictive ability when CTP maps are unavailable. Moreover, our model with the synthetic CTP maps also outperformed three state-of-the-art models trained on our dataset (Table 3) (ROC curves shown in Supplementary Fig. S2). This demonstrates that training strategies incorporating CTP map synthesis may be able to encourage the models to concentrate more on the most clinically relevant features in NCCT and CTA images for outcome prediction. Such training strategies may help build models that not only have better performance but are also clinically trusted, given their ability to demonstrate the replication of key clinical imaging features. Validation set results are similar to these (Supplementary Tables S3-S5)."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,4.0,Conclusion,"This study demonstrates that CTP maps, which are known to provide critical information for clinicians, also benefit prognostic prediction using deep learning techniques. When CTP maps are not available at hospital admission, their benefits can still be largely retained through image synthesis. Using multi-task learning with a simple clinical task, our model outperformed other synthesis methods in both image quality and the performance of prognostic prediction.Our synthetic CTP maps show key clinical features that are able to be readily discerned upon visual inspection. These findings verify the advantages of including additional CTP maps in LVO prognostication and establish the ability to effectively synthesize such maps to retain their benefits. While we acknowledge that our network architectures are not novel, we highlight the novelty of our architectures for stroke prognostication. The proposed framework can provide significant utility in the future to aid in the selection of patients for high-stakes time-critical EVT, particularly for those who have limited access to advanced imaging. Furthermore, by demonstrating the key clinical imaging features, our framework may improve confidence in building a clinically trusted model."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,,Table 1 .,Ours 82.5 ± 0.4 86.6 ± 0.2 60.7 ± 0.4 69.2 ± 0.5 31.6 ± 0.3 35.2 ± 0.1 20.0 ± 0.1 20.2 ± 0.1
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,1.0,Introduction,"Inner ear malformations are found in 20-30% of children with congenital hearing loss [1]. While the prevalence of bilateral congenital hearing loss is estimated to be 1.33 per 1000 live births in North America and Europe, it is much higher in sub-Saharan Africa (19 per 1,000 newborns) and South Asia (up to 24 per 1,000) [8]. Early detection of sensorineural hearing loss is crucial for appropriate intervention, such as cochlear implant therapy, which is prescribed to approximately 80,000 infants and toddlers annually worldwide [16]. Radiological examination is essential for an early diagnosis of congenital inner ear malformation, particularly when cochlear implant therapy is planned. However, detecting and classifying such malformations from standard imaging modalities is a complex task even for expert clinicians, and presents challenges during CI surgery [2]. Previous studies have proposed methods to classify congenital inner ear malformations based on explicit measurements and visual analysis of CT scans [5]. These methods are time-consuming and subject to clinician subjectivity. A suggested approach for the automated detection of inner ear malformation has relied on deep reinforcement learning trained for landmark location in normal anatomies based on an anomaly detection technique [9]. However, this method is only limited to the detection of a malformation but does not attempt to classify them.Currently, supervised deep metric learning garners significant interest due to its exceptional efficacy in data clustering and pathology classification. Most of these approaches are fully supervised and use supervisory signals that model the training by creating tuples of labeled training data. These tuples are then used to optimize the intra-class distance of the different samples in the latent space, as has been done mostly for 2D images [15,20,21] and 2D representation of 3D images [4]. Several recent studies have demonstrated promising outcomes from unsupervised contrastive learning from natural images. However, their utility in the medical image domain is limited due to the high degree of inter-class similarity. Particularly in heterogeneous real clinical datasets in which the image quality and appearance can significantly impact the performance of such methods, rendering them less effective. In [22] an unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text in 2D images is proposed. Typically, in 3D images, an unsupervised lowdimensional representation is utilized for further clustering, as demonstrated in [14]. Nonetheless, such approaches are commonly developed using quite homogeneous datasets that are not representative of real-world applications and the diverse clinical settings in which they must operate.Our objective is to develop a fully automated pipeline for the classification of inner ear malformations, utilizing a relatively large and unique dataset of such anomalies. The pipeline's design necessitates a profound comprehension of this data type and the congenital malformations themselves. Given the CT scans in this region are complex, and the images originate from diverse sources, we employ an unsupervised approach, uniquely based on the 3D shape of the cochlear structure. We have observed that the cochlear structure can be roughly but consistently segmented by a 3D-UNET model trained exclusively on normal cochlear anatomies. We then use these segmentations and adopt an entirely unsupervised approach, meaning the deep learning model is trained from scratch on these segmentations, and the class labels are not used for training. To map these shapes to an optimal latent space representation, we utilize DeepDiffusion, which combines the diffusion distance on a feature manifold with the feature learning of the encoder.In this paper, we present the first automatic approach for the classification of congenital inner ear malformations. We use an unsupervised method to find the latent space representation of cochlear shapes, which allows for their further classification. We demonstrate that shapes from a segmentation model trained on normative cases, albeit imperfect, can be used to represent abnormalities. Moreover, our results indicate the potential for successfully applying this approach to other anatomies."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,2.0,Data,"Our dataset comprises a total of 485 clinical CT scans, consisting of 364 normal scans and 121 scans with various types of inner ear malformations. The distribution of inner ear scans for each type of malformation is shown in Fig. 1. We utilized the region-of-interest (ROI) extraction technique developed by [18], which involves selecting anatomical points of interest that are not part of the inner ear region to achieve a standardized and robust image orientation. To ensure consistency, all images were resampled to a spacing of 0.125 mm, and their intensities were normalized by scaling the 5 th and 95 th percentiles of the intensity distribution of each image to 0 and 1, respectively. Figure 1 also shows the data split used for training our model. We chose to use an approximate 50% split for abnormal cases, while the vast majority of normal cases, approximately 86%, were used for training. Other configurations were explored, including using only normal cases for training. However, it was demonstrated that while this approach may work for anomaly detection, it does not adequately categorize the different types of malformations. Our aim is to find a parametrized shape that is representative of the anatomy of the patient. We decided to focus on the cochlear structure as it is the main structure of interest when trying to identify a malformation in the inner ear. To obtain a 3D segmentation of this structure we use the 3D-UNET [19] presented in [10] which has been trained exclusively in normal anatomies (130 images from diverse imaging equipment) and built using MONAI [12]. Even though no abnormal anatomies have been used for training, given the high contrast between the soft tissue of the cochlear structure and the bony structure that surrounds it, the model still performs quite well to segment the abnormal cases. This can be seen in Fig. 2 where an example of each of the types of malformations used in this study and an anatomically normal case are shown. The largest connected component of the segmentation has been selected to generate the final 3D meshes."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.1,Anatomical Representation,"An overview of our pipeline is presented in Fig. 3. Each 3D mesh obtained from a CT image is transformed into a 1024 point cloud using the Ohbuchi method [13]. Each shape is then normalized by centering its origin in its center of gravity and enclosing the shape within a unit sphere, resulting in the point cloud representation of the shape S. Before the shape S is fed to the encoder, the shape is augmented into shape Ŝ with a probability of 0.8. This augmentation consists of a random rotation with U(-5 • ,5 • ), an anisotropic scaling sampled from U(0.8,1), and a shearing and translation in each axes sampled from U(-0.2,0.2) for both actions. "
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.2,Deep Diffusion Algorithm,"The DeepDiffusion (DD) algorithm [7] incorporates the manifold ranking [23] technique, which uses similarity diffusion on the manifold graph to learn a distance metric among the samples. The DD algorithm optimizes both the feature extraction and the embeddings produced by the encoder, which results in salient features in a continuous and smooth latent space. In this latent space, the Euclidean distance among the latent features approximates the diffusion distance on the latent feature manifold. The crux behind this algorithm is the latent manifold ranking loss (LMR) which is computed using both intrinsic and extrinsic features. The LMR consists of a fitting term, L fit , a smoothing term, L smooth , and a balancing term, λ.Where θ characterizes the encoder and M ∈ R N X P represents the latent feature manifold formed by the training samples, where N is the number of data samples and P is the output dimensions of the encoder. The extrinsic feature f is defined as the output of the encoder and has dimension P . M is initialized by stacking together the embeddings of the first forward pass through the encoder which has been randomly initialized as this has been shown to perform better than randomly initializing the weights of M itself as shown in [7].Every training sample has its unique identification number (ID b ) which is used to specify a diffusion source y b that is consistent throughout the training procedure. L fit constrains the ranking vector r b to being close to the diffusion source y b , which is defined as the vector containing one-hot encoding of ID b . The ranking vector is defined as r b = softmax(f b M T ) and represents the probabilistic similarities between the feature f b and all the intrinsic features contained in M . The fitting term is therefore defined asits minimization results in all the extrinsic features being embedded farther away from each other as they are being pulled toward their respective and unique diffusion source vectors. The smoothing term is defined aswhere the dissimilarity operator is the Jensen-Shannon divergence [6] and t n = softmax(m n M T ) being m n the n th row of the matrix M so that t n contains the ranking score of the intrinsic feature m n to all the intrinsic features. w bn indicates the similarity between the extrinsic feature f b and the neighboring intrinsic feature m n and it is defined as:Minimizing L smooth pulls extrinsic features and their neighboring intrinsic features together which implies that an extrinsic feature is more likely to be projected onto the surface of the latent feature manifold of the intrinsic features when L smooth is smaller."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.3,Implementation,"For our encoder, we use the PointNet [3] architecture which takes 1024 3D points as input, applies input and feature transformations, and then aggregates point features by max pooling to a feature of dimensionality 1024 which is then compressed into dimensionality 254 with two sets of fully connected layers. The network has been trained by using mini-batch (of size 8) gradient descent using the Adam optimizer with a learning rate of 10 -8 and ReLU as the activation function. The DD algorithm is implemented in PyTorch [17] and the code used for this study is available at https://github.com/paulalopez10/ Deep-Diffusion-Unsupervised-Classification-3D-Mesh. The models are trained on an NVIDIA GeForce RTX 3070 Laptop GPU with 8GB VRAM. The different hyper-parameters related to the approach have been explored and it has been empirically found for this specific configuration λ = 0.6 and k = 10 produce the best results that will be analyzed in the following section."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,4.0,Results,"We evaluate the classification performance of our pipeline by analyzing the embeddings generated by the trained encoder. To visualize the projection of the features of the test in 2D we use the U-MAP [11], as illustrated in Fig. 4. The U-MAP visualization demonstrates the clustering of different classes in the latent space. Furthermore, it is very interesting to notice how the latent space representation displays the anatomical changes of the anatomy where the more extreme types of malformations (CA and CC) are the most distant to the normative cochlear structures. The transition between the different classes shown in the latent space properly represents the pathological variations in this anatomy.We have also included, in Fig. 4, the projection of the features projected in the 2D-PCA space defined by the training set, where both, training and testing, sets are included to show not only the clustering in this space but also the similar distribution of the different classes in both sets within the PCA projection. For a further analysis of the performance, we compute some evaluation metrics based on the pairwise cosine distance between samples that can be seen in Fig. 5 c). The average ROC and precision-recall curves for each of the classes can be seen in Fig. 5 a) and b). To calculate those, each test feature vector f b is considered to be the centroid of a kNN(f b ) which consists in the k nearest  features from other samples using the cosine pairwise distance shown in Fig. 5  c). We vary k until all the features from the corresponding class are within the cluster and compute the precision and false positive rate per the different recall steps, the shown results are the average among each class and overall. With the same procedure, different evaluation metrics have been obtained and are shown in Table 1. These metrics encompass the area under the curve (AUC) for the curves shown in Fig. 5 a) and b), both for the average curve and the optimal curve for each class. Furthermore, the maximum and average accuracy has been computed together with the maximum f1-score. Considering the dataset's significant class imbalance, these metrics provide a comprehensive assessment of the performance achieved. Finally, the mean average precision is also included in the table together with the optimal one for each class. The optimal or maximum value of each metric corresponds to when the optimal sample within our test features distribution is being evaluated as the centroid of its own class and the mean values are the average over all the samples. We can observe how a bigger variance is obtained for the classes that contain a few examples as it is expected, given the nature and distribution of our dataset shown in Fig. 1."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,5.0,Conclusion,"We have presented the first approach for the automatic classification of congenital inner ear malformations. We show how using the 3D shape information of the cochlea obtained with a model only trained in normative anatomies is enough to classify the malformations and reduces the influence of the image's source, which is crucial in a clinical application setting.Our method shows a mean average precision of 0.77 with a mean ROC-AUC of 0.91, indicating its effectiveness in classifying inner ear malformations. Furthermore, the representation of the different cases in the latent space shows spatial relation between classes, which is correlated with the anatomical appearance of the different malformations. These promising results pave the way towards assisting clinicians in the challenging assessment of congenital inner ear malformations potentially leading to improved patient outcome of cochlear surgery."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",1.0,Introduction,"In the UK, approximately 11,500 patients are diagnosed with rectal cancer each year [19]. A common form of treatment for such patients is neoadjuvant therapy, including chemotherapy and radiotherapy, which can be given to patients with locally advanced rectal cancer to shrink the tumour prior to surgery. Recent evidence suggests that 10-20% of patients will have a complete pathological response to neoadjuvant therapy and can therefore avoid surgery altogether [2,5]. However, one third of patients do not benefit from radiotherapy treatment prior to surgery [8], hence it is important to determine how a patient will respond to radiotherapy with a personalized approach in order to avoid overtreatment.Histology-based digital biomarkers enable the possibility to predict a patient's response to therapy. The consensus molecular subtypes (CMS) classification system derived from gene expressions [9] has been developed to provide biological insight into metastatic colorectal cancer. It has been shown that these four CMS classes can be predicted directly from the standard haematoxylin and eosin (H&E) stained slide images using deep learning [18]. Various studies have investigated the link between CMS and patient outcomes, suggesting that patients with tumour classified as CMS4, which features stromal invasion [9] and shows significantly higher stroma content [15], have worse survival rates compared to the other CMS classes [5]. Increased stromal content has independently been shown to be a predictor for increased risk of recurrence in early rectal cancer [10], and tumour immune infiltrate evaluated with Immunoscore is a useful prognostic marker [3]. The spatial organisation of the cancerous tissue has been identified as a biomarker for aggressiveness or recurrence [12], and Qi et al. [15] found that the features they developed representing spatial organisation reflected characteristics of the four CMS classes. Interactions between the epithelial tissue (cellular tissue lining) and other prevalent tissue types in the tumour microenvironment are also indicators of prognosis [15], since progression of colorectal cancer is dependent on both the epithelial and stromal tissues [20]. Other work has looked at predicting chemoradiotherapy response in rectal cancer patients from H&E images using different approaches, but without providing contextual interpretations [19,22].As opposed to predicting response to radiotherapy alone, we aim to analyse this prediction in the context of the overall tissue architecture and the tumour biology as captured by CMS. Input to our model is a standard H&E Whole Slide Image (WSI) which is split into smaller patches to overcome the memory limitations of existing GPUs. To achieve our goal we need to capture the heterogeneity at the slide level, which is why applying full or semi-supervised approaches on individual tiles followed by a slide aggregation method is not suitable. Instead, we build on recent graph neural network (GNN) approaches that allow us to model the entire WSI as a graph. As local cell communities form the nodes of such a graph it can effectively model the micro-anatomy of the tissue. At the same time it is possible to make predictions at the node-, graph-, and slide-level. Related Work. To predict the grading of colorectal cancer (CRC), both cellbased and patch-based graphs have been used in separate works [16,23], setting the nodes of the graph as either cell nuclei or square patches, defining the node features as either handcrafted or learned features, and then applying a GNN for outcome prediction. Another patch-based GNN approach to predicting genetic mutations in CRC from H&E slides found their model trained on colon cancer generalised well to rectal cancer. For other cancers, the SlideGraph pipeline clusters nuclei for the graph nodes, and provides node-level predictions to make their model more interpretable [13]. Other approaches to setting the graph nodes include using subgraphs to represent regions [14], and creating superpatches by combining patches [11]. Edges between the nodes are usually defined by a spatial distance metric, which helps model the spatial organisation of the tissue. Common choices for GNNs include a Graph Isomorphism Network (GIN) with jumping connectivity [7,13,14], as we use in this research.Our methodology proposes a novel and disease relevant approach to a more interpretable model that effectively supports a diagnostic task. Pathologists and oncologists can use this information to inspect the validity of the prediction result and interrogate key aspects of the spatial biology that is critical for patient management. Ultimately, this type of information that is not available today will help to characterise interactions between the tumour and the host tissue and therefore help to support choice of therapy. The developed framework combines self-supervised training of a Vision Transformer (ViT) to extract morphological features, a superpixel algorithm for determining nodes of a graph, and a GNN for predictions. We achieve 0.82 AUC predicting complete response to radiotherapy using deep learning on WSIs for CRC patients, whilst providing novel interpretability of the results."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",2.0,Methods,"In this section we present the patch-level feature extraction, provide the detail of the superpixel segmentation of the WSI, and illustrate the resulting graph presentation. A GNN with three branches for our output predictions is used to simultaneously make the three different predictions as shown in Fig. 1.Pipeline. For computational reasons, all images are split into patches of size 256 × 256 pixels. In order to have a common feature set all the way up to the last layer of the GNN, individual patches should be represented by morphological features that are label-agnostic. This last layer of the GNN then splits into three branches to predict response to radiotherapy, the CMS4 subtype classification for CRC, and epithelial tissue regions. This way we can guarantee the common latent features and derivation across branches, maintaining the contextual importance of each branch. The DINO framework [4] uses a self-distillation training approach, using data augmentation to locally crop the patches and train with a local-global student-teacher approach. We use the DINO framework to train a ViT in a self-supervised manner on our H&E slides [6], representing each patch with 384 features. We use only the training set to train this model, and use the image patches at 20x magnification. We extract patch-level features from each WSI using selfsupervised DINO training with a ViT model [4]. The SLIC superpixel algorithm segments the entire slide into smaller regions [1]. We calculate the mean patch features for these superpixel regions, and use the superpixel features and centers as our graph nodes, applying Delaunay triangulation to generate the edges of the graph. A GNN consisting of GINConv layers is trained on these fixed graphs, and the final layer splits into three separate MLP branches to provide predictions of three different outcomes, complete response (CR) to radiotherapy (RT), CMS4 classification, and epithelial tissue. An example output is visualized in Fig. 2.To find the nodes of the WSI graphs, we apply the SLIC superpixel algorithm [1] on the WSIs at 5x magnification to segment the tissue to capture cellular neighbourhoods that are roughly between 80-100 µm 2 /pixels in size. It can be seen that the superpixel boundaries consistently align with the boundaries of tissue compartments.The superpixels centers are used as the nodes of the graph, and the node features are the weighted mean of the corresponding patch features which overlap with the superpixel region. The edges of the graph are determined by nearest neighbours from Delaunay triangulation, as in SlideGraph [13].Building on the ideas introduced by SlideGraph [13] we use GINConv layers [21], adding tempering to avoid overfitting, and replace their logistic regression scaler with a simple sigmoid function. We add three branches to the final layer of the GNN, in the form of three separate multilayer perceptrons (MLPs). Two of these MLPs return a graph-level prediction, for the response to RT and CMS4 predictions, and the final branch returns node-level predictions, predicting whether each node is epithelial tissue or not. Our loss function is defined aswhere BCE is the binary cross entropy loss, ŷRT ∈ R is the slide-level prediction of response to radiotherapy, ŷCMS4 ∈ R is the slide-level prediction of CMS4, ŷepi ∈ R ni are the node-level predictions of epithelial tissue and n i is the number of nodes in the i th WSI graph.For each prediction branch, we can visualize the individual node predictions from the WSI graph, overlaid on the WSI itself, to get an idea of how the node predictions vary across the different tissue regions. Each graph-level prediction is derived from the corresponding branch node predictions, by applying pooling and dropout.Data. We train and validate our methods on two retrospective rectal cancer datasets, Grampian and Aristotle. Both cohorts received standard chemoradiotherapy of pelvic irradiation (45-50.4Gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . The pre-treatment biopsy slides were all sectioned and stained in the same laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an Aperio scanner. Pathological complete response, which we use as a target outcome here, was derived from histopathological assessment from posttreatment resections.The CMS labels for this data are derived from three different transcriptomic versions (single cohort, combined cohort correcting batch effects and combined cohort including 2036 cases run with the same platform), in order to generate robust classifications. In all cases the CMS call was calculated using the CMSclassifier random forest and single sample predictor [9]. Final CMS calls are based on matching calls between the three transcriptomic versions. Despite our efforts to minimise the noise from RNA sequencing, we still expect a certain level of noise in our ground truth data, which we discuss in the Results section.The epithelial labels for each graph node are calculated from epithelial masks for each WSI. These epithelial segmentation masks were generated at 10x magnification (1 µm 2 /pixel) with a U-Net [17] which was trained and validated on 666 full tissue sections belonging to 362 patients from the FOCUS cohort [18]. The ground truth annotations for the training of this model were generated by VK.For consistency the tumour regions were marked up by an expert pathologist. We use these masks in our analysis to filter out background and irrelevant tissue from the images. Grampian and Aristotle are used in both training and validation, with a 70/30% training-validation split, keeping any WSIs from a single patient in the same dataset. We predict complete response to radiotherapy against all other responses, such as partial response and no response. The datasets are unbalanced, since in Grampian only 61/244 slides have complete response, and in Aristotle only 24/121 slides have complete response. They are even more unbalanced for CMS4, since only 28/244 slides in Grampian and 17/121 slides in Aristotle are labelled with CMS4. We address this imbalance in the Supplementary Materials. There are 365 slides total in our dataset, from 249 patients."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",3.0,Experiments,"Implementation. We use the default DINO parameters, but train for 20 epochs with 5 warmup epochs. We apply the SLIC algorithm [1] with compactness of 20, setting the number of segments for each WSI as half the mean size of the WSI. Prior to fitting the graph model we normalize the node features relative to the whole dataset. We train our graph model for 30 epochs using Adam optimizer with learning rate 0.001 and weight decay 0.0001. Our graph model has three GINConv layers [21] with dimensions 64, 32 and 16 respectively. We apply dropout of 0.5 in-between graph layers, use minimum aggregation for message passing between nodes and use maximum pooling for concatenating the node activations. We apply tempering to the outcome of the graph model, dividing the output by 1.5. We evaluate the best validation epoch by finding the best mean AUC across the three prediction branches. We run the whole pipeline on four folds with different random data splits for training and validation. The code for this research will be made available upon request.Table 1. For each fold, we take the mean metrics for the three branch predictions from the best model on our validation data, with the best epoch chosen based on mean AUC for the three predictions. The standard deviation of the metrics across the four folds is provided in brackets. Each prediction uses an optimised threshold value determined from the validation set in order to round the output probabilities to a binary prediction. We use weighted metrics due to the class imbalance in our dataset."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",,Response branch,"Response Results. Despite the noise in our reference data used for training, our model achieves good performance in terms of mean AUC scores on all three prediction branches of our model, predicting complete response to radiotherapy (RT) with 0.819 AUC, CMS4 with 0.819 AUC and epithelial tissue at the node level with 0.760 AUC across folds. Further metrics are provided in Table 1. The prediction performance of the model could be improved by utilising a larger training dataset and performing more exhaustive parameter searches, however the current performance of the model is sufficient to demonstrate the impact of this approach.The predicted response to radiotherapy can now be viewed in the context of disease biology as captured by CMS4. For example, the model demonstrates that CMS4 patients are less likely to respond to radiotherapy. In addition, it is now possible to view the spatial distribution of CMS4 active regions in the tissue architecture context as shown in Fig. 2. Additional samples are presented in the Supplementary Materials. An example of our proposed prediction maps on two slides can be seen in Fig. 2, with further slides in the Supplementary Materials. A pathologist reviewing these maps assesses that the observed patterns fit the known interplay of response to therapy, CMS4 activation, and the spatial localisation of these signals. In the top slide, we observe high CMS4 activation in stromal rich regions, and interestingly also high CMS4 activation in the bottom center, dissociating from the response to RT activation map. This could be explained by the lymphocyte content, supported by the higher epithelial map activations in the same location. Expert pathologists highlight a similar pattern in certain regions of the maps for the bottom slide. Different from the slide above, the CMS4 and response to RT maps have some overlap with moderate activations here, encouraging discovery into tumour-host interactions. Ultimately, a pathologist confirmed that these maps support an interpretable and trustworthy prediction in the context of response to radiotherapy. While we cannot present a more extensive interpretation of these results due to space limitations, these examples already indicate that the proposed approach enables a level of analysis that has not been possible before. We find that predicting these outcomes individually in a single branch model, particularly with response to radiotherapy, can result in slightly higher AUC scores, but we consciously make this trade-off in order to provide better interpretability of the model predictions. The focus of this research is not to achieve the best possible metrics, but to develop robust methods which can add context and explanation to clinical black box deep learning model predictions, with the view to ease clinical translation of such models.To explore the effects of the noisy CMS4 ground truth labels, we remove from our dataset any WSIs classified as 'Unmatched' for the CMS call, which for the main results of this paper we defined as 'Not CMS4'. Removing this data and rerunning our analysis improved our predictions for CMS4 by +0.06 AUC, and reduced our response to radiotherapy and epithelial predictions by -0.02 and -0.01 respectively. The results can be found in the Supplementary Materials. These small changes indicate that the noise in our data does not degrade the performance of our classifier, reinforcing it as a robust and accurate model."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",4.0,Conclusion,"By setting the prediction of response to therapy in context with disease biology and spatial organisation of the tissue we are providing a novel approach for enhancing the interpretablity of complex prediction tasks. These results do not only enhance the interpretability, they also provide new ways to utilise large retrospective clinical trial cohorts for which no additional molecular data is available. Extending the amount of training data and improving model training will improve model performance, which is already impressive.We argue that this work also advances the state of the art in feature representation and analysis. Our prediction maps derive from the same graph model, and hence they share underlying graph features. The prediction branches only diverge at the final stage of translating these graph features into outcome predictions for our three clinically relevant outcomes. Importantly, this level of visualisation is not only accessible to pathologists, this joint prediction model also enhances the communication between pathologists and oncologists which is critical for patient management. By cross-referencing these prediction maps with our prior understanding of cancer biology, this approach can help to establish trust in the prediction model and also help to identify potential failure cases.This work relies on access to well annotated clinical trial samples which will limit our ability to include more data for training and testing. In future, we plan to use these methods to help better characterise tumour-stromal interactions of the tissue. We also plan to use a denser graph with less connectivity to be able to better predict the heterogeneous epithelial tissue.The Aristotle trial was funded by Cancer Research UK (CRUK/08/032). The funders played no role in the analyses performed or the results presented. Financial support: RW -EPSRC Center for Doctoral Training in Health Data Science (EP/S02428X/1), Oxford CRUK Cancer Centre; VHK -Promedica Foundation (F-87701-41-01) and Swiss National Science Foundation (P2SKP3_168322/1, P2SKP3_168322/2); TSM -S:CORT (see above); JR, KS -Oxford NIHR National Oxford Biomedical Research Centre and the PathLAKE consortium (InnovateUK). The computational aspects of this research were funded from the NIHR Oxford BRC with additional support from the Wellcome Trust Core Award Grant Number 203141/Z/16/Z. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_73.
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,1.0,Introduction,"Acute ischemic stroke (AIS) is a disease of ischemic necrosis or softening of localized brain tissue caused by cerebral blood circulation disturbance, ischemia, and hypoxia [1,2]. Intravenous thrombolysis can be used to open blood vessels within 4.5 h. For patients with large vessel occlusion, the internal blood vessels can be opened by removing the thrombus within 6 h. Therefore, determining the time since stroke onset (TSS) is crucial for creating a treatment plan for AIS patients, with a TSS of less than 6 h being critical. However, approximately 30% of AIS occurs at unknown time points due to wake-up stroke (WUS) and unknown onset stroke (UOS) [3]. For such patients, determining the TSS accurately is challenging, and they may be excluded from appropriate treatment. Computed tomography perfusion (CTP) is processed with special software to generate perfusion maps: cerebral blood volume (CBF), cerebral blood volume (CBV), mean transit time (MTT), and peak response time (Tmax) [4]. These perfusion maps can provide sufficient information on cerebral blood flow, ischemic penumbra, and infarction core area.There are some machine learning methods to determine the TSS of AIS by automatic discrimination [5][6][7][8]. For example, Ho et al. [5] first used auto-encoder (AE) to learn magnetic resonance imaging (MRI) and then put the learned and original features into the classifier for TSS classification. Lee et al. [7] analyzed diffusion-weighted imaging (DWI) and fluid-attenuated inversion recovery (FLAIR) using automatic image processing methods to obtain appropriate dimensional features and used machine learning to classify the TSS. The above machine learning-based TSS classification methods often use regions of interest (ROI) while ignoring the spatial correlation between neural images. Several researchers try to employ deep learning techniques to classify AIS TSS, considering the spatial correlation among neural images. Zhang et al. [9] designed a new intra-domain task adaptive migration learning method to classify the TSS of AIS. Polson et al. [10] designed the neighborhood and attention network with segmented weight sharing to learn DWI, apparent diffusion coefficient (ADC), and FLAIR, then used weighted softmax to aggregate sub-features and achieve TSS classification.With the small samples and the high dimension of CTP, the convolution neural network (CNN) cannot effectively extract features, resulting in the problem of network non-convergence. Additionally, some existing TSS classification methods leverage multi-map mainly by simple linear connections, which do not thoroughly learn the supplementary information of CTP [6,10]. Therefore, we design a classification model based on dynamic convolution and multi-map fusion to classify the TSS. Firstly, we replace the ordinary convolution in the feature extraction network with dynamic convolution (DConv) [11] to improve the network performance without increasing the network complexity. Secondly, low-order multi-map features are fused to enhance the acquisition of local information by multi-scale feature fusion (MFF). Then high-order features are fused to obtain the global association information by transformer fusion (Transfus). Finally, multi-head pooling attention (MPA) is used to emphasize the high-order features, and the most discriminative features are selected to classify TSS."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.0,Methodology,"The main framework of our proposed method is depicted in Fig. 1. Specifically, the feature extraction of each map is performed independently using four feature extraction networks, each consisting of five stages. Dconv [11] is used to improve network performance without increasing complexity. In the first three stages, MFF is used to capture and fuse the details of different scales of multi-map features. In the last two stages, Transfus is used to fuse the global correlation of the high-order features. The learned multi-map high-order features are put into the MPA to learn them further and merge the potential tensor sequence. Finally, the selected features are put into the full connection layer to achieve the classification of TSS.  The feature extraction network of a single map consists of five stages. The structure of each stage is shown in Fig. 1 (a). Considering the high data dimension and the small number of samples, the network layers are too deep to cause over-fitting. We replace traditional convolution with dynamic convolution [11]. DConv improves the model expression ability by fusing multiple convolution cores, and its structure is shown in Fig. 1 (b). DConv will can not increase the network complexity and thus improve the network performance. It will not increase too many parameters and calculation amount while increasing the capacity of the model. Inspired by static perceptron [12], Dconv has k convolution cores, which share the same core size and input/output dimensions. An attention block is used [13] to generate the attention weight of k convolution cores, and finally aggregate the results through convolution. The formula is shown in Eq. ( 1):"
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.1,Dynamic Convolution Feature Extraction Network,"where π i (x) is the weight of the i-th convolution, which varies with each input x. . The attention block compresses the features of each channel through global average pooling and then uses two fully connected layers (with ReLU activation function between them) and a Softmax function to generate the attention weight of k convolution cores."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.2,Multi-map Fusion Module,"For multi-map information fusion of low-order features, considering the small area of acute stroke focus, a multi-scale attention module is used to fuse multi-map features in the first three stages. Its structure is shown in Fig. 1 (c). Record the output feature of each stage as x ij (i = 1, 2, 3; j = 1, 2, 3, 4), where i represents the output feature of the i-th stage, and j represents the j-th map. The feature x i of the input multi-scale channel attention module are denoted as:By setting different global average pooling (GAP) sizes, we can focus on the interactive feature information in channel dimensions at multiple scales, and aggregate local and global features. Through point-wise convolution (PWConv), point-wise channel interaction is used for each spatial location to realize the integration of local information. The local channel features are calculated as follows:The global channel features are calculated as follows:The final feature x i is calculated as follows:For the fusion of multi-map information of high-order features, considering the relevance of global information between different modes, the self-attention mechanism [14] in the transformer is used to learn multi-map information, and its structure is shown in Fig. 1 (d). Specifically, the output characteristic of each stage is x ij (i = 4, 5; j = 1, 2, 3, 4), where i represents the output feature of the i-th stage, and j represents the j-th mode. Similar to the previous work of some scholars [15][16][17][18], we think that the middle feature map of each mode is a set rather than a patch, and treat each element in the set as a token [19]. At this time, each token takes into account all the token information of the four branches. Finally, the fused features are superimposed on the branches for the next stage. Let the characteristic x ∈ R N ×D of the input transformer block, where N is the number of tokens in the sequence and each token is represented by the feature vector of dimension D. It uses the scaling dot product between Query and Key to calculate the focus weight and aggregates the value of each Query. Finally, nonlinear transformation is used to calculate the fused output features x out . "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.3,Multi-head Pooling Attention,"With the deepening of the network layers, the semantic information contained in the output features becomes higher and higher. After the post-fusion of the branch network, we use an MPA to learn the high-order semantic details further. Here, a smaller number of tokens is used to increase the dimension of each token to facilitate the storage of more information. Unlike the original multi-head attention (MHA) operator [14], the multi-head pooling attention module gathers the potential tensor sequence to reduce the length of the input sequence, and its structure is shown in Fig. 1 (e). Like MHA [14], Query, Key, and Value are obtained through the linear operation. Add the corresponding pooling layer to Query, Key, and Value to further sample it."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,3.1,Experimental Configuration,"Dataset and Data Preprocessing. The dataset of 200 AIS patients in this paper is from a local hospital. The patients are divided into two categories: positive (TSS < 6 h) and negative (TSS ≥ 6 h). Finally, 133 in the positive subjects and 67 in the negative subjects are included. Each subject contains CBF, CBV, MTT, and Tmax. The size of all CTP images is set to 256 × 256 × 32.Experimental Setup. The network structure is based on PyTorch 1.9.0 framework and CUDA 11.2 Titan × 2. We use a five-fold cross-validation method to verify the effectiveness of our method. 80% of the data is used as a training set and 20% as a test set. During the training process, the Adam optimizer optimizes the parameters, and the learning rate is set to 0.00001. The learning strategy of fixed step attenuation is adopted, in which the step size is set to 15, γ is 0.8. The number of iterations of training is 50."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,3.2,Experimental Results and Analysis,"Comparative Study. We evaluate the effectiveness of our method by comparing it with other approaches on the same dataset [20][21][22][23][24][25]. The results in Table 1 demonstrate that our model achieves at least a 5% higher accuracy than the other methods and outperforms them in other evaluation indicators. Table 1 also shows each method's area under the curve (AUC). Our method achieves an 81% AUC in the TSS classification task, indicating its superiority. To provide a more intuitive comparison of the model's performance under different indicators, we created radar charts to represent the evaluation results of the comparative experiment, as shown in Fig. 2. These charts indicate that our method performs well in all evaluate indicators. To verify the reliability of our method, we conduct T-test verification on the comparison methods and find the p-value to be less than 0.05. Therefore, we believe that our method is valid.    Fusion Effectiveness. To effectively fuse the image features of CBV, CBF, MTT, and Tmax and realize the task of classifying TSS, This section verifies the fusion method in this paper. We mainly compare it with five different feature fusion methods. They are 1) Addition Fusion (AF); 2) Concatenation with Conv Fusion (CCF); 3) Addition with Conv Fusion (ACF); 4) Deep Concatenation Fusion (DCF); 5) Deep Addition Fusion (DAF). The details of these five fusion methods are shown in Fig. 3. Based on the feature extraction network used in this paper, the comparison results are shown in Table 2. It can be seen from the results that the addition fusion method can better fuse features than the concatenation fusion method. The method we proposed is also to fuse features with the addition method. Our method is better than 1) and 4) because we have further learned the fused features. Therefore, our fusion method is the best.Ablation Study. To assess the efficacy of each module in the proposed method, a series of ablation experiments are conducted by gradually incorporating the four main modules, namely Dconv, MFF, TransF, and MPA, into the backbone network. The results of these experiments are presented in Table 3. The first four lines in Table 3 indicate that adding a single module is sufficient to enhance the performance of the backbone net-work. The CTP data is characterized by small size and high dimension, posing challenges to the deep learning model training. The network can extract more critical features without increasing the network depth by substituting the convolution with dynamic convolution on the backbone network. Map fusion on the backbone net-work improves the model accuracy by exploiting complementary information from multiple maps. Subsequently, MPA is added to extract more profound features from the learned features, ultimately improves the model's overall performance. In conclusion, the ablation experiments demonstrate that including the four modules in the proposed method positively impacts the model performance.Map Combination Experiment. To investigate the impact of different modes on the time window of disease onset, a series of experiments are conducted on various mode combinations using the techniques proposed in this study. The results of different map combinations are presented in detail in Table 4. Comparing the results of the second to fourth rows in Table 4 shows that the CBV mode fusion exhibits a higher classification rate, which is superior to the fusion of other groups of modes. Furthermore, the experimental outcomes of the single map are slightly lower than the experimental results after fusion, indicating that multi-map fusion can assist the feature extraction network in obtaining more crucial disease information, thereby enhancing the effectiveness of our network. These observations demonstrate that our approach can improve the TSS classification result by learning the multi-map relationship.Comparison with SOTA Methods. In Table 5, we have chosen relevant works for comparison. These studies aim to classify TSS based on brain images, with a time threshold of 4.5 h. The results demonstrate that our method performs relatively well in comparison. Specifically, our method achieves the best AUC and ACC among all methods, as reported in [8]. This may be attributed to the relatively large size of their dataset. Compared to studies such as [7,26], our method achieves better results despite using the same amount of data. "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,4.0,Conclusion,"In this study, we propose a TSS classification model that integrates dynamic convolution and multi-map fusion to enable rapid and accurate diagnosis of unknown stroke cases.Our approach leverages the dynamic convolution mechanism to enhance model representation without introducing additional network complexity. We also employ a multi-map fusion strategy, consisting of MFF and TransF, to incorporate local and global correlations across low-order and high-order features, respectively. Furthermore, we introduce an MPA module to extract and incorporate as much critical feature information as possible. Through a series of rigorous experiments, our proposed method outperforms several state-of-the-art models in accuracy and robustness. Our findings suggest that our approach holds immense promise in assisting medical practitioners in making effective diagnosis decisions for TSS classification."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,,Table 4 .,"combination study (%) (V: CBV, F: CBF, M: MTT, and T: Tmax)."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,,"London WC2R 2LS, UK","Abstract. Synthetic images generated from deep generative models have the potential to address data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fréchet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that there is an inevitable trade-off between synthetic image fidelity, variety, and privacy. In addition, we have empirically demonstrated that the utility score does not require images with both high fidelity and high variety. For intra-and cross-task data augmentation, mode-collapsed images and low-fidelity images can still demonstrate high utility. Finally, our experiments have also showed that it is possible to produce images with both high utility and privacy, which can provide a strong rationale for the use of deep generative models in privacy-preserving applications. Our study can shore up comprehensive guidance for the evaluation of synthetic Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_2."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,1.0,Introduction,"Fig. 1. The major contribution (a) and conclusions (b,c) of this work. Our study proposes a comprehensive and disentangled four-dimensional framework for synthetic medical images, incorporating fidelity, variety, privacy, and utility. We conduct extensive experiments to investigate the interplay between these dimensions and identify a set of best practices (c) for selecting synthetic models for downstream tasks.In 2002, SMOTE [1] was proposed to generate synthetic samples to increase the accuracy of classification tasks. Since then, synthetic data has emerged as a promising solution for addressing the data scarcity problem by generating additional training data to supplement the limited real-world datasets. In addition, the potential of synthetic data for privacy preservation has led to the development of generative deep learning models that have shown promise in producing high-quality synthetic data which maintain the statistical properties of the original data while preserving the privacy of individuals in the training data.Deep learning practitioners have been using various metrics to evaluate synthetic images, including Fréchet Inception Distance (FID) [4], Inception Score (IS) [10], precision, and recall [7]. However, these measurements are entangled, i.e., they are only able to measure image quality holistically, not a specific aspect. For example, FID is defined byHere, where μ 1 , μ 2 Σ 1 , and Σ 2 are the mean vectors and covariance matrices of the feature representations of two sets of images. A high difference between the image diversity (Σ 1 and Σ 2 ) also leads to a high FID score, which further complicates fidelity evaluation. Another entangled fidelity evaluation is the precision. As is shown in Fig. 2 (a), a high-precision matrix cannot identify non-authentic synthetic images which are copies of real data. Thus, a high-precision matrix can either be caused by high fidelity, or a high privacy breach.When evaluating these entangled metrics, it is difficult to find the true weakness and strengths of synthetic models. In addition, large-scale experiments are currently the only way to measure the utility of synthetic data. The confusion of evaluation metrics and this time and resource-consuming evaluation of synthetic data utility increase the expenses of synthetic model selection and hinder the real-world application of synthetic data.In this study, we aim to provide a set of evaluation metrics that are mathematically disentangled and measure the potential correlation between different aspects of the synthetic image as in Fig. 1 (a). Then, we aim to analyze the predictive ability of these proposed metrics to image utilities for different downstream tasks and provide a set of best practices for selecting synthetic models in various clinical scenarios. We compare two state-of-the-art deep generative models with different parameters using a large open-access X-ray dataset that contains more than 100k data [5]. Through our experiments, we empirically show the negative correlations among synthetic image fidelity, variety, and privacy (Fig. 1  (b)). After analyzing their impacts on downstream tasks, we discovered that the common problems in data synthesis, i.e., mode collapse and low fidelity, can sometimes be a merit according to the various motivations of different downstream tasks.Overall, our study contributes new insights into the use of synthetic data for medical image analysis and provides a more objective and reproducible approach to evaluating synthetic data quality. By addressing these fundamental questions, our work provides a valuable foundation for future research and practical applications of synthetic data in medical imaging."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,2.0,Deep Generative Models and Evaluation Metrics,"In this study, we conducted an empirical evaluation using two state-of-theart deep generative models: StyleGAN2, which has brought new standards for generative modeling regarding image quality [11] and Latent Diffusion Models (LDM) [9]. We proposed an analysis framework for synthetic images based on four key dimensions: fidelity, variety, utility, and privacy. Manual evaluation of synthetic image fidelity typically involves human experts assessing whether synthetic images appear realistic. However, this evaluation can be subjective and have high intra-observer variance. While many algorithms can be used to measure synthetic image quality, most of them are designed to capture more than one of the four key dimensions.With this motivation, we aimed to redefine the conventional evaluation metrics of synthetic images and disentangle them into four independent properties. In this study, we employed a metric-based membership inference attack in an unsupervised manner to evaluate the privacy of our model. We presume that synthetic records should have some similarity with the records used to generate them [2]. Since differential privacy settings could significantly reduce image fidelity, we chose not to perform differential privacy analysis in this study.  -Copy set S copy i : Synthetic sets that are realistic but are also copies of this real image r i . Synthetic images will belong to this set if this synthetic image is closer to r i than any other real images, i.e., D(s j , r i ) < D(r i , r i,1 ). -Real set S real i : Synthetic sets that are realistic and are not copies of this real image r i . Synthetic images will belong to this set if this synthetic image is in the kth-nearest neighbor of r i , i.e., D(s j , r i ) ∈ [(D(r i , r i,1 ), D(r i , r i,k )] -Non-real set S non-real i : Synthetic sets that are not realistic compared to r i . Synthetic images will belong to this set if this synthetic image is not the kth-nearest neighbor of r i , i.e., D(s j , r i ) > D(r i , r i,k )We can compute the privacy preservation score of a single synthetic image s j |j ∈ [0, N)] with the definition of three sets. If the synthetic data p j is in the copy set of any real data, the privacy protection ability of this p j is 0, i.e.,For the synthetic data, the overall privacy protection ability P ∈ [0, 1] was then defined byWith this privacy definition, we have adjusted the original fidelity evaluation [7] shown in Eq. ( 4) to a privacy violation considered formula f p in Eq. 5,The measurements of image distance can be tricky due to the high resolution of the original images (512×512). Thus, we first used VQ-VAE [8,13] to quantize all images to integral latent feature maps, i.e., each pixel in the latent feature maps is a Q-way categorical variable, sampling from 0 to Q -1, and then compute the Hamming distance between these images. In our experiments, we used Q = 256.Variety. To measure the variety, we introduced the JPEG file size of the mean image. The lossless JPEG file size of the group average image was used to measure the inner class variety in the ImageNet dataset [3]. This approach was justified by the authors who presumed that a dataset containing diverse images would result in a blurrier average image, thus reducing the lossless JPEG file size of the mean image. To ensure that the variety score is positively correlated with the true variety, we normalized it to [0, 1] across all groups of synthetic images, and then subtracted it from 1 to obtain the final variety score. It is worth noting that variety can also be quantified by the standard deviation of the discrete latent features. However, in our study, we chose to measure variety in the original image space to better align with human perception.Utility. We divided our X-ray dataset into four groups: training datasets A1 and A2, validation set B, and testing set C. We also included an additional open-access pediatric X-ray dataset, D. For our simulation, we treated A1 as a local dataset and A2 as a remote dataset that cannot be accessed by A1. We evaluated the utility of synthetic data in two conditions:1. A1 vs. adding synthetic data generated from A1. In this condition, no privacy issue is considered. 2. A1 vs. adding synthetic data generated from A2. In this condition, synthetic data will be evaluated using privacy protection skills.Under both conditions, we evaluated the intra-task augmentation utility and cross-task augmentation utility to simulate real-world use cases for synthetic data. Intra-task augmentation utility is measured by the percentage improvement in classification accuracy of C when adding synthetic data to the training dataset. We used a paired Wilcoxon signed-rank test to assess the significance of the accuracy improvement. If the improvement is significant, it indicates that the synthetic images are useful. We compared the augmentation utility with simple augmentations, such as random flipping, rotating, and contrasting. The cross-task augmentation utility is determined by the power of features extracted from the models trained with synthetic data. We used the models to extract features from D and trained a Support Vector Machine classifier on these features to measure accuracy. This allowed us to evaluate whether synthetic images can provide powerful features that facilitate downstream tasks. Similarly, the cross-task augmentation utility is also the percentage improvement in classification accuracy compared to the model trained only on A1."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,3.0,Experimental Settings and Parameters,"We primarily evaluated the performance of synthetic data on the CheXpert dataset, with a focus on identifying the presence of pleural effusion (PE). To perform our evaluation, we split the large dataset into four subsets: A1 (15004 with PE and 5127 without PE), A2 (30692 with PE and 10118 without PE), B (3738 with PE and 1160 PE), and C (12456 with PE and 3863 without PE). To evaluate the cross-task utility of synthetic models, we used an X-ray dataset D1 consisting of 5863 images of pediatric patients with pneumonia and normal controls. We resized all X-ray images to a resolution of 512×512 before evaluation.For the StyleGAN2 method, we utilized six truncation parameters during sampling to generate six sets of synthetic images (φ ∈ [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]). In total, we trained 16 classification models on different combinations of datasets, including A1, A1+A2, A1+StyleGAN2-synthesized A1 (6 models), A1+LDMsynthesized A1, A1+StyleGAN2-synthesized A2 (6 models), and A1+LDM-synthesized A2. For further information on implementation details, hyperparameters and table values, please refer to our supplementary file and our publicized codes https://github.com/ayanglab/MedSynAnalyzer."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.1,The Proposed Metrics Match with Human Perception,"In our work, we proposed to use VQ-VAE to extract discrete features from original high-resolution X-ray images. To prove the validity of our method, we selected twenty images from each synthetic dataset and dataset A1 and invited two clinicians to rate the fidelity and variety manually. The human perceptual fidelity is rated from 0 to 1; and the human perceptual variety is computed by the percentage of different scans identified from the selected twenty synthetic images, i.e., if they thought all twenty patients were derived from the same scan, the human perceptual variety score is 1/20 = 0.05. To assure a fair comparison, we allow discussion between them. The result is shown in Fig. 2 (c). The fidelity and variety score calculated with our method matched perfectly with human perception (p < 0.05), and FID, which was highly influenced by mode collapse and increased over the diversity, failed to provide a valid analysis of image fidelity."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.2,The Trade-Off Between Fidelity and Variety,"All of our experiments showed a strong negative correlation between variety and fidelity, with a Pearson's correlation coefficient of -0.92 (p < 0.01). As it is widely known in GAN-based models, fidelity and variety are in conflict [6]. In this study, we further validated this by introducing the LDM model and demonstrating empirically that deep generative models inevitably face the trade-off between variety and fidelity (as shown by the grey lines in Fig. 3 (a-b)). In this study, we cannot find a significant correlation of utility between neither dimension (fidelity, variety nor privacy), indicating that there is currently no way to measure the utility except for large-scale experiments. However, we did observe a similar pattern of utility in our experiments shown in Fig. 3 (a-b). First, the intra-task augmentation utility favors synthetic data with higher fidelity (i.e., high F ), even under mode collapses. For instance, when φ = 0.2 for StyleGAN, a mode collapse was observed. The model is producing images that look similar to each other, resulting in a mean image with a sharp contrast (Fig. 4 (a4)). However, this mode collapse seems to highlight the difference between the presence and non-presence of PE (Fig. 4 (c)), leading to a performance improvement in intra-task augmentation. The PE in X-ray images is fluid in the lowest part of the chest, forming a concave line obscuring the costophrenic angle and part or all of the hemidiaphragm. These opacity differences were highlighted in the mode-collapsed images, which, on the other hand, improved the classification accuracy of PE identification.For cross-task augmentation, synthetic data with a higher variety is favored for its utility as mode collapse can limit the focus of classification networks and lead to poor generalization performance on other tasks. For instance, a network trained to focus on lung opacity differences near the hemidiaphragm may not help in the accurate diagnosis of pediatric pneumonia, which is the motivation behind dataset D. As shown in Fig. 3 (b), synthetic data with low variety but high fidelity is unable to contribute to powerful feature extraction. Therefore, variety in synthetic data is crucial for effective cross-task augmentation.As mentioned, we invited two radiologists to visually assess synthetic images. The visual inspection showed that all twenty LDM-synthesized images were easily recognized as fake due to their inability to capture the texture of X-ray images, as shown in the supplementary file. However, the shape and boundaries of the lungs were accurately captured by LDM. Despite their low visual fidelity, we demonstrated that these synthetic images still contribute to powerful feature extraction, which is crucial for cross-task utility."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.4,What Kind of Synthetic Data is Desired by Downstream Tasks,"When Privacy is an Issue?It is also discussed in the literature about the dilemma between utility and privacy [12]. We reached a similar conclusion, i.e., there is a trade-off between utility and privacy in intra-class classification tasks, for which fidelity is considered to be crucial. Thus, a shift in the image domain could lead to a decrease in intra-task utility. However, as shown in Fig. 4 (d), we observed that privacy and utility are not always conflicting. As discussed earlier, for cross-task augmentation, the utility favors synthetic images with high variety rather than high fidelity. Therefore, we demonstrated that it is possible to achieve both privacy and utility in cross-task augmentation scenarios. Figure 4 (d) shows an interesting positive correlation between privacy and cross-task utility. However, it is important to note that this does not imply a causal relationship between privacy and cross-task utility. Rather, the positive correlation is caused by the mode collapse during synthesis. Mode collapse can lead to a lack of diversity in generated data, which in turn can make it easier to identify individuals or sensitive information in the generated data, i.e., mode collapses are more likely to result in a high possibility of privacy breach as well as low cross-task utility."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,5.0,Conclusion,"In this work, we proposed a four-dimensional evaluation metric for synthetic images, including a novel privacy evaluation score and utility evaluation score. Through intensive experiments in over 100k chest X-ray images, we drew three major conclusions which we can envision that have broad applicability in medical image synthesis and analysis.Firstly, there is an inevitable trade-off among different aspects of synthetic images, especially between fidelity and variety. Secondly, different downstream tasks require different properties of synthetic images, and synthetic images do not necessarily have to reach high metric scores across all aspects to be useful. Traditionally, low fidelity and mode collapses have been treated as disadvantages in data synthesis, and numerous algorithms have been proposed to fix these issues. However, our work demonstrates that these failures of synthetic data do not always sabotage their utility as expected. Lastly, we have showed that it is possible to achieve both privacy and utility in transfer learning problems.In conclusion, our work contributes to the development of synthetic data as a valuable solution to enrich real-world datasets, to evaluate thoroughly medical image synthesis as a pathway to overall enhance medical image analysis tasks."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,1.0,Introduction,"The brain functional connectome (FC) is a graph with brain regions of interest (ROIs) represented as nodes and pairwise correlations of fMRI time series between the ROIs represented as edges. FC has been shown to be a promising biomarker for the early diagnosis and tracking of neurodegenerative disease progression (e.g., Alzheimer's Disease (AD)) because of its ability to capture disease-related alternations in brain functional organization [25,26]. Recently, the graph neural networks (GNN) has become the model of choice for processing graph structured data with state-of-the-art performance in different tasks [2,11,20]. With regards to FC, GNN has also shown promising results in disease diagnosis [3,4,8,15,23]. However, such studies have only focused on FC at a single time point. For neurodegenerative diseases like AD, it is crucial to investigate longitudinal FC changes [5], including graph topology and attributes, in order to slow down or even halt disease advancement.Node features are commonly utilized in FC to extract important information. It is also essential to recognize the significance of edge features in FC, which are highly informative in characterizing the interdependencies between ROIs. Furthermore, node embeddings obtained from GNN manipulation contain essential information that should be effectively leveraged. Current GNNs feasible to graphs with multiple time points [16,22,24] are suboptimal to FC trajectory, as they fail to incorporate brain edge feature embeddings and/or they rely on conventional operation (e.g., global pooling for readout) which introduces inductive bias and is incapable of extracting sufficient information from the node embeddings [21]. Moreover, these models lack built-in interpretability, which is crucial for clinical applications. And they are unsuitable for small-scale datasets which are common in fMRI research. The longitudinal data with multiple time points of the AD continuum is even more scarce due to the difficulty in data acquisition.In this work, we proposed Brain Tokenized Graph Transformer (Brain TokenGT), the first framework to achieve FC trajectory embeddings with builtin interpretability, shown in Fig. 1. Our contributions are as follows: 1) Drawing on the distinctive characteristics of FC trajectories, we developed Graph Invariant and Variant Embedding (GIVE), which is capable of generating embeddings for both nodes and spatio-temporal edges; 2) Treating embeddings from GIVE as tokens, Brain Informed Graph Transformer Readout (BIGTR) augments tokens with trainable type identifiers and non-trainable node identifiers and feeds them into a standard transformer encoder to readout instead of global pooling, further extracting information from tokens and alleviating over-fitting issue by token-level task; 3) We conducted extensive experiments on two public resting state fMRI datasets (ADNI, OASIS) with three different tasks (Healthy Control (HC) vs. Mild Cognition Impairment (MCI) classification, AD conversion prediction and Amyloid positive vs. negative classification). Our model showed superior results with FC trajectory as input, accompanied by node and edge level interpretations."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.1,Problem Definition,"The input of one subject to the proposed framework is a sequence of brain networks G = [G 1 , G 2 , ..., G t , ..., G T ] with T time points. Each network is a graph G = (V, E, A), with the node set V = {v i } M i=1 , the edge set E = V × V , and the weighted adjacency matrix A ∈ R M ×M describing the degrees of FC between ROIs. The output of the model is an individual-level categorical diagnosis ŷs for each subject s."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.2,Graph Invariant and Variant Embedding (GIVE),"Regarding graph topology, one of the unique characteristics of FC across a trajectory is that it has invariant number and sequence of nodes (ROIs), with variant connections between different ROIs. Here, we designed GIVE, which consists of Invariant Node Embedding (INE) and Variant Edge Embedding (VEE)."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,,Invariant Node Embedding (INE).,"To obtain node embeddings that capture the spatial and temporal information of the FC trajectory, we utilized evolving graph convolution [16] for the K-hop neighbourhood around each node which could be seen as a fully dynamic graph, providing a novel ""zoom in"" perspective to see FC. As suggested in [16], with informative node features, we chose to treat parameters in graph convolutional layers as hidden states of the dynamic system and used a gated recurrent unit (GRU) to update the hidden states.Formally, for each node v i in V , we define a dynamic neighbourhood graph as G i = [g i1 , g i2 , .., g it , ..., g iT ] (Fig. 1), in which g it is the K-hop neighbourhood of node v i at time point t, with adjacency matrix A it . At time t, for dynamic neighbourhood graph G i , l-th layer of evolving graph convolution first updates parameter matrix W l i(t-1) from the last time point to W l it with GRU, then the node embeddings H l it are updated to H l+1 it for next layer using graph convolution network (GCN) [11]:Variant Edge Embedding (VEE). For tasks such as graph classification, an appropriate representation of edges also plays a key role in the successful graph representation learning. To achieve edge embeddings, we first integrated graphs from multiple time points by defining Spatial Edge and Temporal Edge, and then obtained spatial and temporal edge embeddings by transforming an FC trajectory to the dual hypergraph.For each FC trajectory, we should not only investigate the edges between different ROIs in one static FC (i.e., spatial domain) but also capture the longitudinal change across different time points (i.e., time domain). Instead of focusing only on intrinsic connections (i.e., spatial edges (e s )) between different ROIs in each FC, for each of the two consecutive graphs G t and G t+1 , we added M temporal edges (e t ) to connect corresponding nodes in G t and G t+1 , with weights initialized as 1. The attached features to spatial and temporal edges were both initialized by the concatenation of node features from both ends and their initial weights.Accordingly, one trajectory would be treated as a single graph for downstream edge embedding. We denote the giant graph with T time points contained as G T , with weighted adjacency matrix A T ∈ R T M×T M (Fig. 1). G T was first transformed into the dual hypergraph G T * by Dual Hypergraph Transformation (DHT) [7], where the role of nodes and edges in G T was exchanged while their information was preserved. DHT is accomplished by transposing the incidence matrix of the graph to the new incidence matrix of the dual graph, which is formally defined as:is the original node features matrix with a D dimensional feature vector for each node, M ∈ R |E|×M is the original incidence matrix, and E ∈ R |E|×(2D+1) is the initialized edge features matrix.We then performed hypergraph convolution [1] to achieve node embeddings in G T * , which were the corresponding edge embeddings in G T . The hypergraph convolution at l th layer is defined by:where W * is the diagonal hyperedge weight matrix, D and B are the degree matrices of the nodes and hyperedges respectively, and Θ is the parameters matrix.Interpretability is important in decision-critical areas (e.g., disorder analysis). Thanks to the design of spatio-temporal edges, we could achieve built-in binary level interpretability (i.e., both nodes and edges contributing most to the given task, from e t and e s , respectively) by leveraging HyperDrop [7]. The HyperDrop procedure is defined as follows:where 'score' function is hypergraph convolution layers used to compute scores for each hypergraph node (e s or e t in the original graph). 'TopE' selects the nodes with the highest E scores (note: ranking was performed for nodes from e s and e t separately, and HyperDrop was only applied to nodes from e s with hyperparameter E), and idx is the node-wise indexing vector. Finally, the salient nodes (from e t ) and edges (from e s ) were determined by ranking the scores averaged across the trajectory."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.3,Brain Informed Graph Transformer Readout (BIGTR),"Proper readout for the embeddings from GNN manipulation is essential to produce meaningful prediction outcome for assisting diagnosis and prognosis. The vanilla ways are feeding the Node Embeddings, and Spatial and Temporal Edge Embeddings generated from the GIVE module into pooling and fully connected layers. However, this would result in a substantial loss of spatial and temporal information [21], especially under the complex settings of three types of spatial/temporal embeddings. Recently, it has been shown, both in theory and practice, that a standard transformer with appropriate token embeddings yields a powerful graph learner [10]. Here, treating embeddings output from GIVE as tokens, we leveraged graph transformer as a trainable readout function, named as Brain Informed Graph Transformer Readout (BIGTR) (Fig. 1). We first define the Type Identifier (TI) and Node Identifier (NI) under the setting of FC trajectory. Trainable TI encodes whether a token is a node, spatial edge or temporal edge. They are defined as a parameter matrix [P v ; P es ; P et ] ∈ R 3×dp , where P v , P es and P et are node, spatial edge and temporal edge identifier respectively. Specifically, we maintained a dictionary, in which the keys are types of the tokens, the values are learnable embeddings that encodes the corresponding token types. It facilitates the model's learning of type-specific attributes in tokens, compelling attention heads to focus on disease-related token disparities, thereby alleviating overfitting caused by non-disease-related attributes. Besides, it inflates 1 G T for an individual-level task to thousands of tokens, which could also alleviate overfitting in the perspective of small-scale datasets. Non-trainable NI are MT node-wise orthonormal vectors Q ∈ R MT ×dq for an FC trajectory with T time points and M nodes at each time. Then, the augmented token features become:for v, e s and e t respectively, where node u is a neighbour to node v in the spatial domain and node v is a neighbour to node v in the temporal domain, and x is the original token from GIVE. Thus, the augmented token features matrix is Z ∈ R (MT +|E|T +M (T -1))×(h+dp+2dq) , where h is the hidden dimension of embeddings from GIVE. Z would be further projected by a trainable matrix ω ∈ R (h+dp+2dq)×h . As we targeted individual-level (i.e., G T ) diagnosis/prognosis, a graph token X [graph] ∈ R h was appended as well. Thus, the input to transformer is formally defined as:"
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,3.0,Experiments,"Datasets and Experimental Settings. We used brain FC metrics derived from ADNI [6] and OASIS-3 [13] resting state fMRI datasets, following preprocessing pipelines [12,14]. Our framework was evaluated on three classification tasks related to diagnosis or prognosis: Results. AUC and accuracy are presented in Table 1. (Recall and Precision could be found in supplementary materials). Brain TokenGT and its ablations were compared with three types of baseline models, including 1) shallow machine learning: MK-SVM, RF and MLP; 2) one time point feasible deep learning: three representative deep graph models GCN [11], GAT [20] and PNA [2], and four state-of-the-art deep models specifically designed for FC: BrainnetCNN [9], BrainGNN [15], IBGNN+ [4] and BrainnetTF [8]; 3) multiple time points feasible deep learning: Onionnet [24], STGCN [22] and EvolveGCN [16]. To ensure a fair comparison between models, the one-dimensional vectors flattened from FC in all time points were concatenated and used as input for the shallow learning model. For the one time point feasible deep learning models, a prediction value was generated at each time point and subsequently averaged to obtain an individuallevel prediction.The experimental results (Table 1) demonstrate that the Brain TokenGT significantly outperformed all three types of baseline by a large margin. The ablation study further revealed that GIVE w/o e t w/ GP outperformed EvolveGCN by adding VEE without e t , which empirically validates the importance of edge feature embeddings in FC. The performance could be further improved by incorporating e t , suggesting the efficiency of our GIVE design with spatiotemporal edges. Interestingly, BIGTR itself (i.e., the original features were directly input to BIGTR without GIVE) showed competitive performance with STGCN. Replacing GP with transformer (Ours w/o I) led to improved performance even without identifiers, indicating that the embeddings from GIVE may already capture some spatial and temporal information from the FC trajectory. The addition of identifiers further improved performance, possibly because the token-level self-supervised learning could alleviate the over-fitting issue and node identifiers could maintain the localized information effectively. Interpretation. Figure 2 shows the top 5 salient edges and nodes retained by HyperDrop for each of the three tasks. Consistent with previous literature on brain network breakdown in the early stage of AD [17], parahippocampal, orbitofrontal and temporal regions and their connections contributed highly to all three tasks, underscoring their critical roles in AD-specific network dysfunction relevant to disease progression. On the other hand, superior frontal region additionally contributed to the amyloid positive vs. negative classification, which is in line with previous studies in amyloid deposition [18]."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,4.0,Conclusion,"This study proposes the first interpretable framework for the embedding of FC trajectories, which can be applied to the diagnosis and prognosis of neurodegenerative diseases with small scale datasets, namely Brain Tokenized Graph Transformer (Brain TokenGT). Based on longitudinal brain FC, experimental results showed superior performance of our framework with excellent built-in interpretability supporting the AD-specific brain network neurodegeneration. A potential avenue for future research stemming from this study involves enhancing the ""temporal resolution"" of the model. This may entail, for example, incorporating an estimation of uncertainty in both diagnosis and prognosis, accounting for disease progression, and offering time-specific node and edge level interpretation."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,,Table 1 .,-ve). All subjects have 2-3 time points of fMRI data and those with two time points were zero-padded to three time points. FC was built based on the AAL brain atlas with 90 ROIs[19]. The model was trained using Binary Cross-Entropy Loss in an end-to-end fashion.
Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"Esophagogastric varices are one of the common manifestations in patients with liver cirrhosis and portal hypertension and occur in about 50 percent of patients with liver cirrhosis [3,6]. The occurrence of esophagogastric variceal bleeding is the most serious adverse event in patients with cirrhosis, with a 6-week acute bleeding mortality rate as high as 15%-20% percent [14]. It is crucial to identify high-risk patients and offer prophylactic treatment at the appropriate time. Regular endoscopy examinations have been proven an effective clinical approach to promptly detect esophagogastric varices with a high risk of bleeding [7]. Different from the grading of esophageal varices (EV) that is relatively complete [1], the bleeding risk grading of gastric varices (GV) involves complex variables including the diameter, shapes, colors, and locations. Several rating systems have been proposed to describe GV based on the anatomical area. Sarin et al. [16] described and divided GV into 2 groups according to their locations and extensions. Hashizume et al. [10] published a more detailed examination describing the form, location, and color. Although the existing rating systems tried to identify the risk from different perspectives, they still lack clear quantification standard and heavily rely on the endoscopists' subjective judgment. This may cause inconsistency or even misdiagnosis due to the variant experience of endoscopists in different hospitals. Therefore, we aim to build an automatic GV bleeding risk rating method that can learn a stable and robust standard from multiple experienced endoscopists.Recent works have proven the effectiveness and superiority of deep learning (DL) technologies in handling esophagogastroduodenoscopy (EGD) tasks, such as the detection of gastric cancer and neoplasia [4]. It is even demonstrated that AI can detect neoplasia in Barrett's esophagus at a higher accuracy than endoscopists [8]. Intuitively we may regard the GV bleeding risk rating as an image classification task and apply typical classification architectures (e.g., ResNet [12]) or state-of-the-art gastric lesion classification methods to it. However, they may raise poor performance due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks. First, the GV area may look like regular stomach rugae as it is caused by the blood vessels bulging and crumpling up the stomach (see Fig. 1). Also, since the GV images are taken from different distances and angles, the number of pixels of the GV area may not reflect its actual size. Consequently, the model may fail to focus on the important GV areas for prediction as shown in Fig. 3. To encourage the model to learn more robust representations, we constructively introduce segmentation into the classification framework. With the segmentation information, we further propose a region-constraint module (RCM) and a cross-region attention module (CRAM) for better feature localization and utilization. Specifically, in RCM, we utilize the segmentation results to constrain the CAM heatmaps of the feature maps extracted by the classification backbone, avoiding the model making predictions based on incorrect areas. In CRAM, the varices features are extracted using the segmentation results and combined with an attention mechanism to learn the intra-class correlation and cross-region correlation between the target area and the context.To learn from experienced endoscopists, GV datasets with bleeding risks annotation is needed. While most works and public datasets focus on colonoscopy [13,15] and esophagus [5,9], with a lack of study on gastroscopy images. In the public dataset of EndoCV challenge [2], the majority are colonoscopies while only few are gastroscopy images. In this work, we collect a GV bleeding risks rating dataset (GVbleed) that contains 1678 gastroscopy images from 411 patients with different levels of GV bleeding risks. Three senior clinical endoscopists are invited to grade the bleeding risk of the retrospective data in three levels and annotated the corresponding segmentation masks of GV areas.In sum, the contributions of this paper are: 1) a novel GV bleeding risk rating framework that constructively introduces segmentation to enhance the robustness of representation learning; 2) a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context; 3) a GV bleeding risk rating dataset (GVbleed) with high-quality annotation from multiple experienced endoscopists. Baseline methods have been evaluated on the newly collected GVbleed dataset. Experimental results demonstrate the effectiveness of our proposed framework and modules, where we improve the accuracy by nearly 5% compared to the baseline model. "
Automatic Bleeding Risk Rating System of Gastric Varices,2.0,Methodology,"The architecture of the proposed framework is depicted in Fig. 2, which consists of a segmentation module (SM), a region constraint module (RCM), and a crossregion attention module (CRAM). Given a gastroscopy image, the SM is first applied to generate the varices mask of the image. Then, the image together with the mask are fed into the CRAM to extract the cross-region attentive feature map, and a class activation map (CAM) is calculated to represent the concentrated regions through RCM. Finally, a simple classifier is used to predict the bleeding risk using the extracted feature map."
Automatic Bleeding Risk Rating System of Gastric Varices,2.1,Segmentation Module,"Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models exhibit poor perform and tend to lose focus on the GV areas. To solve this issue, we first embed a segmentation network into the classification framework. The predict the varices mask is then used to assist the GV feature to obtain the final bleeding risk rate. Specifically, we use SwinUNet [11] as the segmentation network, considering its great performance, and calculate the DiceLoss between the segmentaion result M p and ground truth mask of vaices region M gt for optimizing the network:where is a smooth constant equals to 10 -5 .A straightforward strategy to utilize the segmentation mask is directly using it as an input of the classification model, such as concatenating the image with the mask as the input. Although such strategy can improve the classification performance, it may still lose focus in some hard cases where the GV area can hardly be distinguished. To further regularize the attention and fully utilize the context information around the GV area, on top of the segmentation framework we proposed the cross-region attention module and the region-constraint module."
Automatic Bleeding Risk Rating System of Gastric Varices,2.2,Cross-Region Attention Module,"Inspired by the self-attention mechanism [17,18], we propose a cross-region attention module (CRAM) to learn the correlation of context information. The CRAM consists of an image encoder f im , a varices local encoder f vl and a varices global encoder f ve . Given the image I and the predicted varices mask M p , a feature extraction step is first performed to generate the image feature V m , the local varices feature V vl and global varices feature V vg :Then, through similarity measuring, we can compute the attention withwhich composes of two correlations: self-attention over varices regions and crossregion attention between varices and background regions. Finally, the output feature is calculated as:where γ is a learnable parameter. Then the cross-region attentive feature V is fed into a classifier to predict the bleeding risk."
Automatic Bleeding Risk Rating System of Gastric Varices,2.3,Region Constraint Module,"To improve the focus ability of the model, we propose the region constraint module (RCM) to add a constraint on the class activation map (CAM) of the classification model. Specifically, we use the feature map after the last convolutional layer to calculate the CAM [19], which computes the weighted sum of feature maps from the convolutional layer using the weights of the FC layer.After getting the CAM, we regularize CAM by calculating the dice loss between the CAM and ground truth mask of varices region l co ."
Automatic Bleeding Risk Rating System of Gastric Varices,2.4,Network Training,"In our framework, we use the cross entropy loss as the classification loss: where p is the prediction of the classifier and y is the ground-truth label. And the total loss of our framework can be summarized as:where N is the total number of samples, ω s , ω co and ω cl are weights of the three losses, respectively.The training process of the proposed network consists of three steps: 1) The segmentation network is trained first; 2) The ground-truth segmentation masks and images are used as the inputs of the CRAM, the classification network, including CRAM and RCM, are jointly trained; 3) The whole framework is jointly fine-tuned."
Automatic Bleeding Risk Rating System of Gastric Varices,3.0,GVBleed Dataset,"Data Collection and Annotation. The GVBleed dataset contains 1678 endoscopic images with gastric varices from 527 cases. All of these cases are collected from 411 patients in a Grade-III Class-A hospital during the period from 2017 to 2022. In the current version, images from patients with ages elder than 18 are retained 1 . The images are selected from the raw endoscopic videos and frames. To maximize the variations, non-consecutive frames with larger angle differences are selected. To ensure the quality of our dataset, senior endoscopists are invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and NBI pictures.Criterion of GV Bleeding Risk Level Rating. Based on the clinical experience in practice, the GV bleeding risks in our dataset are rated into three levels, i.e., mild, moderate, and severe. The detailed rating standard is as follows: 1) Mild: low risk of bleeding, and regular follow-up is sufficient (usually with a diameter less than or equal to 5 mm). 2) Moderate: moderate risk of bleeding, and endoscopic treatment is necessary, with relatively low endoscopic treatment difficulty (usually with a diameter between 5 mm and 10 mm). 3) Severe: high risk of bleeding and endoscopic treatment is necessary, with high endoscopic treatment difficulty. The varices are thicker (usually with a diameter greater than 10 mm) or less than 10mm but with positive red signs. Note that the diameter is only one reference for the final risk rating since the GV is with 1 Please refer to the supplementary material for more detailed information about our dataset. various 3D shapes and locations. The other facts are more subjectively evaluated based on the experience of endoscopists. To ensure the accuracy of our annotation, three senior endoscopists with more than 10 years of clinical experience are invited to jointly label each sample in our dataset. If three endoscopists have inconsistent ratings for a sample, the final decision is judged by voting. A sample is selected and labeled with a specific bleeding risk level only when two or more endoscopists reach a consensus on it. The GVBleed dataset is partitioned into training and testing sets for evaluation, where the training set contains 1337 images and the testing set has 341 images. The detailed statistics of the three levels of GV bleeding risk in each set are shown in Table 1. The dataset is planned to be released in the future."
Automatic Bleeding Risk Rating System of Gastric Varices,4.1,Implementation Details,"In experiments, the weights ω s , ω co , and ω cl of the segmentation loss, region constraint loss, and classification loss are set to 0.2, 1, and 1, respectively. The details of the three-step training are as follows: 1) Segmentation module: We trained the segmentation network for 600 epochs, using Adam as the optimizer, and the learning rate is initialized as 1e-3 and drops to 1e-4 after 300 epochs. 2) Cross-region attention module and region constraint module: We used the ground-truth varices masks and images as the inputs of the CRAM, and jointly trained the CRAM and RCM for 100 epochs. Adam is used as the optimizer, the learning rate is set to 1e-3; 3) Jointly fine-tuning: The whole framework is jointly fine-tuned for 100 epochs with Adam as optimizer and the learning rate set to 1e-3. In addition, common data augmentation techniques such as rotation and flipping were adopted here."
Automatic Bleeding Risk Rating System of Gastric Varices,4.2,Results Analysis,"Table 2 reports the quantitative results of different models and Fig. 3 shows the CAM visualizations. We tested several baseline models, including both simple CNN models and state-of-the-art transformer-based models. However, the transformer-based models achieves much worse performances since they always require more training data, which is not available in our task. Thus, we selected the simple CNN models as baselines since they achieve better performances. As shown in the figure, the baseline model cannot focus on the varices regions due to the complexity of GV structures with large intra-class variations and small inter-class variations. By introducing the segmentation of GV into the framework, concatenating the image with its segmentation mask as the inputs of the classifier can improve the classification accuracy by 1.2%. And the focus ability of the classifier is stronger than the baseline model. With the help of CRAM, the performance of the model can be further improved. Although the model can extract more important context information at the varices regions, the performance improvement is not very large since the focus ability is not the best and the model may still make predictions based on the incorrect regions for some hard images. By adding the RCM to the CRAM, the focus ability of the model can be further improved, and thus the model has a significant improvement in performance by 5% compared to the baseline model, this proves the effectiveness of our proposed modules. Note that, the baseline model tends to predict the images as severe, thus the f1-score of severe is high but the f1-scores of mild and moderate are significantly lower than other models. More quantitative and visualization results are shown in supplementary material. In addition, given the input image with resolution 512 × 512, the parameters and computational cost of our framework are 40.2M, and 52.4G MACs, and 29 ms inference time for a single image on GPU RTX2080. For comparison, a single ResNet152 model has 60.19M parameters with 60.62 G MACs. "
Automatic Bleeding Risk Rating System of Gastric Varices,5.0,Conclusions,"In this paper, we propose a novel bleeding risk rating framework for gastric varices. Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models cannot correctly focus on the varices regions and always raise poor performance. To solve this issue, we constructively introduce segmentation to enhance the robustness of representation learning. Besides, we further design a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context. In addition, we collected the GVBleed dataset with high-quality annotation of three-level of GV bleeding risks. The experiments on our dataset demonstrated the effectiveness and superiority of our framework."
Automatic Bleeding Risk Rating System of Gastric Varices,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_1.
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,1.0,Introduction,"Radiology is a critical medical field that relies on accurate and efficient communication between radiologists and other healthcare professionals enabled through radiology reports. However, generating these reports takes a lot of time and is prone to errors, as it often relies on ambiguous natural language. One alternative to free-text reports is to use structured reporting, which is endorsed by radiology societies, saves time, and offers standardized content and terminology [8,18].Automated report generation can reduce radiologists' workload and support quick diagnostic decisions. Most current research focuses on generating freetext reports, which lack standardization, and still face challenges of ambiguity and difficulties in clinical correctness evaluation [9,14,19,23,26]. In comparison, automating structured reporting allows an accurate evaluation of clinical correctness and can enforce the prediction of detailed findings. However, for automating structured reporting, the research is limited. Some studies predict high-level abnormalities using pre-defined template sentences [10,19], or predict location and attributes for a single disease [2]. Syeda-Mahmood et al. [21] predict fine-grained but unstructured labels to retrieve and adapt free-text reports from a database. However, none of these works predict highly-detailed and structured annotations as needed to populate an entire structured report. A significant challenge towards this goal is the lack of public benchmarks with highly detailed structured annotations. To facilitate future research, we introduce Rad-ReStruct, the first dataset of publicly available, fine-grained, and structured annotations for Chest X-Rays. To create Rad-ReStruct, we define a detailed, multi-level structured reporting template and automatically populate it by parsing and analyzing unstructured finding summaries from the IU X-ray dataset [5].Structured reports with high standardization have a structured layout and content, e.g., organized in expendable trees with drop-down menus to select answers [18]. A user interface for structured reporting would pose a series of questions that, dependent on the answer, lead to expandable follow-up questions. In this setup, structured reporting can be considered several classification tasks on different levels. We model this as a hierarchical visual question answering (VQA) task and propose hi-VQA, a hierarchical, autoregressive VQA method for populating structured reports by successively filling out all fields in the report while preserving consistency. Hi-VQA considers the prior context of previously asked questions and answers, allowing to exploit inter-dependencies between questions about the same image. For structured report population, this is essential, as lower-level questions directly depend on higher levels. Further, our autoregressive formulation enhances explainability, showing at which level and for which question type the model made mistakes. As backbone, we propose a simple yet effective VQA architecture relying on large pretrained image and text encoders and a transformer-based fusion module. Using VQA [1] allows to exploit the knowledge encoded in large language models. It has recently received attention in the medical field, mainly on small datasets, where every question is answered independently [4,11,17,20,23]. One previous work explicitly models question consistency for medical VQA in the loss [24]. Another work had promising results using an unstructured question history in a visual dialog setting [12]. They ask for high-level abnormalities, such as ""Pneumonia?"" and use a randomly sampled, fixed history of other abnormality questions. In contrast, we define a hierarchical history with detailed questions and an autoregressive model.We demonstrate the effectiveness of our streamlined design and hierarchical framework in our experimental results, reaching competitive results to the SOTA on the medical VQA benchmark VQARad and setting a baseline for Rad-ReStruct. Overall, our work is a significant step towards automating the population of structured radiology reports and provides a valuable benchmark for future research in this area."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,2.1,Rad-ReStruct Dataset,"We propose the first benchmark dataset to enable the development and comparison of methods for the population of structured reports entailing hierarchical and fine-grained classifications for radiological images. Rad-ReStruct is based upon the IU-Xray dataset [5] and consists of X-Ray images paired with fine-grained radiological findings organized as a structured report. To create the dataset, we first define a detailed report template and then populate it automatically by parsing and analyzing the unstructured expert annotations of the reports in IU-Xray.Creation of Structured Report Template. We build upon the semistructured encoded findings provided for the IU-XRay data collection [5]. The encoded findings were provided by medical experts, who labeled the IU-XRay free-text reports using MeSH (Medical Subject Headings) [7] and RadLex (Radiology Lexicon) [13] codes. They accurately summarize all findings in the radiological images together with a detailed attribute description. They are an unstructured collection of findings, with a sequence of annotation terms representing each finding (e.g., ""infiltrate/lung/upper lobe/left/patchy/mild""...). The codes use a controlled vocabulary containing 178 terms, which include anatomies, diseases, pathological signs, foreign objects, and attributes. Anatomies and diseases can be matched to broad body regions, such as the respiratory or skeletal system. Attributes include degree, descriptive, and positional attributes.We utilize this semi-structured finding representation to construct a highly detailed report template as shown in Fig. 1. Our report template is structured into multiple sections and employs a multi-level hierarchy of questions that delve deeper into the findings at each level. The template can be considered a large decision tree with questions at every level. The highest level asks for the general existence of findings (signs, diseases, abnormal regions, or objects), the second level asks for specific elements, such as a certain object or disease, and the lowestlevel questions ask for specific attributes. Table 1 shows how often which question type occurs. To create the template, we parse the codes of all patients and identify all occurrences of term combinations at all levels of the defined hierarchy. We then remove unseen options to produce a streamlined report template that includes only possible options for all findings. Further, we mark all questions as either single-or multi-choice and add a ""no selection"" option.Overall, our structured report template provides a rigorous and comprehensive framework for classifying radiological images and mimics the style of a structured report in a clinical setting. This enables the development and comparison of methods for the population of structured reports and the prediction of finegrained radiological findings.Dataset and Evaluation Metrics. Our dataset consists of structured reports for each patient in the IU-XRay data collection, for which finding codes and a frontal X-Ray were available. The new dataset includes 3720 images matched to 3597 structured patient reports entailing more than 180k questions. If multiple images belong to one patient, each image is considered an independent sample. We use a 80-10-10 split to create train, validation and test set. To avoid data leakage, we ensure that different images of the same patient are in the same split.The goal of our task is to produce fine-grained finding classifications for populating a structured report given an X-Ray image of a patient. This task involves answering a series of questions about the image, gradually adding more detail. We define several evaluation metrics for the proposed benchmark. As the distribution of questions and answers is very imbalanced, we evaluate with the macro precision, recall, and F1 score over all possible paths in the question tree to encourage methods that also perform well in under-represented question-answer combinations. One path is a unique position in the report combined with a specific answer option. Further, we employ report-level accuracy to measure how many predicted reports are entirely correct. During the evaluation, we enforce consistency within the question hierarchy. For example, if the answer to a higherlevel question is ""no"", we prohibit to answer a lower-level question positively. This ensures the generated reports are consistent and coherent, as in a real medical report. Lastly, as multiple instances of an object, sign or pathology can occur for one patient, we iteratively ask for further occurrences, when the model predicts a positive answer. (e.g., ""Are there other opacities in the lung?""). We restrict the number of follow-up questions by the maximum of per-patient occurrences in the data. As the order of occurrences is ambiguous, we apply instance matching during the metric computation. We order all predicted instances such that the highest F1 score for this finding is achieved."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,2.2,Hierarchical Visual Question Answering,"With Rad-ReStruct, we propose a hierarchical VQA task, where lower level questions are dependent on context information. For instance, to answer the questions ""What is the degree?"" it is essential to know what the question is referring to. This information is given through the previous question, which could be ""Is there Pneumonia in the lung? Yes"". To integrate this context, we propose a hierarchical VQA framework that can effectively answer questions about medical images by considering previously asked questions. We extend the input to the model by pre-pending the current question with the history of previously asked questions and the model's answers. This extension enables interpretable and consistent results.We leverage a pretrained image encoder, EfficientNet-b5 [22], and a domainspecific pretrained text encoder, RadBERT [27], to extract features from the image, history, and question. The extracted features are fused by a transformer [25] layer, adapted to handle multi-modal input. The fused features are then used to perform a multi-label classification over all answer options. However, we only consider outputs that are valid answers to the current question. For single-choice questions, we predict a single label applying a softmax over all valid answers, while for multi-choice questions, we predict multiple labels using a sigmoid function. Figure 2 shows an overview of the proposed framework.Feature Encoding. For fusing the image and text features, we construct a token sequence (Nx458x768) of the form <image_tokens> <history_tokens> <question_tokens>. The image tokens (Nx196x768) consist of the flattened embedding representation of the image encoder, while the history and question text (Nx259x768) is encoded jointly using RadBERT. The different parts are separated by a <SEP> token and fused by a single transformer layer. We encode the type of input in the token-type IDs with different IDs for the image tokens, history questions, history answers, and the current question. Further, we use modified positional encodings to preserve the 2D spatial information of the image as well as the sequential order of the text. We create a joint positional encoding (Nx768x458) by concatenating a 1D positional encoding [25] for the text and a 2D positional encoding [3] for the image (each of shape Nx384x458). We set the non-used part of the encoding per modality to zero. The token-type IDs and positional encodings are added to the feature vector. Training and Evaluation. During training, we use teacher forcing, relying on ground truth answers for prior questions in the history, allowing for efficient batch-wise processing. At inference all answers are predicted, and no ground truth is used. The model is trained end-to-end, using a weighted masked crossentropy loss to optimize the classification performance. For every sample in a training batch, we only consider the loss for the labels corresponding to the asked question to avoid optimizing the model on currently irrelevant outputs. Further, we apply positive weighting per class. The evaluation is autoregressive, thus, the model utilizes the previously asked questions and their predicted answers as history. In a hierarchical VQA task such as Rad-ReStruct, the inference is interrupted if the model predicts a negative answer and sub-questions lower in the hierarchy are automatically answered as negative (set to No/No selection), enforcing consistency of the prediction. This also improves the explainability of predictions by allowing to track errors back to their source and showing at which level the model made a mistake. For non-hierarchical VQA tasks, the history is utilized solely as context information, allowing the model to exploit inter-dependencies between different questions about the same image."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,3.0,Experiments and Results,"We test our model on Rad-ReStruct, setting a baseline for this new dataset. To further validate our model design, we compare the performance of our model with previous work on the standard VQA benchmark VQARad. We train all models on a NVIDIA A40 GPU. We use pytorch-lightning 1.8.3. and the AdamW optimizer with a learning rate of 5e-5 for VQARad and 1e-5 for Rad-ReStruct. For all models, we set the number of epochs by maximizing validation set performance. Rad-ReStruct. For Rad-ReStruct, the history includes all higher-level questions on the same question path. Additionally, attribute questions asked pre- viously about an element, are included in the history, enabling the model to provide consistent predictions. Lastly, the history includes previously predicted instances of the same element. Table 2 and Table 3 show the overall and questionlevel results of our model. We compare hi-VQA to a visual baseline, consisting of our image encoder and a classification layer. Hi-VQA achieves better performance than the visual baseline in all metrics, indicating the benefits of targeted information retrieval using a large language model. When comparing the RadBERT text encoder, a RoBERTa model [16] pretrained with radiology reports, to RoBERTa BASE , which was pre-trained on general text, the RadBERT encoder is superior. This indicates that our method can benefit from better domain-specific language encoders. Using history information improves report accuracy and precision with a slightly decreased recall and a similar F1 score, showing the benefit of history integration. We emphasize, that the history is especially important for the lowlevel attribute questions, as these are only meaningful with context. Therefore, it will be even more impactful with improved performance for these questions.Our labels' hierarchical, structured formulation enables a performance analysis on different topics and levels. Hi-VQA performs well in detecting the existence of sub-topics like objects, diseases, signs, and abnormalities. However, attribute prediction performance is much lower, likely due to the rarity and complexity of these questions and error propagation from higher levels. Such an analysis is precious to understand what a model learned and when it should be trusted. VQARad is a medical VQA benchmark with 315 radiological images and 3515 questions. The task is to make a classification over 2248 possible answers. In VQARad multiple questions are asked about one image, but in previous work they are always answered separately. To make use of possible inter-dependencies between questions, we define five question levels based on question topics in VQARad, ranging from general to specific: Modality → Plane → Organ → Presence, Count, Abnormality → Color, Position, Size, Attributes, Other. For a certain question, previously asked questions from lower or the same level are included in the history. During training, we augment the history by randomly dropping and reordering questions within a level to prevent overfitting on this small dataset. Table 4 shows the performance of hi-VQA compared to previous methods. Amongst the methods without domain-specific joined image-text pretraining, we reach SOTA results, even without history context. When integrating history information, our model achieves competitive results with the current SOTA method, M3AE [4]. This result demonstrates the promise of jointly answering questions about the same image in medical VQA tasks. Lastly, we again compare using the RadBERT encoder to RoBERTa BASE [16]. We can observe, also on VQARad, using RadBERT improves the performance notably, again indicating that VQA tasks benefit from domain-specific text encoders."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,4.0,Discussion and Conclusion,"By introducing Rad-ReStruct, the first structured radiology reporting benchmark, we create a framework to develop, evaluate, and compare structured reporting methods. The structured formulation enables an accurate evaluation of clinical correctness at different levels of granularity, focusing on levels with greater clinical importance. Moreover, such a structured finding representation could then again, rule-based, be converted to a text report while maintaining clinical accuracy. To model structured reporting, we present hi-VQA, a novel, hierarchical VQA framework with a streamlined architecture that leverages history context for multi-question and multi-level tasks. The autoregressive formulation and consistent evaluation increase interpretability and mimic the workflow of structured reporting. Moreover, as each prediction takes previous answers into account, it would allow for an interactive workflow, where the model can make predictions and react to corrections while a radiologist fills out a report.We set a first baseline for Rad-ReStruct, with particularly good performance on higher-level questions. Although our model has limited performance on the low-level attribute questions, it performed competitive to state-of-the-art on VQARad, indicating the difficulty of our new task. We see this as an opportunity to develop methods for fine-grained understanding of radiology images, rather than solely focusing on higher-level diagnoses. Further, we show the positive effect of history integration, which is crucial for hierarchical and contextdependent tasks such as structured report population. Our work represents a significant step forward in the development of automated structured radiology report population methods, while allowing an accurate and multi-level evaluation of clinical correctness and fostering fine-grained, in-depth radiological image understanding."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_40.
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,1.0,Introduction,"Bias in medicine has demonstrated a notable challenge for providing comprehensive and equitable care. Implicit biases can negatively affect patient care, particularly for marginalized populations with lower socioeconomic status [30]. Evidence has demonstrated that implicit biases in healthcare providers could contribute to exacerbating these healthcare inequalities and create a more unfair system for people of lower socioeconomic status [30]. Based on the data with racial bias, the unfairness presents in developing evaluative algorithms. In an algorithm used to predict healthcare costs, black patients who received the same health risk scores as white patients were consistently sicker [21]. Using biased data for AI models reinforces racial inequities, worsening disparities among minorities in healthcare decision-making [22].Within the radiology arm of AI research, there have been significant advances in diagnostics and decision making [19]. Along these advancements, bias in healthcare and AI are exposing poignant gaps in the field's understanding of model implementation and their utility [25,26]. AI model quality relies on input data and addressing bias is a crucial research area. Systemic bias poses a greater threat to AI model's applications, as these biases can be baked right into the model's decision process [22].Pulmonary embolism (PE) is an example of health disparities related to race. Black patients exhibit a 50% higher age-standardized PE fatality rate and a twofold risk for PE hospitalization than White patients [18,24]. Hospitalized Black patients with PE were younger than Whites. In terms of PE severity, Blacks received fewer surgical interventions for intermediate PE but more for highseverity PE [24]. Racial disparities exist in PE and demonstrate the inequities that affect Black patients. The Pulmonary Embolism Severity Index (PESI) is a well-validated clinical tool based on 11 clinical variables and used for outcome prediction measurement [2]. Survival analysis is often used in PE to assess how survival is affected by different variables, using a statistical method like Kaplan-Meier method and Cox proportional-hazards regression model [7,12,14].However, one issue with traditional survival analysis is bias from single modal data that gets compounded when curating multimodal datasets, as different combinations of modes and datasets create with a unified structure. Multimodal data sets are useful for fair AI model development as the bias complementary from different sources can make de-biased decisions and assessments. In that process, the biases of each individual data set will get pooled together, creating a multimodal data set that inherits multiple biases, such as racial bias [1,15,23]. In addition, it has been found that creating multimodal datasets without any debiasing techniques does not improve performance significantly and does increase bias and reduce fairness [5]. Overall, a holistic approach to model development would be beneficial in reducing bias aggregation in multimodal datasets. In recent years, Disentangled Representation Learning (DRL) [4] for bias disentanglement improves model generalization for fairness [3,6,27].We developed a PE outcome model that predicted mortality and detected bias in the output. We then implemented methods to remove racial bias in our dataset and model and output unbiased PE outcomes as a result. Our contributions are as follows: (1) We identified bias diversity in multimodal information using a survival prediction fusion framework. (2) We proposed a de-biased survival prediction framework with demographic bias disentanglement. (3) The multimodal CPH learning models improve fairness with unbiased features. "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,2.0,Bias in Survival Prediction,"This section describes the detail of how we identify the varying degrees of bias in multimodal information and illustrates bias using the relative difference in survival outcomes. We will first introduce our pulmonary embolism multimodal datasets, including survival and race labels. Then, we evaluate the baseline survival learning framework without de-biasing in the various racial groups.Dataset. The Pulmonary Embolism dataset used in this study from 918 patients (163 deceased, median age 64 years, range 13-99 years, 52% female), including 3978 CTPA images and 918 clinical reports, which were identified via retrospective review across three institutions. The clinical reports from physicians that provided crucial information are anonymized and divided into four parts: medical history, clinical diagnosis, observations and radiologist's opinion. For each patient, the race labels, survival time-to-event labels and PESI variables are collected from clinical data, and the 11 PESI variables are used to calculate the PESI scores, which include age, sex, comorbid illnesses (cancer, heart failure, chronic lung disease), pulse, systolic blood pressure, respiratory rate, temperature, altered mental status, and arterial oxygen saturation at the time of diagnosis [2].Diverse Bias of Multimodal Survival Prediction Model. We designed a deep survival prediction (SP) baseline framework for multimodal data as shown in Fig. 1, which compares the impact of different population distributions. The frameworks without de-basing are evaluated for risk prediction in the test set by performing survival prediction on CTPA images, clinical reports, and clinical variables, respectively. First, we use two large-scale data-trained models as backbones to respectively extract features from preprocessed images and cleaned clinical reports. A state-of-the-art PE detecting model, PENet [11] is used as the backbone model for analyzing imaging risk and extracting information from multiple slices of volumetric CTPA scans to locate the PE. The feature with the highest PE probability from a patient's multiple CTPAs is considered as the most PE-related visual representation. Next, the GatorTron [29] model is employed to recognize clinical concepts and identify medical relations for getting accurate patient information from PE clinical reports. The extracted features from the backbones and PESI variables are represented as F m , m ∈ [Img, Text, Var]. The survival prediction baseline framework, built upon the backbones, consists of three multi-layer perceptron (MLP) modules named Imaging-based, Text-based and Variable-based SP modules. To encode survival features Z m sur from image, text and PESI variables, these modules are trained to distinguish critical disease from non-critical disease with Cox partial log-likelihood loss (CoxPHloss) [13]. The framework also consists of a Cox proportional hazard (CoxPH) model [7] that is trained to predict patient ranking using a multimodal combination of risk predictions from the above three SP modules. These CoxPH models calculate the corresponding time-to-event evaluation and predict the fusion of patients' risk as the survival outcome. We evaluate the performance of each module with concordance probability (C-index), which measures the accuracy of prediction in terms of ranking the order of survival times [8]. For reference, the C-index of PESI scores is additionally provided for comparative analysis.In Table 1 (Baseline), we computed the C-index between the predicted risk of each model and time-to-event labels. When debiasing is not performed, significant differences exist among the different modalities, with the image modality exhibiting the most pronounced deviation, followed by text and PESI variables. The biased performance of the imaging-based module is likely caused by the richness of redundant information in images, which includes implicit features such as body structure and posture that reflect the distribution of different races. This redundancy leads to model overfitting on race, compromising the fairness of risk prediction across different races. Besides, clinical data in the form of text reports and PESI variables objectively reflect the patient's physiological information and the physician's diagnosis, exhibiting smaller race biases in correlation with survival across different races. Moreover, the multimodal fusion strategy is found to be effective, yielding more relevant survival outcomes than the clinical gold standard PESI scores."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,3.0,De-biased Survival Prediction Model,"Based on our SP baseline framework and multimodal findings from Sect. 2, we present a feature-level de-biased SP module that enhances fairness in survival outcomes by decoupling race attributes, as shown in the lower right of Fig. 1. In the de-biased SP module, firstly, two separate encoders E m i and E m c are formulated to embed features F m into disentangled latent vectors for race-intrinsic attributes z ID or race-conflicting attributes z sur implied survival information [16]. Then, the linear classifiers C m i and C m c constructed to predict the race label y ID with concatenated vector z = [z ID ; z sur ]. To disentangle survival features from the race identification, we use the generalized cross-entropy (GCE) loss [31] to train E m c and C m c to overfit to race label while training E m i and C m i with crossentropy (CE) loss. The relative difficulty scores W as defined in Eq. 1 reweight and enhance the learning of the race-intrinsic attributes [20]. The objective function for disentanglement shown in Eq. 2, but the parameters of ID or survival branch are only updated by their respective losses:To promote race-intrinsic learning in E m i and C m i , we apply diversify with latent vectors swapping. The randomly permuted zsur in each mini-batch concatenate with z ID to obtain z sw = [z ID ; zsur ]. The two neural networks are trained to predict y ID or ỹID with CE loss or GCE loss. As the random combination are generated from different samples, the swapping decreases the correlation of these feature vectors, thereby enhancing the race-intrinsic attributes. The loss functions of swapping augmentation added to train two neural networks is defined as:The survival prediction head C m sur predicts the risk on the survival feature z sur . CoxPH loss function [13], which optimizes the Cox partial likelihood, is used to maximize concordance differentiable and update model weights of the survival branch. Thus, CoxPH loss and overall loss function are formulated as:where Y t and Y e are survival labels including the survival time and the event, respectively. The weights λ sw and λ sur are assigned as 0.5 and 0.8, respectively, to balance the feature disentanglement and survival prediction."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.0,Experiment,"We validate the proposed de-biased survival prediction frameworks on the collected multi-modality PE data. The data from 3 institutions are randomly split The lung region of CPTA images is extracted with a slice thickness of 1.25 mm and scaled to N × 512 × 512 pixels [10]. Hounsfield units (HU) of all slices are clipped to the range of [-1000, 900] and applied with zero-centered normalization. The PENet-based imaging backbone consists of a 77-layer 3D convolutional neural network and linear regression layers. It takes in a sliding window of 24 slices at a time, resulting in a window-level prediction that represents the probability of PE for the current slices [11]. The PENet is pre-trained on large-scale CTPA studies and shows excellent PE detection performance with an AUROC of 0.85 on our entire dataset. The 2048 dimensional features from the last convolution with the highest probability of PE, are designated as the imaging features.The GatorTron [29] uses a transformer-based architecture to extract features from the clinical text, which was pre-trained on over 82 billion words of de-identified clinical text. We used the Huggingface library [28] to deploy the 345m-parameter cased model as the clinical report feature extractor. The outputs from each patient's medical history, clinical diagnosis, observations, and radiologist impression are separately generated and concatenated to form the 1024 × 4 features.We build the encoders of the baseline SP modules and de-biased SP modules with multi-layer perceptron (MLP) neural networks and ReLu activation. The MLPs with 3 hidden layers are used to encode image and text features, and another MLPs with 2 layers encodes the features of PESI variables. A fully connected layer with sigmoid activation acts as a risk classifier C m sur (z m sur ) for survival prediction, where z m sur is the feature encoded from single modal data. For training the biased and de-biased SP modules, we collect data from one modality as a batch with synchronized batch normalization. The SP modules are optimized using the AdamW [17] optimizer with a momentum of 0.9, a weight decay of 0.0005, and a learning rate of 0.001. We apply early stopping when validation loss doesn't decrease for 600 epochs. Experiments are conducted on an Nvidia GV100 GPU.  "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.1,Results,"Table 1 shows the quantitative comparisons of the baseline and de-biased frameworks with the C-indexes of the multimodal survival predictions. In general, our framework including de-biased SP modules shows significantly better predictions in testing set than the PESI-based outcome estimation with C-indexes of 0.669, 0.654, 0.697, 0.043 for the overall testset, White testset, Color testset and race bias. The de-biased results outperform the baseline in overall survival C-index and show a lower race bias, especially in imaging-and fusion-based predictions. The results indicate the effectiveness of the proposed de-biasing in mitigating race inequity. The results also prove the observations for the different biases present in different modalities, especially in the CTPA images containing more abundant race-related information. It also explains the limited effectiveness of de-biasing the clinical results, which contain less racial identification. The pre- diction performance based on multiply modalities is significantly better than the PESI-based outcome estimation. The disentangled representations, transformed from latent space to a 2D plane via tSNE and color-coded by race [9], are shown in Fig. 2. We observe the disentanglement in the visualization of the ID features z ID , while the survival features z sur eliminate the race bias. The lack of apparent race bias observed in both the original features and those encoded in the baseline can be attributed to the subordinate role that ID features play in the multimodal information. The Kaplan-Meier (K-M) survival curve [14], as shown in Fig. 3, is used to compare the survival prediction between high-risk and lowrisk patient groups. The p-values in the hypothesis test were found to be less than 0.001, which is considered statistically significant difference. In addition, the predictions of the de-biased framework show favorable performance, and our multimodal fusion demonstrates a more pronounced discriminative ability in the K-M survival analysis compared to the single-modal results.We conducted ablation studies to examine the effect of the two key components, including swapping feature augmentation and race-balance resampling. As shown in Table 2, the different training settings show significant differences in survival prediction performance across modalities. The swapping augmentation provides a strong bias correction effect for image data with obvious bias. For clinical data, the resampling generally improves performance in most cases. Overall, multimodal fusion approaches are effective in all training settings, and the CoxPH model can actively learn the optimal combination of multimodal features to predict survival outcomes."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,5.0,Discussions and Conclusions,"In this work, we developed a de-biased survival prediction framework based on the race-disentangled representation. The proposed de-biased SP framework, based on the SOTA PE detection backbone and large-scale clinical language model, can predict the PE outcome with a higher survival correlation ahead of the clinical evaluation index. We detected indications of racial bias in our dataset and conducted an analysis of the multimodal diversity. Experimental results illustrate that our approach is effective for eliminating racial bias while resulting in an overall improved model performance. The proposed technique is clinically relevant as it can address the pervasive presence of racial bias in healthcare systems and offer a solution for minimizing or eliminating bias without pausing to evaluate their affection for the models and tools. Our study is significant as it highlights and evaluates the negative impact of racial bias on deep learning models. The proposed de-biased method has already shown the capacity to relieve them, which is vital when serving patients with an accurate analysis. The research in our paper demonstrates and proves that eliminating racial biases from data improves performance, and yields a more precise and robust survival prediction tool. In the future, these de-biased SP modules can be plugged into other models, offering a fairer method to predict survival outcomes."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_50.
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,1.0,Introduction,"Alzheimer's disease (AD) is one of the most pervasive neurodegenerative disorders, causing an increasing morbidity burden that may outstrip diagnosis and management capacity with the population ages. The assessment of AD usually involves the acquisition of structural magnetic resonance imaging (sMRI) images, since it offers accurate visualization of the anatomy and pathology of the brain. Brain abnormalities (e.g., atrophy, enlargement, malformation) are known to be the most discriminative and reliable biomarkers [1] of AD that can be observed and analyzed through sMRI. However, automatic and reproducible identification of AD remains challenging due to heterogeneous of sMRI collected from different centers.Recently, convolutional neural networks (CNN) have been used for automatic classification of AD from sMRI. Many methods [2,3] use a bag of patches selected from the skull-stripped brain region, which ignores global context information that can play a significant role in identifying lesions for accurate inference [4]. Many studies [5][6][7][8] proposed to characterize AD using segmented anatomies (e.g., gray matter or hippcampus). These methods rely on the accurate segmentation of the anatomies which is usually performed in a multi-stage data processing pipeline with the help of third-party softwares (e.g., FreeSurfer [9]) driven by a prior template. However, template-driven methods depend on variable image registration accuracy and highly affected by the anatomical variability between subjects, introducing errors to the characterization of individualized abnormalities. Similarly, methods (e.g., [10]) use detected landmarks also depend on a template-driven pipeline. Taking advantage of attention mechanism, some methods [5] proposed to diagnose AD using sMRI images from multiple centers. However, the classification performance is either hardly reproducible or difficult to compare across studies. One of the major reasons is that existing methods are often trained with samples from the same training (source) domain, while testing samples come from an independent new (target) domain with a different feature distribution. In the literature, this situation relates to domain adaptation [11][12][13][14][15][16] or domain generalization [17][18][19]. A widely used solution for the problem is to learn a domain-invariant latent feature space [20]. Unfortunately, there is no guarantee that the target samples' features will fall into the shared source domain-invariant representation, and in practice it is that new domains typically do not.In this paper, we propose a novel domain-knowledge-constrained neural network for the diagnosis of AD using sMRI from multiple source domains. We designed a new domain-knowledge encoding module into a ResNet-like architecture for feature learning that yields a latent feature space with domain specific and domain shared information. In addition, we propose to use segmentationfree, resampling-free, patch-free 3D sub-images, which offers global context information and subject-level abnormalities to further refines generalizable and reproducible predictions."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.0,Methods,"We propose to design and implement an end-to-end neural network (Fig. 1) for automatic, robust, and reproducible diagnosis of AD using sMRI images, with the hope to identify and understand the most discriminative anatomical regions associate with AD. The model operates in 3 major steps: a) crop the input sMRI image to keep a sub-region (red rectangle), containing relevant anatomy structures (e.g., hippocampus, caudate, ventricles) associate with AD; b) extract features shared by all training sources based on ResNet [21]; c) design a domainknowledge encoding module and a set of label predictors to constrain the feature learning process for better generalization. "
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.1,Patch-Free 3D Feature Extractor,"We first estimate a bounding box around relevant anatomical objects in the input sMRI. The objects are automatically identified by affine registration, which transforms the reference template to each image in the dataset to estimate label for the image. We note that, the estimated labels are only used to locate the bounding box, it has no effect on the individual's atrophy since we pad extra space to ensure the cropped image contain all interested objects with respect to registration errors. Then, we crop the input image using the located bounding box to obtain the sub-image as input to our network. It need to be clarified that the cropping size is a fixed tuple determined by the maximum bounding box containing informative anatomical objects associated with AD.To encode global context information, we propose a patch-free 3D feature extractor for different source domains, which is expected to learn domaininvariant features while not eliminating domain-specific features. Each domain has a unique label classifier, allowing adjustments for domain differences. Based on ResNet, we design our feature extractor as shown in Fig. 1. Each basic block consists of two convolutional layers. Each convolutional layer is followed by a batch normalization and a nonlinear activation function LeakyReLU. The basic block can be wrote as:where X l and X l+1 are the input and output of the basic block and F (W i , X l ) denotes the nonlinear mapping in the basic block. Since the dimensions of X and F (W i , X) must be the same for summation, we use the linear mapping W s to adjust the dimensions of X in the shortcut connection.In the proposed method, we use global average pooling function which is more suitable for disease classification, because the global average pooling operation reflects the information of gray matter volume in brain regions and preserves the relative position relationship between different channels of the feature map.In the output layer, we use a softmax classifier based on cross-entropy loss to calculate the loss between the predicted and true labels."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.2,Global Average Pooling,"Global average pooling solves the problem of excessive image feature dimensions.If the feature maps of 3D images are directly expanded for classification, it will significantly increase the number of classifier parameters and increase the time and space complexity of training. Global average pooling averages the 3D feature maps in the channel dimension, preserving the relative position relationship between channels and reducing the resources required for model training.The dimension change in the global average pooling is, where B denotes the batch-size and C denotes the channel number.where δ denotes the image feature extracted by ResNet, and D, H, W denote the three dimensions of the feature. Since global average pooling has fewer parameters, it can prevent over-fitting to some extent, further more, global average pooling sums out the spatial information, thus it is more robust to spatial translation of the input."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.3,Domain-Knowledge Encoding,"The domain-knowledge encoding module is designed to give relative similarity weights to source domains from a new sample. The weights reflect the similarity between the testing sample and source domains, allowing the module to share strength only between similar domains.Our model uses multiple classifiers for prediction from the features extracted by the feature extractor. The classifiers are independent from each other. We feed the image features to different classifiers and generate weights to each classifier, summing the predictions of each classifier according to the weights as the final output.where Y denotes the prediction result of X, c_num denotes the number of classifiers, D i denotes the center which X belongs, δ denotes the extracted feature from X, classif ier j denotes one classifier and θ j are the parameters in classif ier j . Multiple classifiers can capture the invariant and specific feature distributions between different domains, comparing the similarity of feature distributions between training source and unseen target domains by a joint training of the admixture classifiers, generating weights to integrate the feature distributions of known domains to fit the unknown domain feature distributions."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.1,Data Description,"Structural T1-weighted brain MRI data of 809 subjects (468 male, 341 female, age 68.16 ± 8.12 years, range 42-89 year) were acquired from 7 in-house independent multiple centers as detailed in [5,22]. In total, 552 subjects (295 of normal control (NC), 257 of AD) were used for leave-center-out training. The rest 257 subjects with mild cognitive impairment (MCI) were used as an independent dataset for evaluation and compared with clinical diagnosis metrics."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.2,Implementation Details,"We first evaluated the model using leave-center-out cross-validation, where one center was selected for testing at a time and all remaining centers were used for training. Then, we applied the trained model on an independent validation set of unseen images for subjects with MCI. All images were cropped to have the same size of [80,128,72]. Image features were extracted with 3 × 3 × 3 convolution in the network and 2 × 2 × 2 convolution with a stride of 2 replacing the maximum pooling. The extracted features were passed through a global average pooling layer (Sect. 2.1). N = 6 independent classifiers were used.During training, we sorted all training centers and feed the image features from site i to all classifiers, and set the weight of classif ier j(j=i) to 1 and the weight of the rest classifiers to 0. We used cross-entropy to calculate the prediction error and update the parameters of the feature extractor and classif ier j by backpropagation. In testing stage, we feed the image features from the test center to all classifiers, and the final prediction was used the weighted average of predicted probability over all classifiers as the final prediction.We used SGD algorithm to optimize the model coefficients, and set the initial learning rate to 0.001 and reduce the learning rate to one-tenth of the previous value every 50 epochs. The method was implemented using PyTorch 1.1 with Python 3.7. The experiments were run on an Intel Xeon CPU with 16 cores, 43 GB. RAM and a NVIDIA A5000 GPU with 24 GB memory. The code and model are available at https://github.com/Yanjie-Z/DomainKnowledge4AD. Fig. 2. First row: the left panel evaluates the AUC-ROC curve for each domain through leave-center-out cross validation, and the right panel investigates the association between the predicted probabilities and clinical measure (MMSE) in subjects with Alzheimer's disease (AD), mild cognitive impairment (MCI), and healthy controls (NC). Second row: attention map for an arbitrary example sMRI of a subject with AD, illustrating the most discriminative features learnt from the proposed approach."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.3,Performance Evaluation,"To evaluate the proposed approach, we feed 2 different types of input to the conventional 3D-ResNet [21] and each obtains a models: 1) ResNet, which use the original image as input, and 2) Baseline, which use the bounding box cropping strategy as proposed in Sect. 2.1. In addition, we incorporated the patch-free cropping strategy inspired by [4] to crop the middle-half sub-region of the original input sMRI image of the brain, and feed to ResNet, which we denote as ResNet-PF. The prediction performance are compared in Table 1. Our model achieves an average classification accuracy of 89.25% on all test centers during cross-validation, compared to the average classification accuracy of 85.95% with baseline (without the use of domain knowledge encoding module).We used AUC-ROC curves to evaluate the classification effectiveness [13,17,23] of the model on the test centers, and we counted the AUC-ROC curves for seven centers and compared them accordingly in Fig. 2.To evaluate the interpretability of the model, we used Grad-CAM [24] to analyze the sensitive regions of the model in discriminating AD. We found that the model focused on the hippocampus in the images during prediction, which confirms that AD and the hippocampus have a significant correlation. We also find that the model pays more attention to the hippocampus in discriminating AD than healthy controls. Figure 3 compares the 3D attention map between a subject with AD and a healthy subject who never has AD, demonstrating obvious higher values in hippocampus region."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,4.0,Discussion,"We proposed a novel reproducible and generalizable neural network to assist the automatically diagnosis of AD that benefits from domain knowledge and global contextual information with the help of segmentation-free, resamplingfree, patch-free sub-image. The model was evaluated with leave-center-out crossvalidation and with an independent set of unseen images for subjects with MCI (Fig. 2). It obtains an average accuracy of 89.25%, loss of 0.39 and AUC of 0.92 comparing with 85.95%, 0.58 and 0.91 using ResNet. We apply the proposed model to images from a new domain (never used during training), demonstrating promising results.We did ablation studies to evaluate the proposed method (Table 1), unsurprisingly, the cropped images obtain the best performance. Figures 2 and3 evaluated the explainability of the proposed neural network. The results suggest that the hippocampus and ventricles regions suffer the most in AD, which are consistent with multi-stage segmentation-based methods [5], and clinical measures (in terms of MMSE) on an independent dataset (Fig. 2).Our results and all comparative frameworks tend to predict worse for center 3, probably because it has some subjects with AD who have higher MMSE (Fig. 2) making the diagnosis challenging. As opposite, all models provide the best accuracy for center 5. We will further explore possible reasons of this center imbalance in future work. Another limitation of the presented study is the empirical estimation of early stop strategy during leave-center-out cross validation based on the observed loss ranges. In future work, we will also explore a more automated mechanism to increase model robustness for images from more center."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,5.0,Conclusion,"We proposed a novel end-to-end domain-knowledge constrained neural network for automatic and reproducible diagnosis of AD using sMRI images. We proposed a new domain-knowledge encoding module that learn simultaneously with a ResNet-like feature extractor for domain specific and domain shared representations. The network directly takes the segmentation-free, patch-free images in original resolution as input, which is able to learn with global contextual information for subject-level pathological brain dysmorphologies features to further refines reproducible predictions. Our experiments demonstrate superior performance and generalize well to completely unseen domain."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,1.0,Introduction,"Breast cancer impacts women globally [15] and mammographic screening for women over a certain age has been shown to reduce mortality [7,10,23]. However, studies suggest that mammography alone has limited sensitivity [22]. To mitigate this, supplemental screening like MRI or a tailored screening interval have been explored to add to the screening protocol [1,13]. However, these imaging techniques are expensive and add additional burdens for the patient. Recently, several studies [8,32,33] revealed the potential of artificial intelligence (AI) to develop a better risk assessment model to identify women who may benefit from supplemental screening or a personalized screening interval and these may lead to improved screening outcomes.In clinical practice, breast density and traditional statistical methods for predicting breast cancer risks such as the Gail [14] and the Tyrer-Cuzick models [27] have been used to estimate an individual's risk of developing breast cancer. However, these models do not perform well enough to be utilized in practical screening settings [3] and require the collection of data that is not always available. Recently, deep neural network based models that predict a patient's risk score directly from mammograms have shown promising results [3,8,9,20,33]. These models do not require additional patient information and have been shown to outperform traditional statistical models.When prior mammograms are available, radiologists compare prior exams to the current mammogram to aid in the detection of breast cancer. Several studies have shown that utilizing past mammograms can improve the classification performance of radiologists in the classification of benign and malignant masses [11,25,26,29], especially for the detection of subtle abnormalities [25]. More recently, deep learning models trained on both prior and current mammograms have shown improved performance in breast cancer classification tasks [24]. Integrating prior mammograms into deep learning models for breast cancer risk prediction can provide a more comprehensive evaluation of a patient's breast health.In this paper, we introduce a deep neural network that makes use of prior mammograms, to assess a patient's risk of developing breast cancer, dubbed PRIME+ (PRIor Mammogram Enabled risk prediction). We hypothesize that mammographic parenchymal pattern changes between current and prior allow the model to better assess a patient's risk. Our method is based on a transformer model that uses attention [30], similar to how radiologists would compare current and prior mammograms.The method is trained and evaluated on a large and diverse dataset of over 9,000 patients and shown to outperform a model based on state-of-the art risk prediction techniques for mammography [33]. Although previous models such as LRP-NET and RADIFUSION [5,34] have utilized prior mammograms, PRIME+ sets itself apart by employing an attention mechanism to extract information about the prior scan."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.1,Risk Prediction,"Survival analysis is done to predict whether events will occur sometime in the future. The data comprises three main elements: features x, time of the event t, and the occurrence of the event e [18]. For medical applications, x typically represents patient information like age, family history, genetic makeup, and diagnostic test results (e.g., a mammogram). If the event has not yet occurred by the end of the study or observation period, the data is referred to as right-censored (Fig. 1).We typically want to estimate the hazard function h(t), which measures the rate at which patients experience the event of interest at time t, given that they have survived up to that point. The hazard function can be expressed as the limit of the conditional probability of an event T occurring within a small time interval [t, t + Δt), given that the event has not yet occurred by time t: The cumulative hazard function H(t) is another commonly used function in survival analysis, which gives the accumulated probability of experiencing the event of interest up to time t. This function is obtained by integrating the hazard function over time from 0 to t: H(t) = t 0 h(s)ds."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.2,Architecture Overview,"We build on the current state-of-the art MIRAI [33] architecture, which is trained to predict the cumulative hazard function. We use an ImageNet pretrained ResNet-34 [12] as the image feature backbone. The backbone network extracts features from the mammograms, and the fully connected layer produces the final feature vector x. We make use of two additional fully connected layers to calculate base hazard θ b and time-dependent hazard θ u , respectively. The predicted cumulative hazard is obtained by adding the base hazard and time-dependent hazard, according to:When dealing with right-censored data, we use an indicator function δ i (t) to determine whether the information for sample i at time t should be included in the loss calculation or not. This helps us exclude unknown periods and only use the available information. It is defined as follows:Here, e i is a binary variable indicating whether the event of interest occurs for sample i (i.e., e i = 1) or not (i.e., e i = 0), and C i is the censoring time for sample i, which is the last known time when the sample was cancer-free.We define the ground-truth H is a binary vector of length T max , where T max is the maximum observation period. Specifically, H(t) is 1 if the patient is diagnosed with cancer within t years and 0 otherwise. We use binary cross entropy to calculate the loss at time t for sample i:. The total loss is defined as:Here, N is the number of exams in the training set. The goal of training the model is to minimize this loss function, which encourages the model to make accurate predictions of the risk of developing breast cancer over time."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.3,Incorporating Prior Mammograms,"To improve the performance of the breast cancer risk prediction model, we incorporate information from prior mammograms taken with the same view, using a transformer decoder structure [30]. This structure allows the current and prior mammogram features to interact with each other, similar to how radiologists check for changes between current and prior mammograms.During training, we randomly select one prior mammogram, regardless of when they were taken. This allows the model to generalize to varying time intervals. To pair each current mammogram during inference with the most relevant prior mammogram, we first select the prior mammogram taken at the time closest to the current time. This approach is based on research showing that radiologists often use the closest prior mammogram to aid in the detection of breast cancer [26].Next, a shared backbone network is used to output the current feature x curr and the prior feature x prior . These features are then flattened and fed as input to the transformer decoder, where multi-head attention is used to find information related to the current feature in the prior feature. The resulting output is concatenated and passed through a linear layer to produce the current-prior comparison feature x CP C . The current-prior comparison feature and current feature are concatenated to produce the final feature x * = x CP C ⊕ x curr , which is then used by the base hazard network and time-dependent hazard network to predict the cumulative hazard function Ĥ."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.1,Dataset,"We compiled an in-house mammography dataset comprising 16,113 exams (64,452 images) from 9,113 patients across institutions from the United States, gathered between 2010 and 2021. Each mammogram includes at least one prior mammogram. The dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams, and 7,094 normal exams. Mammograms were captured using Hologic (72.3%) and Siemens (27.7%) devices. We partitioned the dataset by patient to create training, validation, and test sets. The validation set contains 800 exams (198 cancer, 210 benign, 392 normal) from 400 patients, and the test set contains 1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. All data was de-identified according to the U.S HHS Safe Harbor Method. Therefore, the data has no PHI (Protected Health Information) and IRB (Institutional Review Board) approval is not required."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.2,Evaluation,"We make use of Uno's C-index [28] and the time-dependent AUC [16]. The Cindex measures the performance of a model by evaluating how well it correctly predicts the relative order of survival times for pairs of individuals in the dataset. The C-index ranges from 0 to 1, with a value of 0.5 indicating random predictions and a value of 1 indicating that the model is perfect. Time-dependent ROC analysis generates an ROC curve and the area under the curve (AUC) for each specific time point in the follow-up period, enabling evaluation of the model's performance over time. To compare the C-index of two models, we employ the compareC [17] test, and make use of the DeLong test [6] to compare the timedependent AUC values. Confidence bounds are generated using bootstrapping with 1,000 bootstrap samples.We evaluate the effectiveness of PRIME+ by comparing it with two other models: (1) baseline based on MIRAI, a state-of-the art risk prediction method from [33], and (2) PRIME, a model that uses prior images by simply summing x curr and x prior without the use of the transformer decoder."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.3,Implementation Details,"Our model is implemented in Pytorch and trained on four V100 GPUs. We trained the model using stochastic gradient descent (SGD) for 20K iterations with a learning rate of 0.005, weight decay of 0.0001, and momentum of 0.9. We use a cosine annealing learning rate scheduling strategy [21].We resize the images to 960 × 640 pixels and use a batch size of 96. To augment the training data, we apply geometric transformations such as vertical flipping, rotation and photometric transformations such as brightness/contrast adjustment, Gaussian noise, sharpen, CLAHE, and solarize. Empirically, we find that strong photometric augmentations improved the risk prediction model's performance, while strong geometric transformations had a negative impact. This is consistent with prior work [20] showing that risk prediction models focus on overall parenchymal pattern."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.4,Results,"Ablation Study. To better understand the merit of the transformer decoder, we first performed an ablation study on the architecture. Our findings, summarized in Table 1, include two sets of results: one for all exams in the test set and the other by excluding cancer exams within 180 days of cancer diagnosis which are likely to have visible symptoms of cancer, by following a previous study [33]. This latter set of results is particularly relevant as risk prediction aims to predict unseen risks beyond visible cancer patterns. We also compare our method to two other models, the state-of-the-art baseline and PRIME models.As shown in the top rows in Table 1, the baseline obtained a C-index of 0.68 (0.65 to 0.71). By using the transformer decoder to jointly model prior images, we observed improved C-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76). The C-index as well as all AUC differences between the baseline and the PRIME+ are all statistically significant (p < 0.05) except the 4-year AUC where we had a limited number of test cases.We observe similar performance improvements when evaluating cases with at least 180 days to cancer diagnosis. Interestingly, the C-index as well as timedependent AUCs of all three methods decreased compared to when evaluating using all cases. The intuition behind this result is that mammograms taken near the cancer diagnosis (<180 days) likely contain visible signs of cancer and thus the task of risk prediction is easier. The model must learn patterns of risk, not Lastly, we empirically confirm that a transformer decoder effectively models spatial relations between prior and current mammograms by demonstrating consistent performance improvements of PRIME+ across both short-term and longterm risk prediction settings. Our results suggest that incorporating changes in patients using prior mammograms and a transformer decoder improves the performance of breast cancer risk prediction models.Analysis Based on Density. To better understand why adding prior images improves performance, we divided our test set into subgroups to examine the performance of the baseline model and the PRIME+ model on each of these groups. Mammographic breast density is one of the most important risk factor to predict breast cancer [19,31]. Women with dense breasts have a four-to six-fold higher risk of breast cancer [2]. The addition of mammographic breast density has improved the performance traditional breast cancer risk models [4] and can therefore help us understand why the addition of prior images works.Mammographic breast density was determined using the Breast Imaging Reporting and Data System (BI-RADS) composition classification. BI-RADS category A, B are defined as fatty breasts and BI-RADS category C, D are classified as dense breasts. To determine the density category, we employed an internally developed density prediction model, as most exams lack BI-RADS ground truth. This model achieved an accuracy of 0.81 on the internal density validation set.We categorized the exams into two groups based on changes in density: ""change"" and ""no change"". Density change was defined according to whether the BI-RADS category changed in the current image as compared to the prior image. As shown in Table 2, the baseline model performs poorly for ""change"", with a C-index of 0.63 (0.49 to 0.77), especially for long-term risk prediction, with 3-year AUC of 0.56 (0.40 to 0.72). This suggests that the baseline model has limitations in accurately predicting long-term risk when there is a density change from the prior exam. However, PRIME+ is able to predict long-term risk accurately even when a density change has occurred (3-year AUC = 0.74 (0.60 to 0.88)), by learning to refer previous exams properly. This demonstrates the potential usefulness of incorporating past mammogram information into breast cancer risk prediction models. Thus, we believe that incorporating prior exams is important to identify changes in texture which are important for long term risk prediction (Table 3). Lastly, we divided the exams based on the level of breast density, with a fatty group consisting of density A and B, and a dense group consisting of density C and D. Both the baseline and PRIME+ performs better in fatty group than dense group. We suspect this is because deep neural networks generally work better on low density images given that visual cues of cancer in images with lower breast density are more clearly visible."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,4.0,Conclusion,"In this paper, we introduce a novel breast cancer risk prediction method, PRIME+, which incorporates prior mammograms with a transformer decoder to capture changes in breast tissue over time. By doing so, we achieve high performance for both short-term and long-term risk prediction. Our extensive experiments on a dataset of 16,113 exams show that PRIME+ outperformed a model based on the state-of-the-art for breast cancer risk prediction [33]. Our method performed particularly well in cases where there was a change in breast density from the previous exam. We believe that our method has the potential to improve breast cancer risk prediction and ultimately contribute to earlier detection of the disease."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible progressive neurodegenerative brain disorder that ranks as the sixth leading cause of death in the United States [2]. While AD cannot currently be prevented or cured once established, early diagnosis and intervention during the mild cognitive impairment (MCI -precursor of AD) stage offer a feasible way for patients to plan for the future. Although the neuropathological mechanism of MCI is not fully understood, accumulating evidence suggests that both structural and functional brain alterations have been found in MCI patients [8]. Consequently, numerous multimodal fusion approaches have been published [10,14,20,22,24,27,29], significantly enhancing our comprehension of MCI and AD. Combining various modalities of brain data through multimodal fusion provides an invaluable chance to exploit complementary cross-modal information. The outstanding outcomes obtained by these multimodal fusion techniques in MCI/AD research highlight the essential importance of integrating multimodal brain networks and comprehending their intricate connections in the investigation of brain disorders.In the most of existing multimodal fusion methods, Euclidean space is typically assumed as the natural geometry of the brain. As such, both feature embedding and model establishment are conducted in the Euclidean space. However, recent studies have suggested that non-Euclidean hyperbolic space may offer a more accurate interpretation of brain connectomes than Euclidean space [1,28]. For example, [1] studied the navigability properties of structural brain networks across species and found that the Euclidean distance cannot fully explain the structural organization of brain connectomes, while hyperbolic space provides almost perfectly navigable maps of connectomes for all species. Similarly, the research in [28] found that the structure of the human brain remains self-similar (a non-Euclidean property) when the resolution of the network is progressively decreased by hierarchical coarse graining of the anatomical regions. Additionally, in the general AI domain, consistent results have been reported, demonstrating that learning hierarchical representations of graphs is easier in the hyperbolic space due to its curvature and geometrical properties [3,7,15]. These compelling findings raise the question of whether hyperbolic space is a better choice for multimodal fusion in the study of brain diseases.To answer this question, we propose a novel graph-based hyperbolic deep model to conduct multimodal fusion in hyperbolic space for MCI study. Specifically, we embedded brain functional activities into hyperbolic space, where a hyperbolic graph convolution neural network (HGCN) [7] is developed to integrate structural substrate and functional hyperbolic embeddings. The HGCN takes into two inputs: the topology of the network and the associated node features. The input topology is initialized by the individual structural network and iteratively updated by incorporating the corresponding individual functional hyperbolic features to maximize the classification power between elder normal control (NC) and MCI patients. This results in a learned topology that becomes a deeply hybrid connectome embedded in hyperbolic space, combining both brain structural substrate and functional influences. The associated node features are the functional similarities derived from the functional hyperbolic features. By graph convolution in hyperbolic space, node features are aggregated along the topology and used to conduct MCI/NC classification. In the experiments, we comprehensively evaluate the proposed model from three perspectives: Firstly, we compared the classification performance of our proposed model with stateof-the-art methods. Secondly, we conducted an ablation study to evaluate the effectiveness of both the hyperbolic feature embedding and the hyperbolic graph convolutional neural network. Finally, we visualized, analyzed, and compared the feature representation in hyperbolic space and its Euclidean counterpart. The proposed model achieves the outstanding classification performance, and the results demonstrate the advantages of using hyperbolic space for multimodal fusion in the study of brain diseases."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,2.0,Related Work,"Gromov has demonstrated that hyperbolic spaces are particularly well-suited for representing tree-like structures [13], as they allow for the representation of objects that would require an exponential number of dimensions in Euclidean space in only a polynomial number of dimensions in hyperbolic space, with low distortion. To make full use of the geometric properties of hyperbolic space, novel hyperbolic deep neural networks have been proposed and shown to have superior model compactness and predictive performance when compared to their counterparts in Euclidean space. Pioneering works, such as [6,17], have introduced the concept of graph embeddings in hyperbolic space. Recently, HGNN [15] and HGCN [7] were proposed to build graph neural networks in hyperbolic space and achieved remarkable success in graph-related tasks, including node classification [7], graph classification [15], link prediction [7], and graph embedding [3]. Inspired by these works, we attempted to apply the HGCN to brain multimodal fusion studies. In our work, the topology of the HGCN is learnable and incorporates multimodal information from different modalities."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.1,Data Description and Preprocessing,"We used 209 subjects, comprising 116 individuals from the NC group (60 females, 56 males; 74.26 ± 8.42 years) and 93 subjects from the MCI group (53 females, 40 males; 74.24 ± 8.63 years) from ADNI dataset. Each subject has structural MRI (T1-weighted), resting-state fMRI (rs-fMRI), and DTI. We performed standard preprocessing procedures as described in [25,26]. In summary, these steps involved skull removal for all modalities. For rs-fMRI images, we conducted spatial smoothing, slice time correction, temporal pre-whitening, global drift removal, and band-pass filtering (0.01-0.1 Hz) using FMRIB Software Library (FSL) FEAT. As for DTI images, we performed eddy current correction using FSL and fiber tracking using MedINRIA. T1-weighted images were registered to DTI space using FSL FLIRT, followed by segmentation using the FreeSurfer package. Subsequently, the Destrieux Atlas [9] was utilized for ROI labeling, and the brain cortex was partitioned into 148 regions. We calculated the average fMRI signal for each brain region as node features and generated the individual structural network using fiber count."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.2,Preliminary,"The hyperbolic space H n K is a specific type of n-dimension Riemannian manifold (M, g) with a constant negative sectional curvature K [4]. It has some unique properties, such as its exponential growth rate, which makes it useful in areas like geometry, topology, and computer science [18]. In hyperbolic space, isometric hyperbolic models are used to maintain the relationship between points without distorting distances. The hyperbolic space is usually defined via five isometric hyperbolic models [19], among which the n-dimensional Poincaré ball model is frequently used because it provides a more intuitive visualization of hyperbolic space, which can be easier to understand than other models. Therefore, in this work, we align with [7] and build our model upon the Poincaré ball model B n K . Existing studies [11,16,21] adopt the Möbius transformations as the non-associative algebraic formalism for hyperbolic geometry. Three basic operations of two elements x, y on Poincaré ball model, including addition, scalar multiplication, and matrix-vector multiplication, are defined as:A manifold M of n-dimension is a topological space that is locally Euclidean. For all x ∈ M, the tangent space T x M at a point x is the vector space of the same n-dimension as M, containing all tangent vector tangentially pass through x. The metric tensor g x [12] on point x defines an inner product on the associated tangent space. Accordingly, two mappings are defined to achieve transformation between the Euclidean tangent space T x M and hyperbolic space M: Exponential map Exp K x : T x M → M that maps an arbitrary tangent vector v ∈ T x M to M and logarithmic map Log Kx : M → T x M that maps an arbitrary point y ∈ M to T x M:where λ K x = 2 1+K||x|| 2 is the conformal factor at point x."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.3,Functional Profile Learning in Hyperbolic Space,"In this work, we aim to learn a disease-related functional profile in the hyperbolic space. To this end, we firstly mapped the averaged functional signal of region i in Euclidean -f E i , to hyperbolic space via (4):), here we choose point 0, the origin in hyperbolic space to conduct transformation. Then upon f H i , we defined the parameterized functional-pairwise distance between region i and region j in hyperbolic space by a learnable mapping matrix M :It is worth noting that ( 6) is a linear projection. It is inadequate for modeling the distance/similarity of the complicated fMRI signals. To alleviate this issue, we introduced nonlinear projection by applying Gaussian kernel:))Here, A f H i,j ∈ B n K represents the pairwise functional profile between brain regions i and j in hyperbolic space. σ is the bandwidth parameter of Gaussian kernel and is treated as a hyper-parameter. In the proposed model, M is initialized as identity matrix to avoid introducing any bias and iteratively updated during the training process based on classification results."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.4,Multimodal Fusion by HGCN,"A major goal of this work is to conduct effective multimodal fusion of brain structural and functional data in hyperbolic space for MCI study. To achieve this aim, we combined the learned functional profile with the brain structural network in the hyperbolic space by:where I is the identity matrix, A S E is the original individual structural network calculated by fiber count, and A S H = Exp K 0 (A S E ) is the hyperbolic counterpart of A S E . θ ∈ (0, 1) is a learnable parameter to control the contributions of structural and functional components in the combined new brain connectome ÂH . In the training process, A f H and A S H will be iteratively updated, and disease-related knowledge (from classification) is extracted and passed to functional profile (A f H ) and structural network (A S H ) and then transferred to the new brain connectome ÂH . Next, ÂH will be used as the new topology along with node features in graph convolution conducted by HGCN.HGCN performs graph convolution within the hyperbolic space in two steps:where h (l) i is the input of i-th node in the l-th layer, W ∈ R d×d and b ∈ R d are the weight and bias of the l-th layer. ( 9) is the operation of input features and model parameters. After that, for each node i, the feature vector of its neighbors -h(l) j , will firstly be mapped to i's local tangent space by Log K h(l) i to conduct graph convolution with adjacency matrix A, then the results will be mapped back to the hyperbolic space by Exp K h(l) i and used as the input in next layer in HGCN. In a previous study [26], seven different methods for creating functional connectivity were compared, and it was found that the Pearson correlation coefficient (PCC) outperformed other approaches. Therefore, in our work, the PCC of f H i is used as node features (hi ) and ÂH is used as the adjacency matrix A in Eq. 10."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.1,Experimental Setting,"Data Settings. In this study, the entire brain was partitioned into 148 distinct regions using the well-established Destrieux Atlas. Averaged fMRI signals were then calculated for each brain region, and the brain structural network (A S ) was generated for each individual. 5-fold cross-validation was performed using a cohort of 209 individuals, consisting of 116 elder normal control (NC) and 93 MCI patients.Model Settings. The HGCN model in this work has two layers, wherein the output of each HGCN layer was set to 148 and 296, respectively. These outputs were subsequently combined using a hyperbolic fully connected layer [11] with an output size denoted by C, where C corresponds to the number of classes for classification purposes (in our work, C = 2). To optimize the model's parameters, an end-to-end training approach was adopted, and the Xavier initialization scheme was utilized. During the training process, the Riemannian SGD [5] optimizer was applied with a standard learning rate of 1. "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.2,Classification Performance Comparison,"We compared the classification performance of our proposed model with other state-of-the-art methods on MCI/NC classification task using multi-modal data and presented the results in Table 1. The feature embedding and model development of all the other studies were conducted in Euclidean space. In addition, some of them employed PET data, which tends to yield higher classification accuracy than MRI-based modalities. However, it is noteworthy that our approach utilized noninvasive DTI/fMRI data and achieved outstanding performance with a classification accuracy of 92.3% for MCI/NC classification. Although study [23] reported a slightly higher accuracy than ours, we sought to eliminate biases arising from factors other than the methods by implementing the methods used in [23] with the same data settings as ours and showed results in the "" [23] re-implement"" row. Notably, our model outperforming [23] in MCI/NC classification when using the same data setting. It is worth mentioning that the only difference between "" [23] re-implement"" and our method is the space, suggesting that hyperbolic space is a superior choice for multi-modal fusion in MCI/NC classification. "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.3,Ablation Study,"The proposed model implements both feature embedding and graph neural network establishment in hyperbolic space. We conducted ablation studies to evaluate the impact of the two factors on the classification performance of MCI/NC classification. In contrast to Euclidean space, where the curvature is a constant 0, hyperbolic space has negative curvature, and we evaluated a range of curvature values from -0.4 to -1.4. The results were summarized in Table 2, with the best and worst performances highlighted in blue and red colors, respectively. The best results were obtained when both feature embedding and graph neural network establishment were implemented in hyperbolic space, while the worst results were observed in the pure Euclidean setting. We found that the hybrid combination of GCN established in Euclidean space with the feature embedding in hyperbolic space achieved better performance compared to the pure Euclidean setting, but in most cases, it performed worse than the pure hyperbolic setting. These findings indicate the superiority of hyperbolic feature embedding and graph neural network over their Euclidean counterparts."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.4,Feature Representation,"To gain further insight into the impact of space choice on feature representation, we visualized the distribution of two distinct feature representations in Euclidean space and hyperbolic space. Firstly, we employed principal component analysis (PCA) to project the high-dimensional input feature vectors from both the Euclidean space and the hyperbolic space onto a two-dimensional space. The resulting visualizations are presented in Fig. 1 (a). Notably, the feature vectors in hyperbolic space exhibited a more uniform distribution, with samples distributed more evenly across the space. In contrast, the feature distribution in Euclidean space was non-uniform, with some samples scattered far away from others (highlighted by purple arrows). These differences highlight the advantages of hyperbolic space for representing diverse and complex relationships of input data. Secondly, we projected the latent feature vectors before the classification layer onto a two-dimensional space and presented the results in Fig. 1 (b). These feature vectors are directly related to the classification performance, and their distribution can have a significant impact on classification accuracy. In the hyperbolic space, it is evident that the feature vectors of the two classes have no overlapping areas, while in the Euclidean space, there is a considerable amount of mixed data where the two types are handed over (highlighted by oval circles). This difference highlights the superiority of hyperbolic space in achieving a clear and distinguishable feature representation for classification tasks."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,5.0,Conclusion,"It is widely believed that the AD/MCI related brain alterations involve both brain structure and function. However, effectively modeling the complex relationships between structural and functional data and integrating them at the network level remains a challenge. Recent advances in graph modeling in hyperbolic space have inspired us to integrate multimodal brain networks via graph convolution conducted in the hyperbolic space. To this end, we mapped brain structural and functional features into hyperbolic space and conducted graph convolution by a hyperbolic graph convolution neural network, which enabled us to obtain a new brain connectome that incorporates multimodal information.Our developed model has demonstrated exceptional performance when compared to state-of-the-art methods. Furthermore, the results suggest that feature embedding and graph convolution neural network establishment in hyperbolic space are both crucial for enhancing performance."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,1.0,Introduction,"Deep learning models have demonstrated undoubted potential in achieving expert-level medical image interpretation when powered by large-scale labeled datasets [4,7,20]. However, medical image annotations require expert knowledge thus are extremely costly and difficult to obtain at scale. Such an issue has even become a bottleneck for advancing deep learning models in medical applications. To tackle this issue, recent efforts have resorted to the text reports that are paired with the medical images, aiming to leverage the detailed text interpretation provided by radiologists to assist the representation learning of medical images without relying on any manual labels [3,10,18,[25][26][27]. The learned image representations have proven to be generalizable to other downstream tasks of medical image analysis, which can significantly reduce the amount of labeled data required for fine-tuning. This topic has been actively studied and seen rapid progress with evaluation on chest X-ray datasets, because of both the clinical importance of radiograph screening and the availability of large-scale datasets of public chest X-ray images with paired radiology reports [14].The current mainstream approaches for medical image-text pre-training are based on the popular self-supervised learning (SSL) technique known as contrastive learning [2,9], which maximizes agreement between global and local representations of paired images and reports with a contrastive loss [1,10,15,22,26,27]. These methods have demonstrated the effectiveness of using medical reports as a form of free supervision to enhance the learning of general image representations. Another well-demonstrated SSL method is masked autoencoding, which achieves representation learning via solving the pretext task of recovering masked image patches with unmasked ones [8]. Until very recently, the potential of masked autoencoding had only begun to be explored for medical image-text pre-training. In a latest work, Zhou et al. [28] propose to learn radiograph representations with a unified framework that requires the unmasked image patches to recover masked images and complete text reports. While both contrastive learning and masked autoencoding have demonstrated their ability to learn effective image representations, the two major SSL techniques have only been separately explored. The latest research has started to combine the two SSL techniques for joint benefits, but their focus is on the image domain rather than cross-modal learning in medical images and paired text reports [11,13,17]. It remains an interesting and unexplored question whether contrastive learning and masked autoencoding can benefit each other and how to jointly exploit their strengths for medical image-text pre-training.In fact, the learning principles of contrastive learning and masked autoencoding suggest that they could be complementary to each other. Contrastive imagetext learning explicitly discriminates the positive and negative pairs of images and text reports, making it good at promoting strong discriminative capabilities of image representations. Instead, masked autoencoding aims to reconstruct masked image/text tokens, which emphasizes learning local image structures, but may be less effective in capturing discriminative representations. This motivates us to propose a novel contrastive masked image-text modeling (CMITM) method for medical visual representation learning. Our framework is designed to accomplish three self-supervised learning tasks: First, aligning the representations of masked images with text reports. Second, reconstructing the masked images themselves. Third, reconstructing the masked text reports using the learned image representations. To reduce the information misalignment between the masked images and text reports, we incorporate a representation decoder to recover the missed information in images, which benefits the cross-modal learning. Moreover, the synergy of contrastive learning and masked autoencoding is unleashed via a cascaded training strategy. Our framework is pre-trained on a large-scale medical dataset MIMIC-CXR with paired chest X-ray images and reports, and is extensively validated on four downstream classification datasets with improved fine-tuning performance. Combining the two techniques yields consistent performance increase and the improvement of our method even surpasses the benefits of adding data from 1% to 100% labels on CheXpert dataset."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.0,Method,"Figure 1 illustrates our contrastive masked image-text modeling (CMITM) framework. In this section, we first present how the cross-modal contrastive learning and masked autoencoding are realized in the framework respectively. Then we introduce the training procedures and implementation details of the framework."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.1,Cross-Modal Contrastive Learning with Masked Images,"Cross-modal contrastive learning has demonstrated to be an effective tool to align the representations of a medical image with that of its paired text report. In this way, the network is guided to interpret the image contents with the knowledge provided by medical reports. Different from previous methods, the cross-modal contrastive learning in our framework is between the representations of masked images and unmasked reports, aiming to integrate the benefits of both contrastive learning and masked image modeling.Specifically, denote D = {x v,i , x t,i } N i=1 as the multi-modal dataset consisting of N pairs of medical images x v,i and medical reports x t,i . Each input image is split into 16 × 16 non-overlap patches and tokenized as image tokens a v,i , and each text report is also tokenized as text tokens a t,i . A random subset of image patches is masked out following masked autoencoder (MAE) [8]. As shown in Fig. 1, the unmasked patches are forwarded to the image encoder E v , which embeds the inputs by a linear projection layer with added positional embeddings and then applies a series of transformer blocks to obtain token representations of unmasked patches q um v,i . Directly utilizing the representations of only unmasked patches to perform contrastive learning with the text could be less effective, since a large portion of image contents has been masked out and the information from the images and texts are misaligned. To recover the missing information in the images, we feed both the encoded visible patches q um v,i and trainable mask tokens q m v,i with added positional embeddings e p v,i to a representation decoder D r with two layers of transformer blocks. The representation decoder aims to output the representations of all image patches, i.e., qv,i = D r ([q um v,i , q m v,i ] + e p v,i ). Such a design helps to avoid that the contrastive learning is confused by the misaligned information between masked images and text reports. Finally we apply a global average pooling operation and a project layer h v to obtain the image embeddings z v,i , i.e., z v,i = h v (AvgPool(q v,i ). For text branch, we consider the full reports could give more meaningful guidance to image understanding than masked ones. So we forward the full text tokens without masking to the text encoder E t and the text project layer h t to obtain the global text embeddings z t,i , i.e., z t,i = h t (E t (a t,i )). To ensure that the embeddings of images are aligned with those of paired texts while remaining distant from unpaired texts, we employ cross-modal contrastive learning with the following symmetric InfoNCE loss [19].where s vt i,j = z T v,i z t,j , s tv i,j = z T t,i z v,j , τ denotes the temperature which is set to be 0.07 following common practice, and B is the number of image-report pairs in a batch. The cross-modal contrastive loss is used to supervise the network training associated with the data flow in orange line in Fig. 1."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.2,Masked Image-Text Modeling,"The masked image-text modeling component in our framework consists of two parallel tasks, i.e., masked image reconstruction with image only information and masked text reconstruction with cross-modal information. We follow the design in [8,28] for the masked image-text modeling since our main focus is whether masked autoencoding and contrastive learning can have joint benefits."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,,Masked Image Reconstruction.,"As aforementioned, the input images are masked and processed by the image encoder E v to obtain q um v,i . As shown in Fig. 1, besides the representation decoder to reconstruct image representations, we also connect both the encoded visible patches and learnable unmasked tokens with added positional embeddings to an image decoder D v to reconstruct the pixel values of masked patches, i.e., xv,i = D v ([q um v,i , q m v,i ] + e p v,i )). The image decoder consists of a series of transformer blocks and a linear projection layer to predict the values for each pixel in a patch. To enhance the learning of local details, the image decoder is required to reconstruct a high-resolution patch, which is twice the input resolution [28]. The training of image reconstruction is supervised by a mean squared error loss L vr which computes the difference between the reconstructed and original pixel values for the masked patches:where xm v,i , xm v,i denote the predicted and the original high-resolution masked patches, | • | calculates the number of masked patches, and norm denotes the pixel normalization with the mean and standard deviation of all pixels in a patch suggested in MAE [8]. The loss is only computed on the masked patches."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,,Cross-Modal Masked Text Reconstruction.,"To make the most of the text reports paired with imaging data for learning visual representations, the task of cross-modal masked text modeling aims to encourage the encoded visible image tokens q um v,i to participate in completing the masked text reports. Specifically, besides the full texts, we also forward a masked text report with a masking ratio of 50% to the text encoder E t . Following [28], this value is deliberately set to be higher than the masking ratio of 15% in BERT [5] in order to enforce the image encoder to better understand the image contents by trying to reconstruct a large portion of masked texts. Then the global embedding of corresponding unmasked image patches AvgPool(q um v,i ) is added to the text token embeddings q um t,i to form a multi-modal embeddings. To reconstruct the masked text tokens, the multi-modal embeddings are processed by the text encoder E t and a text token classifier D t , i.e., ât,i = D t (E t (q um t,i + AvgPool(q um v,i ))). The training of text reconstruction is supervised by the cross entropy loss between the predictions and original text tokens as follows:where a m t,i , âm t,i denote the original and recovered masked text tokens respectively, H denotes the cross entropy loss. Similar to masked image reconstruction, the loss is also only computed on the masked text tokens. The image and text reconstruction losses are used to supervise the network training associated with the data flow in gray line in Fig. 1."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.3,Training Procedures and Implementation Details,"Training a framework that combines cross-modal contrastive learning and masked autoencoding is non-trivial. As observed in prior work [13], forming the training as a parallel multi-task learning task can lead to decreased performance, which might be caused by the conflicting gradients of the contrastive and reconstruction losses. Similar phenomenon has also been observed in our experiments. We therefore adopt a cascaded training strategy, that is the framework is first trained with the reconstruction loss L r = L vr + L tr and is further trained with the contrastive loss L c . Such a training order is considered based on the insights that masked autoencoding focuses more on the lower layers with local details while contrastive learning is effective in learning semantic information for higher layers. Specifically, the first stage of pre-training follows [28] to employ the loss L r for training 200 epochs. The model is trained using AdamW [16] optimizer with a learning rate of 1.5e-4 and a weight decay of 0.05. In the second stage of pre-training, the image encoder, text encoder, and representation decoder are further trained with the loss L c for 50 epochs. Similarly a AdamW optimizer with a learning rate of 2e-5 and a weight decay of 0.05 is adopted. The framework is implemented on 4 pieces of Tesla V100 GPU with a batch size of 256. For network configurations, we use ViT-B/16 [6] as the image encoder and BERT [5] as the text encoder. The image decoder and representation decoder consists of four transformer blocks and two transformer blocks respectively."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,3.0,Experiments,"To validate our framework, the image encoder of the pre-trained model is used to initialize a classification network with a ViT-B/16 backbone and a linear classification head. We adopt the fine-tuning strategy as used in [26,28], where both the encoder and classification head are fine-tuned. This fine-tuning setting reflects how the pre-trained weights can be applied in practical applications. For each dataset, the model is fine-tuned with 1%, 10%, and 100% labeled training data to extensively evaluate the data efficiency of different pre-trained models. The dataset split remains consistent across all approaches.Pre-training Dataset. To pre-train our framework, we utilize MIMIC-CXR dataset [14], which is a large public chest X-ray collection. The dataset contains 377,110 images extracted from 227,835 radiographic studies and each radiograph is associated with one radiology report. For pre-training, images are resized and randomly cropped into the size of 448 × 448 and 224 × 224 as the high-resolution image reconstruction ground truth and low-resolution inputs respectively.Fine-Tuning Datasets. We transfer the learned image representations to four datasets for chest X-ray classification. NIH ChestX-ray [24] includes 112,120 chest X-ray images with 14 disease labels. Each chest radiograph can associate with multiple diseases. We follow [28] to split the dataset into 70%/10%/20% for training, validation, and testing. CheXpert [12] comprises 191,229 chest X-ray images for multi-label classification, i.e., atelectasis, cardiomegaly, consolidation, edema, and pleural effusion. We follow previous work to use the official validation set as the test images and randomly split 5,000 images from training data as the validation set. RSNA Pneumonia [21] dataset contains 29,684 chest X-rays for a binary classification task of distinguishing between normal and pneumonia. Following [28], the dataset is split as training/validation/testing with 25,184/1,500/3,000 images respectively. COVIDx [23] is a three-class classification dataset with 29,986 chest radiographs from 16,648 patients. The task either masked autoencoding or contrastive learning alone, showing the effectiveness of combining the two SSL techniques. It can be observed that our method shows the most obvious improvements in scenarios with limited data. When fine-tuning with 1% labeled data, our method outperforms the current bestperforming method MRM by 1.0% on NIH ChestX-ray dataset and 1.5% on COVIDx dataset. On CheXpert dataset, MRM model shows 0.2% performance gain when increasing labeled data from 1% to 100%, but with our method, fine-tuning on 1% labeled data already outperforms MRM model fine-tuned on 100% labeled data. These results may indicate that masked autoencoding and contrastive learning benefit each other more in data-scare scenarios.Ablation Study. We perform ablation analysis on RSNA and COVIDx datasets with 1% labeled data to investigate the effect of each component in the proposed method. In Fig. 2, we can see that removing either the masked image-text modeling (MITM) or cross-modal contrastive learning (CCL) in our method lead to a decrease in fine-tuning results. This again reflects the complementary role of the two SSL components. In Table 2, we show that the designs of cascaded training strategy, image masking in the contrastive learning, and representation decoder play important role to the performance of our method. Notably, results significantly decrease if not using cascaded training, that means directly using reconstruction loss and contrastive loss for joint training bring negative effects. These ablation results show that combing the two SSL approaches is non-trivial and requires careful designs to make it to be effective. For masking ratio, previous work [28] has shown that the masking ratio of 75% works well in masked medical image-text reconstruction. So we directly adopt the masking ratio of 75% for the masked image-text modeling in the first pre-training stage, but we analyze how the performance changes with the masking ratio for the contrastive learning in the second pre-training stage. As shown in Fig. 3, it is interesting to see that the optimal masking ratio is also 75%. This might indicate that the masking ratio should keep as the same during the cascaded training."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,4.0,Conclusion,"We present a novel framework for medical visual representation learning by integrating the strengths of both cross-modal contrastive learning and masked image-text modeling. With careful designs, the effectiveness of our method is demonstrated on four downstream classification datasets, consistently improving data efficiency under data-scarce scenarios. This shows the complementary benefits of the two SSL techniques in medical visual representation learning. One limitation of the work is that the pre-training model is evaluated solely on classification tasks. A compelling extension of this work would be to conduct further evaluation on a broader spectrum of downstream tasks, including organ segmentation, lesion detection, and image retrieval, thereby providing a more comprehensive evaluation of our model's capabilities."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,1.0,Introduction,"Lung cancer screening has a significant impact on the rate of mortality associated with lung cancer. Studies have proven that regular lung cancer screening with low-dose computed tomography (LDCT) can lessen the rate of lung cancer mortality by up to 20% [1,14]. As most (e.g., 95% [13]) of the detected nodules are benign, it is critical to accurately assess their malignancy on CT to achieve a timely diagnosis of malignant nodules and avoid unnecessary procedures such as biopsy for benign ones. Particularly, the evaluation of nodule (i.e., 8-30mm) malignancy is recommended in the guidelines [13].Fig. 1. In PARE, a nodule is diagnosed from two levels: first parsing the contextual information contained in the nodule itself, and then recalling the previously learned nodules to look for related clues.One of the major challenges of lung nodule malignancy prediction is the quality of datasets [6]. It is characterized by a lack of standard-oftruth of labels for malignancy [16,27], and due to this limitation, many studies use radiologists' subjective judgment on CT as labels, such as LIDC-IDRI [3]. Recent works have focused on collecting pathologically labeled data to develop reliable malignancy prediction models [16,17,19]. For example, Shao et al. [16] collated a pathological gold standard dataset of 990 CT scans. Another issue is most of the studies focus on LDCT for malignancy prediction [10]. However, the majority of lung nodules are incidentally detected by routine imaging other than LDCT [4,15], such as noncontrast chest CT (NCCT, the most frequently performed CT exam, nearly 40% [18]).Technically, current studies on lung nodule malignancy prediction mainly focus on deep learning-based techniques [10,12,17,23,24]. Liao et al. [10] trained a 3D region proposal network to detect suspicious nodules and then selected the top five to predict the probability of lung cancer for the whole CT scan, instead of each nodule. To achieve the nodule-level prediction, Xie et al. [24] introduced a knowledge-based collaborative model that hierarchically ensembles multi-view predictions at the decision level for each nodule. Liu et al. [12] extracted both nodules' and contextual features and fused them for malignancy prediction. Shi et al. [17] effectively improved the malignancy prediction accuracy by using a transfer learning and semi-supervised strategy. Despite their advantages in representation learning, these methods do not take into account expert diagnostic knowledge and experience, which may lead to a bad consequence of poor generalization. We believe a robust algorithm should be closely related to the diagnosis experience of professionals, working like a radiologist rather than a black box.In this paper, we suggest mimicking radiologists' diagnostic procedures from intra-context parsing and inter-nodule recalling (see illustrations in Fig. 1), "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,abbreviated as PARE.,"At the intra-level, the contextual information of the nodules provides clues about their shape, size, and surroundings, and the integration of this information can facilitate a more reliable diagnosis of whether they are benign or malignant. Motivated by this, we first segment the context structure, i.e., nodule and its surroundings, and then aggregate the context information to the nodule representation via the attention-based dependency modeling, allowing for a more comprehensive understanding of the nodule itself. At the inter-level, we hypothesize that the diagnosis process does not have to rely solely on the current nodule itself, but can also find clues from past learned cases. This is similar to how radiologists rely on their accumulated experience in clinical practice. Thus, the model is expected to have the ability to store and recall knowledge, i.e., the knowledge learned can be recorded in time and then recalled as a reference for comparative analysis. To achieve this, we condense the learned nodule knowledge in the form of prototypes, and recall them to explore potential inter-level clues as an additional discriminant criterion for the new case. To fulfill both LDCT and NCCT screening needs, we curate a large-scale lung nodule dataset with pathology-or follow-up-confirmed benign/malignant labels. For the LDCT, we annotate more than 12,852 nodules from 8,271 patients from the NLST dataset [14]. For the NCCT, we annotate over 4,029 nodules from over 2,565 patients from our collaborating hospital. Experimental results on several datasets demonstrate that our method achieves outstanding performance on both LDCT and NCCT screening scenarios.Our contributions are summarized as follows: (1) We propose context parsing to extract and aggregate rich contextual information for each nodule. (2) We condense the diagnostic knowledge from the learned nodules into the prototypes and use them as a reference to assist in diagnosing new nodules. (3) We curate the largest-scale lung nodule dataset with high-quality benign/malignant labels to fulfill both LDCT and NCCT screening needs. (4) Our method achieves advanced malignancy prediction performance in both screening scenarios (0.931 AUC), and exhibits strong generalization in external validation, setting a new state of the art on LUNGx (0.801 AUC)."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.0,Method,"Figure 2 illustrates the overall architecture of PARE, which consists of three stages: context segmentation, intra context parsing, and inter prototype recalling. We now delve into different stages in detail in the following subsections."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.1,Context Segmentation,"The nodule context information has an important effect on the benign and malignant diagnosis. For example, a nodule associated with vessel feeding is more likely to be malignant than a solitary one [22]. Therefore, we use a U-like network (UNet) to parse the semantic mask m for the input image patch x, thus allowing subsequent context modeling of both the nodule and its surrounding structures. Specifically, each voxel of m belongs to {0 : background, 1 : lung, 2 : nodule, 3 : vessel, 4 : trachea}. This segmentation process allows PARE to gather comprehensive context information that is crucial for an accurate diagnosis. For the diagnosis purpose, we extract the global feature from the bottleneck of UNet as the nodule embedding q, which will be used in later diagnostic stages."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.2,Intra Context Parse,"In this stage, we attempt to enhance the discriminative representations of nodules by aggregating contextual information produced by the segmentation model. Specifically, the context mask is tokenized into a set of sequences via the overlapped patch embedding. The input image is also split into patches and then embedded into the context tokens to keep the original image information. Besides, positional encoding is added in a learnable manner to retain location information. Similar to the class token in ViT [7], we prepend the nodule embedding token to the context sequences, denoted by [q; t 1 , ..., t g ] ∈ R (g+1)×D . Here g is the number of context tokens, and D represents the embedding dimension. Then we perform the self-attention modeling on these tokens simultaneously, called Self Context Attention (SCA), to aggregate context information into the nodule embedding. The nodule embedding token at the output of the last SCA block serves as the updated nodule representation. We believe that explicitly modeling the dependency between nodule embedding and its contextual structure can lead to the evolution of more discriminative representations, thereby improving discrimination between benign and malignant nodules."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.3,Inter Prototype Recall,"Definition of the Prototype: To retain previously acquired knowledge, a more efficient approach is needed instead of storing all learned nodules in memory, which leads to a waste of storage and computing resources. To simplify this process, we suggest condensing these pertinent nodules into a form of prototypes. As for a group of nodules, we cluster them into N groups {C 1 , ..., C N } by minimizing the objective function N i=1 p∈Ci d(p, P i ) where d is the Euclidean distance function and p represents the nodule embedding, and refer the center of each cluster, P i = 1 |Ci| p∈Ci p, as its prototype. Considering the differences between benign and malignant nodules, we deliberately divide the prototypes into benign and malignant groups, denoted by P B ∈ R N/2×D and P M ∈ R N/2×D . Cross Prototype Attention: In addition to parsing intra context, we also encourage the model to capture inter-level dependencies between nodules and external prototypes. This enables PARE to explore relevant identification basis beyond individual nodules. To accomplish this, we develop a Cross-Prototype Attention (CPA) module that utilizes nodule embedding as the query and the prototypes as the key and value. It allows the nodule embedding to selectively attend to the most relevant parts of prototype sequences. The state of query at the output of the last CPA module servers as the final nodule representation to predict its malignancy label, either ""benign"" (y = 0) or ""malignant"" (y = 1)."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Updating Prototype Online:,"The prototypes are updated in an online manner, thereby allowing them to adjust quickly to changes in the nodule representations. As for the nodule embedding q of the data (x, y), its nearest prototype is singled out and then updated by the following momentum rules,where λ is the momentum factor, set to 0.95 by default. The momentum updating can help accelerate the convergence and improve the generalization ability."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.4,Training Process of PARE,"The Algorithm 1 outlines the training process of our PARE model which is based on two objectives: segmentation and classification. The Dice and crossentropy loss are combined for segmentation, while cross-entropy loss is used for classification. Additionally, deep classification supervision is utilized to enhance the representation of nodule embedding in shallow layers like the output of the UNet and SCA modules."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,"Data Collection and Curation: NLST is the first large-scale LDCT dataset for low-dose CT lung cancer screening purpose [14]. There are 8,271 patients enrolled in this study. An experienced radiologist chose the last CT scan of each for l = 1, ..., L do 12:Cross prototype attention 13:end for 15:"
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,16:,Update prototype according to Eq. 1
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,17:,"J ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) Update loss18: end for patient, and localized and labeled the nodules in the scan as benign or malignant based on the rough candidate nodule location and whether the patient develops lung cancer provided by NLST metadata. The nodules with a diameter smaller than 4mm were excluded. The in-house cohort was retrospectively collected from 2,565 patients at our collaborating hospital between 2019 and 2022. Unlike NLST, this dataset is noncontrast chest CT, which is used for routine clinical care. Segmentation annotation: We provide the segmentation mask for our in-house data, but not for the NLST data considering its high cost of pixel-level labeling. The nodule mask of each in-house data was manually annotated with the assistance of CT labeler [20] by our radiologists, while other contextual masks such as lung, vessel, and trachea were generated using the TotalSegmentator [21]."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Train-Val-Test:,"The training set contains 9,910 (9,413 benign and 497 malignant) nodules from 6,366 patients at NLST, and 2,592 (843 benign and 1,749 malignant) nodules from 2,113 patients at the in-house cohort. The validation set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients at NLST. The NLST test set has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. The in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452 patients. We additionally evaluate our method on the LUNGx [2] challenge dataset, which is usually used for external validation in previous work [6,11,24]. LUNGx contains 83 (42 benign and 41 malignant) nodules, part of which (13 scans) were contrast-enhanced. Segmentation: We also evaluate the segmentation performance of our method on the public nodule segmentation dataset LIDC-IDRI [3], which has 2,630 nodules with nodule segmentation mask. Evaluation metrics: The area under the receiver operating characteristic curve (AUC) is used to evaluate the malignancy prediction performance.Implementation: All experiments in this work were implemented based on the nnUnet framework [8], with the input size of 32 × 48 × 48, batch size of 64, and total training iterations of 10K. In the context patch embedding, each patch token is generated from a window of 8 × 8 × 8. The hyper-parameters of PARE are empirically set based on the ablation experiments on the validation set. For example, the Transformer layer is set to 4 in both SCA and CPA modules, and the number of prototypes is fixed to 40 by default. More details can be found in the ablation. Due to the lack of manual annotation of nodule masks for the NLST dataset, we can only optimize the segmentation task using our in-house dataset, which has manual nodule masks."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.2,Experiment Results,"Ablation Study: In Table 1, we investigate the impact of different configurations on the performance of PARE on the validation set, including Transformer layers, number of prototypes, embedding dimension, and deep supervision. We observe that a higher AUC score can be obtained by increasing the number of Transformer layers, increasing the number of prototypes, doubling the channel dimension of token embeddings, or using deep classification supervision. Based on the highest AUC score of 0.931, we empirically set L=4, N=40, D=256, and DS=True in the following experiments. In Table 2, we investigate the ablation study of different methods/modules on the validation set and observe the following results: (1) The pure segmentation method performs better than the pure classification method, primarily because it enables greater supervision at the pixel level, (2) the joint segmentation and classification is superior to any single method, indicating the complementary effect of both tasks, (3) both context parsing and prototype comparing contribute to improved performance on the strong baseline, demonstrating the effectiveness of both modules, and (4) segmenting more contextual structures such as vessels, lungs, and trachea provide a slight improvement, compared to solely segmenting nodules.  External Evaluation on LUNGx: We used LUNGx [2] as an external test to evaluate the generalization of PARE. It is worth noting that these compared methods have never been trained on LUNGx. Table 4 shows that our PARE model achieves the highest AUC of 0.801, which is 2% higher than the best method DAR [11]. We also conducted a reader study to compare PARE with two experienced radiologists, who have 8 and 13 years of lung nodule diagnosis experience respectively. Results in Fig. 3 reveal that our method achieves performance comparable to that of radiologists."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Generalization on LDCT and NCCT:,"Our model is trained on a mix of LDCT and NCCT datasets, which can perform robustly across low-dose and regular-dose applications. We compare the generalization performance of the models obtained under three training data configurations (LDCT, NCCT, and a combination of them). We find that the models trained on either LDCT or NCCT dataset alone cannot generalize well to other modalities, with at least a 6% AUC drop. However, our mixed training approach performs best on both LDCT and NCCT with almost no performance degradation. Method AUC NLNL [9] 0.683 D2CNN [26] 0.746 KBC [24] 0.768 DAR [11] 0.781 PARE (Ours) 0.801 Fig. 3. Reader study compared with AI."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,4.0,Conclusion,"In this work, we propose the PARE model to mimic the radiologists' diagnostic procedures for accurate lung nodule malignancy prediction. Concretely, we achieve this purpose by parsing the contextual information from the nodule itself and recalling the previous diagnostic knowledge to explore related benign or malignancy clues. Besides, we curate a large-scale pathological-confirmed dataset with up to 13,000 nodules to fulfill the needs of both LDCT and NCCT screening scenarios. With the support of a high-quality dataset, our PARE achieves outstanding malignancy prediction performance in both scenarios and demonstrates a strong generalization ability on the external validation."
Uncertainty Inspired Autism Spectrum Disorder Screening,1.0,Introduction,"Autism Spectrum Disorder (ASD) has been a prevalent neurodevelopmental disorder worldwide that one in 44 kids aged 8 years in the United States suffers from it as reported in 2021 [15] and there is still a steady and substantial growth in the population.However, diagnosing ASD relies on subjective evaluations that are expensive and clinically demanding.Seminal works [3,6,12,16,17,19] have pointed out that eye movement patterns of people with ASD play an irreplaceable vital role in identifying ASD.Early efforts [7,13,18,20,21] usually focus on low-level behavior features combined with machine learning algorithms to identify autism, while in recent years, the eye-tracking data driven method [2,10,14] boosts the performance of ASD screening by utilizing deep neural networks (DNNs) which extract high-level semantic information of eye movement.However, existing deep-learning-based methods usually define an imageranking strategy as a pre-processing step to select a certain number of images.During training, each visual stimulus is treated equally, ignoring the distinct contributions of different stimuli. Besides, during the diagnosis procedure, a fixed number of images are shown to a subject which takes a relatively long time, thereby leading to poor cooperation of subjects, especially little kids.To tackle the above issues, in this paper, we propose a novel Uncertaintyinspired ASD Screening Network, named UASN, to distinguish the importance of each visual stimulus for different individuals. Despite the success of uncertainty in computer vision [1,5,22,23], to our best knowledge, this is the first attempt to introduce uncertainty estimation into ASD screening. Our uncertainty-inspired UASN can enforce the model learning from more distinctive gaze patterns during training. Meanwhile, when the model is deployed in the real clinical scenario, we further design an efficient personalized diagnosis strategy, which can dramatically reduce the diagnosis time without a performance drop.Specifically, the uncertainty in UASN works in two ways to ensure both higher accuracy and lower time consumption. On the one hand, given an input gaze pattern, we estimate the uncertainty by comparing the difference between the fixation map and the ones of ASD and TD groups. The uncertainty will be assigned a lower value for a larger disparity, suggesting the importance of the given gaze pattern for identifying a certain individual. Subsequently, guided by the estimated uncertainty, we design a truncated weighting loss to select the most distinctive gaze patterns and further dynamically adjust the contributions made by different stimuli, resulting in a more efficient classification. On the other hand, how to reduce the diagnosis time is also a key factor in real clinical applications, especially for preschool children. To achieve this goal, we propose a personalized diagnosis method that ranks the stimuli according to the estimated uncertainty. Instead of the random shuffle mode for image viewing, we recommend a top similar or dissimilar stimulus for the next viewing according to the decision of the previous gaze patterns. Following the proposed protocol, our method achieves state-of-the-art performance while spending much less diagnosis time.In general, our contributions can be summarized as follows: 1) we propose the first usage of the Uncertainty-inspired ASD Screening Network, named UASN, for identifying ASD people; 2) we estimate the uncertainty of each gaze pattern and further design a truncated weighting loss, which can enforce the model to dynamically adjust the contributions of different gaze patterns during training; 3) we design a personalized online diagnosis protocol that can dramatically reduce the diagnosis time without losing accuracy; 4) we conduct comprehensive experiments on the Saliency4ASD benchmark and achieve state-of-the-art performance only using 1/5 visual stimuli compared with other leading approaches."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.0,Uncertainty Inspired ASD Screening,"Our UASN is built upon traditional DNNs and consists of two novel stages: 1) uncertainty-guided training, and 2) uncertainty-guided personalized diagnosis.During training, we estimate an uncertainty value for each gaze pattern and further apply it for weighting the training loss. Besides, for a more simplified diagnosis procedure, we design an uncertainty-based strategy that adaptively selects the most discriminative images based on the subject's gaze behaviors."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.1,Uncertainty Guided Training,"Figure 1 illustrates the detailed training process of UASN. Firstly, we extract the features of gaze patterns by taking the temporal eye tracking information as input and resulting in the classification prediction. Then, we design an uncertainty estimation module to compute the uncertainty values of each subject on all the visual stimuli. Moreover, we further apply the estimated uncertainty to weight the training samples by a truncated loss in a reasonable manner.Gaze Pattern Feature Extraction. Formally, by collecting a group of ASD and TD subjects S = {s i } M i=1 's eye movement data on a set of images X = {x j } N j=1 , we get the corresponding scanpaths which comprise each fixation point's position and duration in the temporal order. The labels of the two clinical groups are denoted as Y = {y i } M i=1 ∈ {0, 1}. To generate the discriminative features of the given gaze pattern (i-th subject watching j-th image), we first feed the image x j into a ResNet-50 [9] with the last max pooling layer removed to learn a 2048-dimension visual feature. Then the visual feature sequence taken from the scanpath of i-th subject is fed into a Long Short Term Memory (LSTM) network [8]. A final hidden state is obtained at the end of the sequence which is then fed into a fully connected (FC) layer followed by a sigmoid function to get the prediction result for the i-th subject viewing j-th image, which is denoted as ŷj i . The network can be optimized by the binary cross-entropy (BCE) loss:where ŷj i and y j i denote the predicted and ground truth labels of i-th subject respectively. If belonging to ASD, y j i = 1 for all images {x j } N j=1 , otherwise 0. Uncertainty Estimation. We believe that different images contribute unequally to a subject's final classification due to the subject's unique preferences for viewing images. As a result, we estimate an uncertainty value for each gaze pattern based on the variance between its fixation map and the ones of the two clinical groups (i.e., ASD and TD). For instance, an ASD's fixation map on a discriminative image should appear more similar to the ASD group's averaged one than the TD group's so the variance is supposed to be large.We first generate the fixation map for each gaze pattern according to the fixation data. Then, given two groups' fixation maps on each image in the dataset, for each subject, we apply cosine similarity to compute an uncertainty measurement on each image. Let F j i denote the fixation map of the i-th subject's fixation map on the j-th image, and F j + , F j -denote the fixation maps of ASD and TD group for j-th image respectively. The uncertainty can be written as:where C is the cosine similarity function, D j i is the distinguishability of the j-th image when viewed by the i-th subject and µ j i denotes the uncertainty. Specifically, for images that do not contain certain subjects' eye-tracking data, we reasonably set the µ j i to be large because we assume that the absence of eye-tracking data is due to a lack of interest, and further signals ineffectiveness.Truncated Weighting Loss. Upon obtaining the uncertainty value µ j i , we can utilize the uncertainty to re-weight the training loss by teaching the model which images to trust and which to discredit. We hope that the larger the µ is, the less the image contributes to the final classification, so the corresponding loss needs to be reduced correspondingly.However, for some gaze patterns that are confusing and much more difficult to distinguish between ASD and healthy people, it is more suitable to discard those unreliable patterns. Considering this, we finally propose a truncated weighting BCE loss for training. Specifically, when the estimated uncertainty value is larger than a pre-defined threshold t, we set the corresponding loss to zero. In summary, The final loss function is denoted as:where I is the indicator function and t is the threshold. Only when the condition of [µ ≤ t] is met, is the value of the indicator function set to 1, and the L tr _bce remains. Specifically, for a more reasonable computation, we do not simply set the t to be a fixed value. Instead, we determine t with an adaptive technique.For each subject, we sort the uncertainty values from low to high and choose the 1/3 of the images with the lowest uncertainty and retain the contributions they make to the prediction while discarding the remainder."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.2,Uncertainty Guided Personalized Diagnosis,"On the basis of training our model in an uncertainty-guided way, we are encouraged to go deeper to simplify the diagnostic procedure. To this end, we incorporate personalized diagnosis into our work, taking into account the gaze behavior features of each subject. Figure 2 presents the workflow of our personalized diagnosis protocol after completing the training process in Fig. 1. Specifically, by extracting features of images and forming a feature bank, we selectively choose the most suitable images to update the viewing list according to the subject's viewing pattern. Image Feature Extraction. First, we assume that the visual similarity between images brings the potential of personalized diagnosis that similar images may contribute similarly in distinguishing an individual. From this point, we extract the feature of all the 300 images in the dataset (the Image set in Fig. 2) using a ResNet-18 [9] followed by an FC to get a 128-dimension feature, thereby forming a feature bank getting prepared for the further dynamical ranking procedure.Similarity-Based Image Ranking. Then, we build a viewing list simulating the diagnosis procedure where images are shown to a subject one by one and the list is updated in real-time. In the beginning, we select an image from the image set randomly to initialize the viewing list. When a trial is completed, the viewing list is subsequently updated. In each trial, we generate an average feature of all images in the viewing list. We then compute the cosine similarity between the feature of the current image and images in the feature bank to obtain a similarity list. We sort the list in similarity-ascending order.Uncertainty-Based Viewing List Updating. To determine which images should be included in the viewing list, a method based on uncertainty is developed. First, we feed the image in the list and the corresponding eye-tracking data into the ASD screening network which is composed of the Uncertainty Estimation module and the Gaze Pattern Feature Extraction module in Fig. 1 to get the uncertainty and the prediction result. By pre-defining a threshold p, we separate the scenario into a positive case and a negative one. When the average uncertainty value is larger than p, we consider it negative so we select the top K dissimilar images to join the viewing list and vice versa. After T trials, we achieve a relatively confident prediction."
Uncertainty Inspired Autism Spectrum Disorder Screening,3.1,Dataset and Experimental Settings,"Dataset. So far, only Saliency4ASD [4] is publicly released for the evaluation of ASD screening. It consists of 300 images from a public dataset collected by Judd et al . [11] and the eye movement data collected from 14 kids with ASD and 14 with TD. For each image, the scanpath of each subject is provided, allowing us to derive the single fixation map for additional uncertainty computation.Evaluation Protocol. We employ the leave-one-subject-out cross-validation method for evaluating the model's performance. Specifically, for the Saliency4ASD dataset, we perform a 28-round validation with each round selecting only one subject for testing while the remaining 27 subjects work as the training data.Metrics. We follow the previous works [2,10] to adopt the accuracy, sensitivity, specificity, and AUC to evaluate the performance of the prediction for each subject. Besides, we assess the performance of the prediction for each scanpath to generate a more strict measurement. We still adopt accuracy, sensitivity, and specificity, called Acc_I, Sen_I, and Spe_I. Accordingly, for the previously used three metrics, we denote them as Acc_S, Sen_S, and Spe_S.Implementation Details. The experimental setting mostly follows the [2] during training. Besides, we manually set the uncertainty values of those images without some certain subjects' eye movement data to be 1-10 -5 which is a relatively large margin that rarely contributes to the classification. For personalized diagnosis, we initialize the viewing list by randomly selecting an image. By considering the gaze pattern with an uncertainty level lower than p = 0.9 a positive case, we choose the top K most similar images to update the viewing list. We set K to be 1 and update the viewing list T = 20 trials in total."
Uncertainty Inspired Autism Spectrum Disorder Screening,3.2,Comparison with State-of-the-Art,"We conduct extensive experiments to explore whether our proposed model outperforms the state-of-the-art. We compare our UASN with [2] which selects a fixed 100 images subset out of a total of 300 images according to a Fisher-Scorebased image selection strategy for training and testing. We design the following three UASN variants: 1) UASN-noUnLoss only uses uncertainty to choose the top most discriminative images and removes the weighting loss procedure for training; 2) UASN-Fixed selects a 100-image subset with the lowest uncertainty for each subject during training and testing without dynamically adjusting the diagnosis process; 3) UASN-Dynamic uses 100 images with low uncertainty for training. In the diagnosis process, we adopt the proposed uncertainty-guided personalized diagnosis to recommend a small number of images, which significantly reduces the diagnosis time while maintaining high accuracy. Table 1 shows the results. We can see that our model's three variants all outperform the baseline model by all evaluation metrics. The result demonstrates that introducing uncertainty during training can reach 100% accuracy. When removing the uncertainty weighting loss part, the UASN-noUnLoss model's performance drops slightly in scanpath level metrics than the best UASN-Dynamic, i.e., 3% (Acc_I), 3% (Sen_I) and 6% (Spe_I). Besides, the personalized diagnosis strategy achieves the same accuracy but largely reduces the number of images to 1/5 (20 images), which decreases the diagnosis time dramatically. In terms of scanpath level metrics, our UASN-Dynamic outperforms the previous leading model [2] by 20% in Acc_I and Sen_I, and 19% in Spe_I. The results suggest introducing uncertainty both in the training and testing stage achieves the best performance with a quite small image subset at classifying the two clinical groups."
Uncertainty Inspired Autism Spectrum Disorder Screening,3.3,Ablation Study,"Effect of the Uncertainty Estimation. To verify the effect of uncertainty estimation, we divide the 300 images into three non-overlapping subsets based on the ascending order of uncertainty level, denoted as top-100, middle-100, and bottom-100 subsets. Table 2 shows the results. It is not surprising that UASN-Dynamic with top-100 achieves the best performance and the large performance drop on both UASN-Fixed and UASN-Dynamic approves our model's strong capability of selecting the most discriminative images for classifying ASD and TD. We further visualize the relation between Acc_I and uncertainty, as well as give some samples of ASD and TD's fixation maps with different uncertainty values to support the effectiveness of our method. Details are given in Fig. I  Effect of Different Inference Strategies. During the diagnosis process, we need to define the number of trials (T ) and the number of images (K) selected to append to the viewing list. The results are given in Table 3. We tried various permutations of T and K and finally found that appending one image each trial "
Uncertainty Inspired Autism Spectrum Disorder Screening,4.0,Conclusion,"In this paper, we present UASN, a novel ASD screening approach, inspired by uncertainty. The uncertainty benefits the ASD diagnosis in two ways: a weighted truncated training loss that enables the model to learn the most discriminative and effective features of gaze patterns and a personalized procedure that dynamically ranks the stimuli according to the subject's gaze behaviors. Comprehensive results show superior performance in classifying ASD people."
Uncertainty Inspired Autism Spectrum Disorder Screening,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_39.
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,1.0,Introduction,"Screening mammography helps detect breast cancer earlier and has reduced the breast cancer mortality rate significantly [4]. Computer-aided diagnosis (CAD) software was developed to aid radiologists, but its effectiveness has been questioned following recent large-scale clinical studies [6]. In particular, the high  [9]) to few negative cases (DDSM [8], INBreast [17]). To illustrate the distribution shift, we train four popular dense detectors using a standard setup that includes only annotated malignant and benign cases [1,13,16]. We utilize OPTIMAM [7], a large dataset with a significant proportion of negatives (Table 1), for training and evaluation. Across all dense models, there is a large performance drop in the clinically representative setting that includes negative images. This means that the dense models are producing too many FPs on negative images. Our model, M&M, successfully tackles this performance gap.rate of false positive (FP) predictions of CAD can cause a significant reduction in radiologists' specificity [6]. Surprisingly, recent deep learning literature [3,5,13,16,20,21,32] focuses on improving recall without considering the need to operate at low FP rates. As shown in Fig. 1a, most works focus on reporting recalls outside the clinically relevant region of less than 1 FP/image. To tackle the high rate of false positives in mammography, we identify three challenges: (1) A malignant mammogram typically contains only one malignant finding. This is different from natural images: for example, an image in COCO contains on average 7.7 objects [11]. This calls into question the usage of dense detectors for mammography; (2) A standard screening exam consists of two views per breast. Both views are essential in making a clinical decision because a finding may appear suspicious in one view but not the other; (3) Most mammograms are negative: they do not contain any findings. However, excluding negative images from training and evaluation leads to a distribution shift since negative images are abundant in clinical practice. Concretely, the false positive rate is low for a typical evaluation data distribution but much higher for a clinicallyrepresentative data distribution, as shown in Fig. 1b.In this work, we tackle these challenges and propose a Multi-view and Multiinstance learning system, M&M. M&M is an end-to-end system that detects malignant findings and provides breast-level classification. To achieve these goals, M&M leverages three components: (1) Sparse R-CNN to replace dense anchors with a set of sparse proposals; (2) Multi-view cross-attention to synthesize information from two views and iteratively refine the predictions, and (3) Multiinstance learning (MIL) to include negative images during training. Ultimately, each component contributes to our goal of reducing false positives.We validate M&M through evaluation on five datasets: two in-house datasets, two public datasets -DDSM [8] and CBIS-DDSM [9], and OPTIMAM [7]. We perform ablation studies to verify the contribution of each component of M&M. To summarize, our contributions are:1. We show that sparsity of proposals is beneficial to the analysis of mammograms, which have low disease prevalence (Sec. With MIL, M&M improves the recall at 0.1 FP/image by 12.6% (Fig. 4). Furthermore, M&M can provide breast-level classification predictions, achieving AUCs of more than 0.88 on four different datasets (Table 3)."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.1,Sparse R-CNN with Dual Classification Heads,"The sparsity of malignant findings calls into question the use of dense detectors.As shown in Fig. 1b, dense detectors generalize poorly to negative images as they produce too many false positives. Thus, we propose to use Sparse R-CNN [24]. Sparse R-CNN utilizes a sparse set of N learnable proposals consisting of b 0 ∈ R N ×4 coordinates and h 0 ∈ R N ×D features. The architecture uses 6 cascading heads to iteratively refine the proposals. Within the i th head, the proposals h i-1 first interact with themselves via self-attention, and then generate DynamicConv (Fig. 4, [24]) to interact with RoI features cropped by b i-1 . The resulting outputs h i ∈ R N ×D are features for the (i+1) th head. In addition, a regression module is applied to h i to generate boxes b i ∈ R N ×4 , and a classification module generates scores p i ∈ R N ×C , with C being the number of classes.We modify Sparse R-CNN to include dual classification modules (Fig. 2). First, an objectness module produces objectness logits o i ∈ R N to distinguish all findings -malignant and benign -from the background. By utilizing all findings, the objectness head increases the training sample size [1,13,16], but also increases FPs because it flags benign findings. To mitigate this side effect, we include a dedicated malignancy module [W i , b i ] to generate malignancy logits m i ∈ R N that is trained to distinguish malignant from benign findings:(1) The strictly positive function SoftPlus(x) = log(1 + e x ) is chosen to enforce consistency: a high objectness logit o i is required to generate a high malignancy logit m i . Thus, at the finding level, we obtain the following losswhere L giou and L L1 are regression losses as in Sparse R-CNN. L objectness and L malignancy are focal losses applied to the predicted objectness o i and the predicted malignancy m i across all cascading heads 1 ≤ i ≤ 6, respectively."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.2,Multi-view Reasoning,"A standard screening exam includes two standard views of each breast. The craniocaudal (CC) view is taken from the top down, while the mediolateral oblique (MLO) view is captured from the side at an oblique angle. Radiologists examine both views when making a clinical decision as a finding may look innocuous in one view but suspicious in the other.To enable multi-view reasoning, M&M incorporates a cross-attention module [28] into every cascading head. Recall that within the i th cascading head, selfattention is first applied to proposal features h i-1 to reason about the relations between objects. After this self-attention module, we introduce a cross-attention module (Fig. 2, Appendix Algo. 1) to reason about the relations between CC view feature h CC i-1 and MLO view feature h MLO i-1 :The enhanced embeddings hCC i-1 , hMLO i-1 then generate DynamicConv to interact with RoI features and produce new features h CC i , h MLO i for the (i + 1) th head. Thus, with the proposed cross-attention module, the CC view's proposal features are refined iteratively using the MLO view's proposal features and vice versa."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.3,Multi-instance Learning,"Mammogram annotation is costly to obtain due to a dependency on radiologists. This high cost means that bounding boxes are often unavailable. Further, most mammograms are negative: they do not contain any findings. Yet, a model generalizes poorly if these negative images are dropped during training (Fig. 1b).Since image-and breast-level labels are available, we adopt an MIL module to include images without bounding boxes during training. To compute imageand breast-level scores, we leverage the proposal malignancy logits m i (Eq. ( 1)). Since an image is malignant if it contains a malignant lesion, we obtain imagelevel scores by applying the NoisyOR function fNext, as CC and MLO views offer complimentary information on a breast, we obtain breast-level malignancy score by averaging the image-level scores across these views.We apply cross-entropy losses L image and L breast at the image and breast level for all training samples. The lesion loss L lesion (Eq. ( 2)) is only applied for annotated lesions. We thus obtain the following total training loss for M&M: L = 1 annotated lesion L lesion + 0.5L image + 0.5L breast .(5)"
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,3.0,Experiments,"Implementation Details. We use PyTorch 1.10. The training settings follow Sparse R-CNN [24]. We apply random horizontal flipping and random rotation. We resize the images' shorter edges to 2560 with the larger edges no longer than 3328. We utilize a COCO-pretrained PVT-B2-Li backbone [30]. We use AdamW optimizer with 5 × 10 -5 learning rate and 0.0001 weight decay. The model is trained for 9000 iterations, and the learning rate is scaled by 0.1 at the 6750 and 8250 iterations. Each batch contains 16 breasts (32 images). We employ a 1:1 sampling ratio between unannotated and annotated images.Datasets. We utilize three 2D digital mammography datasets: (1) OPTIMAM : a development dataset derived from the OPTIMAM database [7], which is funded by Cancer Research UK. We split the data into train/val/test with an 80:10:10 ratio at the patient level; (2) Inhouse-A: an evaluation dataset collected from a U.S. multi-site mammography operator; (3) Inhouse-B : an evaluation dataset collected from a U.S. academic hospital (see [18], Sec. 2.2 for more details on the inhouse datasets). We also utilize two film mammography datasets: (4) DDSM: a dataset maintained at the University of South Florida [8]. We followed the methods by [3,5,13,16] to split the test set; (5) CBIS-DDSM: a curated subset of DDSM [9]. We only include breasts that have one CC view and one MLO view. Dataset statistics are reported in Table 1.Metrics. We report average precision with Intersection over Union from 0.25 to 0.75. AP mb denotes average precision on the set of annotated malignant and benign images. AP denotes average precision when all data is included. We report free response operating characteristic (FROC) curves and recalls at various FP/image (R@t). Following [3,5,16,29], a proposal is considered true positive if its center lies within the ground truth box. For classification, we report the area under the receiver operating characteristic curve (AUC).Detection Results.  GMIC [23] 0.911 0.896 0.814 0.815 0.796 HCT [25] 0.923 0.912 0.816 0.817 0.793 M&M (ours) 0.960 0.942 0.920 0.910 0.898ResNet50 [14] 0.724 Shared ResNet [31] 0.735 PHResNet50 [14] 0.739 Cross-view Transformer [27] 0.803 * M&M (ours) 0.88323 points (pt) between excluding and including negative images. Large Δ means the models are producing too many FPs on negative images. Sparse R-CNN [24] generalizes significantly better with a gap of 17pt. This shows the importance of sparsity for reducing FP. By adding both multi-view and MIL, M&M successfully reduces the Δ gap to 3.5pt. With this performance gap closed, M&M is able to achieve a high recall of 87.7% at just 0.1 FP/image. Figure 1a compares M&M with recent literature evaluated on DDSM. M&M adopts the same DDSM splits used by [3,12,13,16,33], while [5,21,32] use other splits. M&M (87% R@0.5) outperforms all recent SOTA with the same test split, including 2022 SOTA [33] (83% R@0.5), by at least 4%.Classification Results. Table 3a reports M&M's breast-level and exam-level classification results on OPTIMAM and the two inhouse datasets. We use GMIC [23] and HCT [25] as baselines since they are open-sourced classifiers developed for mammography. All three models were trained only on OPTIMAM. For all models, the breast-level score is the average of the CC score and MLO score, while the exam-level score is the max of the left breast score and right breast score. Both baseline models suffer large generalization drops of approximately  3b compares M&M with recent literature reporting on the public CBIS-DDSM dataset. In particular, M&M outperforms the cross-view transformer [27] and PHResNet50 [14] by 0.08 and 0.14 breast AUC, respectively. Qualitative Evaluation. Figure 3 presents a qualitative evaluation of the multi-view module. With multi-view, M&M produces a tighter box on the CC view and recovers a missed finding on the MLO view. Ablation Studies. Figure 4 presents ablation results using the OPTIMAM validation split. On the left, we demonstrate how each component of M&M contributes to closing the gap Δ between evaluating with and without negative images. Notably, without using any extra training samples, multi-view reasoning reduces Δ to only -5.9pt (Row 3). MIL allows the model to train with significantly more negative images, reducing Δ to -3.6pt (Row 4). On the right of Fig. 4, the FROC curves show how each component of M&M improves recall significantly at low FP/image. In particular, M&M's recall at 0.1FP/image is 86.3%, +21.2% over vanilla Sparse R-CNN.Further studies. In the appendix, we present more qualitative evaluation as well as further ablation studies on (1) number of learnable proposals, (2) different MIL schemes, (3) backbone choices and (4) positional encoding."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,4.0,Discussion and Conclusion,"We present M&M, an end-to-end model leveraging multi-view reasoning and multi-instance learning for mammography detection and classification.As a detector, M&M offers significant improvement in recall at low FP/image (Fig. 1a, Table 2). This success comes from three points of advancement. First, unlike previous works that do not consider the impact of sparsity [13,16,33], we show that sparsity of proposals is beneficial for false positive reduction (Table 2).Second, M&M incorporates multi-view reasoning through iterative application of cross-attention and proposal refinement in the cascading heads. M&M's multiview module is effective (Fig. 4) yet simple, requiring neither positional encoding [13,16,32] nor extra proposal correspondence annotations [33]. Finally, our MIL formulation allows for training with representative data distribution in an endto-end one stage pipeline. This is more advantageous than previous pipelines that require additional stages or classifiers to reduce false positives [15,22,29].As a classifier, M&M establishes strong performance on several datasets (Table 3). M&M offers two advantages over image classifiers: (1) Image classifiers are often pre-trained as patch classifiers with patches cropped from bounding box annotations [14,23,25]. In comparison, M&M utilizes these bounding boxes to learn localization and can be trained directly in a single stage from COCO/ImageNet weights; (2) Image classifiers offer limited explainability, while M&M's breast-level prediction is more interpretable through its localization ability."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,,Table 2,"presents quantitative detection evaluation on OPTIMAM. All dense detectors[2,10,19,26] suffer a large Δ gap of more than"
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_75.
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,1.0,Introduction,"The early detection of lesions in medical images is critical for the diagnosis and treatment of various conditions, including neurological disorders. Stroke is a leading cause of death and disability, where early detection and treatment can significantly improve patient outcomes. However, the quantification of lesion burden is challenging and can be time-consuming and subjective when performed manually by medical professionals [14]. While supervised learning methods [10,11] have proven to be effective in lesion segmentation, they rely heavily on large Fig. 1. Overview of PHANES (see Fig. 2). Our method can use both expert annotatedor unsupervised generated masks to reverse and segment anomalies annotated datasets for training and tend to generalize poorly beyond the learned labels [21]. On the other hand, unsupervised methods focus on detecting patterns that significantly deviate from the norm by training only on normal data.One widely used category of unsupervised methods is latent restoration methods. They involve autoencoders (AEs) that learn low-dimensional representations of data and detect anomalies through inaccurate reconstructions of abnormal samples [17]. However, developing compact and comprehensive representations of the healthy distribution is challenging [1], as recent studies suggest AEs perform better reconstructions on out-of-distribution (OOD) samples than on training samples [23]. Various techniques have been introduced to enhance representation learning, including discretizing the latent space [15], disentangling compounding factors [2], and variational autoencoders (VAEs) that introduce a prior into the latent distribution [26,29]. However, methods that can enforce the reconstruction of healthy generally tend to produce blurry reconstructions.In contrast, generative adversarial networks (GANs) [8,18,24] are capable of producing high-resolution images. New adversarial AEs combine VAEs' latent representations with GANs' generative abilities, achieving SOTA results in image generation and outlier detection [1,5,6,19]. Nevertheless, latent methods still face difficulties in accurately reconstructing data from their low-dimensional representations, causing false positive detections on healthy tissues.Several techniques have been proposed that make use of the inherent spatial information in the data rather than relying on constrained latent representations [12,25,30]. These methods are often trained on a pretext task, such as recovering masked input content [30]. De-noising AEs [12] are trained to eliminate synthetic noise patterns, utilizing skip connections to preserve the spatial information and achieve SOTA brain tumor segmentation. However, they heavily rely on a learned noise model and may miss anomalies that deviate from the noise distribution [1]. More recently, diffusion models [9] apply a more complex de-noising process to detect anomalies [25]. However, the choice and granularity of the applied noise is crucial for breaking the structure of anomalies [25]. Adapting the noise distribution to the diversity and heterogeneity of pathology is inherently difficult, and even if achieved, the noising process disrupts the structure of both healthy and anomalous regions throughout the entire image.In related computer vision areas, such as industrial inspection [3], the topperforming methods do not focus on reversing anomalies, but rather on detecting them by using large nominal banks [7,20], or pre-trained features from large natural imaging datasets like ImageNet [4,22]. Salehi et al. [22] have employed multi-scale knowledge distillation to detect anomalies in industrial and medical imaging. However, the application of these networks in medical anomaly segmentation, particularly in brain MRI, is limited by various challenges specific to the medical imaging domain. They include the variability and complexity of normal data, subtlety of anomalies, limited size of datasets, and domain shifts.This work aims to combine the advantages of constrained latent restoration for understanding healthy data distribution with generative in-painting networks. Unlike previous methods, our approach does not rely on a learned noise model, but instead creates masks of probable anomalies using latent restoration. These guide generative in-painting networks to reverse anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting in anomalous regions. We believe that our proposed method will open new avenues for interpretable, fast, and accurate anomaly segmentation and support various clinical-oriented downstream tasks, such as investigating progression of disease, patient stratification and treatment planning. In summary our main contributions are:• We investigate and measure the ability of SOTA methods to reverse synthetic anomalies on real brain T1w MRI data. • We propose a novel unsupervised segmentation framework, that we call PHANES, that is able to preserve healthy regions and utilize them to generate pseudo-healthy reconstructions on anomalous regions. • We demonstrate a significant advancement in the challenging task of unsupervised ischemic stroke lesion segmentation."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,2.0,Background,"Latent restoration methods use neural networks to estimate the parameters θ, φ of an encoder E θ and a decoder D φ . The aim is to restore the input from its lower-dimensional latent representation with minimal loss. The standard objective is to minimize the residual, e.g., using mean squared error (MSE) loss:In the context of variational inference [13], the goal is to optimize the parameters θ of a latent variable model p θ (x) by maximizing the log-likelihood of the observed samples x: log p θ (x). The term is intractable, but the true posterior p θ (z|x) can be approximated by q φ (z|x):(KL is the Kullback-Leibler divergence; q φ (z|x) and p θ (x|z) are usually known as the encoder E φ and decoder D θ ; the prior p(z) is usually the normal distribution N (μ 0 , σ 0 ); and the ELBO denotes the Evidence Lower Bound. In unsupervised anomaly detection, the networks are trained only on normal samples x ∈ X ⊂ R N . Given an anomalous input x / ∈ X , it is assumed that the reconstruction x ph = (D φ (E θ (x))) ∈ X represents its pseudo-healthy version. The aim of the pseudo-healthy reconstructions is to accurately reverse abnormalities present in the input images. This is achieved by preserving the healthy regions while generating healthy-like tissues in anomalous regions. Thus, anomalies can ideally be directly localized by computing the difference between the anomalous input and the pseudo-healthy reconstructions: s(x) = |x -x ph |."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,3.0,Method,"Figure 2 shows an overview of our proposed method. We introduce an innovative approach by utilizing masks produced by latent generative networks to condition generative inpainting networks only on healthy tissues. Our framework is modular, which allows for the flexibility of choosing a preferred generative network, such as adversarial, or diffusion-based models for predicting the pseudo-healthy reconstructions. In the following we describe each component in detail. Latent Generative Network. The first step involves generating masks to cover potential anomalous regions in the input image. The goal of this step is to achieve unbiased detection of various pathologies and minimize false positives. It is therefore important to use a method that is restricted to in-distribution samples, particularly healthy samples, while also accurately reconstructing inputs. Here, we have adopted our previous work [1] that augments a soft introspective variational auto-encoder with a reversed embedding similarity loss with the aim to enforcing more accurate pseudo-healthy reconstructions. The training process encourages the encoder to distinguish between real and generated samples by minimizing the Kullback-Leibler (KL) divergence of the latent distribution of real samples and the prior, and maximizing the KL divergence of generated samples. On the other hand, the decoder is trained to deceive the encoder by reconstructing real data samples using the standard ELBO and minimizing the KL divergence of generated samples compressed by the encoder:where E l φ is the l-th embedding of the L encoder layers,, and L Sim is the cosine similarity. Mask Generation. Simple residual errors have a strong dependence on the underlying intensities [16]. As it is important to assign higher values to (subtle) pathological structures, we compute anomaly masks as proposed in [1] by applying adaptive histogram equalization (eq), normalizing with the 95th percentile, and augmenting the errors with perceptual differences for robustness:with S lpips being the learned perceptual image patch similarity [28]. Finally, we binarize the masks using the 99th percentile value on the healthy validation set.Inpainting Generative Network. The objective of the refined PH generative network is to complete the masked image by utilizing the remaining healthy tissues to generate a full PH version of the input. Considering computational efficiency, we have employed the recent in-painting AOT-GAN [27]. The method uses a generator (G) and discriminator neural network to optimize losses based on residual and perceptual differences, resulting in accurate and visually precise inpainted images. Additionally, the discriminator predicts the input mask from the inpainted image to improve the synthesis of fine textures. Anomaly Maps. The PH reconstruction is computed as follows:) m, with being the pixel-wise multiplication. We compute the final anomaly maps based on residual and perceptual differences:"
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.0,Experiments,"Datasets. We trained our model using two publicly available brain T1w MRI datasets, including FastMRI+ (131 train, 15 val, 30 test) and IXI (581 train samples), to capture the healthy distribution. Performance evaluation was done on a large stroke T1-weighted MRI dataset, ATLAS v2.0 [14], containing 655 images  with manually segmented lesion masks for training and 355 test images with hidden lesion masks. We evaluated the model using the 655 training images with public annotations. The mid axial slices were normalized to the 98 th percentile, padded, and resized to 128 × 128 resolution. During training, we performed random rotations up to 10 • , translations up to 0.1, scaling from 0.9 to 1.1, and horizontal flips with a 0.5 probability. We trained for 1500 epochs, with a batch size of 8, lr of 5e -5 , and early stopping."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.1,Reversing Synthetic Anomalies,"In this section, we test whether reconstruction-based methods can generate pseudo-healthy images and reverse synthetic anomalies. Results are evaluated in Table 1 and Fig. 3 using 30 test images and synthetic masks as reference.VAEs produce blurry results that lead to poor reconstruction of both healthy and anomalous regions (LPIPS) and thus poor segmentation performance. While DAEs preserve the healthy tissues well with an LPIPS of 3.94, they do not gen- erate pseudo-healthy reconstructions in anomalous regions (LPIPS ≈ 20). However, they change the intensity of some structures, e.g., hypo-intensities, allowing for improved detection accuracy (see AUPRC and Dice). Simplex noise in [25] is designed to detect large hypo-intense lesions, leaving small anomalies undetected by AnoDDPM. SI-VAEs and RA produce pseudo healthy versions of the abnormal inputs, with the latter achieving the best results among the baselines. Our proposed method, PHANES, successfully reverses the synthetic anomalies, with its reconstructions being the most similar to ground truth healthy samples, as can be seen in Fig. 3. It achieved an improvement of 77% and 47% in generating pseudo-healthy samples in healthy and anomalous regions, respectfully. This enables the precised localization of anomalies (see bottom row in Fig. 3)."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.2,Ischemic Stroke Lesion Segmentation on T1w Brain MRI,"In this section, we evaluate the performance of our approach in segmenting stroke lesions and show the results in Table 2 and Fig. 4. For completeness, we compare our approach to teacher-student methods that use multi-scale knowledge distillation (MKD) for anomaly segmentation. The unsupervised detection of (subtle) stroke lesions is challenging. The lack of healthy data from the same scanner and the limited size of the healthy datasets limit the successful application of such methods, with a maximum achievable Dice score of just under 6%. On the other hand, PatchCore, which is currently the SOTA method in industrial anomaly detection, has demonstrated comparable performance to the top-performing baselines. VAEs yield many false positive detections due to the blurry reconstructions and achieve poor localization results. DAEs can identify anomalies that resemble the learned noise distribution and improve segmentation results (AUPRC of 9.22), despite not producing pseudo-healthy reconstructions of abnormal samples (see Subsect. 4.1). The best performing latent restoration method is RA, achieving a remarkable 79% improvement over SI-VAE. Unlike experiments in Subsect. 4.1, the Simplex noise aligns more closely with the hypointense pathology distribution of stroke in T1w brain MRI. As a result, AnoD-DPM achieves the highest detection accuracy among the baselines. Compared to AnoDDPM, PHANES increases the detection results by 22% AUPRC. Figure 4 shows a visual comparison between the two approaches. Diffusion models tend to be more susceptible to domain shifts (first three rows) and yield more false positives. In contrast, PHANES demonstrates more precise localization, especially for subtle anomalies (bottom rows). Generally, unsupervised methods tend to have lower Dice scores partly due to unlabeled artefacts in the dataset. These include non-pathological (rows 1,2) as well as other pathological effects, such as changes in ventricle structure (rows 3,4). PHANES correctly identifies these as anomalous, but their lack of annotation limits numerical evaluations."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,5.0,Discussion,"This paper presents a novel unsupervised anomaly segmentation framework, called PHANES. It possesses the ability to reverse anomalies in medical images by preserving healthy tissues and substituting anomalous regions with pseudohealthy reconstructions. By generating pseudo-healthy versions of images containing anomalies, PHANES can be a useful tool in supporting clinical studies. While we are encouraged by these achievements, we also recognize certain limitations and areas for improvement. For example, the current binarization of anomaly maps does not account for the inherent uncertainty in the maps, which we aim to explore in future research. Additionally, our method relies on accurate initial estimates of the latent restoration and anomaly maps. Nevertheless, our proposed concept is independent of specific approaches and can leverage advancements in both domains. Our method is not optimized for detecting a certain anomaly distribution but rather demonstrates robustness in handling various small synthetic anomalies and diverse stroke lesions. We look forward to generalizing our method to other anatomies and imaging modalities, paving the way for exciting future research in the field of anomaly detection.In conclusion, we demonstrated exceptional performance in reversing synthetic anomalies and segmenting stroke lesions on brain T1w MRIs. We believe that deliberate masking of (possible) abnormal regions will pave new ways for novel anomaly segmentation methods and empower further clinical applications."
DiffULD: Diffusive Universal Lesion Detection,1.0,Introduction,"Universal Lesion Detection (ULD) in computed tomography (CT) [3,13,14,16,20,[28][29][30][33][34][35][36][37]40,44] plays an important role in computer-aided diagnosis (CAD) [42,43]. The design of detection-only instead of identifying the lesion types in ULD [1,6,17,19,22,24,39,41] prominently decreases the difficulty of this task for a specific organ (e.g., lung, liver), but it is still challenging for lesions vary in shapes and sizes among whole human body.Previous arts in ULD are mainly motivated by the anchor-based detection framework, e.g., Faster-RCNN [21]. These studies focus on adapting the detection backbone to universally locate lesions in CT scans. For instance, Li et al. [16] propose the so-called MVP-Net, a multi-view FPN with a position-aware attention mechanism to assist ULD training. Yang et al. [36][37][38] propose a series of 3D feature fusion operators to incorporate context information from several adjacent CT slices for better performance. Li et al. [15] introduce a plug-and-play transformer block to form hybrid backbones which can better model long-distance feature dependency. While achieving success, these anchor-based methods have inherent drawbacks: (i) Insufficient training target. In stage-1, anchor-based methods identify the positive (lesion) anchors and label them as the region of interest (RoI) based on the IoU between anchors and ground-truth (GT) bounding boxes (BBoxes). An anchor is considered positive if its IoU with any GT BBox is greater than the IoU threshold and negative otherwise [21]. The positive anchors are sufficient in natural images as they usually have many targets per image [13]. However, the number of lesions per CT scan is limited, most CT slices only contain one or two lesions (i.e., detection targets in ULD) per CT slice [14]. Still applying the IoU-based anchor matching mechanism with such limited targets can lead to severe data imbalance and further hinders network convergence. Simply lowering the positive IoU threshold in the anchor-selecting mechanism can alleviate the shortage of positive anchors to some degree, but it leads to a higher false positive (FP) rate by labeling more low-IoU anchors as positive. (ii) Difficulties in anchor design. In anchor-based methods, the size, ratio and number of anchors are pre-defined hyper-parameters that significantly influence the detection performance [26]. Thus a proper design of anchor hyper-parameters is of great importance. However, tuning anchor hyper-parameters is a challenging task in ULD because of the variety of lesions (target) with diverse diameters (from 0.21 to 342.5 mm). Even with a careful design, the fixed rectangle anchor boxes can be a kind of obstruction in capturing heterogeneous lesions, which have irregular shapes and vary in size.To get rid of the drawbacks caused by the anchor mechanism, researchers resort to anchor-free detection, e.g., FCOS [31] and DETR [5]. But these methods experience difficulties in achieving state-of-the-art results in ULD, as they lack the initialization of position prior provided by anchors.Recently, the diffusion probabilistic model (DPM) [2,8,10,11,23] has demonstrated its outstanding capabilities in various vision tasks. Chen et al. follow the key idea of DPM and propose a noise-to-box pipeline, DiffusionDet [7], for natural image object detection. They achieved success on natural images with sufficient training targets, but still experience difficulties in dealing with tasks with insufficient training targets like ULD. This is because the DPM's denoising is a dense distribution-to-distribution forecasting procedure that heavily relied on a large number of high-quality training targets to learn targets' distribution accurately.To address this issue, we hereby introduce a novel center-aligned BBox padding strategy in DPM detection to form a diffusion-based detector for Universal Lesion Detection, termed DiffULD.As shown in Fig. 1, DiffULD also formulates lesion detection as a denoising diffusion process from noisy boxes to prediction boxes similar to [7], but we further introduce the center-aligned BBox padding before DiffULD's forward diffusion process to generate perturbated GT. Specifically, we add random perturbations to both scales and center coordinates of the original GT BBox, resulting in perturbated boxes whose centers remain aligned with the corresponding original GT BBox. Next, original GT boxes paired with these perturbated boxes are called perturbated GT boxes for simplicity. Finally, we feed the perturbated GT boxes to the model as the training objective during training. Compared with other training target padding methods (e.g., padding with random boxes), our strategy can provide additional targets of higher quality, i.e., center aligned with the original GT BBox. This approach effectively expands the insufficient training targets on CT scans, enhancing DPM's detection performance and avoiding deterioration triggered by adding random targets.The following DPM training procedure contains two diffusion processes. i) In the forward training process, DiffULD corrupts the perturbated GT with Gaussian noise gradually to generate noisy boxes step by step. Then the model is trained to remove the noise and reconstruct the original perturbated GT boxes. ii) In the reverse inference process, the trained DiffULD can refine a set of randomly generated boxes iteratively to obtain the final detect predictions.Our method gets rid of the drawbacks of pre-defined anchors and the deterioration of training DPM with insufficient GT targets. Besides, DiffULD is inherently advanced in locating targets with diverse sizes since it can predict with arbitrary boxes, which is an advantageous feature for detecting lesions of irregular shapes and various sizes. To validate the effectiveness of our method, we conduct experiments against seven state-of-the-art ULD methods on the public dataset DeepLesion [32]. The results demonstrate that our method achieves competitive performance compared to state-of-the-art ULD approaches."
DiffULD: Diffusive Universal Lesion Detection,2.0,Method,"In this section, we first formulate our overall detection process for DiffULD and then specify the training manner, inference process and backbone design."
DiffULD: Diffusive Universal Lesion Detection,2.1,Diffusion-Based Detector for Lesion Detection,"Universal Lesion Detection can be formulated as locating lesions in input CT scan I ct with a set of boxes predictions z 0 . For a particular box z , it can be denoted as z = [x 1 , y 1 , x 2 , y 2 ], where x 1 , y 1 and x 2 , y 2 are the coordinates of the top-left and bottom-right corners, respectively.We design our model based on a diffusion model mentioned in [7]. As shown in Fig. 2, our method consists of two stages, a forward diffusion (or training) process and a reverse refinement (or inference) process. In the forward process, We denote GT BBoxes as z 0 and generate corrupted training samples z 1 , z 2 , ..., z T for latter DiffULD training by adding Gaussian noise iteratively, which can be defined as:where ᾱt represents the noise variance schedule and t ∈ {0, 1, ..., T }. Subsequently, a neural network f θ (z t , t, I ct ) conditioned on the corresponding CT scan I ct is trained to predict z 0 from a noisy box z T by reversing the noising process step by step. During inference, for an input CT scan I ct with a set of random boxes, the model is able to refine the random boxes to get a lesion detection prediction box z 0 , iteratively."
DiffULD: Diffusive Universal Lesion Detection,2.2,Training,"In this section, we specify the training process with our novelty introduced 'Center-aligned BBox padding'.Center-Aligned BBox Padding. As shown in Fig. We consider the generation in two parts: box scaling and center sampling. (i) Box scaling: We set a hyper-parameter λ scale ∈ (0, 1) for scaling. For z i , ] of perturbated boxes from a 2D Gaussian distribution N whose probability density function can be denoted as:where σ is a size-adaptive parameter, which can be calculated according to the z i 's width and height:Besides, for each input CT scan I ct , we collect all GT BBoxes in z and add random perturbations to them and generate multiple perturbated boxes for each of them. Thus the number of perturbated boxes in an image varies with its number of GT BBoxes. For better training, we fix the number of perturbated boxes as N for all training images. As shown in Fig. 1., the perturbated boxes cluster together and their centers are still aligned with the corresponding original GT BBox. Subsequently, perturbated GT boxes z 0 are sent for corruption as the training objective."
DiffULD: Diffusive Universal Lesion Detection,,Box Corruption.,"As shown in Fig. 1, we corrupt the parameters of z 0 with Gaussian noises. The noise scale is controlled by ᾱt (in Eq. 1), which adopts the decreasing cosine scheduler in the different time step t."
DiffULD: Diffusive Universal Lesion Detection,,Loss function.,"As the model generates the same number of (N ) predictions for the input image, termed as a prediction set, the loss function should be set-wised [5]. Specifically, each GT is matched with the prediction by the least matching cost, and the overall training loss [5] can be represented as:where L L1box and L giou are the pairwise box loss. We adopt λ L1box = 2.0 and λ giou = 5.0."
DiffULD: Diffusive Universal Lesion Detection,2.3,Inference,"At the inference stage, with a set of random boxes sampled from Gaussian distribution, the model does refinement step by step to obtain the final predictions z 0 . For better performance, two key components are used:Box Filtering. In each refinement step, the model receives a set of box proposals from the last step. As the prediction starts from arbitrary boxes and the lack of GT (lesion), most of these proposals are very far from lesions. Keeping refining them in the following steps will hinder network training. Toward efficient detection, we send the proposals to the detection head and remove the boxes whose confidential scores are lower than a particular threshold λ conf . The remaining high-quality proposals are sent for followed DDIM sampling.Box Update with DDIM Sampling. DDIM [27] is utilized to further refine the received box proposals by denoising. Next, these refined boxes are sent to the next step and start a new round of refinement. After multiple steps, final predictions are obtained. However, we observe that if we just filter out boxes with low scores during iterative refinement, the model runs out of usable box proposals rapidly, which also leads to a deterioration in performance. Therefore, after the box updating, we add new boxes sampled from a Gaussian Distribution to the set of remaining boxes with. The number of box proposals per image is padded to the fixed number N before they are sent to the next refinement step."
DiffULD: Diffusive Universal Lesion Detection,2.4,Backbone Design,"Our overall backbone design is identical to [16]. The input CT scan is rendered with different window widths and window levels. Then, multi-window features are extracted with a ConvNeXt-T [18] shared network and sent to 3D context feature fusion module. Subsequently, the fused feature is sent to the detector.Multi-window Input. Most prior arts in ULD use a single and fixed window (e.g., a wide window of [1024,4096]) to render the input CT scan, which suppresses organ-specific information and makes it hard for the network to focus on the various lesions. Therefore, taking cues from [16], we introduce 3 organ-specific HU windows to highlight multiple organs of interest. Their window widths and window levels are: W 1 = [1200, -600] for chest organs, W 2 = [400, 50] for soft tissues and W 3 = [200, 30] for abdomenal organs.3D Context Feature Fusion. We modify the original A3D [37] DenseNet backbone for context fusion. We remove the first Conv3D Block and use the truncated network as our 3D context fusion module, which fuses the multiwindow features from the last module."
DiffULD: Diffusive Universal Lesion Detection,3.1,Settings,"Our experiments are conducted on the standard ULD dataset DeepLesion [32]. The dataset contains 32,735 lesions on 32,120 axial slices from 10,594 CT studies of 4,427 unique patients. We use the official data split of DeepLesion which consists of 70%, 15%, 15% for training, validation, and test, respectively. Besides, we also evaluate the performance of 3 methods based on a revised test set from [4].Training Details. DiffULD is trained on CT scans of size 512 × 512 with a batch size of 4 on 4 NVIDIA RTX Titan GPUs with 24GB memory. For hyperparameters, the threshold N for box padding is set to 300. λ scale for box scaling is set to 0.4. λ conf for box filtering is set to 0.5. We use the Adam optimizer with an initial learning rate of 2e -4 and the weight decay as 1e -4. The default training schedule is 120K iterations, with the learning rate divided by 10 at 60K and 100K iterations. Data augmentation strategies contain random horizontal flipping, rotation, and random brightness adjustment.Evaluation Metrics. The lesion detection is classified as true positive (TP) when the IoU between the predicted and the GT BBox is larger than 0.5. Average sensitivities computed at 0.5, 1, 2, and 4 false-positives (FP) per image are reported as the evaluation metrics on the test set for a fair comparison (Table 2).Table 1. Sensitivity (%) at various FPPI on the standard test set of DeepLesion. DKA-ULD [25] and SATr [15] are up-to-date SOTA ULD methods under the settings of 3 slices and 7 slices, respectively. The numbers in brackets indicate the performance gains of our method comparing to the previous SOTA methods under the same settings."
DiffULD: Diffusive Universal Lesion Detection,,Methods,Slices @0.5 @1 @2 @4 Avg.[0.  
DiffULD: Diffusive Universal Lesion Detection,3.2,Lesion Detection Performance,"We evaluate the effectiveness of DiffULD against anchor-based ULD approaches such as 3DCE [33], MVP-Net [16], A3D [37] and SATr [15] on DeepLesion. Several anchor-free natural image detection methods such as FCOS [31] and DN-DETR [12] are also introduced for comparison. We report the performance of DiffusionDet [7] trained with our proposed backbone in 2.4 as well. In addition,we conduct an extensive experiment to explore DiffULD's potential on an revised test set of completely annotated DeepLesion volumes, introduced by Lesion-Harvester [4]. Table 1 demonstrates that our proposed DiffULD achieves favorable performances when compared to recent state-of-the-art anchor-based ULD approaches such as SATr on both 3 slices and 7 slices. It outperforms prior well-established methods such as A3D and MULAN by a non-trivial margin. This validates that with our padding strategy, the concise DPM can be utilized in general medical object detection tasks such as ULD and attain impressive performance."
DiffULD: Diffusive Universal Lesion Detection,3.3,Ablation Study,"We provide an ablation study about our proposed approach: center-aligned BBox padding. As shown in Table 3., we compared it with various other padding strategies, including: (i) duplicating original GT boxes; (ii) padding random boxes sampled from a uniform distribution; (iii) padding random boxes sampled from a Gaussian distribution; (iv) padding with center-aligned strategy.Our baseline method is training the diffusion model [7] directly with no box padding, using our proposed backbone in 2.4. The performance is increased by 0.30% by simply duplicating the original GT boxes. Padding random boxes following uniform and Gaussian distributions brings 0.51% and 0.91% improvement respectively. Our center-aligned padding strategy accounts for 1.13% improvement from the baseline. We attribute this performance boost to center-aligned padding's ability to provide high-quality additional training targets. It effectively expands the insufficient training targets on CT scans and enhances DPM's detection performance while avoiding deterioration triggered by adding random targets. This property is favorable for utilizing DPMs on a limited amount of GT like ULD."
DiffULD: Diffusive Universal Lesion Detection,4.0,Conclusion,"In this paper, we propose a novel ULD method termed DiffULD by introducing the diffusion probability model (DPM) to Universal Lesion Detection. We present a novel center-aligned BBox padding strategy to tackle the performance deterioration caused by directly utilizing DPM on CT scans with sparse lesion BBoxes. Compared with other training target padding methods (e.g., padding with random boxes), our strategy can provide additional training targets of higher quality and boost detection performance while avoiding significant deterioration. DiffULD is inherently advanced in locating lesions with diverse sizes and shapes since it can predict with arbitrary boxes, making it a promising method for ULD. Experiments on both standard and revised DeepLesion datasets show that our proposed method can achieve competitive performance compared to state-of-the-art ULD approaches."
DiffULD: Diffusive Universal Lesion Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 10.
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"Breast cancer is one of the high-mortality cancers among women in the 21st century. Every year, 1.2 million women around the world suffer from breast cancer and about 0.5 million die of it [3]. Accurate identification of cancer types will make a correct assessment of the patient's risk and improve the chances of survival. However, the traditional analysis method is time-consuming, as it mainly depends on the experience and skills of the doctors. Therefore, it is essential to develop computer-aided diagnosis (CADx) for assisting doctors to realize the rapid detection and classification.Due to being collected by various devices, the resolution of histopathological images extracted may not always be high. Low-resolution (LR) images lack of lots of details, which will have an important impact on doctors' diagnosis. Considering the improvement of histopathological images' acquisition equipment will cost lots of money while significantly increasing patients' expense of detection. The super-resolution (SR) algorithms that improve the resolution of LR images at a small cost can be a practical solution to assist doctors in diagnosis. At present, most single super-resolution methods only have fixed receptive fields [7,10,11,18]. These models cannot capture multi-scale features and do not solve the problems caused by LR in various magnification factors well. MRC-Net [6] adopted LSTM [9] and Multi-scale Refined Context to improve the effect of reconstructing histopathological images. It considered the problem of multi-scale, but only fused two scales features. This limits its performance in the scenarios with various magnification factors. Therefore, designing an appropriate feature extraction block for SR of the histopathological images is still a challenging task.In recent years, a series of deep learning methods have been proposed to solve the breast cancer histopathological image classification issue by the highresolution (HR) histopathological images. [12,21,22] improved the specific model structure to classify breast histopathology images, which showed a significant improvement in recognition accuracy compared with the previous works [1,20]. SSCA [24] considered the problem of multi-scale feature extraction which utilized feature pyramid network (FPN) [15] and attention mechanism to extract discriminative features from complex backgrounds. However, it only concatenates multi-scale features and does not consider the problem of feature fusion. So it is still worth to explore the potential of extraction and fusion of multi-scale features for breast images classification.To tackle the problem of LR breast cancer histopathological images reconstruction and diagnosis, we propose the Single Histopathological Image Super-Resolution Classification network (SHISRCNet) integrating Super-Resolution (SR) and Classification (CF) modules. The main contributions of this paper can be described as follows:(1) In the SR module, we design a new block called Multi-Features Extraction block (MFEblock) as the backbone. MFEblock adopts multi-scale receptive fields to obtain multi-scale features. In order to better fuse multi-scale features, a new fusion method named multi-scale selective fusion (MSF) is used for multi-scale features. These make MFEblock reconstruct LR images into SR images well.(2) The CF module completes the task of image classification by utilizing the SR images. Like SR module, it also needs to extract multi-scale features. The difference is that the CF module can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet) [13]  with the feature pyramid network (FPN) to achieve the feature extraction of this module. In FPN, we design a cross-scale selective fusion block (CSFblock) to fuse features of different scales.(3) Through the joint training of these two designed modules, the superresolution and classification of low-resolution histopathological images are integrated into our model. For improving the performance of CF module and reducing the error caused by the reconstructed SR images, we introduce HR images to CF module in the training stage. The experimental results demonstrate that the effects of our method are close to those of SOTA methods that take HR breast cancer histopathological images as inputs."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.0,Methods,"This section describes the proposed SHISRCNet. The overall pipeline of the proposed network is shown in Fig. 1(a). It composes two modules: SR module and CF module. The SR module reconstructs the LR image into the SR image. The CF module utilize the reconstructed SR images to diagnose histopathological images. In the training stage, we introduce HR images to improve the performance of CF module and alleviate the error caused by SR images."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.1,Super-Resolution Module,"To better extract and fuse multi-scale features for super-resolution, we propose a new SR network, called SRMFENet. Like SRResNet [11], SRMFENet takes a single low-resolution image as input and uses the pixelshuffle layer to get the restructured image. The difference between SRMFENet and SRResNet is that a Multi-Features Extraction block (MFEblock) is proposed to extract and fuse multi-scale histopathological images' features. The structure of the MFEblock is shown in Fig. 1(b). The input features X capture multi-scale features Y i through four 3×3 atrous convolutions [4] with different rates:where n is the number of atrous convolutions and is set to 4 by the experiments. This design not only preserves the depth of the network, but also increases the width of the network. It is beneficial for the network to extract shallow local texture information and global semantic information. After the feature extraction phase, a new fusion method named MSF fuses all of different scale features Y i . In the end, the input features X are added with the fused features. The details of MSF show in the Fig. 1(c). Firstly, we conduct Global Average Pooling (GAP) [14] on the multi-scale features to obtain their average channel-wise weights. Then using Sigmoid activation function to map weight to 0 to 1. Next softmax operation normalizes the same position of the obtained multi-scale average channel-wise weights. Finally, the features are multiplied by the corresponding normalized weights and the processed features are added together to generate the new multi-scale features. MFEblock is very applicable to process histopathological images of different magnification factors, as it employs convolution and attention operations to capture local and global image context information and fuse them well."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.2,Classification Module,"The task of the CF module is to classify the reconstructed SR images. It can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet as backbone network) with the FPN (a downsampling method) to achieve the feature extraction of this module. In Fig. 1(a), the multi-sacle features extracted by SKNet are the input of FPN. We propose a new fusion method, called cross-scale selective fusion block (CSFblock) to effectively fuse high-resolution and low-resolution features in FPN. After the fused features are processed by GAP, they are aggregated into a new multi-scale feature by concatenate operation. Finally, the aggregated multi-scale features are classified through the fully connected (FC) layer. The structure of CSFblock is shown in Fig. 1(d). The inputs of CSFblock are two-way inputs which are the high-resolution features X h ∈ W ×H×C and the low-resolution features X l ∈ W 1 ×H 1 ×C. In CSFblock, the upsampling operations are performed on the low-resolution features X l to realize consistency with X h dimension. X h and restructured X l are fused via an element-wise summation:Then, using GAP along the channel dimension to get the global information S. A FC layer generates a compact feature vector Z which guides the feature selection procedure. And Z is reconstructed into two weight vectors a, b of the same dimension as S through two FC layers, which can be defined as:where δ denotes ReLU and W a , W b , W c , means the weight of the FC layers. Specifically, a softmax operator is applied on a and b 's channel-wise digits:The fused feature map F is obtained through the attention weights on multi-scale features:"
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3.0,Loss Function,"The SR module and the CF module exploit different loss functions for training.In the SR module, L1 Loss is used for super-resolution. In the CF module, we introduce HR images to CF module in the training stage for improving the performance of CF module and reducing the error caused by the reconstructed SR images. We use F ocal Loss [16] to alleviate the class imbalanced data problem of the HR and SR images' classification. Inspired by the contrastive learning algorithm SimCLR [5], the HR and SR of the same images are similar to two different views. So the NT -Xent loss [19] is adopted to calculate similarity between SR multi-scale features and HR multi-scale features for CF module's robustness. The total loss function can be expressed as:where λ 1 , λ 2 and λ 3 are the weights of L1 Loss, F ocal Loss and NT -Xent Loss, respectively. In the inference stage, only SR images are taken as inputs by CF module. In our experiment, λ 1 , λ 2 and λ 3 are set to 0.6, 0.3 and 0.1, respectively. And the temperature parameter in NT -Xent Loss is set to 0.5."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3.0,Experiment,Dataset: This work uses the breast cancer histopathological image database (BreaKHis)1  [20]. The images in the dataset have four magnification factors The model is trained using the ADAM optimizer [25] with the learning rate set to 1x10 -3 . The learning rate is multiplied by 0.9 for every two epochs. We use SKNet-26 [13] as the backbone network in the CF module. The total training epochs are 100.
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,4.1,The Results of Super-Resolution and Classification,"Table 1 shows the results of the super-resolution phase. We adopt Peak Signal to Noise Ratio (PSNR) and structural similarity index (SSIM) [6] to evaluate the performance of the SR model. MRC-Net and our proposed SRMFENet (SR module) achieves better metrics than the other algorithms. This proves the effectiveness of multi-scale features extraction. Compared with MRC-Net, our MFEblocks can extract and fuse multi-scale features well. And the joint training of SRMFENet and CF module improves the performance of super-resolution. Figure 2 demonstrates that our model can recover more details with less blurring. We compare our introduced CF module with five state-of-the-art breast cancer histopathological image models and Diagnosis Network with MRC-Net [6], as shown in Table 2. The results illustrate that the CF module reaches the best   "
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,,Table 1 .,Fig. 2. Qualitative Comparison with SR methods on breast cancer histopathological images x8 and x4.
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,1.0,Introduction,"Alzheimer's disease (AD) is one of the most common neurological diseases in elderly people, accounting for 50-70% of dementia cases [31]. The progression of AD triggers memory deterioration, impairment of cognition, irreversible loss of neurons, and further genetically complex disorders as well. Mild Cognitive Impairment (MCI), the prodromal stage of AD, has been shown as the optimal stage to be treated to prevent the MCI-to-AD conversion [29]. Progressive MCI (pMCI) group denotes those MCI patients who progress to AD within 3 years, while stable MCI (sMCI) patients remained stable over the same time period. pMCI is an important group to study longitudinal changes associated with the development of AD [26]. By predicting the pMCI progress of the patients, we can intervene and delay the progress of AD. Thus, it is valuable to distinguish patients with pMCI from those with sMCI in the early stage with a computeraided diagnosis system.In recent years, deep learning (DL) based methods [2,8,11,13] have been widely used to identify pMCI or AD based on brain MRI data. Several works [9,24,29] take both brain MRI and clinical tabular data into account, using convolutional neural networks (CNNs) and multi-layer perceptions as the feature encoder. Due to the limited data in pMCI diagnosis, some works [19,29] resort to transfer learning to fine-tune the model on the pMCI-related task, by pre-training weights on the AD detection task. Previous CNN-based approaches [9,24,29] may fail on the lack of modeling long-range relationship, while the models based on two-stage fine-tuning [19,29] are inefficient and may cause networks to forget learned knowledge [32].Inspired by the advance in transformers [6,17,28] and prompt learning [4,15,25], we tailor-design an effective multi-modal transformer-based framework based on prompt learning for pMCI detection. The proposed framework is composed of a transformer-based visual encoder, a transformer-based attribute encoder, a multi-modal fusion module, and a prompt learning scheme. Clinical attributes and brain MR images are sent to the attribute encoder and visual encoder with the prompt tokens, respectively. Then the high-level visual and tabular features are aggregated and sent to fully-connected layers for final classification. For the proposed prompt tuning scheme, we first pre-train the neural network on the AD classification task. After that, we fine-tune the neural network by introducing and updating only a few trainable parameters. Importantly, we observe that the number of image patches and input tokens have a gap between 2D natural images and 3D MR images, due to the different dimensions. To complement local interactions between prompt and other tokens, we develop a global prompt token to strengthen the global guidance and make the visual feature extraction process more efficient and stable.Our contributions have three folds: (1) We propose a visual-attribute prompt learning framework based on transformers (VAP-Former) for pMCI detection.(2) We design a global prompt to adapt to high-dimension MRI data and build a prompt learning framework for transferring knowledge from AD diagnosis to pMCI detection. (3) Experiments not only show that our VAP-Former obtains state-of-the-art results on pMCI prediction task by exceeding the full fine-tuning methods, but also verify the global prompt can make the training more efficient and stable. "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.0,Methodology,"We aim to predict if an MCI patient will remain stable or develop Alzheimer's Disease, and formulate the problem as a binary classification task based on the brain MR image and the corresponding tabular attribute information of an MCI patient from baseline examination. As Fig. 1 shows, we adopt the model weights learned from AD identification to initialize the prediction model of MCI conversion. In the prompt fine-tuning stage, we keep all encoders frozen, only optimizing the prompts concatenated with feature tokens."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.1,Network Architecture,"We propose a transformer-based framework for pMCI prediction (VAP-Former) based on visual and attribute data, which is shown in Fig. 1. VAP-Former is mainly composed of three parts: a visual encoder for processing MRI data, an attribute encoder for processing attribute data, and a transformer-based fusion block for combining the multi-modal feature. Considering that capturing the long-range relationship of MRI is important, we employ the encoder of 3D UNETR++ [28] as our visual encoder. For the attribute encoder, since the clinical variables have no order or position, we embed the tabular data with the transformer blocks [30]. Followed by [6,7], we prepend a class token for dualmodality before the transformer-based fusion block. A class token is a learnable vector concatenated with dual-modal feature vectors. The class token of dualmodal is further processed by fully-connected layers for the final classification. "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.2,Knowledge Transfer with Multi-modal Prompt Learning,"To effectively transfer the knowledge learned from AD prediction task to the pMCI prediction task, we propose a multi-modal prompt fine-tuning strategy that adds a small number of learnable parameters (i.e., prompt tokens) to the input of the transformer layer and keeps the backbone frozen. The overall pipeline is shown in Fig. 2, where the upper part indicates adding the prompt tokens to the attribute transformer, while the lower part denotes sending the prompt tokens to the visual transformer.Tabular Context Prompt Learning. In the attribute encoder, we insert prompts into every transformer layer's input [15]. For the (i)-th Layer L i of SA transformer block, we denote collection of p prompts aswhere k is the number of the tabular prompt. The prompt fine-tuning Tabtransformer can be formulated as:where X i ∈ R M ×C denotes the attribute embedding at L i 's output space and P i denotes the attribute prompt at L i+1 's input space concatenated with X i .Visual Context Prompt Learning with Paired Attention. For the visual prompt learning part, we concatenate a small number of prompts with visual embedding to take part in the spatial-wise and channel-wise attention block [12,16,18] denoted as the prompt fine-tuning efficient paired attention block in Fig. 2. Within the prompt fine-tuning efficient paired attention block, we insert shared prompts into the spatial-wise attention module (SWA) and the channel-wise attention module (CWA), respectively. With a shared keys-queries scheme, queries, keys, and values are noted as Q p shared , K p shared , V p spatial , and•] be the concatenation operation, the SWA and CWA can be formulated as:where I S ∈ R N ×C and P spatial ∈ R P 2 ×C are spatial-wise visual embedding and prompts, and I C ∈ R N ×C and P channel ∈ R P 2 ×C are channel-wise visual embedding and prompts, and P is the number of visual prompt. After that, the initial feature map I is added to the attention feature map using a skip connection. Let + be the element-wise addition, this process is formulated as I = I + (I S + I C )."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.3,Global Prompt for Better Visual Prompt Learning,"Compared to natural images with relatively low dimensionality (e.g., shaped 224 × 224 × 3) and the salient region usually locates in a small part of the image, brain MRIs for diagnosis of Alzheimer's are usually high dimensional (e.g., shaped 144 × 144 × 144) and the cues to diagnosis disease (e.g., cortical atrophy and beta protein deposition) can occupy a large area of the image. Therefore, vanilla prompt learning [32] methods that are designed for natural images may not be directly and effectively applied to MRI's Recognition of Alzheimer's disease. We consider that the above differences lead to the following two problems: (1) vanilla prompt token often focuses on local visual information features; (2) the interaction between vanilla prompt token and visual feature is insufficient. Therefore, we consider that a sophisticated prompt learning module should be able to address the above-mentioned issues with the following feature:(1) the prompt token can influence the global feature; (2) the prompt token can effectively interact with the visual input features. A simple approach to achieve the second goal is to increase the number of prompt tokens so that they can better interact with other input features. However, the experiment proves that this is not feasible. We think it is because too many randomly initialized prompt tokens (i.e., unknown information) will make the model hard to train.Thus, we tailor-design a global prompt token g to achieve the above two goals. Specifically, we apply a linear transformation T to the input prompt tokens P to obtain the global prompt token g (i.e., a vector), which further multiplies the global feature map. Since the vector is directly multiplied with the global feature, we can better find the global feature responses in each layer of the visual network, which enables the model to better focus on some important global features for pMCI diagnosis, such as cortical atrophy. Since this linear transformation, T operation is learnable in the prompt training stage, the original prompt token can better interact with other features through the global token. To embed the global prompt token into our framework, we rewrite Eq. 3 as:where T denotes the linear transformation layer with P × C elements as input and 1×C element as output. Experiments not only demonstrate the effectiveness of the above method but also make the training stage more stable."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.1,Datasets and Implementation Details,"The datasets are from Alzheimer's Disease Neuroimaging Initiative (ADNI) [14], including ADNI-1 and ADNI-2. Following [19,22], we adopt ADNI- We preprocess the MR image as [22]. All MRI scans are pre-processed via 4 steps: (1) motion correction, (2) intensity normalization, (3) skull stripping, and (4) spatial normalization to a template. We use the Computational Anatomy Toolbox (CAT12)1 via Statistical Parametric Mapping software (SPM12)2 to perform the above procedures. Then all images are re-sampled as the size of 113×137×113 and the resolution of 1×1×1 mm 3 . For tabular clinical data, we select 7 attributes including age, gender, education, ApoE4, P-tau181, T-tau, and a summary measure (FDG) derived from 18F-fluorodeoxyglucose PET imaging. For tabular clinical data, we apply one-hot encoding to the categorical variables and min-max normalization to the numerical variables. We adopt the model weights learned from AD identification to initialize the prediction model of MCI conversion. Our model is trained using 1 NVIDIA V100 GPU of 32GB via AdamW optimizer [20] with 30 epochs for AD identification and 20 epochs for pMCI detection. The batch size is set to 4. We adopt the ReduceLROnPlateau [1] learning rate decay strategy with an initial learning rate of 1e-5. The loss function is binary crossentropy loss. The number of visual and tabular prompts is 10 and 5, respectively. We take F1-score [23], class balanced accuracy (BACC) [3], and the area under the receiver operating characteristic curve (AUC) [23] as evaluation metrics."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.2,Comparison with the State-of-the-Art Methods,"To validate the proposed VAP-Former and prompt fine-tuning (PT) strategy, we compare our model with three unimodal baselines: 1) UNETR++, which denotes the encoder of UNETR++ [28] only using MRI data as input, 2) Tabformer [21], which only uses tabular data and is similar to the attribute encoder work in our model. 3) 4-Way Classifier [27] which used 3D DenseNet as the backbone to construct Alzheimer's disease diagnosis model only using MRI data as input. To further evaluate the efficiency of our model and the proposed PT strategy, we integrate the DAFT [24], HAMT [5], and DFAF [10] into the same attribute and visual encoder for a fair comparison. And we fine-tune the proposed model with two strategies including full fine-tuning (FT) and prompt tuning (PT).In Table 1, the proposed model with PT strategy achieves 79.22% BACC, 63.13% F1, and 86.31% AUC. VAP-Former and VA-Former outperform all unimodal baselines in all metrics, indicating that our model can effectively exploit the relation between MRI data and tabular data to improve the prediction of pMCI. By using the PT strategy, the VAP-Former outperforms the second-best model, DAFT, by 1.16% BACC, 0.21% F1, and 0.98% AUC, demonstrating that the proposed PT strategy can efficiently adapt learned knowledge to pMCI detection and even achieves better classification results. Besides that, VAP-Former achieves the best results by tuning the minimum number of parameters."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.3,Ablation Study and Investigation of Hyper-parameters,"In this section, we study the internal settings and mechanism of the proposed PT strategy. First, we investigate how the prompts affect the VAP-Former performance. So we remove the visual prompts from the VAP-Former (denoted as TabPrompt in Table 2) and tabular prompts (denoted as VisPrompt), respectively. Compared with VA-Former, VisPrompt outperforms it by 0.07% AUC and TabPrompt degrades the performance. However, VAP-Former, which combines tabular prompts and visual prompts, significantly outperforms VA-Former by 1.54% AUC, indicating that introducing both types of prompts into the model simultaneously results in a more robust pMCI classification model. We further validate the importance of the global prompt module in the VAP-Former by"
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,4.0,Conclusion,"To detect pMCI with visual and attribute data, we propose a simple but effective transformer-based model, VA-Former, to learn multi-modal representations. Besides, we propose a global prompt-based tuning strategy, which is integrated with the VA-Former to obtain our overall framework VAP-Former. The proposed framework can efficiently transfer the learned knowledge from AD classification to the pMCI prediction task. The experimental results not only show that the VAP-Former performs better than uni-modal models, but also suggest that the VAP-Former with the proposed prompt tuning strategy even surpasses the full fine-tuning while dramatically reducing the tuned parameters."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 53.  removing it from the model (denoted as Vis-TabPrompt). As shown in Table 2, removing the global prompt results in degraded performance by 1.08% AUC.To investigate the impact of the number of prompts on performance, we evaluate VAP-Former with varying numbers of prompts. Given that the number of visual tokens exceeds that of tabular tokens. We fix the number of tabular prompts at 5 (left plot in Fig. 3) and fix the number of visual prompts at 10 (right plot), respectively while changing the other one. We hypothesize that the interaction between prompts and feature tokens is insufficient, so we gradually increase the number of tokens. As shown in Fig. 3, we observe that the model's performance ceases to increase after a certain number of prompts, confirming our assumption in Sect. 3.2 that too many randomly initialized prompt tokens make the model difficult to train. Conversely, when the number of prompts is small, the prompts can not learn enough information for the task and the interaction between prompt and feature tokens is not sufficient. Furthermore, as depicted in Fig. 3, the light blue area is smaller than the orange area, suggesting that the global prompt module makes VAP-Former more robust by helping the model to focus on important global features for pMCI detection."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,1.0,Introduction,"Deep learning for medical imaging has achieved remarkable progress, leading to a growing body of parameter-tuning strategies [1][2][3]. Those approaches are often designed to address disease-specific problems with limitations in their generalizability. In parallel, foundation models [4] have surged in computer vision [5,6] and natural language processing [7,8] with growing model capacity and data size, opening up perspectives in utilizing foundation models and large-scale clinical data for diagnostic tasks. However, pure imaging data can be insufficient to adapt foundation models with large model capacity to the medical field. Given the complex tissue characteristics of pathological whole slide images (WSI), it is crucial to develop adaptation strategies allowing (1) training data efficiency, and (2) data fusion flexibility for pathological image analysis. Although foundation models promise a strong generalization ability [4], there is an inherent domain shift between medical and natural concepts in both vision and language modalities. Pre-trained biomedical language models are increasingly applied to medical context understanding [9][10][11]. Language models prove to be effective in capturing semantic characteristics with a lower data acquisition and annotation cost in medical areas [12]. Such property is desired to address the dilemma of medical imaging cohorts, where well-annotated, high-quality medical imaging cohorts are expensive to collect and curate compared with text inputs [13]. In addition, vision-language models demonstrate the importance of joining multi-modal information for learning strong encoders [5,6,14]. Thus, connecting visual representations with text information from biomedical language models becomes increasingly critical to adapting foundation models for medical image classification, particularly in the challenging setting of data deficiency.In this study, we propose CITE, a data-efficient adaptation framework that Connects Image and Text Embeddings from foundation models to perform pathological image classification with limited training samples (see Fig. 1). To enable language comprehension, CITE makes use of large language models pretrained on biomedical text datasets [10,11] with rich and professional biomedical knowledge. Meanwhile, for visual understanding, CITE only introduces a small number of trainable parameters to a pre-trained foundation model, for example, CLIP [5] and INTERN [6], in order to capture domain-specific knowledge without modifying the backbone parameters. In this framework, we emphasize the utility of text information to play a substitutive role as traditional classification heads, guiding the adaptation of the vision encoder. A favorable contribution of our approach is to retain the completeness of both pre-trained models, enabling a low-cost adaptation given the large capacity of foundation models. Overall, our contributions are summarized as follows:1. We demonstrate the usefulness of injecting biomedical text knowledge into foundation model adaptation for improved pathological image classification. "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,2.0,Related Work,"Medical Image Classification. Deep learning for medical image classification has long relied on training large models from scratch [1,15]. Also, fine-tuning or linear-probing the pre-trained models obtained from natural images [16][17][18] is reasonable. However, those methods are supported by sufficient high-quality data expensive to collect and curate [19]. In addition, task-specific models do not generalize well with different image modalities [2]. To tackle this issue, we emphasize the adaptation of foundation models in a data-efficient manner.Vision-Language Pre-training. Recent work has made efforts in pre-training vision-language models. CLIP [5] collects 400 million image-text pairs from the internet and trains aligned vision and text encoders from scratch. LiT [20] trains a text encoder aligned with a fixed pre-trained vision encoder. BLIP-2 [14] trains a query transformer by bootstrapping from pre-trained encoders. REACT [21] fixes both pre-trained encoders and tunes extra gated self-attention modules. However, those methods establish vision-language alignment by pre-training on large-scale image-text pairs. Instead, we combine pre-trained unimodal models on downstream tasks and build a multi-modal classifier with only a few data.Model Adaptation via Prompt Tuning. Prompt tuning proves to be an efficient adaptation method for both vision and language models [22,23]. Originating from natural language processing, ""prompting"" refers to adding (manual) text instructions to model inputs, whose goal is to help the pre-trained model better understand the current task. For instance, CoOp [22] introduces learnable prompt parameters to the text branch of vision-language models. VPT [23] demonstrates the effectiveness of prompt tuning with pre-trained vision encoders. In this study, we adopt prompt tuning for adaptation because it is lightweight and only modifies the input while keeping the whole pre-trained model unchanged. However, existing prompt tuning methods lack expert knowledge and understanding of downstream medical tasks. To address this challenge, we leverage large language models pre-trained with biomedical text to inject medical domain knowledge.Biomedical Language Model Utilization. Biomedical text mining promises to offer the necessary knowledge base in medicine [9][10][11]. Leveraging language models pre-trained with biomedical text for medical language tasks is a common application. For instance, Alsentzer et al. [9] pre-train a clinical text model with BioBERT [10] initialization and show a significant improvement on five clinical language tasks. However, the potential of biomedical text information in medical imaging applications has not been explicitly addressed. In our efforts, we emphasize the importance of utilizing biomedical language models for adapting foundational vision models into cancer pathological analysis."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.0,Methodology,"Figure 2 depicts an overview of our approach CITE for data-efficient pathological image classification. CITE jointly understands the image features extracted by vision encoders pre-trained with natural imaging, and text insights encoded in large language models pre-trained with biomedical text (e.g., BioLinkBERT [11] which captures rich text insights spanning across biomedical papers via citations). We connect text and imaging by a projection and classify the images by comparing the cosine similarity between image and text embeddings. Importantly, we introduce two low-cost sets of trainable parameters to the vision encoder in order to adapt the model with the guidance of text information. They are (1) prompt tokens in the input space to model task-specific information, and (2) a projection layer in the latent space to align image and text embeddings. During model adaptation, we freeze the pre-trained encoders and only tune the introduced parameters, which not only saves remarkable training data and computational resources but also makes our approach favorable with various foundation model architectures."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.1,Connecting Text and Imaging,"An image I to be classified is processed through a pre-trained vision encoder to generate the image embedding x v with dimension d v , where v stands for ""vision"":For the label information, we encode the class names T c (c ∈ [1, C]) with a pre-trained biomedical language model instead of training a classification head (see Fig. 2(e)). We tokenize and process T c through the language encoder to generate the text embedding x c l with dimension d l , where l stands for ""language"":Vision-language models like CLIP [5] contain both a vision encoder and a language encoder, which provide well-aligned embeddings in the same feature space. In this case, prediction ŷ is obtained by applying softmax on scaled cosine similarities between the image and text embeddings (see Fig. 2(d)):where sim(•, •) refers to cosine similarity and τ is the temperature parameter.For irrelevant vision and language encoders, we introduce an extra projection layer to the end of the vision encoder to map the image embeddings to the same latent space as the text embeddings. We replace x v in Eq. ( 3) with x v :During adaptation, the extra parameters are updated by minimizing the cross-entropy of the predictions from Eq. ( 3) and the ground truth labels."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.2,Learning Visual Prompt,"Medical concepts exhibit a great visual distribution shift from natural images, which becomes impractical for a fixed vision encoder to capture task-specific information in few-shot scenarios. Visual prompt tuning (VPT [23]) is a lightweight adaptation method that can alleviate such an inherent difference by only tuning prompt tokens added to the visual inputs of a fixed vision transformer [24], showing impressive performance especially under data deficiency. Thus, we adopt VPT to adapt the vision encoder in our approach.A vision transformer first cuts the image into a sequence of n patches and projects them to patch embeddings E 0 ∈ R n×dv , where d v represents the visual embedding dimension. A CLS token c 0 ∈ R dv is prepended to the embeddings, together passing through K transformer layers {L k v } k=1,2,...,K . CLS embedding of the last layer output is the image feature x v . Following the setting of shallow VPT, we concatenate the learnable prompt tokens P = [p1 , . . . , p p ] ∈ R p×dv , where p is the prompt length, with CLS token c 0 and patch embeddings E 0 before they are processed through the first transformer layer:where [•, •] refers to concatenation along the sequence length dimension, and Z k ∈ R p×dv represents the output embeddings of the k-th transformer layer at the position of the prompts (see Fig. 2(a-c)). The prompt parameters are updated together with the projection layer introduced in Sect. 3.1."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,4.0,Experimental Settings,"Dataset. We adopt the PatchGastric [25] dataset, which includes histopathological image patches extracted from H&E stained whole slide images (WSI) of stomach adenocarcinoma endoscopic biopsy specimens. There are 262,777 patches of size 300 × 300 extracted from 991 WSIs at x20 magnification. The dataset contains 9 subtypes of gastric adenocarcinoma. We choose 3 major subtypes including ""well differentiated tubular adenocarcinoma"", ""moderately differentiated tubular adenocarcinoma"", and ""poorly differentiated adenocarcinoma"" to form a 3-class grading-like classification task with 179,285 patches from 693 WSIs. We randomly split the WSIs into train (20%) and validation (80%) subsets for measuring the model performance. To extend our evaluation into the real-world setting with insufficient data, we additionally choose 1, 2, 4, 8, or 16 WSIs with the largest numbers of patches from each class as the training set.The evaluation metric is patient-wise accuracy, where the prediction of a WSI is obtained by a soft vote over the patches, and accuracy is averaged class-wise.Implementation. We use CLIP ViT-B/16 [5] as the visual backbone, with input image size 224 × 224, patch size 16 × 16, and embedding dimension d v = 512. We adopt BioLinkBERT-large [11] as the biomedical language model, with embedding dimension d l = 1, 024. To show the extensibility of our approach, we additionally test on vision encoders including ImageNet-21k ViT-B/16 [24,26] and INTERN ViT-B/16 [6], and biomedical language model BioBERT-large [10].Our implementation is based on CLIP 1 , HuggingFace2 and MMClassification3 .Training Details. Prompt length p is set to 1. We resize the images to 224×224 to fit the model and follow the original data pipeline in PatchGastric [25]. A class-balanced sampling strategy is adopted by choosing one image from each class in turn. Training is done with 1,000 iterations of stochastic gradient descent (SGD), and the mini-batch size is 128, requiring 11.6 GB of GPU memory and 11 min on two NVIDIA GeForce RTX 2080 Ti GPUs. All our experiment results are averaged on 3 random seeds unless otherwise specified. "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,5.0,Results,"CITE Consistently Outperforms all Baselines Under all Data Scales. Figure 3 shows the classification accuracy on the PatchGastric dataset of our approach compared with baseline methods and related works, including (1) R50-21k: fine-tune the whole ResNet50 [27] backbone pre-trained on ImageNet-21k [26].(2) Linear probe: train a classification head while freezing the backbone encoder.(3) Fine-tune: train a classification head together with the backbone encoder. (4) CLAM [18]: apply an attention network on image features to predict pseudo labels and cluster the images. ( 5) Zero-shot [5]: classify images to the nearest text embeddings obtained by class names, without training. (6) Few-shot [28]: cluster image features of the training data and classify images to the nearest class center. ( 7) VPT [23]: train a classification head together with visual prompts. Note that CLIP ViT-B/16 vision encoder is adopted as the backbone for ( 2)- (7) Table 2. CITE fits in with various pre-trained encoders. We include CLIP ViT-B/16 [5], ImageNet-21k ViT-B/16 [26] and INTERN ViT-B/16 [6] visual encoders, combined with CLIP textual encoder [5], BioBERT (BB) [10] and BioLinkBERT (BLB) [11] language models. The highest performance of each visual encoder is bolded.For each combination, CITE consistently outperforms linear and fine-tune baselines. CITE Shows Model Extensibility. We evaluate our approach with additional backbones and biomedical language models to assess its potential extensibility. Table 2 displays the findings of our approach compared with linear probe and fine-tune baselines. The results demonstrate that CITE is compatible with a variety of pre-trained models, making it immune to upstream model modifications. The text information encoded in biomedical language models allows vision models pre-trained with natural imaging to bridge the domain gap without taskspecific pre-training on medical imaging. Importantly, when using both the vision and language encoders of CLIP ViT-B/16, our approach still outperforms the baselines by a remarkable margin (47.7% to 60.1%), demonstrating the importance of multi-modal information. While CLIP gains such modality matching through pre-training, our CITE shows an appealing trait that irrelevant vision and language models can be combined to exhibit similar multi-modal insights on pathological tasks without a need of joint pre-training."
Text-Guided Foundation Model Adaptation for Pathological Image Classification,6.0,Conclusion,"Adapting powerful foundation models into medical imaging constantly faces data-limited challenges. In this study, we propose CITE, a data-efficient and model-agnostic approach to adapt foundation models for pathological image classification. Our key contribution is to inject meaningful medical domain knowledge to advance pathological image embedding and classification. By tuning only a small number of parameters guided by biomedical text information, our approach effectively learns task-specific information with only limited training samples, while showing strong compatibility with various foundation models. To augment the current pipeline, the use of synthetic pathological images is promising [29]. Also, foundation training on multi-modal medical images is of substantial interest to enhance model robustness under data-limited conditions [30]."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,1.0,Introduction,"Magnetic Resonance Imaging (MRI) has been extensively applied to clinical diagnosis [16]. Compared with Computed Tomography (CT), a brain MRI is more sensitive for multiple stroke types [3], therefore considered as the gold standard for stroke diagnosis. Nevertheless, the long acquisition time for a brain MRI (20 to 30 min) imposes challenges, especially in cases of acute stroke where rapid diagnosis is essential and patient movement during this distressing period of time commonly limits evaluation. As a result, MRI acceleration techniques have been developed to achieve more rapid diagnosis, increasing resource availability while reducing costs [14,18]. A k-space sub-sampling (KS) approach serves as a simple MRI acceleration solution [20], compared with other hardware-based acceleration methods. However, the signal loss by KS leads to blurry reconstructed MR images that are less than ideal for a reliable clinical diagnosis.Artificial Intelligence (AI) plays an increasingly important role in MRI-based diagnosis, for both MR image reconstruction and clinical decision making. Deep Neural Networks (DNN) were trained to reconstruct the MR images from the sub-sampled k-space [10,13], which provides a better reconstruction than the Inverse Fast Fourier Transform (IFFT). Nevertheless, detailed information in the brain may still be lost in the reconstructed MR images due to the signal sparsity in the k-space. On the other hand, traditional Convolutional Neural Network (CNN) [15] and the latest Vision Transformer (ViT)-based [6] predictive models have shown impressive prediction accuracy on stroke diagnosis tasks, such as slice classification and lesion segmentation [7,11]. However, these DNNs trained on clean images through Empirical Risk Minimization (ERM) are vulnerable to perturbations in the input images [2]. Whatever the reconstruction method used, even the slightest perturbation in accelerated MR images can lead to a wrong stroke prediction from the AI models. Therefore, building robust DNN models to handle the perturbed MR image input is important for MRI acceleration.In this paper, we introduce a Distributionally Robust Learning (DRL)-based approach [4] into the deep MR image classifier training, in order to improve the model robustness to the image perturbation resulting from the signal sparsity in accelerated MRI. Compared with ERM, DRL is an optimization method minimizing the worst-case loss over an ambiguity set, therefore, can tolerate outliers in the data [5]. We implemented DRL to different linear layers in deep CNN/ViT classifiers, and applied a randomized training approach to improve the training efficiency. Our results show that on a real-world dataset, DRL can significantly improve the stroke classification performance of ERM and other baseline defensive training methods, when the signal sparsity and noise in accelerated MRI are generated by the Cartesian Undersampling (CU) method [20] and White Gaussian Noise (WGN). We further show that in highly perturbed MR images where the ERM model and even clinicians cannot give a reliable diagnosis, our DRL model can still correctly recognize stroke, which establishes that our method can assist accelerated MRI diagnosis."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,2.1,Distributionally Robust Learning,"We will use the DRL framework under a multi-class classification setting developed by [4] in a DNN-based stroke diagnosis application. We provide a brief overview of the DRL model. Assume that there are K classes, and our goal is to classify an example with an input feature x ∈ R d to one of the K classes with a one-hot class label y ∈ {0, 1} K . Logistic regression solves this problem by minimizing the following expected true riskwhereis the coefficient matrix, P * is the true distribution of the data (x, y), h B (x, y) log 1 e B x -y B x is the loss function to be minimized, and E P * denotes the expectation under the distribution P * . Since P * is usually unknown, Problem (1) cannot be solved directly. The ERM approach tackles this by replacing the expected true risk by a sample averaged risk. Given N realizations of (x, y), ERM minimizes the following empirical riskERM can produce unreliable solutions when the samples are contaminated by noise or drawn from an outlying distribution. To obtain robust estimators that can hedge against noise in the training data and generalize well out-of-sample, [4] proposed the DRL framework under the Wasserstein metric. Specifically, it minimizes the worst-case expected loss over a set of probability distributionswhere Ω contains a set of probability distributions that are close to the empirical distribution PN measured by the Wasserstein metric, Ω {Q ∈ P(Z) : W 1 (Q, PN ) ≤ }, where Z is the set of possible values for (x, y), P(Z) is the space of all probability distributions supported on Z, is a pre-specified radius of the ambiguity set Ω, PN is the empirical distribution that assigns an equal probability 1/N to each observed sample (x i , y i ), and W 1 (Q, PN ) is the order-1 Wasserstein distance between Q and PN defined aswhere Π is the joint distribution of z 1 (x 1 , y 1 ) and z 2 (x 2 , y 2 ) with marginals Q and PN , respectively, and l is a distance metric on the data space.An equivalent reformulation (4) of (3) was developed by [4] when, where W is a positive semidefinite weight matrix to account for any transformation on the input feature x and can be estimated from data using metric learning (see Sec. 2.2) and with M → ∞:where"
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,2.2,DRL for Deep Stroke Diagnosis Networks,"We apply DRL to ViT and CNN-based MR image stroke classification models, in order to enhance their robustness against image perturbations in accelerated MRI. We apply the DRL reformulation (4) to the last layer B, and certain intermediate linear layers in a deep MR image classifier. For a ViT model (cf. Fig. 1), we apply DRL to the patch projection layer P and the final linear classification layer B, in order to predict if an MR image slice is normal (label 0) or depicts a stroke lesion (label 1). To speed up the training process, during each epoch, we randomly pick one layer L to train while keeping all other layers frozen. A validation set V is used to tune the hyperparameters (e.g., regularization coefficients). To account for the non-linear transformation of the raw image resulted from all layers before L, we solve the following Linear Matrix Inequality (LMI) problem [4] to estimate a weight matrix W:where xi is the perturbed version of an MR image slice x i , D the training set, S {(i, j)|x i , x j ∈ D, y i = y j }, |S| denotes the cardinality of the set S, φ L is the input to L and φ (t) L is the t-th hidden state (i.e., the vector representation for each instance in the sequence, output by and fed into different layers in ViT) in the sequence φ L of length T , and c is a fixed parameter. T = 1 if L refers to the B layer. The intuition of ( 5) is that in the transformed feature space, distance between the clean and perturbed version of a slice will be minimized, while slices from different classes (normal and stroke) are sufficiently far away. For a CNN model, we only applied DRL to the final linear layer B.We chose two approaches to generate the perturbation in accelerated MRI. Cartesian Undersampling (CU) perturbation [20] keeps only the central and a few randomly-sampled parts of the k-space; the corresponding reconstructed MR image only keeps the main structural information in the brain and introduces misalignments. A smaller central fraction f used in k-space indicates a larger perturbation. Noise might be introduced during the signal transmission, so we add White Gaussian Noise (WGN) as another type of perturbation, where the standard deviation σ is regarded as the perturbation intensity. To show the strength of DRL, in addition to ERM, we also apply Brute-force Adversarial Training (BAT) [2] and Projected Gradient Descent (PGD) [12] as baseline methods. Among all of the current defensive training methods which improve the model robustness against perturbations, BAT and PGD are representative. BAT adds noisy samples into the training set, therefore is simple and widely used. PGD is known to be robust to a wide range of image perturbations, and is considered a state-of-the-art method. "
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.1,Experimental Materials and Settings,"Our dataset included MRI brain scans from 226 patients performed at an urban tertiary referral academic medical center that is a comprehensive stroke center. Clinical scans of adult patients aged 18-89 years with recent (acute or subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in this study via a search of the Philips Performance Bridge. Scans meeting this criteria were downloaded and simultaneously anonymized to preserve patient anonymity and prevent disclosure of protected health information as part of this IRB exempt study. No patient demographic information was retained for the scans, as it was considered to represent an unnecessary risk for accidental release of protected health information. The diffusion weighted images with a gradient of B=1000 were utilized for the analysis (see the Supplement1 for information about the MRI scanner and parameter settings). Each MR image contains multiple slices, and every slice was annotated as normal or stroke by a board-certified neuroradiologist with a subspecialty certification. Annotation of the strokes was performed on the diffusion weighted images using ITK-SNAP (ver. 3.80) [19], and all included MRI examinations were reviewed by the neuroradiologist during the annotation process to ensure that the images were of diagnostic quality without significant motion degradation or other artifacts. To avoid the dependency among the slices from the same subject, we applied a 2-d acquisition during the MR imaging, and implemented a slice-level MR image preprocessing. While the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%) stroke slices, we further randomly split them into training/validation/test sets using the ratio 80%/10%/10%. For the training set, we implemented data augmentation strategies by rotating or flipping each slice. Finally, the training/validation/test set contains 31,356/653/654 slices, correspondingly. We implemented DRL to both CNN and ViT models. For the CNN model, we used a ResNet-18 [9] architecture, while for the ViT model, we first pre-trained a 4-layer ViT using a self-supervised pre-training method called Masked Autoencoder (MAE) [8], using the T1/T2-weighted brain MR images in the IXI dataset [1]. MAE pre-training first randomly masks 75% of the image patches in an MRI slice input, and then uses a ViT encoder-decoder architecture to reconstruct the masked MRI patches, in order to learn the dependency among different locations in the brain. After 400 pre-training epochs, an overall satisfying reconstruction result can be observed in Fig. 2.To evaluate the binary classification performance of different models, we use the Area Under the Receiver Operating Characteristic (AUROC) curve as our main metric. As our dataset is unbalanced, we also considered the Area Under Precision-Recall Curve (AUPRC). We ran the experiments 3 times using different random seeds. The training of our DNNs were implemented on 3 NVIDIA RTX A6000 (48GB VRAM) GPUs, and each DRL training epoch can be completed within 0.03 GPU hours. We used a learning rate of 1 × 10 -5 and batchsize of 128 for DRL training, while no weight decay was applied. To solve the LMI problem in (5), we used SDPT3 v4.0 [17] as the solver. We set the CU perturbation with the acceleration factor of 4, 6, 8, 12 with the central fraction of 8%, 6%, 4% and 2% in k-space respectively, and the remaining parts were chosen randomly in the peripheral region accordingly."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.2,Results,"We show the stroke classification AUROC in Fig. 3. When the k-space subsampling fraction decreased and the signal became sparser, the performance of both ViT and CNN models trained under ERM dropped significantly, from around 95% to below 80%. DRL significantly improved the AUROC of the ERMbased ViT model from 74.5% to 83.1% when the MR images were under extreme CU perturbation, while only slightly influenced model performance on the clean test set. For WGN, the largest improvement brought to ERM-based ViT model was 16.9%. Although we only applied DRL to the last layer of the CNN model, the improvement against ERM was still remarkable, up to 11.9%/4.9% for CU/WGN. With BAT and PGD adversarial training, the corresponding ViT or CNN models were also improved, though when DRL was combined with BAT and PGD, the model robustness can be further enhanced. Table 1 shows the maximum AUROC and AUPRC improvement that DRL can bring to different baseline methods. For ViT and CNN models, the AUROC improvement w.r.t BAT/PGD defensive methods is up to 23.9%/12.2%, respectively. Note that the perturbed MRI samples used to implement BAT were the same as those used by DRL, which shows that DRL is a more effective way to exploit the information in adversarial samples, compared to simply adding the blurry images into the training set. For CU perturbation, our best combined model using DRL improved the AUROC/AUPRC of the ERM model by up to 15%/12.5%, while this improvement under WGN perturbation was up to 18.8%/36.2%. Under CU perturbation, we further show that our DRL model can recognize stroke while clinicians may fail to. In Fig. 4, the stroke MRI slices from the test sets are under different levels of CU perturbations. For both ERM-and DRLbased ViT models, we maximized the F1 score of the stroke class on the training set to calculate the optimal decision threshold for stroke prediction, in order to balance the precision and recall. When the k-space signal becomes more sparse, the reconstructed MRI slices get more blurry and the lesion areas become less recognizable, even for human eyes. As a result, the ERM model fails to detect stroke under high perturbation levels. Nevertheless, the DRL model can tolerate more intense CU perturbation and recognize stroke slices that may even be misclassified by clinicians, which reveals its value in improving the diagnosis in  an accelerated MRI mode. We verified the effectiveness of our approach on the actual clinical scans acquired for clinical care and not just for research purposes, suggesting that the methods and findings in the current study should be generalizable to routine clinical practice conditions and potentially other types of clinical image-based diagnosis (e.g., brain tumor) as well. In addition, our DRL framework does not necessarily need to be used in isolation, rather it can also be combined with other performance boosting methods in accelerated MRI to further improve them, just like for BAT and PGD."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,4.0,Conclusions,"In this study, we implemented a DRL-based robust learning approach to improve the robustness of deep image classifiers, in order to achieve more accurate stroke classification from brain MR images reconstructed from a sub-sampled k-space.Our work can potentially be applied to accelerate and improve time-critical stroke diagnosis. Future work can apply DRL to more MRI diagnosis tasks (e.g., lesion area segmentation), justifying its effectiveness on more types of subsampling methods in MRI acceleration."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_74.
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,1.0,Introduction,"Computer-aided diagnosis utilizes machine learning techniques to conduct a pathological diagnosis concerning biomedical imaging data collected from various pathological modalities, such as computed tomography [19], magnetic resonance imaging [11], ultrasound [23], and angiography [9]. With the assistance of CAD techniques, the clinicians merely need to check the possible pathological regions narrowed down by computer-aided diagnosis method, significantly reducing the entire diagnosis time. With the recent success of deep learning, researchers are able to raise the reliability of CAD methods and assist clinicians in diagnosing more complex clinical tasks. However, a reliable machine learning-based CAD method usually relies on the supervision of abundant annotated training data. Yet diseased pathological data are rare and diverse, and acquiring reliable pathological annotations are labor-intensive and expertise-required. As a result, the difficulty of data collection restricts the development of the supervised CAD.Due to the difficulty of acquiring the abundant annotated training data, the current SOTA method, i.e., CSM [14], proposes a MIL-based WVAD manner to specifically tackle one specific disease detection task, i.e., colorectal cancer diagnosis via colonoscopy. Considering the case of colonoscopy, the CSM's anomaly detection setting is used to handle the rare and diverse diseased pathological data by commonly assuming that only video-level annotations are available for training. Furthermore, its video setting concerns the temporal correlation within data. The setting of such MIL-based weakly supervision prevents the need for abundant annotated training data by assuming that merely the video-level annotations, including normal and diseased ones, are available for training.Similar to the previous MIL-based WVAD methods [4,13,14], our model assumes all training snippets (consecutive video frames) within a non-diseased video are all normal snippets, yet each diseased video has at least one abnormal snippet. Furthermore, the proposed contrastive feature decoupling network treats disease detection as an out-of-distribution task. Precisely, our CFD learns a memory bank to learn normal features. A snippet that failed to be well reconstructed with these normal features is considered diseased. On the other hand, the residual of a snippet and its reconstructed one reveals the snippet's abnormal ingredients. Consequently, we are able to decouple each snippet as normal ingredients (reconstructed parts) and abnormal ingredients (residual parts) by leveraging the memory bank. With the decoupled snippet-level feature ingredients, our CFD employs both the normal and abnormal feature ingredients via a contrastive learning paradigm to concurrently optimize video-level and snippetlevel disease scores for pursuing more accurate detection.To assess the proposed contrastive feature decoupling network, we conduct experiments on two datasets, i.e., Polyp and PANDA-MIL. The main contributions are summarized as follows.-Our contrastive feature decoupling network learns a memory bank to learn normal atoms for decoupling each snippet as normal and diseased feature ingredients as opposite contrastive learning samples. Such a feature decoupling intrinsically fits the contrastive learning paradigm for optimizing MIL objectives on bags and instances. 2 Related Work"
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,2.1,Disease Detection,"With the evolution of artificial intelligence techniques in the past decades, deep learning has shown its potential for computer-aided diagnosis of various symptoms [6,8,14,17,22]. For example, Li et al. [8] established a large-scale attentionbased database and designed a specialized model using retinal fundus images for detecting glaucoma. Windsor et al. [17] constructed a transformer-based model to detect spinal cancer for MRI scans. More recently, Tian et al. [14] formulated polyp detection in a WVAD scheme while tackling polyp detection using colonoscopy videos to search colon polyps in the temporal sequence. Unlike previous methods of handling one specific pathological modality, we simultaneously address disease detection across pathological modalities of colonoscopy videos and prostate tissue biopsies using our contrastive feature decoupling network."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,2.2,Contrastive Learning,"The characteristics of self-supervised learning are defining the proxy objective or addressing pretext tasks using pseudo labels for the unlabeled instances. One popular branch is contrastive learning which shows a remarkable ability to obtain the desired semantic representation from various perspectives. For example, CoLA [21] tackled action localization by proposing snippet contrast loss to refine the feature representations of hard snippets according to the easily discriminative snippets. CSM [14] borrowed the concept from CoLA and defined the hard/easy snippets for representing normal/abnormal from colonoscopy videos. They empirically selected hard snippets based on the transitional edge and missed disease snippets, such as an occlusive polyp. In this work, we employ a similar contrastive learning strategy as CSM while preventing their rule-based contrastive training samples selection by intrinsically leveraging the decoupled features derived from our feature decoupling process via memory bank."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.0,Method,"We aim to design a MIL-based WVAD model for tackling disease detection across different pathological modalities. Our model contains an offline trained memory bank to store feature atoms before the CFD training procedure, which associates a contrastive loss to boost the model performance using decoupled features per instance. Winin the MIL scenario, our model employs two classifiers to enable reasoning of the disease scores at instance and bag levels. Figure 1 overviews the working flow of the proposed contrastive feature decoupling network. "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.1,Memory Bank Construction,"Given dataset D comprising normal sub-dataset D 0 and abnormal sub-dataset D 1 , we first encode all instances per bag B ∈ D into instance-level feature set F = {f t } T t=1 ∈ R T ×C via a pre-trained feature extractor E. That is, F = E(B), T is the number of instances, and C represents the instance-level feature dimension. We then collect all normal instance-level features f ∈ R 1×C from D 0 to learn the memory bank M by using the dictionary learning technique [7] where D 0 is the normal sub-dataset collected from the training split, w t is the learned weights within the memory bank learning process, and λ is a hyperparameter to constrain the memory bank sparsity."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.2,Contrastive Feature Decoupling,"With the learned normal instance features stored in the memory bank M, we are able to reconstruct a normal version for any given bag-level feature F. Such a normal version is denoted as a normal-like feature F H . To this end, we reconstruct a given F concerning M via the following equationwhere σ stands for the softmax function; φ q , φ k , and φ v respectively represent the query, key, and value linear projections, as introduced in the self-attention framework [15]. To make a robust learning process by mining the hard and easy instances for the subsequent contrastive loss, CSM [14] designed a rule-based instances selection concerning the transitional edge and missed disease instances, such as occlusion or invisibility from the polyp. Different from the CSM model, we generate the disease-like feature F D referring to F H as follows:where ω ∈ R 1×C is the weight for reweighting F by channel-wise multiplication.For depicting the degree of disease/abnormal to estimate the channel-wise weight ω, we consider attending to the distant features with respect to the normal ones, i.e., F H . In practice, we estimate the degree of disease/abnormal based on the difference between F and F H bywhere φ d , G, and Ψ are linear projections, global average pooling, and the multiscale temporal network [13], respectively. With the decoupled features F H and F D , our MIL-based instance-level classifier aims to carry out the discriminating decision, and the bag-level classifier seeks the prediction as fitting annotation y as possible. Following recent MILbased methods, we select top-K instances of normal and abnormal in each bag to form the separation loss aswhere δ is the hyperparameter for constrained margin and {•} K is the operator that selects top-K instances. The other common loss used in the recent MILbased works is the classification loss building upon the binary cross entropywhere S = φ I (F D ) represents the instance-level prediction inferred by an instance-level classifier φ I and s = φ B (G((1ω) F H )) means the bag-level prediction resulting from a bag-level classifier φ B ."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.3,Regularization,"Motivated by the recent WVAD methods [4,13,14,18], which adopt auxiliary losses to regularize the learning procedure, we consider the conventional regularization losses, such as temporal smoothness and sparsity, as followsBy using the decoupled features F H and F D , we are ready to regularize the opposite decoupled features across bags with the aid of a contrastive loss. An expected contrastive loss aims to make our model attract features from the same category while distracting the features from distinct classes. In practice, we formulate such a contrastive loss bywhere τ denotes the temperature parameter in the normalized temperaturescaled loss. Notice that ( 8) is simplified for the sake of clarity. A complete objective should consider the symmetric form by switching D 1 and D 0 in (8)."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.1,Dataset and Metric,"We evaluate our model against SOTAs on the existing Polyp [14] dataset and the PANDA-MIL dataset introduced in this work. We employ the same evaluation criteria as the previous work for a fair comparison. Please refer to the supplementary material for the statistics of the two datasets.Polyp. This dataset collects colonoscopy videos from Hyper-Kvasir [1] and LDPolypVideo [10]. Its training split contains 163 videos of video-level annotations, and the testing split includes 90 videos of frame-level annotations."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,,PANDA-MIL.,"The Prostate cANcer graDe Assessment (PANDA) challenge [2] comprises over 10K whole-slide images (WSIs) of digitized hematoxylin and eosin-stained biopsies originating from Radboud University Medical Center and Karolinska Institute. PANDA-MIL collects the eosin-stained biopsies with region-based masks indicating the benign (normal) and cancerous (abnormal) tissue, combined by stroma and epithelium. To fit the MIL-based WVAD task, we non-overlapped partition each WSI (bag) into patches (instances) and only keep those patches comprising tissue over the 50% patch size. Each kept patch gets its patch-level annotations from PANDA, and a WSI comprising any abnormal patch is treated as an abnormal WSI. In sum, PANDA-MIL's training split contains 3,925 bags of bag-level annotations, and the testing split includes 975 bags of instance-level annotations.Metric. We follow the previous methods [4,13,14] to employ the instant-level Area Under Curve (AUC) and the Average Precision (AP) for a fair comparison. The larger values of both metrics mean better disease detection performance. "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.2,Implementation Details,"All the evaluated methods in the experiment used the same feature encoder, i.e., I3D [3] pre-trained on Kinetics-400 [5], for a fair comparison. Our method is trained using Adam optimizer with the learning rate of 0.001, batch size 32, and 200 epochs. Each bag/video is encoded into T = 32 snippets among both datasets via linear interpolation."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.3,Comparison Results,"Table 1 shows the compared results of our CFD model against recent WVAD methods [4, 13,14,18] for tackling the disease detection task. The results in Table 1 demonstrate that our CFD consistently outperforms all the other methods on two datasets. Precisely, our model achieves the new SOTA by 1.1% AUC and 1.5% AP improvements on the Polyp dataset and 1.09% AUC and 2.45% AP improvements on the PANDA-MIL dataset. Please refer to the supplementary material for the completed results, including more WVAD methods [12,16,20,24]. Figure 2 visualizes one disease detection result of our CFD model on the PANDA-MIL dataset. The disease score per instance/patch predicted by our method is close to the ground-truth annotations, in which the clear margin between cancerous and benign validates the robust prediction of the proposed CFD model."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.4,Ablation Study,"We analysis on why the CFD network performs better than other methods listed in Table 1 by ablating the contributed components in CFD. The ablation study in Table 2 is conducted on the PANDA-MIL dataset to evaluate the effectiveness of the memory bank and loss functions in our model. Row one in Table 2 indicates our CFD model without regularization, yet it has shown better AUC values than other methods besides the S3R. While employing the three loss functions for regularization, as described in Sect. 3.3, each loss function shows its improvement in our model performance. The contrastive loss contributes the most to AUC improvement, enabling our model to achieve the SOTA performance.  "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,5.0,Conclusion,"This paper casts disease detection as a MIL-based WVAD task and introduces Contrastive Feature Decoupling (CFD) network to learn a memory bank boosted with contrastive learning. With the learned feature atoms stored in the memory bank, our contrastive feature decoupling is able to decouple each snippet as normal and abnormal proxies. Further, the decoupled abnormal proxies highlight the abnormal feature ingredients for better reasoning the disease score. Our feature decoupling intrinsically fits the contrastive learning paradigm to define opposite training samples for model optimization. Besides, we introduce a new dataset of prostate cancer detection, i.e., PANDA-MIL, to provide a biomedical imaging dataset concerning a different pathological modality. Experiments demonstrate that our CFD network achieves new SOTA performance on the Polyp and PANDA-MIL datasets, indicating that our method effectively addresses the disease detection task across different pathological modalities."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,1.0,Introduction,"Curriculum learning methods in deep learning are inspired by human education and involve structuring the training data from easy to hard to teach networks progressively. However, developing a good difficulty metric or measurer for curriculum learning is a challenge. Recently, there are two main categories of approaches to designing the difficulty metrics. The first is that human experts quantify data difficulty based on data characteristics such as complexity [19]. These metrics not only require sufficient domain knowledge, but they also run the risk that metrics of difficulty from a human perspective may not be applicable to a learning model due to different decision boundaries between the model and the human [25].Another popular approach is difficulty measurers based on the network, including transfer learning [7,26] and loss function [14]. Transfer learning is the scoring of samples using predictions of a reference model on the same training data. These measurers are fixed and do not take feedback from the progress of the current model into account during the training process. Difficulty measurers based on loss function tend to select samples with small training losses as a priority to train the model. However, this type of approach suffers from the uncertainty of quantifying the difficulty of data due to insufficient training in the early stages. In addition, while deep learning models have achieved impressive performance in the medical image analysis field, there remain challenges in measuring and developing a model with low in-domain uncertainty. Recently, uncertainty estimation has emerged as an effective tool for measuring the indomain uncertainty of models. However, reducing the in-domain uncertainty of models is an active research direction [6]. Then, based on uncertainty estimates, we use UAS to sort all data from easy to hard and select easier samples to update the parameters of the network. This process ensures that the network is trained on easier samples at the beginning when it is less mature and gradually moves towards harder samples as the network improves.We propose a new approach to address the challenges of curriculum learning, which we call Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU). Our approach is motivated by two key observations: 1) sample difficulty is influenced by both the complexity of the data and the model's inability to explain data, related to in-domain uncertainty, and 2) reducing in-domain uncertainty by improving the learning process can boost model performance. To estimate in-domain uncertainty, we use a Dirichlet distribution classifier, which provides uncertainty estimates and predictions simultaneously. DCLU then sorts the training data from easy to hard based on in-domain uncertainty estimation, allowing the model to focus on easier samples first. Our approach does not require additional networks to capture uncertainty and is end-to-end.In particular, Our dynamic difficulty measurer (DDM) generates uncertainties and predictions for each image simultaneously. Uncertainties reflect the difficulty and we use these as the criteria for data rearrangement. Uncertainty estimation runs at each iteration to ingest the feedback of the current network. We also propose an effective uncertainty-aware sampling pacing function (UAS) to sort all training data according to the latest results of DDM and gradually introduce progressively harder samples to learn new parameter vectors and update the network until the entire dataset is covered. The full process of the proposed method is shown in Fig. 1. We evaluate our method on two medical image datasets ISIC 2018 task 3 and Chest-Xray8 (COVID- 19). Results indicate that our method outperforms other curriculum learning works. In addition, with our proposed approach, the uncertainty of the model can be mitigated effectively."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,2.0,Related Work,"In-Domain Uncertainty. In-domain uncertainty represents the uncertainty associated with inputs extracted from a data distribution equivalent to the training data distribution [6]. Deep learning models can experience in-domain uncertainty as they lack the necessary in-domain knowledge to interpret indomain samples. In-domain uncertainty is caused by two types of uncertainty: data uncertainty and model uncertainty. Data uncertainty represents the complexity of data, which is related to noise and variations in observations [6]. On the other hand, model uncertainty arises due to shortcomings of the model such as a poor fit to the training dataset or lack of knowledge [6].In-Domain Uncertainty Estimate. MC dropout [4] and deep ensemble [15] have emerged as two widely adopted techniques for estimating uncertainty in recent years. These methods need significant computing sources and extra metrics to quantify in-domain uncertainty. As our proposal exploits in-domain uncertainties from the current network at every iteration, we want to avoid using additional modules and metrics to obtain similar uncertainty estimates from the above methods and thereby reduce computational resource requirements. Therefore, we apply a classifier with Dirichlet distribution to gain direct both the predictions and uncertainty estimation simultaneously.Curriculum Learning. Bengio et al. [2] first brought curriculum learning into the field of machine learning, which has prompted quite a bit of interest in the field of computer vision [1,5,16,20,21]. Hacohen et al. [7] used the confidence score obtained from transfer learning or bootstrapping to determine a fixed curriculum. Self-paced learning(SPL) was proposed by Kumar et al. [14], which applied example-wise training loss at each iteration as the difficulty measurer. The drawback of SPL is that some easier data appear all the time since SPL tended to select data with lower current losses. Jiang et al. [11] indicated a novel curriculum learning method named self-paced curriculum learning (SPCL) that combined predefined curriculum learning and self-paced learning so that prior knowledge before training and information during training are used effectively. In addition, Kong et al. [12] presented a linear combination of the current model loss and prior knowledge to adapt the difficulty measurer of the current model. In our work, we want to build a dynamic difficulty measurer through the uncertainty estimates generated by each iteration of the network. This means our approach does not require the prior knowledge provided by the pre-trained model and is able to update the curricula during the training process."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.1,Overview,"In DCLU, we randomly input data into the dynamic difficulty measurer (DDM) at the first epoch to obtain the uncertainty for each data point. Next, we apply the uncertainty-aware sampling pacing function (UAS) to present the training data to the DDM in order of the ascending uncertainties to synchronously produce predictions and new uncertainty estimates. New uncertainties can be used as a criterion for sorting data in the next iteration. Additionally, our pacing function selects a fraction of easier samples and learns a parameter vector to update the network. With the training process progressing, the proportion of selected samples increases until it eventually comprises the entire dataset. The pseudo-code for our method is given in the Appendix."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.2,Dynamic Difficulty Measurer,"The difficulty measurer is used to measure the difficulty of the data to decide on the order of the training data, which is a crucial component of curriculum learning. Our dynamic difficulty measurer (DDM) is a multi-class classifier with Dirichlet distribution, based on evidential deep learning [22]. The setting of our work is focusing on the K-classes classification task. In detail, first of all, we assume e k ≥ 0 to be evidence of k-th output of the activation function (e.g. softplus [17]) of our DDM. Then, DDM assigns a belief mass b k for each category and an overall uncertainty mass u, which can be defined as (1):where K is the total number of classes. We allocate b k in correspondence to Dirichlet distribution with parameters α k = e k + 1. So, the belief mass and uncertainty can be formulated asAdditionally, the Dirichlet distribution of DDM can be defined with parameters α = [α 1 , ..., α K ] as (2):where K k=1 p k = 1 and 0 ≤ p 1 , ..., p K ≤ 1. B(α) is a K-dimensional multinomial beta function [13]. It represents the density of each probability distribution, which is based on parameters derived from the evidence vector [10]. The expected probability of the k-th output of the classifier can be formulated as (3):Thus, DDM can generate both predictions and in-domain uncertainty estimates for each sample simultaneously. To be specific, in-domain uncertainty estimates include data and model uncertainty. Data uncertainty cannot be eliminated with training and model uncertainty can be reduced by improving the learning process [6]. Inspired by this phenomenon, we employ in-domain uncertainty to measure the difficulty of data at each iteration. This not only allows the data uncertainty as a prior for the criterion of difficulty measure but also allows the measure of data difficulty to be updated efficiently based on model uncertainty estimated from the current state of the model. The method can achieve a dynamic curriculum."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.3,Uncertainty-Aware Sampling Pacing Function,"The pacing function is based on using the difficulty measurer to determine how training examples (data) are fed into the network during the training process. Our dynamic difficulty measurer (DDM) can provide difficulty scores for all data at each iteration. However, some existing pacing functions attempt to partition the dataset into multiple subsets and gradually feed them into the network during training -this fails to satisfy the requirement of our difficulty measurer. To address this problem, we propose the uncertainty-aware sampling pacing function (UAS) consisting of two modules: the reorder and sampling modules. Within the reorder module, UAS sorts all training data from easy to hard according to ascending uncertainties from DDM at the last epoch and sends them into the network to yield predictions and new uncertainty estimates. The sampling module specifies a fraction of easier samples to update to the network. The weight assigned to the selected examples is set to 1, while the weight for other data is set to 0. These weights α will be applied to the objective function, which allows the parameters learned from the specified examples to update the network. We increase the fraction exponentially in each epoch until it eventually comprises the entire dataset.In our work, we implement UAS through two approaches. Firstly, UAS (exponential) incorporates both the reorder and sampling modules, prompting the network to prioritize learning easier examples during the initial stages of training and then gradually learn more difficult examples. In each epoch, UAS (exponential) first utilizes the reorder module to sort all training data from easy to hard. Then, it applies the sampling module to select a fraction of easier examples to update the network. Additionally, UAS (full) only contains the reorder module to enable the network to learn all sorted data, ranging from easy to hard examples."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.4,Loss Function,"Assume D(p k |α k ) is the prior on the cross-entropy loss. The classification loss function for each sample can be formulated by the Bayes risk by (4):where ψ is the digamma function, p ik is the estimated probability of the k-th class of the i-th sample. Additionally, Kullback-Leibler (KL) divergence is used to reduce the total evidence to zero under the condition of incorrect classification, which can be denoted by ( 5):(5) where Γ denotes the gamma function. To achieve pace control and reduce the effects of overfitting due to easier data that may always appear at the beginning of training, the final objective function is:where α i is the weight from the UAS pacing function to control the pace, λ t = min(1, t/50) is the annealing parameter, t is the current training epoch and w is weights of the network."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,4.1,Dataset and Experimental Setup,"Dataset. We evaluated our method on two public medical image datasets including ISIC 2018 Task 3 [8,23]  Evaluation Metrics. For both datasets, the performance of diagnosis is evaluated with both accuracy and F1 score. Moreover, for the assessment of uncertainty estimation, we apply expected calibration error (ECE) as the metric.Implementation Details. We employ ResNet-18 [9] as the backbone for both tasks. In our experiments, we used the TensorFlow framework and trained on an NVIDIA 3090 GPU with 32G of RAM. An Adam optimizer with β = 0 and α = 0.99 and a linear learning rate scheduler are used to tune the network. The initial learning rate is 0.0001 and the learning rate is decreased to 0.00001 after 20 epochs with batch size 16. The total epoch is 50."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,4.2,Experimental Results,"Comparison with State-of-the-Art. Our method is compared with various curriculum learning methods: 1) Vanilla samples batches randomly on the entire dataset without any curriculum learning techniques; 2) Fixed curriculum learning (FCL) [7] sorts data with the confidence score derived from transfer learning; 3) Self-pace learning (SPL) [14] chooses the data with minimum losses to train the network in advance; 4) Self-pace curriculum learning (SPCL) [11] is the combination of self-paced learning and predefined curriculum learning; 5. Adaptive Curriculum learning (Adaptive CL) [12] adjusts the difficulty measurer by incorporating the prior knowledge and feedback from the current model. The results in Table 1 show that our method (DDM) with UAS (full) outperforms both datasets. Our method with UAS (exponential) performs better than other methods on Chest-Xray 8 (COVID-19) and has the second best performance on ISIC 2018 Task 3. The performance gap between our two methods may be Evaluation In-Domain Uncertainty. To verify that our method is effective in reducing in-domain uncertainty, our method is compared with vanilla. Figure 2 shows that our method is more robust than vanilla. Details of the comparison with other curriculum learning methods are referred to in the Appendix. our method performs 8.93% better than FCL using the same exponential pacing functions. This indicates that DDM performs quite well compared to the fixed curriculum obtained by transfer learning. Moreover, In order to demonstrate that the order generated by DDM is robust, we have conducted experiments on different backbones, details of which can be found in the Appendix. Next, we compare our proposed pacing function to three pacing functions, which are fixed exponential, single-step and linear pacing functions [7]. Both single-step and linear pacing functions divide the entire data set into subsets and put them into the model progressively. The difference between the two is in the number of subsets. Single-step pacing function divides into two subsets and the linear pacing function splits into several subsets (e.g. 3 subsets). The results in Table 2 show that UAS is more suited to DDM than the other pacing functions. When the other three pacing functions are used, new samples added to the current subset will disrupt the existing order of data when the size of the subset is increased. It causes the network to have to relearn and reorder the new subset. Finally, We compare the effect of using the loss function with and without L2 regular term on the model performance, finding that using the loss function with L2 can eliminate the effect of overfitting."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,5.0,Conclusion,"Our approach, called Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU), introduces a new perspective on curriculum design by utilizing the current stage of the network to estimate the difficulty of data based on its in-domain uncertainty. To support this, we also introduce an uncertainty-aware sampling pacing function that is compatible with our dynamic difficulty measurer. Our experimental results confirm that DCLU is successful in reducing uncertainty, and we demonstrate the effectiveness of our approach on two medical image datasets through extensive experimentation."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,,Table 1 .,Fig. 2. Compare our method and vanilla under ECE metric on Chest-Xray 8 (COVID-19)
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,,Table 2 .,"Ablation Study. We present the results of ablation studies on Chest-Xray 8 (COVID-19) in Table2to show the effectiveness of key components, i.e., DDM, UAS pacing function and the loss function with L2 regular term. First of all,"
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,1.0,Introduction,"Brain functional networks characterize functional interactions of human brain, where brain regions correspond to nodes and functional interactions between brain regions are considered as edges. Brain functional networks are widely utilized to classify brain diseases, including Alzheimer's disease [13], attention deficit hyperactivity disorder (ADHD) [7], major depressive disorder [4] and schizophrenia [26]. In these studies, various network characteristics, e.g., degree, clustering coefficient [17], ordinal pattern [16,27] are utilized to represent brain functional network and then used to calculate network measurements for classify brain diseases. However, these network characteristics are built on local connections (e.g., node degree) and ignore the transfer of information between brain regions and the global geometric information in brain functional networks.Recently, Wasserstein distance has attracted broad attention in image processing [6], computer vision [18] and neural network [1]. The Wasserstein distance, also known as the optimal transport distance or Earth Mover's distance [28], was originally proposed to investigate the translocation of masses [8]. In mathematics, the Wasserstein distance is used to quantify the dissimilarity between two probability distributions based on a given ground metric [11,28]. Graph data can be represented as probability distributions using graph or network embedding methods [20]. Graph kernels based on the Wasserstein distance can capture the global geometric information of graphs, making them useful for measuring the similarities between graphs, including brain networks. For example, in the graph kernel based on Wasserstein distance [19], Wasserstein distance is used to measure the similarity of bag-of-vectors between two graphs where the eigen-decompositions of adjacency matrix are used to represent the graphs as bag-of-vectors. In Wasserstein Weisfeiler-Lehman graph kernels [22], the Weisfeiler-Lehman scheme is used to generate node label sequences, and the Wasserstein distance is utilized to measure the similarity of label sequences between two graphs. In the optimal transport based ordinal pattern tree kernel [16], optimal transport distance (i.e., Wasserstein distance) is used to measure the similarity of ordinal pattern tree in brain functional networks and is then applied to classify brain diseases. However, these graph kernels based on Wasserstein distance ignore the transfer of information between brain regions, and their positive definition is hardly guaranteed. To tackle these problems, we develop a sliced Wasserstein graph kernel to measure the similarity of brain functional networks. Firstly, we use Laplacian embedding as a feature projection function to project each brain functional network into a set of points in Euclidean space. Then, we calculate the eigendecomposition on these embeddings and acquire the Laplacian eigenvalues and eigenvectors for a brain functional network. On Laplacian eigenvectors, each row of Laplacian eigenvectors is the representation of a node in brain functional network. We utilize the Wasserstein distance to measure the cost of transporting information between nodes based on these Laplacian eigenvectors. At last, we calculate one-dimensional empirical measures for the points in Euclidean space and calculate sliced Wasserstein graph kernel. We apply the proposed sliced Wasserstein graph kernel to support vector machine (SVM) classifier [14] for brain disease diagnosis. Figure 1 presents the schematic diagram of the proposed framework with each network representing a specific subject. Specifically, our work has following advantages:-We provide a new approach for investigating the transfer of information between brain regions with sliced Wasserstein distance. -The proposed sliced Wasserstein graph kernel is positive definite and a faster method for comparing brain functional networks. -The proposed sliced Wasserstein graph kernel can improve classification accuracy compared to state-of-the-art graph methods for classifying brain diseases."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.1,Data and Preprocessing,"The functional network data used in the experiments are based on three datasets of brain diseases: ADHD1 , autistic spectrum disorder (ASD)2 , and early mild cognitive impairment (EMCI) 3 . The ADHD dataset consists of 121 ADHD patients and 101 normal controls (NCs). The ASD dataset includes 36 ASD patients and 38 NCs. The EMCI dataset includes 56 EMCI patients and 50 NCs. These brain network data were generated using resting-state functional magnetic resonance imaging (rs-fMRI) data [24]. The rs-fMRI data underwent several preprocessing steps, including brain skull removal, motion correction, temporal pre-whitening, spatial smoothing, global drift removal, slice time correction, and bandpass filtering. Following this, the entire cortical and subcortical structures of the brain were subdivided into 90 brain regions for each subject, based on the Automated Anatomical Labeling atlas. The linear correlation between the mean time series of a pair of brain regions was then calculated to measure the functional connectivity. Finally, a 90 × 90 fully-connected weighted functional network was constructed for each subject. In this work, we remove the negative connections from brain functional networks."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.2,Sliced Wasserstein Graph Kernel,"Sliced Wasserstein graph kernel is used to measure the similarity of paired brain functional networks. In this subsection, we firstly introduce graph sliced Wasserstein distance and then use it to calculate sliced Wasserstein graph kernel.Throughout the paper, we will refer to brain functional network when mentioning the graph, unless noted otherwise.In image processing, computer vision, and graph comparison, many efficient algorithms of machine learning are available in Euclidean space [10]. We define graph sliced Wasserstein distance in Euclidean space. In other words, Euclidean space is as metric space, i.e., M ⊆ R d , and we use Euclidean distance as the ground distance, i.e., d(x, y) = |x -y|, when defining Wasserstein and graph sliced Wasserstein distances. Here, we provide the definition of the Wasserstein distance in Euclidean space.Let r and c be two probability measures on R d . The Wasserstein distance between r and c is defined aswhere p ∈ [1, ∞] and Γ (r, c) denotes the set of all transportation plans of r and c.The sliced Wasserstein kernel on Wasserstein distance has been studied in [2,10]. In this paper, we extend sliced Wasserstein distance to graph domain and design graph sliced Wasserstein distance and sliced Wasserstein graph kernel."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Graph Sliced Wasserstein Distance: Given two graphs,"and c Φ(G2) denote probability measures on d-dimensional feature representation of graph G 1 and G 2 . The graph sliced Wasserstein distance between G 1 and G 2 is defined aswhere Φ denotes the d-dimensional feature projection. r Φ(G1) and c Φ(G2) are the probability measures of graph G 1 and G 2 , respectively, in d-dimensional Euclidean space. The 2-sliced Wasserstein distance is defined aswhere g θ# r Φ(G1) and g θ# c Φ(G2) are the one-dimensional projections of the measure r Φ(G1) and c Φ(G2) . θ is a one dimensional absolutely continuous positive probability density function.Proof. According to [10], W 2 2 is distance metric which is symmetric, nonnegativity, identity of indiscernibles, and triangle inequality. D Φ GSW is an integral of W 2  2 terms. Hence, D Φ GSW is also a distance metric and satisfies nonnegativity, symmetry, identity of indiscernibles, and triangle inequality."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Algorithm 1. Compute sliced Wasserstein graph kernel,"Input: Two graphs G1, G2, parameter λ Output: kernel value KSW G(G1, G2) XG 1 ← Φ(G1); %Compute feature representations of graph G1 and G2% XG 2 ← Φ(G2); r ← r(XG 1 ); %Compute probability measure on feature representations%According to the definition of graph sliced Wasserstein distance in Eq.( 3), we can find the thought behind graph sliced Wasserstein distance is to achieve the one-dimensional representations for the probability measures on d-dimensional feature representation of graph. Then, the distance between two input probability measures is computed as a function on Wasserstein distance of their corresponding one-dimensional representations.Feature Projection: We use Laplacian embedding as a feature projection function to project a graph into a set of points in Euclidean space. Then, we calculate the eigen-decomposition on these embeddings and acquire the Laplacian eigenvalues and eigenvectors. On Laplacian eigenvectors, we construct the points in Euclidean space where each row of Laplacian eigenvectors is a node representation.In Eq.(3), r Φ(G1) and c Φ(G2) are the feature projection of graph G 1 and G 2 by using Laplacian embedding. Assume thatis the Laplacian eigenvectors of graph G 1 and G 2 . We reformulate the graph sliced Wasserstein distance as a sum rather than an integral. Following the work of [9], we calculate the graph sliced Wasserstein distance by sorting the samples and calculate the L 2 distance between the sorted samples. The graph sliced Wasserstein distance between r and c can be approximated from their node representation which is defined aswhere i[n] and j[n] are the indices of sorted>. By combining graph sliced Wasserstein distance and feature projection on graphs, we can construct a new graph kernel called sliced Wasserstein graph kernel which can be used to measure the similarity between the paired graphs."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Sliced Wasserstein Graph Kernel: Given two graphs,"and graph sliced Wasserstein distance on them (i.e., D Φ GSW (G 1 , G 2 )). We define the sliced Wasserstein graph (SWG) kernel asObviously, sliced Wasserstein graph kernel is a case of Laplacian kernel. The procedure of calculating SWG kernel is described in Algorithm 1. According the theorems in [10], one-dimensional Wasserstein space is a flat space. The graph sliced Wasserstein distance is induced from one-dimensional Wasserstein distance. Hence, the graph sliced Wasserstein distance is isometric. SWG kernel based on graph sliced Wasserstein distance is positive definite. Theorem 2. Sliced Wasserstein graph kernel is positive definite and differentiable for all λ > 0.Proof. The sliced Wasserstein graph kernel is the extension of sliced Wasserstein kernel on graphs. According to [2,10], sliced Wasserstein kernel is positive definite. Hence, sliced Wasserstein graph kernel is also positive definite."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.3,Sliced Wasserstein Graph Kernel Based Learning,"We use the image processing method described in the data preprocessing to analyze the rs-fMRI data for all subjects and create a brain functional network for each subject. In these brain networks, brain regions are represented as nodes, while the functional connections between paired brain regions are represented as edges. After constructing the brain functional networks for all subjects, we compute the sliced Wasserstein graph kernel using Eq.( 5) and apply SVM for disease classification."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.1,Experimental Setup,"In the experiments, we compare our proposed method with the state-of-the-art graph methods including graph kernels and graph neural networks. Graph kernels include Weisfeiler-Lehman subtree (WL-ST) kernel [21], Weisfeiler-Lehman shortest path (WL-SP) kernel [21], random walk (RW) kernel [23], Wasserstein Weisfeiler-Lehman (WWL) kernel [22], GraphHopper (GH) kernel [5], depthfirst-based ordinal pattern (DOP) kernel [15], optimal transport based ordinal pattern tree (OT-OPT) kernel [16]. Graph neural networks include DIFFPOOL [25] and BrainGNN [12]. SVM [3] as the final classifier is exploited to conduct the classification experiment. We perform the leave-one-out cross-validation for all the classification experiments. In the experiments, uniform weight λ is chosen from "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.2,Classification Results,"We compare the proposed SWG kernel with the state-of-the-art graph kernels on three classification tasks, i.e., ADHD vs. NCs, ASD vs. NCs and EMCI vs. NCs classification. Classification performance is evaluated by accuracy (ACC) and area under receiver operating characteristic curve (AUC). The classification results are shown in Table 1. From Table 1, we can find that our proposed method achieves the best performance on three tasks. For instance, the accuracy achieved by our method is respectively 78.83%, 90.54%, and 85.44% in ADHD vs. NCs, ASD vs. NCs and EMCI vs. NCs classification, which is better than the second best result obtained by BrainGNN and DIFFPOOL. This demonstrates that the proposed SWG kernel is good at distinguishing the patients with brain diseases (i.e., ADHD, ASD, and EMCI) from NCs, compared with the state-of-the-art graph kernels and graph neural networks. We select four graph methods, including DOP kernel, OT-OPT kernel, BrainGNN and DIFFPOOL, whose classification accuracies are superior to those of other graph methods, except for the SWG kernel. We have recorded the computational time required by these methods, as shown in Table 2. The results in  Table 2 demonstrate that our proposed method can enhance computational efficiency as compared to other graph kernels and graph neural networks for the classification of brain diseases."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.3,Analysis on Wasserstein Distance,"The Wasserstein distance was initially proposed to examine mass translocation [28]. In this subsection, we utilize sliced Wasserstein distance to investigate the transportation cost of brain functional networks and brain regions (i.e., nodes). We respectively selected 10 patients and 10 NCs from each dataset, and calculated sliced Wasserstein distances for all selected subjects. We generated a matrix heatmap of the sliced Wasserstein distances for ADHD, ASD, and EMCI, as shown in Fig. 2. The results in Fig. 2 indicate that the sliced Wasserstein distances of patients with brain diseases (e.g., ADHD, ASD, and EMCI) are greater than those of NCs. These results suggest that the transportation cost of information transfer in the brain for patients is higher than that of NCs. We use Eq.( 4) to calculate the sliced Wasserstein distances between brain regions for ADHD, ASD and EMCI and then conduct statistical analysis on these distances. We identify three important brain regions where the sliced Wasserstein distances of patients with brain diseases significantly differed from those of NCs. In ADHD, important brain regions involve right paracentral lobule (PCL.R), left parahippocampal gyrus (PHG.L), and right angular gyrus (ANG.R). In ASD, important brain regions involve left superior frontal gyrus-medial orbital (ORBsupmed.L), right posterior cingulate gyrus (PCG.R), and right superior temporal gyrus (STG.R). In EMCI, important brain regions include right supplementary motor area (SMA.R), right superior occipital gyrus (SOG.R), and right inferior temporal gyrus (ITG.R)."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,4.0,Conclusion,"In this paper, we propose a sliced Wasserstein graph kernel to measure the similarities between a pair of brain functional networks. We use this graph kernel to develop a classification framework of brain functional network. We perform the classification experiments in the brain functional network data including ADHD, ASD, and EMCI constructed from fMRI data. The results indicate that our proposed method outperforms the existing state-of-the-art graph kernels and graph neural networks in classification tasks. In computational speed, the proposed method is faster than latest graph kernels and graph neural networks."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,1.0,Introduction,"Medical lesion detection plays an important role in assisting doctors with the interpretation of medical images for disease diagnosing, cancer staging, etc., which can improve efficiency and reduce human errors [9,19]. Current object detection approaches are mainly based on supervised learning with abundant well-paired image-level annotations, which heavily rely on expert-level knowledge. As such, these supervised approaches may not be suitable for medical lesion detection due to the laborious labeling.Recently, large-scale pre-trained vision-language models (VLMs), by learning the visual concepts in the images through the weak labels from text, have prevailed in natural object detection or visual grounding and shown extraordinary performance. These models, such as GLIP [11], X-VLM [10], and VinVL [24], can perform well in detection tasks without supervised annotations. Therefore, substituting conventional object detection with VLMs is possible and necessary. The VLMs are first pre-trained to learn universal representations via large-scale unlabelled data and can be effectively transferred to downstream tasks. For example, a recent study [15] has demonstrated that the pre-trained VLMs can be used for zero-shot medical lesion detection with the help of well-designed prompts.However, current existing VLMs are mostly based on a single prompt to establish textual and visual alignment. This prompt needs refining to cover all the features of the target as much as possible. Apparently, even a well-designed prompt is not always able to combine all expressive attributes into one sentence without semantic and syntactic ambiguity, e.g., the prompt design for melanoma detection should include numerous kinds of information describing attributes complementing each other, such as shape, color, size, etc [8]. In addition, each keyword in a single lengthy prompt cannot take effect equally as we expect, where the essential information can be ignored. This problem motivates us to study alternative approaches with multiple prompt fusion.In this work, instead of striving to design a single satisfying prompt, we aim to take advantage of pre-trained VLMs in a more flexible way with the form of multiple prompts, where each prompt can elicit respective knowledge from the model which can then be fused for better lesion detection performance. To achieve this, we propose an ensemble guided fusion approach derived from clustering ensemble learning [3], where we design a step-wise clustering mechanism to gradually screen out the implausible intermediate candidates during the grounding process, and an integration module to obtain the final results by uniting the mutually independent candidates from each prompt. In addition, we also examine the language syntax based prompt fusion approach as a comparison, and explore several fusion strategies by first grouping the prompts either with described attributes or categories and then repeating the fusion process.We evaluate the proposed approach on a broad range of public medical datasets across different modalities including photography images for skin lesion detection ISIC 2016 [2], endoscopy images for polyp detection CVC-300 [21], and cytology images for blood cell detection BCCD. The proposed approach exhibits extraordinary superiority compared to those with single prompt and other common ensemble learning based methods for zero-shot medical lesion detection. Considering the practical need of lesion detection, we further provide significantly improved fine-tuning results with a few labeled examples."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,2.0,Related Work,"Object Detection and Vision-Language Models. In the vision-language field, phrase grounding can be regarded as another solution for object detection apart from conventional R-CNNs [5,6,18]. Recently, vision-language models have achieved exciting performance in the zero-shot and few-shot visual recognition [4,16]. GLIP [11] unifies phrase grounding and object detection tasks, demonstrating outstanding transfer capability. In addition, ViLD [7] is proposed for open-vocabulary object detection taking advantage of the rich knowledge learned from CLIP [4] and text input.Ensemble Learning. As pointed out by a review [3], ensemble learning methods achieve better performance by producing predictions based on extracted features and fusing via various voting mechanisms. For example, a selective ensemble of classifier chains [13] is proposed to reduce the computational cost and the storage cost arose in multi-label learning [12] by decreasing the ensemble size. UNDEED [23], a semi-supervised classification method, is presented to increase the classifier accuracy on labeled data and diversity on unlabeled data simultaneously. And a hybrid semi-supervised clustering ensemble algorithm [22] is also proposed to generate basic clustering partitions with prior knowledge."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.0,Method,"In this section, we first briefly introduce the vision-language model for unifying object detection as phrase grounding, e.g., GLIP [11] (Sect. 3.1). Then we present a simple language syntax based prompt fusion approach in Sect. 3.2. Finally, the proposed ensemble-guided fusion approach and several fusion strategies are detailed in Sect. 3.3 to improve the zero-shot lesion detection."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.1,Preliminaries,"Phrase grounding is the task of identifying the fine-grained correspondence between phrases in a sentence and objects in an image. The GLIP model takes as input an image I and a text prompt p that describes all the M candidate categories for the target objects. Both inputs will go through specific encoders Enc I and Enc T to obtain unaligned representations. Then, GLIP uses a grounding module to align image boxes with corresponding phrases in the text prompt. The whole process can be formulated as follows:where O ∈ R N ×d , P ∈ R M ×d denote the image and text features respectively for N candidate region proposals and M target objects, S ground ∈ R N ×M represents the cross-modal alignment scores, and T ∈ {0, 1} N ×M is the target matrix."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.2,Language Syntax Based Prompt Fusion,"As mentioned above, it is difficult for a single prompt input structure such as GLIP to cover all necessary descriptions even through careful designation of the prompt. Therefore, we propose to use multiple prompts instead of a single prompt for thorough and improved grounding. However, it is challenging to combine the grounding results from multiple prompts since manual integration is subjective, ineffective, and lacks uniform standards. Here, we take the first step to fuse the multiple prompts at the prompt level. We achieve this by extracting and fusing the prefixes and suffixes of each prompt based on language conventions and grammar rules. As shown in Fig. 1 (a), given serials of multiple prompts p 1 , p 2 , . . . , p k , the final fused prompt P fuse from k single prompts is given by:(2)"
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.3,Ensemble Learning Based Fusion,"Although the syntax based fusion approach is simple and sufficient, it is restricted by the form of text descriptions which may cause ambiguity in the fused prompt during processing. Moreover, the fused prompts are normally too long that the model could lose proper attention to the key information, resulting in extremely unstable performance (results shown in Sect. 4.2).Therefore, in this subsection, we further explore fusion approaches based on ensemble learning. More specifically, the VLM outputs a set of candidate region proposals C i for each prompt p i , and these candidates carry more multidimensional information than prompts. We find in our preliminary experiments that direct concatenation of the candidates is not satisfactory and effective, since simply integration hardly screens out the bad predictions. In addition, the candidate, e.g., c ij ∈ C i , carries richer information that can be further utilized, such as central coordinate x j and y j , region size w j and h j , category label, and prediction confidence score. Therefore, we consider step-wise clustering mechanisms using the above information to screen out the implausible candidates based on clustering ensemble learning [3].Another observation in our preliminary experiments is that most of the candidates distribute near the target if the prompt description matches better with the object. Moreover, the candidate regions of inappropriate size containing too much background or only part of the object should be abandoned directly. As such, we consider clustering the center coordinate (x j , y j ) and region size (w j , h j ) respectively to filter out those candidates with the wrong location and size.This step-wise clustering with the aid of different features embodies a typical ensemble learning idea. Therefore, we propose a method called Ensemble Guided Fusion based on semi-clustering ensemble, as detailed in Fig. 1 (b). There are four sub-modules in our approach, where the location cluster f loc and size cluster f size discard the candidates with large deviations and abnormal sizes. Then, in the prediction corrector f correct , we utilize the voting mechanism to select the remaining candidates with appropriate category tags and relatively high prediction confidence. After the first three steps of processing, the remaining candidates C originated from each prompt can be written as:The remaining candidates are then transferred to the integration module for being integrated into the final fused result C fuse that is mutually independent:Besides, we also propose three fusion strategies to recluster candidates in different ways before executing ensemble guided fusion, i.e., fusing the multiple prompts equally, by category, and by attribute. Compared to the first strategy, fusing by category and by attribute both have an additional step of reorgnization. Candidates whose prompts belong to the same category or have identical attributes will share the similar distribution. Accordingly, we rearrange these candidates C i into a new set C for the subsequent fusion process."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,4.1,Experimental Settings,"We collect three public medical image datasets across various modalities including skin lesion detection dataset ISIC 2016 [2], polyp detection dataset CVC-300 [21], and blood cell detection dataset BCCD to validate our proposed approach for zero-shot medical lesion detection. For the experiments, we use the GLIP-T variant [11] as our base pre-trained model and adopt two metrics for the grounding evaluation, including Average Precision (AP) and AP50. More details on the dataset and implementation are described in the appendix. Multiple NMS [14] 12.0 20.6 27.9 37.9 11.9 21.4 Soft-NMS [1] 18.8 30. 3  "
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,4.2,Results,"This section demonstrates that our proposed ensemble guided fusion approach can effectively benefit the model's performance.The Proposed Approach Achieves the Best Performance in Zero-Shot Lesion Detection Compared to Baselines. To confirm the validity of our method, we conduct extensive experiments under the zero-shot setting and include a series of fusion baselines: Concatenation, Non-Maximum Suppression (NMS) [14], Soft-NMS [1] and Weighted Boxes Fusion (WBF) [20] for comparisons. As illustrated in Table 1, our ensemble guided fusion rivals the GLIP [11] with single prompt and other fusion baselines across all datasets. The first three rows in Table 1 represent the results of single prompt by only providing shape, color, and location information, respectively. Furthermore, we conduct a comparison between YOLOv5 [17] and our method on CVC-300 under 10-shot settings. Table 2 shows that our method outperforms YOLOv5, which indicates fullysupervised models such as YOLO may not be suitable for medical scenarios where a large labeled dataset is often not available. In addition, we utilize the Automatic Prompt Engineering (APE) [25] method to generate prompts. These prompts give comparable performance to our single prompt and can be still be improved by our fusion method. And the details are described in the appendix.    Here we present part of the single prompts used in the experiments for illustration. The misclassification problem in some of the single prompts is corrected (i.e., malignant to benign) on the first dataset. For all datasets, the candidate boxes are more precise and associated with higher confidence scores.Fine-Tuned Models Can Further Improve the Detection Performance. We conduct 10-shot fine-tuning experiments as a complement, and find the performance greatly improved. As shown in Table 3 and Fig. 2, with the same group of multiple prompts, the accuracy of fine-tuned model has increased almost twice as much as that of zero-shot, further demonstrating the effectiveness of our method in both settings. Therefore, we can conclude that the pre-trained GLIP model has the ability to learn a reasonable alignment between textual and visual modalities in medical domains.Visualizations. Figure 3 shows the visualization of the zero-shot results across three datasets. Syntax based fusion sometimes fails to filter out unreasonable predictions because these regions are generated directly by the VLM without further processing and eventually resulting in unstable detection performance. On the contrary, our approach consistently gives a better prediction that defeats all single prompts with a relatively proper description, yet syntax based fusion relies too much on the format and content of inputs, which results in great variance and uninterpretability. The step-wise clustering mechanism based on ensemble learning enables our method to exploit multi-dimensional information besides visual features. In addition, the key components in our proposed approach are unsupervised, which also enhances stability and generalization.Comparison Among Fusion Strategies. In this work, we not only provide various solutions to fuse multiple prompts but also propose three fusion strategies to validate the generalization of ensemble-guided fusion. As shown in Table 4, we present the results obtained with three different fusion strategies: equally, by category, and by attribute. The first strategy is to process each prompt equally, which is the most convenient and suitable in any situation. Fusing prompts by category is specifically for multi-category datasets to first gather the prompts belonging to the same category and make further fusion. Similarly, Fusing by attribute is to fuse the candidates, whose prompts are describing the same attribute. The strategy of fusing by attribute, outperforms the other ones, due to the fact that candidates with the same attribute share a similar distribution, which is prone to obtain a more reasonable cluster. On the contrary, it is possible to neglect this distribution when fusing each prompt equally.Ablation Study. As shown in Table 5, we perform ablation studies on three datasets. Our approach has three key components, i.e., location cluster, size cluster and prediction corrector. The location cluster filters out the candidates with severe deviation from the target. The size cluster removes those abnormal ones. Finally, the prediction corrector further eliminates the candidates that cause low accuracy. The results show that when combining the above three components, the proposed approach gives the best lesion detection performance, suggesting that all components are necessary and effective in the proposed approach."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,5.0,Conclusion,"In this paper, we propose an ensemble guided fusion approach to leverage multiple text descriptions when tackling the zero-shot medical lesion detection based on vision-language models and conduct extensive experiments to demonstrate the effectiveness of our approach. Compared to a single prompt that typically requires exhaustive engineering and designation, the multiple medical prompts provide a flexible way of covering all key information that help with lesion detection. We also present several fusion strategies for better exploiting the relationship among multiple prompts. One limitation of our method is that it requires diverse prompts for effective clustering of the candidates. However, with the help of other prompt engineering methods, the limitation can be relatively alleviated."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_28.
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,1.0,Introduction,"Weakly supervised anomaly detection holds significant potential in real-world clinical applications [27], particularly for new pandemic diseases where obtaining pixel-wise annotations from human experts is challenging or even impossible [16]. However, dominant anomaly detection methods based on the one-class classification paradigm [8,11] often overlook the binary labels of healthy and disease samples available in clinical centers, limiting their detection granularity. Traditional clustering techniques like z-score thresholding [15], PCA [14], and SVM [26] have limited clinical value as they mainly generate image-level anomaly results. To address this paradox, deep generative models leverage additional pixel-wise domain knowledge captured through adversarial training [1,12,24] or latent representation encoding [3,4,6,32]. While these approaches allow obtaining pixelwise anomaly maps through Out-Of-Distribution (OOD) detection, they often fail to generate high-fidelity segmentation maps due to inadequate utilization of image conditions. Thus, the utilization of domain knowledge from weakly supervised data becomes a crucial factor in achieving high-quality anomaly detection."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,,Fig. 1. Non-Markovian Diffusion Framework:,"To enhance information transfer, we make an assumption that the forward states and reverse states at the same time step t follow different distributions. Based on this assumption, we introduce conditions in the forward process, where all previous states are used as conditions for the current state. This enables efficient information transfer between states. And the final state xT incorporates comprehensive information from the forward states. Similarly, the reverse process incorporates information from previous reverse states.Moreover, traditional generative models, such as GANs [1,12] and normalizing flows [11], commonly used for pixel-wise anomaly detection, are constrained by one-step data projection in handling complex data distributions. This dilemma can be overcome by employing probabilistic diffusion models [13,25] that capture data knowledge through a series of step-by-step Markovian processes [5]. The high-fidelity generation proficiency, flexible network architecture, and stable training scheme of existing diffusion-based anomaly detection approaches have demonstrated promising performance in detecting anomaly regions [23,[27][28][29]. However, diffusionnv-based approaches have high computational costs due to iterative evaluations. [20] explores the diffusion on the latent space with smaller sizes instead of pixel space to reduce the computations. Moreover, all these methods still face challenges including the lack of fine-grained guidance and gradual loss of anatomical information in Markovian chains. To address these limitations, we propose a novel non-Markovian hybridconditioned diffusion model with fast samplers. Our approach utilizes strong hybrid image conditions that provide powerful sampling guidance by integrating coarse segmentation maps and original instance information based on the non-Markovian assumption (as shown in Fig. 1). Additionally, we modify the forward and reverse process as a higher-order deterministic Ordinary Differential Equation (ODE) sampler to accelerate inference. We validate our framework on two brain medical datasets, demonstrating the effectiveness of the framework components and showing more accurate detection results of anomaly regions."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.0,Method,"In this section, we present a fast non-Markovian diffusion model that utilizes pixel-wise strong conditions and encoding/sampling accelerator for anomaly segmentation to enhance generation fidelity and sampling speed. Section 2.1 introduces the non-Markonvian model and hybrid conditions for guided sampling. Section 2.2 proposes the acceleration approach for encoding and sampling."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.1,Non-Markovian Diffusion Model with Hybrid Condition,"Learning deterministic mappings between diseased and healthy samples sharing the same anatomical structures is essential to enhance inaccurate and timeconsuming diffusion-based approaches, which require strong guidance during sampling. However, current diffusion-based models only provide insufficient conditions (such as binary classification results), leading to vague anomaly distributions. To achieve consistent and stable generation, we propose a hybrid conditional diffusion model dependent on the non-Markovian assumption. It injects noise into the original distribution sequentially using the Gaussian distribution and then reconstructs the original distribution by reverse sampling. Following the expression of [13], the Markovian-based diffusion framework is defined as:where the discrete states {x t } T t=0 are from step 0 to T , forward step q and trained reverse step p θ have one-to-one mapping. Denoting {α t } T t=0 and {σ t } T t=0 as variance scales for noise perturbation, the Gaussian transition kernels are:To keep the anatomical information across states, the proposed non-Markovian anatomy structure mappings are built by adding previous-state information into forward and reverse states, which preserves distribution prior for high-quality reconstruction. Denoting all accumulated states from the forward process as c and the state in the backward step t as xt , we formulate the Generalized Non-Markovian Diffusion Framework (GNDF) as:Similar to vanilla DDPM [13], our conditional noise prediction network is trained according to the negative log-likelihood (NLL) lower bound minimization of generated distributions. It is further transformed into the L2 loss between the estimated conditional noise and the ground-truth Gaussian noise as:To enable the diffusion model to effectively differentiate between anatomical and anomaly information from previous states, we introduce a hybrid condition that includes the input state x 0 , coarse segmentation maps, and classifier gradients derived from healthy labels. In order to simplify the computational complexity and leverage the rich information contained in x 0 , we replace the forward state conditions c with the original state x 0 .Regarding hybrid condition implementation, we train a binary classifier with healthy and diseased images to provide further guidance on anomaly regions independently, following class-conditional methods [10,27]. A memory bank [21] is applied to store representative features of healthy samples, which enables quick generation of coarse segmentation maps x seg during the testing phase, addressing the issue of knowledge forgetting in original diffusion models. To keep the active segmentation map as a condition in the diffusion model training, a health image x 0 is transformed into a diseased image x n 0 based on a random x seg . Overall, we train our non-Markovian diffusion model depending on the current states, the coarse segmentation maps, image labels, and the original data during the diffusion process with healthy and diseased images. The training objective is given by:where x t is the concatenation of xt , x n 0 , and x seg . y denotes the corresponding binary label for each data. ω(t) is a weighted function for providing dynamic weights to explore density regions. In this work, we simply set ω(t) as 1. The full framework of our model is shown in Fig. 2."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.2,Accelerated Encoding and Sampling,"Diffusion acceleration, which is equivalent to reconstruction error minimization [7], is implemented by balancing the trade-off between sample quality and computation cost. Existing diffusion-based anomaly detectors [27] require a significant number of evaluation steps for encoding and sampling, which makes them impractical for clinical applications. To address this paradox, we adapt an Ordinary Differential Equation solver [17] for fast and stable encoding & sampling independent of the complex trade-off.Following the setting of continuous-time Ito stochastic differential equation, where the drift coefficient, the diffusion coefficient, and the Wiener process are denoted as f (x, t), g(t), w, we have our forward process and probability flow ODE sampling [25] as:By decomposing the conditional sampling scheme according to Bayes Theorem, the guided diffusion process can be achieved by mixture guidance composed of conditional noise prediction model and classifier gradient:Denote the classifier as C, the binary label as y, the signal-to-noise coefficient as λ, and the conditional noise prediction network as θ x , λ, y , we further have the hybrid conditional diffusion network ˆ θ as: ˆ θ (x , λ, y) := θ (x , λ, y) + s • C(x t , t, y).(8) The semi-linear probability flow ODE is solved reversely with second-order multi-step numerical methods [18]. Then, we apply hybrid conditional sampling to noisy data xt to reproduce a healthy one with the same anatomy structure by conditional data prediction network and the binary classifier [10]. Following the symbol of [17,18], the conditional sampling is:Finally, we post-process the reconstructed samples by subtracting original inputs and performing Otsu's threshold to obtain the anomaly segmentation map. "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.1,Dataset and Evaluation Metric,"BRATS 2020 [2] is a brain tumor segmentation dataset containing the MR sequences of T1, T1Gd, T2, and FLAIR. Following the preprocessing approach of [27], we concatenate all image modalities along the channel dimension, prune the upper and lower axial slices, and pad each slice into 256 × 256. All tumor classes are merged into a single class in the segmentation mask. The training set contains 10,410 slices with tumors and 5,809 healthy slices. The testing set includes 1,316 images with tumors. ISLES 2022 [19] is an MR image dataset for stroke lesions segmentation. It contains ischemic strokes of various sizes and from different disease stages. We extract the axial slices from DWI sequence and resize them into 256 × 256. The training set includes 2,707 healthy slices and 1,483 slices with lesions. The testing set contains 282 slices with lesions. Note that only the image-level binary labels are used in our training. The evaluation metrics include Dice, Volumetric Similarity, and Hausdorff Distance, which are calculated in slice level and volume level for BRATS and ISLES, respectively."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.2,Implementation Details,"To fairly compare with previous state-of-the-art diffusion models, we use the same network architectures as DiffANO [27]. We set the batch sizes of diffusion model and classifier as 3 and 10, respectively. The Adam optimizer with a learning rate of 0.0001 is used to train the diffusion model for 130,000 iterations and train the classifier for 150,000 iterations. The training diffusion step is 1000. The forward encoding and sampling steps are both set to 50 in the inference. We randomly generate the lesion masks as [30] and corrupt the corresponding regions on the conditioning images in the training phase."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.3,Comparison with State-of-the-Art Methods,"To compare the performance, we choose the anomaly detection methods including memory-based methods (such as PatchCore [8] and PadimCore [8]), normalizing flow based methods (such as CSFlow [22] and FastFlow [31]), distillationbased methods (such as Reverse Distillation [9] and STFPM [33]), and a diffusion-based method (DiffANO [27]) which also utilizes image-level binary labels. We train them on BRATS2020 and ISLES datasets. Table 1 shows the segmentation results. Our FNDM outperforms the existing methods in all metrics on both datasets. Diffusion methods have a more powerful generation ability than non-diffusion methods, and our method outperforms the best non-diffusion methods over 20% Dice. FNDM also outperforms the previous state-of-theart diffusion method, DiffANO, by a large gap of +9.56% Dice, -0.98% HDis, and +0.54% VSim on BRATS dataset, and +19.98% Dice, -1.39% HDis, and +26.73% VSim on ISLES dataset, revealing that our FNDM is effective to reconstruct the healthy image from diseased to detect the anomaly regions in brain MR images. Thanks to non-Markovian procedure and pixel-wise hybrid guidance, the performance improvement of our method is larger on the ISLES dataset where stroke lesions are more challenging due to smaller sizes and irregular shapes.  "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.4,Ablation Study,"We conduct ablation studies for hybrid conditions and the steps of encoding and sampling procedures. From Table 2, we decompose the overall pixel-wise hybrid condition into classifier gradient (CG), Non-Markovian (NM), and Memory Bank (MB), comparing all possible combinations on BRATS2020 dataset. We observe that combinations with more components achieve better performance, and NM module achieves a higher increase than MB module. In Fig. 4, we further evaluate the segmentation performance of DiffANO [27] and ours across diverse steps on BRATS2020 dataset. DiffANO performs best at 300 steps, concluding that it is the optima where anomaly information vanishes and anatomy information preserves partly. Our method only needs 50 steps which achieve 6-time acceleration compared to DiffANO when both approaches reach the best Dice scores. Besides, our method performs stably as the step amount exceeds 50 since non-Markovain strong guidance ensures high-quality information transition along the timeline, independent of the loss of anatomy structure."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,4.0,Conclusion and Discussion,"We propose a Fast Non-Markovian Diffusion Model (FNDM) for weakly supervised anomaly detection. FNDM first encodes the images into noisy ones, then applies hybrid conditional generation to reconstructed original images without anomalies. FNDM achieves high-fidelity generation on weak labels by leveraging non-Markovian modeling and pixel-wise hybrid conditions. Besides, FNDM conducts ODE fast solver for encoding and sampling to reach 6-time acceleration. Extensive experiments on two brain datasets reveal the effectiveness and superiority of our approach for anomaly detection. The limitation of our method is that, as a diffusion-based method, it still needs more evaluation steps than GANs. In the future, we could investigate the knowledge distillation techniques to further reduce the sampling steps and apply FNDM in other modalities."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,1.0,Introduction,"Computer-aided diagnosis systems have become a prominent tool in medical diagnosis. Yet, their adoption is limited by the need for large amounts of annotated data for training, which hinders their scalability and adaptability to new clinical findings [3,12]. Moreover, adapting to a new reporting template or clinical protocol necessitates new annotations, further reducing their feasibility in clinical settings. Recently, zero-shot [1,4,14,15,17] and few-shot [1,4,8] learning methods have been proposed as a potential solution, utilizing contrastive pretraining [13,19] on pairs of radiology reports and images, and achieving performance on par with radiologists [15]. However, these methods lack the level of detail of radiology reports and inherent explainability, impeding their adoption in clinical settings [7]. Particularly, explaining the diagnosis with image descriptors is crucial to increase trust in the system and allow radiologists to verify the results [9].Inspired by the success of using large language models to predict image descriptors in natural images [10], we introduce Xplainer, a novel framework that enhances the explainability of zero-shot diagnosis in the clinical setting. Xplainer leverages the classification-by-description approach [10] of vision-language models and adapts it to the multi-label medical diagnosis task. Specifically, we task the model to classify the existence of descriptive observations, which a radiologist would examine on an X-Ray scan, instead of directly predicting a diagnosis. This model design imbues our framework with intrinsic explainability, as the final diagnosis prediction is predicated on the underlying descriptor predictions.We evaluate Xplainer on two chest X-ray datasets, CheXpert [5] and ChestX-ray14 [16], and demonstrate its efficacy in enhancing the performance and explainability of zero-shot diagnosis in the clinical setting. Our results highlight that Xplainer provides a more comprehensive understanding of the diagnosis prediction process, thereby serving as a valuable tool for clinical decision-making. In summary, Xplainer presents a novel framework for zero-shot diagnosis that not only improves explainability and accuracy but also provides an invaluable tool for computer-aided diagnosis."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,2.1,Model Overview,"We propose Xplainer, an explainable zero-shot classification-by-description approach for diagnosing pathologies from X-Ray scans. Given an image i and a list of clinical observations o p1-n per pathology p, the goal is to make a multi-label prediction indicating the diagnosis for the patient.Our zero-shot approach leverages the alignment of image and text embeddings provided by contrastive language-image pretraining (CLIP) [13] and therefore does not require any labeled data. We built upon BioVil [1], a CLIP model pretrained on pairs of radiology reports and images. Employing the text and image encoders from BioVil, we calculate the cosine similarity between an Xray image and each of N pre-defined clinical observations o p1-N describing a pathology. Then we calculate observation probabilities P pos (o pi ) for every observation. Analogously, we calculate probabilities for the absence of all observations P neg (o pi ) by defining negated prompts for all observations. Using the softmax over the positive and negative probability, we calculate the final probability of the presence of an observation P (o pi ). Given these observation probabilities P (o pi ), i ∈ 1, ..., N , we estimate a joined probability to determine the likelihood of the presence of a pathology P (p):We repeat this process for all pathologies we want to diagnose in the image. As the prediction of a pathology diagnosis is directly extracted from the observation probabilities, our method is explainable by design, producing a diagnosis prediction and the detected X-ray observations leading to that prediction. Moreover, the observation probabilities show which observations the model mainly considers for its diagnosis. Figure 1 shows an overview of our framework.To integrate multiple images of one patient, we calculate positive and negative observation probabilities for each image and average them before calculating the pathology probability."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,2.2,Prompt Engineering,"Successful zero-shot inference relies on a good alignment between the contrastive pretraining and the downstream task [13]. As BioVil [1] was trained on pairs of radiological images and reports, we need to keep our observation prompts close to the style of medical reports. To initialize our prompts, we employ ChatGPT [11] and query it to describe observations in X-ray images that would occur in a radiology report indicating specific pathologies. We further refined the prompts with the help of an experienced radiologist, who manually verified and adapted the descriptors. Human refinement cost was low, taking the radiologist only a few hours. We provide a complete list of the descriptors in the supplementary.Radiology reports often include both presence and absence of particular observations. When comparing a prompt with an image embedding, it is hard for the model to differentiate between an observation's positive and negative occurrence, as their formulation can be very similar. Previous work [14,15] has shown that introducing negative prompts can circumvent this problem. Therefore, instead of thresholding the similarity between a positive prompt and an image, we prompt the model with both a positive and a negated version of each observation prompt and compare their probabilities. We adapt our prompts in two additional steps to align them with the text in radiology reports. First, we add a disease indication, as radiology reports usually contain observations paired with conclusions. Further, this reduces the ambiguity of our prompts, as in radiology, one sign (e.g., Lung Opacity) can indicate multiple pathologies (e.g., Pneumonia, Atelectasis, or Edema). Additionally, we frame all our observations in a sentence structure sounding more like an actual report by adding ""There is/are"" before every observation. Putting all of this together, we define the following prompt structure: ""There is/are (no) <observation> indicating <pathology>."" Lastly, we define contrastive pathology-based prompts to compare to our observation-based prompting. In this setting, only two prompts, one positive and one negative prompt, are used per pathology. Overall, we compare the following styles of prompting to show the benefit of observation-based, contrastive prompting with disease indication and report style: "
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,3.0,Experiments and Results,"We evaluate Xplainer in a zero-shot setting on the commonly used chest X-ray datasets, CheXpert [5], and ChestX-ray14 [16]. The CheXpert dataset provides a manually labeled validation and test set with 200 and 500 patients, respectively, and 14 classes, including ""No Finding"", ""Support Devices/Foreign Objects"", and 12 pathology labels. ChestX-ray14 is evaluated on 14 pathology labels on a test set of 25.596 images. For both datasets, we use the official validation and test splits. We perform a multi-label classification for both datasets and evaluate the performance via the Area Under the ROC-curve (AUC) between the positive pathology probabilities and the labels. Table 1 shows our results compared to previously proposed zero-shot pathology prediction approaches. On CheXpert, we compare with Seibold et al. [14] on the validation set, as they only reported validation performance. For the comparison with CheXzero [15], as well as the ChestX-ray14 dataset, we compare test set results. We outperform both previous works in an out-of-domain setting, where the zero-shot inference is performed on a different dataset than CLIP was trained on. The state-of-the-art results on both datasets show the effectiveness of our observation-based modeling. Further, in Table 2, we provide a detailed breakdown of our results per pathology and dataset. Ablation Studies. In our ablation studies, we investigate the impact of our prompt design and the effect of using multiple images. Table 3 shows the results on the CheXpert validation set using different prompting styles. We observe that pathology-based prompting, which reaches an AUC of 76.14%, is significantly worse than observation-based prompting, which reaches an AUC of 84.92%, again highlighting the benefit of observation-based prompting. Comparing the basic observation-based prompting, using only positive prompts per observation, to contrastive prompting, we see a substantial performance gap, showing the importance of using negative prompts to differentiate between positive and negative occurrences. We also show the effect of formulating our prompts unambiguously and in the style of an actual radiology report by adding pathology indication and report style. Adding pathology indication to the contrastive observation-based prompting significantly improves performance, achieving an AUC of 84.35%. Finally, incorporating report style in the prompts leads to the highest AUC of 84.92%, indicating that a contrastive observation-based prompt with pathology indication and report style is the most effective for zero-shot X-ray pathology classification.Additionally, we compare the initial ChatGPT output to our refined prompts (Table 4). Refinement was performed by deleting irrelevant, redundant, or incorrect descriptors. We observe an improvement through the refinement, indicating that including domain knowledge further improves our method. Nevertheless, the original ChatGPT prompts already perform quite well, showing the impressive potential of combining large generic language models with large domain-specific contrastive models.For the ""No Finding"" class, we compare to either define specific prompts such as ""Clear lung fields"" or ""Normal heart size and shape"" to classify ""No Finding"" or model it as the absence of all of the other 13 labels (Rule-based).  shows that a rule-based modeling of this class leads to better results. A reason for this could be that there is no clearly defined list of observations that indicate a healthy X-ray scan, which a radiologist would mention in his report. Lastly, we investigate different image aggregation methods for pathology prediction. We compare only using a single frontal view X-ray to using all images available for a patient. For aggregation, we compute positive and negative observation probabilities for every image. In Max aggregation, we then use the highest observation probability. The intuition behind this approach is that an observation might be seen much better from one perspective than another, and then only the perspective where the model is most confident should be used. On the other hand, different views give different insights about which kind of observation a visual cue on the image indicates. To leverage this multi-view information, we test Mean aggregation, where all observation probabilities are averaged over multiple images. The results shown in Table 6 indicate Mean aggregation to be superior, while both aggregation methods outperform using just a single image.Qualitative Results. Figure 2 shows qualitative examples of our model's predictions. For the true positive prediction, it can be seen that most of the descriptors are detected, and the model recognizes the descriptor ""Mass in the mediastinum"" as the main indication for the Enlarged Cardiomediastinum. For the True Negative case, the model, correctly, detected none of the descriptors. For the false positive example, one can clearly see that the model made a mistake because it detected an air bronchogram with relatively high certainty and no consolidation. Therefore, this false positive finding is easily falsified by the radiologist since an air bronchogram is a finding that co-occurs with consolidation (i.e., air-filled bronchi in consolidated areas). Thus, knowing which combination of descriptors leads to such a decision substantially improves explainability. In the false positive case, the model misses the pacemaker but detects some implant, showing the model understands there is some foreign object, but can not identify it, which is easily detected by the radiologist. Overall the classification-bydescription may facilitate a plausibility check of a specific inference result and an understanding of the source of errors.Discussion. One downside of modeling a joint probability is that it assumes that all descriptors appear simultaneously and gives all descriptors the same Fig. 2. Qualitative results of Xplainer importance. While this estimation leads to good results, the assumption does not always hold, as a pathology does not always present with the same signs. Further, there might be inter-dependencies between the descriptors, e.g., there can be descriptors that strongly correlate with the presence of a disease when combined with one descriptor but much less when combined with another. As a first try to model the importance of descriptors, we look into a supervised, outof-domain approach to model these inter-dependencies. For this, we train a Naive Bayes [2,18] CheXpert classifier on MIMIC-CXR [6], predicting a diagnosis given the descriptor probabilities, allowing the model to focus more on more relevant descriptors. While this approach relies on labels for MIMIC, these labels can be automatically generated by the CheXpert labeler [5], still not requiring human effort for labeling. We observe a slight performance increase on the test set from 80.58% to 81.37% AUC. This shows that the descriptor importance learned on MIMIC can partially be transferred to an out-of-domain dataset. We believe investigating methods to consider varying importance and complex relations between the descriptors is an essential and exciting direction to investigate in future work. Moreover, as Xplainer is not tied to specific image and text encoders, orthogonal works that lead to better encoders can be used to improve our results further.The use of descriptors in Xplainer provides a flexible and adaptive approach to automated diagnosis prediction. By identifying and classifying the presence of descriptive observations, our model can capture the underlying characteristics of a disease without relying on labeled data. This means that our system can easily adapt to new settings with different clinical findings, including new conditions where the symptoms are known, but there is no training data available yet. Additionally, using descriptors allows for adapting the system to specific populations, where the essential descriptors can differ. This is because the model is not constrained by pre-defined labels but rather by the meaningful underlying features of a given diagnosis."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,4.0,Conclusion,"In this work, we present a novel and effective zero-shot approach for chest X-ray diagnosis prediction, which provides an explanation for the model's decision. We leverage BioVil, a pretrained, domain-specific CLIP model, and use contrastive observation-based prompting to make predictions without label supervision. Our approach significantly outperforms previous zero-shot methods on CheXpert and Chest-Xray14, showcasing the effectiveness of our approach. Furthermore, we show that designing informative prompts is crucial to improve model performance. Our ablation studies demonstrate that adding disease indication and report style formulation to observation-based prompts notably enhances performance, underscoring the importance of aligning prompts with the domainspecific language used in medical reports. Additionally, contrastive prompts significantly boost performance, suggesting that the model can benefit from explicitly contrasting positive and negative examples.Our work highlights the potential of contrastive pretraining combined with observation-based prompting as a promising avenue for zero-shot medical image classification, where labeled data is scarce or expensive to obtain, and explainability is vital. We envision that our approach can be extended to other medical imaging domains and have practical applications in real-world scenarios. Our findings contribute to the growing body of research to improve the accuracy and interpretability of medical image diagnosis."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_41.
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,1.0,Introduction,"With emerging evidence from various post-mortem and in vivo tau PET imaging studies, the existence of several well-recognized atypical patterns of neurofibrillary tangle distribution in subsets of patients and abnormal clinical presentations challenge [4] the notion that tau pathology follows a uniform Braak-like progression throughout the brain in Alzheimer's disease (AD) [2]. Disentangling tau pathology heterogeneity can thus greatly contribute to develop more accurate diagnosis, prognosis and targeted treatment for AD.In contrast to many previous studies [7,14,16] utilizing clustering methods based on spatial variants for subtyping, spatiotemporal methods have recently gained popularity, combining spatial patterns and disease staging in a single framework. Event-based model [5,17] is a state-of-art spatiotemporal method that requires only cross-sectional data to automatically detect multiple spatiotemporal trajectories. However, the trajectory depends on the biomarker selection, and the predefined region-of-interest (ROI) based parcellations ignore the regional relations and the topological changes of pathology within the brain. Besides, the method focuses more on temporal transitions but neglects regional coherence. Moreover, the estimation requires large sample sizes to cover enough phenotype and intensity variants for better model fitting.To address these limitations, this paper presents a novel Reeb graph representation that encodes the topography of tau pathology from PET imaging, and a directed-graph-based framework for uncovering spatiotemporal heterogeneity from cross-sectional tau PET data. More specifically, we first generate a pattern representation from Reeb graph analysis on cortical surfaces to encode the tau pathology pattern. Secondly, the spatial coherence and temporal consistency of tau spreading patterns across subjects are combined within a directed graph for clustering, which is robust to sample sizes and intensity deviations. With largescale imaging data from the ADNI and A4 studies, we obtained three subtypes with systematic spatiotemporal variations in tau spreading patterns by utilizing an efficient community detection method on graphs. We also demonstrate that our method exhibits more robust generalization performance than event-based model on both synthetic and real data."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,2.1,Reeb Graph Analysis for Pathology Detection,"The reeb graph encodes the topology of pathology on the cortical surface and is used to extract the salient tau pathological patterns. Given a Morse function f on the mesh M, its Reeb graph is defined as follows [10]:In our work, M is a common template surface fsaverage from FreeSurfer [6], and f is the tau standardized uptake value ratio (SUVR) map defined on M. For numerical calculation of the Reeb graph of f on M, we use the algorithm proposed in [11]. Because the topology only changes at critical points (minimum, maximum and saddle points), for a given SUVR map f on cortical surface M, we first sort its critical pointsThe surface can be partitioned by using the level contours in the neighborhood of these critical points, and the neighboring nodes are connected in the Reeb graph through arcs by applying We further develop a Reeb graph simplification scheme to remove the noisy peaks of the original SUVR function (Fig. 1(a)) for salient pattern detection. Firstly, we iteratively apply Laplacian smoothing on an SUVR map until the Reeb graph reaches a common level of complexity, which is measured as the number of nodes in the graph. Then we use a Reeb graph simplification algorithm [12] to merge nodes around saddle points based on their persistence. For an edgeits weight is defined according to its persistence:where A k is area of the patch enclosed by triangles belonging to this edge, and f (C j ) is the peak SUVR value of this patch. To simplify the Reeb graph, we iteratively remove saddle points and spurious edges based on the persistence threshold δ. At each iteration, we scan these saddle points sequentially and for an edgewe collapse this edge by removing C i and adding all its connections to C j . The weights of these new edges are updated according to Eq. 1. These steps can be repeated until the persistent threshold is reached. The pruned Reeb graph is illustrated in Fig. 1(c).The number of critical points in the pruned Reeb graph is determined by the complexity of SUVR function and the persistence threshold. We set δ = 300 so the pruned Reeb graph typically has less than 10 patches. The edges of the simplified Reeb graph are sets of triangles with topological changes, and the patches enclosed by these triangle sets are the regions with salient tau pathology."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,2.2,Directed Graph Construction for Spatiotemporal Subtyping,"The Reeb graph patches of different subjects typically have distinct shapes, which makes patch matching an essential step for pattern comparison. The solution we develop here is to establish patch correspondence between subjects based on their spatial proximity by an assignment algorithm. We define the cost function for matching the patch x i of subject x and the patch y j of subject y as:where dP eak is the distance between the peaks, and dHausdorf f is the Hausdorff distance between two patch sets. Both are calculated based on geodesic distances on the surface. Patch matching is achieved through iterative assignments to accommodate both one-to-one and many-to-one matching. The latter one is illustrated in Fig. 2(a) that multiple small patches have the same correspondences. At iteration t, a cost function C t is calculated for the unpaired patches in x and all patches in the reference subject y. A linear assignment [3] is applied with the cost function C t and the unpaired cost c t to establish the one-to-one correspondences and add it into the correspondence set Φ. The iterations repeat until min(C t ) > c t , and the resulting correspondences represent patches with spatial proximity across subjects.The spatiotemporal similarity of tau pathology implies spatial coherence and temporal consistency, and the directed graph can be used to encode both the similarity as the edge weight and the disease staging as edge direction. The temporal similarity is derived from the consistency of the pathology occurring orders based on the correspondences. Based on the assumption that tau accumulation is a monotonic process, we sort the Reeb graph patches y i in a peak SUVR descending order, and this sequence implies the patch occurring order. When comparing with subject x, patches x i are arranged according to their correspondences y i . For patches of x having the same correspondence in y, they are sorted based on their own peak SUVR values decreasingly. The temporal crossings between the two sequences as indicated in Fig. 2(a) imply the existence of temporal inconsistency between the two SUVR maps. A crossing penalty is then defined to quantify this inconsistency:where SU V R xi is the peak SUVR of x i , and P atchS xi is the patch size.The spatial coherence can be estimated from the spatial deviations caused by the unpaired patches. An unpaired penalty is defined as:The crossing and unpaired penalties are combined to define a distance matrix:where D is weighted combination of two penaty terms by α and β. For the construction of a dense graph, we transform D into a similarity matrix S = 1/(1 + D) as the weights between each pair of subjects. To maintain sparsity and form clusters, we primarily focus on subjects exhibiting similar patterns and adopt a K-nearest-neighbor(KNN) approach to keep only the top K-relevant connections for each subject. Disease staging is determined based on the assumption that later-stage subjects typically have more widespread distribution of tau pathology and/or higher peak intensities. To generate directionality between subjects in our graph representation, we calculate an unnormalized unpaired penalty as Dunpair x,y = Dunpair x,y × max xi SU V R xi because it is composed of both the spreading size and SUVR values. Specifically, if Dunpair x,y > Dunpair y,x , then subject x is considered to be in a later stage than y because of having more tau pathology and higher SUVR values, so the edge direction is y → x. By applying the directions to the KNN graph, we get a directed graph for representing the spatiotemporal relationships between subjects as illustrated in Fig. 2(b). For the subtyping of tau spatiotempral topography, the Louvain community detection method [1] is applied for clustering on the directed graph by maximizing modularity. The modularity is high when the intra-subtype connections are dense while the inter-subtype connections are sparse."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.0,Experiments and Results,"In the current work, we choose the unpaired costs c 1 = 300 and c i = 500(i > 1) in linear assignment of patches, the weights for distance matrix definition as α = 2 and β = 1, and K = 10 for KNN-based graph construction. For Louvain community detection on directed graphs, we set its resolution parameter γ = 0.3. Beside clustering based on the directed graph from training data, we can apply the trained model from our method to estimate the subtype of a validation/test set via major voting by the K most similar training samples.For the event-based SuStaIn method [17], the same experimental settings in [15] were used. The SUVR values are first normalized based on the normal control samples to create the tau z-scores. We used the z-score of 5 ROIs (parietal, frontal, occipital, temporal, and medial temporal lobe) and cut-off thresholds of z = 2, 5 and 10 for event definition. To apply the trained model to a validation/test data, we obtain subtype membership by fitting the new subject into the pre-trained SuStaIn model. "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.1,Synthetic Experiments,"Synthetic Data. The synthetic tau SUVR data is generated by adding simulated values derived from a Gaussian mixture model to an SUVR template (SU V R tem ), which is calculated as the mean of normal controls, as follows:where f is a Gaussian distribution, x k is the distance vector to seed k, and the standard deviation σ measures the spreading size. The magnitudes of the seeds a k are sampled from the ranges shown in Fig. 3"
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,,(b).,"A training set and four test sets with different magnitude ranges and seeds randomly sampled from temporal, parietal and frontal lobe, respectively, were generated. Each set has three subtypes with distinct tau propagation orders as shown in Fig. 3(a), and each subtype contains 50 samples.Synthetic Results. The performances are measured by accuracy and shown in Fig. 3. First, our method (89.33%) achieved better performance over the SuS-taIn method (80%) on training set. Second, we applied the trained model of both methods to the four test sets and results are shown in Fig. 3(c). With the increasing difference between the test and training set, performance of the SuS-taIn method decreases quite significantly while our method maintains a stable performance. This shows the robustness of our method with respect to intensity changes from the training to the testing set and potentially better generalization ability as we will further demonstrate next in real data experiments."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.2,Real Data Experiments,"Real Dataset. 706 tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI) (adni.loni.usc.edu) and Anti-Amyloid Treatment in Asymptomatic Alzheimer's (A4) [13] are used. 427 amyloid-positive-tau-positive (A+T+) images are used for subtyping analysis. A+/A-labels were provided by the ADNI and A4 study, T+/T-status was defined if the image with peak SUVR value exceeding 1.5 [9]. Besides, 279 cognitively normal A-T-subjects were selected as normal controls. The demographic information of the cohorts are listed in Table 1. All tau PET images were averaged across frames and registered to individual T1 space to obtain the skull-striped images. Besides, T1 images were preprocessed using Freesurfer [6] to get their volume parcellation using the Desikan-Killiany atlas. The registered tau PET images were intensitynormalized using an inferior cerebellar gray matter reference region, resulting in SUVR images.Subtyping A+T+ Subjects. Using A+T+ data, we obtained three pathologically different subtypes for both methods (Fig. 4). In our method, the staging was derived from the degree of the nodes in the dense directed graph as d i = indegree ioutdegree i . Two thresholds (-50 and 50) were set as the cutoffs to define three stages for each subtype. For subtype 1, more tau pathology distributes in the temporal lobe, and its propagation is consistent with the classic Braak staging [2]; for subtype 2, tau pathology primarily occurs in the occipital area and sequentially spreads to parietal and other regions; subtype 3 does not show significant differences among brain regions, and the overall SUVR is lower among all subtypes. For both methods, the average patterns are similar in three subtypes. However, comparing across stages, the intensities in early stage   Generalization Performance Across Cohorts. The generalization performance of two methods was compared by the consistency of subtyping results of same data from models trained by different cohorts. For both methods, a model with three subtypes is first trained separately on ADNI and A4 data and then applied to both cohorts. Rand index [8] was used to measure the similarity of the subtyping results using different models and quantify the generalization. Compared to the SuStaIn method, our method is more stable on both datasets as shown in Table 2. The inferior performance of the SuStaIn method is caused by the SUVR differences between cohorts as indicated in Fig. 5(a). This is consistent with the results from the synthetic experiment where the SuS-taIn method becomes unstable when the training A4 data has limited intensity range. Our method, however, focuses on the spreading patterns and is robust to intensity variations. The subtyping patterns of ADNI data with our models trained on both cohorts are shown in Fig. 5. Both results are consistent with the patterns derived from whole A+T+ samples in Fig. 4. Further evidence of the clinical relevance of the subtyping results from our method is indicated by the cognitive scores of ADNI subjects in Table 3. All subtypes show deviations from the normal control, and particularly subtype 3 is different from the other two subtypes with statistical significance (P < 0.05)."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,4.0,Conclusions,"In the current study, we proposed a novel directed-graph-based framework with a new spatiotemporal pattern representation for parsing tau pathology heterogeneity and demonstrated its improved performance over the state-of-art SuStaIn method. Application of the proposed method on large-scale tau PET imaging datasets successfully demonstrated three subtypes with clear relevance to previously well-described clinical subtypes with distinct spatiotemporal patterns."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"Renal cancer is the most lethal malignant tumor of the urinary system, and the incidence is steadily rising [13]. Conventional B-mode ultrasound (US) is a good screening tool but can be limited in its ability to characterize complicated renal lesions. Contrast-enhanced ultrasound (CEUS) can provide information on microcirculatory perfusion. Compared with CT and MRI, CEUS is radiation-free, cost-effective, and safe in patients with renal dysfunction. Due to these benefits, CEUS is becoming increasingly popular in diagnosing renal lesions. However, recognizing important diagnostic features from CEUS videos to diagnose lesions as benign or malignant is non-trivial and requires lots of experience.To improve diagnostic efficiency and accuracy, many computational methods were proposed to analyze renal US images and could assist radiologists in making clinical decisions [6]. However, most of these methods only focused on conventional B-mode images. In recent years, there has been increasing interest in multi-modal medical image fusion [1]. Directly concatenation and addition were the most common methods, such as [3,4,12]. These simple operations might not highlight essential information from different modalities. Weight-based fusion methods generally used an importance prediction module to learn the weight of each modality and then performed sum, replacement, or exchange based on the weights [7,16,17,19]. Although effective, these methods did not allow direct interaction between multi-modal information. To address this, attention-based methods were proposed. They utilized cross-attention to establish the feature correlation of different modalities and self-attention to focus on global feature modeling [9,18]. Nevertheless, we prove in our experiments that these attentionbased methods may have the potential risks of entangling features of different modalities.In practice, experienced radiologists usually utilize dynamic information on tumors' blood supply in CEUS videos to make diagnoses [8]. Previous researches have proved that temporal information is effective in improving the performance of deep learning models. Lin et al. [11] proposed a network for breast lesion detection in US videos by aggregating temporal features, which outperformed other image-based methods. Chen et al. [2] showed that CEUS videos can provide more detailed blood supply information of tumors allowing a more accurate breast lesion diagnosis than static US images.In this work, we propose a novel multi-modal US video fusion network (MUVF-YOLOX) based on CEUS videos for renal tumor diagnosis. Our main contributions are fourfold. (1) To the best of our knowledge, this is the first deep learning-based multi-modal framework that integrates both B-mode and CEUSmode information for renal tumor diagnosis using US videos. (2) We propose an attention-based multi-modal fusion (AMF) module consisting of cross-attention and self-attention blocks to capture modality-invariant and modality-specific features in parallel. (3) We design an object-level temporal aggregation (OTA) module to make video-based diagnostic decisions based on the information from multi-frames. (4) We build the first multi-modal US video datatset containing B-mode and CEUS-mode videos for renal tumor diagnosis. Experimental results show that the proposed framework outperforms single-modal, single-frame, and other state-of-the-art methods in renal tumor diagnosis."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.1,Overview of Framework,"The proposed MUVF-YOLOX framework is shown in Fig. 1. It can be divided into two stages: single-frame detection stage and video-based diagnosis stage. (1) In the single-frame detection stage, the network predicts the tumor bounding box and category on each frame in the multi-modal CEUS video clips. Dual-branch backbone is adopted to extract the features from two modalities and followed by the AMF module to fuse these features. During the diagnostic process, experienced radiologists usually take the global features of US images into consideration [20]. Therefore, we modify the backbone of YOLOX from CSP-Darknet to Swin-Transformer-Tiny, which is a more suitable choice by the virtue of its global modeling capabilities [15]. (2) In the video-based diagnosis stage, the network automatically chooses high-confidence region features of each frame according to the single-frame detection results and performs temporal aggregation to output a more accurate diagnosis. The above two stages are trained successively. We first perform a strong data augmentation to train the network for tumor detection and classification on individual frames. After that, the first stage model is switched to the evaluation mode and predicts the label of each frame in the video clip. Finally, we train the OTA module to aggregate the temporal information for precise diagnosis."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.2,Dual-Attention Strategy for Multimodal Fusion,"Using complementary information between multi-modal data can greatly improve the precision of detection. Therefore, we propose a novel AMF module to fuse the features of different modalities. As shown in Fig. 1, the features of each modality will be input into cross-attention and self-attention blocks in parallel to extract modality-invariant features and modality-specific features simultaneously.Taking the B-mode as an example, we first map the B-mode features F B and the CEUS-mode features) using linear projection. Then cross-attention uses scaled dot-product to calculate the similarity between Q B and K C . The similarity is used to weight V C . Crossattention extracts modality-invariant features through correlation calculation but ignores modality-specific features in individual modalities. Therefore, we apply self-attention in parallel to highlight these features. The self-attention calculates the similarity between Q B and K B and then uses the similarity to weight V B . Similarly, the features of the CEUS modality go through the same process in parallel. Finally, we merge the two cross-attention outputs by addition since they are both invariant features of two modalities and concatenate the obtained sum and the outputs of the two self-attention blocks. The process mentioned above can be formulated as follows:where, F invar represents the modality-invariant features. F B-spec and F C-spec represent the modal-specific features of B-mode and CEUS-mode respectively. F AM F is the output of the AMF module."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.3,Video-Level Decision Generation,"In clinical practice, the dynamic changes in US videos provide useful information for radiologists to make diagnoses. Therefore, we design an OTA module that aggregates single-frame renal tumor detection results in temporal dimension for diagnosing tumors as benign and malignant. First, we utilize a feature selection module [14] to select high-quality features of each frame from the Cls_conv and Reg_conv layers. Specifically, we select the top 750 grid cells on the prediction grid according to the confidence score. Then, 30 of the top 750 grid cells are chosen by the non-maximum suppression algorithm for reducing redundancy. The features are finally picked out from the Cls_conv and Reg_conv layers guided by the positions of the top 30 grid cells. Let F Cls = {Cls 1 , Cls 2 , ...Cls l } and F Reg = {Reg 1 , Reg 2 , ...Reg l } denote the above obtained high-quality features from l frames. After feature selection, we aggregate the features in the temporal dimension by time attention. F Cls and F Reg are mapped into (Q Cls , K Cls , V Cls ) and (Q Reg , K Reg ) via linear projection. Then, we utilize scaled dot-product to compute the attention weights of V Cls as:After temporal feature aggregation, F temp is fed into a multilayer perceptron head to predict the class of tumor.3 Experimental Results"
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.1,Materials and Implementations,"We collect a renal tumor US dataset of 179 cases from two medical centers, which is split into the training and validation sets. We further collect 36 cases from the two medical centers mentioned above (14 benign cases) and another center (Fujian Provincial Hospital, 22 malignant cases) to form the test set. Each case has a video with simultaneous imaging of B-mode and CEUS-mode. Some examples of the images are shown in Fig. 2. There is an obvious visual difference between the images from the Fujian Provincial Hospital (last column in Fig. 2) and the other two centers, which raises the complexity of the task but can better verify our method's generalization ability. More than two radiologists with ten years of experience manually annotate the tumor bounding box and class label at the frame level using the Pair annotation software package (https://www.aipair. com.cn/en/, Version 2.7, RayShape, Shenzhen, China) [10]. Each case has 40-50 labeled frames, and these frames cover the complete contrast-enhanced imaging cycle. The number of cases and annotated frames is summarized in Table 1.Weights pre-trained from ImageNet are used to initialize the Swin-Transformer backbone. Data augmentation strategies are applied synchronously to B-mode and CEUS-mode images for all experiments, including random rotation, mosaic, mixup, and so on. All models are trained for 150 epochs. The batch size is set to 2. We use the SGD optimizer with a learning rate of 0.0025. The weight decay is set to 0.0005 and the momentum is set to 0.9. In the test phase, we use the weights of the best model in validation to make predictions. All Experiments are implemented in PyTorch with an NVIDIA RTX A6000 GPU. AP 50 and AP 75 are used to assess the performance of single-frame detection. Accuracy and F1-score are used to evaluate the video-based tumor diagnosis.  "
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.2,Ablation Study,"Single-Frame Detection. We explore the impact of different backbones in YOLOX and different ways of multi-modal fusion. As shown in Table 2, using Swin-Transformer as the backbone in YOLOX achieves better performance than the original backbone while reducing half of the parameters. The improvement may stem from the fact that Swin-Transformer has a better ability to characterize global features, which is critical in US image diagnosis. In addition, we explore the role of cross-attention and self-attention blocks in multi-modal tasks, as well as the optimal strategy for combining their outputs. Comparing row 5 with row 7 and row 8 in Table 2, the dual-attention mechanism outperforms the single crossattention. It indicates that we need to pay attention to both modality-invariant and modality-specific features in our multi-modal task through cross-attention and self-attention blocks. However, ""CA+SA"" (row 6 in Table 2) obtains inferior performance than ""CA"" (row 5 in Table 2). We conjecture that connecting the two attention modules in series leads to the entanglement of modality-specific and modality-invariant information, which would disrupt the model training. On the contrary, the ""CA//SA"" method, combining two attention modules in parallel, enables the model to capture and digest modality-specific and modality-invariant features independently. For the same reason, we concatenate the outputs of the attention blocks rather than summing, which further avoids confusing modality-specific and modality-invariant information. Therefore, the proposed method achieves the best performance.Table 2. The results of ablation study. ""CA"" and ""SA"" denote cross-attention and selfattention respectively. ""//"" and ""+"" mean parallel connection and series connection. Video-Based Diagnosis. We investigate the performance of the OTA module for renal tumor diagnosis in multi-modal videos. We generate a video clip with l frames from annotated frames at a fixed interval forward. As shown in Table 3, gradually increasing the clip length can effectively improve the accuracy. This suggests that the multi-frame model can provide a more comprehensive characterization of the tumor and thus achieves better performance. Meanwhile, increasing the sampling interval tends to decrease the performance (row 4 and row 5 in Table 3). It indicates that continuous inter-frame information is beneficial for renal tumor diagnosis. "
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.3,Comparison with Other Methods,"The comparison results are shown in Table 4. Compared to the single-modal models, directly concatenating multi-modal features (row 3 in Table 4) improves AP 50 and AP 75 by more than 15%. This proves that complementary information exists among different modalities. For a fair comparison with other fusion methods, we embed their fusion modules into our framework so that different approaches can be validated in the same environment. CMML [19] and CEN [17] merge the multi-modal features or pick one of them by automatically generating channel-wise weights for each modality. They score higher AP in the validation set but lower one in the test set than ""Concatenate"". This may be because the generated weights are biased to make similar decisions to the source domain, thereby reducing model generalization in the external data. Moreover, CMF only highlights similar features between two modalities, ignoring that each modality contains some unique features. TMM focuses on both modality-specific and modality-invariant information, but the chaotic confusion of the two types of information deteriorates the model performance. Therefore, both CMF [17] and TMM [9] fail to outperform weight-based models. On the contrary, our AMF module prevents information entanglement by conducting cross-attention and self-attention blocks in parallel. It achieves AP 50 = 82.8, AP 75 = 60.6 in the validation set and AP 50 = 79.5, AP 75 = 39.2 in the test set, outperforming all competing methods while demonstrating superior generalization ability. Meanwhile, the improvement of the detection performance is beneficial to our OTA module to obtain lesion features from more precise locations, thereby improving the accuracy of benign and malignant renal tumor diagnosis. "
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,4.0,Conclusions,"In this paper, we create the first multi-modal CEUS video dataset and propose a novel attention-based multi-modal video fusion framework for renal tumor diagnosis using B-mode and CEUS-mode US videos. It encourages interactions between different modalities via a weight-sharing dual-branch backbone and automatically captures the modality-invariant and modality-specific information by the AMF module. It also utilizes a portable OTA module to aggregate information in the temporal dimension of videos, making video-level decisions. The design of the AMF module and OTA module is plug-and-play and could be applied to other multi-modal video tasks. The experimental results show that the proposed method outperforms single-modal, single-frame, and other stateof-the-art multi-modal approaches."
Visual Grounding of Whole Radiology Reports for 3D CT Images,1.0,Introduction,"In recent years, a number of medical image recognition systems have been developed [6] to alleviate the increasing burden on radiologists [2,21,22]. In the development of such systems, the task of manually labeling images is a significant bottleneck. Auto-labeling, the process of automatically assigning labels to images using machine learning algorithms, has emerged as a promising solution to this problem. In cases where there are plenty of image and caption pairs, one potential approach to auto-labeling is visual grounding [12], which utilizes natural language descriptions to identify and localize objects in images. With the recent advances in cross-modal technology based on deep learning, many frameworks for visual grounding has been proposed [7,11]. Within the medical domain, several large scale datasets with radiology reports are available (e.g. OpenI [3], MIMIC-CXR [9]), and these produced researches on medical image visual grounding [1,25]. However, to the best of our knowledge, prior studies have focused on 2D X-ray images [28] or videos [15], and there has been no research applying visual grounding to 3D computed tomography (CT) images so far. Visual grounding on CT images has the following difficulties: 1) Large number of anomaly types to detect: Existing researches on visual grounding using X-ray images handled only chest X-ray images. The number of anomaly types to detect is at most dozen or so (e.g. 13 findings [8]). In contrast, our research handles CT images including various parts of the human body. Consequently, the number of anomaly types to be detected is larger than one hundred.2) Long and complex sentences: Radiology reports on X-ray images are often simple, noting only the presence or absence of anomalies. On the other hand, in CT examinations, the qualitative diagnosis of each anomaly is often performed. In cases, multiple anomalies are simultaneously described in a sentence. Therefore, the description tend to be long and complicated with multiple sentences (Fig. 1). Visual grounding for CT images requires the extraction of information about the location and type of each anomaly from these complex sentences.In this work, we propose a novel visual grounding framework for 3D CT images and radiology reports. The main idea is to separate the task into three parts: 1) anatomical segmentation on images, 2) report structuring, and 3) localization of described anomalies. In the anatomical segmentation, multiple organs and tissues are extracted using the deep learning based segmentation model and provided as landmarks. The report structuring model, which is based on BERT [5], is also introduced to extract information of each anomaly from a complex report. Both of these features are fed into the grounding model (3) to extrapolate medical domain knowledge, thereby enabling accurate visual grounding.Our contributions are as follows:-We show the first visual grounding results for 3D CT images that covers various body parts and anomalies. -We introduce a novel grounding architecture that can leverage report structuring results of presence/type/location of described anomalies. -We validate the efficacy of the proposed framework using a large-scale dataset with region-description correspondence annotations."
Visual Grounding of Whole Radiology Reports for 3D CT Images,2.0,Related Work,"Visual Grounding. Visual grounding task involves learning the correspondences between descriptions in the text and image regions from a given training set of region-description pairs [12]. There are mainly two approaches: onestage approach and two-stage approach. Most studies follow a two-stage approach [14,17]. However, this approach usually employs a pre-trained object detector, and it leads to restrict the capability of categories and attributes in grounding. Accordingly, recent studies is shifting to employ the one-stage approach, in which visual grounding is performed by end-to-end training [4,10,27].Vision-Language Tasks on Medical Image. The existence of public datasets with paired images and reports [3,9,26] has accelerated research on cross-modal tasks in the medical field [16,25]. Inspired by the success of visual grounding, several studies of visual grounding for medical images and radiology reports have also been reported [1,23,28]. These studies utilized a large scale dataset and an attention-based language interpretation model such as BERT [5] to ground the descriptions in the report. However, these studies have focused on X-ray images, and to the best of our knowledge, there have been no studies on CT images, which cover the entire body and have a complex report."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.0,Methods,"We first formulate the problem. Next, we explain three key components of anatomical segmentation, report structuring, and anomaly localization in our framework. In our framework, multiple organ labels obtained as the output of anatomical segmentation encourage the grounding model to learn detailed anatomy, and report structuring allows the grounding model to accurately extract the features of the target anomaly from complex sentences."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.1,Problem Formulation,"Our research assumes that a dataset of image-report pairs with regiondescription correspondence annotations is provided for training. We show the overall framework in Fig. 2. We denote an image and a paired report as I and T respectively. Let I a be a label image in which multiple organs are extracted from I. Each report T contains descriptions of multiple (image) anomalies. We denote each anomalies as t i ∈ {t 1 , t 2 , ..., t N }. Given an image I and corresponding organ label images I a encoded as V ∈ R dz×dy×dx×d and a description about an anomaly t i encoded as L ti ∈ R d , the goal of our framework is to generate a segmentation map M ti that represents the location of the anomaly t i .Fig. 2. The proposed framework for 3D-CT visual grounding."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.2,Anatomical Segmentation,"The task of the anatomical segmentation is to extract relevant anatomies that can be clues for visual grounding. We use the commercial version of the 3D image analysis software (Synapse 3D V6.8, FUJIFILM corporation, Japan) to extract 32 organs and tissues (See Appendix Table. A1). In this software, anatomies are extracted using U-Net based architectures [13,18]. The extracted anatomical label images are I a ."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.3,Report Structuring,"The tasks of the report structuring are as follows: 1) anatomical prediction, 2) phrase recognition, and 3) relationship estimation between phrases (See Appendix Fig. A1). The anatomical prediction is a sentence-wise prediction to determine which organ or body part is mentioned in each sentence. The organs and body parts to be recognized are shown in Appendix Table . A2. The sentences belonging to the same class are concatenated, then the phrase recognition and the relationship estimation are performed for each class. The phrase recognition module extracts phrases and classifies each of them into 9 classes (See Appendix Table. A2). Subsequently, the relationship estimation module determines whether there is a relationship between anomaly phrases (e.g. 'nodule', 'fracture') and other phrases (e.g. '6mm', 'Liver S6'), resulting in the grouping of phrases related to the same anomaly. If multiple anatomical phrases are grouped in the same group, they are split into separate groups on a rule basis (e.g. ['right S1', 'left S6', 'nodule'] -> ['right S1', 'nodule'], ['left S6', 'nodule']). More details of implementation and training methods are reported in Nakano et al. [20] and Tagawa et al [24]."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.4,Anomaly Localization,"The task of the anomaly localization is to output a localization map of the anomaly mentioned in the input report T . The CT image I and the organ label image I a are concatenated along the channel dimension and encoded by a convolutional backbone to generate a visual embedding V . The sentences in the report T are encoded by BERT [5] to generate embeddings for each character. Let r = {r 1 , r 2 , ..., r NC } be the set of character embeddings where N C is the number of characters. Our framework next adopt the Anomaly-Wise Feature Aggregator (AFA). For each anomaly t i , AFA generates a representative embedding L ti by aggregating the embeddings of related phrases based on report structuring results. The final grounding result M ti is obtained by the following Source-Target Attention.whereThe overall architecture of this module is illustrated in Appendix Fig. A2.Anomaly-Wise Feature Aggregator. The results of the report structuring m ti ∈ R NC are defined as follows:where c j is the class index labeled by the phrase recognition module (Let C be the number of classes). In this module, aggregate character-wise embeddings based on the following formula.e k = {r j |m tij = k} (4)L ti = LSTM([v organ ; p 1 ; e 1 ; p 2 ; e 2 ; ..., p C ; e C ])where v organ and p k are trainable embeddings for each organ and each class label respectively. [•; •] stands for concatenation operation. In this way, embeddings of characters related to the anomaly t i are aggregated and concatenated. Subsequently, representative embeddings of the anomaly are generated by an LSTM layer. In the task of visual grounding focused on 3D CT images, the size of the dataset that can be created is relatively small. Considering this limitation, we use an LSTM layer with strong inductive bias to achieve high generalization performance."
Visual Grounding of Whole Radiology Reports for 3D CT Images,4.1,Clinical Data,"We retrospectively collected 10,410 CT studies (11,163 volumes/7,321 unique patients) and 671,691 radiology reports from one university hospital in Japan.We assigned a bounding box to each anomaly described in the reports as shown in Appendix Fig. A3. The total category number is about 130 in combination of anatomical regions and anomaly types (The details are in Fig. 4) For each anomaly, a correspondence annotation was made with anomaly phrases in the report. The total number of annotated regions is 17,536 (head: 713 regions, neck: 285 regions, chest: 8,598 regions, and abdomen: 7,940 regions). We divide the data into 9,163/1,000/1,000 volumes as a training/validation/test split."
Visual Grounding of Whole Radiology Reports for 3D CT Images,4.2,Implementation Details,"We use a VGG-like network as Image Encoder, with 15 3D-convolutional layers and 3 max pooling layers. For training, the voxel spacings in all three dimensions are normalized to 1.0 mm. CT values are linearly normalized to obtain a value of [0-1]. The anatomy label image, in which only one label is assigned to each voxel, is also normalized to the value [0-1], and the CT image and the label image are concatenated along the channel dimension. As our Text Encoder, we use a BERT with 12 transformer encoder layers, each with hidden dimension of 768 and 12 heads in the multi-head attention. At first, we pre-train the BERT using 6.7M sentences extracted from the reports in a Masked Language Model task. Then we train the whole architecture jointly using dice loss [19] with the first 8 transformer encoder layers of the BERT frozen. Further information about implementation are shown in Appendix Table. A3."
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.0,Experiments,"We did two kinds of experiments for comparison and ablation studies. The comparison study was made against TransVG [4] and MDETR [10] that are onestage visual grounding approaches and established state-of-the-art performances on photos and captions. To adapt TransVG and MDETR for the 3D modality, the backbone was changed to a VGG-like network with 3D convolution layers, the same as the proposed method. We refer one of the proposed method without anatomical segmentation and report structuring as the baseline model."
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.1,Evaluation Metrics,"We report segmentation performance using Dice score, mean intersection over union (mIoU), and the grounding accuracy. The output masks are thresholded to compute mIoU and grounding accuracy score. The mIoU is defined as an average IoU over the thresholds [0.1, 0.2, 0.3, 0.4, 0.5]. The grounding accuracy is defined as the percentage of anomalies for which the IoU exceeds 0.1 under the threshold 0.1."
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.2,Results,"The experimental results of the two studies are shown in Table . 1. Both of MDETR and TransVG failed to achieve stable grounding in this task. A main difference between these models and our baseline model is using a source-target attention layer instead of the transformer. It is known that a transformer-based algorithm with many parameters and no strong inductive bias is difficult to generalize with such a relatively limited number of training data. For this reason, the baseline model achieved a much higher accuracy than the comparison methods. The ablation study showed that the anatomical segmentation and the report structuring can improve the performance. In Fig. 3 (upper row), we demonstrate several cases that facilitate an intuitive understanding of each effect. Longer reports often mention more than one anomaly, making it difficult to recognize the grounding target and cause localization errors. The proposed method can explicitly indicate phrases such as the location and size of the target anomaly, reducing the risk of failure. Figure 3 (lower row) shows examples of grounding results when a query that is not related to the image is inputted. In this case, the grounding results were less consistent with the anatomical phrases. The results suggest that the model performs grounding with an emphasis on anatomical information against the backdrop of abundant anatomical knowledge.The grounding performance for each combination of organ and anomaly type is shown in Fig. 4. The performance is relatively high for organ shape abnormalities (e.g. swelling, duct dilation) and high-frequency anomalies in small organs (e.g. thyroid/prostate mass). For these anomaly types, our model is considered to be available for automatic training data generation. On the other hand, the performance tends to be low for rare anomalies (e.g. mass in small intestine) and anomalies in large body part (e.g. limb). Improving grounding performance for these targets will be an important future work. "
Visual Grounding of Whole Radiology Reports for 3D CT Images,6.0,Conclusion,"In this paper, we proposed the first visual grounding framework for 3D CT images and reports. To deal with various type of anomalies throughout the body and complex reports, we introduced a new approach using anatomical recognition results and report structuring results. The experiments showed the effectiveness of our approach and achieved higher performance compared to prior techniques. However, in clinical practice, radiologists write reports from comparing multiple images such as time-series images, or multi-phase scans. Realizing such sophisticated diagnose process by a visual grounding model will be a future research."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,1.0,Introduction,"Pathological image analysis is a vital area of research within medical image analysis, focused on utilizing computer technology to aid doctors in diagnosing and treating diseases by analyzing pathological tissue slide images [5]. Advancements in pathological image analysis have been made in early cancer diagnosis, tumor localization, and grading, and treatment planning [3,10]. Multi-instance learning [2] is the primary analysis method used, which involves analyzing tasks based on slide labels and patches. Despite this, the clinical pathological analysis presents certain challenges and complexities, with the ultimate diagnosis relying on patients rather than slides.Specifically, in clinical problems of pathological image analysis, doctors usually summarize patient-level labels based on slide labels as the diagnostic results [1,6]. For example, for the pathological discrimination diagnosis task of intestinal tuberculosis(ITB) and Crohn's desease(CD), the categories of postoperative slides are divided into three types (normal, CD, ITB), and doctors will summarize the binary results of patients (ITB or CD) based on slide-level labels [6]. Similar situations exist in other tasks, such as the classification of breast cancer metastases in lymph nodes, where slide categories may have different classifications, and the corresponding diagnosis of the same patient is whether the cancer has spread to the regional lymph nodes (N-stage) [1]. Therefore, as shown in Fig. 1, actual pathological image analysis involves the relationships of patches, slides, and patients, which is called a multi-level multi-instance learning (ML-MIL) problem. Among them, for patients and slides, patients are bags while slides are instances, and for slides and patches, slides are bags while patches are instances.There are generally two methods to solve the ML-MIL problem. The first method is to directly average the prediction values of slides or take the maximum prediction value [9]. This method is relatively simple, but the information exchange between slides is not fully utilized, which may lead to errors in the summary result. The second method is to treat slide-patient as a new MIL problem according to the traditional MIL thinking, where slides are regarded as instances and patient labels as bags. Although this method seems reasonable, the number of patients is usually relatively small, and deep learning models usually require a large amount of data for training. Therefore, the insufficient number of samples at the slide-patient level may make it difficult for the model to learn enough information.To address the multi-level multi-instance learning (ML-MIL) problem in medical field, we propose a novel framework called Patients and Slides are Equal (P&SrE). Inspired by the iterative labeling process in medical diagnosis, this framework treats patients and slides as instances at the same level and uses transformers and attention mechanisms to build connections between them. This simple yet effective method allows for interaction between patient-level and slidelevel information to correct their respective features and improve classification performance. Our framework consists of two steps: first, at the patch-slide level, a common MIL framework is used to train a MIL neural network and obtain slide-level feature vectors; then, at the slide-patient level, we use self-attention mechanisms to combine the slides of the same patient into patient-level feature vectors, and treat these patient-level feature vectors together with all slide-level feature vectors of the same patient as instances at the same level, which are inputted into transformers for feature interaction and prediction of patient-and slide-level labels. Our method can effectively solve the problem of difficult training due to the scarcity of samples at the highest level in ML-MIL, and can be integrated into two state-of-the-art methods to further improve performance. We conducted rigorous experiments on two datasets and demonstrated the effectiveness of our method. Our contributions include:1) Proposing a novel general framework to address the unique ""patch-slidepatient"" ML-MIL problem in the medical field. Before this, no other framework had directly tackled this specific problem, making our proposal a ground-breaking step in the application of ML-MIL in healthcare; 2) Proposing a simple yet highly effective method that leverages self-attention mechanisms and transformer models to enhance the interaction between slide and patient information. This innovative approach not only improves the classification performance at the patient level but also at the slide level, showcasing its effectiveness and versatility; 3) Conducting extensive experiments on two separate datasets. Our method was seamlessly integrated with two prior state-of-the-art methods, demonstrating its compatibility and adaptability. The experiments resulted in improved performance, indicating that our method enhances the efficacy of these existing approaches."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.1,Overview,"Our proposed method P&SrE is illustrated in Fig. 2. Specifically, the framework consists of two parts. The first part is the slide-patch level MIL based on a state-of-the-art MIL method. The second part is the patient-slide level MIL, which generates patient-level features using attention mechanism and interacts the features with transformer. To enhance readability, we first provide the following symbolization for ML-MIL: For a patient X i , it has a patient-level classification label Y i . For patient X i , there may exist N i slides S i ={s j |j=1 to N i }, where the classification label for each slide s j is denoted as z j . For each slide s j , it may be divided into M j patches P j ={p k |k=1 to M j }. Here, i,j, and k are indices for patient, slide, and patch levels, respectively. Our proposed framework has strong scalability as it can be based on any attention-based MIL method. Therefore, we directly use the state-of-the-art (SOTA) MIL methods, ABMIL [8] and DSMIL [9] for the slide-patch stage. These two methods differ in their attention computing approach for each patch.For ABMIL, the attention of each patch is computed by an MLP. Specifically, for M j patches p k , an encoder is applied to obtain the patch feature matrix F i , where,F i ∈ R Mj ×1024 . Then, F i is passed through an fc layer followed by a Tanh activation and another fc layer followed by a sigmoid activation to obtain two feature matrices, F i and F i , both ∈ R Mj ×128 . These matrices are elementwise multiplied and then passed through an fc layer to obtain the weight of each patch, ω k .For DSMIL, the attention of each patch is based on the cosine distance between instances and key instances. First, an fc layer is applied to the patch feature matrix F i to obtain the importance score θ k for each patch. The patch with the highest score is selected as the key instance. Then, the feature matrix F i is mapped to a matrix Q i ∈ R Mj ×128 and the cosine similarity between all instances and the key instance is computed as the weight of each patch, ω k .Although ABMIL and DSMIL compute attention differently, both methods compute the attention-weighted sum of patch instances features as the bag representation of the slide. Therefore, the slide feature output by both methods can be generalized as:Finally, we obtain the feature vector set H i ={h j |j=1 to N i } for all slides {s j } of patientX i through patch-slide MIL."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.3,Patient-Slide Level MIL,"After performing patch-slide level MIL, we move on to patient-slide level MIL. In general MIL algorithms, the patient is regarded as the bag and the slide as the instance. However, considering the diagnostic process in clinical practice, we propose to treat both patients and slides as instances at the same level. Specifically, our P&SrE framework for patient-slide level consists of two parts: patient-level feature generation based on self-attention and patient-slide feature interaction based on Transformer [11].Patient-Level Feature Generation Based on Self-attention. Doctors usually select certain key slides for careful observation and information aggregation during diagnosis, similar to the self-attention mechanism. Therefore, we directly use a fully connected (FC) layer to integrate the feature-level features into patient-level features v i through attention mechanism, serving as patient instances. Specifically, given the feature vector collection {h j } from multiple slides in the previous step, we input it to the FC layer and apply the sigmoid activation function to output the weight α j for each h j . Then, we perform a weighted average of the vectors based on this weight to obtain the patient feature v i :Patient-Slide Feature Interaction Based on Transformer. This process is where our framework shines. After doctors summarize the patient-level results, they typically review the slides to double-check the diagnosis results. This patient-slide feature interaction (PSFI) naturally lends itself to the construction of a Transformer, and information exchange and integration between slides and patient level are bidirectional. Thus, self-attention is more ideal for this purpose than other kinds of attention (such as cross-attention or doctors' attention). By using the self-attention-based transformer structure, each input token is treated equally (i.e., viewed as the same instance level), and tokens can interact extensively with each other, enabling mutual correction between patients and slides and even between slides. Specifically, we merge the slide feature set {h j } and the patient feature v i into the input tokensand then input them into a multi-layer transformer through self-attention and feed-forward neural network layers to obtain the interaction information between slides and output tokens T out i :where d is the dimension of the token, and t k and t l come from T in i . β k,l is multi-head attention matrix, and W Q , W K , and W V are weight matrices of query, key, and value, respectively. W R and W O are transformation matrices. b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the t k are fed to the successive transformer layer. Finally, we obtain the output tokensThen, all output tokens are input into a shared FC layer, and the patient's predicted logits Y i and the predicted classification logits {z j |j = 1 to N i } for each slide are output.Training Progress and Loss Function. During training, we sampled one patient at a time and pre-extracted their batch-level features for all slides, in order to save GPU memory. Due to the issue of class imbalance in both slide level and patient level, we use the LADE [7] loss function."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.1,Dataset and Evaluation,"CD-ITB Dataset. CD-ITB is a private dataset consisting of 853 slides from 163 patients, with binary patient-level labels of CD or ITB in a ratio of 103:60 and tri-class slide-level labels of CD, ITB, and normal slides in a ratio of 436:121:296, respectively. On average, there were 5 slides per patient. The slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were curated by experienced pathologists. We adopted a patient-level stratification approach for 5-fold cross-validation, with 20% of the training set randomly assigned as the validation set for each fold. The dataset comprises an average of 2.3k instances per bag, with the largest bag containing over 16k instances.Camelyon17 Dataset. Camelyon17 [1] is a publicly dataset, and its training set comprises 500 slides from 100 breast cancer patients with lymph node metastases. The slides are classified into four distinct categories, namely negative, ITC, micro, and macro, in proportions of 318:36:59:87, respectively. There were 5 slides per patient on average. The patients are divided into two groups based on their pN stage, namely lymph node positive and lymph node negative, in proportions of 24:76, respectively. The data folding method is the same as the CD-ITB dataset. The average number of instances per bag is approximately 6.1k, and the largest bag contains over 23k instances.Metrics. We report class-wise weighted accuracy (Acc), precision(Pre), Recall, and F1-score (F1). To avoid randomness, we run all experiments five times and report the averaged metrics."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.2,Implementation Details,"We utilized ResNet50, which was pre-trained on ImageNet1K, to extract features from patches. Each patch was of size 512 × 512 pixels. For both ABMIL and DSMIL networks, we kept the original parameters for the number of channels at each layer. Following the reference [4], we employed a transformer with 8 heads and 8 layers in the patient-slide feature interactions. All networks are implemented using PyTorch and trained on a NVIDIA RTX TITAN GPU with 24 GB memory. We employed two Adam optimizers with a maximum learning rate of 1e-4 and a cosine annealing update strategy that gradually decreased the learning rate to 1e-12 over 300 epochs."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.3,Comparisons and Results,"We compared our strategy with two state-of-the-art MIL methods to evaluate its performance. To investigate the impact of self-attention and transformers on slide-level and case-level results, we conducted ablation experiments: ""ABMIL + P&SrE (with/without PSFI)"" and ""DSMIL + P&SrE (with/without PSFI)"", respectively. For slide-level classification, we used mean pooling and max pooling to pool feature vectors of patches into a representative vector for the slide, which was then fed into a fully connected layer for classification. At the patient level, we used two approaches for prediction: MaxS, where the feature of the instance that achieves the maximum positive probability from the slide-level MIL model is selected to patient-level model, and MaxMinS, where the mean value of features of the maximum and minimum positive probability from the slide-level MIL model is selected to patient-level model.The results of 5-fold CV at the slide and patient levels are reported in Table 1 and Table 2, respectively. Our P&SrE framework improves both ABMIL and DSMIL methods at both levels. ABMIL with P&SrE improves the F1 score from 0.565 to 0.579 for the CD-ITB dataset and from 0.529 to 0.571 for the Camelyon17 dataset at the slide-level, and improves the F1 score from 0.522 to 0.599 for the CD-ITB dataset and from 0.842 to 0.861 for the Camelyon17 dataset at the patient-level. Therefore, the ablation experiments demonstrate the effectiveness of P&SrE in enhancing the classification performance at both the slide and patient levels. "
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,4.0,Limitations,"Our study has some limitations that should be addressed. For instance, we did not explore the possibility of treating patches as an equivalent level to slides and patients. The primary reason is that the vast number of patches required for analysis is significantly larger than that of slides and patients, which presents a computational challenge for training. As a result, we have not yet explored this avenue. In the future, we plan to leverage clustering and active learning methods to reduce the number of patches and enable the interaction of all three levels with the Transformer, which would further enhance the accuracy and efficiency of our proposed method."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,5.0,Conclusion,"This study proposes a highly scalable and versatile framework to address M-MIL problems. We first classify the process from patch to slide to the patient in medical pathology diagnosis as a multi-level MIL problem. Based on existing state-of-the-art MIL methods, we then extend the framework to P&SrE, which conducts feature extraction and interaction at the slide-patient level. By introducing a transformer, the framework enables iterative interaction and correction of information between patients and slides, resulting in better performance at both the patient level and slide level compared to existing state-of-the-art algorithms on two validation datasets."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,1.0,Introduction,"Deep neural networks have recently shown impressive performance on lesion quantification in positron emission tomography (PET) images [6]; however, they usually rely on a large amount of well-annotated, diverse data for model training. This is difficult or even infeasible for some applications such as lesion identification in neuroendocrine tumor (NET) images, because NETs are rare tumors and lesion annotation in low-resolution, noisy PET images is expensive. To address the data shortage issue, we propose to train a deep model for lesion detection with synthesized PET images generated from list mode PET data, which is low-cost and does not require human effort for manual data annotation.Synthesized PET images may exhibit a different data distribution from real clinical images (see Fig. 1), i.e., a domain shift, which can pose significant challenges to model generalization. To address domain shifts, domain adaptation requires access to target data for model training [5,29], while domain generalization (DG) trains a model with only source data [39] and has recently attracted increasing attention in medical imaging [1,13,15,18]. Most of current DG methods rely on multiple sources of data to learn a generalizable model, i.e., multisource DG (MDG); however, multi-source data collection is often difficult in real practice due to privacy concerns or budget deficits. Although single-source DG (SDG) using only one source dataset has been applied to medical images [12,14,32], very few studies focus on SDG with PET imaging and the current SDG methods may not be suitable for lesion identification on PET data. For instance, many existing methods use a complicated, multi-stage model design pipeline [10,23,30], which introduces an additional layer of algorithm variability. This situation will become worse for PET images, which typically have a poor signal-to-noise ratio and low spatial resolution. Several other SDG approaches [26,31,34] leverage unique characteristics of the imaging modalities, e.g., color spectrum of histological stained images, which are not applicable to PET data.In this paper, we propose a novel single-stage SDG framework, which learns with human annotation-free, list mode-synthesized PET images for generalizable lesion detection in real clinical data. Compared with domain adaptation and MDG, the proposed method, while more challenging, is quite practical for real applications due to the relatively cheaper NET data collection and annotation. Specifically, we design a new data augmentation module, which generates out-of-domain samples from single-source data with multi-scale random convolutions. We integrate this module into a deep lesion detection neural network and introduce a cross-domain consistency constraint for feature encoding between original synthesized and augmented images. Furthermore, we incorporate a novel patch-based gradient reversal mechanism into the network and accomplish a pretext task of domain classification, which explicitly promotes domain-invariant, generalizable representation learning. Trained with a single-source synthesized dataset, the proposed method provides superior performance of hepatic lesion detection in multiple cross-scanner real clinical PET image datasets, compared with the reference baseline and recent state-of-the-art SDG methods."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.0,Methodology,"Figure 2 presents the proposed SDG framework. Given a source-domain dataset of list mode-synthesized 3D PET images and corresponding lesion labels (X S , Y S ), the goal of the framework is to learn a lesion detection model H , composed of E and D, which generalizes to real clinical PET image data. The framework first feeds synthesized images X S into a random-convolution data augmentation module A and generates out-of-domain samples X A = A(X S ). Then, it provides both original and augmented images, X S and X A , to a feature encoder E , which is followed by a decoder D for lesion detection. The framework imposes a crossdomain consistency constraint on the encoder E to promote consistent feature learning between X S and X A . Meanwhile, it uses a patch gradient reversalbased domain classifier C to differentiate X A from X S and further encourages the encoder E to learn domain-agnostic representations for H ."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.1,Synthesized Data Augmentation,"In the synthesized PET image dataset, each subject have multiple simulated lesions of varying size with known boundaries [11], and thus no human annotation is required. However, this synthesized dataset presents a significant domain shift from real clinical data, as they have markedly different image textures and voxel intensity values (see Fig. 1). Inspired by previous domain generalization work [39], we introduce a specific data augmentation module to generate out-of-domain samples from this single-source synthesized dataset for generalizable model learning (see Fig. 2). Specifically, we tailor a random convolution technique [33] for synthesized PET image augmentation with the following substantial improvement: 1) extend it from single value-prediction image classification to a more challenging dense prediction task of lesion detection; 2) refine it to produce realistic augmented images where the organ regions are brighter than image background, instead of randomly switching the foreground and background intensity values; 3) place a cross-domain consistency constraint on the encoding features, rather than output predictions, of original synthesized and augmented images, so as to directly encourage consistent representation learning between the source and other domains. This module can preserve global shapes or the structure of objects (e.g., lesions and livers) in images but distorts local textures, so that the lesion detection model learned with these augmented images can generalize to unseen real-world PET image data, which typically have high lesion heterogeneity and divergent texture styles.Given a synthesized input image x S ∈ X S , our data augmentation module A first performs a random convolution operation R(x S ) with a k × k kernel R, where the kernel size k and the convolutional weights are randomly sampled from a multi-scale set K = {1, 3, 5, 7} and a normal distribution N (0, 1/k 2 ), respectively. Then, inspired by [7,35,36], we mix R(x S ) and x S to generate a new mixed image x M via a convex combination,where α ∈ [0, 1] is randomly sampled from a uniform distribution U(0, 1). This data mixing strategy allows continuous interpolation between the source domain and a randomly generated out-of-distribution domain to improve model generalizability. Finally, if the foreground (i.e., lesion region) of x M has a higher mean intensity value than the background (non-lesion region), we use x M as the final augmented image, x A = x M . Otherwise, we invert the image intensity of x M to obtainis the maximum/minimum intensity of x M and 1 is a matrix with all elements being one and the same dimension as x M . This intensity inversion operation is to ensure the lesion region has higher intensity values than other regions, mimicking the image characteristics of real-world PET data in our experiments. Here we calculate the mean intensity value of the background from the regions that have a distance greater than half of the image width from the closest lesion center.In our modeling, for each synthesized training image x S , we generate multiple augmented images (i.e., 3), {x i A } 3 i=1 , and feed them into the encoder E for feature learning. Due to the distance preservation property of random convolutions [33], the module A changes local textures but preserves object shapes at different scales, and thus x S and {x i A } 3 i=1 should have identical semantic content, such as lesion presence, quantity and positions. Therefore, they should have consistent representations in the feature space, i.e., E (x S ) ≈ E (x i A ), i = 1, 2, 3. To this end, we place a cross-domain consistency loss L con on top of the encoder E aswhere E is an expectation operator, |E (x S )| is the number of elements in E (x S ), and || • || F denotes the Frobenius norm. Unlike the previously reported work [33] promotes consistent output-layer predictions, the loss L con in Eq. ( 1) directly encourages the encoder E to extract cross-domain consistent representations, which improves model generalization more effectively for dense prediction tasks [8], such as lesion detection. We hypothesize that forcing similar feature encoding between x S and x i A can facilitate image content preservation for lesion detection. In addition, we adopt a mean squared error (MSE) to measure the consistency, different from [33] using the Kullback-Leibler divergence for image classification, which is not suitable for our application. Note that the convolution weights in module A are randomly sampled within each iteration and are not updated during model training."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.2,Patch Gradient Reversal,"Because of random convolution weights, the original synthesized X S and augmented X A data can have substantially different image appearances. Consequently, the use of the loss L con in Eq. (1) may not be sufficient to enforce consistent feature encoding. To address this issue, we propose to use a pretext task as an additional information resource for the encoder E and to further promote domain-agnostic representation learning. Specifically, we incorporate a domain classifier C on top of the encoder E to perform a pretext task of domain discrimination, i.e., predict whether each input image is from the original synthesized data X S or augmented data X A . This domain classification accompanies the main task of lesion detection (see Fig. 2) to assist with feature learning. In this way, the encoder E improves feature invariance to domain changes by penalizing domain classification accuracy, while retaining feature discriminativeness to lesion prediction via the decoder D. This is different from other methods [2,25] that use intrinsic supervision signals within a single image to perform an auxiliary task, e.g., solving jigsaw puzzles, for model generalization enhancement.In general, the classifier C will encourage the encoder E to learn discriminative features for accurate domain classification. In order to make features invariant to different domains, we reverse the gradient propagated from the domain classifier C with a multiplication of -1 [3] and send this reversed gradient to the encoder E , while keeping all the other gradient flows unchanged during the backpropagation for model training. Note that the computation in forward propagation of our network is the same as that in a standard feed-forward neural network. Compared with [3], we make the following significant improvements: 1) Instead of back propagating the reversed gradient from a single-valued prediction of the domain label of the entire input image, we introduce a patch-based gradient reversal to enhance feature representation invariance to local texture or style changes. Inspired by [9], we design the domain classifier C with a fully convolutional network and produce a prediction map, where each element corresponds to a local patch of input image, i.e., conducting small patch categorization. We then apply the reversal operation to the gradient propagated from the prediction map and feed it into the encoder E for feature learning. 2) Motivated by [17], we remove the sigmoid layer in [3] and replace the cross-entropy loss by an MSE loss, which can facilitate the adversarial training caused by the gradient reversal. With the MSE loss, the patch-based gradient reversal penalizes image structures and enhances feature robustness and invariance to style shifts at the local-patch level, so that the lesion detection model H (i.e., E followed by D) learned with source data annotations is directly applicable to unseen domains [4,24], based on the covariate shift assumption [20].Formally, let X = {X S , X A } denote the input data for the encoder E and Z = {Z S , Z A } represent the corresponding domain category labels, with Z S and Z A for the original source images X S and corresponding random convolutionaugmented image X A , respectively. Each label z ∈ Z is a 3D image with all voxel intensity being 0's for z ∈ Z S or 1's for z ∈ Z A . We define the domain classification objective L cls as followswhere ẑ = C (E (x )) is the prediction of x . For source-domain data (X S , Y S ), the augmented images X A have the same gold-standard lesion labels Y A = Y S , each of which is a 3D binary image with 1 s for lesion voxels and 0 s for non-lesion regions. Let Y = {Y S , Y A }. We formulate the lesion detection objective L det aswhere the first and second terms in Eq. ( 4) are a weighted binary cross-entropy loss and a Dice loss, respectively. We add a smooth term, = 10 -6 , to the Dice loss to avoid division by zero. The y j and ŷj are the j-th values of y and corresponding prediction ŷ, respectively. The β controls the relative importance between the two losses, and γ emphasizes the lesions in each image. The combo loss L det can further help address the data imbalance issue [22], i.e., lesions have significantly fewer voxels than the non-lesion regions including the background. With the losses in Eqs. ( 1)-( 4), we define the following full objective aswhere λ con and λ cls are weighting parameters. Note that while we minimize L for model training, we reverse the gradient propagated from the domain classifier C before sending it to the encoder E during the backpropagation."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,3.0,Experiments,"Datasets. We evaluate the proposed method with multiple 68 Ga-DOTATATE PET liver NET image datasets that are acquired using different PET/CT scanners and/or imaging protocols. The synthesized source-domain dataset contains 103 simulated subjects, with an average of 5 lesions and 153 transverse slices per subject. This dataset is synthesized using list mode data from a single real, healthy subject acquired on a GE Discovery MI PET/CT scanner with list mode reconstruction [11,37]. We collect two additional real 68 Ga-DOTATATE PET liver NET image datasets that serve as unseen domains. The first dataset (Real1) has 123 real subjects with about 230 hepatic lesions in total and is acquired using clinical reconstructions with a photomultiplier tube-based PET/CT scanner (GE Discovery STE). The second real-world dataset (Real2) consists of 65 cases with around 113 lesions and is acquired from clinical reconstructions using a digital PET/CT scanner (GE Discovery MI). Following [28,38], we randomly split the synthesized dataset and the Real1 dataset into 60%, 20% and 20% for training, validation and testing, respectively. Due to the relatively small size of Real2, we use a two-fold cross-validation for model evaluation on this dataset. Here we split the real datasets to learn fully supervised models for a comparison with the proposed method. Implementation Details and Evaluation Metrics. We implement the encoder E and the decoder D with a U-Net architecture [19], with four downsampling and upsampling layers in the encoder and decoder, respectively. We build the domain classifier C using three stacked stride-1 convolutional layers of kernel size of 4, and each convolution is followed by a batch normalization and a leaky ReLU activation [16]. We set β = 6, γ = 5 in Eq. ( 4) and λ con = 1, λ cls = 1 in Eq. (5). We train the model using stochastic gradient descent with Nesterov momentum with learning rate = 5 × 10 -4 , momentum = 0.99 and batch size = 1. We perform standard image augmentation including random scaling, noise adding and image contrast adjustment before applying random convolutions in the module A. In the testing stage, we adopt the model H to produce a prediction map for each input image, and identify lesions with a threshold (i.e., 0.1) to binarize the map followed by a connected component analysis, which helps detect individual lesions by identifying connected regions from the binarized map. We use precision, recall and F 1 score as model evaluation metrics [21,28,38].Comparison with State of the Art. We compare our method with several recent state-of-the-art SDG approaches, including causality-inspired SDG (CISDG) [18], RandConv [33], and learning to diversify (L2D) [27]. We run each model 5 times with different random seeds and report the mean and standard deviation. Table 1 presents the comparison results on the two unseen-domain datasets. Our method significantly outperforms the state-of-the-art approaches in terms of F 1 score, with p-value < 0.05 in Student's t-test for almost all cases on both datasets. In addition, our method gives lower standard deviation of F 1 than others. This indicates that compared with the competitor approaches, our method is relatively more effective and stable in learning generalizable representations for lesion detection in a very challenging situation, i.e., learning with a single-source synthesized PET image dataset to generalize to real clinical data.The qualitative results are provided in the Supplementary Material.Ablation Study. In Table 1, the Baseline represents a lesion detection model trained with the source data but without the data augmentation module A, L con or L cls . We then evaluate different variants of our method by sequentially adding one component to the Baseline model: 1) using only the module A for model training (Aug.), 2) using module A and L con (Aug.+L con ), and 3) using module A, L con and L cls (Ours). We also report the performance of the model, Aug.+L con +gGR, which does not use the proposed patch-based gradient reversal but outputs a single-value prediction for the entire input image, i.e., global gradient reversal (gGR). The U pper-bound means training with real-world images and gold-standard labels from the testing datasets. We note that using the data augmentation module A can significantly improve the lesion detection performance compared with the Baseline on the Real1 dataset, and combining data augmentation and patch gradient reversal can further close the gap to the U pper-bound model. Our method also outperforms the Baseline model by a large margin on the Real2 dataset, suggesting the effectiveness of our method. Effects of Parameters. We evaluate the effects of λ con and λ cls of our method on lesion detection in Fig. 3. The lesion detection performance improves when increasing λ con from 0.1 to 1. However, a further emphasis on consistent feature encoding, e.g., λ con ≥ 5, decreases the F 1 score. This suggests the importance of an appropriate λ con value. In addition, we observe a similar trend of the F 1 curve for the λ cls , especially for the Real1 dataset, and this indicates the necessity of the domain classification pretext task."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,4.0,Conclusion,"We propose a novel SDG framework that uses only a single dataset for hepatic lesion detection in real clinical PET images, without any human data annotations. With a specific data augmentation module and a new patch-based gradient reversal, the framework can learn domain-invariant representations and generalize to unseen domains. The experiments show that our method outperforms the reference baseline and recent state-of-the-art SDG approaches on cross-scanner or -protocol real PET image datasets. A potential limitation may be the need of a proper selection of weights for different tasks during model training."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 12.
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,1.0,Introduction,"Identifying unusual patterns in data is of great interest in many applications such as medical diagnosis, industrial defect inspection, or financial fraud detection. Finding anomalies in medical images is especially hard due to large inter-patient variance of normality, the irregular appearance-, and often rare occurrence of diseases. Therefore, it is difficult and expensive to collect large amounts of annotated samples that cover the full abnormality spectrum, with supervised [7,15] Fig. 1. MorphAEus outperforms AEs by generating ID reconstructions even for far OoD cases (Fig. 1a), enabling accurate pathology localization (Figure 1b). and self-supervised [10,16] methods only capturing limited facets of the abnormal distribution [31]. However, since it is more feasible to obtain large data sets with normal samples, it is common to detect outliers by detecting patterns that deviate from the expected normative distribution.Reconstruction-based AEs have emerged as a very popular framework for unsupervised anomaly detection and are widely adopted in medical imaging [2]. They provide straight-forward residual error maps, which are essential for safetycritical domains such as medical image analysis. However, recent work suggests that AEs might reconstruct out-of-distribution (OoD) samples even better than in-distribution (ID) samples [28], with the learned likelihood being dominated by common low-level features [32]. While this can be useful for some tasks such as reconstruction [34], or restoration [21], it often fails for pathology detection as anomalies can be missed due to small residual errors. In Fig. 1 we show that AEs that have only been trained on healthy chest X-rays are also able to reconstruct OoD samples like pathologies or hands. Similarly, Perera et al. [24] showed that AEs trained on the digit 8 can also reconstruct digits 1,5,6 and 9.Much effort has been made in the medical imaging community to improve the limitations of traditional anomaly detection methods, particularly in the context of brain MRI. Apart from the reduced dimensionality of the bottleneck, several other techniques have been introduced to regularize AEs [11,20,29,38]. Recently, self-supervised denoising AEs [16] achieved SOTA results on brain pathology segmentation. They explicitly feed noise-corrupted inputs x = x+ to the network with the aim at reconstructing the original input x. However, this is especially beneficial when the anomaly distribution is known a priori. Variational AEs (VAEs) [5,12,17,40] estimate the distribution over the latent space that is regularized to be similar to a prior distribution, usually a standard isotropic Gaussian. Generative adversarial networks have also been applied to anomaly detection [24,33]. Pidhorskyi et al. [26] trained AEs with an adversarial loss to detect OoD samples. More recently, introspective variational AEs [8] use the VAE encoder to differentiate between real and reconstructed samples, achieving SOTA image generations and outlier detection performance. Recently, Zhou et al. [39] investigated the limitations of AEs for OoD. Similarly, we believe that AEs should have two properties: i) minimality: the networks should be constrained to only reconstruct ID samples and ii) sufficiency: the decoder should have sufficient capacity to reconstruct ID samples with high accuracy. In contrast to [39], where the authors aim at reconstructing only low-dimensional features needed for the classification task, we are interested in reconstructing pseudo-healthy images to enable pixel-wise localization of anomalies.In this work, we first investigate whether SOTA AEs can learn meaningful representations for anomaly detection. Specifically, we investigate whether AEs can learn the healthy anatomy, i.e., absence of pathology, and generate pseudohealthy reconstructions of abnormal samples on challenging medical anomaly detection tasks. Our findings are that SOTA AEs either do not efficiently constrain the latent space and allow the reconstruction of anomalous patterns, or that the decoder cannot accurately restore images from their latent representation. The imperfect reconstructions yield high residual errors on normal regions (false positives) that can easily overshadow residuals of interest, i.e., pathology [23]. We then propose MorphAEus, novel deformable AEs to learn minimal and sufficient features for anomaly detection and drastically reduce false positives. Figure 1a shows that MorphAEus learns the training distribution of healthy chest X-rays and yields pseudo-healthy ID reconstructions even for far OoD samples. This allows to localize pathologies, as seen in Fig. 1b.Our manuscript advances the understanding of anomaly detection by providing insights into what AEs learn. In summary, our contributions are:• We broaden the understanding of AEs and highlight their limitations.• We test whether SOTA AEs can learn the training distribution of the healthy population, accurately reconstruct inputs from their latent representation and reliably detect anomalies. • As a solution, we propose MorphAEus, novel deformable AEs that provide pseudo-healthy reconstructions of abnormal samples and drastically reduce false positives, achieving SOTA unsupervised pathology detection."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.0,Background,"The widely held popular belief is that AEs can learn the distribution of the training data and identify outliers from inaccurate reconstructions of abnormal samples [31]. This section aims to discuss the common assumptions of unsupervised anomaly detection, specifically for AEs, while also outlining the challenges and desired properties associated with these techniques."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.1,Unsupervised Anomaly Detection: Assumptions,"Let X ⊂ R N be the data space that describes normal instances for a given task.The manifold hypothesis implies that there exists a low-dimensional manifold M ⊂ R D ⊂ X where all the points x ∈ X lie, with D N [9]. For example, a set of images in pixel space X could have a compact representation describing features like structure, shape, or orientation in M.Given a set of unlabeled data x 1 , .., x n ∈ X the objective of unsupervised representation learning is to find a function f : R N → R D and its inverse g : R D → R N , such that x ≈ g(f (x)), with the mapping f defining the lowdimensional manifold M. The core assumption of unsupervised anomaly detection is that once such functions f and g are found, the learned manifold M would best describe the normal data samples in X and results in high reconstruction errors for data-points x / ∈ X , that we call anomalous. An anomaly score is therefore usually derived directly from the pixel-wise difference:The nominal and abnormal distributions are considerably separated from each other when x is from a different domain. However, anomalies are often defects in otherwise normal images. In medical imaging, the set X describes the healthy anatomy and the data set X usually contains images with both healthy and pathological regions. The two distributions usually come from the same domain and might overlap considerably. The core assumption is that only the normal structures can be reconstructed from their latent representation very well, with the pathological regions ideally replaced by healthy structures. Therefore x ≈ g(f (x)) ∈ X would represent the healthy synthesis of the abnormal sample x and the residual |x -g(f (x))| would highlight only the abnormal regions."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.2,Auto-Encoders: Challenges,"AEs aim to extract meaningful representations from data, by learning to compress inputs to a lower-dimensional manifold and reconstruct them with minimal error. They use neural networks to learn the functions f and g, often denoted as encoder E θ with parameters θ and decoder D φ parameterized by a set of parameters φ. The embedding z = E(x|θ) is a projection of the input to a lower-dimensional manifold Z, also referred to as the bottleneck or latent representation of x. The standard objective of AEs is finding the set of parameters θ and φ that minimize the residual, with the mean squared error (MSE) being a popular choice for the reconstruction error:In the introduction, we presented the desired properties of AEs for outlier detection, namely i) reconstructions should match the training distribution and ii) decoders have sufficient capacity to accurately restore inputs. Figure 2 shows reconstructions of spatial AEs [2] for near OoD samples, i.e., containing pathologies, and far OoD samples using real images of celebrities [19]. AEs with few encoding layers learn to copy and can reconstruct both near-and far OoD. Interestingly, with increasing layer depth, AEs can learn the prior over the training distribution, avoid the reconstruction of pathologies, and project OoD celebrities to the closest chest X-ray counterparts. However, this comes at the cost of losing spatial information and not reconstructing small details, e.g., ribs. The resulting high residual error on healthy tissues (false positives) overshadows the error on pathology [23], rendering standard AEs unsuitable for anomaly detection."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,3.0,MorphAEus: Deformable Auto-encoders,"We propose MorphAEus, deformable AEs that learn minimal and sufficient features for anomaly detection, see Fig. 3. We use deep perceptual AE to provide pseudo-healthy ID reconstructions and leverage estimated deep deformation fields to drastically reduce the false positives.Pseudo-healthy Reconstructions. Given a dataset X = {x 1 , .., x n } we optimize the encoder and decoder with parameters θ, φ to minimize the MSE loss between the input and its reconstruction. For minimality, we propose to use deep AEs constrained to reconstruct only ID samples (see Fig. 2), but add a perceptual loss (PL) [14,37] to encourage reconstructions that are perceptually similar to the training distribution. The reconstruction loss is given by:with x rec = D φ (E θ (x)), P L(x, x rec ) = l (V GG l (x)-V GG l (x rec )) 2 with V GG l being the output of the l ∈ {1, 6, 11, 20}-th layer of a pre-trained VGG-19 encoder. We have empirically found α = 0.05 to be a good weight to predict perceptually similar reconstructions without compromising pixel-wise accuracy.Local Deformation. Imperfect reconstructions yield high residuals on normal regions which might overshadow the residuals errors associated with anomalous regions. Skip connections [30] would allow the network to bypass the learned manifold and copy anomalies at inference. Instead, we propose to align the reconstruction with the input using corresponding encoder and decoder features. We denote these shared parameters as θ S ⊂ θ and φ S ⊂ φ. Inspired by the advances in image registration [1] and computer vision [4,36] we estimate dense deformation fields Φ to allow local morphometric adaptations:where x morph = x rec • Φ, ψ are the deformation parameters, LNCC is the local normalized cross correlation, • is a spatial transformer and β weights the smoothness constraint on the deformation fields. We opted for the LNCC instead of the MSE to emphasize shape registration and enhance robustness to intensity variations in the inputs and reconstructions. By sharing encoder/decoder parameters, the deformation maps are not only beneficial at inference time, but also guide the training process to learn more accurate features. The full objective is given by the two losses:To ensure a good initialization for the deformation estimation, we introduce the deformation loss after 10 epochs. Deformable registration between normal and pathological samples is in itself an active area of research [18,25]. In particular, the deformation could mask structural abnormalities at inference if not constrained. In our experiments, we linearly increase β from 1e -3 to 3 to constrain the deformation more as the reconstruction improves (see Appendix for details). Nevertheless, recent advances allow the estimation of the optimal registration parameters automatically [13]. If not specified otherwise, we employ x morph for inference."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,4.0,Pathology Detection on Chest X-rays,"In this section, we investigate whether AEs can learn the healthy anatomy, i.e., absence of pathology, and generate pseudo-healthy reconstructions of abnormal chest X-ray images. Pathology detection algorithms are often applied to finding hyper-intense lesions, such as tumors or multiple sclerosis on brain scans. However, it has been shown that thresholding techniques can outperform learningbased methods [22]. In contrast, the detection of pathology on chest radiographs is much more difficult due to the high variability and complexity of nominal features and the diversity and irregularity of abnormalities.Datasets. We use the Covid-19 radiography database on Kaggle [6,27]. We used the RSNA dataset [35], which contains 10K CXR images of normal subjects and 6K lung opacity cases. For the detection of Covid-19, we used the Padchest  dataset [3] containing CXR images manually annotated by trained radiologists. We used 1.3K healthy control images and 2.5K cases of Covid-19.Results. Of the baselines, only adversarially-trained AEs can reconstruct pseudo-healthy images from abnormal samples, as shown in Fig. 4. However, their imperfect reconstructions overshadow the error on pathology leading to poor anomaly detection results, as reflected in the SSIM and AUROC in Table 1. Spatial AEs and DAEs have a tendency to reproduce the input and produce reconstructions of structures that are not included in the training distribution, such as medical devices and pathologies, despite not being trained on OoD data. This can lead to false negatives. DAEs can avoid the reconstruction of some pathologies and achieve good anomaly detection results. However, pathological regions that do not conform to the learned noise model are completely missed, as shown in Fig. 4. The next three methods (AE-D, VAE, and β-VAE) produce blurry reconstructions, as it can be best seen in the LPIPS score. MorphAEus is the only method to yield accurate pseudo-healthy reconstructions and effectively remove anomalies, such as pathology or implanted medical devices. This enables to precisely localize pathologies, considerably outperforming the baselines.Ablation Study: Importance of Morphological Adaptations. We evaluate the effectiveness of individual components of MorphAEus in Fig. 5. AEs without perceptual loss tend to not reconstruct small but important features such as ribs and yield false positives on healthy tissue. Interestingly, AEs with perceptual loss achieve more visually appealing reconstructions, but fail at detecting anomalies because the pathological region is overshadowed by false positive residuals on edges and misaligned ribs. Morphometric adaptations guide the networks to learn better representations, reduce the number of false positives, and enable the localization of pathologies. This considerably improves the detection results to 84.8 and 82.1 for AEs with and without perceptual loss, respectively. It is important to note that the deformations are not only beneficial at the time of inference, but also drive the learning process towards better representations. Thereby, the average pathology detection increases from 66.8 to 80.2, even if no adaptations are made during inference, i.e., using x rec instead of x morph for inference (see Appendix for details)."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,5.0,Discussion,"In this work, we have investigated whether AEs learn meaningful representations to solve pathology detection tasks. We stipulate that AEs should have the desired property of learning the normative distribution (minimality) and producing highly accurate reconstructions of ID samples (sufficiency). We have shown that standard, variational, and recent adversarial AEs generally do not satisfy both conditions, nor are they very suitable for pathology detection tasks where the distribution of normal and abnormal instances highly overlap.In this paper, we introduced MorphAEus, a novel deformable AE that demonstrated notable performance improvement in detecting pathology. We believe our method is adaptable to various anomaly types, and we are eager to extend our research to different anatomies and imaging modalities, building upon promising early experiments. However, it is important to address false positive detection, which could be influenced by unlabelled artifacts like medical devices. Our future work aims to conduct a thorough analysis of false positives and explore strategies to mitigate their impact, ultimately enhancing the accuracy.Although there are obstacles to overcome, AEs remain a viable option for producing easily understandable and interpretable outcomes. Nevertheless, it is crucial to continue improving the quality of the representations to advance unsupervised anomaly detection. Our findings demonstrate that MorphAEus is capable of learning superior representations, and can leverage the predicted dense displacement fields to refine its predictions and minimize the occurrence of false positives. This allows for accurate identification and localization of diseases, resulting in SOTA unsupervised pathology detection on chest X-rays."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,,Table 1 .,* 2.8 * 78.0 ± 0.5 76.1 ± 0.7 77.0 MorphAEus (ours) 85.7 9.5 83.6 ±
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,1.0,Introduction,"In current radiology practice, a signed report is often the primary form of communication, to communicate results of a radiological imaging exam between radiologist. Speech recognition software (SRS), which converts dictated words or sentences into text in a report, is widely used by radiologists. Despite SRS reducing the turn-around times for radiology reports, correcting any transcription errors in the report has been assumed by the radiologists themselves. But, persistent report communication errors due to SRS can significantly impact report interpretation, and also have dire consequences for radiologists in terms of medical malpractice [1]. These errors are most common for cross-sectional imaging exams (e.g., CT, MR) and chest radiography [2]. Problems also arise when re-examining the results from external examinations and in interventional radiology procedural reports. Such errors are due to many factors, including SRS finding a nearest match for a dictated word, the lack of natural language processing (NLP) for real-time recognition and dictation conversion [2], and unnoticed typographical mistakes. To mitigate these errors, a promising alternative is to automate the pre-filling of a radiology report with salient information for a radiologist to review. This enables standardized reporting via structured reporting. A number of methods to generate radiology reports have been proposed previously, with significant focus on CXR images [3][4][5][6][7][8][9][10][11]. Various attention mechanisms were proposed [4,6,12] to drive the encoder and the decoder to emphasize more informative words in the report, or visual regions in the CXR, and improve generation accuracy. Other approaches [8][9][10] effectively used Transformer-based models with memory matricies to store salient information for enhanced report generation quality. Despite these advances, there has been scarce research into harnessing the potential of longitudinal patient visits for improved patient care.In practice, CXR images from multiple patient visits are usually examined simultaneously to find interval changes; e.g., a radiologist may compare a patient's current CXR to a previous CXR, and identify deterioration or improvement in the lungs for pneumonia. Reports from longitudinal visits contain valuable information regarding the patient's history, and harnessing the longitudinal multimodal data is vital for the automated pre-filling of a comprehensive ""findings"" section in the report.In this work, we propose to use longitudinal multi-modal data, i.e., previous visit CXR, current visit CXR, and previous visit report, to pre-fill the ""findings"" section of the patient's current visit report. To do so, we first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset1 and created a new dataset called Longitudinal-MIMIC. Using this new dataset, we trained a transformer-based model containing a cross-attention-based multimodal fusion module and a hierarchical memory-driven decoder to capture the features of longitudinal multi-modal data (CXR images + reports). In contrast to current approaches that only use the current visit data as input, our model exploits the longitudinal information available to pre-fill the ""findings"" section of reports with accurate content. Experiments conducted with the proposed dataset and model validate the utility of our proposed approach. Our main contribution in this work is training a transformer-based model that fully tackles the longitudinal multi-modal patient visit data to pre-fill the ""findings"" section of reports."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,2.0,Methods,"Dataset. The construction of the Longitudinal-MIMIC dataset involved several steps, starting with the MIMIC-CXR dataset, which is a large publicly available dataset of 377,110 chest X-ray images corresponding to 227,835 radiographic reports from 65,379 patients [13]. The first step in creating the Longitudinal-MIMIC dataset was to pre-process MIMIC-CXR to ensure consistency with prior works [8,9]. Specifically, patient visits where the report did not contain a ""findings"" section were excluded. For each patient visit, there was at least one chest X-ray image (frontal, lateral or other view) and a corresponding medical report. In our work, we only generated pre-filled reports with the ""findings"" section. Next, the pre-processed dataset was partitioned into training, validation, and test sets using the official split provided with the MIMIC-CXR dataset. Table 1 shows that 26,625 patients in MIMIC-CXR had ≥ 2 visit records, providing a large cohort of patients with longitudinal study data that could be used for our goal of pre-filling radiology reports. For patients with ≥2 visits, consecutive pairs of visits were used to capture richer longitudinal information. The dataset was then arranged chronologically based on the ""StudyTime"" attribute present in the MIMIC-CXR dataset. ""StudyTime"" represents the exact time at which a particular chest X-ray image and its corresponding medical report were acquired.Following this, patients with ≥2 visit records were selected, resulting in 26,625 patients in the final Longitudinal-MIMIC dataset with a total of 94,169 samples. Each sample used during model training consisted of the current visit CXR, current visit report, previous visit CXR, and the previous visit report. The final dataset was divided into training (26,156 patients and 92,374 samples), validation (203 patients and 737 samples), and test (266 patients and 2,058 samples) splits. We aimed to create the Longitudinal-MIMIC dataset to enable the development and evaluation of models leveraging multi-modal data (CXR + reports) from longitudinal patient visits.Model Architecture. Figure 1 shows the pipeline to generate a pre-filled ""findings"" section in the current visit report R C , given the current visit CXR image I C , previous visit CXR image I P , and the previous visit report R P . Mathematically, we can write:, where w i is the i-th word in the current report. The Text Encoder encoded text information for language feature embedding using a previously published method [15]. First, the radiology report R P was tokenized into a sequence of M tokens, and then transformed into vector representations V = [v 1 , . . . , v M ] using a lookup table [16]. They were then fed to the text encoder, which had the same architecture as the image encoder, but with distinct network parameters. The final text feature embedding H RP was defined as:"
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Encoder. Our model uses an Image Encoder,"where θ E R refers to the parameters of the report text encoder. Cross-Attention Fusion Module. A multi-modal fusion module integrated longitudinal representations of images and texts using a cross-attention mechanism [17], which was defined as: Sub-block-1 uses H IC and consists of a self-attention layer, an encoderdecoder attention layer, and feed-forward layers. It also employs residual connections and conditional layer normalization [8]. The encoder-decoder attention layer performs multi-head attention over H IC . It also uses a memory matrix M to store output and important pattern information. The memory representations not only store the information of generated current reports over time in the decoder, but also the information across different encoders. Following [8], we adopted a matrix M to store the output over multiple generation steps and record important pattern information. Then we enhance M by aligning it with H IC to create an attention-aligned memory M IC matrix. Different from [8], we use M IC while transforming the normalized data instead of M . The decoding process of sub-block-1 D I is formalized as: H dec,b,I = D I (H O , H IC , M IC ), where b stands for the block index. The output of sub-block 1 is combined with H O through a fusion layer: H dec,b = (1β)H O + βH dec,b,I . β is a hyper-parameter to balance H O and H dec,b,I . In our experiment, we set it to 0.2.The input to sub-block-2 D L is H dec,b . This structure is similar to sub-block-1, but interacts with H L instead of H IC . The output of this block is H dec,b,L and combined with H dec,b,I by adding them together. After fusing these embeddings and doing traditional layer normalization for them, we use these embeddings as the output of a block. The output of the previous block is used as the input of the next block. After N blocks, the final hidden states are obtained and used with a Linear and Softmax layer to get the target report probability distributions."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,3.0,Experiments and Results,"Baseline Comparisons. We compared our proposed method against prior image captioning and medical report generation works respectively. The same Longitudinal-MIMIC dataset was used to train all baseline models, such as AoANet [18], CNNTrans [16], Transformer [15], R2gen [8], and R2CMN [9]. Implementation of these methods is detailed in the supplementary material.Evaluation Metrics. Conventional natural language generation (NLG) metrics, such as BLEU [19], MET EOR [20], and Rouge L [21] were used to evaluate the utility of our approach against other baseline methods. Similar to prior work [8,16], the CheXpert labeler [22] classified the predicted report for the presence of 14 disease conditions2 and compared them against the labels of the groundtruth report. Clinical Efficacy (CE) metrics, such as; accuracy, precision, recall, and F-1 score, were used to evaluate model performance. 2 shows the summary of the NLG metrics and CE metrics for the 14 disease observations for our proposed approach when compared against prior baseline approaches. In particular, our model achieves the best performance over previous baselines across all NLG and CE metrics."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Results. Table,"Generic image captioning approaches like AoANet resulted in unsatisfactory performance on the Longitudinal-MIMIC dataset as they failed to capture specific disease observations. Moreover, our approach outperforms previous report generation methods, R2Gen and R2CMN that also use memory-based models, due to the added longitudinal context arising from the use of longitudinal multimodal study data (CXR images + reports). In our results, the BLEU scores show a substantial improvement, particularly in BLEU-4, where we achieve a 1.4% increase compared to the previous method R2CMN. BLEU scores measure how many continuous sequences of words appear in predicted reports, while Rouge L evaluates the fluency and sufficiency of predicted reports. The highest Rouge L score demonstrates the ability of our approach to generate accurate reports, rather than meaningless word combinations. We also use METEOR for evaluation, taking into account the precision, recall, and alignment of words and phrases in generated reports and the ground truth. Our METEOR score shows a 1.1% improvement over the previous outstanding method, which further solidifies the effectiveness of our approach. Meanwhile, our model exhibits a significant Fig. 2. Two examples of pre-filled ""findings"" sections of reports. Gray highlighted text indicates the same words or words with similar meaning that appear in the current reports and other reports. Purple highlighted text represents similar words in the current visit report generated by our approach, previous visit reports, and groundtruth current visit report. The red highlighted text indicates similar words that only exist in the report generated by our approach and the current ground truth report. R2Gen was the baseline method that generated the report. The ""Labels"" array shows the CheXpert classification of 14 disease observations (see text for details) as positive (1), negative (-1), uncertain (0) or unmentioned (×). (Color figure online) improvement in clinical efficacy metrics compared to other baselines. Notably, F1 is the most important metric, as it provides a balanced measure of both precision and recall. Our approach outperforms the best-performing method by 3.1% in terms of F1 score. These observations are particularly significant, as higher NLG scores do not necessarily correspond to higher clinical scores [8], confirming the effectiveness of our proposed method.Effect of Model Components. We also studied the contribution of different model components and detail results in Table 2. The Baseline experiment refers to a basic Transformer model trained to generate a pre-filled report given a chest CXR image without any additional longitudinal information. The NLG and CE metrics are poor for the vanilla transformer compared to our proposed approach. We also analyze the contributions of the previous chest CXR image + image and previous visit report + report when added to the model separately. These two experiments included memory-enhanced conditional normalization. We observed that with each added feature enhanced the pre-filled report quality compared to the baseline, but the previous visit report had a higher impact than the previous CXR image. We hypothesize that the previous visit reports contain more text that can be directly transferred to the current visit reports.In our simple fusion experiment, we removed the cross-attention module and concatenated the encoded embeddings of the previous CXR image and previous visit report as one longitudinal embedding, while retaining the rest of the model. We saw a performance drop compared to our approach on our dataset, and also noticed that the results were worse than using the images or reports alone. These experiments demonstrate the utility of the cross-attention module in our proposed work."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,4.0,Discussion and Conclusion,"Case Study. We also ran a qualitative evaluation of our proposed approach on two cases as seen in Fig. 2. In these cases, we compare our generated report with the report generated by the R2Gen. In the first case, certain highlighted words in purple, such as ""status post"", ""aortic valve"" and ""cardiac silhouette in the predicted current visit report are also seen in the previous visit report. The CheXpert classified ""Labels"" also show the pre-filled ""findings"" generated is highly consistent with the ground truth report in contrast to the R2Gen model. For example, the ""cardiac silhouette enlarged"" was not generated by the R2Gen model, but our prediction contains them and is consistent with the word ""cardiomegaly"" in the ground truth report. In the second case, our generated report is also superior. Not only does our report generate more of the same content as the ground truth, but the positive diagnosis labels classified by CheXpert in our report are completely consistent with those in the ground truth. We also provide more cases in the supplementary material.Error Analysis. To analyze errors from our model, we examine generated reports alongside ground truths and longitudinal information. It is found that the label accuracy of the observations in the generated reports is greatly affected by the previous information. For example, as time changes, for the same observation ""pneumothorax"", the label can change from ""positive"" to ""negative"". And such changing examples are more difficult to generate accurately. According to our statistics, on the one hand, when the label results of current and previous report are the same, 88.96% percent of the generated results match them. On the other hand, despite mentioning the same observations, when the labels of current and previous report are different, there is an 84.42% probability of generated results being incorrect. Thus how to track and generate the label accurately of these examples is a possible future work to improve the generated radiology reports. One possible way to address this issue is to use active learning [23] or curriculum learning [24] methods to differentiate different types of samples and better train the machine learning models."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Conclusion.,"In this paper, we propose to pre-fill the ""findings"" section of chest X-Ray radiology reports by considering the longitudinal multi-modal (CXR images + reports) information available in the MIMIC-CXR dataset. We gathered 26,625 patients with multiple visits to constitute the new Longitudinal-MIMIC dataset, and proposed a model to fuse encoded embeddings of multi-modal data along with a hierarchical memory-driven decoder. The model generated a pre-filled ""findings"" section of the report, and we evaluated the generated results against prior image captioning and medical report generation works. Our model yielded a ≥ 3% improvement in terms of the clinical efficacy F-1 score on the Longitudinal-MIMIC dataset. Moreover, experiments that evaluated the utility of different components of our model proved its effectiveness for the task of pre-filling the ""findings"" section of the report."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,,"where q(•), k(•), and v(•) are linear transformation layers applied to features of proposals. d k is the number of attention heads for normalization. Finally, H"
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 19.
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",1.0,Introduction,"Pulp calcification is a type of pulp degeneration, characterized by the deposition of calcified tissue in the root canal. Clinically, pulp calcification usually occurs with pulp periapical diseases, which brings great challenges to root canal therapy. Finding pulp calcification before root canal treatment is very important for dentists to decide treatment strategies [1]. However, teeth with pulp calcification usually show few clinical symptoms, which are mainly found and diagnosed by radiographic examination. Compared with X-ray, cone beam computed tomography (CBCT) performs better in displaying root canal structure and pulp disease, so it is widely used for pulp calcification detection [2]. On CBCT images, pulp calcification showed partial or complete high attenuation root canal occlusion as shown in Fig. 1. Currently, the diagnosis of pulp calcification mainly depends on the image recognition of root canal occlusion by dentists. On the one hand, although high-resolution CBCT is used, the image contrast of calcified area is rather low, and human cannot easily find all calcified tubes. On the other hand, human recognition is time-consuming and laborious, and the agreement between observers is far from satisfactory. Therefore, an intelligent recognition method of pulp calcification is urgently needed for digital dentistry in clinical practice. With the introduction of artificial intelligence into various fields of medical images, research has proposed tooth and root canal segmentation models based on deep learning networks and CBCT images [3][4][5][6][7], which have achieved good performance in segmentation. However, how to intelligently recognize pulp calcification in small root canals is still very difficult and has not yet been explored. First, the calcified area has no clear morphological characteristics and is difficult for feature representation. Then, the resolution of CBCT image has a high resolution (672 × 688 × 688 in this work), while the tooth calcification areas are relatively small and in low contrast. It is very difficult to directly process such large volume data based on deep learning networks. Reducing the high-resolution of CBCT image will weaken the local tooth calcification area information, which brings certain challenges to the intelligent recognition of pulp calcification in CBCT. In addition, the current digital dentistry needs the whole process from 3D volume input, tooth segmentation, identification, and lesion recognition. However, the current relevant research [3][4][5][6][7] separately focuses on functions such as tooth segmentation, root canal segmentation or lesion detection, and has not yet built an integrated intelligent diagnosis process.To this end, we propose a new method of tooth segmentation, identification and pulp calcification recognition based on Transformer to achieve accurate recognition of pulp calcification in high-resolution CBCT images. Specifically, we propose a coarse-to-fine method to segment the tooth instance in the low-resolution CBCT image, and back to the high-resolution CBCT image to intercept the region of the tooth as the input for the fine segmentation, identification and calcification recognition of the tooth instance. In order to enhance the weak distinction between normal teeth and calcified teeth, we put forward tooth instance correlation and triple loss to further improve the recognition performance of calcification. Finally, we introduce transformer to realize the above three tasks in an integrated way, and achieve mutual promotion of task performance. The clinical oral CBCT image data is used to verify the effectiveness of the proposed method."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.1,The Proposed Framework,"The network structure designed in this work is shown in Fig. 2. It mainly includes two modules: tooth segmentation and identification module, and pulp calcification recognition module. First, we stack the swin-transformer as the backbone of the network, and save the computation of processing high-resolution CBCT images through down-sampling. Then, we introduce shallower features through skip connection. Those features with higher resolution and shallower layers will contain relatively rich low-level information, which is more conducive to accurate tooth segmentation and identification recognition. In addition, through the multi-task learning mechanism based on Transformer, the performance of tooth segmentation, identification and calcification recognition can be mutually improved.In the pulp calcification recognition module, we extract the features of each tooth from the deep feature through the results of tooth segmentation, and input them into the pulp calcification recognition module. Specifically, we design an instance correlation transformer (ICT) block. This block allows teeth to learn information from other teeth, so that different teeth can interact, which enables the network itself to explore the relationship between instances, thus improving the recognition performance of calcified teeth. In addition, we introduce a discriminator in the ICT block, which uses triple loss to learn the spatial distribution between categories, so as to learn better classification embedding."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.2,Tooth Segmentation and Identification Module,"In order to obtain the features of each tooth from the high-resolution CBCT image, we first recognize the segmented and identificated teeth,and combine the result of them for tooth instance segmentation. We use swin-transformer as the network backbone. Swin-transformer [8] is based on the idea of ViT model, which innovatively introduces the sliding window mechanism, so that the model can learn cross-window information. Meanwhile, through the down-sampling layer, the model can process super-resolution images, save computation and focus on global and local information. In addition, it has been proved to have advantages in the process of modeling an end-to-end multi-task system [9]. Unlike the typical swin-transformer, which only uses the Patch Merging layer in the deep layer, we first reduce the resolution of the input feature map through the Patch Merging layer in each stage, and then conduct the next sampling operation. This is conducive to our processing of high-resolution CBCT images. In addition, we have adopted the reverse skip connection (RSC). The information from the deep layer can reversely guide the learning of the shallow details to achieve better segmentation effect, which has been proved effective in segmentation task [10]. In this way, we can distinguish the edges of adjacent teeth more clearly, thus promoting better segmentation recognition.The segmentation loss L seg of the model can be defined as L seg = L cs + γ 1 L dice + γ 2 L bs , where γ 1 ,γ 2 are the balance parameters, where L cs is the 33 categories of pixels (32 teeth classes + background) cross entropy loss. L dice is the dice loss of each tooth instance segmentation. L bs is binary segmentation. We calculate the cross entropy loss between teeth and background. The purpose of this step is to assist the model to distinguish foreground and background Calcified root canals, especially small root canal calcification, are shown on CBCT images as the shadow in the center of the cross section faded and disappeared, and the density of the shadow is close to or the same as that of the surrounding dentin, which is significantly different from the root canal images of other root canals of the same affected tooth and normal adjacent teeth. Based on the above clinical observation, we design the tooth instance correlation block for better calcification recognition. The multi-head attention layer is widely used to build image relationship models in channel and spatial dimensions [11,12], so we believe it can be extended to explore the relationship between tooth instances. Specifically, we propose an ICT related to tooth instances to better identify calcified teeth. The ICT module is shown in Fig. 3. First, in the input module of ICT, we extract the tooth deep feature from the output feature of the encoder through the prediction of instance segmentation. The specific method is to extract the tooth deep feature one by one according to the tooth id after the prediction label is de-sampled. We splice the extracted tooth instance features in a new dimension I. Then, we reduce the channel dimension to C/16 by convolution operation for this high-dimensional feature (dimensions : (B,C,I,D,H,W)). The purpose of this step is to reduce the heavy calculation caused by the excessive size of the channel dimension. Then, we divide the reshape into (B, C, I, N), where B is the batch size, I is the number of tooth samples, C and N are the number of channels and pixels, respectively. Through the tooth instance correlation block, we learn the cross attention in the I dimension. Given a CBCT image X i contains multiple tooth instances x i,1 ,x i,2 ,...,x i,n . The tooth instance correlation block is constructed as follows:where MSA is multi-head self attention, L is the layer of MSA, and LN is the standardization layer."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.4,Triple Loss and Total Loss Function,"In order to make the model more discriminative for the recognition of calcified teeth, a discriminator is designed in this work, which uses triplet loss to make the embedding of the input classifier more discriminative. This process is to make the features of the same category as close as possible, and the features of different categories as far away as possible. Meanwhile, in order to prevent the features of the instance from converging into a very small space, it is required that for two positive cases and one negative case, the negative case should be at least margin away from the positive case [13]. Specifically, we randomly selected an instance in the I dimension: Anchor (a), and randomly selected an instance that belongs to the same class as Anchor: Positive (p), and randomly selected an instance that belongs to a different class from Anchor: Negative (n). The goal of model learning is to make the distance D(a, p) between Positive and Anchor as small as possible, and the distance D(a, n) between Negative and Anchor as large as possible L t . It is defined as follows:where D is the European distance, α is a margin between positive and negative pairs. The classification loss is defined as L cls = L pc + γ 3 L t , where γ 3 is the balance parameter, L pc is the cross entropy loss as the loss of calcified tooth classification. Finally, the total loss function of the network is defined as L total = L seg + L cls ."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.5,Implementation,"The initialization setting of the learning rate is 1e-3, with 60000 iterations. The Adam algorithm is used to minimize the objective function. Two RTX3090 GPUs are used, each with 24G memory. The attenuation setting of the learning rate is 0.99 for every 500 iterations. All parameters, including weights and deviations, are initialized using a truncated normal distribution with a standard deviation of 0.1. In the tooth instance segmentation task, we use connected component analysis to extract the maximum area of predicted voxels and remove some small false-positive voxels. Code is available at: https://github.com/Lsx0802/ ToothICT."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",3.1,"Clinical Data, Experimental Setup and Evaluation Metric","This study was performed in line with the principles of the Declaration of Helsinki. In this work, 151 CBCT imaging data from the Imaging department of the local institute were acquired. The image resolution of the CBCT equipment used was 0.2 ∼ 1.0 mm, and the size of the CBCT volume is 672 × 688 × 688. The bulb voltage was 60 ∼ 90 kV, and the bulb current was 1 ∼ 10 mA. 151 cases of dental symptoms were identified as CBCT oral indications by two dentists with 10 years of clinical experience. Among them, 60 patients had dental pulp calcification. In addition, each tooth was also marked for calcification. One dentist is responsible for the data label, and two doctors review it. When they disagree, they will reach an agreement through negotiation.The CBCT data is preprocessed as follows. First, considering the balance between computational efficiency and instance segmentation accuracy, all CBCT images are normalized to 0.4 × 0.4 × 0.4 mm 3 . Then, in order to reduce the impact of extreme values, especially in the metal artifact area, we cut the voxellevel intensity value of each CBCT scan to [0, 2500], and finally normalized the pixel value to the interval [0,1]. For Pulp calcification recognitionin, the adopted evaluation metrics include: Accuracy, Precision, Recall, F1, Dice. The measurement results were conducted with 10 times of four-fold cross validation. In addition, we have compared the performance of the proposed method with the relevant tooth segmentation methods, we used typical segmentation metrics for performance evaluation: Dice, Jaccard similarity coefficient (Jaccard), 95% Hausdorff distance (HD95), Average surface distance (ASD) and have conducted the ablation study of the proposed method."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",3.2,Performance Evaluation,"As shown in Table 1, Dice is based on the instance segmentation performance of each tooth. Backbone1 uses the swin transformer with skip connection to segment the teeth. w/o RSC is a model for eliminating reverse skip connection design. The segmentation results of tooth instance show that the proposed method is superior to other relevant segmentation methods. The main reason is that the proposed method uses the transformer structure. Its multi-head attention mechanism can capture global information, which is superior to the U-net structure based on CNN local features in the relevant methods. In addition, the ablation study for instance segmentation also shows the effectiveness of the proposed module.Our model adopts a network based on swin transformer. Through its powerful global and local modeling ability, while retaining the jumping connection in UNet to retain the shallow features, the performance of Backbone1 is better than the previous segmentation model. In particular, we use reverse skip connection and use deep features to guide shallow feature learning, which has achieved obvious improvement in segmentation performance. After combining the task of calcification classification, the segmentation network has been improved a little, which benefits from the ICT module we adopted, because it not only learns the correlation characteristics between calcified teeth and normal teeth, but also learns the morphological correlation between teeth, which is beneficial to tooth segmentation.Table 2 shows the ablation experimental results of calcified tooth recognition. Backbone 2 is the calcified tooth recognition of the whole module of tooth instance segmentation+classifier. w/o ICT is to remove the tooth instance correlation block, and w/o L t is to remove the discriminator. We can find that the proposed two modules can effectively improve the performance of calcification recognition.The accuracy of the model is only 74.62% when only swin transformer is used to classify tooth samples, while our proposed model can improve the performance of pulp calcification recognition by 3.85%. Especially, in the ablation experiment, when the ICT module is removed, the model performance drops obviously, which proves that our proposed ICT module can effectively learn the relationship between dental examples. In addition, after the loss Lt of the discriminant module is removed, the accuracy of the model decreases by about 1.16%, which proves that this method can effectively reduce the distance between similar samples and increase the distance between different samples. (See the supplementary materials for more visualization results) "
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",4.0,Conclusion,"In this study, we proposed a calcified tooth recognition method based on transformer, which can detect calcified teeth in high-resolution CBCT images while achieving tooth instance segmentation and identification. Specifically, we proposed a coarse-to-fine processing method to make it possible to process highresolution CBCT with deep network for calcification recognition. In addition, the design of instance correlation and triple loss further improved the accuracy of calcification detection. The validation of clinical data showed the effectiveness and advantages of the proposed method. We believe that this research will bring help to the intellectualization of oral imaging diagnosis and the navigation of oral surgery."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,1.0,Introduction,"Deep learning has enabled significant progress in image-based computer-aided diagnosis [8,10,23,26,33]. However, the increasing memory requirements of deep neural networks limit their practical deployment in hardware-constrained environments. One promising approach to reducing memory usage and inference latency is model pruning , which aims to remove redundant or unimportant model weights [21]. Since modern deep neural networks are often overparameterized, they can be heavily pruned with minimal impact on overall performance [6,20,22,34]. This being said, the impact of pruning on model behavior beyond high-level performance metrics like top-1 accuracy remain unclear. This gap in understanding has major implications for real-world deployment of neural networks for high-risk tasks like disease diagnosis, where pruning may cause unexpected consequences that could potentially threaten patient well-being.To bridge this gap, this study aims to answer the following guiding questions by conducting experiments to dissect the differential impact of pruning: Q1. What is the impact of pruning on overall performance in longtailed multi-label medical image classification? Q2. Which disease classes are most affected by pruning and why? Q3. How does disease co-occurrence influence the impact of pruning? Q4. Which individual images are most vulnerable to pruning?We focus our experiments on thorax disease classification on chest X-rays (CXRs), a challenging long-tailed and multi-label computer-aided diagnosis problem, where patients may present with multiple abnormal findings in one exam and most findings are rare relative to the few most common diseases [12].This study draws inspiration from Hooker et al. [13], who found that pruning disparately impacts a small subset of classes in order to maintain overall performance. The authors also introduced pruning-identified exemplars (PIEs), images where an uncompressed and heavily pruned model disagree. They discovered that PIEs share common characteristics such as multiple salient objects and noisy, fine-grained labels. While these findings uncover what neural networks ""forget"" upon pruning, the insights are limited to highly curated natural image datasets where each image belongs to one class. Previous studies have shown that pruning can enhance fairness [31], robustness [1], and efficiency for medical image classification [4,7,32] and segmentation [3,16,24,25,29] tasks. However, these efforts also either focused solely on high-level performance or did not consider settings with severe class imbalance or co-occurrence.Unlike existing work, we explicitly connect class ""forgettability"" to the unique aspects of our problem setting: disease frequency (long-tailedness) and disease cooccurrence (multi-label behavior). Since many diagnostic exams, like CXR, are long-tailed and multi-label, this work fills a critical knowledge gap enabling more informed deployment of pruned disease classifiers. We hope that our findings can provide a foundation for future research on pruning in clinically realistic settings."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.1,Preliminaries,"Datasets. For this study, we use expanded versions of NIH ChestXRay14 [30] and MIMIC-CXR [18], two large-scale CXR datasets for multi-label disease classification. 1 As described in Holste et al. [12], we augmented the set of possible labels for each image by adding five new rare disease findings parsed from radiology reports. This creates a challenging long-tailed classification problem, with training class prevalence ranging from under 100 to over 70,000 (Supplement). NIH-CXR-LT contains 112,120 CXRs, each labeled with at least one of 20 classes, while MIMIC-CXR-LT contains 257,018 frontal CXRs labeled with at least one of 19 classes. Each dataset was split into training (70%), validation (10%), and test (20%) sets at the patient level.Model Pruning & Evaluation. Following Hooker et al. [13], we focus on global unstructured L1 pruning [34]. After training a disease classifier, a fraction k of weights with the smallest magnitude are ""pruned"" (set to zero); for instance, k = 0.9 means 90% of weights have been pruned. While area under the receiver operating characteristic curve is a standard metric on related datasets [26,28,30], it can become heavily inflated in the presence of class imbalance [2,5]. Since we seek a metric that is both resistant to imbalance and captures performance across thresholds (as choosing a threshold is non-trivial in the multi-label setting [27]), we use average precision (AP) as our primary metric."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.2,Assessing the Impact of Pruning,"Experimental Setup. We first train a baseline model to classify thorax diseases on both NIH-CXR-LT and MIMIC-CXR-LT. The architecture used was a ResNet50 [9] with ImageNet-pretrained weights and a sigmoid cross-entropy loss. For full training details, please see the Supplemental Materials and code repository. Following Hooker et al. [13], we then repeat this process with 30 unique random initializations, performing L1 pruning at a range of sparsity ratios k ∈ {0, 0.05, . . . , 0.9, 0.95} on each model and dataset. Using a ""population"" of 30 models allows for reliable estimation of model performance at each sparsity ratio. We then analyze how pruning impacts overall, disease-level, and image-level model behavior with increasing sparsity as described below.Overall and Class-Level Analysis. To evaluate the overall impact of pruning, we compute the mean AP across classes for each sparsity ratio and dataset. We use Welch's t-test to assess performance differences between the 30 uncompressed models and 30 k-sparse models. We then characterize the class-level impact of pruning by considering the relative change in AP from an uncompressed model to its k-sparse counterpart for all k. Using relative change in AP allows for comparison of the impact of pruning regardless of class difficulty. We then define the forgettability curve of a class c as follows: med AP i,k,c -AP i,0,c AP i,0,c i∈{1,...,30} k∈{0,0.05,...,0.9,0.95} (1) where AP i,k,c := AP of the i th model with sparsity k on class c, and med(•) := median across all 30 runs. We analyze how these curves relate to class frequency and co-occurrence using Pearson (r) and Spearman (ρ) correlation tests. Incorporating Disease Co-occurrence Behavior. For each unique pair of NIH-CXR-LT classes, we compute the Forgettability Curve Dissimilarity (FCD), the mean squared error (MSE) between the forgettability curves of each disease. FCD quantifies how similar two classes are with respect to their forgetting behavior over all sparsity ratios. Ordinary least squares (OLS) linear regression is employed to understand the interaction between difference in class frequency and class co-occurrence with respect to FCD for a given disease pair."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.3,Pruning-Identified Exemplars (PIEs),"Definition. After evaluating the overall and class-level impact of pruning on CXR classification, we investigate which individual images are most vulnerable to pruning. Like Hooker et al. [13], we consider PIEs to be images where an uncompressed and pruned model disagree. Letting C be the number of classes, we compute the average prediction 1 30 i ŷ0 ∈ R C of the uncompressed models and average prediction 1 30 i ŷ0.9 ∈ R C of the L1-pruned models at 90% sparsity for all NIH-CXR-LT test set images. Then the Spearman rank correlation σ( 1 30 i ŷ0 , 1 30 i ŷ0.9 ) represents the agreement between the uncompressed and heavily pruned models for each image; we define PIEs as images whose correlation falls in the bottom 5 th percentile of test images."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,,Analysis and Human Study.,"To understand the common characteristics of PIEs, we compare how frequently (i) each class appears and (ii) images with d = 0, . . . , 3, 4+ simultaneous diseases appear in PIEs relative to non-PIEs. To further analyze qualities of CXRs that require domain expertise, we conducted a human study to assess radiologist perceptions of PIEs. Six board-certified attending radiologists were each presented with a unique set of 40 CXRs (half PIE, half non-PIE). Each image was presented along with its ground-truth labels and the following three questions:  "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.1,What is the Overall Effect of Pruning?,"We find that under L1 pruning, the first sparsity ratio causing a significant drop in mean AP is 65% for NIH-CXR-LT (P < 0.001) and 60% for MIMIC-CXR-LT (P < 0.001) (Fig. 1,left). This observation may be explained by the fact that ResNet50 is highly overparameterized for this task. Since only a subset of weights are required to adequately model the data, the trained classifiers have naturally sparse activations (Fig. 1, right). For example, over half of all learned weights have magnitude under 0.01. However, beyond a sparsity ratio of 60%, we observe a steep decline in performance with increasing sparsity for both datasets.  "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.2,Which Diseases are Most Vulnerable to Pruning and Why?,"Class forgettability curves in Fig. 2 depict the relative change in AP by sparsity ratio for a representative subset of classes. Although these curves follow a similar general trend to Fig. 1, some curves (i) drop earlier and (ii) drop more considerably at high sparsity. Notably, we find a strong positive relationship between training class frequency and (i) the first sparsity ratio at which a class experienced a median 20% relative drop in AP (ρ = 0.61, P = 0.005 for NIH-CXR-LT; ρ = 0.93, P 0.001 for MIMIC-CXR-LT) and (ii) the median relative change in AP at 95% sparsity (ρ = 0.75, P < 0.001 for NIH-CXR-LT; ρ = 0.75, P < 0.001 for MIMI-CXR-LT). These findings indicate that, in general, rare diseases are forgotten earlier (Fig. 3, left) and are more severely impacted at high sparsity (Fig. 3, right)."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.3,How Does Disease Co-occurrence Influence Class Forgettability?,"Our analysis reveals that for NIH-CXR-LT, the absolute difference in log test frequency between two diseases is a strong predictor of the pair's FCD (ρ = 0.64, P 0.001). This finding suggests that diseases with larger differences in prevalence exhibit more distinct forgettability behavior upon L1 pruning (Fig. 4, left). To account for the multi-label nature of thorax disease classification, we also explore the relationship between intersection over union (IoU) -a measure of co-occurrence between two diseases -and FCD. Our analysis indicates that the IoU between two diseases is negatively associated with FCD (ρ = -0.47, P 0.001). This suggests that the more two diseases co-occur, the more similar their forgetting trajectories are across all sparsity ratios (Fig. 4, right). For example, the disease pair (Infiltration, Hernia) has a dramatic difference in prevalence (|LogFreqDiff| = 4.58) and rare co-occurrence (IoU 1/4 = 0.15), resulting in an extremely high FCD for the pair of diseases.We also find, however, that there is a push and pull between differences in individual class frequency and class co-occurrence with respect to FCD. To illustrate, consider the disease pair (Emphysema, Pneumomediastinum) marked in black in Fig. 4. These classes have an absolute difference in log frequency of 2.04, which would suggest an FCD of around 0.58. However, because Emphysema and Pneumomediastinum co-occur relatively often (IoU 1/4 = 0.37), their forgettability curves are more similar than prevalence alone would dictate, resulting in a lower FCD of 0.18. To quantify this effect, we obtain an OLS model that fitted FCD as a function of |LogFreqDiff|, IoU 1/4 , and their interaction: FCD = 0.27+0.21|LogFreqDiff|-0.05(IoU) 1/4 -0.31|LogFreqDiff| * (IoU) 1/4 (2)We observe a statistically significant interaction effect between the difference in individual class frequency and class co-occurrence on FCD (β 3 = -0.31, P = 0.005). Thus, for disease pairs with a very large difference in prevalence, the effect of co-occurrence on FCD is even more pronounced (Supplement)."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.4,What Do Pruning-Identified CXRs have in Common?,"For NIH-CXR-LT, we find that PIEs are more likely to contain rare diseases and more likely to contain 3+ simultaneous diseases when compared to non-PIEs (Fig. 5). The five rarest classes appear 3-15x more often in PIEs than non-PIEs, and images with 4+ diseases appear 3.2x more often in PIEs.In a human reader study involving 240 CXRs from the NIH-CXR-LT test set (120 PIEs and 120 non-PIEs), radiologists perceived that PIEs had more label noise, lower image quality, and higher diagnosis difficulty (Fig. 6). However, due to small sample size and large variability, these differences are not statistically significant. Respondents fully agreed with the label 55% of the time  for PIEs and 57.5% of the time for non-PIEs (P = 0.35), gave an average image quality of 3.6 for PIEs and 3.8 for non-PIEs (P = 0.09), and gave an average diagnosis difficulty of 2.5 for PIEs and 2.05 for non-PIEs (P = 0.25).Overall, these findings suggest that pruning identifies CXRs with many potential sources of difficulty, such as containing underrepresented diseases, (partially) incorrect labels, low image quality, and complex disease presentation."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,4.0,Discussion and Conclusion,"In conclusion, we conducted the first study of the effect of pruning on multi-label, long-tailed medical image classification, focusing on thorax disease diagnosis in CXRs. Our findings are summarized as follows:1. As observed in standard image classification, CXR classifiers can be heavily pruned (up to 60% sparsity) before dropping in overall performance. 2. Class frequency is a strong predictor of both when and how severely a class is impacted by pruning. Rare classes suffer the most. 3. Large differences in class frequency lead to dissimilar ""forgettability"" behavior and stronger co-occurrence leads to more similar forgettability behavior.-Further, we discover a significant interaction effect between these two factors with respect to how similarly pruning impacts two classes.4. We adapt PIEs to the multi-label setting, observing that PIEs are far more likely to contain rare diseases and multiple concurrent diseases.-A radiologist study further suggests that PIEs have more label noise, lower image quality, and higher diagnosis difficulty.It should be noted that this study is limited to the analysis of global unstructured L1 (magnitude-based) pruning, a simple heuristic for post-training network pruning. Meanwhile, other state-of-the-art pruning approaches [6,20,22] and model compression techniques beyond pruning (e.g., weight quantization [14] and knowledge distillation [11]) could be employed to strengthen this work. Additionally, since our experiments only consider the ResNet50 architecture, it remains unclear whether other training approaches, architectures, or compression methods could mitigate the adverse effects of pruning on rare classes. In line with recent work [15,17,19], future research may leverage the insights gained from this study to develop an algorithm for improved long-tailed learning on medical image analysis tasks. For example, PIEs could be interpreted as salient, difficult examples that warrant greater weight during training. Conversely, PIEs may just as well be regarded as noisy examples to be ignored, using pruning as a tool for data cleaning."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 64.
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest forms of human cancer, with a 5-year survival rate of only 9% [16]. Neoadjuvant chemotherapy can increase the likelihood of achieving a margin-negative resection and avoid unnecessary surgery in patients with aggressive tumor types [23]. Providing accurate and objective preoperative biomarkers is crucial for triaging patients who are most likely to benefit from neoadjuvant chemotherapy. However, current clinical markers such as larger tumor size and high carbohydrate antigen (CA) 19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for patients [19]. Therefore, multi-phase contrast-enhanced CT has a great potential to enable personalized prognostic prediction for PDAC, leveraging its ability to provide a wealth of texture information that can aid in the development of accurate and effective prognostic models [2,10].Previous studies have utilized image texture analysis with hand-crafted features to predict the survival of patients with PDACs [1], but the representational Fig. 1. Two examples of spatial information between vessel (orange region) and tumor (green region). The minimum distance, which refers to the closest distance between the Superior Mesenteric Artery (SMA) and the PDAC tumor region, is almost identical in these two cases. We define the surface-to-surface distance based on point-to-surface distance (weighted-average of red lines from ♦ to ) instead of point-to-point distance (blue lines) to better capture the relationship between the tumor and the perivascular tissue.Here ♦ and are points sampled from subset Vc and Pc defined in Eq. power of these features may be limited. In recent years, deep learning-based methods have shown promising results in prognosis models [3,6,12]. However, PDACs differ significantly from the tumors in these studies. A clinical investigation based on contrast-enhanced CT has revealed a dynamic correlation between the internal stromal fractions of PDACs and their surrounding vasculature [14]. Therefore, focusing solely on the texture information of the tumor itself may not be effective for the prognostic prediction of PDAC. It is necessary to incorporate tumor-vascular involvement into the feature extraction process of the prognostic model. Although some studies have investigated tumor-vascular relationships [21,22], these methods may not be sufficiently capable of capturing the complex dynamics between the tumor and its environment.We propose a novel approach for measuring the relative position relationship between the tumor and the vessel by explicitly using the distance between them. Typically, Chamfer distance [7], Hausdorff distance [8], or other surfaceawareness metrics are used. However, as shown in Fig. 1, these point-to-point distances cannot differentiate the degree of tumor-vascular invasion [18]. To address this limitation, we propose a learnable neural distance that considers all relevant points on different surfaces and uses an attention mechanism to compute a combined distance that is more suitable for determining the degree of invasion. Furthermore, to capture the tumor enhancement patterns across multi-phase CT images, we are the first to combine convolutional neural networks (CNN) and transformer [4] modules for extracting the dynamic texture patterns of PDAC and its surroundings. This approach takes advantage of the visual transformer's adeptness in capturing long-distance information compared to the CNN-onlybased framework in the original approach. By incorporating texture information between PDAC, pancreas, and peripancreatic vessels, as well as the local tumor information captured by CNN, we aim to improve the accuracy of our prognostic prediction model.In this study, we make the following contributions: (1) We propose a novel approach for aiding survival prediction in PDAC by introducing a learnable neural distance that explicitly evaluates the degree of vascular invasion between the tumor and its surrounding vessels. (2) We introduce a texture-aware transformer block to enhance the feature extraction approach, combining local and global information for comprehensive texture information. We validate that the cross-attention is utilized to capture cross-modality information and integrate it with in-modality information, resulting in a more accurate and robust prognostic prediction model for PDAC. (3) Through extensive evaluation and statistical analysis, we demonstrate the effectiveness of our proposed method. The signature built from our model remains statistically significant in multivariable analysis after adjusting for established clinical predictors. Our proposed model has the potential to be used in combination with clinical factors for risk stratification and treatment decisions for patients with PDAC."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.0,Methods,"As shown in Fig. 2, the proposed method consists of two main components. The first component combines the CNN and transformer to enhance the extraction of tumor dynamic texture features. The second component proposes a neural distance metric between PDAC and important vessels to assess their involvements."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.1,Texture-Aware Vision Transformer: Combination of CNN and Transformer,"Recently, self-attention models, specifically vision transformers (ViTs [4]), have emerged as an alternative to CNNs in survival prediction [15,25]. Our proposed texture-aware transformer, inspired by MobileViT [13], aims to combine both local information (such as PDAC texture) and global information (such as the relationship between PDAC and the pancreas). This approach is different from previous methods that rely solely on either CNN-based or transformer-based backbones, focusing only on local or global information, respectively. The texture-aware transformer (Fig. 2) comprises three blocks, each consisting of a texture-aware CNN block and a texture-aware self-attention block. These blocks encode the input feature of an image F i ∈ R H×W ×D×C to the hidden feature F c ∈ R H×W ×D×C l using a 3 × 3 × 3 convolutional layer, followed by a We first select related points set from the closest sub-surface on PDAC and vessels respectively. Then we use a cross-attention block to obtain the neural distance. Finally, we concatenate features from three branches to obtain the survival outcome OOS.1 × 1 × 1 convolutional layer. The 3 × 3 × 3 convolution captures local spatial information, while the 1 × 1 × 1 convolution maps the input tensor to a higherdimensional space (i.e., C l > C). The texture-aware CNN block downsamples the input, and the texture-aware self-attention block captures long-range nonlocal dependencies through a patch-wise self-attention mechanism.In the texture-aware self-attention block, the input feature F c is divided into N non-overlapping 3D patches F u ∈ R V ×N ×Cu , where V = hwd and N = HW D/V is the number of patches, and h, w, d are the height, width, and depth of a patch, respectively. For each voxel position within a patch, we apply a multi-head self-attention block and a feed-forward block following [20] to obtain the output feature F o . In this study, preoperative multi-phase CE-CT pancreatic imaging includes the non-contrast phase, the pancreatic phase and venous phase. Therefore, we obtain three outputs from the transformer block with the input of these phases, denoted asInstead of directly fusing the outputs as in previous work, we employ a 3-way cross-attention block to extract cross-modality information from these phases. The cross-attention is performed on the concatenated self-attention matrix with an extra mask M ∈ {0, -∞} 3C×3C , defined as:Here, Q, K, V are the query, key, and value matrices, respectively, obtained by linearly projecting the input F T o ∈ R 3C×D . The cross-modality output F cross and in-modality output F T o are then concatenated and passed through an average pooling layer to obtain the final output feature of the texture branch, denoted as F t ∈ R Ct ."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.2,Neural Distance: Positional and Structural Information,"Between PDAC and VesselsThe vascular involvement in patients with PDAC affects the resectability and treatment planning [5]. In this study, we investigate four important vessels: portal vein and splenic vein (PVSV), superior mesenteric artery (SMA), superior mesenteric vein (SMV), and truncus coeliacus (TC). We used a semi-supervised nnUnet model to segment PDAC and the surrounding vessels, following recent work [11,21]. We define a general distance between the surface boundaries of PDAC (P) and the aforementioned four types of vessels (V) as D(V, P), which can be derived as follows:where v ∈ V and p ∈ P are points on the surfaces of blood vessels and PDAC, respectively. The point-to-surface distance d ps (v, P) is the distance from a point v on V to P, defined as d ps (v, P) = min p∈P v -p 2 2 , and vice versa. To numerically calculate the integrals in the previous equation, we uniformly sample from the surfaces V and P to obtain the sets V and P consisting of N v points and N p points, respectively. The distance is then calculated between the two sets using the following equation:However, the above distance treats all points equally and may not be flexible enough to adapt to individualized prognostic predictions. Therefore, we improve the above equation in two ways. Firstly, we focus on the sub-sets Vc and Pc of V and P, respectively, which only contain the K closest points to the opposite surfaces P and V, respectively. The sub-sets are defined as:Secondly, we regard the entire sets Vc and Pc as sequences and calculate the distance using a 2-way cross-attention block (similar to Eq. 1) to build a neural distance based on the 3D spatial coordinates of each point:Neural distance allows for the flexible assignment of weights to different points and is able to find positional information that is more suitable for PDAC prognosis prediction. In addition to neural distance, we use the 3D-CNN model introduced in [22] to extract the structural relationship between PDAC and the vessels. Specifically, we concatenate each PDAC-vessel pair X v s ∈ R 2×H×W ×D , where v ∈{PVSV, SMV, SMA, TC} and obtain the structure feature F s ∈ R Cs .Finally, we concatenate the features extracted from the two components and apply a fully-connected layer to predict the survival outcome, denoted as O OS , which is a value between 0 and 1. To optimize the proposed model, we use the negative log partial likelihood as the survival loss [9]."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"Dataset. In this study, we used data from Shengjing Hospital to train our method with 892 patients, and data from three other centers, including Guangdong Provincial People's Hospital, Tianjin Medical University and Sun Yatsen University Cancer Center for independent testing with 178 patients. The contrast-enhanced CT protocol included non-contrast, pancreatic, and portal venous phases. PDAC masks for 340 patients were manually labeled by a radiologist from Shengjing Hospital with 18 years of experience in pancreatic cancer, while the rest were predicted using self-learning models [11,24] and checked by the same annotator. Other vessel masks were generated using the same semisupervised segmentation models. C-index was used as our primary evaluation metric for survival prediction. We also reported the survival AUC, which estimates the cumulative area under the ROC curve for the first 36 months.Implementation Details: We used nested 5-fold cross-validation and augmented the training data by rotating volumetric tumors in the axial direction and randomly selecting cropped regions with random shifts. We also set the output feature dimensions to C t = 64 for the texture-aware transformer, C s = 64 for the structure extraction and K = 32 for the neural distance. The batch size was 16 and the maximum iteration was set to 1000 epochs, and we selected the model with the best performance on the validation set during training for testing. We implemented our experiments using PyTorch 1.11 and trained the models on a single NVIDIA 32G-V100 GPU.Ablation Study. We first evaluated the performance of our proposed textureaware transformer (TAT) by comparing it with the ResNet18 CNN backbone and ViT transformer backbone, as shown in Table 1. Our model leverages the strengths of both local and global information in the pancreas and achieved the best result. Next, we compared different methods for multi-phase stages, including LSTM, early fusion (Fusion), and cross-attention (Cross) in our method. Cross-attention is more effective and lightweight than LSTM. Moreover, we separated texture features into in-phase features and cross-phase features, which is more reasonable than early fusion.Secondly, we evaluated each component in our proposed method, as shown in Fig. 2, and presented the results in Table 1. Combining the texture-aware transformer and regular structure information improved the results from 0.630 to 0.648, as tumor invasion strongly affects the survival of PDAC patients. We also employed a simple 4-variable regression model that used only the Chamfer distance of the tumor and the four vessels for prognostic prediction. The resulting C-index of 0.611 confirmed the correlation of the distance with the survival, which is consistent with clinical findings [18]. Explicitly adding the distance measure further improved the results. Our proposed neural distance metric outperformed traditional surface distance metrics like Chamfer distance, indicating its suitability for distinguishing the severity of PDAC."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,,Comparisons.,"To further evaluate the performance of our proposed model, we compared it with recent deep prediction methods [17,21] and report the results in Table 2. We modified baseline deep learning models [12,17] and used their network architectures to take a single pancreatic phase or all three phases as inputs. DeepCT-PDAC [21] is the most recent method that considers both tumor-related and tumor-vascular relationships using 3D CNNs. Our proposed method, which uses the transformer and structure-aware blocks to capture tumor enhancement patterns and tumor-vascular involvement, demonstrated its effectiveness with better performance in both nested 5-fold cross-validation and the multi-center independent test set.In Table 3, we used univariate and multivariate Cox proportional-hazards models to evaluate our signature and other clinicopathologic factors in the independent test set. The proposed risk stratification was a significant prognostic factor, along with other factors like pathological TNM stages. After selecting significant variables (p < 0.05) in univariate analysis, our proposed staging remained strong in multivariable analysis after adjusting for important prognostic markers like pT and resection margins. Notably, our proposed marker remained the strongest among all pre-operative markers, such as tumor size and CA 19-9.Neoadjuvant Therapy Selection. To demonstrate the added value of our signature as a tool to select patients for neoadjuvant treatment before surgery, we plotted Kaplan-Meier survival curves in Fig. 3. We further stratify patients by our signature after grouping them by tumor size and CA19-9, two clinically used preoperative criteria for selection, and also age. Our signature could significantly stratify patients in all cases and those in the high-risk group had worse outcomes and might be considered as potential neoadjuvant treatment candidates (e.g. 33 high-risk patients with larger tumor size and high CA19-9).  "
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"In our paper, we propose a multi-branch transformer-based framework for predicting cancer survival. Our framework includes a texture-aware transformer that captures both local and global information about the PDAC and pancreas. We also introduce a neural distance to calculate a more reasonable distance between PDAC and vessels, which is highly correlated with PDAC survival. We have extensively evaluated and statistically analyzed our proposed method, demonstrating its effectiveness. Furthermore, our model can be combined with established high-risk features to aid in the patient selections who might benefit from neoadjuvant therapy before surgery."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 24.
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,1.0,Introduction,"Existing tumor augmentation methods, including ""Copy-Paste"" strategy based methods [15][16][17]19] and style-transfer based methods [5], only considered content or style information when synthesizing new samples, which leads to a distortion gap in content or domain space between the true image and synthetic image, and further causes a distortion problem [14] as shown in Fig. 1 (1). The distortion problem damages the effectiveness of DCNNs in feature representation learning as proven in many studies [1,5,18]. Therefore, a domain and content simultaneously aware data augmentation method is urgently needed to eliminate and avoid the distortion challenges during tumor generation. It remains, however, a very challenging task because the content and domain space lack of clear border, and the domain information always influences the distribution of content. This is also the main reason that style transfer [7,8,10] still suffers from spurious artifacts such as disharmonious colors and repetitive patterns, and a large gap is still left between real artwork and synthetic style [2,3]. Therefore, it's necessary to reduce the influence of the domain on content and keep the content consistent during image generation.To overcome the above challenges, a Domain-aware and Content-consistent tumor Augmentation method, named DCAug, is developed (Fig. 1 Experimental results on two public tumor segmentation datasets show that DCAug improves the tumor segmentation accuracy compared with state-of-theart tumor augmentation methods. In summary, our contributions are as follows:-A content-aware and domain-aware tumor augmentation method is proposed, which eliminates the distortion in content and domain space between the true tumor image and synthetic tumor image. -Our novel DaCL and CdCL disentangle the image information into two completely independent parts: 1) domain-invariant content information; 2) individual-specific domain information. It has the advantage of alleviating the challenge of distortion in synthetic tumor images. -Experimental results on two public tumor segmentation datasets demonstrate that DCAug improves the diversity and quality of synthetic tumor images."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.1,Problem Definition,"Formulation: Given two images and the corresponding tumor labels {X A , Y A }, {X B , Y B }, tumor composition process can be formulated as:where, respectively. There are two challenges need to be solved: 1) X b→A A , X a→B B , by adjusting the domain information of the copied tumor, making the copied tumor have the same domain space as the target image to avoid domain distortion; 2)B , maintaining the domain-invariant content information consistency during tumor copy to avoid content distortion.To achieve the above goals, a novel Cross-cycle Framework (Fig. 2) is designed, which consists of two generators and can disentangle the tumor information into two solely independent parts: 1) Domain-invariant content information, 2) Individual-specific domain information, through two new learning strategies: 1) Domain-aware contrastive learning (DaCL); 2) Cross-domain consistency learning (CdCL). When generating new sample, the domain-invariant content information is preserved by CdCL, while the individual-specific domain information is adjusted by DaCL based on the domain space of target tumor image. The details are described as follows."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.2,Domain-Aware Contrastive Learning for Domain Adaptation,"Our domain-aware contrastive learning (DaCL) strategy can adaptively adjust the domain space of the transferred tumor and makes the domain space consistent for domain adaptation. Specifically, the input of DCAug is two combined images X b A , X a B that consist of source images and tumor regions copied from another image. The synthetic tumors X b→A A generated by the generator, the  A as the anchor, the positive and the negative sample, respectively. To find the domain space of these samples for contrast, a fixed pre-trained style representation extractor f is used to obtain domain representations for different images. Thus, DaCL between the anchor, the positive, and the negative sample can be formulated as:where D(x, y) is the L 2 distance between x and y, w i is weighting factor. Additionally, to further disentangle the individual-specific domain information, a reversed process is designed. By utilizing the synthetic tumors X a→B B , X b→A A , the reversed images X a→B A ,X b→A B can be construed as:The whole reversed process receives the reversed images X a→B A , X b→A B as inputs and tries to restore the original domain information of the synthetic tumor XA , XB .where φ denotes the ith layer of the VGG-19 network, μ and σ represent the mean and standard deviation of feature maps extracted by φ, respectively. In summary, the total loss for the cross-cycle framework is) where α, β, and γ represent the weight coefficients."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.1,Datasets and Implementation Details,"ATLAS Dataset [11]: The ATLAS dataset consists of 229 T1-weighted MR images from 220 subjects with chronic stroke lesions. These images were acquired from different cohorts and different scanners. The chronic stroke lesions are annotated by a group of 11 experts. The dimension of the pre-processed images is 197 × 233 × 189 with an isotropic 1mm 3 resolution. Identical with the study in [17], We selected 50 images as the test set and the rest of the cases as the training set. KiTS19 Dataset [4]: The KiTS19 consists of 210 3D abdominal CT images with kidney tumor subtypes and segmentation of kidney and kidney tumors. These CT images are from more than 50 institutions and scanned with different CT scanners and acquisition protocols. In our experiment, we randomly split the published 210 images into a training set with 168 images and a testing set with 42 images. Training Details: The generator in DCAug is built on the RAIN [12] backbone, all of the weights in generators are shared. Our DCAug is implemented using PyTorch [13] and trained end-to-end with Adam [9] optimization method. In the training phase, the learning rate is initially set to 0.0001 and decreased by a weight decay of 1.0 × 10 -6 after each epoch. The experiments were carried out on one NVIDIA RTX A4000 GPU with 16 GB memory. The weight valule of α, β, and γ is 1.0,1.0,1.0, separately. Baseline: nnUNet [6] is selected as the baseline model. The default hyperparameters and default traditional data augmentation (TDA) including rotation, scaling, mirroring, elastic deformation, intensity perturbation are used when model training. The maximum number of training epochs was set to 500 for the two datasets. Parts of tumors generated are shown in Fig. 3. And the dice coefficients of the segmentation results on the same test set are computed to evaluate the effectiveness of methods.  "
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.2,Comparison with State-of-the-Art Methods,"Experimental results in Table 1 and Fig. 4 show that compared with other stateof-the-art methods, including Mixup [16], CutMix [15], CarveMix [17], SelfMix [19], StyleMix [5], nnUnet combined with DCAug achieves the highest improvement on the two datasets, which convincingly demonstrates the innovations and contribution of DCAug in generating higher quality tumor. And it is worth noting that CutMix (""Copy-Paste""method that only considers content information) even degrades the segmentation performance, which indicates that both content and domain information has a significant influence on the tumor segmentation.The representative segmentation scans are shown in Fig. 4. Our DCAug produced better segmentation results than the competing methods, which further proves the effectiveness of DCAug in tumor generation. What's more, the potential of DCAug in an extremely low-data regime is also demonstrated. We randomly select 25% and 50% of data from the training set same as training data. DCAug also assists the baseline model to achieve higher  Dice coefficients, which convincingly demonstrates the effectiveness of DCAug in generating new tumor samples."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.3,Significant in Improving Existing Tumor Augmentation Methods,"The necessity of considering both content and domain information in the tumor generation is also demonstrated, three representative methods, Mixup (""Copy-Paste""), CutMix (""Copy-Paste""), and StyleMix (style-transfer), are selected. The DCAug optimizes generated samples from above methods from content and domain aspects to further improve the quality of generated samples. And the nnUet are trained by optimized samples. From the segmentation performances (Table 2), we can notice that DCAug can further boost the quality of generated samples produced by existing methods. Specifically, the DCAug assists the Mixup, CutMix, and StyleMix to obtain a 3.15%, 8.53%, and 0.60% improvement in segmentation performance, respectively, which demonstrates that 1) it is necessary to consider both content and domain information during samples generation; 2) avoiding the content and domain distortion challenge can further improve the quality of generated samples; 3) DCAug can alleviate the challenge of distortion problem present in existing tumor augmentation methods."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,4.0,Conclusion,"In this paper, our domain-aware and content-consistent tumor augmentation method eliminated the content distortion and domain gap between the true tumor and synthetic tumor by simultaneously focusing the content information and domain information. Specifically, DCAug can maintain the domain-invariant content information consistency and adaptive adjust individual-specific domain information by a new cross-cycle framework and two novel contrastive learning strategies when generating synthetic tumor. Experimental results on two tumor segmentation tasks show that our DCAug can significantly improve the quality of the synthetic tumors, eliminate the gaps, and has practical value in medical imaging applications."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,,Table 1 .,Dataset Num Means and Standard deviations of the Dice coefficients (%) TDA Mixup CutMix CarveMix SelfMix StyleMix DCAug ATLAS 25% 49.87 ± 32.19 49.18 ± 32.72 41.19 ± 33.98 55.16 ± 32.16 57.89 ± 31.05 52.84 ± 34.36 56.43 ± 32.33 50% 56.72 ± 30.74 58.40 ± 29.35 54.25 ± 30.24 58.34 ± 31.32 58.81 ± 31.75 58.04 ± 30.39 59.75 ± 31.41 100% 59.39 ± 32.45 59.33 ± 33.06 56.11 ± 32.44 62.32 ± 31.10 63.5 ± 31.06 64.00 ± 28.89 64.64 ± 29.91
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.3,Cross-Domain Consistency Learning for Content Preservation,"Cross-domain consistency learning (CdCL) strategy can preserve the domaininvariant content information of tumor in the synthesized images X b→A A , X a→B B for avoiding content distortion. Specifically, given the original imagesproduced by generator, and the reconstructed images XA , XB generated by the reversed process. The tumor can be first extracted from those imagesAlthough the domain space is various, the tumor content insideTo evaluate the tumor content inside cross-domain images, the content consistency losses, including L A pixel (X A , XA ), L B pixel (X B , XB ), L a→B content , L b→A content , are computed between those images for supervising the content change. The details of content consistency loss are described in the next section."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.4,Loss Function,"In summary, three types of losses are used to supervise the cross-cycle framework. Specifically, given the original images X A , X B and the combined images X b A , X B a , the synthesized images X a→B B , X b→A A are produced by the generator, and the reconstructed images XA , XB are generated by the reversed process.The pixel-wise loss (L pixel ) computes the difference between original images and reconstructed images at the pixel level.To disentangle the individual-specific domain information, the higher feature representations extracted from pre-trained networks combined with CL are used:And two content loss L b→A content , L a→B content are employed to maintain tumor content information during the domain adaptation:"
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,1.0,Introduction,"Chest X-rays (CXRs) are frequently used for disease detection and disease progression monitoring. However, interpreting CXRs can be challenging and timeconsuming, particularly in regions with a shortage of radiologists. This can lead to delayed or inaccurate diagnoses and management, potentially harming patients. Automating the CXR interpretation process can lead to faster and more accurate diagnoses. Advances in Artificial Intelligence (AI), particularly in the field of computer vision for medical imaging, have significantly alleviated the challenges faced in radiology. The availability of large labeled collections of CXRs has been instrumental in driving progress in this area [8,10]. Both CXR disease detection and automatic report generation have witnessed substantial improvements [15,16,23]. Remarkably, AI-based methods for finding detection are now approaching the performance level of experienced radiologists [21,27]. Moreover, just as vision transformers have revolutionized various areas of computer vision [12], they have also become an integral part of automatic CXR analysis [18].Although significant strides have been made in AI-assisted medical image segmentation and disease detection, tasks requiring intricate reasoning have received less attention. One such complex task is monitoring disease progression in a sequence of images, which is particularly critical in assessing patients with pneumonia and other CXR findings. For example, temporal lung changes serve as vital indicators of patient outcomes and are routinely mentioned in radiology reports for determining the course of treatment [20]. Prior work has investigated tracking the progression of COVID-19 pulmonary diseases and predicting outcomes [13]. Recently, CheXRelNet was proposed, which utilizes graph attention networks to capture anatomical correlations and detect changes in CXRs using both local and global anatomical information [11]. Change detection between longitudinal patient visits has also been studied in modalities beyond CXR, such as osteoarthritis in knee radiographs and retinopathy in retinal photographs [14]. Nonetheless, prior works have faced limitations in effectively attending to finegrained relevant changes while disregarding irrelevant variations. Additionally, it is important to capture long-range spatial and temporal information to identify pertinent changes in medical images effectively.Inspired by the success of Transformer models in remote sensing change detection tasks [3,28], we introduce CheXRelFormer, an end-to-end siamese disease progression model. CheXRelFormer takes a pair of CXR images as input, extracts visual features with a hierarchical vision Transformer module, and subsequently computes multi-level feature differences with a difference module. A self-attention mechanism allows the model to identify the most informative regions of the input images. By attending to fine-grained relevant changes, our model can accurately detect whether the patient's condition has improved, worsened, or remained unchanged. We evaluate the performance of our model on a large dataset of paired CXR images with corresponding disease progression labels. Experimental results demonstrate that our model outperforms existing state-of-the-art methods in detecting disease progression in CXR images. Our model has the potential to improve the efficiency and accuracy of CXR interpretation and thereby lead to more personalized treatment plans for patients. The contributions of our work can be summarized as follows:(1) We propose CheXRelFormer, an end-to-end siamese disease progression model that can accurately detect changes in CXR image pairs by attend- ing to informative regions and identifying fine-grained relevant visual differences.(2) CheXRelFormer leverages hierarchical vision Transformers and a difference module to compute multi-level feature differences across CXR images, allowing the model to capture long-range spatial and temporal information. (3) We experimentally demonstrate that CheXRelFormer outperforms existing state-of-the-art baselines in detecting disease progression in CXR images."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,2.0,Methodology,"Let C = {(X, X ) i } N i=1 be a set of CXR image pairs, where X, X ∈ R H×W ×C , and H, W , and C are the height, width, and number of channels, respectively. Each image pair (X, X ) i is associated with a set of labels Y i = {y i,m } M m=1 , where y i,m ∈ {0, 1, 2} indicates whether the pathology m appearing in the image pair has improved, worsened, or remained the same. The goal is to design a model that accurately predicts the disease progression labels for an unseen image pair (X, X ) and a wide range of pathologies.To this end, we use a hierarchical Transformer [12] encoder to process each image pair. Specifically, let X, X ∈ R H×W ×C be the input image pair. The encoder consists of L identical Transformer layers, each with a multi-head selfattention block followed by a position-wise feedforward network. The multi-head self-attention block contains a series of self-attention heads and is defined aswhere Q, K, V ∈ R N ×C are the queries, keys, and values, respectively; W O is a learned weight matrix, J is the number of heads, and head j is the j-th attention head, computed asHere, d k is the dimensionality of the key and query vectors in each head, and the softmax function is applied along the rows of the matrix. The queries, keys, and values Q j , K j , and V j are obtained via a set of linear projection matrices aswhere W Q , W K , W V are learned weight tensors that project the input embeddings onto a lower-dimensional space. Similarly, the query, key, and value matrices for the second image in the pair are computed aswhere the weight tensors W Q , W K , W V are shared across the two images in the pair. The output of each multi-head self-attention block for each image pair, denoted by (F, F ), is then fed into a position-wise feedforward network which consists of two linear transformations and a depth-wise convolution [4] that captures local spatial information:Here, W d is the shared depth-wise convolution weight matrix and each feedforward layer f 1 , f 2 consists of a linear transformation followed by a non-linear activation. The difference module then processes the visual features from each Transformer layer to compute multi-level feature differences as follows:Here, l = 1, . . . , L denotes the l-th Transformer layer, with initial inputs (F 1 , F 1 ) = (F c , F c ). Furthermore, φ is a non-linear activation, W l is a learned weight parameter that essentially represents a multi-scale trainable distance metric, and [•, •] is the concatenation operation. By computing differences between features at different scales, the proposed model can capture local and global structures that are relevant to the disease progression task. Each multi-scale feature difference map is then passed through a feed-forward layer that maps the input features to a common feature spacewhere θ l represents the set of learnable parameters for the l-th feed-forward network. The concatenated feature tensor combines information from multiple scales and is denoted aswhere [•] denotes concatenation along the channel dimension. A feed-forward network with a global average pooling step, denoted by g, creates a fused feature representation with fixed dimensionality, which is finally passed through the final classification layer, denoted by h, to obtain the label predictionsThe network is trained end-to-end with a multi-label cross-entropy classification losswhere σ represents the sigmoid function and ŷi,m , y i,m are the model prediction and the ground truth for example (X, X ) i . An overview of the model architecture is represented in Fig. 1."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.1,Implementation Details,"CheXRelFormer is implemented in Pytorch [19]. The encoder comprises four Transformer blocks with embedding dimensions 32, 64, 128, and 256, respectively. The number of heads on each multi-head attention block is 2, 2, 4, and 8. We train the encoder with a stochastic depth decay rule [6], with depths 3, 3, 6, and 18, for each Transformer block. To decrease the spatial dimension and reduce complexity, we perform spatial-reduction operations [22]. The positionwise feedforward network uses Gaussian Error Linear Unit (GELU) [5] activation functions. The multi-level image features extracted from the Transformer encoder are passed to the difference module. The difference module is composed of 2D convolutions, ReLU activations [2], and Batch Normalization [7]. The feedforward layers consist of 64 neurons. The outputs from the difference module are upsampled, concatenated, and passed through a linear fusion layer followed by global average pooling. The model is trained using AdamW optimizer [17] with a learning rate of 6 × 10 -5 and 16 batch size."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.2,Dataset,"We make use of the Chest ImaGenome dataset [25], which comprises 242, 072 frontal MIMIC-CXRs [9] that were locally labeled using a combination of rulebased natural language processing (NLP) and CXR atlas-based bounding box detection techniques [24,26] to generate the annotations. Chest ImaGenome is represented as an anatomy-centered scene graph with 1, 256 combinations of relation annotations between 29 CXR anatomical locations and their attributes. Each image is structured as one scene graph, resulting in approximately 670, 000 localized comparison relations between the anatomical locations across sequential exams. In this work, we focus on the localized comparison relations data within Chest ImaGenome that pertains to cross-image relations for nine diseases of interest. Each comparison relation in the Chest ImaGenome dataset includes the DICOM identifiers of the two CXRs being compared, the comparison label, and the disease label name. The comparison is labeled as ""no change"", ""improved"" or ""worsened"", which indicates whether the patient's condition w.r.t. the disease has remained stable, improved, or worsened, respectively. The dataset contains 122, 444 unique comparisons. We use 35, 908 CXR pairs in total that pertain to the nine diseases of interest. The distribution of the data is improved (12,396), worsened (12,287) and no change (11,205). Table 1 presents high-level dataset statistics and training/validation/test splits employed in our experiments."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.3,Baselines,"To assess the performance of the proposed CheXRelFormer model, we conduct a comparative analysis with several baselines.Local: This model employs a previously proposed siamese network [11] that only focuses on specific regions of the image, without considering inter-region dependencies or global information. The Local model is essentially a siamese network with a pretrained ResNet101 autoencoder trained on cropped Regionsof-Interest (RoIs), which are available in the Chest ImaGenome dataset."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,,Global:,The Global model is a siamese network similar to the Local model but encodes global image-level information.CheXRelNet: CheXRelNet combines global image-level information with local intra-image and inter-image information [11]. This model consists of a 2-layer graph neural network with a ResNet101 autoencoder for feature extraction. 
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.4,Experimental Results,"Table 2 lists the CXR change detection accuracy of all models across the nine diseases. We also report the mean weighted overall accuracy. CheXRelFormer outperforms baselines with a mean accuracy of 0.493±0.0012 in this three-way classification task. The closest baseline is CheXRelNet with an accuracy of 0.468 ± 0.0041. Additionally, we perform a one-tailed t-test between CheXRelFormer and CheXRelNet, with p = 0.00027 indicating that CheXRelFormer significantly outperforms CheXRelNet in seven of the nine diseases (pleural effusion, atelectasis, pulmonary edema/hazy opacity, heart failure, pneumonia, and consolidation).Most importantly, we observe up to 12% performance gains for pathology labels with limited amounts of data, such as atelectasis and consolidation. These findings suggest that CheXRelFormer has the potential to be a valuable tool for detecting changes in CXR images associated with various common diseases."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.5,Ablations on CheXRelFormer Architecture Components,"We perform an ablation study to understand the impact of four factors, the difference module, the use of global vs. localized visual information, and the impact of multi-level features. Specifically, in  An interesting observation is that CheXRelFormer Local underperforms as the focus on specific anatomies limits the visual information available to the model. Given the highly fine-grained nature of this task, this result suggests that the relationship between an area and its surroundings is critical to a radiologist's perception of change, and the local Transformer cannot provide the necessary second-order information to the model. Therefore, our results show that global image-level information is crucial for accurately predicting disease change.In addition, the absolute difference model, CheXRelFormer AbsDiff, failed to perform the disease change classification task, indicating the importance of the proposed difference module. By incorporating the difference module and computing multi-level feature differences at multiple resolutions, CheXRelFormer learns to focus on the changes between two CXRs and to ignore irrelevant information. Our results demonstrate that multi-level feature differences are critical for improving performance in predicting disease change."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.6,Qualitative Analysis,"In Fig. 2, we visualize the model predictions from CheXRelFormer using attention rollout [1]. The produced attention maps clearly show the model's focus regions, which confirm that the model concentrated on the correct region in each image pair. CheXRelFormer can better differentiate between important and extraneous visual signals, allowing it to more accurately predict the 'no change' label. The model's ability to learn the optimal distance metric for each scale allows differentiating between relevant and irrelevant differences. In addition, analyzing multiple scales of visual features enables capturing subtle changes in pairs of CXR images, resulting in a better predictive performance for the 'improved' label. In contrast, the CheXRelFormer AbsDiff model has difficulty in predicting both 'no change' and 'improved' labels (as shown in Fig. 3) due to the fact that images are not co-registered and exhibit several differences in their spatial or spectral characteristics -even though there was no actual change in the observed pathology."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,4.0,Conclusion,"Monitoring disease progression is a critical aspect of patient management. This task requires skilled clinicians to carefully reason and evaluate changes in a patient's condition. In this paper, we propose CheXRelFormer, a hierarchical Transformer with a multi-scale difference module, trained on global image pair information to detect disease changes. Our model is inspired by the way clinicians monitor changes between CXRs, and improves the state of the art in this challenging medical imaging task. Our ablation studies show that global attention and the proposed difference module are critical components, and both help detect fine-grained changes between images. While our work shows significant progress, given the fine-grained nature of visual features that characterize findings in CXRs, disease progression remains a challenging task. In future work, we intend to include multimodal contextual information beyond the images, such as patient history and reports, to enhance the results. CheXRelFormer offers a promising solution for monitoring disease progression, and future work can extend the proposed methodology to various medical imaging modalities."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,1.0,Introduction,"Intracerebral Hemorrhage (ICH) is a bleeding into the brain parenchyma, which has the second-highest incidence of stroke (accounts for more than 10% of strokes) and remains the deadliest type of stroke with mortality more than 40% [4][5][6]. Timely and proper treatments are crucial in reducing mortality [15], as well as improving functional outcomes, which is clinically deemed more valuable for prognostic model [2]. However, the treatment decision-making of ICH still remains problematic despite progression of clinical practice [20]. It is widely accepted that there is currently no effective approach in clinical practice to aid in decision-making regarding the evaluation of risks and benefits of a treatment [7,9,30]. Thus, there is an urgent need for reliable treatment recommendation model in clinical practice. Unfortunately, existing works of ICH treatment outcome prediction can either predict the outcome under a certain type of treatment [11,14,29], or consider treatment assignment as an input variable but ignore potential differences in outcomes due to varying treatment assignments [7,10,18], making it still challenging to determine from data which treatment would yield better outcomes. For this reason, we seek to provide a treatment recommendation model that outputs the reliable outcomes of all potential treatment assignments and focuses on the effect of different treatments.One of the major challenges in treatment effect estimation is missing counterfactual outcome [19,24]. This means that we can only observe the outcomes of the actual treatment decision made for an individual. As a consequence, the counterfactuals, that are, the outcomes that would have resulted from treatment decisions not given to the patient are missing. Another challenge is selection bias brought by non-randomized controlled trials [1], that the treatment assignments may highly depend on patients' characteristics. For instance, for the ICH patients with a Glasgow Coma Scale (GCS) score 9-12 [31], early surgery is generally preferred over conservative treatment [30]. This selection bias can thus make the model unreliable in predicting the outcome of conservative treatment for patients with GCS 9-12 due to lack of observational data. These factors lead to inaccurate comparisons of treatment effects, as we can only get reliable outcome on one side (i.e., early surgery or conservative treatment).To handle these challenges, some related works were based on the concept of balanced representation learning, which proposes to use additional loss to mitigate the aforementioned selection bias in the representation space [19,26,27,35]. Other approaches attempted to tackle this issue by utilizing generative models, such as variational autoencoder (VAE) [23,34] and generative adversarial network (GAN) [36], which utilize the favorable characteristics of generative models to generate either hidden unobserved variables, balanced latent variable, or uncertainties of counterfactual outcomes. These mentioned works have only shown encouraging results in estimating treatment effects from single-modality data. In practical scenarios, however, doctors routinely integrate both imaging and non-imaging data when making prognoses, and the interpretation of imaging data is substantially impacted by clinical information [17]. In this regard, we consider two key ingredients. Firstly, the selection bias commonly exists in clinical scenarios, and the sysematic imbalance brought by this bias is amplified in high-dimensional data, as the higher number of covariates makes it more challenging to establish and verify overlap [3]. Therefore, it would be significant if we could map imbalanced high-dimensional data into a balanced low-dimensional representation. We thus seek to generate the distribution of low-dimensional prognostic score [12], which we will explain in Sect. 2 later. Secondly, motivated by the existing multi-modality VAE models [21,22,28,33], we can fuse multimodality distributions into a joint distribution with reasonable feasibility, which can be leveraged to construct a multi-modality model for prognosis.In this paper, we propose a novel prognostic model that leverages both imaging and tabular data to achieve accurate treatment outcome prediction. This model is intended to be trained on observational data obtained from the nonrandomized controlled trials. Specifically, to increase the reliability of the model, we employ a variational autoencoder model to generate a low-dimensional prognostic score that alleviates the problem of selection bias. Moreover, we introduce a variational distributions combination module that integrate information from imaging data and non-imaging clinical data to generate the aforementioned prognostic score. We evaluate our proposed model on a clinical dataset of intracerebral hemorrhage and demonstrate a significant improvement in treatment outcome prediction compared to existing treatment effect estimation techniques. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.1,Formulation and Motivation,"We aim to predict the individualized treatment outcome based on a set of observations that include the actual treatment T , observed covariates X, and factual outcome Y . In this paper, we study the one-year functional outcome of patient who underwent either conservative treatment (T = 0) or surgery (T = 1). For each individual, let t ∈ {0, 1} denote the treatment assignment, x = (x img , x tab )represent the observed covariates comprising imaging data x img and non-imaging tabular data x tab , and y indicate the factual outcome. In this study, the treatment outcome was assessed using 1-year modified Rankin Scale (mRS) [32]. Our objective is to estimateThe non-randomized controlled trials impacted by treatment preference can lead to selection bias, rendering the model unreliable due to potential encounters with unobserved scenarios during training. To address this issue, our model is inspired by the approach commonly used by doctors in clinical practice: using a combination of imaging data and non-imaging biomarkers to generate a prognostic score (e.g., GCS score) that predicts the likelihood of good or poor condition after treatment. In this study, a prognostic score is defined as any function f T (X) of X and T that Markov separates Y and X, such that Y ⫫ X|f T (X). The insight is that a patient's health status can be effectively captured by a lowdimensional score Z = f T (X), which is a form of dimension reduction that is sufficient for causal inference and can naturally mitigate the problem brought by non-randomized controlled trials. This is because, as illustrated in Fig. 1(b), the difficulty of establishing and verifying overlap (between samples with T = 0 and samples with T = 1) increases in high-dimensional feature space compared to low-dimensional feature space [3]. We consider utilizing a VAE-based model for generating a prognostic score, due to two key ingredients: On the one hand, modeling score through a conditional distribution instead of a deterministic function offers greater flexibility [34]. On the other hand, VAE is a good model for dimension reduction, compared with vanilla encoder and other generative models. It has also been proved to be effective for treatment effect estimation."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.2,Generative Prognostic Model,"Architecture. As can be seen in Fig. 1(a), we first use two parallel networks to generate latent variables of imaging data and non-imaging tabular data, respectively. For the imaging data, we employ a 3D ResNet-34 [13] as our feature extraction network and modify the final fully connected layers. We then generate the features conditioned on different treatments by concatenating the extracted features with their respective treatment assignments t and forwarding them to a shared fully connected layer (FC layer), yielding Φ respectively. This allows us to incorporate treatment assignment information and generate the prognostic score more effectively. For the non-imaging tabular data, we employ three blocks of a FC layer, followed by a Batch Normalization layer, and a ReLU activation function to generate the features Φ tab 0 and Φ tab 1 . These features are then forwarded to a variational distribution combination (VDC) module, which we will describe in detail later. Through the VDC module, we can estimate the prior distribution p(z | x, t) of the prognostic score z = (z 0 , z 1 ). In addition, during the training phase, the true posterior distribution q(z | x, y, t) can be approximated, which is additionally conditioned on y and can help the model learn how to estimate an accurate prior distribution. The prognostic score z 0 , z 1 are then concatenated with treatment t = 1 and t = 0 respectively and are passed through a decoder consisting of a shared FC layer, to output the predicted potential outcomes with different treatment assignments, i.e., ŷ0 and ŷ1 . Notably, during the inference phase, we use the prior distribution p(z | x, t) to generate z. In contrast, during the training phase, we use the posterior distribution q(z | x, y, t) to generate z and predict the outcomes. Training Scheme. The evidence lower bound (ELBO) of our model is given by:where D KL (q(z | x, y, t)∥p(z | x, t)) is the Kullback-Leibler (KL) divergence between distributions q(z | x, y, t) and p(z | x, t), and β is the weight balancing the terms in the ELBO. Note that the first term of Eq. 1 corresponds the classification error, which is minimized by the cross entropy loss. The second term of Eq. 1 uses KL divergence to encourage convergence of the prior distribution towards the posterior distribution. The training objective is to maximize the ELBO given the observational data, so that the model can be optimized."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.3,Variational Distributions Combination,"Once the features of imaging and non-imaging tabular data have been extracted, the primary challenge is to effectively integrate the multi-modal information.One approach that is immediately apparent is to train a single encoder network that takes all modalities as input, which can explicitly parameterize the joint distribution. Another commonly used method called Mixture-of-Experts (MoE) proposes to fuse the distributions from different modalities by weighting [28]. However, for this study, we generate the distribution of each modality separately and then use the Product-of-Experts (PoE) method to combine the two distributions into a single one [22,33]. As can be seen in Fig. 1 whereand Σ pri t are mean and covariance of universal prior expert, which is typically a spherical Gaussian (N (0, 1)). For posterior distribution q(z t | x, y, t), we first additionally concatenate the features and y together, and then generate the joint distribution by the same way. The PoE for generating joint distributions offers several advantages over the aforementioned approaches. Compared with the approaches that simply combing the features and then generating the joint distributions, PoE not only can effectively address the potential issue of prediction outcomes being overly influenced by the modality with a more abundant feature [17], but also is more flexible and has the potential to handle missing modalities. Compared to that using a mixture-of-experts, it can produce sharper distributions [16,22], which is desirable for our multi-modality data with complementary or unevenly distributed information."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,3.1,Dataset and Experimental Setup,"Datasets. We utilized an in-house dataset of intracerebral hemorrhage cases obtained from the Hong Kong Hospital Authority. The dataset comprises 504 cases who underwent head CT scans and were diagnosed with ICH. Among them, 364 cases received conservative treatment, and 140 cases underwent surgery treatment. For each case, we collected both CT imaging and non-imaging clinical data. The non-imaging data have 17 clinical characteristics which have been proved to be potentially associated with the treatment outcome in clinical practice [18], including gender, age, admission type, GCS, the history of smoking and drinking, hypertension, diabetes mellitus, hyperlipidemia, history of atrial fibrillation, coronary heart disease, history of stroke, pre-admission anticoagulation, pre-admission antiplatelet, pre-admission statin, small-vessel vascular disease and lower cholesterol. To address the selection bias resulting from the non-randomized controlled trials, we intentionally increased the imbalance of the dataset. Specifically, we selected 50 out of 68 cases who had IVH (another subtype of brain hemorrhage which can be infered from the CT image) and were treated conservatively, and 50 out of 61 cases who had a GCS score below 9 and underwent surgery. We used these samples for testing and reserved the remaining cases for training our model. As a result, the dataset is systematically imbalanced, which presents challenges for the model in producing reliable outcomes on test set. We also conducted additional experiments with different setting, as shown in the supplementary. A favorable outcome was defined as an mRS score of 0 to 3 (247 cases in total), while an unfavorable outcome was defined as an mRS score of 4 to 6 (257 cases in total) [8,11,29].Evaluation Metrics. We employed three evaluation metrics that are commonly used in treatment effect estimation and outcome prediction in our experiments, including the policy risk (P ROL ), the accuracy (Acc) and the area under the ROC curve (AU C). P ROL measures the average loss incurred when utilizing the treatment predicted by the treatment outcome estimator [26], which is a lower-is-better metric. Besides, we calculate Acc 0 /AU C 0 for samples factually treated with T = 0 and Acc 1 /AU C 1 for samples factually treated with T = 1."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,,Implementation Details.,"In preprocessing the imaging data, raw image intensity values were truncated to [-20, 100], normalized to have zero mean and unit variance, and slices were uniformly resized to 224×224 in the axial plane. We implemented our model using PyTorch and executed it on an NVIDIA A100 SXM4 card. For training, we used the Adam optimizer, a weight decay of 5×10 -3 , and an initial learning rate of 5 × 10 -3 . The training process lasted for a total of 2 h, consisting of 1000 epochs with a batch size of 128. Our reported results are the average and standard deviation obtained from three independent runs. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,3.2,Experiment Results,"Comparison with State-of-the-Art Methods. We benchmarked our method against state-of-the-art approaches for treatment effect estimation, which are recognized as strong competitors in this field. These approaches include BNN [19], which is a representative work that balances the distribution of different treatment groups by discrepancy distance minimization, CFR-WASS [26], which use separate heads to estimate the treatment effect and use Wasserstein Distance to balance the distribution, SITE [35], which prioritizes hard samples to preserve local similarity while also balancing the distribution of data, and β-Intact-VAE [34], which proposes to use a novel VAE model to generate low-dimensional representation conditioned on both covariates and treatment assignments to handle the selection bias problem. These methods were primarily developed for estimating treatment effects on single-modality data. To apply these methods to our multi-modality data, we utilized the same feature extraction architectures as our approach. The features extracted from these networks are concatenated. The fused features are then forwarded to the specific networks described in these papers. Moreover, we compared our method with DAFT [25], which designs a block to suppress high-level concepts from 3D images while considering both image and tabular data, making it great for processing multi-modal data.Table 1 shows a comparison of results from different methods for estimating treatment outcomes in ICH. Our proposed method demonstrated a significant improvement in model performance compared to other methods, as evaluated using all five metrics. Compared with the classic methods based on balanced presentation learning, β-Intact-VAE seeks to generate balanced latent variable so that is more effective and can achieve more accurate performance in our experiment setting. However, this strategy lacks the ability to extract complementary information as it does not include a specially designed module for combining the distributions extracted from different modalities. Instead of simply concatenating features and generating a low-dimensional representation, our model utilizes the PoE technique to combine two low-dimensional distributions generated from two distinct modalities, which can effectively mitigate the risk of prediction outcomes being disproportionately influenced by the more feature-rich modality [17]. For these reasons, our proposed method improved performances by 3.1% on R P OL , 2.3%/2.7% on AU C 0 /AU C 1 , and 2.0%/2.7% on Acc 0 /Acc 1 , respectively. Ablation Analysis. We then conducted comprehensive ablation studies. Initially, we studied the necessity of using a VAE structure for obtaining a lowdimensional prognostic score instead of a vanilla encoder. As shown in Fig. 2(a), we systematically changed the degree of selection bias by varying the number of cases with IVH who underwent conservative treatment (68 in total) in the training set and test set. The ratios of training set/test set were: 68/0 (Degree 1), 48/20 (Degree 2), 18/50 (Degree 3) and 0/68 (Degree 4). When there are fewer cases in the training set, the selection bias increases, leading to a reduced ability of the model to predict such cases in the test set. The experiment showed that as the selection bias increased in the training set (ranging from Degree 1 to Degree 3), the difference between using a VAE structure and a vanilla encoder became more prominent. This is due to the VAE's effective dimension reduction. When there were no cases related to the outcome of interest (Degree 4) in the training set, further increases were stopped. This is expected since in such situations, there are no related cases that the model can learn from, thus rendering the advantages of dimension reduction ineffective.Next, we studied the contributions of multi-modality distribution combination. As can be seen in Fig. 2(b), despite using the proposed generative prognostic model, satisfactory performance cannot be achieved by simply using a single modality. Furthermore, compared to other commonly used approaches for integrating two generated distributions, such as simply combining the feature maps before generating the prognostic score (Multi-modality w/o VDC) and Mixtureof-Experts (Multi-modality w/ MoE), our proposed model (Multi-modality w/ PoE) achieved better performance. This highlights the effectiveness of distribution combination via PoE. Additionally, in Fig. 2(c) and (d), we compared the performance of the model trained with different dimensions of generated prognostic score and the values of β in Eq. 1. The dimension of the generated prognostic score is a trade-off between the degree of eliminating selection bias and the amount of accessible information. The results demonstrate that the optimal choice of dimension is 10. Moreover, note that β controls the trade-off between outcome reconstruction and prognostic score recovery. Figure 2(d) suggests β should be 0.5 for low imbalance degree and 1.0 for high imbalance degree."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,4.0,Conclusion,"This paper introduces a novel generative prognostic model for predicting ICH treatment outcomes using imaging and non-imaging data. The model is designed to be trained on data collected from non-randomized controlled trials, addressing the imbalance problem with a VAE model and integrating multi-modality information using a variational distribution combination module. The model was evaluated on a large-scale dataset, confirming its effectiveness."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,,Table 1 .,Ours.502 ± .023 .820 ± .019 .801 ± .018 .793 ± .012 .780 ± .040
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,1.0,Introduction,"Multiple Instance Learning (MIL) [6,21] is a type of weakly supervised learning that has become very popular in biomedical imaging diagnostics due to the reduced annotation effort it requires [8,13]. In the case of MIL binary classification, the training set is partitioned into bags of instances. Both bags and instances have labels, but only bag labels are observed while instance labels remain unknown. It is assumed that a bag label is positive if and only if the bag contains at least one positive instance [10]. The goal is to produce a method that, trained on bag labels only, is capable of predicting both bag and instance labels.Among the proposed approaches for learning in the MIL scenario [21], deep learning (DL) methods stand out when dealing with highly structured data (such as medical images and videos) [17]. The most successful deep MIL approaches combine an instance-level processing mechanism (i.e., a feature extractor) with a pooling mechanism to aggregate information from instances in a bag [8,13]. Among the pooling operators, the attention-based weight pooling proposed in [15] is frequently used as a way to discover key instances, i.e., those responsible for the label of a bag. However, this pooling operator was formulated under strong assumptions of independence between the instances in a bag. This is a drawback in biomedical imaging problems, where instances in a bag are often spatially or sequentially ordered and their diagnostic importance is expected to be similar for neighboring instances [18,24].In this work, we are particularly interested in the detection of intracranial hemorrhage (ICH), a serious life-threatening emergency caused by blood leakage inside the brain [5,22]. Radiologists confirm the presence of ICH by using computed tomography (CT) scans [9], which consist of a significant number of slices, each representing a section of the head at a given height. Unfortunately, the shortage of specialized radiologists and their increasing workload sometimes lead to delayed and erroneous diagnoses [3,12,20,25], which may result in potentially preventable cerebral injury or morbidity [9,11]. For this reason, there is a growing interest in the development of automated systems to assist radiologists in making rapid and reliable diagnoses.State-of-the-art ICH detection methods rely on DL models, specifically convolutional neural networks (CNNs), to extract meaningful ICH features [31]. However, 2D CNNs need to be coupled with other mechanisms such as recurrent neural networks (RNNs) [14,30] or 3D CNNs [2,7,16,27] to account for interslice dependencies. Although these approaches are quite successful in terms of performance, their use is limited by the large amount of labeled data they require [31]. To address this issue, the ICH detection task has been formulated as an MIL problem, achieving comparable performance to fully supervised models while reducing the workload of radiologists [26,29]. Note that the MIL framework is naturally suited for the ICH detection problem since a CT scan (i.e., a bag) is considered positive if it contains at least one slice (i.e., an instance) with evidence of hemorrhage (i.e., positive instance).In this work, we improve upon the state-of-the-art deep MIL methods by introducing dependencies between instances in a sound probabilistic manner. These dependencies are formulated over a neighborhood graph to impose smoothness on the latent function that encodes the attention given to each instance. Smoothness is achieved by introducing specific first-and second-order constraints on the latent function. Our model, called SA-DMIL, is applied to the ICH detection problem, obtaining (a) significant improvements upon the performance of non-smooth models at both scan and slice levels, (b) smoother attention weights across slices by benefiting from the inter-slice dependencies, and (c) a superior performance against other popular MIL methods on the same test set."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.1,Problem Formulation,"We start by formulating ICH detection as a Multiple Instance Learning (MIL) problem. To do so, we map slices to instances and CT scans to bags. The slices (instances) will be denoted by, where H and W are the height and width of the image, 3 is the number of color channels, b is the index of the scan to which the slice belongs to and i is the index of the slice inside the bag. We will denote the label of a slice by y b i ∈ {0, 1}. If the slice contains hemorrhage, then y b i = 1, otherwise y b i = 0. Note that the slice labels remain unknown since only scan labels are given. As we know, slices are grouped to form the CT scans. Each scan (bag) will be denoted byHere, N b is the number of slices in bag b. We will assume that B CT scans are given, so b ∈ {1, . . . , B}. Given a CT scan b, we will denote its label by T b ∈ {0, 1}. Notice that T b = 1 if and only if some of y i b = 1, i.e., the following relationship between scan and slice labels holds,"
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.2,Attention-Based Multiple Instance Learning Pooling,"The attention-based MIL pooling was proposed in [15] as a way to discover key instances, i.e., those responsible for the diagnosis of a scan. It consists of a weighted average of instances (low-dimensional embeddings) where the weights are parameterized by a neural network. Formally, given a bag of N b embeddings, where z b i ∈ R D , the attention-based MIL pooling computeswhereNotice that w ∈ R L and V ∈ R L×D are trainable parameters, where D denotes the size of feature vectors. We refer to s z b i as attention weights and to f z b i as attention values. This operator was proposed under the assumption that the instances in a bag show neither dependency nor order among each other. Although this may be the case in simple problems, it does not occur in problems such as ICH detection. Note that the attention weights of slices in a bag are correlated: given a slice containing ICH, we expect that the adjacent slices will also contain ICH with high probabilities. This is essential in finding slices with ICH. In the next subsection, we show how to introduce this correlation between attention weights."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.3,Modeling Correlation Through the Attention Mechanism,"Ideally, in the case of a positive scan (T b = 1), high attention weights should be assigned to slices that are likely to have a positive label (y b i = 1). Given the dependency between slices, contiguous slices should have similar attention values. In other words, the differences between the attention values of contiguous slices should be small. Thus, for each bag b, these quantities should be smallwhere A b ij = 1 if the slices i, j are related in bag b, and 0 otherwise. We smooth f z b i instead of s z b i because a non-constrained parameter f ensures consistent smoothing while s requires a normalization across instances in a bag.Equations ( 4) and ( 5) correspond, respectively, to the energies of the, so called, conditional and simultaneous autoregressive models in the statistics literature [4,23]. For our problem, they model the value of f at a given location (instance) given the values at neighboring instances. From the regularization viewpoint, these terms constrain the first and second derivatives of the function f , respectively, which favors smoother functions (examine the zero of the derivative of f ). That is, a priori all attention weights are expected to be the same because f is expected to be constant. As observations arrive, they change to reflect the importance of each instance. Note that (4) and ( 5) impose smoothness but they can be modified to model, for example, competition between the attention weights by simply replacing the minus sign with a plus sign.To compute L b S1 and L b S2 efficiently we consider the simple graph defined by the dependency between slices. For a bag b, its adjacency matrix isij is a diagonal matrix that contains the degree of each slice (the degree of the slice i is the number of slices j such that A b ij = 1). This is, D b ii = degree(i) and D b ij = 0 if i = j. Using these, one can compute the graph Laplacian matrix of a bag aswherewhere k ∈ {1, 2}, can be added to the loss function of a network to be minimized along the taskspecific loss. Note that these two terms provide two different approaches to exploiting the correlations between instances through the loss function. We will refer to this approach as smooth attention (SA) loss. In the following subsection, we propose a model that can use either L S1 or L S2 . The effect of each term will be discussed in Sect. 4."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.4,SA-DMIL Model Description,"We propose to couple the attention-based MIL pooling with the SA loss terms introduced in Subsect. 2.3. The proposed model, named Smooth Attention Deep Multiple Instance Learning (SA-DMIL), is depicted in Fig. 1. We use a Convolutional Neural Network (CNN), denoted by Φ CNN , as a feature extractor to obtain a vector of low dimensional embeddings for each instance. That is, given a bagThe CNN module in Fig. 1 is implemented with six convolutional blocks, followed by a flatten layer. Z b is then fed into the attention layer Φ Att described in Subsect. 2.3 to obtain a scan representation. After that, the scan representation passes through a classifier Φ c (i.e., one fully connected layer with a sigmoid activation) to predict the scan labels,where we have writtenwhere α ∈ [0, 1] is an hyperparameter and L CE the common cross-entropy loss,where k ∈ {1, 2}, and L Sk = b L b Sk (see Eqs. ( 4) and ( 5)). Depending on the value of k, we obtain two variations of SA-DMIL, which will be referred to as SA-DMIL-S1 and SA-DMIL-S2. The baseline model, Att-MIL (non-smooth attention), is recovered when α = 0.0 [15]. Following the approach of previous studies [19,29], attention weights will be used to obtain predictions at the slice level (although they are not specifically designed for it). If a scan is predicted to be negative, all slices are also predicted to be negative, while if a scan is predicted to correspond to an ICH, slices whose attention weight is above a threshold (i.e., 1/N b , with N b being the number of slices in that scan) are predicted as ICH."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,3.1,Data and Data Preprocessing,"The dataset used in this work was obtained from the 2019 Radiological Society of North America (RSNA) challenge [1], which included 39650 CT slices from 1150 subjects. The data were split among subjects, with 1000 scans (ICH: Normal scans = 411: 589; ICH: Normal slices = 4976: 29520) used for training and validation, and the remaining 150 scans (ICH: Normal scans = 72: 78; ICH: Normal slices = 806: 4448) used for held-out testing. The number of slices in the scans varied from 24 to 57. All CT slices underwent the same preprocessing procedure as described in [29]. Each CT slice had three windows applied to its original Hounsfield Units by changing the window Width (W) and Center (C) to manipulate the display of specific tissues, as radiologists typically do when diagnosing brain CTs. Here, we selected the brain (W: 80, C:40), subdural (W:200, C:80) and soft tissue (W:380, C: 40) windows. All images were then resized to the same size of 512 × 512 and normalized to the range [0, 1]. CTs were annotated at both the scan and slice levels, but slice labels were used for evaluation only, while scan labels were used for training and evaluation."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,3.2,Experimental Settings,"We fix D = 128 and L = 50 in Eq. ( 3). We use the Adam optimizer with the learning rate starting at 10 -4 . The batch size is set to 4, the maximum number of epochs is set to 200 and the patience for early stopping is set to 8. We test different values of the α hyperparameter, between 0 and 1 with a jump of 0.1. All experiments were run 5 independent times and the mean and standard deviation were reported in the held-old testing set at both scan and slice levels. The average training time is 10.3 h for SA-DMIL-S1 and 10.5 h for SA-DMIL-S2. The prediction time is approximately 15.8 s for each scan. All experiments were conducted using Tensorflow 2.11 in Python 3.8 on a single GPU (NVIDIA Quadro RTX 8000). The code will be available via GitHub."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.1,Hyperparameters Tuning,"In this subsection, we study the effect of SA loss in terms of performance. Table 1 compares the performance of models for different values of α. The standard deviation and other values of α can be found in the appendix, Tables S1 andS2. The results show that at both scan and slice levels, adding a smoothness term to the loss function (α > 0.0) achieves better performance than Att-MIL (α = 0.0). These improvements are significant, with increases in accuracy, F1 and AUC scores of approximately 7%, 9% and 5% respectively, at scan level, and increases in accuracy and F1 score of 8% and 11% respectively, at slice level. The recall is the only metric in which our model does not excel, where the baseline Att-MIL obtains the best value. However, this is associated with very low precision values. Note that, as α increases, the performance of the model first improves and then drops, which is consistent with the role played by the SA loss as a regularization term. The difference between L S1 and L S2 is not significant although L S1 performs slightly better. In fact, when using L S1 , α = 0.5 gives the best diagnostic performance with an AUC of 0.879 (± 0.003) at scan level and an accuracy of 0.834 (± 0.010) at slice level. "
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.2,Smooth Attention MIL vs. Other MIL Methods,"The performance of other popular MIL methods is also included in Table 1. All method share the same CNN architecture to extract slice features, but they differ in the pooling operator they use: Max [28], Mean [28], Attention [15] or Gaussian Process (GP) [29]. These results show that the performance of SA-DMIL is consistently better than other methods across different metrics and at both scan and slice levels. Only the precision of MIL+Max agg. and the recall of AttCNN+VGPMIL at scan level are higher than those obtained by SA-DMIL. However, considering the trade-off between precision and recall given by F1, our method achieves a superior performance. In tasks like ICH detection, where neighbouring instances are expected to have similar diagnostic importance. Unlike other MIL methods that assume each instance to be independently distributed, SA-DMIL stands out by considering the spatial correlation between instances, which compels it to learn more meaningful features for making accurate bag predictions. Notably, this is achieved by simply adding a smoothing term to the loss function without increasing the number of model parameters. This can potentially be applied to existing architectures to further improve performance without adding complexity."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.3,Visualizing Smooth Regularizing Effects at Slice Level,"So far we have observed enhanced performance through the SA term. In this subsection, we visually illustrate how this novel term imposes smoothness between attention scores of consecutive slices, leading to more accurate predictions.Figure 2 shows plots of the attention scores assigned by SA-DMIL-S1 and Att-MIL to the slices of three different scans (Fig. S1 in the appendix contains an analogous plot for SA-DMIL-S2). As expected, introducing the SA loss results in smoother attention weights. Note that the smoothness constraint of SA-DMIL effectively penalizes the appearance of isolated non-smooth attention weights that incorrectly jump over or below the threshold. We also include visual examples of consecutive CT slices in Fig. 3. In Scan 1, the baseline Att-MIL produces a wrong prediction at scan level. When using SA, the prediction is correct since dependencies between adjacent slices have been learned. In Scan 2, both models produce correct predictions at scan level, but SA-DMIL is more accurate at slice level. This occurs thanks to the SA loss, that turns the attention scores into smoother values and, therefore, avoids random jumps up and down the decision threshold."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,5.0,Conclusion,"In this study we have proposed SA-DMIL, a new model that obtains significant improvements in ICH classification compared to state-of-the-art MIL methods. This is done by adding a smoothing regularizing term to the loss function. This term imposes a smoothness constraint on the latent function that encodes the attention weights, which forces our model to learn dependencies between instances rather than training each instance independently in a bag. This flexible approach does not introduce any additional complexity, so similar ideas can be applied to other methods to model dependencies between neighboring instances."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,,Data Use Declaration,The dataset used in this study is from the 2019 RSNA Intracranial Hemorrhage Detection Challenge and is publicly available in this link.
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 32.
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,1.0,Introduction,"Images and text are inherently intertwined in clinical diagnosis and treatment. Having an automated approach that is able to answer questions based on images, giving insight to clinicians and patients, can be a valuable asset. In such a medical Visual Question Answering (VQA) setting the common approach is to treat VQA as a multi-class classification problem solved by neural networks. Given a joint encoded representation of the image and question, the model classifies it into a predefined set of answers. Although these approaches yield good performance [5,18,24,33], they deal with closed-set predictions, which is not an ideal solution for VQA. For instance, medical VQA datasets commonly contain hundreds to thousands of free-form answers [11], which is suboptimal to be treated as a classification task. Moreover, the severe class imbalance and out-of-vocabulary answers further hinder the generalizability of these classification methods.We believe that a possible solution can be found in the generative capability of language models, since they are able to produce free text, instead of being limited to closed-set predictions. However, leveraging language models for solving open-ended medical VQA is limited due to several challenges, such as finding ways to properly communicate the visual features and letting such large-scale models be employed on small-sized medical VQA datasets.Inspired by recent image captioning models [22], we propose to use the medical images by converting them into a set of learnable tokens through a small-scale mapping network. These tokens can then be interpreted as a visual prefix for the language model [1,4,23]. Afterward, the visual prefix is used together with the question as input to the language model, which generates the answer token by token [25].Furthermore, large-scale language models can generalize across domains while keeping their weights frozen [30]. This makes them very appealing for the medical domain, which inherently does not possess large quantities of labeled data required to train these models from scratch [29]. Models like BioGPT [21] and BioMedLM [31] are based on the generic GPT2 language model [26] and are trained on biomedical text corpora. They perform quite well compared to their general counterparts on specific biomedical language tasks, like question answering or relation extraction. We design our model in a flexible manner, which allows us to incorporate any of these pre-trained language models.In summary, we contribute in three major aspects: (i) We propose the first large-scale language model-based method for open-ended medical VQA. (ii) We adopt parameter-efficient tuning strategies for the language backbone, which gives us the ability to fine-tune a large model with a small dataset without the danger of overfitting. (iii) We demonstrate through extensive experiments on relevant benchmarks that our model yields strong open-ended VQA performance without the need for extensive computational resources."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,2.0,Related Works,"To describe existing medical VQA methods, we make a distinction between classification methods and generative methods. The majority of methods are classification-based and make use of different types of encoders, such as CNNs or Transformers [3,9,11,17,32] followed by a classification layer.Classification-Based VQA. We highlight a number of methods that showed good performance on current competitive medical VQA datasets. The Mixture Enhanced Visual Features (MEVF) [24] is initialized based on pre-trained weights from the Model-Agnostic Meta-Learning (MAML) model [7] in combination with image feature extraction from Conditional Denoising Auto-Encoders (CDAE) to generate a joint question-answer representation using Bilinear (BAN) or Stacked (SAN) Attention Networks. Do et al. [5] create a similar embedding space by extracting annotations from multiple pre-trained meta-models, and learning meta-annotations by training each meta-model. Linear combinations [10] or question-conditioned selections [34] from this multi-modal embedding space can further enhance performance. The use of Transformer [14] and CLIP [6,25] encoders also results in strong VQA classification performance.Open-Ended VQA. MedFuseNet [28] is one of the few methods performing and reporting open-ended visual question answering on recent public datasets. They do so by creating a BERT-based multi-modal representation of image and question and subsequently passing it through an LSTM decoder. Ren et al. [27] create open-ended answers by using the masked token prediction functionality of BERT. We aim to show that generative language models are more versatile and better suited for this task."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.1,Problem Statement,"Given an input image I and an input question in natural language Q, our method aims to sequentially generate an answer A = {A 0 , A 1 , ..., A N } composed of N tokens, by conditioning on both inputs. From a model definition perspective, we aim to find the optimal parameters θ * for a model by maximizing the conditional log-likelihood as follows:(1)"
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.2,Model Architecture,"Our VQA model is designed as an encoder-decoder architecture, with a twostream encoder and a language model (LM) as a decoder, as illustrated in Fig. 1. Specifically, the two streams encode the two input modalities, namely the image I and the question Q. The language model is defined as a causal language Transformer [26], and it generates the answer A in an autoregressive manner. It closely follows the prefix tuning technique for prompting a language model to produce an output of a particular style [16], such as in our case an answer given a question and an image1 ."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,,Vision Encoding Stream.,"For encoding the image, we employ a pre-trained vision encoder to extract visual features {x 1 , x 2 ...x x }. To use these features as input to the decoder, they should be mapped into the latent space of the language decoder. Following [22], we define a mapping network f M , implemented as a three-layer MLP. This network maps the visual features into a visual prefix {v 1 , v 2 , . . . v x } ∈ R x×e for the language model, where e is the embedding size."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,,Language Encoding Stream.,"Regarding the encoding of the textual part, firstly we utilize a standard tokenization process to obtain a sequence of tokens, both for the question Q = {q 1 , q 2 ...q q } ∈ R q ×e and answer A = {a 1 , a 2 ...a a } ∈ R a ×e . This is followed by embedding the tokens using the embedding function of a pre-trained language model. Prompt Structure. To create a structured prompt, following existing QA methods using language models [2,26], we prepend the question, image, and answer tokens with tokenized descriptive strings, namely question:, context: and answer:. By placing the embeddings of the question before the visual tokens we mitigate the problem of fixation of the language model on the question [21,22]. As an example this would yield the following prompt template: p =[question: What does the right side of the field show? context: v 1 , v 2 , . . . v x answer: ] which is fed as input to the language model. Language Model. Following standard language modeling systems, we treat VQA as a conditional generation of text, and we optimize the standard maximum likelihood objective during training. The language model receives the prompt sequence p as input and outputs the answer A, token by token. Specifically, at each time step i, the output of the model are the logits parametrizing a categorical distribution p θ (A) over the vocabulary tokens. This distribution is represented as follows:The parameters of the language model are initialized from a pre-trained model, which has been previously pre-trained on huge web-collected datasets. "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.3,Parameter-Efficient Strategies for Fine-Tuning the Language Model,"Standard fine-tuning of language models can hurt the generalization capabilities of the model, especially if small, domain-specific datasets are used as in our case. Therefore, we consider four different parameter-efficient strategies that adapt the attention blocks of language models, as illustrated in Fig. 2 and outlined below:Frozen Method: the parameters of the language model are kept entirely frozen during training, following [30]. In this setting, only the mapping network is updated through backpropagation. Prompt Tuning: we prepend a set of m learnable tokens M ∈ R m×e to the input prompt sequence, which yields [M, p] [15] as input to the frozen language model. Besides updating the mapping network, this approach also involves updating these learnable tokens through backpropagation. Prefix Tuning: we prepend a learnable prefix P j to the query Q j of each attention block j in the Transformer, such that Q ft j = [P j , Q j ] [16]. Similar as in prompt tuning, we update both the mapping function and the learnable prefixes of the queries. Low-Rank Adaptation (LoRA): We add learnable weight matrices to the query Q and value V of the attention blocks in each layer of the frozen language model as W + ΔW following [12]. Again, the mapping function is trained together with the learnable weight matrices."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,4.0,Experimental Setup,"Datasets. The three datasets used for the evaluation of our method are Slake [20], PathVQA [11], and OVQA [13]. These three datasets are the current most suitable VQA datasets given their large variety in answers and the manual curation of answers by domain experts. Each dataset is split 50/50 between 'yes/no' and open-set answers. See the datatset details in Table 1. We use the official train/validation/test splits across all three datasets. Evaluation Protocol. We evaluate our approach using the conventional metrics BLEU-1 and F1 Score. Additionally, we measure the contextual capturing of information with BERTScore [35] this method can handle synonyms. Lastly to allow for comparison against existing classification-based methods we also report accuracy and F1 score. Implementation Details. We extract the visual features using a pre-trained CLIP model with ViT backbone [25], having a dimensionality of 512. The MLP layers of the mapping network f M have sizes {512, ( x • e)/2, x • e}. The length of x is set at 8. The lengths q and a are dataset dependent and defined by the mean number of tokens in the train set plus three times its standard deviation. Zero padding is added to the right side of the sequence for batch-wise learning. We use the following language models: GPT2-XL [26], a causal language model with 1.5B parameters trained on WebText [26]. BioMedLM [31] and BioGPT [21] are both GPT2-based models, pre-trained on PubMed and biomedical data from The Pile [8], with a size of 1.5B and 2.7B parameters, respectively. All models are able to train on a single NVIDIA RTX 2080ti GPU (average training time ≈ 3 h). We use the AdamW optimizer with 600 warmup steps and a learning rate of 5e-3 and apply early stopping with a tolerance of 3."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,5.0,Results,"Benefits of Parameter-Efficient Fine-Tuning. The evaluation of our method across various language models and fine-tuning settings in Table 2 shows Table 2. Performance across different language models and fine-tuning strategies, measured in BLEU1 (BL1), BERTScore (BS), F1 and accuracy. Params% is the amount of trainable parameters in the language model. Our method using GPT2 in combination with LoRA yields the best performance across all datasets.  that language models can perform open-ended medical VQA. Specifically, we outperform the only existing method MedFuseNet [28] that does open-ended VQA, due to the capability of pre-trained language models to capture long-term dependencies when generating free-form answers. Additionally, prefix [16] and prompt tuning [15] do not improve the performance of the model as much as using LoRA [12] which directly adapts the Q and V weight matrices of the attention blocks. Moreover, larger datasets show the most consistent performance gain of parameter-efficient fine-tuning across all metrics.Comparison Between Standard and Medical LMs. Using a language model pre-trained on a general text corpus, such as GPT2 [26], improves the overall performance compared to its medically-trained models (e.g. BioGPT or BioMedLM), as can be observed in Table 2. BioGPT and BioMedLM could be overoptimized to their medical text corpora, which leads to lack of generalization to different downstream domains. As mentioned in [21,31], these models require full fine-tuning on the respective downstream tasks, to achieve the desired performance. On the other hand, GPT2 benefits from observing diverse data during pre-training which also encompasses medically oriented text. This enables GPT2 models to generalize easily to other domains, which is relevant for our different VQA datasets.Benefit of Open-Ended Answer Generation. Our method is performing significantly better on the open-set answering, in comparison to classificationbased methods, as shown in Table 3. We also confirm that CLIP based image embeddings perform well in the medical domain [6] compared to the conventional use of CNNs. Since our approach is generative, it is not bounded by the class imbalance issue, which is considered a bottleneck of classification-based VQA Table 4. Effect of using different prompt structures. Note that Q and I denote the question and image respectively. The regular setting with the question embeddings followed by the visual prefix (Fig. 1) leads to the best overall performance. models. Our method performs especially well compared to other method on PathVQA, which relatively has the largest class imbalance, accentuating this effect. Even on the simple 'yes/no' questions, the performance is better, showing that this simple yet effective method provides a more natural way of doing VQA. It worth noting that the comparison of accuracy as a metric for exact matches, between classification and generation methods is not in favor of generative methods. Despite that, we outperform existing methods on all datasets and metrics, which is a testament to the benefit of phrasing VQA as an open-ended generation problem.In Fig. 3(a-c), we show qualitative examples of capability of the language model to successfully predict the correct answer. However, in Fig. 3 (d,e) we show cases where our method predicts a factually correct answer which is not specific enough.Effect of Using Different Prompt Structures. We also investigate the influence of the prompt structure on the overall performance, demonstrated in Table 4. It can be observed that the performance largely decreases when the question is removed, compared to when the visual information is removed. This suggests that the question plays a more important role in answer generation.Interestingly, the model is sensitive the order of the elements in the prompt, as the swapping of the question embeddings and the visual prefix yields decreases the performance. The reason for this is that the language model conveys lower to no importance the visual information if it is located in front of the question. In this situation the language model basically generates blind answers. This highlights the importance of prompt structure."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,6.0,Conclusion,"In this paper, we propose a new perspective on medical VQA. We are using generative language models to generate answers in an open-ended manner, instead of performing a closed-set classification. Additionally, by using various parameterefficient fine-tuning strategies we are able to use language models with billions of parameters, even though dataset sizes in this domain are small. This leads to excellent performance compared to classification-based methods. In conclusion, our approach offers a more accurate and efficient solution for medical VQA."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,1.0,Introduction,"Neurodegenerative diseases like Alzheimer's disease (AD) are the main causes of cognitive impairment and earlier neuropathological alterations [7]. Cortical Y. Shi-This work is supported by the National Institute of Health (NIH) under grants R01EB022744, RF1AG077578, RF1AG056573, RF1AG064584, R21AG064776, P41EB015922, U19AG078109.thickness change is an essential feature that can quantify the potential brain atrophy in these diseases [9,12]. To analyze cortical thickness across different subjects, one essential step is to find the correspondence between surface points for thickness analysis [14,15]. This is traditionally achieved through 1-to-1 mapping or averaging feature in the region of interest (ROI). Mapping methods typically rely on surface registration techniques on a canonical space such as the unit sphere or high-dimensional embedding space [5,6,10].However,surface registration methods have limitations in accounting for the intrinsic variations in cortical topography among different individuals. For instance, a one-to-one mapping using surface registration could wrongly map a gyral region into a sulcal region due to distinguished local topography between subjects,as example shown in Fig. 1(B), thereby introducing noise into cortical thickness analysis since gyral and sulcal regions have distinct thickness profiles,as example shown in Fig. 1(A). Alternatively, ROI-based methodologies rely on the correspondence of anatomically segmented ROIs and establish comparisons through mean feature extraction. This approach disregards intra-ROI variability and the averaging process obscures detailed folding patterns. To mitigate the inherent challenges of surface mapping and mean ROI feature extraction, novel methodologies that consider the variability in folding patterns for identifying correspondences across distinct topographies are imperative. In this paper, we propose a novel personalized patch-based folding analysis method that finds correspondence through segmented patch similarity and personalized template set. Comparing to conventional methods, our method accounts for the gyral/sulcal mismatch problem by explicitly matching gyral and sulcal patches to their respective regions and selecting a personalized set of templates for each patch, as shown in Fig. 1(C), such that only comparable features are measured together to increase sensitivity in brain atrophy detection and reduce noise introduced by mismatching."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.0,Methods,"In this section, we will present the technical details of our method, which involve three main parts: surface segmentation, similarity metric and personalized templates. By integrating these 3 parts, we demonstrate the effectiveness of our method through normality assessment experiments on patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD) on data from Alzheimer Disease Neuroimaging Initiative(ADNI) [11]."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.1,Brain Surface Segmentation,"We employ FreeSurfer to extract 3D surface mesh from T1-weighted Volumetric MRI data. [2]. We extracted both pial and white surfaces, which have vertex correspondence by reconstruction method. The meshes are decimated to 50000 vertices for computational cost. The brain mesh is then subdivided into gyral and sulcal sub meshes following the methods in [13] using shape index and graph cut. The intricate folding pattern of the brain's cortical surface can make it difficult to accurately classify certain gyral structures that extend deeper from the outer surface. However, these structures can be better detected on the white surface, which has more prominent gyrus. Therefore, the segmentations from pial and white surfaces are combined through vertex correspondence. Specifically, given two array mask p and mask w as segmentation mask from pial and white surfaces, the final mask is mask p |mask w , where entry 1 is gyrus and 0 is sulcus. The resulting gyral/sulcal surfaces are 3D meshes with disconnected components and boundaries due to noise in reconstruction and the intrinsic geometry of the folding pattern. Therefore, filtering is applied to eliminate components that have less vertices than a threshold, which is empirically set at 50.The segmented patches are generated for gyral and sulcal sub meshes separately, which is shown in Fig. 2(B). Given a 3d mesh M (V, F ), V as set of vertices and F as set of faces, the boundary of M is defined as B(M ), the set of vertices that has at least one neighboring edge that is contained in only one triangular face. Next, the distance transform of vertex v ∈ V is defined as:The Geo denotes the geodesic distance between v and b. The distance transform computation is implemented using fast marching algorithm. [8] From the distance Each Voronoi cell defines a segmented patch. Since the surface is not perfectly smooth, the mesh is usually over segmented. To address this issue, we employ a region grow strategy to merge patches. Starting from one patch, we compute the geodesic distances between its center vertex and those of all neighboring patches. If the computed distance falls below a specified threshold, the two patches are merged into a single patch. In the event that no neighboring patch meets this criterion, we proceed to an unvisited patch. The iteration repeats until all patches are visited. The threshold is empirically set as twice the mean edge length on the mesh. Patches are labeled as gyral or sulcal based on the sub-mesh they're generated from."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.2,Patch Similarity Metric,"We employ shape similarity metrics to identify comparable patches. Histogrambased shape descriptors are used for topological similarity. Specifically, a histogram H is constructed for a function F defined on all vertices in a patch. The input values for histogram generation are normalized by the root mean square (RMS) of the function. The maximum bin value of the histogram is set as the maximum feature value, and the number of bins is fixed at 20 to ensure comparability between histograms. The values in each bin are normalized with respect to the total number of vertices in the patch, ensuring that the histogram values add up to 1. This normalization is done to avoid the scaling problem, as patches with similar topography can have different sizes. The distance transform, described in previous section, and shape index are used for constructing histogram, H dt and H si . Distance transform is used since the distance to boundary encodes information about the gyral/sulcal patches,such as width of the gyrus. For example, a crest gyral structure and a plateau one have different distance to boundary distribution. For topological comparison, chi square distance is computed as similarity. Given histograms H i and H j , the chi square distance is defined below:H i [k] denotes the value in the kth bin of H i . For any two patches,i and j, we can define a distance vectorThe final similarity score S(i, j) is defined as, where w is a weighting vector."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.3,Personalized Template Set,"To mitigate the impact of inter-subject variability in brain structure folding patterns, a personalized set of templates is chosen for each patch. This involves selecting a cohort of N cognitively normal (CN) subjects as the templates for comparison, while a separate group of M mild cognitive impairment (MCI) and M Alzheimer's disease (AD) subjects is selected for testing purposes.For all subjects in both the template and test groups, we first obtain the sets T temp and T test of segmented patches as described in the previous section. From the output of Freesurfer, we compute the vertex-wise spherical registered coordinates (stored in the sphere.reg file) and cortical thickness (stored in the .thickness file) as SR(v) and CT (v), respectively, where v denotes a vertex [4]. For a query patch P i , which is a set of vertices and has a gyral/sulcal label from the sub mesh they are generated from, we first compute its patch spherical registered coordinates P SR i = 1 |Pi| v∈Pi SR(v) and patch cortical thicknesswhere |P i | is the cardinality of P i . Then, for each patch P i ∈ T test , we compute the nearest 200 patches P 1...200 ⊂ T temp that has the same gyral/sulcal label as query patch in terms of ||P SR i -P SR j ||, where P j ∈ T test . This is to leverage the location information of the patch with respect to the whole hemisphere, as matched patches should be from the neighborhood region of the query patch and label restriction is to reduce mismatch of gyral/sulcal regions. From P 1...200 , we select the 50 most similar patches based on their similarity scores. Specifically, we compute the similarity score S(i, j) for patch P i and patch P j ∈ P 1...200 , and choose the top 50 patches in P 1...200 with the largest similarity scores to form a personalized template set T Pi for patch P i . We repeat this process for each patch in the test set.In the next step, we define the normality metric used to measure the normality of a patch. For each patch P i and its personalized template set T Pi , we first compute the mean (μ) and standard deviation (std) of the patch cortical thickness for all patches in T Pi . We then define the z-score of P i as Z(P|. This z-score is used as a normality measure because if P i is abnormal, it will have a larger z-score and vice versa. Finally, we use this z-score for each patch in the test subject set to conduct normality assessment experiments. To investigate the discriminative power of our patch-based personalized matching approach, we randomly selected 200 cognitive normal (CN) subjects as templates, 100 mild cognitive impairment (MCI) and 100 Alzheimer's disease (AD) subjects from the ADNI dataset to compare our method to Freesurfer's Spherical Registration. The experiments are limited to the left hemisphere for computing cost. Specifically, we used the sphere.reg file output from Freesurfer, which was decimated to 50000 vertices. For each query subject's surface, we computed the spherical registration of each vertex to a template surface by finding the closest vertex in the template's vertices based on Euclidean distance on the unit sphere space, as described by the sphere.reg file. We then computed a patch-based zscore for each patch P i using the thickness of P i and the spherically matched vertices from each template subject, which we denote as the sphere-based zscore. Alternatively, we matched patches as previously described in the Method section, and computed a patch-based z-score for each patch in the MCI and AD subject sets."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,3.1,Normality Assessment Experiments,"For each MCI and AD subject, the mean sphere-based z-score and mean patch-based z-score are computed by averaging the respective scores of patches for 1 subject. These mean z-scores represent how abnormal a subject is. The resulting distributions are shown in Fig. 3. The results show that our method yields a higher z-score for MCI and AD subjects compared to the sphere-based z-score, indicating that our model is more sensitive in detecting brain atrophy. As a qualitative analysis, the patch-based z-score maps and sphere-based z-score maps for selected MCI and AD subjects are also shown in Fig. 3. The patch-based z-score map shows more brain atrophy than the sphere-based z-score map, which further verifies our model's advantage in brain atrophy detection."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,3.2,"CN vs MCI, AD Prediction Experiment","In this experiment, we conduct CN vs MCI and CN vs AD prediction accuracy test. Similar to previous experiment, we choose 200 CN subjects as templates and another 100 CN subjects, 100 MCI subjects and 100 AD subjects for testing. Afterwards, sphere-based z-scores and patch-based z-scores are computed for all patches in CN, MCI and AD subjects from 200 template subjects. For training feature, we use the mean z-score of patches in each ROI in one subject, which is a length-34 vector for 34 ROIs from Freesurfer(.aparc.annot) [3]. Each subject has one feature vector generated from patch-based z-score and one from spherebased z-score. The CN, MCI and AD are used as labels. The feature vectors and labels are randomly split into train and test set at 8:2 ratio. The classifier we use is a generic support vector machine from sklearn package [1]. The test is conducted for CN vs AD and CN vs MCI binary classifications. We used 5-fold cross validation for evaluation on stability. The resulting test accuracy is shown in Table 1. The results show that our method performs better in accuracy than sphere-based method. "
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,4.0,Conclusion,"In this paper, we proposed a novel personalized patch-based method for brain atrophy detection by matching segmented patches based on gyral/sulcal label, location, shape similarity and constructing personalized template set for abnormal detection. Through normality assessment and MCI AD prediction experiments, the method is shown to be more effective at detecting brain atrophy."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,1.0,Introduction,"Coronavirus disease 2019 (COVID-19) has been a highly infectious viral disease, that can affect people of all ages, with a persistently high incidence after the outbreak in 2019 [1,2]. Early detection of the lung inflammatory reaction is crucial to initiate prompt treatment decisions, where the clinical assessment typically depends on two imaging techniques in conjunction, namely X-ray and CT scans [3,4]. Specifically, CT scans can provide a three-dimensional volumetric characterization of the patient's lung; while X-rays offer a two-dimensional landscape [5]. Recently, the use of multimodality data in COVID-19 diagnosis has received increasing interest as it can significantly improve prediction accuracy with complementary information [6].During the inference stage, modalities can be incomplete amongst some test samples, which is known as incomplete multimodal learning [7,8]. To address this, it is necessary to implement a strategy that reduces the impact of missing modalities during the inference stage while also offering clinicians the flexibility to use any possible combination of data. Various strategies have been proposed to address this issue, such as generating the missing data [9,10]. However, this approach requires training a generative model for each possible missing modality and a larger training set, making it computationally expensive. Therefore, more compact models have emerged that reduces the number of generators [7]. However, these models are prone to be biased when dealing with multiple modalities. Consequently, a modality-invariant embedding model that makes use of Transformers has been introduced [11]. Despite the excellent performance, all of the models discussed above have been applied to the datasets, where the modalities used were restricted to three-dimensional, and the structures are similar. However, in the case of COVID-19 pneumonia detection, the clinicians practically employ different dimensional data to interpret the results. One of the major problems during the analysis of medical data is that, sometimes, some data modalities are missing, e.g., the case reported negative, or the examination device was unavailable. [12] Therefore being able to have a model that is able to adapt to all possible conditions will be greatly beneficial for the detection of COVID-19 pneumonia or any other disease. To this end, a novel method to diagnose COVID-19 pneumonia which can take incomplete CT and X-Ray multimodal data is proposed. The main contributions in this model are three-fold: (1) We propose a dual feature fusion across different dimensionality data. Instead of sole pre-fusion or post-fusion, both are performed in the multi-modality prediction model. (2) We design a feature matrix dropout regularization method to improve the reliability and generalization of the model. (3) A feature correlation block is proposed between two of the given modalities to extract latent dependencies. This attention layer further improves the understanding of the patients' evolution when multi-modality images are acquired at different stages of the disease. Overview of the proposed model. This model is composed by three image modalities, on the first modality we use a convolutional-based feature extractor, followed by a Transformer encoder and a random features dropout. The other two modalities consist of an early fusion with the first modality features followed by a transformer encoder. Finally, all of the features have a features dropout layer followed by a Convolutional layer and a fully connected layer."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.1,Architecture Overview,"We propose a model that can detect the COVID-19 pneumonia status from incomplete multi-modalities of CT scans and X-ray images. Specifically, CT scans and X-ray images are first embedded using convolutional layers and then processed by Transformer layers to obtain the global feature correlations. Then, a novel feature fusion layer can simulate incomplete modalities in the latent space while learning to fuse the features. Finally, the predictions are made using a ResNet based classification model followed by a learnable MLP layer."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.2,Feature Fusion Layer,"The objective of this layer is to combine features from different modalities, which have varying dimensions. To achieve this, we need to reduce the threedimensional CT data to two-dimensional by convolutional layers before fusing them with two-dimensional X-ray data. We then reshape the data into smaller patches and apply a Transformer encoder to it, resulting in a two-dimensional matrix of the desired size. In this study, we investigate the data fusion at two stages, namely the early fusion and late fusion [13], where the data are fused twice in our model. Empirically, finding that dual fusion has some slight improvement compared to only early and late fusion as shown in Table 3."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.3,Feature Matrix Dropout Layer,"This layer helps regularize the model towards learning a modality agnostic representation. Specifically, during the feature fusion layer, random features are dropped from the feature matrix ∈ R X,Y [14]. In our design certain percentage of data in the form of patches are dropped, where a patch is defined as a subpart of the feature matrix where all values have been set to 0. To generate the patches, we start by creating a random matrix Mδ ∈ R X N , Y N with random values between 0 and 1, where N is the size of the dropout patches on the output images. We then round the values of Mδ using a threshold T , where all values below T are set to 0, and all values equal to or above T are set to 1. This gives us a binary matrix M δ that indicates which parts of the feature matrix should be dropped, i.e.where R T (•) is the round function that converts all values of M δ into 0 if they are under the threshold T and 1 otherwise. Finally the obtained matrix is interpolated or upsampled by a given scale N to match the size of the feature matriceswhere F D represents the final feature map after the patch dropout, F M is the initial feature map, I M (•) represents a nearest interpolation and (•) (•) represents an element-wise matrix multiplication. This gives us the final feature map F D , which has a similar structure to F M but with some parts of size N × N converted to 0."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.4,Transformer Layer,"Transformers helps finding the different dependencies between the different modalities making use of their attention-based nature. Therefore, in this paper we will make use of the benefits that the transformers offers by implementing a ViT based transformer layer [15]. In this case we will use the transformer layer as a feature extractor and find the dependencies between the different embedded features. To extract the features, we will first split the image into patches of a fixed size, to afterwards be processed by an embedding layer, in this case the embedding layer will be a convolutional layer. Then each one of the embedded feature will pass through a different Self Attention Layer (SA) formulated aswhere σ(•) represents the softmax function, d represents the size of each one of the heads, and W 0 denotes a value embedding. Meanwhile, Q, K, V ∈ R X,d represent the Query, key and embedding respectively. To constitute the ViT the values of each of the SA are concatenated forming the Multi-Headed Self Attention Layer (MSA) to afterward be normalized by a layer normalization. Finally, a Multi-Layer Perceptron (MLP) is applied to give non-linearity to the data, obtainingwhere A k-1 is the previous transformer layer, LN(•) is a Layer Normalization. In the vanilla ViT a final MLP is introduced to obtain the probabilities of each class however, in this paper the transformer layer is only used for feature extraction, therefore, the final MLP is deleted."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.5,Dual X-Ray Attention,"The Dual X-Ray attention block main idea is to find the different dependencies between the two input X-Ray images. Transformers have showed great results when looking for dependencies [11,15]. Therefore, this paper introduced a new attention layer that extracts the dependencies between two X-Ray images. The dependencies are extracted using the Transformer mentioned above Layer having as input both X-Ray images in the multimodality. This layer can also be seen as a fusion layer between two of the input modalities."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.1,Dataset,"We leverage images from the BIMCV-COVID19 dataset [16], a public dataset with thousands of different positive and negative cases, for performance evaluation. This dataset is composed of 1200 unique cases from a combination of 1 CT Scan, 1 CT Scan and 1 X-Ray, 1 CT Scan and 2 X-Ray, 1 X-Ray or 2 X-Ray. One patient may have more than one image modality of CT or X-Ray. Regarding the image size of the dataset, the dimension of the CT scans is non-fixed per image, with the average image size around 500 × 500 × 400. To facilitate the training on GPUs, the images perform dimension reduction with factor 2 and resulted in a final image size of 250 × 250 × 200. The dataset is composed of approximately 2900 cases. From which around 1700 are negative, and 1200 are positive. The distribution is unbalanced because we wanted to extract as much data as possible with a balanced number of miss-modalities."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.2,Implementation Details,"This framework is implemented with Pytorch 1.13.0 using an NVIDIA A10 GPU with 24 GB of VRAM. The input size is 250 × 250 × 200 for the CT scan and 2048 × 2048 with a batch size of 4 for the X-Ray. For the proposed model, we applied Adam optimizer with an cosine annealing scheduler of an initial learning rate 1e-5, and 100 epochs for training. The data partition for training, validation, and testing is 80%, 10%, and 10% on patient level."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.3,Results,"We use the following metrics to evaluate the performance of the binary classification model: the AUC score, Recall and Precision. To obtain the values on the Table 1, we made a split form the original dataset without miss-modalities. This table shows the difference between the possible combinations of models we can build to interpret the data. Taking in account the AUC, the worst model is the one using the CT as its only input with only a 65.74% in the AUC score, followed by the model with two X-Ray images as its input, obtaining a 67.98%. The result obtained in the Dual X-Ray multimodality performs around 2% worse than just having one X-Ray as an input. Finally, when having all the modalities performs 3% better in the AUC score than when only having one CT and one X-Ray which has around 71.26% in the AUC metric. A comparison is made between our model and others with different possible variations, shown in the Table 2. The first model is used as a comparison is a fully convolutional model where all of its transformer components are converted into convolutions. This variant shows a significant reduction in all of its parameters with a sharp decrease in the performance of 70.18%, 59.9% and 68.93% in the  "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.4,Ablation Study,"We show the classification performance on Table 3, of the different model components that introduced in the Fig. 1. We make use of the original dataset with missing modalities. From the Table 3, the addition of the Fusion Layer gives an slight increase of 0.19% in the AUC-score metric compared with the baseline. Meanwhile, we see an observable increase in the recall metric of 2.93%. The addition of the Dual X-Ray layer further increases the performance of the model increasing of 1.8% and 3.43% in the AUC-score and Precision respectively, yet a slight reduction in the recall metric by around 1.35%. Finally, the Regularization layer is topped up to further increases the AUC by a 2.66% and recall by a 13.90%, yet lower Precision score of 5.44%. Thus, compared to the baseline, the final model shows an increase of 4.65% in the AUC metric, a boost of 15.58% in the recall score and only a 2.47% decrease in the precision metric.The saliency maps are visualized to pinpoint the diagnostic areas in a CT or X-ray image. Clinically, the saliency maps are helpful to assist radiologists. In the Fig. 2,(a) shows an example of a COVID-19 positive slice extracted from a complete CT scan. Its saliency map is situated next to the image, where a big opacity is found in the left patient's lung. The top-right figure gives an example of COVID-19 positive X-Ray which its correspondent saliency map, which mainly focus on the bottom contour of the lung. Moving to the bottom-left figure we can see a COVID-19 negative CT scan using a different angle compared to the first introduced figure. In this case, similar to what has been seen in the positive case, some opacities have been found however, in this case, the image is negative. Finally, the bottom-right figure is a negative X-Ray case, in this case, similar  to what has been found in the positive X-Ray figure the model mainly looks for the contours of the lungs. Therefore, through this images, a difference can be seen between the extracted features using the CT scans and the ones extracted using the X-Ray images. Finally, in the Fig. 3  "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,4.0,Conclusion,"In this paper, we propose a novel multi-modality framework for COVID-19 pneumonia image diagnosis. A Dual fusion layer is introduced to help establish the dependencies between the different input modalities, as well as to take in different dimensionality inputs. The proposed Dual X-Ray attention layer makes it possible to effectively extract dependencies from the two X-Ray images focusing on the salient features between multi-modality images, whereas irrelevant features are ignored. The feature deletion layer helps to regularize the model dropping random features and improving the generalization of the model. Consequently, we provide the possibility to use one modality of CT or X-Ray for COVID-19 pneumonia diagnosis. Moreover, this model has the potential to be applied to other chest abnormalities in clinical practice."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,1.0,Introduction,"Assessing vascular lumen and topology is among the most critical tasks for timely diagnosis of acute cerebrovascular disease, including the detection of vessel occlusions (e.g., by left-right hemisphere comparisons) and the assessment of redundant blood flow paths via the communicating arteries [2] in stroke diagnosis. Identifying impairments such as thrombi for stroke analysis in the nested cerebral artery system from a Computed Tomography Angiography (CTA) scan can be inefficient due to the need to interact with the visualization to properly review all relevant vessels. Instead of manually tracking individual vessels across multiple slices, it is also possible to project their complete lumen into a single image plane, called unfolding. Yet, analyzing the global appearance of all cerebral vessels at once, primarily the main arteries forming and surrounding a ring structure called Circle of Willis (CoW; exists in many common norm variants) can be equally important.However, this circular structure cannot be properly unfolded with the common Curved Planar Reformation (CPR) technique [3], its untangled extension or through conformal mappings [15] due to imperfect circle symmetry leading to different lengths when unfolding the opposing sides. Hence, unfolding is done individually per vessel [12] or by simply showing the configuration in bullet-maps [9]. For many other anatomical structures exist already flattening techniques [4], e.g., using ray-casting for rip unfolding [8]. Relevant unfolding techniques in the brain include aneurysm maps [10] or cortex flattening using mesh deformation [1].In computer graphics, disk-like mesh parameterization [6] or angle-preserving conformal maps [7] can be used to bring objects in a planar representation. For the involved task of mesh deformation, algorithms such as the As-Rigid-As-Possible (ARAP) method [13] aim to preserve shape during the process, which was already adapted for medical purposes such as pelvis [5] or heart (vessel) unfolding. However, as shown in the vessel tree in Fig. 1, cerebral vessels often bifurcate in perpendicular directions or run in parallel, eliminating the possibility to use simple geometric primitives for the whole CoW. Consequently, comprehensive, contiguous unfoldings of the cerebrovascular system have not been demonstrated before.This work aims to generate a complete textbook-style vessel map of the cerebral vasculature along the CoW to generate a standardized overview image. Building on the ARAP algorithm, we propose CeVasMap (Cerebral Vasculature Mapping): a method to unfold circular structures which can be flexibly extended with peripheral vessels, either by including them in the unfolding directly or attaching them individually. This results in locally restricted distortions, retain-ing curvature information and keeping most of the image distortion-free. Given labeled centerlines, we create a smooth initial mesh with optimal viewing direction dependent on the vessels' principal components and deform it as rigidly as possible to jointly display all vessels of interest. We provide a comprehensive vessel-level evaluation by calculating and visualizing the distortions resulting from the underlying 2D-3D vector field. "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.1,Data,"We use a data set consisting of 30 CTA scans (Siemens Somatom Definition AS+) from stroke patients (63.3% males, 74, 5 ± 12, 5 years, 33.3% MCA stroke) with an average voxel spacing of 0.634 mm (in-plane) and 0.57 mm (axial). The brain vasculature is segmented and labeled [11,14]. An example is displayed in the CoW scheme in Fig. 2."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.2,Rationale,"The goal of this work is to generate a single 2D image which contains all vessels of interest jointly with the surrounding parenchyma. Hence, the task at hand is to find an unfolding transformation Φ( u) = x : R 2 → R 3 mapping from a 2D position u in the unfolded target image to its 3D CTA volume location x. The corresponding coordinate systems are illustrated in Fig. 1.Clinically useful images should be merged at the bifurcations to allow consistent path tracing. Additionally, anatomical properties such as vessel curvature should be preserved whereas strong distortions are to be avoided. The main concept of the proposed method is the generation of a joint mapping for the CoW. Further attached vessels can then be merged to it to form a complete overview at the cost of a higher distortion, see Sect. 2.3. For configurations that lead to strong distortions, outer vessels are unfolded individually and attached to the main component. Their arrangement is inspired by the common textbook CoW schematic for better orientation, see Fig. 2. Unfolding of individual as well as combined vessels is described in Sect. 2.4, merging and image assembly in Sect. 2.5."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.3,Measuring Distortions,"An image resulting from a transformation Φ lacks a consistent pixel spacing, the specified one is simply an average from the covered 3D distances, hence pixel distances deviating from that average are distorted. We compute distortion metrics on the vector field Φ to guide the decision process during the merging and for the evaluation. Since conventional metrics to find sources and sinks such as the Jacobian determinant can only be calculated for equidimensional Φs, we compute a scalar metric d per pixel u = (u, v) by deriving the transformation w.r.t. both image directions ∂Φ( u)  ∂ u ∈ R 3×2 and applying the Frobenius norm,When sampling a 2D image of size N × M with isotropic sampling μ, a pixel gradient of 1 in one image direction implies a sampling in 3D with μ, meaning no distortion. To normalize distortion-free values of"
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.4,ARAP Vessel Unfolding,"CPR unfolding can lead to displeasing results, e.g., at highly curved segments perpendicular to image read-out direction, impairing the quality of the displayed lumen and parenchyma in the whole image row, see bottom right in Fig. 4. Instead of sampling read-out points p ∈ R 3 line by line starting from the centerline as done by Kanitsar et al. [3], one could also assume a continuous (triangular) mesh surface S with p i being the vertex position i and downstream image read-out points. One would then find and deform an initial mesh representation, fitting it to the vessels of interest. When deforming such a mesh into S with p i , one has direct control of the rigidity between neighboring vertices by measuring the local rigidity energy [13]:with N (i) being the 1-step neighborhood of i. By minimizing the norm of the rigid error on the right-hand side, one can approximate a rigid transformation, since actual rigidness is infeasible. For the derivation of the rotation R i and the weights w ij and w i , we refer to Sorkine et al. [13].On one hand, we lack a fully defined object surface due to our sparse vessel structure. On the other hand, staying close to a meshed plane would mimic multiplanar reformation images and help with the orientation. Since this structure would lead to strong distortion due to its simplicity compared to the centerlines, we define our read-out mesh in a two-step approach. First, we use the principal component vectors stored column-wise in A ∈ R 3×3 , using column a 3 as the optimal viewing direction by projecting the points c j from the set of centerline points j ∈ C along the two maximum principal components to acquire two 1D distributions k 1,2 , see Fig. 2. To create a smooth surface independent of point density in certain areas, a B-spline b k is fitted for each of the distributions with the number of knots dependent on the vessel length. Sampling from those functions creates an initial smooth shape S init withas vertex positions where u and v are sampled uniformly fromand c is the centerline mean position. To avoid escalating border regions, the last value of qis set to 0 outside of the centerline bounding box. Next, S init is deformed to contain all vessel points following the ARAP minimization in Eq. 2 by restraining a subset of p i to c j . Volume intensities can be read out at the vertex positions of the resulting mesh S, forming the target vector field Φ CeVasMap ( u) = p beginning from the unfolded image."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.5,Merging and Image Assembly,"In principle, the described approach can be applied to all vessels of interest at once, introducing high distortion at nearly perpendicular bifurcations. Due to a varying amount of contrast agent; scan, segmentation or labeling quality; or occlusions in stroke, the vessels can differ greatly in length. Long, curved structures influence the unfolding more severely since S init has to adapt to more points c j , increasing the initial fitting error. In such cases, individual vessels are unfolded separately to maintain sufficient image quality and included in the cerebral vessel map at the position and rotation angle according to the textbook schematic, see Fig. 2. To obtain a standardized image, the CoW (AComm, A1, PComm, P1, ICA C7) is always unfolded jointly to form the center of the image. Afterwards, vessels can be merged based on their importance to the task at hand and by setting an upper threshold D < β. An exemplary strategy in stroke scenarios would encourage a merging of the often-affected MCAs, together with the anterior part, followed by the PCAs."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.6,Evaluation,"The distortion by the proposed transformation and the baseline is evaluated quantitatively and qualitatively using the metrics from 2.3. The baseline method is the CPR transformation Φ CPR ( u) following Kanitsar et al. [3], linearly sampling image rows starting from equidistant points on the centerline. We present the results of both the best and worst CPR viewing. All vessels are unfolded individually with approx. the same isotropic spacing (0.256 mm) and zoom level. Metrics are computed only for pixels that are sampled from a conservatively chosen radius of 10 mm around the centerline in the volume, since larger images favor our approach due to its distortion-free property in areas distant to the vessel.The pixel-wise metric d uv is used to visually inspect the results. Quantitatively, distortion is assessed showing the mean-like metric D as a distribution over all 30 patients. Additionally, the median of |d uv | per vessel unfolding averaged over all patients is shown to also investigate the maximal distortion of the lesser affected half of the pixels. To assess the expected distortion increase caused by merging, we split the cerebral vessels into two groups: For the CoW vessel segments (merged by default), D and median metric are calculated over the image patches of each vessel, both for individual unfoldings and extracted from the merging. Similarly, the D-distribution and frequency of successfully merging outer vessel pairs to the unfolded circle structure is reported when setting an upper distortion threshold of D < 25% for the vessel of interest. To avoid that simultaneously merged vessel pairs influence each other or the circle, we compute S init only from the circle and then merge vessel pairs using ARAP. This also eliminates the need to evaluate distortions for all possible vessel combinations.  "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,3.0,Results,"Quantitative metrics per vessel (pair) are presented in Fig. 3 and a qualitative example of a cerebral vessel map with distortion heatmaps is shown in Fig. 4. Even using the narrow 10 mm distance threshold, CeVasMap consistently has lower median values than the best CPR angles, except for the short ICA C7 and the VA. Within the CoW, all median values are below 10 µm/mm distortion, the outer vessels are all below 50 µm/mm, except for VA. However, the distributions generally show a slightly lower mean for CPR (best), together with a lower average standard deviation for all vessels. Especially for long vessels (MCA, VA, P2, ICA), we observe higher maximum values in the D-distribution and standard deviation. Qualitative analysis (cf. the example in Fig. 4) confirms the theoretical properties of the two compared approaches. While our approach has locally restricted distortions around the centerline (ARAP target points) with a higher amplitude, CPR affects the distortion along an image row, potentially resulting in inflated vessels due to oversampling inside the lumen at curvature points, see Fig. 4 on the bottom right. For CeVasMap, high distortion in the VA occurs a the end of the labeled vessel, caused by absent neighboring target points. Slight distortions for our approach outside of the vessel (orange shade in ICA r) are caused by the sampling of S init . For more qualitative examples, see Suppl. Fig. 1.When merging the CoW (Fig. 3a), the median distortions (65 µm/mm for the complete circle) naturally increase but are still almost on par with (P1, A1) or even outperform (AComm, PComm) the individually unfolded worst CPR angle, except for the ICA C7 segment which is often oriented nearly perpendicular to the remaining circle segments. Using our threshold for the outer vessels, it is possible to merge the MCA in 88.3% of the cases. Vessels perpendicular to the CoW plane could however only be merged in 10.2% (ICA) or 43.3% (BA). "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,4.0,Discussion and Conclusion,"We present CeVasMap, a flexible method to generate textbook-style vessel maps of the main cerebral arteries, including an unfolding technique capable of displaying circular structures infeasible for the widely used CPR approach. To unfold individual vessels or vessel groups, a read-out mesh is generated by fitting splines to centerline projections along the principal components. This initial mesh is deformed using the ARAP algorithm to display the full vessel lumen. In our approach, we suggest jointly mapping the inner circular structure of the CoW by default and extending it with outer vessels depending on the task at hand and the introduced distortion. For this purpose, a gradient-based distortion metric is calculated on the 2D-3D unfolding vector field, which also facilitates quantitative and qualitative assessment of the mapping quality.The method results in higher standard deviations and slightly higher mean values of the proposed gradient-based distortion metric D compared to the best possible CPR angle. However, CeVasMap generally achieves lower median values (≤10 µm/mm for the CoW vessels, and mostly ≤50 µm/mm for the longer outer segments), meaning larger parts of the image are distortion-free. Even when merging the complete CoW (average distortion median of 65 µm/mm), our method can still compete with the individual CPR unfolding using less favorable angles. When merged with the CoW, distortion for outer vessels, especially those nearly perpendicular to the CoW plane, is quite high, and only the MCAs, PCAs and A2 arteries achieve satisfying results due to their orientation. For instance, the ICA could only be merged in 10.2% while the MCA has a 88.3% success rate with our proposed threshold. To assess only distortions close to the structures of interest, i.e., the vessels, we evaluated our metrics within a narrow corridor around them. When generating full cerebrovascular unfoldings, one would rather unfold more parenchyma for a hole-free image (cf. Fig. 4), leading to a significant improvement of our metrics.Rotating the unfolded views as often done in CPR is in theory feasible with our approach-by rotating the principal components A-but is not expected to give pleasing results as ARAP target points are rotated out-of-plane. Insufficient unfolding length or asymmetric manifestations of vessel pairs can occur due to segmentation or labeling inconsistencies. Nevertheless, our method is more robust against segmentation or labeling errors causing strongly curved, incorrect or fistula-like pathways. In contrast to CPR, our method is robust against varying centerline point distances and inconsistent centerline point ordering.The flexible nature of our approach can also be used to focus on the arteries of interest by merging them selectively and keeping the remaining vessels separate but arranged intuitively inspired by textbook presentations, effectively reducing overall distortion. Such a selective merging could also be done interactively as, compared to higher computation times for segmentation and labeling, most unfolding steps are parallelizable and can be computed in few seconds.The clinical applications of a cerebrovascular overview map are manifold. The configuration of the CoW and surrounding major vessels is visualized, giving insight into the collateral blood supply or supporting contralateral comparisons for time-critical stroke detection while being able to view the vessel lumen and its pathologies at a glance. To evaluate challenging real-world scenarios, this work is specifically tested using stroke data as it one of the main reasons for topological impairments of the CoW. This unique representation (compared to volume renderings or slice images) of the cerebrovascular system can be useful for diagnostic reports, comparing patients, disease tracking or to simply to mark locations of possible findings. The maps are also helpful for navigation within the volume as the transformation allows us to readily display multiplanar reformations of the original volume centered at 3D positions corresponding to (manually selected) positions in the unfolded overview image (""picking""). Finally, this condensed lower-dimensional representation could also be beneficial for training downstream deep learning models more efficiently."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_71.
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"Gastric cancer (GC) is the third leading cause of cancer-related deaths worldwide [19]. The five-year survival rate for GC is approximately 33% [16], which is mainly attributed to patients being diagnosed with advanced-stage disease harboring unresectable tumors. This is often due to the latent and nonspecific signs and symptoms of early-stage GC. However, patients with early-stage disease have a substantially higher five-year survival rate of around 72% [16]. Therefore, early detection of resectable/curable gastric cancers, preferably before the onset of symptoms, presents a promising strategy to reduce associated mortality. Unfortunately, current guidelines do not recommend any screening tests for GC [22]. While several screening tools have been developed, such as Barium-meal gastric photofluorography [5], upper endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to apply to the general population due to their invasiveness, moderate sensitivity/specificity, high cost, or side effects. Therefore, there is an urgent need for novel screening methods that are noninvasive, highly accurate, low-cost, and ready to distribute.Non-contrast CT is a commonly used imaging protocol for various clinical purposes. It is a non-invasive, relatively low-cost, and safe procedure that exposes patients to less radiation dose and does not require the use of contrast injection that may cause serious side effects (compared to multi-phase contrastenhanced CT). With recent advances in AI, opportunistic screening of diseases using non-contrast CT during routine clinical care performed for other clinical indications, such as lung and colorectal cancer screening, presents an attractive approach to early detect treatable and preventable diseases [17]. However, whether early detection of gastric cancer using non-contrast CT scans is possible remains unknown. This is because early-stage gastric tumors may only invade the mucosal and muscularis layers, which are difficult to identify without the help of stomach preparation and contrast injection. Additionally, the poor contrast between the tumor and normal stomach wall/tissues on non-contrast CT scans and various shape alterations of gastric cancer, further exacerbates this challenge.In this paper, we propose a novel approach for detecting gastric cancer on non-contrast CT scans. Unlike the conventional ""segmentation for classification"" methods that directly employ segmentation networks, we developed a clusterinduced Mask Transformer that performs segmentation and global classification simultaneously. Given the high variability in shape and texture of gastric cancer, we encode these features into learnable clusters and utilize cluster analysis during inference. By incorporating self-attention layers for global context modeling, our model can leverage both local and global cues for accurate detection. In our experiments, the proposed approach outperforms nnUNet [8] by 0.032 in AUC, 5.0% in sensitivity, and 4.1% in specificity. These results demonstrate the potential of our approach for opportunistic screening of gastric cancer in asymptomatic patients using non-contrast CT scans."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,"Automated Cancer Detection. Researchers have explored automated tumor detection techniques on endoscopic [13,14], pathological images [20], and the prediction of cancer prognosis [12]. Recent developments in deep learning have significantly improved the segmentation of gastric tumors [11], which is critical for their detection. However, our framework is specifically designed for noncontrast CT scans, which is beneficial for asymptomatic patients. While previous studies have successfully detected pancreatic [25] and esophageal [26] cancers on non-contrast CT, identifying gastric cancer presents a unique challenge due to its subtle texture changes, various shape alterations, and complex background, e.g., irregular gastric wall; liquid and contents in the stomach."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Mask Transformers.,"Recent studies have used Transformers for natural and medical image segmentation [21]. Mask Transformers [3,24,29] further enhance CNN-based backbones by incorporating stand-alone Transformer blocks, treating object queries in DETR [1] as memory-encoded queries for segmentation. CMT-Deeplab [27] and KMaX-Deeplab [28] have recently proposed interpreting the queries as clustering centers and adding regulatory constraints for learning the cluster representations of the queries. Mask Transformers are locally sensitive to image textures for precise segmentation and globally aware of organtumor morphology for recognition. Their cluster representations demonstrate a remarkable balance of intra-cluster similarity and inter-class discrepancy. Therefore, Mask Transformers are an ideal choice for an end-to-end joint segmentation and classification system for detecting gastric cancer."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,3.0,Methods,"Problem Formulation. Given a non-contrast CT scan, cancer screening is a binary classification with two classes as L = {0, 1}, where 0 stands for""normal"" and 1 for""GC"" (gastric cancer). The entire dataset is denoted by, where X i is the i-th non-contrast CT volume, with Y i being the voxel-wise label map of the same size as X i and K channels. Here, K = 3 represents the background, stomach, and GC tumor. P i ∈ L is the class label of the image, confirmed by pathology, radiology, or clinical records. In the testing phase, only X i is given, and our goal is to predict a class label for X i ."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"To address difficulties with tumor annotation on non-contrast CTs, the radiologists start by annotating a voxel-wise tumor mask on the contrast-enhanced CT, referring to clinical and endoscopy reports as needed. DEEDs [6] registration is then performed to align the contrast-enhanced CT with the non-contrast CT and the resulting deformation field is applied to the annotated mask. Any misaligned ones are revised manually. In this manner (Fig. 1d), a relatively coarse yet highly reliable tumor mask can be obtained for the non-contrast CT image. Cluster-Induced Classification with Mask Transformers. Segmentation for classification is widely used in tumor detection [25,26,32]. We first train a UNet [8,18] to segment the stomach and tumor regions using the masks from the previous step. This UNet considers local information and can only extract stomach ROIs well during testing. However, local textures are inadequate for accurate gastric tumor detection on non-contrast CTs, so we need a network of both local sensitivity to textures and global awareness of the organ-tumor morphology. Mask transformer [3,24] is a well-suited approach to boost the CNN backbone with stand-alone transformer blocks. Recent studies [27,28] suggest interpreting object queries as cluster centers, which naturally exhibit intra-cluster similarity and inter-class discrepancy. Inspired by this, we further develop a deep classification model on top of learnable cluster representations.Specifically, given image X ∈ R H×W ×D , annotation Y ∈ R K×HW D , and patient class P ∈ L, our model consists of three components: 1) a CNN backbone to extract its pixel-wise features F ∈ R C×HW D (Fig. 1a), 2) a transformer module (Fig. 1b), and 3) a multi-task cluster inference module (Fig. 1c). The transformer module gradually updates a set of randomly initialized object queries C ∈ R N ×C , i.e., to meaningful mask embedding vectors through cross-attention between object queries and multi-scale pixel features,where c and p stand for query and pixel features, Q c , K p , V p represent linearly projected query, key, and value. We adopt cluster-wise argmax from KMax-DeepLab [28] to substitute spatial-wise softmax in the original settings.We further interpret the object queries as cluster centers from a cluster analysis perspective. All the pixels in the convolutional feature map are assigned to different clusters based on these centers. The assignment of clusters (a.k.a. mask prediction) M ∈ R N ×HW D is computed as the cluster-wise softmax function over the matrix product between the cluster centers C and pixel-wise feature matrix F, i.e.,The final segmentation logits Z ∈ R K×HW D are obtained by aggregating the pixels within each cluster according to cluster-wise classification, which treats pixels within a cluster as a whole. The aggregation of pixels is achieved by Z = C K M, where the cluster-wise classification C K is represented by an MLP that projects the cluster centers C to K channels (the number of segmentation classes).The learned cluster centers possess high-level semantics with both intercluster discrepancy and intra-cluster similarity for effective classification. Rather than directly classifying the final feature map, we first generate the clusterpath feature vector by taking the channel-wise average of cluster centers C =Additionally, to enhance the consistency between the segmentation and classification outputs, we apply global max pooling to cluster assignments R to obtain the pixel-path feature vector R ∈ R N . This establishes a direct connection between classification features and segmentation predictions. Finally, we concatenate these two feature vectors to obtain the final feature and project it onto the classification prediction P ∈ R 2 via a two-layer MLP.The overall training objective is formulated as,where the segmentation loss L seg (•, •) is a combination of Dice and cross entropy losses, and the classification loss L cls (•, •) is cross entropy loss."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"Dataset and Ground Truth. Our study analyzed a dataset of CT scans collected from Guangdong Province People's Hospital between years 2018 and 2020, with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. We used the latest patients in the second half of 2020 as a hold-out test set, resulting in a training set of 687 gastric cancer and 1,204 normal cases, and a test set of 100 gastric cancer and 148 normal cases. We randomly selected 20% of the training data as an internal validation set. To further evaluate specificity in a larger population, we collected an external test set of 903 normal cases from Shengjing Hospital. Cancer cases were confirmed through endoscopy (and pathology) reports, while normal cases were confirmed by radiology reports and a two-year follow-up. All patients underwent multi-phase CTs with a median spacing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. Tumors were annotated on the venous phase by an experienced radiologist specializing in gastric imaging using CTLabeler [23], while the stomach was automatically annotated using a self-learning model [31].Implementation Details. We resampled each CT volume to the median spacing while normalizing it to have zero mean and unit variance. During training, we cropped the 3D bounding box of the stomach and added a small margin of (32,32,4). We used nnUNet [8] as the backbone, with four transformer decoders, each taking pixel features with output strides of 32, 16, 8, and 4. We set the number of object queries N to 8, with each having a dimension of 128, and included an eight-head self-attention layer in each block. The patch size used during training and inference is (192, 224, 40) voxel. We followed [8] to augment data. We trained the model with RAdam using a learning rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs, with a frozen backbone of the pretrained nnUNet [8] for the first 50 epochs. To enhance performance, we added deep supervision by aligning the cross-attention map with the final segmentation map, as per KMax-Deeplab [27]. The hidden layer dimension in the two-layer MLP is 128. We also trained a standard UNet [8,18] to localize the stomach region in the entire image in the testing phase."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Evaluation Metrics and Reader Study.,"For the binary classification, model performance is evaluated using area under ROC curve (AUC), sensitivity (Sens.), and specificity (Spec.). And successful localization of the tumors is considered when the overlap between the segmentation mask generated by the model and the ground truth is greater than 0.01, measured by the Dice score. A reader study was conducted with two experienced radiologists, one from Guangdong Province People's Hospital with 20 years of experience and the other from The First Affiliated Hospital of Zhejiang University with 9 years of experience in gastric imaging. The readers were given 248 non-contrast CT scans from the test set and asked to provide a binary decision for each scan, indicating whether the scan showed gastric cancer. No patient information or records were provided to the readers. Readers were informed that the dataset might contain more tumor cases than the standard prevalence observed in screening, but the proportion of case types was not disclosed. Readers used ITK-SNAP [30] to interpret the CT scans without any time constraints.  1 presents a comparative analysis of our proposed method with three baselines. The first two approaches belong to ""Segmentation for classification"" (S4C) [26,32], using nnUNet [8] and TransUNet [2]. A case is classified as positive if the segmented tumor volume exceeds a threshold that maximizes the sum of sensitivity and specificity on the validation set. The third baseline (denoted as ""nnUNet-Joint"") integrates a CNN classification head into UNet [8] and trained end-to-end. We obtain the 95% confidence interval of AUC, sensitivity, and specificity values from 1000 bootstrap replicas of the test dataset for statistical analysis. For statistical significance, we conduct a DeLong test between two AUCs (ours vs. compared method) and a permutation test between two sensitivities or specificities (ours vs. compared method and radiologists)."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.2,Results,"Our method Outperforms Baselines. Our method outperforms three baselines (Table 1) in all metrics, particularly in AUC and sensitivity. The advantage of our approach is that it captures the local and global information simultaneously in virtue of the unique architecture of mask transformer. It also extracts high-level semantics from cluster representations, making it suitable for classification and facilitating a holistic decision-making process. Moreover, our method reaches a considerable specificity of 97.7% on the external test set, which is crucial in opportunistic screening for less false positives and unnecessary human workload."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,AI Models Surpass Experienced Radiologists on Non-contrast CT Scans.,"As shown in Fig. 2a, our AI model's ROC curve is superior to that of two experienced radiologists. The model achieves a sensitivity of 85.0% in detecting gastric cancer, which significantly exceeds the mean performance of doctors (73.5%) and also surpasses the best performing doctor (R2: 75.0%), while maintaining a high specificity. A visual example is presented in Fig. 2b. This early-stage cancer (T1) is miss-detected by both radiologists, whereas classified and localized precisely by our model."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Subgroup Analysis.,"In Table 2, we report the performance of patient-level detection and tumor-level localization stratified by tumor (T) stage. We compare our model's performance with that of both radiologists. The results show that our model performs better in detecting early stage tumors (T1, T2) and provides more precise tumor localization. Specifically, our model detects 60.0% (6/10) T1 cancers, and 77.8% (7/9) T2 cancers, surpassing the best performing expert (50% T1, 55.6% T2). Meanwhile, our model maintains a reliable detection rate and credible localization accuracy for T3 and T4 tumors (2 of 34 T3 tumors missed).Comparison with Established Screening Tools. Our method surpasses or performs on par with established screening tools [4,7,10] in terms of sensitivity for gastric cancer detection at a similar specificity level with a relatively large testing patient size (n = 1151 by integrating the internal and external test sets), as shown in Table 3. This finding sheds light on the opportunity to employ automated AI systems to screen gastric cancer using non-contrast CT scans."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,5.0,Conclusion,"We propose a novel Cluster-induced Mask Transformer for gastric cancer detection on non-contrast CT scans. Our approach outperforms strong baselines and experienced radiologists. Compared to other screening methods, such as blood tests, endoscopy, upper-gastrointestinal series, and ME-NBI, our approach is non-invasive, cost-effective, safe, and more accurate for detecting early-stage tumors. The robust performance of our approach demonstrates its potential for opportunistic screening of gastric cancer in the general population."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Table 3 .,†Compared Baselines. Table
Self-supervised Polyp Re-identification in Colonoscopy,1.0,Introduction,"Optical colonoscopy is the standard of care screening procedure for the prevention and early detection of colorectal cancer (CRC). The primary goal of a screening colonoscopy is polyp detection and preventive removal. It is well known that many polyps go unnoticed during colonoscopy [22]. To deal with this problem, computer-aided polyp detector (CADe) was introduced [13][14][15][16] and recently became commercially available [3]. The success of polyp detector sparkled the development of new CAD tools for colonoscopy, including polyp characterization (CADx, or optical biopsy), extraction of various quality metrics, and automatic reporting. Many of those new CAD applications require aggregation of all available data on a polyp into a single unified entity. For example, one would expect higher accuracy for CADx when it analyzes all frames where a polyp is observed. Clustering polyp detections into polyp entities is a prerequisite for computing such quality metrics as Polyp Detection Rate (PDR) and Polyps Per Colonoscopy (PPC), and for listing detected polyps in a report.One may notice that the described task generally falls into the category of the well known multiple object tracking (MOT) problem [26,27]. While this is true, there are a few factors specific to the colonoscopy setup: (a) Due to abrupt endoscope camera movements, targets (polyps) often go out of the field of view, (b) Because of heavy imaging conditions (liquids, debris, low illumination) and non-rigid nature of the colon, targets may change their appearance significantly, (c) Many targets (polyps) are quite similar in appearance. Those factors limit the scope and accuracy of existing frame-by-frame spatio-temporal tracking methods, which typically yield an over-fragmented result. That is, the track is often lost, resulting in relatively short tracklets (temporal sequences of same target detections in multiple near-consecutive frames), see Supplementary Fig. 1.A recently published method [2] addresses this limitation by combining spatial target proximity and visual similarity to match a polyp detected in the current frame to ""active"" polyp tracklets dynamically maintained by the system. The tracklets are built incrementally, by adding a single frame detection to the matched tracklet, one-by-one. However, this approach limits itself to use of closein-time consistent detections, and cannot handle the frequent cases where polyp gets out of the field of view and long range association is required.In this work we propose an alternative approach that allows polyp detections grouping over an extended period of time (up to 10 min), relaxing the spatiotemporal proximity limitation. It involves two steps: (I) a short-term multi-object tracking, which forms initial, relatively short tracklets, followed by (II) a longerterm tracklets grouping by appearance-based polyp re-identification (ReID). As the first step can be done by any generic multiple object tracking algorithm (e.g. we use a tracking by detection method [27]), in this paper we focus on the second step.To avoid manual data annotation, which is extremely ineffective in our case, we turn to self-supervision and adapt the widely used contrastive learning approach [5] to video input and object tracking scenario.As tracklet re-identification is a sequence-to-sequence matching problem, the standard solution is comparing sequences element-wise and then aggregating the per-element comparisons, e.g. by averaging or max/min pooling [21] -the so-called late fusion technique. We, on the other hand, follow an early fusion approach by building a joint representation for the whole sequence. We use an advanced transformer network [23] to leverage the attention paradigm for nonuniform weighing and ""knowledge exchange"" between tracklet frames.We extensively test the proposed method on hundreds of colonoscopy videos and evaluate the contribution of method components using an ablation study. Finally, we demonstrate the effectiveness of the proposed ReID method for improving the accuracy of polyp characterization (CADx).To summarize, the three main contributions of the paper are:-An adaptation of contrastive learning to video input for the purpose of appearance based object tracking. -An early fusion, joint multi-view object representation for ReID, based on transformer networks. -The application of polyp ReID to boost the polyp CADx performance."
Self-supervised Polyp Re-identification in Colonoscopy,2.0,Methods,"This work assumes the availability of an automatic polyp detector. Quite a few highly accurate polyp detectors were recently reported [14][15][16], detecting (multiple) polyps in a single frame. Our ultimate goal is to group those detections into sets corresponding to distinct polyps.As briefly mentioned above, the proposed approach starts with an initial grouping of polyp detections using an off-the-shelf multiple object tracking algorithm. Such a tracker is expected to track polyps through consecutive frames as long as they do not leave the camera field of view, forming disjoint, time separated polyp tracklets. In this work we use the ByteTrack [27] ""tracking by detection"" algorithm, but, in principle, any other tracker could be used instead.The resulting tracklets are typically relatively short, and there are quite a few tracklets corresponding to the same polyp. To improve the result, we propose an Appearance-based Polyp Re-Identification (ReID), which groups multiple disjoint tracklets by their visual appearance into a joint tracklet, associated with a single polyp. In what follows we describe in detail the proposed ReID component.As stated above, the objective of ReID is to ascertain whether two timeseparated, disjoint tracklets belong to the same polyp. To this end we seek a tracklet representation that allows measuring visual similarity between tracklets. The two basic alternatives are either a single representation for the whole tracklet, or a sequence of single-frame representations for each tracklet frame. We will consider both options below."
Self-supervised Polyp Re-identification in Colonoscopy,2.1,Single-Frame Representation for ReID,"To generate a single frame representation we train an embedding model that maps a polyp image into a latent space, s.t. the vectors of different views of the same polyp are placed closer, and of different polyps away from each other [11].A straightforward approach to train such model is supervised learning, which requires forming a large collection of polyp image pairs, manually labeled as same/not same polyp [1]. Such annotation turned out to be inaccurate and expensive. In addition, finding hard negative pairs is especially challenging, as images of two randomly sampled polyps are usually very dissimilar. Moreover, self-supervised techniques using extensive unannotated datasets has exhibited substantial advantages within the medical domain [12].Hence, we turn to SimCLR [5], a contrastive self-supervised learning technique, which requires no manual labeling. In SimCLR the loss is calculated over the whole batch where all input samples serve as negatives of each other and positive samples are generated via image augmentations. Combined with the temperature mechanism this allows for hard negative mining by prioritizing hard-to-distinguish pairs, resulting in a more effective loss weighting scheme.One caveat of SimCLR is the difficulty to generate augmentations beneficial for the learning process [5]. Specifically for colonoscopy, the standard image augmentations do not capture the diversity of polyp appearances in different views (see Fig. 1(c)).Instead of customizing the augmentations to fit the colonoscopy setup, we leverage the temporal nature of videos, and take different polyp views from the same tracklet as positive samples (see Fig. 1(b)). Formally, a batch is formed by sampling one tracklet from N different procedures to ensure the tracklets belong to different polyps. Two polyp views i, j are sampled from each tracklet as positive pairs (same polyp). Let f be the embedding model. The loss function for the positive pair (i, j) is defined as:where sim is the dot product and τ is the temperature parameter [24]. The final loss is computed across all positive pairs in the batch.Tracklets represented as sequences of per-frame embeddings can be matched by computing pair-wise distances between frames, followed by an aggregatione.g. min/max/mean distance [4,10]. An example of similarities between frames can be seen in Supplementary Fig. 2."
Self-supervised Polyp Re-identification in Colonoscopy,2.2,Multi-view Tracklet Representation for ReID,"As discussed earlier, an alternative to the single frame approach, is a unified representation for the whole tracklet. A commonly used practice is to compute single frame embedding (for each view) and fuse them [8,21], e.g. by averaging. The downside of those simple techniques is that they treat every frame in the same way, including bad quality, repeating, non-informative views. We postulate that learning a joint embedding of multiple views in an end-to-end manner will produce a better representation of the visual properties of a polyp, by allowing ""knowledge exchange"" between the tracklet frames.To achieve this, we employ a transformer network [23], with the addition of BERT [7] classification token (CLS). The attention mechanism enables both frame based intra attention and selective weighting of the frames thus providing a more comprehensive tracklet representation. The overview of the architecture is presented in Fig. 2. Training this multi-view encoder is done similarly to training a single-view encoder using SimCLR, but now, instead of pairs of frames, we deal with pairs of tracklet. To generate positive tracklet pairs, we cannot apply the trick used for single frames, where positive pairs are sampled within the same tracklet. Instead we generate ""pseudo positive"" pairs from existing tracklets. We artificially split a tracklet into 3 disjoint segments, where the middle segment is discarded, and the first and the last segments are used as a positive pair, thus providing sufficiently different appearances of the same polyp as would happen in real procedures. In addition, this type of sampling approach, which effectively discards highly correlated samples from training, has been shown to improve model performance in [17]."
Self-supervised Polyp Re-identification in Colonoscopy,3.0,Experiments,This section includes two parts. The first provides a stand-alone evaluation of the proposed ReID method. The second assesses the impact of ReID on polyp classification accuracy.
Self-supervised Polyp Re-identification in Colonoscopy,3.1,ReID Standalone Evaluation,"Dataset. We use 22,283 colonoscopy videos, split into training (21,737) and test (546) sets. These recordings were captured from standard colonoscopy procedures conducted at six medical centers during the period of 2019 to 2022. The average length of the recorded procedures is 15 min, with a median duration of 13 min. For training, we automatically generated polyp tracklets using automatic polyp detection and tracking as described in Sect. 2.The tracking algorithm might produce short and uninformative tracklets as well as outliers. The following clean up steps were performed on the training set: we filtered out tracklets shorter than 1 s or having less than 15 high confidence detections, as defined in [27], and took only the longest tracklet from every procedure. The thresholds were determined using analysis of the training set tracklets distribution. This yielded the training set of 15,465 tracklets (mean duration of 377 frames or 29 s). For evaluation, the test set polyp tracklets were manually annotated (timestamps and bounding boxes) by certified physicians. In addition, tracklet pairs from the same procedure were manually labeled as either belonging to the same polyp or not. This yielded 348 negative and 252 positive tracklet pairs.Training. We utilize ResNet50V2 [9] as the single frame encoder, with an MLP head projecting the representation into a 128-dimensional embedding vector. We initialize the model using pre-trained ImageNet [6] weights. While Ima-geNet weights are not optimal for medical tasks [18,19], they offer training speedups [18]. The multi-view encoder consists of 3 transformer encoder blocks with an MLP projection head. We use LARS optimizer [25] with the learning rate of 0.01 and τ = 0.1 as suggested in [5]. The batch size is set to 1024 for training both the single frame and the multi-view encoder.We first train the single frame encoder and use its weights to initialize the single frame module of the multi-view encoder. Due to memory limitations, we use 8 views per tracklet during training, resulting in 1024 * 8 = 8192 images per training step. The model was trained for 5,000 steps using cloud v3 TPUs with 16 cores. The single frame encoder has 24M parameters, and the multi-view encoder adds an additional 1M parameters.Evaluation. We start by comparing various ReID techniques described in Sect. 2. Namely, we evaluate the accuracy of tracklet re-identification using: (a) single-frame representation with pairwise distances aggregation by Min / Max / Mean functions [4,10]; (b) multi-view representation by frame embeddings averaging; and, finally, (c) the joint embedding multi-view model. We evaluate the performance using AUC of the ROC and precision-recall curve (PRC) for tracklet similarity scores over the test set (see Table 1 and Supplementary Fig. 3). One can see that the joint embedding multi-view model outperforms all other techniques both on ROC and PRC.In addition, we evaluate the effectiveness of ReID by measuring the average polyp fragmentation rate (FR), defined as the average number of tracklets polyps are split into. Obviously, lower fragmentation rate means better result (with the best fragmentation of 1), but it may come at the expense of wrong tracklet matching (false positive). We measure the fragmentation rate at the operating point of 5% false positive rate. The number of polyp fragments is determined by matching tracklets to manually annotated polyps and counting  "
Self-supervised Polyp Re-identification in Colonoscopy,3.2,ReID for CADx,"In this section, we investigate the potential benefits of using polyp ReID as part of a CADx system. Polyp CADx aims to assist physicians to figure out, in real time, during the procedure, whether the detected polyp is an adenoma. Most reported CADx systems compute a classification score for each frame, and aggregate scores from multiple frames to determine the final polyp classification. Grouping polyp frames into a tracklet, to be fed into the CADx, is usually done by a spatio-temporal tracker [2]. Longer tracklets provide more information for polyp classification.Here, we investigate if the proposed ReID model, used to group disjoint tracklets of the same polyp, can increase the accuracy of CADx.Data. We use 3290 colonoscopy videos split into train, validation, and test sets (2666, 296, and 328 videos respectively). The videos are processed by a polyp detector and tracker to form polyp tracklets. The tracklets are then manually grouped together to build a single sequence for every polyp. Each polyp is annotated by a certified gastroenterologist as either adenoma or non-adenoma.CADx. We trained a simple image classification CNN, composed of a MobileNet [20] backbone, followed by an MLP layer with a sigmoid activation, to predict the non-adenoma/adenoma score in [0, 1], for each frame. The chosen architecture has 2.4M parameters and can run in real-time. The model was trained on Nvidia Tesla V100 GPU for 200 epochs with a learning rate of 0.001, using Adam optimizer.For evaluation, we used the model to predict the classification score for each frame and aggregated the scores using soft voting to achieve the final prediction for each tracklet.Evaluation. To assess the contribution of the ReID to polyp classification, we compare the CADx results on the test set, while using different grouping methods to merge multiple polyp detections into tracklets. The 3 evaluated methods are: (1) manual annotation (2) grouping by tracking, and (3) grouping by ReID. The manually annotated tracklets -the ground truth (GT) -are the longest sequences, containing all frames of each polyp in the test set. In grouping by tracking, we use tracklets generated by the spatio-temporal tracking algorithm [27]. Finally, for ReID, we merge disjoint tracklets by their appearance using the ReID model. By construction, tracklets generated by methods ( 2) and (3) are subsets of the corresponding manually annotated GT tracklet, and are assigned its polyp classification label. A visualization of the resulting tracklets using different grouping methods is provided in Supplementary Fig. 4. The number of resulting tracklets in the test set for each grouping method and polyp labels distribution are summarized in Table 3. We ran the CADx model on tracklets generated by the 3 grouping methods. We compute the F 1 score and the AUC for the tracklet classification task. In addition, we measure the CADx sensitivity at specificity=0.9. The results are summarized in Table 4. The result on the manually annotated data is the accuracy upper-bound and is brought as a reference point. One can see that the ReID based approach significantly improves the CADx accuracy compared to the tracking-based grouping. "
Self-supervised Polyp Re-identification in Colonoscopy,4.0,Conclusions,"In this study we present a novel multi-view self-supervised learning method for learning informative representations of a sequence of video frames. By jointly encoding multiple views of the same object, we get more discriminative features in comparison to traditional embedding fusion techniques. This approach can be used to group disjoint tracklets generated by a spatio-temporal tracking algorithm based on their appearance, by measuring the similarity between tracklets representations. Its applicability to medical contexts is of particular relevance, as medical data annotation often requires specific expertise and may be costly and time consuming. We use this method to train a polyp re-identification model (ReID) from large unlabeled data, and show that using the ReID model as part of a CADx system enhances the performance of polyp classification. There are some limitations however in identifying polyps based on their appearance, as it may be changed drastically during the procedure (for example, during resection). In future work we may examine the use of ReID for additional medical applications, such as listing detected polyps in an automatic report, bookmarking of specific areas of the colon during the procedure, and calculation of clinical metrics such as Polyp Detection Rate and Polyps Per Colonoscopy."
Self-supervised Polyp Re-identification in Colonoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_57.
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,1.0,Introduction,"Punctate white matter lesion (PWML) is a typical type of cerebral white matter injury in preterm infants, potentially leading to psychomotor developmental delay, motor delay, and cerebral palsy without timely treatment [2]. The early detection and quantitative analysis of PWMLs are critical for diagnosis and treatment, especially considering that some PWML subtypes are only detectable by magnetic resonance imaging (MRI) shortly after birth (e.g., around the third week) and will become invisible thereafter [6]. The PWMLs are small targets that typically locate anterior or adjacent to the ventricles [8]. Manually annotating them in MR images is very time-consuming and relies on expertise. Thus there is an urgent need, from neuroradiologists, to develop reliable and fully automatic methods for 3D PWML segmentation.Automated localization and delineation of PWML are practically challenging. This is mainly because that PWMLs are isolated small objects, with typically only dozens of voxels for a lesion and varying numbers of lesions across different subjects. Also, due to underlying immature myelination of infant brains [1], the tissue-to-tissue and lesion-to-tissue contrasts are both very low, especially in T1w MR images commonly used in clinical practice. In addition to conventional methods based on thresholding [3] or stochastic likelihood estimation [4], recent works attempted to apply advanced deep neural networks in the specific task of PWML segmentation [9,10,12]. For example, Liu et al. [10] extended Mask R-CNN [7] to detect and segment PWMLs in 2D image slices. Li et al. [9] implemented a 3D ResU-Net to segment diffuse white matter abnormality from T2w images. Overall, these existing learning-based methods usually use general network architectures. They may fail to completely capture fine-grained positional information to localize small and low-contrast PWMLs, potentially resulting in high under-segmentations.Counterfactual reasoning, explained by our task, studies how a real clinical brain image appearance (factual) changes in a hypothetical scenario (whether lesion exist or not). This idea has been applied as structural causal models (SCMs) in a deep learning way in recent years. At the theoretical level, Monteiro et al. [11] have presented a theoretically grounded framework to evaluate counterfactual inference models. Due to the advantage of being verifiable, this idea appeared in many medical scenarios. Pawlowski et al. [14] proposed a general framework for building SCMs and validated on a MNIST-like dataset and a brain MRI dataset. Reinhold et al. [15] developed a SCM that generates images to show what an MR image would look like if demographic or disease covariates are changed. In this paper, we propose a fully automatic deep-learning framework (DeepPWML) that leverages counterfactual reasoning coupled with location information from the brain tissue segmentation to capture fine-grained positional information for PWML localization and segmentation. Specifically, based on patch-level weak-supervision, we design a counterfactual reasoning strategy to learn voxel-wise residual maps to manipulate the classification labels of input patches (i.e., containing PWMLs or not). In turn, such fine-grained residual maps could initially capture the spatial locations and morphological patterns of potential lesions. In this article, we define this residual map as counterfactual map which may be different from the meaning of counterfactual map in other articles, hereby declare. And to refine the information learned by the counterfactual part, we further include brain tissue segmentation as an auxiliary task. Given the fact that PWMLs have specific spatial correlations with different brain tissues, the segmentation probability maps (and inherent location information) could provide a certain level anatomical contextual information to assist lesion identification. Finally, by using the counterfactual maps and segmentation proba- bility maps as the auxiliary input, we learn a lightweight sub-network for PMWL segmentation.Overall, our DeepPWML is practically easy to implement, as the counterfactual part learns simple but effective linear manipulations, the tissue segmentation part can adopt any off-the-shelf networks, and the PWML segmentation part only needs a lightweight design. On a real-clinical dataset, our method led to a state-of-the-art performance in the infant PWML segmentation task."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.0,Method,"As shown in Fig. 1, our DeepPMWL consists of four parts, i.e., the tissue segmentation module (T-SEG), the classification module (CLS), the counterfactual map generator (CMG), and the PWML segmentation module (P-SEG). Specifically, in the training stage, T-SEG is learned on control data, while other modules are learned on PWML data. Given an image patch as the input, CLS is trained to distinguish positive (containing PWML) or negative (no PWML) cases, based on which CMG is further trained to produce a counterfactual map to linearly manipulate the input to change the CLS result.The high-resolution counterfactual maps (CF maps) and segmentation probability maps (SP maps) are further combined with the input patch to train a lightweight P-SEG for PWML segmentation. In the test stage, an input patch is first determined by the CLS module whether it is positive. Positive inputs will pass through the T-SEG, CMG, and P-SEG modules to get the PWML segmentation results. It is worth noting that the test patches are generated by sliding windows, and the overlapping results are averaged to get the final segmentation results for the image, which reduces the impact of incorrect classification of the CLS module. In our experiments, T-SEG used the voxel-wise Cross-Entropy Loss. CLS used the Categorical Cross-Entropy Loss, CMG combined the sparsity loss (L1 and L2 norms) with the classification loss. Finally, P-SEG used the Dice Loss. In the following subsections, we will introduce every module in our design."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.1,Tissue Segmentation Module,"The task is to mark every pixel of the brain as cerebrospinal fluid (CSF), gray matter (GM), or white matter (WM). The choice of this module can be flexible, and there are many off-the-shelf architecture designs available. We adopt a simple Dense-Unet architecture [16] for the T-SEG module. It is trained on control premature infants' images. This module will output the SP map in which segmentation result can be obtained. Therefore, this SP map naturally contains some level anatomy information. Moreover, when an input with PWML goes through a network that has only been trained on control data, the segmentation mistake is partly due to the existence of PWML. Therebefore, this module can output a SP map carrying both potential location and anatomy guidance for PWML localization and segmentation."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.2,Classification Module and Counterfactual Map Generator,"The CLS and the CMG are trained sequentially. The CLS is trained to determine whether the current patches have lesions. The CMG is a counterfactual reasoning step for the CLS. Based on the characteristic of PWML, CMG learns a simple linear sparse transform shown as the CF map. This map aims to offset the bright PWML pixels of the image patches, which are classified as positive, or seed PWML on the patches judged as negative. In other words, CMG is learning a residual activation map for conversion between control and PWML. We adopt the T-SEG module's encoder with two fully connected layers as the CLS module. Furthermore, the architecture of CMG is a simple U-net adding a ""switch"" state in its skip-connection parts according to the method of Oh et al. [13]. Based on the nature of PWMLs, the last layer of CMG is Relu activation to ensure that the generated CF map is a positive activation.The state of the ""switch"" is determined by the classifier's result on the current patch. If the judgement is positive, correspondingly, the ""switch"" status is 0. In this condition, the activated areas in the CF map should be where PWMLs exist. Then the pseudo patches in Fig. 1, obtained by subtracting the CF map from the input patches, should be judged as negative by the fixed CLS. Another state of the ""switch"" is used to generate PWMLs. When the CLS judges are negative, the ""switch"" status is 1. in this situation, the input patches combining the CF map should be classified as positive. This training strategy is to make CMG learn PWML features better. When it comes to the test phase, the switch status will be fixed to 0. Because in the test phase, the CF map only needs to capture PWML.The CMG module is summarised as follows: Firstly, PWML patches C P and control patches C N are fed to the encoder to obtain encoded representations F P and F N :Secondly, ""switch"" filled with zeros/ones with the same size as PWML/normal representations F P /F N are added to these representations and then pass through the decoder to obtain the CF maps M P /M N :Finally, the original patches C P /C N are added/subtracted to the CF maps M P /M N to yield the transformed patches C P / C P , which are classified by the CLS module as the opposite classes:"
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.3,PWML Segmentation Module,"The SP map includes the potential PWML existence, but also a lot of tissue segmentation uncertainty. The CF map directly shows the PWML location, but due to the accuracy of the CLS module, the CF map itself will also carry some false positives fault. If we synthesize the CF map, the SP map and the original input patches for appearance information, the best segmentation result can be achieved by allowing the network to verify and filter out each information in a learnable way. The P-SEG module is implemented as a lightweight variant of the Dense-Unet. Different simplified versions have been tested, with the results summarized in Sect. 3.2. After getting the PWML segmentation result, we use the tissue segmentation result to filter out PWMLs mis-segmented at the background and CSF. "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,3.1,Dataset and Experimental Setting,"Dataset: Experiments were performed on a dataset with two groups (control and PWML), where control included 52 subjects without PWML observed, and PWML included 47 subjects with PWMLs. All infants in this study were born with gestational age (GA) between 28 to 40 weeks and scanned at postmenstrual age (PMA) between 37 to 42 weeks. Two neuroscientists manually labeled PWML areas and corrected tissue labels generated by iBeat [5]. Written Consent was obtained from all parents under the institutional review board, and T1-weighted MR images were collected using a 3T MRI scanner, resampling the resolution of all images into 0.9375 × 0.9375 × 1 mm 3 . All images are cropped to 130 × 130 × 170.Experimental Setting: Our method was implemented using Tensorflow. All modules were trained and tested on an NVIDIA GeForce RTX 3060 GPU. We adopted Adam as the optimizer, with the learning rate varying from 0.001 to 0.00001 according to modules. The inputs were fixed-size patches (32 × 32 × 32) cut from the T1w images. The train/validation/test ratio was 0.7/0.15/0.15 and divided on subject-level. We didn't use any data augmentation during training. We used Dice, True Positive Rate (TPR), and Positive Predictive Value (PPV) to quantitatively evaluate the segmentation performance. "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,3.2,Results,"First, the T-SEG module is trained using a fully supervised way. Its tissue segmentation accuracy on the test set is about 93% in terms of Dice. Second, the CLS and other modules are trained with PWML group data. We defined the input training patches' class labels by whether they contain PWMLs or not. In other words, if any patch has at least one lesion voxel, it is positive. The accuracy of the test set can reach around 90%. Third, we train the CMG module based on the well-trained and fixed CLS module. Finally, based on T-SEG and CMG, we train P-SEG. We combine the SP map, CF map, and T1w image in a channel-wise way as the input of the module without any additional processing of these features.Comparison Results: We compared our method with the state-of-the-art method [10]. As is shown in Table 1, our method outperforms the state-of-the-art method and the baseline model in all three indexes. The visualization results are shown in Fig. 2, from which it can be seen that our method can segment smallsize PWMLs more accurately and segment PWMLs with different severities more completely.Ablation Studies: We further evaluated the effectiveness of our design by comparing the results of the pipelines with and without SP maps and CF maps. The ablation results are shown in the last six rows of Table 1. The baseline model using the same dense-Unet is trained to segment the PWML from T1w images. Other settings are consistent with our final module. Then we will add our designed modules step by step to verify the effectiveness of two kinds of auxiliary information.By comparing ""baseline"", ""SP map"", and ""CF map"", we can find that the two kinds of information individually are not good for segmenting PWMLs. The reason is as follows. The SP map mainly focuses on tissue segmentation task. The CF map has some false activation due to the offset of the highlighted areas for PWML. Fusing these two kinds of information has reduced their respective defects (""SP map + CF map""). The icing on the cake is that when the appearance features of T1w are used again, the accuracy will be significantly improved (""SP map + T1"" and ""CF map + T1""). This means ""SP map"" and ""CF map"" each can be an auxiliary information but not sufficient resource for this task. Finally, after combining the three together, all indicators have been significantly improved (""SP map + CF map + T1"").Visual Analysis: Figure 3 shows the T1w images, tissue segmentation maps, CF maps, labels, and segmentation results. By selecting the most likely category from the SP map as the label, the tissue segmentation map can be obtained. As shown in the tissue segmentation maps, PWML voxels tend to be classified as gray matter surrounded by white matter which obviously does not conform to the general anatomy knowledge. The reason of this phenomenon may be that the intensity of gray matter and PWML are higher than white matter in T1w image at this age. It also can be seen from the CF maps that these maps have a preliminary localization of PWML. The last row shows the situation without PWML. It can be seen that the tissue segmentation is reasonable. The CF map has a small amount of activation and the intensity is significantly lower than the first three rows. In conclusion, these two maps complementarily provide the anatomical and morphological information for the segmentation of the PWML.Comparison of Different Backbones of the P-SEG Module: We test from simple several layers to the whole dense-Unet to determine the required complexity in Table 2. We compared six designs with different network sizes in Table 2. The first three methods are several convolution layers with the same resolution. The latter three reduce the number of down-samplings in the original dense-Unet. By comparing the Dice index, it is obvious that the simple convolution operation cannot integrate the three kinds of input information well. The results show that the encoder-decoder can better fuse information. Perhaps because of the small size of PWML, it does not require too much down-sampling to get a similar result as the optimal result. The result also indicates that a certain degree of network size is needed to learn the PWMl characteristics."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,4.0,Conclusion,"In this study, we designed a simple and easy-to-implement deep learning framework (i.e. DeepPWML) to segment PWMLs. Leveraging the idea of generative counterfactual inference combined with an auxiliary task of brain tissue segmentation, we learn fine-grained positional and morphological representations of PWMLs to achieve accurate localization and segmentation. Our lightweight PWML segmentation network combines lesion counterfactual maps with tissue segmentation probability maps, achieving state-of-the-art performance on a real clinical dataset of infant T1w MR images. Moreover, our method provides a new perspective for the small-size segmentation task."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,1.0,Introduction,"Epilepsy is a chronic neurological condition that affects more than 60 million people worldwide in which patients experience epileptic seizures due to abnormal brain activity [17]. Different types of seizures are associated with the specific part of the brain involved in the abnormal activity [8]. Thus, accurate detection of the type of epileptic seizure is essential to epilepsy diagnosis, prognosis, drug selection and treatment. Concurrently, real-time seizure alerts are also essential for caregivers to prevent potential complications, such as related injuries and accidents, that may result from seizures. Particularly, patients suffering from tonic-clonic seizures (TCSs) are at a high risk of sudden unexpected death in epilepsy (SUDEP) [18]. Studies have shown that SUDEP is caused by severe alteration of cardiac activity actuated by TCS, leading to immediate death or cardiac arrest within minutes after the seizure [5]. Therefore, it is critical to accurately and promptly detect and classify epileptic seizures to provide better patient care and prevent any potentially catastrophic events.The current gold standard practice for detection and classification of epileptic seizures is the hospital-based Video EEG Monitoring (VEM) units [23]. However, this approach is expensive and time consuming which is only available at specialized centers [3]. To address this issue, the research community has developed automated methods to detect and classify seizures based on several modalities -EEG [7,30], accelerometer [16], and even functional neuroimaging modalities such as fMRI [22] and electrocorticography (ECoG) [24]. Although, there have been developments of approaches for the above modalities, seizure detection using videos remains highly desirable as it involves no contact with the patient and is easier to setup and acquire data compared to other modalities. Thus, researchers have also developed automated approaches for the video modality.Initial works primarily employed hand-crafted features based on patient motion trajectory by attaching infrared reflective markers to specific body key points [4,15]. However, these approaches were limited in performance due to their inability to generalize to changing luminance (night time seizures) or when the patient is occluded (covered by a bed sheet) [14]. Thus, very recently deep learning (DL) models have been explored for this task [1,2,12,21,29]. [29] demonstrated that DL models could detect generalized tonic-clonic seizures (GTCSs) from the RGB video of seizures. Authors in [21] radically used transfer learning (from action recognition task) to train DL networks for distinguishing focal onset seizures (FOSs) from bilateral TCSs using features extracted from the RGB video of seizures. Whereas, the authors in [12] developed a DL model to discriminate dystonia and emotion in videos of Hyperkinetic seizures. However, these developed approaches have two crucial limitations -(1) As these approaches directly operate on RGB videos, there is a possibility of privacy leakage of the sensitive patient data from videos. Moreover, obtaining consent from patients to share their raw RGB video data for building inter-cohort validation studies and generalizing these approaches on a large scale becomes challenging; (2) The current approaches consider the full video of a seizure to make predictions, which makes early detection of seizures impossible. The duration of a seizure varies significantly among patients, with some lasting as short as 30 s while others can take minutes to self-terminate. Thus, it is unrealistic to wait until the completion of a long seizure to make a prediction and alert caregivers.In this work, we address the above two challenges by building an in-house dataset of privacy-preserved extracted features from a video and propose a framework for early detection of seizures. Specifically, we investigate two aspects - (1) The feasibility of detecting and classifying seizures based only on optical flow, a modality that captures temporal differences in a scene while being intrinsically privacy-preserving. (2) The potential of predicting the type of seizure during its progression by analyzing only a fraction of the video sample. Our early detection approach is inspired by recent developments in early action recognition in videos [9,10,19,21,28,31]. We develop a custom feature extractor-transformer framework, named SEizure TRansformer (SETR) block for processing a single video sample. To achieve early detection from a fraction of the sample, we propose Progressive Knowledge Distillation (PKD), where we gradually distill knowledge from SETR blocks trained on longer portions of a video sample to SETR blocks which will operate on shorter portions. We evaluate our proposed SETR-PKD framework on two datasets -an in-house dataset collected from a VEM unit in a hospital and a publicly available dataset of video-extracted features (GESTURES) [21]. Our experiments demonstrate that our proposed SETR-PKD framework can detect TCS seizures with an accuracy of 83.9% in a privacy-preserving manner when they are only half-way into their progression. Furthermore, we comprehensively compare the performance of direct knowledge distillation with our PKD approach on both optical flow features (in-house dataset) and raw video features (public dataset). We firmly believe that our proposed method makes the first step towards developing a privacypreserving real-time system for seizure detection in clinical practice."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.0,Proposed Method,"In this section, we first outline the process of extracting privacy-preserving information from RGB video samples to build our in-house dataset. Later, we explain our proposed approach for early detection of seizures in a sample."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.1,Privacy Preserving Optical Flow Acquisition,"Our in-house dataset of RGB videos of patients experiencing seizures resides on hospital premises and is not exportable due to the hospital's ethics agreement1 . To work around this limitation, we develop a pipeline to extract optical flow information [11] from the videos. This pipeline runs locally within the hospital and preserves the privacy of the patients while providing us with motion semiotics of the seizures. An example of the extracted optical flow video sample can be seen in Fig. 1. We use the TV-L1 algorithm [20] to extract the optical flow features for each video, which we then export out of the hospital for building our proposed approach. We provide more information about our dataset, including the number of patients and seizures, annotation protocol, etc. in Sect. 3."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.2,Early Detection of Seizures in a Sample,"Consider an input optical flow video sample V i as shown in Fig. 1(a) with a time period of T i , consisting of N frames -{f 0 , f 1 , ...f N -1 }, and having a ground truth label of y i ∈ {0, 1, ...C} where is C the total number of categories. Then, the task of early detection is to build a framework that could classify the category of the sample correctly by analyzing the least possible partial segment of the sample. Thus, to define the problem of early detection, we split the sample V i into k segments -{0, 1, ...k -1} starting from the beginning to the end as shown in Fig. 1(b). Here V k-1 i corresponds to the full video sample and the descending segments correspond to the reduced partial video samples. We build these partial segments by equally adding the temporal information throughout the sample i.e. the time period for a partial subset V j i of a sample V i is computed as (j + 1) × T i /k. Thus, the early detection task is to correctly predict the category y i of the sample V i from the lowest possible (j) partial segment V j i of V i . In Fig. 1, we illustrate our proposed framework where -(a) First, we build a Seizure Transformer (SETR) block for processing a single optical flow video sample (b) Later, we employ SETR based Progressive Knowledge Distillation (SETR-PKD) to achieve early detection in a sample.Processing a Single Sample. Since seizure patterns comprise of body movements, we implement transfer learning from a feature extractor pre-trained on action recognition task to extract the spatial features from the optical flow frames. Prior work [21] has shown that Temporal Segment Networks (TSNs) [27] pretrained on RGB videos of various actions are effective at extracting features from videos of seizures. We also utilize TSNs but pretrained on the optical flow modality, since we have privacy-preserved optical flow frames. The TSNs extract a 1D feature sequence for each frame f j , referred as spatial features in Fig. 1(a). The spatial features are then processed by a linear transformation (1-layer MLP) that maps them into motion tokens ∈ R N ×D , where each token has D-dimensions.We leverage transformers to effectively learn temporal relations between the extracted spatial features of the seizure patterns. Following the strategy of ViT [6], after extracting the spatial features, we append a trainable class embedding class embed ∈ R D to the motion tokens. This class embedding serves to represent the temporal relationships between the motion tokens and is later used for classification (class token in Fig. 1(a)). As the order of the motion tokens is not known, we also add a learnable positional encoding L P OS ∈ R (N +1)×D to the combined motion tokens and class embed . This is achieved using an element-wise addition and we term it as the input X i for the input sample V i .To enable the interaction between tokens and learn temporal relationships for input sample classification, we employ the Vanilla Multi-Head Self Attention (MHSA) mechanism [26]. First, we normalize the input sequence X i ∈ R (N +1)×D by passing it through a layer normalization, yielding X i . We then use projection matrices  for query, key, and value respectively. Next, we compute a dot product of Q with K and apply a softmax layer to obtain weights on the values. We repeat this self-attention computation N h times, where N h is the number of heads, and concatenate their outputs. Eq. 1, 2 depict the MHSA process in general.Subsequently, the output of MHSA is passed to a two-layered MLP with GELU non-linearity while applying layer normalization and residual connections concurrently. Eq. 3, 4 represent this overall process.where m L ∈ R (N +1)×D are the final output feature representations and L is the total number of encoding layers in the Transformer Encoder. Note that the first R N ×D features correspond to the patch tokens , while the final R D correspond to the class token of the m L as shown in Fig. 1(a). As mentioned earlier, we then use a one-layer MLP to predict the class label from the class token . We refer to this whole process as a SEizure TRansformer (SETR) block shown in Fig. 1(a).Progressive Knowledge Distillation. To achieve early detection, we use Knowledge Distillation in a Progressive manner (PKD), starting from a SETR block trained on a full video sample and gradually moving to a SETR block trained on a partial video sample, as shown in Fig. 1(b). Directly distilling from a SETR block which has seen a significantly longer portion of the video (say V k-1 i ) to a SETR block which has only seen a smaller portion of the video sample (say V 0 i ) will lead to considerable mismatches between the features extracted from the two SETRs as there is a large portion of the input sample that the student 0 SETR has not seen. In contrast, our proposed PKD operates in steps. First we pass the knowledge from teacher (T eacher k-1 in Fig. 1; Later, the Subteacher k-2 SETR passes its distilled knowledge to its subsequent student (Sub-teacher k-3 ) SETR, and this continues until the final Sub-teacher 1 SETR passes its knowledge to the bottom most Student 0 SETR. Since the consecutive segments of the videos do not differ significantly, PKD is more effective than direct distillation, which is proven by results in Sect. 3.4.For distilling knowledge we consider both class token and patch tokens of the teacher and student networks. A standard Kullback-Leibler divergence (L KL ) loss is applied between the probabilities generated from class token of the teacher and student SETR, whereas a mean squared error (L MSE ) loss is computed between the patch tokens of teacher and student SETR. Overall, a student SETR is trained with three losses -L KL and L MSE loss for knowledge distillation, and a cross-entropy (L CE ) loss for classification, given by the equations below.where q S j and q T j are the soft probabilities (moderated by temperature τ ) of the student and teacher SETRs for the j th class, respectively.where N is the number of patches and p T i and p S i are the patches of teacher and student SETRs respectively.where α and β are the weights for L KL and L MSE loss respectively.3 Datasets and Experimental Results"
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.1,In-House and Public Dataset,"Our in-house dataset 2 contains optical flow information extracted from highdefinition (1920 × 1080 pixels at 30 frames per second) video recordings of TCS seizures (infrared cameras are used for nighttime seizures) in a VEM unit in hospital. To annotate the dataset, two neurologists examined both the video and corresponding EEG to identify the clinical seizure onset (t ON ) and clinical seizure offset (t OF F ) times for each seizure sample. We curated a dataset comprising of 40 TCSs from 40 epileptic patients, with one sample per patient. The duration (in seconds) of the 40 TCSs in our dataset ranges from 52 to 367 s, with a median duration of 114 s. We also prepared normal samples (no seizure) for each patient by considering the pre-ictal duration from (t ON -300) to (t ON -60) seconds, resulting in dataset of 80 samples (40 normal and 40 TCSs). We refrain from using the 60 s prior to clinical onset as it corresponds to the transition period to the seizure containing preictal activity [13,25]. We use a 5-fold cross validation (split based on patients) for training and testing on our dataset. We also evaluate the effectiveness of our early detection approach on the GESTURES dataset [21], which contains features extracted from RGB video samples of seizures. The dataset includes two seizure types -106 focal onset seizures (FOS) and 77 Tonic-Clonic Seizures (TCS). In contrast to our in-house dataset, the features are provided by the authors, and we directly input them into our SETR block without using a feature extractor. To evaluate our method, we adopt the stratified 10-fold cross-validation protocol as used in GESTURES."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.2,Training Implementation and Evaluation Metrics,"We implement all experiments in PyTorch 1.8.1 on a single A100 GPU. The SETR block takes in a total of 64 frames (N ) with 512 1-D spatial feature per frame, has 8 MHSA heads (N h ) with a dropout rate of 0.1, 3 encoder layers (L), and 256 hidden dimensions (D). For early detection, we experiment by progressively segmenting a sample into -{4,8,16} parts (k). We employ a grid search to select the weight of 0.2 and 0.5 for KL divergence (τ = 10) and MSE loss respectively. We train all methods with a batch size of 16, a learning rate of 1e-3 and use the AdamW optimizer with a weight decay of 1e-4 for a total 50 epochs. For GESTURES dataset, we implement a weighted BCE loss to deal with the dataset imbalance, whereas for our in-house dataset we implement the standard BCE loss. We use precision, recall and f1-score for benchmarking."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.3,Performance for Early Detection,"Table 1 shows the benchmarking performance of all techniques with varying fractions of input video samples on both datasets. We observed three key findings from the results in Table 1. First, transformer-based methods such as our proposed SETR-PKD and OaDTR exhibit better performance retention compared to LSTM-based techniques (RULSTM, Slowfast RULSTM, EgoAKD, GESTURES) with a reduction in the fraction of input sample. Second, SETR-PKD performance increases with k=8 from k=4, but saturates at k=16 for inhouse dataset, whereas it achieves the best performance for k=4 for GESTURES dataset. The median seizure length for the in-house dataset and GESTURES dataset is 114 s and 71 s, respectively. As a result, PKD using relatively longer partial segments (k=4) is sufficient for GESTURES, while shorter partial segments (k=8) are required for our dataset. Thus, the optimal value of k for PKD may vary depending on a dataset. Finally, we observed better performance on the GESTURES dataset, which is expected given the more detailed and refined features extracted from RGB video compared to optical flow information. To validate our approach of progressive knowledge distillation in a fair manner, we conducted an ablation study to compare it with direct knowledge distillation. Figure 2 shows the comparison of the accuracy of the two approaches for different fractions of the input video sample on both datasets. The results indicate that although direct knowledge distillation can increase performance, it is less effective when the knowledge gap is wide, i.e., from a SETR block trained on a full input sample to a SETR block trained on a minimal fraction of the input sample (1/8, 1/4, .. 1/2) compared to when the knowledge gap is small (5/8, .. 7/8). On the other hand, our SETR-PKD approach significantly improves performance for minimal fractions of input samples on both datasets."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,4.0,Conclusion,"In this work, we show that it is possible to detect epileptic seizures from optical flow modality in a privacy-preserving manner. Moreover, to achieve real-time seizure detection, we specifically develop a novel approach using progressive knowledge distillation which proves to detect seizures more accurately during their progression itself. We believe that our proposed privacy-preserving early detection of seizures will inspire the research community to pursue real-time seizure detection in videos as well as facilitate inter-cohort studies."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_21.
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,1.0,Introduction,"Colonoscopy plays a crucial role in identifying and removing early polyps and reducing mortality rates associated with rectal cancer. Over the past few years, the research community has devoted great effort to understanding colonoscopy videos using either optical flow [22,23] or temporal information aggregation [5,12,16,19] between multiple frames.However, those works are mainly designed based on the experience of previous natural video object detection studies, ignoring the inherent uniqueness of the colonoscopy motion patterns. Thus, we rethink the video polyp detection task and conclude three core challenges in colonoscopy videos. 1) Fast motion speed. In Fig. 1(a), we show the target motion speed [26] 1 on ImageNetVID [14] (natural) and LDPolypVideo [9] (colonoscopy) dataset. The motion speed in ImageNetVID evenly distributes in three intervals. In contrast, most targets in LDPolypVideo fall in the fast speed zone, leading to a large variance in the adjacent foreground features, like motion blur or occlusion, as shown in Fig. 1(c). Thus we conjecture that collaborating too many frames for polyp video detection will increase the misalignment between adjacent frames and leads to poor detection performance. Figure 1(b) shows the performance of FGFA [26] on two datasets with increasing reference frames. The different trends of the two lines confirm our hypothesis. 2) Complex background. Different from the common camera-fixed videos, the camera-moving of colonoscopy video will introduce large disturbances between adjacent frames (e.g., specular reflection, bubbles, water, etc.), as shown in Fig. 1(d). Those abnormalities disrupt the integrity of background structures and thus affect the effect of multi-frame fusion. 3) Concealed polyps. As shown in Fig. 1(e), we noticed that some polyps could be seen as concealed objects in the colonoscopy video since such polyps have a very similar appearance to the intestine wall. The model will be confused by such frames in inference and result in high false-positive or false-negative predictions.To address the above issues, we propose the YONA framework, which fully exploits the reference frame information and only needs one adjacent reference frame for accurate video polyp detection. Specifically, we propose the Foreground Temporal Alignment (FTA) module to explicitly align the foreground channel activation patterns between adjacent features according to their foreground similarity. In addition, we design the Background Dynamic Alignment (BDA) module after FTA that further learns the inter-frame background spatial dynamics to better eliminate the influence of motion speed and increase the training robustness. Finally, parallel to FTA and BDA, we introduce the Cross-frame Boxassisted Contrastive Learning (CBCL) that fully utilizes the box annotations to enlarge polyp and background discrimination in embedding space.In summary, our contributions are in three-folds: (1) To the best of our knowledge, we are the first to investigate the obstacles to the development of existing video polyp detectors and conclude that two-frame collaboration is enough for video polyp detection. (2) We propose the YONA, a novel framework for video polyp detection. It composes the foreground and background alignment modules to align the features under the fast-moving condition. It further introduces the cross-frame contrastive learning module to enhance the model's discrimination ability of polyps and intestine walls. (3) Extensive experiments demonstrate that our YONA achieves new state-of-the-art performance on three large-scale public video polyp detection datasets."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.0,Method,"The whole pipeline is shown in Fig. 2. We leverage the CenterNet [25] as the base detector. Given a clip of a colonoscopy video, we take the current frame as anchor I a and its adjacent previous frame as reference I r . The binary maps M a , M r are generated using the bounding box of anchor and reference, where the foreground pixels are assigned with 1 while the background with 0. At each step, YONA first extracts multi-scale features from I a , I r using the backbone. Then, multi-scale features are fused and up-sampled to the resolution of the first stage as the intermediate features F a , F r . Then, we conduct foreground temporal alignment (Fig. 2(a)) on intermediate features to align their channel activation pattern. Next, the enhanced anchor feature F is further refined by the background dynamic alignment module (Fig. 2(b)) to mitigate the rapid dynamic changes in the spatial field. The BDA's output F * is used to compute the detection loss. Meanwhile, the intermediate features and binary maps are used to calculate the contrastive loss during training to improve the model's perception of polyp and background (Fig. 2(c)).Overall, the whole network is optimized with the combination loss function in an end-to-end manner. The final loss is composed of the same detection loss with CenterNet and our proposed contrastive loss, formulated as L = L detection + λ contrast L contrast . "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.1,Foreground Temporal Alignment,"Since the camera moves at a high speed, the changes in the frame are very drastic for both foreground and background targets. As a result, multi-frame (reference>3) fusion may easily incorporate more noise features into the aggregation features. On the other hand, the occluded or distorted foreground context may also influence the quality of aggregation. Thus we propose to conduct temporal alignment between adjacent features by leveraging the foreground context of only one adjacent reference frame. It is designed to align the certain channel's activation pattern of anchor feature to its preceding reference feature. Specifically, given the intermediate features F a , F r and reference binary map M r , we first pooling F r to 1D channel pattern f r by the binary map on the spatial dimension (R N ×C×H×W → R N ×C×1 ) and normalize it to [0, 1]:Then, the foreground temporal alignment is implemented by channel attention mechanism, where the attention maps are computed by weighted dot-product.We obtain the enhanced anchor feature by adding the attention maps with the original anchor feature through skip connection to keep the gradient flow.where α is the adaptive weight by similarity measuring. At the training stage, the ground truth boxes of the reference frame are used to generate the binary map M r . During the inference stage, we conduct FTA only if the validated bounding box of the reference frame exists, where ""validated"" denotes the confidence scores of detected boxes are greater than 0.6. Otherwise, we will skip this process and feed the original inputs to the next module.Adaptive Re-weighting by Similarity Measuring. As discussed above, due to video jitters, adjacent frames may change rapidly at the temporal level, and directly fusing the reference feature will introduce noisy information and misguide the training. Thus we designed an adaptive re-weighting method by measuring the feature similarity, where the weight indicates the importance of the reference feature to the anchor feature. Specifically, if the foreground feature of the reference is close to the anchor, it is assigned a larger weight at all channels. Otherwise, a smaller weight is assigned. For efficiency, we use the cosine similarity metric [8] to measure the similarity, where f a is the 1D channel pattern of F a computed with Eq. 1:"
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.2,Background Dynamic Alignment,"The traditional convolutional-based object detector can detect objects well when the background is stable. However, once it receives obvious interference, such as light or shadow, the background changes may cause the degradation of spatial correlation and lead to many false-positive predictions. Motivated by the inter-frame difference method [20], we first mine the dynamic field of adjacent background contents, then consult to deformable convolution [3] to learn the inherent geometric transformations according to the intensity of the dynamic field. In practice, given the enhanced anchor feature F from FTA and reference feature F r , the inter-frame difference is defined as the element-wise subtraction of enhanced anchor and reference feature. Then a 1 × 1 convolution is applied on the difference to generate dynamic field D, which encodes all spatial dynamic changes between adjacent frames.Finally, a 3 × 3 deformable convolution embeds the spatial dynamic changes of D on the enhanced anchor feature F .where D works as the deformable offset and F * is the final aligned anchor feature.Then the enhanced anchor feature is fed into three detection heads composed of a 3 × 3 Conv and a 1 × 1 Conv to produce center, size, and offset features for detection loss:where L focal is focal loss and L L1 is L1 loss."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.3,Cross-Frame Box-Assisted Contrastive Learning,"Typically, in colonoscopy videos, some concealed polyps appear very similar to the intestine wall in color and texture. Thus, an advanced training strategy is required to distinguish such homogeneity. Inspired by recent studies on supervised contrastive learning [18], we select the foreground and background region on both two frames guided by ground truth boxes to conduct contrastive learning. In practice, Given a batch of intermediate feature maps F a , F r ∈ R N ×T ×C×H×W and corresponding binary maps M a , M r ∈ R N ×T ×H×W , we first concatenate the anchor and reference at the batch-wise level as F ∈ R NT ×C×H×W and M ∈ R NT ×H×W to exploit the cross-frame information. Then we extract the foreground and background channel patterns of cross-frame feature F using the Eq. 1 base on M (x, y) = 1 and M (x, y) = 0, respectively. After that, for each foreground channel pattern, which is the ""query"", we randomly select another different foreground feature as the ""positive"", while all the background features in the same batch are taken as the ""negatives"". Finally, we calculate the one-step contrastive loss by InfoNCE [18]:where q j ∈ R C , j = 0, ..., N T is the query feature, i + ∈ R C and i -∈ R NT ×C are positives and negatives. N j denote embedding collections of the negatives. We repeat this process until every foreground channel pattern is selected and sum all steps as the final contrastive loss:3 ExperimentsWe evaluate the proposed method on three public video polyp detection benchmarks: SUN Colonoscopy Video Database [7,10] (train set: 19,544 frames, test set: 12,522 frames), LDPolypVideo [9] (train set: 20,942 frames, test set: 12,933 frames), and CVC-VideoClinicDB [1] (train set: 7995 frames, test set: 2030 frames). For the fairness of the experiments, we keep the same dataset settings for YONA and all other methods. We use ResNet-50 [6] as our backbone and CenterNet [25] as our base detector. Following the same setting in CenterNet, we set λ size = 0.1 and λ of f = 1. We set λ contrast = 0.3 by ablation study. Detailed results are listed in the supplement. We randomly crop and resize the images to 512 × 512 and normalize them using ImageNet settings. Random rotation and flip with probability p = 0.5 are used for data augmentation. We set the batch size N = 32. Our model is trained using the Adam optimizer with a weight decay of 5 × 10 -4 for 64 epochs. The initial learning rate is set to 10 -4 and gradually decays to 10 -5 with cosine annealing. All models are trained with PyTorch [11] framework. The training setting of other competitors follows the best settings given in their paper."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,3.1,Quantitative and Qualitative Comparison,"Quantitative Comparison. The comparison results are shown in Table 1. Following the standard of [1], the Precision, Recall, and F1-scores are used for evaluation. Firstly, compared with the CenterNet baseline, our YONA with three novel designs significantly improved the F1 score by 9.2%, 8.3%, and 7.4% on three benchmarks, demonstrating the effectiveness of the model design. Besides, YONA achieves the best trade-off between accuracy and speed compared with all other image-based SOTAs across all datasets. Second, for video-based competitors, previous video object detectors with multiple frame collaborations lack the ability for accurate detection on challenging datasets. Specifically, YONA surpasses the second-best STFT [19] by 2.2%, 3.0%, and 1.3% on F1 score on three datasets and 33.8 on FPS. All the results confirm the superiority of our proposed framework for accurate and fast video polyp detection.Qualitative Comparison. Figure 3 visualizes the qualitative results of YONA with other competitors [19,25]. Thanks to this one-adjacent-frame framework, our YONA can not only prevent the false positive caused by part occlusion (1st and 2nd clips) but also capture useful information under severe image quality (2nd clip). Moreover, our YONA shows robust performance even for challenging scenarios like concealed polyps (3rd clip)."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,3.2,Ablation Study,"We investigated the effectiveness of each component in YONA on the SUN database, as shown in Table 2. It can be observed that all the modules are necessary for precise detection compared with the baseline results. Due to the large variance of colonoscopy image content, the F1 score slightly decreases if directly adding FTA without the adaptive re-weighting strategy. Adding the adaptive weight greatly improves the F1 score by 5.4. Moreover, we use other two mainstream channel attention mechanisms to replace our proposed FTA for comparison. Compared with them, our FTA with adaptive weighting achieves the largest gain over the baseline and higher FPS. Overall, by combining all the proposed methods, our model can achieve new state-of-the-art performance. Table 2. Ablation studies of YONA under different settings. Ada means the adaptive re-weighting by similarity measuring; CW denotes the channel-wise attention [4]; CA denotes the channel-aware attention [19].FTA CW [4] CA [19]  "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,4.0,Conclusion,"Video polyp detection is a currently challenging task due to the fast-moving property of colonoscopy video. In this paper, We proposed the YONA framework that requires only one adjacent reference frame for accurate and fast video polyp detection. To address the problem of fast-moving polyps, we introduced the foreground temporal alignment module, which explicitly aligns the channel patterns of two frames according to their foreground similarity. For the complex background content, we designed the background dynamic alignment module to mitigate the large variances by exploiting the inter-frame difference. Meanwhile, we employed a cross-frame box-assisted contrastive learning module to enhance the polyp and background discrimination based on box annotations. Extensive experiment results confirmed the effectiveness of our method, demonstrating the potential for practical use in real clinical applications."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_5.
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,1.0,Introduction,"Breast cancer is a life-threatening disease that has surpassed lung cancer as leading cancer in some countries and regions [20]. Breast ultrasound is the primary screening method for diagnosing breast cancer, and accurately distinguishing between malignant and benign breast lesions is crucial. This task is also an essential component of computer-aided diagnosis. Since each frame in an ultrasound video can only capture a specific view of a lesion, it is essential to aggregate information from the entire video to perform accurate automatic lesion diagnosis. Therefore, in this study, we focus on the classification of breast ultrasound videos for detecting malignant and benign breast lesions.  [15] and static images from BUSI [1]. We use a 2D ResNet trained on ultrasound images to get the features.While ultrasound videos offer more information, prior studies have primarily focused on static image classification [2,11,27]. Obtaining ultrasound video data with pathology gold standard results poses a major challenge. Sonographers typically record keyframe images during general ultrasound examinations, not entire videos. Prospective collection requires additional efforts to track corresponding pathological results. Consequently, while there are many breast ultrasound image datasets [1,28], breast ultrasound video datasets remain scarce, with only one relatively small dataset [15] containing 188 videos available currently.Given the difficulties in collecting ultrasound video data, we investigate the feasibility of enhancing the performance of ultrasound video classification using a static image dataset. To achieve this, we first analyze the relationship between ultrasound videos and images. The images in the ultrasound dataset are keyframes of a lesion that exhibit the clearest appearance and most typical symptoms, making them more discriminative for diagnosis. Although ultrasound videos provide more information, the abundance of frames may introduce redundancy or vagueness that could disrupt classification. From the aspect of feature distribution, as shown in Fig. 1, the feature points of static images are more concentrated, while the feature of video frames sometimes are away from the class centers. Frames far from the centers are harder to classify. Therefore, it is a promising approach to guide the video model to pay more attention to important frames close to the class center with the assistance of static keyframe images. Meanwhile, our approach aligns with the diagnosis of ultrasound physicians, automatically evaluates the importance of frames, and diagnoses based on the information of key frames. Additionally, our method provides interpretability through key frames.In this paper, we propose a novel Keyframe Guided Attention Network (KGA-Net) to boost ultrasound video classification. Our approach leverages both image (keyframes) and video datasets to train the network. To classify videos, we use frame attention to predict feature weights for all frames and aggregate them to make the final classification. The feature weights determine the contribution of each frame for the final diagnosis. During training, we construct category feature centers for malignant and benign examples respectively using center loss [26] on static image inputs and use the centers to guide the training of video frame attention. Specifically, we propose coherence loss, which promotes the frames close to the centers to have high attention weights and decreases the weights for frames far from the centers. Due to the feature centers being generated by the larger scale image dataset, it provides more accurate and discriminative feature centers which can guide the video frame attention to focus on important frames, and finally leads to better video classification.Our experimental results on the public BUSV dataset [15] show that our KGA-Net significantly outperforms other video classification models by using an external ultrasound image dataset. Additionally, we visualized attention values guided by the coherence loss. The frames with clear diagnostic characteristics are given higher attention values. This phenomenon makes our method more explainable and provides a new perspective for selecting keyframes from video.In conclusion, our contributions are as follows:1. We analyze the relationship between ultrasound video data and image data, and propose the coherence loss to use image feature centers to guide the training of frame attention. 2. We propose KGA-Net, which adopts a static image dataset to boost the performance of ultrasound video classification. KGA-Net significantly outperforms other video baselines on the BUSV dataset. 3. The qualitative analysis of the frame attention verifies the explainability of our method and provides a new perspective for selecting keyframes."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,2.0,Related Works,"Breast Ultrasound Classification. Breast ultrasound (BUS) plays an important supporting role in the diagnosis of breast-related diseases. Recent research demonstrated the potential of deep learning for breast lesion classification tasks [6,18,19,23,27]. [6,18] design ensemble methods to integrate the features of multiple models to obtain higher accuracy. [19,23,27] utilize multi-task learning to improve the model performance. However, all of them are based on image datasets, such as BUSI [1], while few works focus on the video modality. [14] design a pre-training model based on contrastive learning for ultrasound video classification. [13,25] develop a keyframe extraction model for ultrasound videos and utilized the extracted keyframes to perform various classification tasks. However, these methods rely on keyframe supervision, which limits their applicability. Fortunately, the recent publicly available dataset BUSV [15] has made the research on the task of BUS video-based classification possible. In this paper, we build our model based on this dataset.Video Recognition Based on Neural Networks. Traditional methods are based on Two-stream networks [9,10,24]. Since I3D [3] was proposed, 3D CNNs have dominated video understanding for a long time. [21,22] decompose 3D convolution in different ways to reduce computation complexity without losing performance. [8] designed two branches to focus on temporal information and spatial features, respectively. However, 3D CNNs have a limited receptive field, and thus struggle to capture long-range dependency. Vision Transformers [5,16] have become popular for their ability to aggregate spatial-temporal information.To address computational complexity, MViT [7] employed a hierarchical structure and Video Swin [17] introduced 3D shifted window attention. Our proposed KGA-Net is a simple framework that leverages the frame attention module to aggregate multi-frame features efficiently."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.0,Methodology,"As shown in Fig. 2, our KGA-Net takes the video inputs and static image inputs simultaneously to train the network. The coherence loss is proposed to guide the frame attention by using the feature centers generated by the images. We will then elaborate on each component in the following sections."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.1,Video and Image Classification Network,"The video classification network is illustrated in Fig. 2 (a). The model is composed of a 2D CNN backbone, a frame attention module, and a classification head. For an input video clip V composed of N frames, it is first processed by the backbone network and the feature vectors of the frames {F i } N i=1 are obtained. Then, the frame attention module predicts the attention weight for each frame using a FC and sigmoid layer, and then the features are aggregated by the weights to form an integrated feature vector. Formally,where w i denotes the weight for the i th frame and FC is the fully-connected layer. Then, the features are aggregated byFinally, the classification head is applied to the final result of lesion classification. To train the model, the cross-entropy loss (CE Loss) is applied to the classification prediction of the video. The image classification network is used to assist in training the video model. We use the same 2D CNN as the backbone network in the video classification network. The model weights are shared for the two backbones for better generalization. To promote the formation of feature centers, we apply the center loss [26] to the image model besides the cross-entropy loss. In addition, the frame-level cross-entropy loss is also applied to the video frames to facilitate training."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.2,Training with Coherence Loss,"In this section, we introduce the coherence loss to guide the frame attention with the assistance of the category feature centers. We use the same method as center loss [26] to obtain the feature centers for the malignancy and benign lesions, which are denoted as C mal and C benign , respectively.The distances of frame features and the feature centers can measure the quality of the frames. The frame features close to the centers are more discriminative for the classification task. Therefore, we use these distances to guide the generation of frame attention. Specifically, we push the frames close to the centers to have higher attention weights and decrease the weights far from the centers. To do this, for each video frame with feature F i , we first calculate the feature distance from its corresponding class center. Formally,where Y ∈ {mal, benign} is the label of the video V and d i is the computed distance of frame i. Afterward, we apply coherence loss to the attention weights w = [w 1 , w 2 , ..., w N ] to make them have a similar distribution with the feature distances d = [d 1 , d 2 , ..., d N ] . To supervise the distribution, the coherence loss is defined as the L2 loss of the gram matrix of these two vectorswhereis the gram matrix of normalized attention weights, andis the gram matrix of normalized feature distances. Note that lower distances correspond to stronger attention, hence we use the opposite of w to get Gram w ."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.3,Total Training Loss,"To summarize, the total training loss of our KGA-NetL V CE and L I CE denote the cross-entropy for video classification and image and frame classification. L Center means the center loss. λ is the weight for coherence loss. Empirically, we set λ = 1 in our experiments.During inference, to perform classification on video data, the video classification network can be utilized individually for prediction."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.1,Implementation Details,"Datasets. We use the public BUSV dataset [15] for video classification and the BUSI dataset [1] as the image dataset. BUSV consists of 113 malignant videos and 75 benign videos. BUSI contains 445 images of benign lesions and 210 images of malignant lesions. For the BUSV dataset, we use the official data split in [15]. All images of the BUSI dataset are adopted to train our KGA-Net. Model Details. ResNet-50 [12] pretrained on ImageNet [4] is used as backbone. We use SGD optimizer with an initial learning rate of 0.005, which is reduced by 10× at the 4,000th and 6,000th iteration. The total learning iteration number is 8,000. The learning rate warmup is used in the first 1,000 iterations. For each batch, the video clips and static images are both sampled and sent to the network. We use a total batchsize of 16 and the sample probability of video clips and images is 1:1. We implement the model based on Pytorch and train it with NVIDIA Titan RTX GPU cards.During inference, we use the video classification network individually. In order to satisfy the fixed video length requirement of MViT [7], we sample up to 128 frames of each video to form a video clip and predict its classification result using all the models in experiments."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.2,Comparison with Video Models,"In this section, we compare our KGA-Net with other competitive video classification models. However, comparing with ultrasound-video-based work is challenging due to limited code accessibility and lack of keyframe detection model in existing methods [13,14,25]. Therefore, we compare our method with strong video baselines on natural images. We include CNN-based models, I3D [3], Slow-Fast [8], R(2+1)D [22], and CSN [21], along with the popular transformer-based model MViT [7]. For fairness comparison, we train these models using both video and image data, treating images as static videos. Evaluation metrics are reported on the BUSV test set for performance assessment.As shown in Table 1, by leveraging the guidance of the image dataset, our KGA-Net significantly surpasses all other models on all of the metrics. The video classification model of our KGA-Net is composed of a standard 2D ResNet-50 and a light feature attention module, while the baseline models are with net structures carefully designed for video analysis. Therefore, the success of our KGA-Net lies in the correct usage of the image guidance. The feature centers formed by the image dataset with larger data size and clear appearance effectively improve the accuracy of frame attention hence boosting the video classification performance."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.3,Ablation Study,"In this section, we ablate the contribution of each key design in our KGA-Net. We observe their importance by removing these key components from the whole network. The results are shown in Table 2. The results of KGA-Net are shown in the last row in Table 2, while the components are ablated in the first three rows. We use the same training schedule for all of the experiments.Image guidance is the main purpose of our method. To portray the effect of using the image dataset, we train the KGA-Net using BUSV dataset alone in the first row of Table 2. Without the image dataset, we generate the feature centers from the video frames. As a result, the performance significantly drops due to the decrease in dataset scale. It also shows that the feature centers generated by the image dataset are more discriminative than that of the video dataset. It is not only because the lesion number of BUSI is larger than BUSV, but also because the images in BUSI are all the keyframes that contain typical characteristics of lesions.Frame attention and coherence loss are two essential modules of our KGA-Net. We train a KGA-Net without the coherence loss in the third row of Table 2.In the second row, we further replace the feature attention module with feature averaging of video frames. It can be seen that both of these two modules contribute to the overall performance according to AUC and ACC. It is worth noting that these two models without coherence loss obtain very low sensitivity and high specificity, which means the model predictions are imbalanced and intend to make benign predictions. It is because that clear malignant appearances usually only exist in limited frames in a malignant video. Without our coherence loss or frame attention, it is difficult for the model to focus on typical frames that possess malignant features. This phenomenon certifies the effectiveness of our KGA-Net to prevent false negatives in diagnosis.  In Fig. 3, we illustrate video frames with their corresponding frame attention weights predicted by KGA-Net. Overall speaking, the frames with high attention weights do have clear image appearances for diagnosis. For example, the first three frames in Fig. 3(b) clearly demonstrate the edge micro-lobulation and irregular shapes, which lead to malignant judgment. Furthermore, we plot the relationships between the predicted attention values and the feature distances to the centers. As shown in Fig. 3(e), these two variables are linearly related, which indicates that KGA-Net the attention weights are effectively guided by the feature distances.The qualitative analysis proves the interpretability of our method, which will benefit clinical usage. Moreover, the attention weights reveal the importance of each frame for lesion diagnosis. Therefore, it can provide a new perspective for the keyframe extraction task of ultrasound videos."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,5.0,Conclusion,"We propose KGA-Net, a novel video classification model for breast ultrasound diagnosis. Our KGA-Net takes as input both the video data and image data to train the network. We propose the coherence loss to guide the training of the video model by the guidance of feature centers of the images. Our method significantly exceeds the performance of other competitive video baselines. The visualization of the attention weights validates the effectiveness and interpretability of our KGA-Net."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,1.0,Introduction,"Advances in deep learning have been witnessed in many research areas over the past decade. In medical field, automatic analysis of medical image data has actively been studied. In particular, segmentation which identify region of interest (RoI) in an automatic way is an essential medical imaging process. Thus, deep learning-based segmentation has been utilized in various medical domains such as brain, breast cancers, and colon polyps. Among the popular architectures, variants of U-Net have been widely adopted due to their effective encoderdecoder structure, proficient at capturing the characteristics of cells in images. Recently, it has been demonstrated that the attention modules [4,17,20] enable deep learning networks to better extract robust features, which can be applied in medical image segmentation to learn subtle medical features and achieve higher performance [14,16,18,21].However, as image-only training trains a model with pixels that constitute an image, there is a limit in extracting fine-grained information about a target object even if transfer learning is applied through a pre-trained model. Recently, to overcome this limitation, multi-modality studies have been conducted, aiming to enhance the expressive power of both text and image features. For instance, CLIP [12] used contrastive learning based on image-text pairs to learn the similarity between the image of an object and the text describing it, achieving significant performance gains in a variety of computer vision problems.The trend of text-image multi-modality-based research on image processing has extended to the medical field. [19] proposed a semantic matching loss that learns medical knowledge to supplement the disadvantages of CLIP that cannot capture uncertain medical semantic meaning. In [2], they trained to increase the similarity between the image and text by calculating their influence on each other as a weighted feature. For the segmentation task, LViT [10] generated the positional characteristics of lesions or target objects as text labels. Furthermore, it proposed a Double U-Shaped structure consisting of a U-Shaped ViT that combines image and text information and a U-Shaped CNN that produces a segmentation mask. However, when combining medical images with non-finegrained text information, noise can affect the outcome.In this paper, we propose a new text-guided cross-position attention module (CP AM T G ) that combines text and image. In a medical image, a position attention module (PAM) effectively learns subtle differences among pixels. We utilized PAM which calculates the influence among pixels of an image to capture the association between text and image. To this end, we converted the global text representation generated from the text encoder into a form, such as an image feature map, to create keys and values. The image feature map generated from an image encoder was used as a query. Learning the association between text and image enables us to learn positional information of targets in an image more effectively than existing models that learned multi-modality from medical images. CP AM T G showed an excellent segmentation performance in our comprehensive experiments on various medical images, such as cell, chest X-ray, and magnetic resonance image (MRI). In addition, by applying the proposed technique to the automatic RoI setting module for the deep learning-based diagnosis of sacroiliac arthritis, we confirmed that the proposed method could be effective when it is used in a practical application of computer-aided diagnosis.Our main contributions are as follows:-We devised a text-guided cross-position attention module (CP AM T G ) that efficiently combines text information with image feature maps. -We demonstrated the effect of CP AM T G on segmentation for various types of medical images. -For a practical computer-aided diagnosis system, we confirm the effectiveness of the proposed method in a deep learning-based sacroiliac arthritis diagnosis system. "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.0,Methods,"In this section, we propose text-guided segmentation model that can effectively learn the multi-modality of text and images. Figure 1 shows the overall architecture of the proposed model, which consists of an image encoder for generating a feature map from an input image, a text encoder for embedding a text describing the image, and a cross-attention module. The cross-attention module allows the text to serve as a guide for image segmentation by using the correlation between the global text representation and the image feature map. To achieve robust text encoding, we adopt a transformer [17] structure which performs well in Natural Language Processing (NLP). For image encoding and decoding, we employed U-Net, widely used as a backbone in medical image segmentation. To train our proposed model, we utilize a dataset consisting of image and text pairs."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.1,Configuration of Text-Image Encoder and Decoder,"As Transformer has demonstrated its effectiveness in handling the long-range dependency in sequential data through self-attention [1], it performs well in various fields requiring NLP or contextual information analysis of data. We used a Transformer (Encoder T ) to encode the semantic information of the text describing a medical image into a global text representation v T ∈ R 1×2C as v T = Encoder T (T ). Here, the text semantics (T ) can be a sentence indicating the location or characteristics of an interested region in an image such as a lesion shown in Fig. 1.To create a segmentation mask from medical images (I), we used U-Net [13] which has a relatively simple yet effective structure for biomedical image segmen- tation. U-Net operates as an end-to-end fully connected network-based model consisting of a convolutional encoder and decoder connected by skip connections. This architecture is particularly suitable for our purpose because it can be successfully trained on a small amount of data. In the proposed method, we used VGG-16 [15] as the encoder (Encoder I ) to obtain the image feature F I ∈ R C×H×W as F I = Encoder I (I) and the decoder (Decoder I ) that will generate the segmented image from the enhanced encoding vector obtained by the cross-position attention which will be described in the following subsection.The weights of text and image encoders were initialized by the weights of CLIP's pre-trained transformer and VGG16 pre-trained on ImageNet, respectively, and fine-tuned by a loss function for segmentation which will be described in Sect. 3."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.2,Text-Guided Cross Position Attention Module,"We introduce a text-guided cross-position attention module (CP AM T G ) that integrates cross-attention [3] with the position attention module (PAM) [5] to combine the semantic information of text and image. This module utilizes not only the image feature map from the image encoder but also the global text representation from the text encoder to learn the dependency between various characteristics of text and image. PAM models rich contextual relationships for local features generated from FCNs. It effectively captures spatial dependencies among pixels by generating keys, queries, and values from feature maps. By encoding broad contextual information into local features, and then adaptively gathering spatial contexts, PAM improves representation capability. In particular, this correlation analysis among pixels can effectively analyze medical images in which objects are relatively ambiguous compared to other types of natural images.In Fig. 2, we multiply the learnable parameter (l ∈ R 1×(HW ) ) by the global text representation (v T ) to match the dimension of the text feature with that of the image feature map as The text feature map F T is used as key and value, and the image feature map F I is used as a query to perform self-attention aswhere H Q , H K , and H V are convolution layers with a kernel size of 1, and Q, K, and V are queries, keys, and values for self-attention.Finally, by upsampling the low-dimensional CP AM T G obtained through crossattention of text and image together with skip-connection, more accurate segmentation prediction can express the detailed information of an object.3 Experiments"
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.1,Setup,"Medical Datasets. We evaluated CP AM T G using three datasets: MoNuSeg [8] dataset, QaTa-COV19 [6] dataset, and sacroiliac joint (SIJ) dataset. The first two datasets are the same benchmark datasets used in [10]. MoNuSeg [8] contains 30 digital microscopic tissue images of several patients and QaTa-COV19 are COVID-19 chest X-ray images. The ratio of training, validation, and test sets was the same as in [10]. SIJ is the dataset privately prepared for this study which consists of 804 MRI slices of nineteen healthy subjects and sixty patients diagnosed with axial spondyloarthritis. Among all MRI slices, we selected the gadoliniumenhanced fat-suppressed T1-weighted oblique coronal images, excluding the first and last several slices in which the pelvic bones did not appear, and added the text annotations for the slices.Training and Metrics. For a better training, data augmentation was used. We randomly rotated images by -20 • ∼ +20 • and conducted a horizontal flip with 0.5 probability for only the MoNuSeg and QaTa-COV19 datasets. The batch size and learning rate were set to 2 and 0.001, respectively. The loss function (L T ) for training is the sum of the binary cross-entropy loss (L BCE ) and the dice loss (L DICE ):The mDice and mIoU metrics, widely used to measure the performance of segmentation models, were used to evaluate the performance of object segmentation. For experiments, PyTorch (v1.7.0) were used on a computer with NVIDIA-V100 32 GB GPU."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.2,Segmentation Performance,"Table 1 presents the comparison of image segmentation performance among the proposed model and the U-Net [13], U-Net++ [22], Attention U-Net [11],MedT [16], and LViT [10] methods. Analyzing the results in Table 1, unlike natural image segmentation, the attention module-based method (Attention U-Net) and transformer-based method (MEdT) did not achieve significant performance gains compared to U-Net based methods (U-Net and U-Net++). By contrast, LViT and CP AM T G , which utilize both text and image information, significantly improved image segmentation performance because of multimodal complementarity, even for medical images with complex and ambiguous object boundaries. Furthermore, CP AM T G achieves a better performance by 1 to 3% than LViT [10] on all datasets. This means that the proposed CP AM T G helps to improve segmentation performance by allowing text information to serve as a guide for feature extraction for segmentation.Figure 3 shows the examples of segmentation masks obtained using each method. In Fig. 3, we marked the boundary of the target object with a red box and showed the ground truth masks for these objects in the last column. Similar to the analysis that can be derived from Table 1, Fig. 3 shows that CP AM T G and LViT, which use text information together for image segmentation, create a segmentation mask with more distinctive borders than other methods. In particular, with SIJ, CP AM T G accurately predicted the boundaries of even thin bone parts compared to LViT. Figure 3 also shows that even on the QaTa-COV19 and MoNuSeg datasets, CP AM T G predicted the most accurate segmentation masks (see the red box areas). From these results, we conjecture that the reasons for the performance improvement of CP AM T G are as follows. CP AM T G independently encodes the input text and image and then combines semantic information via a cross-attention module. Consequently, the two types of information (text and image) do not act as noise from each other, and CP AM T G achieves an improved performance compared to LViT."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.3,Ablation Study,"To validate the design of our proposed model, we perform an ablation study on position attention and CP AM T G . Specifically, for the SIJ dataset, we examined the effect of attention in extracting feature maps through comparison with backbone networks (U-Net) and PAM. In addition, we investigated whether text information about images serves as a guide in the position attention process for image segmentation by comparing it with CP AM T G . Table 2 summarizes the result of each case. As can be observed in Table 2, the performance of PAM was higher than that of the backbone. This indicates that PAM improves performance by learning associations between pixels for ambiguous targets, as in medical images. In addition, the best performance results of CP AM T G show that text information provided helpful information in an image segmentation process using the proposed model. "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.4,Application: Deep-Learning Based Disease Diagnosis,"In this section, we confirm the effectiveness of the proposed segmentation method through a practical bio-medical application as a deep learning-based active sacroiliitis diagnosis system. MRI is a representative means for early diagnosis of ""active sacroiliitis in axSpA"". As active sacroiliitis is a disease that occurs between the pelvic bone and sacral bone, when a MR slice is input, the diagnostic system first separates the area around the pelvic bone into an RoI patch and uses it as an input for the active sacroiliitis classification network [7]. However, even in the same pelvis, the shape of the bone shown in MR slices varies depending on the slice position of the MRI and the texture of the tissue around the bone is complex. This makes finding an accurate RoI a challenge.We segmented the pelvic bones in MRI slices using the proposed method to construct a fully automatic deep learning-based active sacroiliitis diagnosis system, including RoI settings from MRI input images. Figure 4 shows the results of generating RoI patches by dividing the pelvic bone from MRI slices using the proposed method. As presented in Table 3, compared to the case of using the original MRI image without the RoI setting, using the hand-crafted RoI patch [9] showed an average of 7% higher performance in recall, precision, and f1. It is noticeable that the automatically set RoI patch showed similar or better performance than the manual RoI patch for each measurement. This indicates that the proposed method can be effectively utilized in practical applications of computer-aided diagnosis."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,4.0,Conclusion,"In this study, we developed a new text-guided cross-attention module (CP AM T G ) that learns text and image information together. The proposed model has a composite structure of position attention and cross-attention in that the key and value are from text data, and the query is created from the image. We use a learnable parameter to convert text features into a tensor of the same dimension as the image feature map to combine text and image information effectively. By calculating the association between the reshaped global text representation and each component of the image feature map, the proposed method outperformed image segmentation performance compared to previous studies using both text and image or image-only training method. We also confirmed that it could be utilized for a deep-learning-based sacroiliac arthritis diagnosis system, one of the use cases for practical medical applications. The proposed method can be further used in various medical applications."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,,Table 3 .,Fig. 4. Generating RoI.
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,1.0,Introduction,"The spreading digitalisation of pathology labs has enabled the development of deep learning (DL) tools that can assist pathologists in their daily tasks. However, supervised DL methods require detailed annotations in whole-slide images (WSIs) which is time-consuming, expensive and prone to inter-observer disagreements [6]. Multiple instance learning (MIL) alleviates the need for detailed annotations and has seen increased adoption in recent years. MIL approaches have proven to work well in academic research on histopathology data [1,17,29] as well as in commercial applications [26]. Most MIL methods for digital pathology employ an attention mechanism as it increases the reliability of the algorithms, which is essential for successful clinical adoption [14].Domain shift in DL occurs when the data distributions of testing and training differs [20,34]. This remains a significant obstacle to the deployment of DL applications in clinical practice [7]. To address this problem previous work either use domain adaptation when data from the target domain is available [32], or domain generalisation when the target data is unavailable [34]. Domain adaptation has been explored in the MIL setting too [22,23,27]. However, it may not be feasible to perform an explicit domain adaptation, and an already adapted model could still experience problems with domain shifts. Hence, it is important to provide indications of the expected performance on a target dataset without requiring annotations [5,25]. Another related topic is out-of-distribution (OOD) detection [33] which aims to detect individual samples that are OOD, in contrast to our objective of estimating a difference of expected performances between some datasets. For supervised algorithms, techniques of uncertainty estimation have been used to measure the effect of domain shift [4,15,18] and to improve the robustness of predictions [19,21,30]. However, the reliability of uncertainty estimates can also be negatively affected by domain shifts [11,31]. Alternatively, a drop in performance can be estimated by comparing the model's softmax outputs [8] or some hidden features [24,28] acquired on in-domain and domain shift datasets. Although such methods have been demonstrated for supervised algorithms, as far as we know no previous work has explored domain shift in the specific context of MIL algorithms. Hence, it is not clear how well they will work in such a scenario.In this work, we evaluate an attention-based MIL model on unseen data from a new hospital and propose a way to quantify the domain shift severity. The model is trained to perform binary classification of WSIs from lymph nodes of breast cancer patients. We split the data from the new hospital into several subsets to investigate clinically realistic scenarios triggering different levels of domain shift. We show that our proposed unsupervised metric for quantifying domain shift correlates best with the changes in performance, in comparison to multiple baselines. The approach of validating a MIL algorithm in a new site without collecting new labels can greatly reduce the cost and time of quality assurance efforts and ensure that the models perform as expected in a variety of settings. The novel contributions of our work can be summarised as:1. Proposing an unsupervised metric named Fréchet Domain Distance (FDD) for quantifying the effects of domain shift in attention-based MIL; 2. Showing how FDD can help to identify subsets of patient cases for which MIL performance is worse than reported on the in-domain test data; 3. Comparing the effectiveness of using uncertainty estimation versus learnt representations for domain shift detection in MIL."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.0,Methods,"Our experiments center on an MIL algorithm with attention developed for classification in digital pathology. The two main components of our domain shift quantification approach are the selection of MIL model features to include and the similarity metric to use, described below."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.1,MIL Method,"As the MIL method for our investigation, we chose the clustering-constrainedattention MIL (CLAM) [17] because it well represents an architecture of MIL with attention, meaning that our approach can equally be applied to many other such methods."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.2,MIL Features,"We explored several different feature sets that can be extracted from the attention-based MIL framework: learnt embedding of the instances (referred to as patch features), and penultimate layer features (penultimate features). A study is conducted to determine the best choices for type and amount of patch features. As a baseline, we take a mean over all patches ignoring their attention scores (mean patch features). Alternatively, the patch features can be selected based on the attention score assigned to them. Positive evidence or Negative evidence are defined as the K patch features that have the K highest or lowest attention scores, respectively. Combined evidence is a combination of an equal number of patch features with the highest and lowest attention scores. To test if the reduction of the number of features in itself has a positive effect on domain shift quantification, we also compare with K randomly selected patch features."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.3,Fréchet Domain Distance,"Fréchet Inception Distance (FID) [10] is commonly used to measure similarity between real and synthetically generated data. Inspired by FID, we propose a metric named Fréchet Domain Distance (FDD) for evaluating if a model is experiencing a drop in performance on some new dataset. The Fréchet distance (FD) between two multivariate Gaussian variables with means µ 1 , µ 2 and covariance matrices C 1 , C 2 is defined as [3]:We are interested in using the FD for measuring the domain shift between different WSI datasets X d . To this end, we extract features from the MIL model applied to all the WSIs in X d , and arrange these in a feature matrix F DD K uses the K aggregated positive evidence features, but in the results we also compare to M d described from penultimate features, mean patch features, and the other evidence feature selection strategies."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,3.0,Datasets,"Grand Challenge Camelyon data [16]  potentially large shift as some patients have already started neoadjuvant treatment as well as the tissue may be affected from the procedure of sentinel lymph node removal. 2a. 207 WSIs with ductal carcinoma (83 WSIs with metastases): a small shift as it is the most common type of carcinoma and relatively easy to diagnose. 2b. 68 WSIs with lobular carcinoma (28 WSIs with metastases): potentially large shift as it is a rare type of carcinoma and relatively difficult to diagnose.The datasets of lobular and ductal carcinomas each contain 50 % of WSIs from sentinel and axillary lymph node procedures. The sentinel/axillary division is motivated by the differing DL prediction performance on such subsets, as observed by Jarkman et al. [13]. Moreover, discussions with pathologists led to the conclusion that it is clinically relevant to evaluate the performance difference between ductal and lobular carcinoma. Our method is intended to avoid requiring dedicated WSI labelling efforts. We deem that the information needed to do this type of subset divisions would be available without labelling since the patient cases in a clinical setting would already contain such information. All datasets are publicly available to be used in legal and ethical medical diagnostics research."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.0,Experiments,"The goal of the study is to evaluate how well F DD K and the baseline methods correlate with the drop in classification performance of attention-based MIL caused by several potential sources of domain shifts. In this section, we describe the experiments we conducted."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.1,MIL Training,"We trained, with default settings, 10 CLAM models to classify WSIs of breast cancer metastases using a 10-fold cross-validation (CV) on the training data. The test data was kept the same for all 10 models. The classification performance is evaluated using the area under receiver operating characteristic curve (ROC-AUC) and Matthews correlation coefficient (MCC) [2]. Following the conclusions of [2] that MCC well represents the full confusion matrix and the fact that in clinical practice a threshold needs to be set for a classification decision, MCC is used as a primary metric of performance for domain shift analysis while ROC-AUC is reported for completeness. Whereas extremely large variations in label prevalence could reduce the reliability of the MCC metric, this is not the case here as label prevalence is similar (35-45%) in our test datasets. For Deep ensemble [15] we trained 4 additional CLAM models for each of the 10 CV folds."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.2,Domain Shift Quantification,"As there is no related work on domain shift detection in the MIL setting, we selected methods developed for supervised algorithms as baselines:-The model's accumulated uncertainty between two datasets. Deep ensemble [15] (DE) and Difference in Confidence with entropy [8] (DoC) compare the mean entropy over all data points. DE uses an ensemble to estimate bettercalibrated uncertainty than the single model in DoC. -The accumulated confidence of a model across two datasets. DoC [8] can be measured on the mean softmax scores of two datasets. A large difference indicates a potential drop in performance. -The hidden features produced by an algorithm. Representation Shift [28] (RS) has shown promising results in detecting domain shift in convolutional neural networks and it is the method most similar to FDD. However, it is not trivial which hidden features of MIL that are most suitable for this task, and we evaluate several options (see Sect. 2.2) with both methods.For all possible pairs of Camelyon and the other test datasets, and for the 10 CV models, we compute the domain shift measures and compare them to the observed drop in performance. The effectiveness is evaluated by Pearson correlation and visual investigation of corresponding scatter plots. All results are reported as mean and standard deviation over the 10-fold CV."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,5.0,Results,"The first part of our results is the performance of the WSI classification task across the subsets, summarized in Table 1. While showing similar trends, there is some discrepancy in the level of domain shift represented by the datasets due to the differences between the MCC and ROC-AUC measures.As we deemed MCC to better represent the clinical use situation (see Sect. 4.1), it was used for our further evaluations. Overall, the performance is in line with previously published work [17,29]. We observe the largest domain shift in terms of MCC on axillary nodes followed by lobular carcinoma and full BRLN datasets. There seems to be no negative effect from processing the sentinel nodes data. CLAM models achieved better performance on ductal carcinoma compared to the in-domain Camelyon test data.Table 2 summarises the Pearson correlation between the change in performance, i.e., the MCC difference between Camelyon and other test datasets, and the domain shift measures for the same pairs. F DD 64 outperforms the baselines substantially, and has the smallest standard deviation. Figure 1 shows how individual drop in performance of model-dataset combinations are related to the F DD 64 metric. For most models detecting larger drop in performance (> 0.05) is easier on axillary lymph nodes data than on any other analysed dataset.  A study of the number and type of MIL attention-based features and FD and RS metrics is presented in Fig. 2. The baseline of randomly selecting patch features resulted in the worst outcome on domain shift detection. Negative evidence with FD achieved high Pearson correlation when K = 4. However, the results were among the worst with any other number of K. Both combined and positive evidence achieved peak performance of 0.68 (0.17) and 0.70 (0.13), respectively, when FD and K = 64 were used. We conclude that in our setup the best and most reliable performance of domain shift quantification is achieved by positive evidence with FD and K = 64, i.e. F DD 64 ."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,6.0,Discussion and Conclusion,"MIL is Affected by Domain Shift. Some previous work claim that MIL is more robust to domain shift as it is trained on more data due to the reduced costs of data annotation [1,17]. We argue that domain shift will still be a factor to consider as an algorithm deployed in clinical practice is likely to encounter unseen varieties of data. However, it may require more effort to determine what type of changes in data distribution are critical. Our results show that domain shift is present between the WSIs from the same hospital (Camelyon data) and another medical centre (BRLN data). However, as clinically relevant subsets of BRLN data are analysed, stark differences in performance and reliability (indicated by the standard deviation) are revealed. Therefore, having a reliable metric for unsupervised domain shift quantification could bring value for evaluating an algorithm at a new site. Domain Shift Detection for Supervised DL Struggles for MIL. An important question is whether can we apply existing techniques from supervised learning algorithms in the MIL setting. The evaluated baselines that use uncertainty and confidence aggregation for domain shift detection, i.e., DE and DoC, showed poor ability to estimate the experienced drop in performance (see Table 2). It is known that supervised DL often suffers from overconfident predictions [9]. This could be a potential cause for the observed poor results by DE and DoC in our experiments. Further investigation on how to improve calibration could help to boost the applicability of uncertainty and confidence measures.The Proposed F DD K Measure Outperforms Alternatives. The highest Pearson correlation between change in performance and a distance metric is achieved by Fréchet distance with 64 positive evidence features, F DD 64 (see Table 2). RS 64 approach performed better than uncertainty/confidence-based methods but still was substantially worse than F DD 64 . Furthermore, F DD 64 resulted in the smallest standard deviation which is an important indicator of the reliability of the metric. Interestingly, using penultimate layer features, which combine all patch features and attention scores, resulted in much worse outcome than F DD 64 , 0.61 versus 0.70. Thus, it seems a critical component in domain shift measurement in attention-based MIL is to correctly make use of the attention scores. From Fig. 1 we can see that if we further investigated all modeldataset combinations that resulted in F DD 64 above 0.5, we would detect many cases with a drop in performance larger than 0.05. However, the drop is easier to detect on axillary and lobular datasets compared to others. An interesting aspect is that the performance was better for the out-of-domain ductal subset compared to in-domain Camelyon WSIs. In practical applications, it may be a problem when the domain shift quantification cannot separate between shifts having positive or negative effect on performance. Such differentiation could be the topic of future work."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,,Conclusion.,"We carried out a study on how clinically realistic domain shifts affect attention-based MIL for digital pathology. The results show that domain shift may raise challenges in MIL algorithms. Furthermore, there is a clear benefit of using attention for feature selection and our proposed F DD K metric for quantification of expected performance drop. Hence, F DD K could aid care providers and vendors in ensuring safe deployment and operation of attention-based MIL in pathology laboratories."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 16.
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,1.0,Introduction,"Discovering disease characteristics from medical images can yield important insights into pathological changes. However, subtle disease characteristics are difficult to discover from imaging data. The reduction in brain volume due to Alzheimer's Disease (AD) is one such example. In this case, comparing co-registered AD and Cognitively Normal (CN) reference magnetic resonance images (MRIs) is the most effective way to reveal subtle AD characteristics. Unfortunately, collecting such co-registered reference images at scale is not practical.Generative Adversarial Networks (GANs) has the potential to synthesize the reference images required for disease discovery. Specifically, style-based generative frameworks [4] could be suitable as they employ mechanisms to ""stylize"" a share anatomical structure into different disease states. However, as we will show, many GAN frameworks (style and non-style based) are unable to produce satisfactory reference AD and CN images for AD characteristic discovery.In this work, we propose Disease Discovery GAN (DiDiGAN), a specialized style-based network for disease characteristic discovery on medical image data. On the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset [7], DiDi-GAN can create not only reference AD and CN MRIs, but also smooth animated transitions between AD and CN. The highlights of DiDiGAN are:-A learnt disease manifold that captures and encodes AD and CN disease state distributions. Style codes sampled from this manifold control the disease expression in the output reference images. -The disease manifold is naturally smooth such that seamless transitions of AD to CN are possible via style interpolation. -The generator uses a low-resolution input image as the source anatomical constraint (to provide coarse structural guidance), it is low-resolution to leave sufficient room for the generator to synthesize disease features. -Anti-aliasing as a key mechanism to maintain anatomical correspondence across generated reference images. Without anti-aliasing, reference images exhibit inconsistent anatomical structures invalidating visual comparisons. -DiDiGAN is a weakly-supervised framework requiring only class labels for images rather than pixel (voxel) labels.DiDiGAN learns to generate representative reference AD and CN images by training on 2D coronal brain slices labelled with AD and CN. The generated reference images and animations clearly show systematic changes in the hippocampus, ventricle, and cortex areas while maintaining anatomical correspondence.The discovered characteristics are corroborated by independent tests which solidify the strengths of the findings. There has not been a previous work that can i) produce visualizations on par with DiDiGAN's and ii) learn a dedicated manifold for disease characteristic discovery."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,2.0,Related Work,"Various generative methods have attempted to synthesize pathological changes for disease modelling. HerstonNet [18] and Bernal et al. [2] synthesize brain MRIs with atrophy, but they rely on induced atrophy by altering the Partial Volume (PV)-maps and segmentation maps. Hence they do not learn disease characteristics on their own. Ravi et al. [17] and ADESyn [8] learn AD characteristics from data, and they can synthesize brain images at different stages of AD. However, ADESyn fails to capture ventricle enlargement which is a key AD while Ravi et al. failed to capture hippocampus shrinkage (which are key AD characteristics). DBCE [16] learns to deform a healthy brain image into an AD image. However, the resulting atrophy is not sufficiently distinct for AD characteristic visualization. Saliency maps must be further computed to reveal AD regions. Xia et al. [23] proposed a GAN for ageing brain synthesis. While the results clearly depict ventricle enlargement for advanced ages, other AD characteristics remain unclear as the images show significant anatomical inconsistencies. Another limitation of the above methods is that disease features are intertwined with anatomical features. For disease modelling, a dedicated mechanism (such as a latent space) to store disease features in an explorable manner would be desirable. The objective to learn a dedicated disease latent space naturally aligns with style-based [4] frameworks, which have mostly been used for medical image translation [1,11]. Style-based frameworks have the potential to enhance generative disease modelling. Fetty et al. [5] have shown that StyleGANs can learn a dedicated latent space (manifold) of medical image features. This latent space can be smoothly interpolated to manipulate synthesis. Other works like StarGAN [4] have shown that this latent can be disentangled from the content. Similarly, in medical image analysis, Chartsias et al. [3] have shown modality information can be disentangled from anatomical content for multi-modal image translation. These findings motivate DiDiGAN to use style to learn a manifold of disease features and then apply the manifold to ""stylized"" an input anatomical constraint into AD and CN images. However, StyleGAN3 [9] discovered that common style-based methods suffer from aliasing [24]. When interpolating the manifold, aliasing causes undesirable texture disruptions, and it threatens to disrupt anatomical structures if applied to disease studies. Thus, applying anti-aliasing (as per Shannon-Nyquist theorem) is critical for DiDiGAN."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,3.0,Methods,"DiDiGAN projects different disease states onto a common anatomical structure to form disease reference images, and the output images must maintain a consistent anatomical structure. A detailed architecture diagram is shown in Fig. 1."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Disease Style w c and Anatomical Constraint,"x AC : Let x c denote the images in a dataset and c ∈ {c 1 , c 2 ...c n } are the disease classifications for the images, the generator G's aim is to synthesise reference disease images x c that share a consistent anatomical structure. This is done by injecting disease styles w c into a shared anatomical constraint x AC to produce the output x c = G(w c , x AC ). w c is created following w c = M (emb(c), z) where emb is a learned 512-d embedding specific to each class, z is a 512-d N (0, 1) noise vector and M is a 2-layer mapping network made up of dense layers. Like StyleGAN, the style code space w c is a learned manifold (of disease states), and it can be interpolated for smooth disease states transitions. The reason to include z is to introduce variability such that w c forms a surface instead of a fixed point. x AC is a 4× down-sampled version of x c which only enforces coarse anatomical structure similarities between the reference images, and it lacks resolution on purpose allowing room for disease features from w c to manifest. As already discussed, full anatomical correspondence requires anti-aliasing to enforce."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Alias-Free Style Generator G:,"Generator G is alias-free as the feature maps are processed carefully following the Shannon-Nyquist sampling theorem. Like StyleGAN 3, G's convolutional (AA Conv) layers apply anti-aliasing to the leakyrelu activation function. This process involves first interpolating discrete feature maps to continuous domains. Then, low-pass filtering is used to remove offending frequencies. Finally, the activation function is applied, and the signal is resampled back to the discrete domain [9]. Architecture wise G is a progressively up-sampling decoder Convolutional Neural Network (CNN) with a starting resolution of 4×4 and an output resolution of 256×256. There are 13 convolutional blocks that gradually perform upsampling and synthesis. All the convolutional layers are modulated convolutions [4] to inject disease features w c . The anatomical constraint x AC is introduced by concatenating it to the input of every block, and we observed more stable training using this method."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Multi-head Discriminator D:,"The discriminator D is an encoder architecture with consecutive convolution and down-sampling. There are three prediction heads D R/F , D c , and D AC which predict a realness logit, disease classification, and reconstruction of constraint x AC , respectively. The main loss functions are these outputs are 1) the standard non-saturating GAN loss L R/F for generating realistic images, 2) the adversarial classification loss L c which supervises the conditioning on c 3) the anatomical reconstruction loss L AC ensuring the generated reference images share the high-level structure of the input anatomical constraint. These losses are as follows:minwhere x c is a real image of class c, x c is a fake image of the same class, w c = M (emb(c), z), and x AC is a down-sampled version of x c . There are also other standard regularization loss functions including the pathlength regularization L pl [9], R1 gradient penalty L R1 [14] and an explicit diversification loss L div = -||G(w, c, z 1 ) -G(w, c, z 2 )|| to prevent the manifold from converging to fixed points. The total loss L total is then defined as  [21], skull stripping using SPM [22]) were applied to the entire dataset. For each 3D scan, the center 40 coronal slices were extracted and zero-padded to 256 × 256 pixels. Down-sampled versions of these slices (64 × 64) were used for X AC . The network was implemented in Pytorch and trained for 60,000 steps (32 images per step) using Adam optimizer. The training hardware is a 32 GB Nvidia V100 graphics card to support a batch size of 32. The baseline methods include a range of style and non-style-based image translation methods. These methods can readily treat reference image generation as unpaired image translation where, for example, a CN brain is translated to an AD version of the same brain. All the methods were trained and evaluated on the exact same data and splits.Quantitative Image Quality Evaluation Using Perceptual Metrics. The quality of the generated reference images (based on the test set, ignoring classes) from all the methods is assessed using Frechet Inception Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS), and Kernel Inception Distance (KID) as they do not require paired data. As Table 1 shows, DiDiGAN generated the most realistic reference images compared to the baselines. MUNIT [6] and CUT [15] failed to reach convergence hence the metrics are omitted.   "
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,AD Characteristic Extraction and Visualization Using DiDiGAN.,"Figure 2 provides AD visualizations by generating reference images with DiDi-GAN and the baseline methods. Since DiDiGAN's uses stylization, it can freely generate correlated AD and CN pairs using the manifold. Comparison between the reference pairs reveals AD characteristics [19,20] such as hippocampus shrinkage, ventricle enlargement, and the thinning of cortical structures. The consistent anatomical structure also facilitates the computation of Jacobian (shrinkage and expansion) maps. For DiDiGAN, the bright regions in DiDi-GAN's Jacobian map more clearly highlight significant hippocampus shrinkage and ventricle enlargement from CN to AD. More examples are shown in Fig. 3.In these examples, we also manually segmented the ventricle and hippocampus, and the notable size changes marked in the figure. Anti-aliasing is crucial for maintaining anatomical correspondence across DiDiGAN's reference images. As an ablation study, a DiDiGAN without anti-aliasing was trained. The generated AD and CN pairs exhibit significant anatomical disturbances (see S1 for examples).Comparing AD Characteristic Visualization with Baselines. The baseline methods produce reference images via direct image translation from CN to an AD reference. The results in Fig. 2 suggest StarGANv2 [4], StyleGAN-SD [10] and UNIT [12] were unable to convey useful AD findings as they struggled to maintain anatomical correspondence. CycleGAN's [25] simpler architecture helped avoid anatomical disruptions. However, the generated AD is almost pixel-identical to the input CN aside from the contrast difference (hence the high FID). The other two methods MUNIT [6] and CUT [15] could not reach convergence on the ADNI dataset despite the documentation being carefully followed.Disease Manifold Formation and Interpolation. DiDiGAN's disease manifold formation is visualized using UMAP [13] where style codes sampled from the manifold are reduced to 2-d from 512-d. Figure 4 shows a 2D cluster containing 10,000 AD and 10,000 CN style codes. While the input disease class is a discrete value, the manifold automatically maps it to a disease distribution with AD distributed on the left and CN on the right. The overlap between the two classes suggests AD is a progressive disease. The smoothness of the manifold facilitates smooth CN-AD transition animations by interpolating between an AD style code and a CN style code. Practically, this animation provides a more intuitive visualization of AD pathological changes.Corroborating Tests: Due to the lack of paired medical images as ground truths, we perform the following tests (I, II, III) to verify DiDiGAN's findings. I: DiDiGAN was independently applied to the sagittal view slices, and similar hippocampus shrinkage and ventricle enlargement were clearly observed (see examples S2). These independent findings corroborate the coronal view visualizations produced by DiDiGAN's reference images. II: SPM segmentation was applied to the test set and DiDiGAN's reference images (generated based on the test set) to help compare the magnitudes of brain volume loss. For each image, the grey matter and white matter pixels were treated as brain mass. As Fig. 5 shows, DiDiGAN's AD reference images consistently show systematic brain mass reduction across all 40 slice positions. This trend is consistent with that of the real data. The box plot suggests an average brain mass reduction of 17.3% compared to the 12.1% of the real data. Limitations. DiDiGAN is an initial step for style-based disease characteristic discovery. In the future, a more rigours examination of the manifold is needed to fully understand the features and trends learned especially for clinical applications. For example, longitudinal trends and atrophy patterns among the AD population. Although DiDiGAN discovered brain tissue shrinkage as indicated by the SPM segmentation analysis, the learned magnitude is different from that of the real data. This is likely a limitation of GANs as the learnt distribution may not exactly match the real data in all aspects. Nonetheless, DiDiGAN's findings could serve as disease characteristic proposals. Additionally, more datasets and diseases should be tested to more thoroughly assess DiDiGAN's generalizability."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,5.0,Conclusion,"DiDiGAN demonstrated disease characteristic discovery by generating reference images that clearly depict relevant pathological features. The main technical novelties of DiDiGAN are i) the use of a learned disease manifold to manipulate disease states AD ii) the ability to interpolate the manifold to enhance visualization and iii) mechanisms including the structural constraint and anti-aliasing to maintain anatomical correspondence without direct registration. In the experiments involving the ADNI dataset, DiDiGAN discovered key AD features such as hippocampus shrinkage, ventricular enlargement, and cortex atrophy where other frameworks failed. DiDiGAN shows potential to aid disease characteristic discovery across time of other chronic diseases such as osteoarthritis."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"Liver cancer is the third leading cause of cancer death world-wide in 2020 [14]. Early detection and accurate diagnosis of liver tumors may improve overall patient outcomes, in which imaging plays a key role [11]. Computed tomography (CT) is one of the most important imaging modalities for liver tumors. Dynamic contrast-enhanced (DCE) CT is widely used for diagnostics, but it requires iodine contrast injection which can cause reaction and potential risks in patients. Recently, non-contrast (NC) CT scans are gaining attention as they are cheaper and safer to acquire, thus can be potential tools for opportunistic tumor screening [18,20]. Meanwhile, finding and diagnosing tumors in NC CTs is also extremely challenging because of the poor contrast between tumors and normal tissues compared to those in DCE CTs. Prior works on pancreas [18] and esophagus [20] have shown that latest deep learning techniques can detect subtle texture and shape changes in NC CT that even human eyes may miss. Thus, we aim to investigate the performance of liver tumor segmentation and classification in NC CTs. Such an approach will be helpful to discover asymptomatic incidental tumors [12] from routine NC CT scans indicated for general diagnostic purposes at no additional cost and radiation exposure. After an incidental tumor is found, the patient may undergo further imaging examination such as a multi-phase DCE CT for differential diagnosis [11], which can provide useful discriminative information such as the vascularity of lesions and the pattern of contrast agent enhancement [19]. Liver is largest solid organ in body and is the site of many tumor types [11]. Therefore, accurate tumor type classification is important for the decision of treatment plans and prognosis.Many researchers have developed algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver tumors in CT to help radiologists improve their accuracy and efficiency. For example, public datasets such as the Liver Tumor Segmentation Benchmark (LiTS) [1] fostered a series of works aiming to segment liver tumors with improved convolutional neural network (CNN) backbones [9,13] and lesion edge information [15]. LiTS only has single-phase CTs (venous phase). Several studies investigated methods to exploit multi-phase CT by methods such as hetero-phase fusion [5] and modality-aware mutual learning [23]. There are few work discussing liver tumor analysis in NC CT [5]. Besides lesion segmentation, CNN-based lesion classification algorithms have been studied to distinguish common lesion types [19,21,25].In this paper, we build a comprehensive framework to address both tumor screening and diagnosis. (1) Tumor screening involves finding tumor patients in a large pool of healthy subjects and patients. Most existing works in tumor segmentation and detection did not explicitly consider it since their training and testing images are all tumor patients. Such models may generate false positives in real-world screening scenario when facing diverse tumor-free images. We collect a large-scale dataset with both tumor and non-tumor subjects, where the non-tumor subjects includes not only healthy ones, but also patients with various diffuse liver diseases such as steatosis and hepatitis to improve the robustness of the algorithm. (2) Most works studied liver tumor segmentation alone without differentiating tumor types, while a few works classify liver tumors on cropped tumor patches [19,21,25]. Meanwhile, we learn tumor segmentation and classification with one network using an instance segmentation framework [3]. We train two networks for NC and multi-phase DCE CTs, respectively. (3) For evaluation, previous segmentation works typically use pixel-level metrics such as Dice coefficient. Such metrics cannot reflect the lesion-level accuracy (how many lesion instances are correctly detected and classified) and may bias to large lesions when a patient has multiple tumors. Patient-level metrics (e.g. classifying whether a subject has malignant tumors) are also useful for treatment recommendation in clinical practice [18,20]. Therefore, we assess our algorithm thoroughly with pixel, lesion, and patient-level metrics.Algorithms for liver tumor segmentation have focused on improving the feature extraction backbone of a fully-convolutional CNN [9,13,15,23]. The pixelwise segmentation architectures may not be optimal for lesion and patient-level evaluation metrics since they cannot consider a lesion or an image holistically. Recently, a series of mask transformer algorithms [3,4,17] have emerged in the computer vision community and achieved the state-of-the-art performance in instance segmentation tasks. In brief, they use object queries to interact with image feature maps and with each other to produce mask and class predictions for each instance. Inspired by them, we propose a novel end-to-end framework named Pixel-Lesion-pAtient Network (PLAN) for lesion segmentation and classification, as well as patient classification. It contains three branches with bottomup cooperation: The segmentation map from the pixel branch helps to initialize the lesion branch, which is an improved mask transformer aiming to segment and classify each lesion; The patient branch aggregates information from the whole image and predicts image-level labels of each lesion type, with regularization terms to encourage consistency with the lesion branch.We collected a large-scale multi-phase dataset containing 810 non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types are extensively annotated based on pathological reports. On the non-contrast tumor screening and diagnosis task, PLAN achieves 95.0%, 96.4%, and 0.965 in patient-level sensitivity, specificity, and average AUC for malignant and benign patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnU-Net [8]. On multi-phase DCE CT, our lesion-level detection precision, recall, and classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnU-Net [8] and Mask2Former [3]. We further conduct a reader study on a holdout set of 250 cases. Our algorithm is on par with a senior radiologist (16 yrs experience), showing the clinical significance of our results. Our codes will be made public upon institutional approval."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.1,Preliminary on Mask Transformer,"Mask transformers are a series of latest works achieving superior accuracy on various segmentation tasks [3,4,17,22]. Different from traditional fullyconvolutional segmentators [8] that predict a class label for each pixel, mask transformers predict a class label and a binary mask for each object. Take Mask2Former [3] as an example. It includes a pixel encoder and a pixel decoder that extract a high-resolution pixel embedding tensor P ∈ R M ×D×H×W from the image, where M is the embedding dimension, D × H × W is the shape of the 3D image. A group of Q learnable feature vectors {q i ∈ R M } Q i=1 are randomly initialized as object queries. They are processed by a transformer decoder to interact with multi-scale image features and each other using cross and self-attention operations. After processing, each query is supposed to contain information of one object, which can be used to predict the class probability c ∈ R C+1 of the object. Here C is the number of object classes, and we add 1 to indicate an additional ""no-object"" class if the query does not match with any object. In training, Mask2Former uses bipartite matching [2] to assign each query to a ground-truth object (or ""no-object""). Multiplying q i with P gives the binary mask m i ∈ R D×H×W of object i. During inference, the class and mask predictions of all queries can be merged by matrix multiplication to obtain the final semantic segmentation result Ŷ ∈ R C×D×H×W . We refer readers to [3] for more details.Mask transformers have various advantages when applied to our task. They can classify a lesion as a whole instead of classifying each pixel, thus can view each lesion holistically. Cross-attention is used to aggregate global features for each lesion. Inter-lesion relation can also be exploited by self-attention operations. In liver CT, inter-lesion relation is diagnostically useful, e.g., metastases and cysts are often multiple. Therefore, We pioneer mask transformers' adaptation for lesion segmentation and classification in 3D medical images. Given a groundtruth or a predicted lesion mask image, we perform connected component (CC) analysis and treat each CC as a lesion instance for training and evaluation."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"Our goal is to segment the mask and classify the type of each tumor in a liver CT. We also hope to make patient-level diagnoses for each CT scan. PLAN is inspired by Mask2Former [3] with three key improvements: (1) A pixel branch is added to provide anchor queries to the lesion branch. (2) The lesion branch is composed of the transformer decoder in Mask2Former, and we improve its segmentation loss to enhance recall of small lesions. (3) A patient branch is attached to make dedicated image-level predictions with a proposed lesion-patient consistency loss. Our framework is shown in Fig. 1.Pixel Branch and Anchor Queries. The pixel branch is a convolutional layer after the pixel decoder and learns to predict pixel-wise segmentation maps similar to traditional segmentators. We do CC analysis to the predicted mask to extract lesion instances, and then average the pixel embeddings inside each predicted lesion to obtain a feature vector. The feature vectors are regarded as anchor queries and work the same way as the randomly initialized queries in the lesion branch. Compared to the random queries in the original Mask2Former, the anchor queries contain prior information of the lesions to be segmented, helping the lesion branch to match with the lesion targets more easily [10].Lesion Branch and Foreground-Enhanced Sampling Loss. Similar to Mask2Former, the lesion branch predicts a binary mask and a class label for each query, see Fig. 1. Mask2Former calculates its segmentation loss on K sampled pixels instead of on the whole image, which is shown to both improve accuracy and reduce GPU memory usage [3]. However, in lesion segmentation, some tumors are very small compared to the whole 3D image. The importance sampling strategy [3] can hardly select any foreground pixels in such cases, so the loss only contains background pixels, degrading the segmentation recall of small lesions. We propose a simple approach to remedy this issue by sampling an extra n foreground pixels for each lesion.Patient Branch. A patient-level diagnosis is useful for triage. For example, diagnosing the subject as normal, benign, or malignant will result in completely different treatments [24]. Intuitively, we can also infer patient-level labels from segmentation results by checking if there is any lesion in the predicted mask. However, certain tumors are often related to signs outside the tumor, e.g. hepatocellular carcinoma and cirrhosis, cholangiocarcinoma and bile duct dilatation, etc. We equip PLAN with a dedicated patient branch to aggregate such global information to make better patient-level prediction. Since one patient can have multiple liver tumors of different types, in our problem, we give each image several hierarchical binary labels. The first label classifies normal and tumor subjects (whether the image contains any tumor); The second and third labels indicate the existence of respectively benign and malignant tumors; The rest C labels suggest the existence of C fine-grained types of tumors. We employ the dual-path transformer block [17] to fuse multi-scale features from the pixel encoder and decoder to generate a feature map, followed by global average pooling and a linear classification layer to predict the C + 3 labels.A lesion-patient consistency loss is further proposed to encourage coherence of the lesion and patient-level predictions. Inspired by multi-instance learning [6], we compute a pseudo patient-level prediction c ∈ R C from the lesion-level predictions by max-pooling the class probability of each class across all lesion queries (discarding the no-object class). We also have the probability vector from the patient branch p ∈ R C corresponding to the C fine-grained classes. Then, we compute the L2 loss between them:The overall loss of PLAN is listed in Eq. 1, where L pixel is the combined crossentropy (CE) and Dice loss for the pixel branch as in nnU-Net [8]; L lesion-class is the CE loss [3] for lesion classification in the lesion branch; L lesion-mask is the combined CE and Dice loss [3] for binary lesion segmentation in the lesion branch with the foreground-enhanced sampling strategy; L patient is the binary CE loss for the multi-label classification task in the patient branch."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"Data. Our dataset contains 810 normal subjects and 939 patients with liver tumors. Each normal subject has a non-contrast (NC) CT, while each patient has a dynamic contrast-enhanced (DCE) CT scan with NC, arterial, and venous phases. We use DEEDS [7] to register NC and arterial phases to the venous phase, and then invite a senior radiologist with 10 years of experience to annotate on the multi-phase CTs using CT Labeler [16]. The 3D mask and the type of all liver tumors are annotated based on pathological reports and magnetic resonance scans if necessary. Eight tumor types are considered in our study: hepatocellular carcinoma (HCC), intrahepatic cholangiocarcinoma (ICC), metastasis (meta), hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (FNH), cyst, and others (all other tumor types). If a lesion's type cannot be determined according to image signs [11] and pathology, it will be marked as ""unknown"" and ignored in training and evaluation. In total, 4010 tumor instances are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . Detailed statistics and examples of the lesions are shown in the supplementary material. We train two separate networks for NC and DCE CTs. In the former setting, both normal and patient data are used and randomly split into 1149 training, 100 validation, and 500 testing. In the latter one, only patient data are used with 641 training, 100 validation, and 200 testing. Another hold-out set of 150 patients and 100 normal CTs are used for reader study to compare our accuracy with two radiologists. Implementation Details. Each CT is resampled to 0.7×0.7×5mm in spacing. We first train an nnU-Net on public datasets to segment liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then crop the liver region to train PLAN. To help PLAN differentiate liver tumors and other organs, we train the network to segment both tumors and organs using the predicted organ labels. PLAN is built on top of the nnU-Net framework [8]. Its pixel encoder is a U-Net encoder, whereas its pixel decoder is a light-weight feature pyramid network [3]. The lesion branch incorporates three transformer decoder blocks with masked attention [3] which use feature maps of strides 16, 8, 4 from the pixel decoder. The number of random queries is Q = 20; the embedding dimension is M = 64; the number of sampled pixels is K = 12544 [3], foreground pixels n = 3; the loss weight is 0.1 for the no-object class while 1 for other classes in the lesion branch [3]. The weights in Eq. 1 areWe use the RAdam optimizer with an initial learning rate of 0.0001. Each training batch contains two patches of size 256 × 256 × 24. For DCE CT, the three phases form a 3-channel image as the network input. Extensive data augmentation is applied including random cropping, scaling, flipping, elastic deformation, and brightness adjustment [8].During training, we first pretrain the backbone and the pixel branch for 500 epochs, and then train the whole network for another 500 epochs.Patient-Level Results. This paper has three major goals: tumor screening in NC CT (classifying a subject as normal or tumor), preliminary diagnosis in NC CT (predicting the existence of malignant and benign tumors), and fine-grained diagnosis in DCE CT (predicting the existence of 8 tumor types). Among the 8 tumor types, HCC, ICC, meta, and hepato are malignant; heman, FNH, and cyst are benign. ""Others"" can be either malignant or benign, thus are excluded in the preliminary diagnosis task. The NC test set contains 198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases which may have larger image noise, artifact, ascites, diffuse liver diseases such as hepatitis and steatosis. These cases are used to test the robustness of the model in real-world screening scenario with diverse tumor-free images. We compare PLAN with a widely-used strong baseline, nnU-Net [8]. The recent mask transformer, Mask2Former [3], is also adapted to 3D for comparison. For the baselines, patient-level labels are inferred from their predicted masks by counting lesion pixels. As displayed in Table 1, PLAN achieves the best accuracy on all tasks, especially in NC preliminary diagnosis tasks, which demonstrates the effectiveness of its dedicated patient branch that can explicitly aggregate features from the whole image.Lesion and Pixel-Level Results. In lesion-level evaluation, we treat a prediction as a true positive if its overlap with a ground-truth lesion is >0.2 in Dice.  Lesions smaller than 3 mm in radius are ignored. As shown in Table 2, the pixellevel accuracy of nnU-Net and PLAN are comparable, but PLAN's lesion-level accuracy is consistently higher than nnU-Net. In this work, we focus more on patient and lesion-level metrics. Although NC images have low contrast, they can still be used to segment and classify lesions with ∼ 80% precision, recall, and classification accuracy. It implies the potential of NC CT, which has been understudied in previous works. Mask2Former has higher precision but lower recall in NC CT, especially for small lesions, while PLAN achieves the best recall using the foreground-enhanced sampling loss. Both PLAN and Mask2Former achieve better classification accuracy, which illustrates the mask transformer architecture is good at lesion-level classification."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,,Comparison with Radiologists.,"In the reader study, we invited a senior radiologist with 16 years of experience in liver imaging, and a junior radiologist with 2 years of experience. They first read the NC CT of all subjects and provided a diagnosis of normal, benign, or malignant. Then, they read the DCE scans and provided a diagnosis of the 8 tumor types. We consider patients with only one tumor type in this study. Their reading process is without time constraint. In Table 3 and Fig. 2, all methods get good specificity probably because the normal subjects are completely healthy. Our model achieves comparable accuracy with the senior radiologist but outperforms the junior one by a large margin in sensitivity and classification accuracy. An ablation study for our method is shown in Table 4. It can be seen that our proposed anchor queries produced by the pixel branch, FES loss, and lesionpatient consistency loss are useful for the final performance. The efficacy of the lesion and patient branches has been analyzed above based on the lesion and patient-level results. Due to space limit, we will show the accuracy for each tumor type and more qualitative examples in the supplementary material.Comparison with Literature. In the pixel level, we obtain Dice scores of 77.2% and 84.2% using NC and DCE CTs, respectively. The current state of the art (SOTA) of LiTS [1] achieved 82.2% in Dice using CTs in venous phase; [23] achieved 81.3% in Dice using DCE CT of two phases. In the lesion level, our precision and recall are 80.1% and 81.9% for NC CT, 92.2% and 89.0% for DCE CT, at 20% overlap. [25] achieved 83% and 93% for DCE CT. SOTA of LiTS achieved 49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes, achieving 84% accuracy for DCE and 49% for NC CT. We classify lesions into 8 classes with 85.9% accuracy for DCE and 78.5% for NC CT. In the patient level, [5] achieved AUC=0.75 in NC CT tumor screening, while our AUC is 0.985. In summary, our results are superior or comparable to existing works. "
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 8.
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,1.0,Introduction,"Nephropathy is a progressive and incurable disease with high mortality, occurring commonly in the general adult population, with a world-wide prevalence of 10% [11]. Therefore, early detection and treatment is of pivotal importance, as it can prevent the death or inevitable renal failure that requires renal dialysis or replacement therapy. Due to the low cost and sensitivity for certain lesion [20], immunofluorescence (IF) images have been increasingly used in the diagnostic process of nephropathy. Most recently, benefiting from the development of deep learning, a couple of deep neural networks (DNNs) have been developed for nephropathy related tasks on IF images [7,8,13,18]. For instance, Ligabue et al. [8] proposed a residual convolutional neural network (CNN) for IF nephropathy reporting. Similarly, in [18], a DNN-based model with a pre-segmentation module and a classification module was introduced to automatically detect the different glomerulus in IF images. Kitamura et al. [7] designed a CNN structure for diabetic nephropathy (DN) diagnosis on IF images, and further visualized where the CNN focused on for diagnosis.There exist only a few IF image based DNN methods, probably because the properties of IF images are complicated and have not been fully exploited. Different from the natural and other medical images, a IF sequence usually include multiple modalities from different types of fluorescent [2]. On the other hand, the collected modalities of a IF sequence are usually incomplete, due to the medical reasons and acquiring processes. This leads to various modality combination in a IF dataset, therefore significantly reducing the learning efficiency of a DNN. More importantly, the correlation between different IF modalities and nephropathy categories is complicated. For instance, as shown in Fig. 1, the experiments in this paper find that anti-neutrophil cytoplasmic antibodies (ANCA) disease is strongly related to the modalities of Immunoglobulin G (IgG) and Immunoglobulin A (IgA). Meanwhile, other modalities like Complement 3 (C3) and Fibronectin (Fib) sometimes mislead the DNN and thus degrade the ANCA diagnosis performance. Unfortunately, all above DNN methods assume different modalities have the equal effect on the diagnosis task, neglecting the medical prior of the importance of individual modality."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Modality,"To address above issue, this paper proposes a novel customized multi-teacher knowledge distillation framework for nephropathy diagnosis on IF images. Specifically, we establish a large-scale IF image dataset including 7 types of nephropathy from 1,582 patients. By mining our dataset, we conduct experiments to explore the importance of a individual modality contributing to each nephropathy, as the empirical medical prior (see Fig. 1). Then, we develop a multi-teacher knowledge distillation framework, in which the knowledge is transferred from the teacher networks trained by individual modalities. Different from the traditional knowledge distillation [5,9], we propose a customized framework with a recruitment module, which learns to select the ""best"" teacher networks based on the medical priors. Benefiting from this, the student network can effectively learns from the individual modalities, thus achieving better overall performance for the clinical IF sequence with incomplete modalities. We show the effectiveness of the proposed method over our dataset and another external dataset for nephropathy diagnosis. In summary, the main contributions of this paper are three-fold.-We establish a large-scale IF dataset containing 7 nephropathy categories and  hospital including 69 IF sequences with 348 images. Note that our work is conducted according to the Declaration of Helsinki. Compared with the existing IF image datasets [7,8], our dataset collects most patients including most categories of nephropathy. In the experiments, our main dataset is randomly divided into training and test sets at a ratio of 4 to 1, while the external set is only used for test."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,2.2,Dataset Analysis,"Based on our main dataset, we further conduct data analysis to obtain the following findings about the relationship between different modalities and nephropathy."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Finding 1: The proportions of each IF modality vary greatly in different nephropathy.,"Analysis: As introduced above, the collected modalities of a IF sequence are usually incomplete. This is partially due to the importance of each IF modality for the specific nephropathy, since the patient may not have the anti-body of the useless fluorescent. Therefore, we count the proportions of 7 IF modalities on each nephropathy over 1,582 IF sequences in our main set. As shown in Fig. 2, there exists a significant inconsistency of the modality proportions between different nephropathy. For instance, IgAN does not have the modality of C1q, while LN has almost equal proportions for all IF modalities. Besides, the proportions of a certain IF modality vary a lot in different nephropathy. For example, the proportions of IgM are 10.9%, 1.4%, 11.9%, 0, 0.7%, 13.5%, 0.7% in 7 modalities, respectively. The above analysis completes the analysis of Finding 1."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,"Finding 2: For certain combinations, the single-modality IF image achieves better diagnosis accuracy than multi-modality IF sequences over DNN models.","Analysis: To explore the impact of each modality in DNN models, we conduct experiments to evaluate the effectiveness of single IF modality for diagnosing each nephropathy. Specifically, we compare the nephropathy diagnosis accuracy of the same DNN model, when trained and tested over multi-modality IF sequences versus single-modality IF images. First, two widely-used classification models (ResNet-18 [4] and ECANet [15]) are implemented for 7-class nephropathy diagnosis. Then, we construct 7 dataset pairs from our main set, according to each IF modality. For each data pair, the DNN models are trained and tested with multi-modality and single-modality, respectively, the diagnosis accuracy of which is recorded as Acc m and Acc s . Subsequently, these two kinds of accuracy are compared by calculating the error weight E as follows,Thus, the higher error weight indicates that the single-modality can achieve more accurate diagnosis compared with using all modalities. Table 1 tabulates the error weights between each pair of IF modality and nephropathy, over ResNet-18 and ECANet. As shown, for certain combinations, such as IgG to ANCA, IgA to DN, and Fib to IgAN, the single-modality can even achieve better performance than using multi-modality IF sequences. Besides, the phenomenon is consistent over multiple DNN models. This implies that there exists a correlation for each single modality for contributing to the diagnosis on each nephropathy.    medical prior from our findings. Then, we transfer the fused knowledge to optimize the student network, via the developed multi-level distillation losses. This way, the student network can dynamically learn from the individual modalities, and finally achieve much better performance overs multi-modality IF sequence."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.1,Nephropathy Diagnosis Network,"Here, we introduce the detailed structure of the nephropathy diagnosis network, as the backbone structure for both student and teacher networks. Note that the backbone structure is flexible, and we implement 4 advanced DNNs in the experimental section. Taking ResNet-18 for the student network as an example, as illustrated in Fig. 3, ResNet-18 is implemented by 4 residual blocks, to extract the features with multiple levels. Each residual block consists of two 3×3 convolutional layers, two batch normalization layers and a ReLU activation layer. Given the input IF sequence X, the multi-level features {FS i } 4 i=1 and prediction logits l s of the student network N s (•) can be obtained as(2) In (7), Res(•), Conv 1×1 (•), and MLP(•) indicate the residual block, 1 × 1 convolutional layer, and multilayer perceptron, respectively. As the supervision of N -class nephropathy diagnosis, the cross-entropy loss is calculated upon the one-hot ground-truth diagnosis label ls and the output predicted logits l s :(3)"
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.2,Customized Recruitment Module,"A customized recruitment module is developed to adaptively select the effective teacher networks, on the top of the medical priors from our findings. As shown in Fig. 3, the recruitment module is composed with medical and learnable parts.For the medical prior part, we first construct the adjacency matrix A ∈ R M ×N , in which M and N indicate the number of IF modalities and nephropathy. In A, the corresponding element is set as 1, when the IF modality is found to have positive influence over 2 DNN models in Table 1. Then, given the ground-truth nephropathy label ls ∈ R N ×1 , the medical prior weights can be obtained as:where α and β are rescaling hyper-parameters. Additional, for the learnable part, the last level feature of the student network FS 4 is passed through a max pooling layer to obtain the representation v s ∈ R K×1 for student network, in which K is channel number of FS 4 . Let θ and {v t,i } M i=1 denote the learnable k-element rescaling weights and M teacher network representations. Then, the correlation between student network and teacher network can be formulated as the inner product between vector representations v t,i , θ v s . In summary, the learnable importance weights w l can be presented aswhere V is the matrix of {v t,i } M i=1 , while denotes element-wise multiplication. Note that V and θ are initialized with 1, and optimized during training. Finally, a overall modality importance weight w t can be obtained as"
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.3,Multi-level Knowledge Distillation,"Here, a multi-level knowledge distillation is developed between teacher and student networks. Based on the modality importance weight w t from our recruitment module, the multi-level features {FC i } M i=1 and predicted logits l c of are fused from multiple teacher networks:In above equation, w j t , l j t , FT j i are the importance weight, predicted logits, and the i-th level features for the j-th teacher networks, respectively. After that, to transfer the learned knowledge from teacher to student network, the logit loss is introduced by calculating the Kullback-Leibler (KL) divergence between the fused l c and predicted logits l s from the student network:Meanwhile, in order to make the student network fully learn from diagnosis processes of teacher networks, mean square error (MSE) losses are conducted on each level of fused {FC i } 4 i=1 and student network features {FS i } 4 i=1 : Finally, the overall loss function for the student network can be written aswhere λ fea , λ logits and λ cls are the hyper-parameters for balancing single losses."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.1,Experimental Settings,"In our experiments, all IF images are resized to 512 × 512 for consistency. During training, the parameters are updated by Adam optimizer with an initial learning rate of 0.0001. Then, each teacher network is pre-trained for 70 epochs, and then the student network with our distillation method is trained for 460 epochs. Finally, our and 8 other compared methods are trained over the training set of main set, and evaluated over the main test set and the external set, by adopting 3 evaluation metrics of accuracy, kappa and F1-score."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.2,Evaluation on Knowledge Distillation,"To evaluate the effectiveness of the proposed customized knowledge distillation method, we implement it over 4 different backbone models of ResNet-18 [4], ResNet-101 [4], DenseNet-121 [6] and Inception-V3 [12]. Figure 4 compares the nephropathy diagnosis results of 4 backbone models after conducting our distillation method. As shown, all backbone models obtain significant improvements when applying the proposed distillation methods. For instance, benefiting the transferred knowledge from single-modality, DenseNet-121 improves 0.053, 0.065, and 0.051 in accuracy, kappa and F1-score over our main dataset. This validates the effectiveness of the proposed method. Note that, we select ResNet-18 as the final model, due to its best performance among 4 backbone structures."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.3,Comparisons with the State-of-the-Art Models,"We evaluate the diagnosis performance of our method over our main dataset, compared with 10 DNN based methods, i.e., ShuffleNet [19], EfficientNet [14], GCNet [1], ECANet [15], KNet [7], MANet [18], Hao et al. [3], Ada-CCFNet  [16], MCL [17] and ITRD [10]. Among them, KNet [7], MANet [18], Hao et al. [3] and Ada-CCFNet [16] are nephropathy diagnosis methods, while MCL [17] and ITRD [10] are knowledge distillation methods. All compared methods are re-trained with the same settings as ours. As shown in Table 2, our proposed method achieves the best performance on IF based nephropathy diagnosis over 3 metrics and 2 datasets. For example, compared with the second best method, our method can improve 0.035/0.043, 0.030/0.060 and 0.023/0.006 in accuracy, kappa and F1-score over our main/external dataset, respectively. This validates the superior performance and generalization ability of the proposed method."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.4,Ablation Study,"We ablate different components of our method to thoroughly analyze their effects on nephropathy diagnosis. Specifically, the accuracy degrades 0.028, 0.023, and 0.031, when ablating medical prior, learnable weights, and recruitment module (equal distillation), respectively. Besides, other ablations, such as the number of teacher networks and distillation loss, are also analyzed. The ablation experiments are reported in Fig. reffinding1 of the supplementary."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,5.0,Conclusion,"In this paper, we propose a customized knowledge distillation method for IF based nephropathy diagnosis. Different from the existing methods that averagely integrate information of different IF modalities, we propose a knowledge distillation framework to transfer knowledge from the trained single-modality teacher networks to a multi-modality student network. In particular, a recruitment module and multi-level knowledge distillation are developed to dynamically select and fuse the knowledge from teacher networks. The extensive experiments on several backbone networks verify the effectiveness of our proposed framework."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_51.
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,1.0,Introduction,"The clinical courses of neurodegenerative pathologies such as Alzheimer's or Parkinson's Diseases span multiple years and encompass intricate evolution of patients' cognitive abilities, physiological biomarkers and brain structure. Longitudinal studies are an essential tool for clinicians to uncover the diseases' mechanisms. In such studies, biomarkers and cognitive scores of patients are repeatedly measured at different times and need to be analyzed together, usually with a two-sided scope. First, to describe the general process at play across a whole cohort of patients: this is population-level modelling and allows to describe the average course of the disease. A second layer aims at explaining and predicting the variability observed among individuals: this is personalized-level modelling.Mixed-effect frameworks are widely adopted to address these multi-layered prospects, offering to disentangle fixed effects (population level) from random effects (individual level) to explain the variability of the disease. Linear mixedeffects models are the simplest instances of such models. Generalized linear and non-linear mixed-models are now often prefered to account for the neurodegenerative diseases' peculiarities, and most state of the art disease progression models (e.g. [2,14,16]) belong to these categories. They are indeed better suited to describe phenomena whose complex dynamic spans multiple years. They have been used with success to describe the natural history of diseases [8] or make individualized predictions, for instance to enrich clinical trials [11]. A general formulation is as follows, where η is a non linear mapping between timepoints and clinical markers, parametrized by fixed-effects α and random effects β i (methods differ by the chosen non-linearity η and how α and β parametrize the disease course):Inter-patient variability is thereby modelled through random perturbations β i around a fixed reference α. However, it is known that some of this clinical variability between patients is explained by external factors (and thus hardly explained entirely by random perturbations). Genetic mutations or external factors such as gender, family history, education or socio-economics levels can influence the course of pathologies. Accumulating evidence suggests that the variability induced by such covariates stems from general mechanisms shared across the population, for instance in Alzheimer's and Parkison's diseases [3,6,7,10]. In the presented models, observed covariates c i are not taken into account and only the repeated observations y i are modelled as a function of the patient's ages t i . Thus, random effects might be such that E [β i | c i ] = 0. This shows that some signal present in covariates to explain the progression of the disease has not been fully exploited.Our contribution is to propose a slight change in this mixed-effect paradigm to allow non-linear models to also be influenced by these variables. Instead of estimating a fixed effect α (parametrizing the average disease course) as well as random effects β i , we introduce a link function f ϕ that can predict, given a set of covariates c i (e.g. sex, education level, SNP arrays, genetic risk scores), an expected trajectory of the disease conditionned by these covariates.The main difference between the standard approach and our method is that the previously introduced fixed-effects α are now estimated for each subject as a deterministic function of their covariates f ϕ (c i ). It also differs from accounting for the heterogeneity through hierarchical progression models [13,17]) since covariates are, in our case, supervisingly used during model calibration and used to navigate through a continuum of disease models, instead of having defined clusters.We demonstrate the value of this approach by adapting a general modelling framework, namely a non-linear Bayesian model: the Disease Course Mapping (DCM) [16]. We show that accounting for time-independent covariates in the longitudinal modelling with this approach can be done in a reasonable statistical setting. A stochastic estimation algorithm can be devised and we propose an instantiation and implementation of our model, which we validate first on synthetic. We then use clinical data from the Alzheimer's Disease Neuroimaging (ADNI) cohort and further demonstrate the clinical interest of the method by estimating new associations between genetics and disease dynamics."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.0,Method,We derive here an algorithm that learns to model repeated observations while accounting for the heterogeneity explained by additional covariates.
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.1,A Generic Mixed-Effects Geometric Model,"In their seminal paper [16], Schirrati et al. introduced a generic framework to model a dataset y i,j of multimodal longitudinal measurements. Here y i,j is a vector of N biomarkers measured for the i-th subject at their j-th visiti.e. at age t i,j . Each observation y i,j is assumed to lie on Riemannian submanifold (M, g) of R N . The average course of the disease is posited to be such that individual progressions stem from a geodesic trajectory γ 0 on the manifold surface. Geodesic equations imply that γ 0 is entirely characterized by its initial position p 0 ∈ M and speed v 0 ∈ T p0 M (tangent space of M at p 0 ) at time t 0 . Individual trajectories are obtained from this reference trajectory γ 0 via a temporal reparametrization t → ψ i (t), used to derive what we name a disease age and enables registering patient's chronological ages onto a common disease timeline. Spatial effects w i are applied to the reference trajectory thanks to an exp-parallelization procedure that identifiably deforms geodesics in the manifold space. We denote η wi γ0 the resulting geodesic. We refer to [16] for extensive details on the geometric properties of these operations (such as commutativity and identifiability of both temporal and spatial effects).The choice of the manifold's metric shapes the geodesic trajectories and thus the disease model [4,15,16]. Clinical knowledge of Alzheimer's Disease suggests that sigmoid shapes as sound candidates to model biomarkers' evolutions (see [5] for clinical considerations, or [12] for the logistic dynamic of imaging-derived features such as brain-averaged protein loads backed by prion-like diffusion hypothesis). We therefore consider a product-metric g p such that geodesic are sigmoids:which gives the trajectories of Eq. ( 1). The resulting trajectories and the geometric interpretation of the (v 0 , p 0 , t 0 ) parameters are also presented in Fig. 1.(1)Biomarker 2 = = = = Fig. 1. A two feature model, with geodesic trajectories on the manifold (left) and the biomarkers observation space (right). This provides the intuition over the effect of the initial position p0 and the initial velocity v0 at time t0."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.2,Covariate Association and Statistical Framework,"We provide here a statistical instantiation of the previous geometric model. As described, given a geodesic trajectory γ 0 (fully specified by its position p 0 and speed v 0 at initial time t 0 ), and a set of random effect ψ i and w i , individual trajectories of an individual i observed at times (t i,j ) j are modelled by the curve η wi γ0 (ψ i (t i,j )). We propose that γ 0 (which represents the reference disease course, as a fixed-effect of the model) is to be computed for each subject i from the measured covariates c i as:where f belongs to a parametrized family of functions and ϕ are its parameters treated as the new fixed-effect of the model. The individual effects to register this computed γ 0,(i) onto observations are characterized by two random effects: an acceleration factor ξ i and a time-shift τ i such that ψ i (t) = e ξi t -t 0,(i) -τ i . On top of these are space-shifts w i ∈ R N , computed thanks to an ICA: w i = As i , where A is a latent matrix of independent directions (fixed effect) and s i is the corresponding individual latent source vector (random effect).Our hierarchical statistical model treats the fixed and random effects as a set of latent variables z which is the reunion of the population and individual variables z pop = {ϕ, A} andWe posit the following priors on these latent parameters, where θ hyper = {σ ϕ , σ A } are fixed hyperparameters and θ model = ϕ, A, σ 2 τ , σ 2 ξ , σ 2 are the parameters of the model to be estimated:A non-informative prior is used over these model parameters due to the lack of a-priori knowledge. We seek to maximize a posteriori the joint-likelihood under the following additive Gaussian noise modellingprior on model parameters (taken non informative)It can be shown that the model's likelihood function lies in the curved exponential family. That is there exist two smooth functions Φ and Ψ functions of θ model and a measurable sufficient statistics function S(y, z) of the data and the latent realizations such that log q(y, z, θ model ) factors as:This allows estimating our model with a Monte-Carlo Markov-Chain Stochastic Approximation version of the Expectation Maximization algorithm (MCMC-SAEM) while enjoying theoretical guarantees of convergence [1]. The expectation phase is therefore built upon a sampling scheme to sample from the posterior distribution of the latent parameters (namely Metropolis-Hastings within Gibbs sampler). The maximization phase follows update rules established by finding critical points of θ → -Φ(θ) + Sp , Ψ(θ) ( Sp is the stochastic approximation of the sufficient statistics built at step p), which yields analytic expressions.We choose to parametrize the link function as a linear mapping between covariates and dynamic parameters f ϕ (c i ) = ϕ slope • c i + ϕ intercept . This will provide an interpretable model to explain the general processes linking covariates to dynamic features such as the base pace of the disease or average onset time. The coefficients of ϕ that correspond to the mapping between the covariates c i and v 0 measure how much a given covariate impact the progression speed of each feature, and can be analyzed easily. Model parameters are initialized by setting their intercept to the models learned by a regular DCM model without considering covariates, while latent parameters are initialized at random."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,3.1,Simulated Data,"We used the generative abilities of the DCM [8] to simulate multimodal longitudinal datasets with covariates influencing the dynamic of the progression. To this end, we fixed some reference models corresponding each to a slightly different pure form of a fictional disease. Then, covariates were simulated either:-as binary covariates that directly dictated which hardcoded model is used to simulate the repeated measurements (covariate thought as a mutation-status or sex for instance). -as continuous covariates, influencing the simulated progression by using them as convex coefficients in a combination of the reference models. The covariates are seen as continuous risk factors of following one form or another.Our simulated datasets typically included 500 subjects with an average of 5 visits and an average follow-up duration of about 5±2 years and a measurement noise of around 5%. Such an experiment is summarized in Fig. 2, where three continuous covariates were simulated on the [0, 1] range, the first one being a risk to develop a motor form of the disease, the second one a memory-form risk and the third a covariate without any influence on the disease.In this example, our calibrated model correctly matches each covariate to its simulated effect. For instance: the coefficients of the link function related to the disease initial speed v 0 (we refer to Fig. 1 for its interpretation) associated with the memory-risk covariate show an acceleration of the decline in memory (multiplicative factor of 1.32 [1.15, 1.62] -credible interval at 95%) contrasted to the two other biomarkers (factor of 0.79 [0.73, 0.90] for the motor and 0.89 [0.80, 1.01] for the language). These intervals are represented in Fig . 2b. These two other features are slowed down relatively to the memory in order for the model to capture the change in the slope-ratio of different features on the fixed effects. If we translate the effects of these coefficients into effects on slope-ratio, we obtain indeed obtain that the ratio between memory-speed and motor-speed goes from 4.22 [3.54, 5.02] (no extra memory-risk) to 7.05 [5.54, 11.26] (maximum extra memory-risk). The ground truth change of slope (from the reference models) was from 4.48 (no particular risk) to 9.19 (full risk) and is therefore covered by our credible intervals.Similarly, the coefficients associated with the irrelevant covariate did not capture any significant effect (factors of 1.03 [0.94, 1.12], 1.05 [0.98, 1.12] and 1.04 [0.97, 1.11] for the memory, motor and language features), which validates the ability of the model to discard covariates without influence on the disease dynamic."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,3.2,Multimodal Clinical Data,"Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). ADNI was launched in 2003 as a public-private partnership led by Michael W. Weiner, MD. For up-to-date information, see https://www.adni-info.org. We selected subjects that eventually converted to an MCI or AD stage during their follow-up. This amounted to 1440 patients for a total of 9343 visits. The follow-up duration was 4.069 (± 3.190) years, with a baseline age of 73.683 (± 7.508) years old. We processed and included biomarkers relevant to monitor AD progression:-Two cognitive scores: the Mini-Mental State Exam (MMSE) and the AD Assessment Scale-Cognitive (ADAS-Cog). We normalized and inverted them so that they both cover the [0, 1] interval, (1 being the highest abnormality). -Hippocampus and Ventricles volumes, measured by structural T1 MRI and normalized by patient's Intracranial Volume (ICV). As for the cognitive scores, these measurements were rescaled to [0, 1] interval. -Contrasted PET imaging derived brain-averaged amyloid β42 and phosphorylated τ proteins loads, also rescaled to [0, 1]. We also sample an irrelevant covariate that is never used to modulate the disease course. In (a) is the resulting model, which we can visualize for any combination of covariates (two combinations are presented). We also plot the credible intervals (at 95%) for the coefficients linking covariates to the speed of progression on each feature (multiplicative effect, thus 1.0 stands for no influence while a coefficient of 1.5 stands for an expected progression speed greater by 50%)."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,,APOE-ε4,". We calibrated our model by including a covariate known to modulate Alzheimer's Disease course, namely the patient's APOE mutation status. The results of this model are showcased in Fig. 3. It shows that the APOE mutation, which is a known risk factor for AD, has a clear effect on the disease dynamic: in the obtained disease course map, the mutation is associated with earlier and faster abnormalities on most biomarkers. We also investigate the learned linked function f ϕ and its coefficients dictating the interaction between the covariates and the speed of progression. This is presented in Fig. 3. This showcases how the contribution of the APOE to the speed of progression (as in the coefficient linking the covariate to the coordinates of v 0 ) is different among biomarkers.Features can be grouped into a cognitive scores group, more impacted than the other features by the mutation (  Fig. 4. Analysis of the learned interaction between some of the SNP included in the analysis and the speed of progression of each of the 6 measured features. Some SNPs present no significant interaction with the progression speed of any of the variables (e.g. rs138727474T) to SNPs that are associated with a group of feature (e.g. cognitive domain for rs114812713C) or single features (e.g. rs11932324A or rs286604821A, in either a protective or risk-inducing direction).SNP Associations. We selected a subset of 69 Single Nucleotide Polymorphisms (SNP) among the top associations with AD diagnosis from a reference Genome-Wide Association Study (GWAS) [9]. We included them in our model as covariates. The results suggest that being associated with the diagnosis does not inform a priori on the influence of each SNP on the disease course. In Fig. 4 we show that, even though all these SNP were selected for being significatively associated with the diagnosis, they can exhibit differences in their association with the disease dynamic."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,4.0,Conclusion,"We proposed a framework to adapt a state of the art Bayesian non-linear mixedeffect disease progression model to capture the effects of external covariates into the disease dynamic. We implemented an estimation algorithm, and show that it reliably provides new interpretable measures of interaction between covariates and the disease course. For instance, we recover the (clinically known) association between the APOE-ε4 mutation and cognitive dysfunction. In particular, its use on genetic data (either single mutation status or SNP arrays) could help to go beyond associations with the sole diagnosis and provide complementary tools to GWAS."
Self-supervised Learning for Endoscopic Video Analysis,1.0,Introduction,"Endoscopic operations are minimally invasive medical procedures which allow physicians to examine inner body organs and cavities. During an endoscopy, a thin, flexible tube with a tiny camera is inserted into the body through a small orifice or incision. It is used to diagnose and treat a variety of conditions, including ulcers, polyps, tumors, and inflammation. Over 250 million endoscopic procedures are performed each year globally and 80 million in the United States, signifying the crucial role of endoscopy in clinical research and care.A cardinal challenge in performing endoscopy is the limited field of view which hinders navigation and proper visual assessment, potentially leading to high detection miss-rate, incorrect diagnosis or insufficient treatment. These limitations have fostered the development of computer-aided systems based on artificial intelligence (AI), resulting in unprecedented performance over a broad range of clinical applications [10,11,17,[23][24][25]. Yet the success of such AI systems heavily relies on acquiring annotated data which requires experts of specific knowledge, leading to an expensive, prolonged process. In the last few years, Self-Supervised Learning (SSL [5][6][7][8]) has been shown to be a revolutionary strategy for unsupervised representation learning, eliminating the need to manually annotate vast quantities of data. Training large models on sizable unlabeled data via SSL leads to powerful representations which are effective for downstream tasks with few labels. However, research in endoscopic video analysis has only scratched the surface of SSL which remains largely unexplored.This study introduces Masked Siamese Networks (MSNs [2]), a prominent SSL framework, into endoscopic video analysis where we focus on laparoscopy and colonoscopy. We first experiment solely on public datasets, Cholec80 [32] and PolypsSet [33], demonstrating performance on-par with the top results reported in the literature. Yet, the power of SSL lies in large data regimes. Therefore, to exploit MSNs to their full extent, we collect and build two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700 videos (>23M frames) and 14, 000 videos (>2M frames) respectively. Through extensive experiments, we find that scaling the data size necessitates scaling the model architecture, leading to state-of-the-art performance in surgical phase recognition of laparoscopic procedures, as well as in polyp characterization of colonoscopic videos. Furthermore, the proposed approach exhibits robust generalization, yielding better performance with only 50% of the annotated data, compared with standard supervised learning using the complete labeled dataset. This shows the potential to reduce significantly the need for expensive annotated medical data."
Self-supervised Learning for Endoscopic Video Analysis,2.0,Background and Related Work,"There exist a wide variety of endoscopic applications. Here, we focus on colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic procedures. Specifically, our study addresses two important common tasks, described below.Cholecystectomy Phase Recognition. Cholecystectomy is the surgical removal of the gallbladder using small incisions and specialized instruments. It is a common procedure performed to treat gallstones, inflammation, or other conditions affecting the gallbladder. Phase recognition in surgical videos is an important task that aims to improve surgical workflow and efficiency. Apart from measuring quality and monitoring adverse event, this task also serves in facilitating education, statistical analysis, and evaluating surgical performance. Furthermore, the ability to recognize phases allows real-time monitoring and decision-making assistance during surgery, thus improving patient safety and outcomes. AI solutions have shown remarkable performance in recognizing surgical phases of cholecystectomy procedures [17,18,32]; however, they typically require large labelled training datasets. As an alternative, SSL methods have been developed [12,28,30], however, these are early-days methods that based on heuristic, often require external information and leads to sub-optimal performance. A recent work [27] presented an extensive analysis of modern SSL techniques for surgical computer vision, yet on relatively small laparoscopic datasets.Optical Polyp Characterization. Colorectal cancer (CRC) remains a critical health concern and significant financial burden worldwide. Optical colonoscopy is the standard of care screening procedure for preventing CRC through the identification and removal of polyps [3]. According to colonoscopy guidelines, all identified polyps must be removed and histologically evaluated regardless of their malignant nature. Optical biopsy enables practitioners to remove pre-cancerous adenoma polyps or leave distal hyperplastic polyps in situ without the need for pathology examination, by visually predicting histology. However, this technique is highly dependent on operator expertise [14]. This limitation has motivated the development of AI systems for automatic optical biopsy, allowing non-experts to also effectively perform optical biopsy during polyp management. In recent years, various AI systems have been developed to this end [1,19]. However, training such automatic optical biopsy systems relies on a large body of annotated data, while SSL has not been investigated in this context, to the best of our knowledge.3 Self-supervised Learning for Endoscopy SSL approaches have produced impressive results recently [5][6][7][8], relying on two key factors: (i) effective algorithms for unsupervised learning and (ii) training on large-scale datasets. Here, we first describe Masked Siamese Networks [2], our chosen SSL framework. Additionally, we present our large-scale data collection (see Fig. 2). Through extensive experiments in Sect. 4, we show that training MSNs on these substantial datasets unlocks their potential, yielding effective representations that transfer well to public laparoscopy and colonoscopy datasets."
Self-supervised Learning for Endoscopic Video Analysis,3.1,Masked Siamese Networks,"SSL has become an active research area, giving rise to efficient learning methods such as SimCLR [7], SwAV [5] and DINO [6]. Recently, Masked Siamese Networks [2] have set a new state-of-the-art among SSL methods on the ImageNet benchmark [29], with a particular focus on the low data regime. This is of great interest for us since our downstream datasets are typically of small size [32,33]. We briefly describe MSNs below and refer the reader to [2] for further details.During pretraining, on each image x i ∈ R n of a mini-batch of B ≥ 1 samples (e.g. laparoscopic images) we apply two sets of random augmentations to generate anchor and target views, denoted by x a i and x t i respectively. We convert each view into a sequence of non-overlapping patches and perform an additional masking (""random"" or ""focal"" styles) step on the anchor view by randomly discarding some of its patches. The resultant anchor and target sequences are used as inputs to their respective image encoders f θ a and f θ t . Both encoders share the same Vision Transformer (ViT [16]) architecture where the parameters θ t of the target encoder are updated via an exponential moving average of the anchor encoder parameters θ a . The outputs of the networks are the representation vectors z a i ∈ R d and z t i ∈ R d , corresponding to the [CLS] tokens of the networks. The similarity between each view and a series of K > 1 learnable prototypes is then computed, and the results undergo a softmax operation to yield the following probabilities p a i = sof tmaxwhere 0 < τ t < τ a < 1 are temperatures and Q ∈ R K×d is a matrix whose rows are the prototypes. The probabilities are promoted to be the same by minimizing the cross-entropy loss H(p t i , p a i ), as illustrated in Fig. 1. In practice, a sequence of M ≥ 1 anchor views are generated, leading to multiple probabilities {p a i,m } M m=1 . Furthermore, to prevent representation collapse and encourage the model to fully exploit the prototypes, a mean entropy maximization (me-max) regularizer [2,22] is added, aiming to maximize the entropy H(p a ) of the average prediction across all the anchor views pa. Thus, the overall training objective to be minimized for both θ a and Q is where λ > 0 is an hyperparameter and the gradients are computed only with respect to the anchor predictions p a i,m (not the target predictions p t i ). Applying MSNs on the large datasets described below, generates representations that serve as a strong basis for various downstream tasks, as shown in the next section."
Self-supervised Learning for Endoscopic Video Analysis,3.2,Private Datasets,"Laparoscopy. We compiled a dataset of laparoscopic procedures videos exclusively performed on patients aged 18 years or older. The dataset consists of 7,877 videos recorded at eight different medical centers in Israel. The dataset predominantly consists of the following procedures: cholecystectomy (35%), appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery (5%). The remaining 21% of the dataset encompasses various standard laparoscopic operations. The recorded procedures have an average duration of 47 min, with a median duration of 40 min. Each video recording was sampled at a rate of 1 frame per second (FPS), resulting in an extensive dataset containing 23.3 million images. Further details are given in the supplementary materials.Colonoscopy. We have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18 years or older. These videos were recorded during standard colonoscopy procedures performed at six different medical centers between the years 2019 and 2022. The average duration of the recorded procedures is 15 min, with a median duration of 13 min. To identify and extract polyps from the videos, we employed a pretrained polyp detection model [21,25,26]. Using this model, we obtained bounding boxes around the detected polyps. To ensure high-quality data, we filtered out detections with confidence scores below 0.5. For each frame, we cropped the bounding boxes to generate individual images of the polyps. This process resulted in a comprehensive collection of 2.2 million polyp images.  "
Self-supervised Learning for Endoscopic Video Analysis,4.0,Experiments,"In this section, we empirically demonstrate the power of SSL in the context of endoscopy. Our experimental protocol is the following: (i) first, we perform SSL pretraining with MSNs over our unlabeled private dataset to learn informative and generic representations, (ii) second we probe these representations by utilizing them for different public downstream tasks. Specifically, we use the following two benchmarks. (a) Cholec80 [32]: 80 videos of cholecystectomy procedures resulting in nearly 200k frames at 1 FPS. Senior surgeons annotated each frame to one out of seven phases. (b) PolypsSet [33]: A unified dataset of 155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic or adenoma) and bounding boxes. We use the provided detections to perform binary classification. Downstream Task Evaluation Protocols. (a) Linear evaluation: A standard protocol consisting in learning a linear classifier on top of frozen SSL features [6,20]. (b) Temporal evaluation: A natural extension of the linear protocol where we learn a temporal model on top of the frame-level frozen features. We specifically use Multi-Stage Temporal Convolution Networks (MS-TCN) as used in [13,27]. This incorporates the temporal context which is crucial for video tasks such as phases recognition. (c) Fine-tuning: An end-to-end training of a classification head on top of the (unfrozen) pretrained backbone. We perform an extensive hyperparameter grid search for all downstream experiments and report the test results for the models that exceed the best validation results. We report the Macro F1 (F-F1) as our primary metric. For phase recognition we also report the per-video F1 (V-F1), computed by averaging the F1 scores across all videos [27].Implementation Details. For SSL we re-implemented MSNs in JAX using Scenic library [15]. As our image encoders we train Vision Transformer (ViT [16]) of different sizes, abbreviated as ViT-S/B/L, using 16 TPUs. Downstream experiments are implemented in TensorFlow where training is performed on 4 Nvidia Tesla V100 GPUs. See the supplementary for further implementation details. 1"
Self-supervised Learning for Endoscopic Video Analysis,4.1,Results and Discussion,"Scaling Laws of SSL. We explore large scale SSL pretraining for endoscopy videos. Table 1 compares the results of pretraining with different datasets (public and private) and model sizes. We pretrain the models with MSN and then report their downstream performances. We present results for the cholecystectomy phase recognition task based on fine-tuned models and for the optical polyp characterization task based on linear evaluation, due to the small size of the public dataset. As baselines, we report fully-supervised ResNet50 results, trained on public datasets. We find that replacing ResNet50 with ViT-S, despite comparable number of parameters, yields sub-optimal performance. SSL pretraining on public datasets (without labels) provides comparable or better results than fully supervised baselines. The performance in per-frame phase recognition is comparable with the baseline. Phase recognition per-video results improve by 1.3 points when using the MSN pretraining, while polyp characterization improve by 2.2 points. Importantly, we see that the performance gap becomes prominent when using the large scale private datasets for SSL pretraining. Here, per-frame and per-video phase recognition performances improve by 6.7% and 8.2%, respectively. When using the private colonoscopy dataset the Macro F1 improves by 11.5% compared to the fully supervised baseline. Notice that the performance improves with scaling both model and private data sizes, demonstrating that both factors are crucial to achieve optimal performance. Low-Shot Regime. Next, we examine the benefits of using MSNs to improve downstream performance in a low-shot regime with few annotated samples.  Note that MSNs have originally been found to produce excellent features for low data regime [2]. We train a linear classifier on top of the extracted features and report the test classification results. Figure 3 shows the low-shot performance for the two endoscopic tasks. We report results using a fraction k = {12%, 25%, 50%, 75%, 100%} of the annotated public videos. We also report results for fully-supervised baselines trained on the same fraction of annotated samples. Each experiment is repeated three times with a random sample of train videos, and we report the mean and standard deviation (shaded area).As seen, SSL-based models provide enhanced robustness to limited annotations. When examining the cholecystectomy phase recognition task, it is evident that we can achieve comparable frame-level performance by using only 12% of the annotated videos. Using 25% of the annotated videos yields comparable results to the fully supervised temporal models. Optical polyp characterization results show a similar trend, but with a greater degree of variability. Using small portions of PolypSet (12% and 25%) hindered the training process and increased sensitivity to the selected portions. However, when using more than 50% of PolypSet, the training process stabilized, yielding results comparable to the fully supervised baseline. This feature is crucial for medical applications, given the time and cost involved in expert-led annotation processes."
Self-supervised Learning for Endoscopic Video Analysis,4.2,Ablation Study,"Table 2 details different design choices regarding our SSL pretraining. Ablations are done on ViT-S trained over the public Cholec80. We report results on the validation set after linear evaluation. In Table 2a), we see that the method is robust to the number of prototypes, though over-clustering [4] with 1k prototypes is optimal. In Table 2b) and Table 2c), we explore the effect of random and focal masking. We see that 50% random masking (i.e. we keep 98 tokens out of 196 for the global view) and using 4 local views gives the best of performance. In Table 2d) we study the effect of data augmentation. SSL augmentation pipelines have been developed on ImageNet-1k [7], hence, it is important to re-evaluate these choices for medical images. Surprisingly, we see that augmentations primarily found to work well on ImageNet-1k are also effective on laparoscopic videos (e.g. color jiterring and horizontal flips). In Table2e), we look at the effect of the training length when starting from scratch or from a good SSL pretrained checkpoint on ImageNet-1k. We observe that excellent performance is achieved with only 10 epochs of finetuning on medical data when starting from a strong DINO checkpoint [6]. Table 2g) shows that ImageNet-1k DINO is a solid starting point compared to other alternatives [9,20,31,34]. Finally, Table2f) confirms the necessity of regularizing with Sinkhorn-Knopp and me-max to avoid representation collapse by encouraging the use of all prototypes. "
Self-supervised Learning for Endoscopic Video Analysis,5.0,Conclusion,"This study showcases the use of Masked Siamese Networks to learn informative representations from large, unlabeled endoscopic datasets. The learnt representations lead to state-of-the-art results in identifying surgical phases of laparoscopic procedures and in optical characterization of colorectal polyps. Moreover, this methodology displays strong generalization, achieving comparable performance with just 50% of labeled data compared to standard supervised training on the complete labeled datasets. This dramatically reduces the need for annotated medical data, thereby facilitating the development of AI methods for healthcare."
Self-supervised Learning for Endoscopic Video Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_55.
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,1.0,Introduction and Related Works,"Deep learning algorithms have shown success in performing computer-aided diagnosis (CAD) tasks using high-dimensional medical images, such as classification [20], detection [18], and segmentation [19]. Physicians typically review slices sequentially in CT and diagnose based on changes in lesion morphology and knowledge within the key slices. However, the variability in the number of slices between volumes challenges the CAD model in capturing the complex associations between slices and assisting medical decision-making.The diagnosis of COVID-19 is challenging. Convolutional Neural Networks (CNNs) and their variants, such as 3D CNNs [15,21] and 2.5D CNNs [18], have shown promise, CNNs required pre-extracted regions of interest (ROI) with aligned size, fine-grained annotation, and high computational complexity. For instance, [21] employed a two-stage process where a segmentation model is trained first using segmentation masks. Then, a fixed-size 3D tensor is cropped from the lung region, transformed into a 4D tensor, and fed into the 3D classification net. Additionally, CT slices have intrinsic non-Euclidean associations, which has led to recent interest in using Transformers and Graph Neural Networks (GNNs) to handle them. For example, ViT [3], and Swin Transformer [8] are variants of Transformers that use multi-head self-attention to learn fullyconnected associations between image patches. However, this architecture had primarily been applied to medical 3D patches [12,16] rather than the complete sequence. Regarding GNNs, ViG [4] organized images into patch sequences but had yet to extend the model to variable-length sequences. Another earlier work [7] used systematic sampling to align the number of slices, introducing sampling bias. We identify a common issue that existed in CNNs [15,18], Transformers [12,16], and GNNs [7] that they cannot be directly trained end-to-end on variable-length slice sequences. Therefore, this paper proposes a graph model to break through this limitation by reconstructing node knowledge at the supernode level.One of the major challenges is achieving consistency training in GCNs while preserving the integrity of the slice information. Existing graph model [7] downsampled slices to align nodes, resulting in the loss of some critical information. This approach also treated slices at the same location after sampling from different volumes equally, assuming they have the same semantics, which contradicts semantic consistency and clinical meanings. Moreover, the complex associations between slices further complicate the modeling. In three-dimensional medical images, the slice sequence dynamics naturally encode critical knowledge about morphology changes, which is of great diagnostic value. A recent study [4] showed that using sparse connections can improve the efficiency of GNNs, at least for natural images, and lead to better performance than other architectures such as CNNs and Transformers. Existing research mainly utilized GNNs to extract associations among slices or patches, constructing topology connections using methods such as k-nearest neighbors [4,13] and cosine similarity [7], as well as a learnable adjacency matrix [13]. Such approaches have limitations in capturing the task-specific local sequential associations between slices and the higher-level global feature associations.Besides, various approaches have been developed to locate key slices in CT volumes under weak supervision. For instance, [7] proposed an end-to-end node masking method. In contrast, [11] used CNNs as feature extractors and selected a fixed number of substantial slices based on Shannon entropy under a multipleinstance learning framework. This method calculates the average prediction distribution of slices under random noise but cannot be trained end-to-end. This limitation is precisely the problem we aim to address. Our contributions are: (1) We propose a Global Semantic-guided Dual-stream Graph model for weakly-supervised graph classification tasks. It contains an unsupervised grouping algorithm called Semantic-guided Grouping, which aligns variable-length slice sequences using shared global semantic vectors to enable precise prediction by aligning both the node numbers and semantics. (2) We develop a dual-stream Base Graph Module incorporating the local slice sequence and global semantic knowledge by learning these two representations jointly. (3) We thoroughly evaluate the effectiveness of our proposed method through comparison and ablation experiments. Our method outperforms the weakly-supervised benchmark GCNs in terms of accuracy of diagnosis and prognosis on a publicly available CT dataset while maintaining similar slice localization performance and offering more interpretability."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.1,Problem Statement,"j=1 , where N V (i) is the cardinality and varies for each volume. The volume-level label is given by y i . We first extract the native descriptors of the slicesusing a spatial feature extractor, where the output of F ext is an m 0dimensional vector. To perform volume-level prediction under the guidance of shared semantic vectors, we introduce the Global Semantic-guided Dual-stream Graph (GSDG) model: ŷ = F GSDG (X, C). C indicate semantic vectors and will be introduced in the following. Figure 1 provides a schematic diagram of our method."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.2,Constructing Super-Nodes,"We introduce a method for grouping native nodes X into super-nodes H which we denote as H = F Gro (X, C). To accomplish this, we propose semantic vectorscorrespond to K groups and are shared across all volumes, end-to-end updated with the model. In an unsupervised setting, previous work [17] has extended cross-entropy minimization to optimal transportation. We build on this approach and draw inspiration from [1] to propose our unsupervised grouping algorithm, Semantic-guided Grouping (SgG). SgG utilizes semantic vectors to guide the grouping process, ensuring that the resulting super-nodes align both the semantic and the number of nodes simultaneously on the variable-length volumes. However, minimizing cross-entropy in unsupervised classification can result in degeneration, where all slices are assigned to a single label. To address this issue, we encode the grouping label of a slice as the posterior distribution q(y c |x i,j ) and re-express cross-entropy as:To achieve semantic uniformity, we reformulate p(y c |x i,j ) as p(y c |x i,j , c yc ):where τ is a temperature hyper-parameter. The mapping from native nodes to semantic vectors is described by Eq. 2. We represent this mapping using Q ∈ R K×N V (i) , and optimize it to maximize the similarity between the native node features and the semantic vectors of their corresponding groups. Therefore, the optimization objective of F Gro can be formulated as follows: min p,q E(p, q) s.t. ∀y c : q(y c |x i,j ) ∈ {0, 1} andWe utilize the Sinkhorn-Knopp (SK) algorithm [2] to handle the constraint term, which aims to distribute N V (i) native nodes uniformly into K groups. The SK algorithm produces an assignment matrix S ∈ R NV ×K , where each row is a one-hot vector indicating the group index to which a native node belongs. Consequently, we compute H = F C(X S S 0 ), where F C is a linear layer and • 0 : R N V (i) ×K → R 1×K computes the column-wise 0-norm of a matrix."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.3,Bi-level Adjacency Matrices,"We aim to train GCN on a dataset of variable-length volumes; and have already grouped the native-nodes into super-nodes, H. Then, we could depict the adjacency relationships between nodes in H by semantic vectors or native nodes. Inspired by the multi-resolution model design in CNNs, we explicitly model the global and local adjacency relations: A G , A L = F Adj (X, S, C). A G represents the global semantic adjacency matrix, while A L the local sequence adjacency matrix of the learned super-nodes from X.Global Adjacency Matrix Based on Grouping Semantic Vectors. The existing study [13] utilized the Gumbel reparameterization trick [6,10] to allow gradient flow through the adjacency matrix. Building upon the global semantic vectors C constructed in Sect. 2.2, we learn representations of the commonality between super-nodes over different volumes. Exactly, a link predictor, constructed by a 2-layer MLP, takes the concatenation of semantic vectors c i and c j as its input and produces the output θ i,j . We calculate the corresponding value,/s , in the global adjacency matrix, g 1 i,j , g 2 i,j ∼ Gumbel(0, 1) and s is a hyper-parameter."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,,Local Adjacency Matrix Based on Native Sequence Association.,"To account for the associations varying with relative distance between slices within a volume, we utilize exponential smoothing to create a sequence adjacency matrix A ∈ R N V (i) ×N V (i) for each volume. The adjacency value between native nodes i and j is calculated by: A i,j = tanh(). Here, s represents the output of the sigmoid function applied to a learnable vector w L ∈ R D , and D is a hyper-parameter. We combine the connectivities of native nodes belonging to the same group using the allocation matrix S introduced in Sect. 2.2 and obtain the reduced local adjacency matrix appliable to super-nodes: A L = S T AS."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.4,Dual-Stream Graph Classifier,"We introduce a graph classification module, denoted as ŷ = F Cls (H, A G , A L ), consisting of stacked Base Graph Modules (BGM) and a classifier. The BGM comprises two parallel isomorphic graph convolutions, a global feature GCN layer and a local sequence GCN layer, and a Multi-graph Bilinear Pooling (MgBP) module.Base Graph Module. The two GCN layers pass messages between super-nodes from distinct perspectives and output H G , H L ∈ R m×K . Then the MgBP module extracts fine-grained graph-level representation, F, using a low-rank multimodal bilinear module: Classification Head. To obtain hierarchical features, we concatenate F i from each BGM layer along the feature dimension, resulting in F f inal = NG i=1 F i ∈ R m×K . We then compute the mean and max along the node dimension separately, resulting in two length-m vectors. These vectors are concatenated and passed through a 2-layer MLP and softmax activation for classification.Weakly-Supervised Informative Slice Localization. Firstly, we obtain the predicted probability p base for the target class from F f inal using the Classification Head. Then, we mask each super-node in turn to create K sub-matrices of size m × (K -1). The Head is utilized again to calculate the new probabilities, which results in a vector p ∈ R K . The groups are ranked by d sn = p basep. Within a group, the distances between the native nodes and the super-node are measured using the dot product and normalized to the interval [0,1], which results in d rn . Slices' global importance within the group i is d sn(i) /d rn . We repeat this procedure for all groups and select the top k slices globally."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.1,Dataset and Pre-processing,"Our experiment used a public CT volume dataset 2019nCoVR [21], which contains complete chest CT scans from 929 COVID-19 (NCP) patients, 964 patients with common pneumonia (CP), and 849 healthy individuals (Normal). Among them, 408 patients are annotated with prognosis labels and some pneumonia patients with slice-level lesion annotations. To make a fair comparison with [7], we divided the dataset into training, validation, and testing sets using the same method with 20 random seeds. Each slice was resized to 224 × 224 pixels and normalized with mean = 0.449 and std = 0.226.We chosen ResNet-50 [5] as the F ext corresponding to m 0 = 2048. Only the frozen F ext module was pre-trained on ImageNet, while all other modules were trained end-to-end on the 2019nCoVR dataset. The model hyper-parameters were set to m = 256, K = 6, N G = 2, d = 64, k = 10, s = 0.5, τ = 1, and D = 8. AdamW [9] served as the optimizer with a learning rate of 3e -3 and a batch size of 64. The model was trained with the cross-entropy loss for 20 epochs. After each feature aggregation, a Dropout [14] layer with a rate of 0.1 was added. The selection of hyper-parameter values is mainly based on experience and constraints in the formula above. The diagnostic model, which has one output node activated by the sigmoid function, was trained from scratch. In contrast, the prognosis model with three output nodes activated by softmax was initialized with the weights of the trained diagnostic model, except for the classification head. We used the same evaluation metric, precision and recall, for weakly-supervised localization as [7]."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.2,"Differential Diagnosis, Prognosis and Weakly-Supervised Localization","Table 1 presents the diagnostic and prognostic performance of two state-of-theart architectures and our proposed method. The clinical AI system based on 3D CNNs [21] was trained with volume-level and additional pixel-level labels.Our proposed method outperforms the state-of-the-art weakly-supervised graph model GCN-DAP [7] and 3D CNNs [21] in terms of diagnostic accuracy and AUC scores. Furthermore, GSDG also surpassed [7] in the prognostic task. These results demonstrate the superiority and effectiveness of our method in modeling full-size variable-length volumes. The left panel of Fig. 2 compares the performance of GSDG and experienced radiologists [21] in diagnostic tasks. For the NCP, GSDG outperformed the radiologists. Moreover, when identifying Normal and CP cases, GSDG achieved similarly high levels of AUCs. These results suggest that our method is advantageous over radiologists in NCP diagnosis.It is worth noting that our model required fewer training epochs than [21], as shown in the right panel of Fig. 2, which highlights the faster convergence of GSDG compared to GCN-DAP [7]. GSDG located the most informative CT slices, achieved 51.60% (2.75%) and 89.27% (8.95%) for precision and recall, respectively, slightly worse than [7], 57.39% (3.32%), in precision, but outperformed [7], 79.89% (3.94%), in recall. During our experiments, we frequently observed that the model became unstable as the precision score increased further and the predictions degraded to a single category. This may be due to the loss of some information that only exists in the Hounsfield Unit, as the 2019nCoVR dataset uses JPG instead of DCM format to save the slices. With the emergence of possible technologies that can recover the JPGs losslessly to DCMs, training the model on lung-masked slices is expected to produce better results."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.3,Visualization of Grouping and Slice Localization,"We visualized the grouping of native slices in Supplementary Material S.1 and the weakly-supervised slice localization for two patients in S.2. It can be observed that slices belonging to the same group display a remarkable degree of visual resemblance. Besides, the group importance distribution of NCP and CP cases are more similar to each other than to the Normal case, which reflects patterns differences between positive and negative cases. Within the positive cases, our model exhibits a tendency to concentrate more on the lung base in the CP case, as evidenced by columns 4 and 5 of Fig. 2 ( Regarding localization, our method's ability to identify lesion slices is superior to its precision performance, as indicated by Figs. 4 and 5 (S.2)."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.4,Ablation Study,"The ablation study was conducted to evaluate the effectiveness of different node alignment methods and graph structures on the performance of the proposed model for variable-length volumes. The results are presented in Tables 1 and2 in Supplementary Material, S.3. We compared the performance of our proposed super-node strategy to systematic sampling [7] for node alignment and found that the former outperformed the latter. We also compared global and local adjacency matrices and found that using both resulted in the best overall performance."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,4.0,Conclusion,"This paper proposes a novel approach for handling variable-length volume while preserving the integrity of the data by not discarding any slices, which is a departure from the previous method. Our approach first introduces a shared global semantic vectors-guided native node grouping scheme. Then we present an efficient and effective dual-stream graph module for simultaneously learning representations from global semantic vectors and sequence associations specific to each volume. Additionally, our approach offers informative slice localization and visually-consistent grouping outcomes, which enhances interpretability for clinical purposes. Moreover, the current dataset format prevents us from using existing semantic segmentation techniques to remove non-pulmonary noise. We will delve deeper into this direction to enhance localization accuracy."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 45.
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,1.0,Introduction,"The wiring system within the human brain can be modeled as a complex graph, where the anatomical regions of interest (ROIs) are represented as nodes, and the white matter connectomes define edges between them [9,18]. The graphbased representation explains actual connections between different nodes, and thus brain network analysis has a significant advantage over traditional spatial analysis to investigate interactions of different ROIs. As various neurodegenerative disorders such as Alzheimer's Disease (AD) are understood as a disconnection syndrome [2,9], studies on the structural connectivities in the brain are of significant interest from both machine learning and clinical perspectives.Recently, variants of graph neural networks (GNNs) have been successful in brain network analysis with feature aggregation and message-passing mechanism on graph nodes [4,5,18]. Notice that, regardless of whether it is spatial or spectral, existing methods heavily rely on graph convolution that operates with the node signals (i.e., ROI measures). Here, the topology of graph plays an indirect role as the domain of the signal, merely selecting specific neighborhood for feature aggregation. The problem becomes even more severe when it comes to actual connectivity analysis without ROI-wise measurements. To utilize GNN methods, auxiliary node-wise measures such as node degree and clustering coefficients are required to perform prediction tasks on the brain networks, and the contribution of connectomic features as a biomarker is often not fully investigated. To perform convolution on edges directly, heuristics such as line graph [15] or defining orthogonal matrices from graph Laplacian [21] exists, but they have disadvantages when the graph is directed or has many components.In order to analyze the connectivity of brain networks directly, we propose a novel graph learning framework that allows a neural network to utilize the topological features by representing the brain network as a simplicial complex. We utilize the Hodge 1-Laplacian, i.e., L 1 ; in geometry, the 1-simplex denotes a line segment, and the Hodge Laplacian L 1 includes connection between different line segments (i.e., edges) in a graph depending on their directions. Leveraging the Hodge Laplacian lets us obtain directed relationships between graph edges (i.e., a brain network) as an undirected graph (i.e., Hodge Laplacian), and the edge weights in the original graph become a signal on the nodes comprehended by the Hodge Laplacian. Spatial convolution with the L 1 combines directed edge weights of the original graph based on the nodes that they are sharing. Together with a spatial graph convolution formulation, we construct our Hodge-Graph Neural Network (Hodge-GNN) which predicts labels of graphs purely based on the topology of the graphs without any node measures.The contributions of our work are 1) proposing a novel graph edge-learning framework on higher-order connectivity (i.e., connectivity between edges) of graphs with Hodge Laplacian, 2) defining spatial edge convolution layer that operates on graph edges directly, and 3) demonstrating superior performance on graph classification with brain connectivity from Alzheimer's Disease Neuroimaging Initiative (ADNI) with interpretability. Using Hodge-GNN, we depict brain connectivities that are highly associated with AD classification, which are corroborated by prior AD literature."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,2.0,Related Work,"Higher-order connectivity in Spatial domain. To capture the relation of higher-order graph structures, Morris et al. [24,25] proposed hierarchical k-GNNs, which are hierarchical GNN architectures based on the k-dimensional Weisfeiler-Lehman (WL) algorithm. By performing message passing directly between subgraph structures, k-GNNs enable the network to capture structural information that is not observable at the node-level. Despite their superiority over 1-GNN, k-GNNs require large memory and high computational cost due to their stacking of models, showing limitation in scalability and effectiveness on large graphs. The authors in [4] define range with diffusion instead of hopdistances and train on the range to obtain desirable node-embeddings.Line Graphs. Line graph transformation interchanges the nodes and edges in the original graph respectively, allowing the node-wise graph convolution to be performed edge-wise, which makes the learning of edge-embeddings feasible [15]. However, line graphs lack the property of injectivity, which implies that different graphs can be transformed into a same line graph. Edge Convolution. [30] performed edge convolution by creating edgeembeddings from neighboring pair of point clouds. However, the suggested method requires node features to generate the edge-embeddings without any graph prior, and thus it requires rich node features to construct strong embeddings.Spectral Filtering of Graph Edges. Huang et al. [14] defined spectral filters for graph signals of nodes and edges using the k-th Hodge Laplacian (HL) operators, i.e., HL-node and HL-edge, and showed effectiveness of capturing the edge-wise relation in heterogeneous brain functional networks. The authors in [21] performed kernel filtering in the spectral domain with a specialized orthonormal graph transform."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,3.0,Preliminaries: Simplicial Complex Representation,"A simplicial complex is a collection of simplices with various dimensional representations, where simplices refer to the basic blocks to represent objects in topological space. In detail, each simplex of various dimension can be seen as nodes (0-simplex), edges (1-simplex), triangles (2-simplex), and other higher dimensional counterparts. A simplicial complex composed of only 0-simplices is called a 0-skeleton, likewise, p-skeleton is composed of 0 to p-simplices. A graph, therefore, is a 1-skeleton with 0-and 1-simplices (nodes and edges) [1,14,19].In a simplicial complex, a p-chain is defined as a sum of p-simplicies, denoted as c = i α i σ i , where σ i are the p-simplices and the α i are either 0 or 1 [8]. A chain complex is defined as the sequence of groups, each of which is made For an oriented p-simplex σ p , the boundary operator can be defined as. Also, the boundary operator ∂ p is represented using a boundary matrix B p to facilitate efficient computation of the Hodge Laplacian. The p-th boundary matrix B p can be defined as [23],where σ p i is the i-th p-simplex, and ∼ and denote similar and dissimilar orientations respectively. The boundary matrix B p relates the two adjacent simplices, i.e., p-and (p -1)-simplex, which will be used to define the Hodge Laplacian for higher-order graph representation in Sec. 4.1."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.0,Proposed Method,"The proposed method is composed of two components; graph transformation of adjacency matrix to Hodge Laplacian L 1 with edge-wise features, and edgewise graph convolution using the L 1 . With the traditional graph convolution formulation, the network can conduct transform of topological features directly instead of using them as indirect measures in previous GNNs."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.1,Hodge Laplacian of Brain Network Data,"Let G = (V, E) be a directed weighted graph, where V is a set of nodes, and E is a set of directed edges consisting of ordered tuples, (u, v), s.t. u, v ∈ V, which denotes an edge from u to v. E is indexed with {e i } E i=1 , |E| = E is the number of edges, and |V| = N is the number of nodes.Hodge Laplacian L p , also known as the p-Laplacian is a generalization of graph Laplacian on higher simplices, i.e., nodes (0-simplices) to p-simplices. The Hodge Laplacian L p is defined using the B p (i, j) in Eq. ( 2) as:From Eq. ( 3), the Hodge 0-Laplacian, L 0 , is equivalent to graph Laplacian, defined aswhere B 1 ∈ R N ×E is a boundary matrix for the 1simplex, i.e., an incidence matrix relating nodes to edges.To enable the graph representation to hold connectivity over edges, we construct Hodge Laplacian L 1 . As a 1-skeleton, i.e. topological graph, is composed of 0 and 1 simplex only (i.e., B 2 = 0), the L 1 ∈ R E×E is derived as:Considering the vertices u, v, t ∈ V, s.t. u =v =t, each element L 1 (i, j) is defined as:A directed weighted graph G can be represented as a binary adjacency matrix A ∈ R N ×N and a weight matrix W ∈ R N ×N for A. Since W holds features for each edge, we can extract the non-zero components of W, which serves as a signal W E ∈ R E on L 1 ."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.2,Convolving Graph Edges via Hodge Laplacian L 1,"GCN [31] performs aggregation of the neighboring node features as:where A ∈ R N ×N is an adjacency matrix, and W (l) is a parameter matrix of the l-th layer. H (l) ∈ R N ×K is the output of the l-th convolution layer with K features, where H (0) is the input of node feature vectors, and σ(•) is a nonlinear activation function. From a spatial perspective, the graph convolution relates the neighboring node features to generate the node embedding utilizing the adjacency matrix as a relational matrix that provides the direct neighboring information.From a weighted adjacency matrix A ∈ R N ×N , we can extract Hodge Laplacian L 1 ∈ R E×E and W E which is a vector of edge weights considered as measurements on the nodes of L 1 . With L 1 , we construct a Hodge graph neural network (Hodge-GNN) whose l-th layer is defined as:where W (l) is a learnable weight parameter and H (l) ∈ R E×K is the output from the l-th convolution layer, with H (0) = W E . The key component here is L 1 H (l) , described as Edge-wise Convolution in Fig. 1.When it comes to graph analysis, most of existing GNN methods assume that features on the nodes exist for node-wise analysis. However, when the measurements on the nodes do not exist, and the analysis must be performed solely with the graph topology and edge information, other GNN methods must define an auxiliary node-wise measures such as node degree and clustering coefficients. Unlike the previous approaches, our framework enables the information from adjacent edges to be given different weights, either positive or negative, depending on the topology of the graph, and the edge-wise convolution can now relate the edge features and generate edge embeddings, allowing the network to utilize the hidden topological features that were not seen in the original input graph form.Finally, the class prediction Ŷ c for each class c is obtained by flattening the H (L) ∈ R E×K and passing it through multi-layer perceptron (MLP), and applying a softmax yieldsThe objective function defined by cross-entropy over all T samples is:where"
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.3,Interpretability of the Connectomes in Brain Dysfunction,"To provide interpretability to the framework, we define gradient-based class activation map on the graph edges using the formulation in [27,33]. Specifically, when a graph (i.e., L 1 and W E ) is inputted to the network, by tracking the backpropagating gradients of the score for a specific class c (i.e., Ŷ c ) with regard to each feature vectors H k ∈ R E×1 of the final convolution layer of GNN (i.e., H (L) ) [27], the importance of each activation α c k and the heatmap H of the specific class c can be computed as:Performing the edge-wise convolution from Hodge Laplacian L 1 , this heatmap H c holds the contribution of each connectome to the classification of developmental stages in brain dysfunction (i.e., Alzheimer's Disease)."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.1,Dataset and Experimental Settings,"Dataset. Our dataset contains structural brain connectivity data derived from Diffusion Tensor Images (DTI) in Alzheimer's Disease Neuroimaging Initiative (ADNI) with tractography. Each sample is given as a directed weighted graph whose weights denote the number of white matter fiber tracts connecting two different ROIs and its corresponding diagnostic label. The ROIs and their connectomes were defined by the Destrieux atlas [7] with 148 cortical and 12 sub-cortical ROIs. As tractography involves probabilistic calculation, the connectivity matrix becomes non-symmetric with varying weights on the same connectivity matrices. The dataset is composed of n=1824 subjects within Control (CN, n=844), Early Mild Cognitive Impairment (EMCI, n=490), Late Mild Cognitive Impairment (LMCI, n=250), and AD (n=240) groups. Edge Preprocessing. Unlike the widely used A ∈ R N ×N or L 0 ∈ R N ×N , the adoption of L 1 ∈ R E×E incurs an exponential rise in both memory and computational costs. Also, the E across different samples varies from 2138 to 11802 (6766 in average). To handle such problems, we used the intersection of edges across entire samples. This preprocessing step yields the number of edges to be E = 530, common across all subjects.Baselines. Our method is validated on various approaches, including conventional classification methods, neural networks, and graph methods of both spatial and spectral domain. In detail, we used support vector machine (SVM), single layer perceptron (SLP), multi-layer perceptron (MLP), and GCN [31] with original graph G and line graph G L as the conventional classification method, as well as the hierarchical k-GNNs (1-2-GNN, 1-2-3-GNN) [25] and MENET [21] for spatial and spectral baselines respectively. Also, DGCNN [30] which extracts edge-embeddings with edge convolution from node features is compared as well.Evaluation. 4-way classification is performed on the AD-specific groups. All models are evaluated using 5-fold cross validation (CV) for unbiased results. On the 4-way classification task for AD-specific groups, average accuracy, Macroprecision, Macro-recall, and Macro-F1-score (Table 1) are compared. Qualitative results of our experiment reveal the important connectivities in classifying AD stages (Fig. 2) from the trained model."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.2,Experimental Results,"Our Hodge-GNN is evaluated based on the 4-way classification of CN, EMCI, LMCI, and AD. The quantitative comparisons are shown in Tab. 1. As k-GNN, MEMET, and GCN with G L capture higher-order connectivity information, they performed better than the conventional GCN with G. However, Hodge-GNN was more effective in the AD classification, achieving the highest performance in all measures by ∼3%p over the second best method. "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.3,Interpretation of AD via Trained Hodge-GNN,"Using the computed importance from Eq. ( 10), significant edges for classifying the stages of Alzheimer's Disease are depicted (Fig. 2). The significant edges are selected by taking intersection of the top-k edges for each class label. The top-k edges are the distinct edges obtained from the top-10 edges across the 5-fold cross validation. Thus, the selected edges represent common edges that shows high importance for classifying the brain dysfunction. During the progress of AD, ROIs of the brain show not only the shrinkage of volume but also weakening of connectivity [2,6]. As in Fig. 2, our classifier picked up connectome of ROIs in the subcortical regions (i.e., amygdala, hippocampus, pallidum, putamen, and thalamus) [3,10,26], temporal lobe (i.e., inferior, middle, and superior temporal cortex) [3,10,11], frontal lobe (i.e., superior frontal gyrus) [10,16], and other important regions that are highly related to AD [12,20].In addition, the depicted edges showed several symmetry found in both left and right hemispheres, such as pallidum-putamen and amygdala-hippocampus connectomes, both of which play a crucial role in the development of AD [13,29,32]. Interestingly, right pallidum was shared on two detected connectivities (i.e., with right-putamen and right-thalamus-proper), which highlights the importance of pallidum connectomes [17]. Also, out of the 17 ROIs consisting the detected connectivities, 5 ROIs (pallidum, amygdala, putamen, hippocampus, and thalamus) were from subcortical regions, denoting that they are critical for our AD-stage classification and subcortical areas are highly implicated in AD as in prior works [22,28]."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,6.0,Conclusion,"We proposed a novel framework for extracting edge-to-edge relations in graph spatial domain using Hodge 1-Laplacian, i.e., Hodge-GNN. The Hodge-GNN performs graph convolution on edges via shared nodes among edges and allows a downstream predictor to accurately classify different stages of AD. The validation experiment showed superiority of performance in prediction together with interpretable outcomes depicting specific connectomes and ROIs for effective AD analysis. "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 76. 
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,1.0,Introduction,"By virtue of the human body's complexity, most diseases present phenotypic variability in terms of symptoms, rate of progression and imaging findings, which challenges diagnostic criteria. Among many hard to diagnose diseases, Chronic Obstructive Pulmonary Disease (COPD) stands out, as it is extensively underand misdiagnosed [20], despite being the 3rd leading cause of death worldwide, with an estimated global prevalence of 10.3% [3]. Its pathological manifestations in the lung range from emphysema to airway disease, leading to a sparse, diffuse, and heterogeneous appearance, as shown in Fig. 1a. Appropriate and earlier diagnosis that accounts for all of its manifestations is therefore of paramount importance for public health [1].Considering the limitations of spirometry as the standard diagnostic method [1], computed tomography (CT) has emerged as a complementary tool for COPD characterization. Initial efforts focused on identifying typical intensity and texture-level imaging features from either inspiration or expiration CT scans [4]. With the advent of deep learning (DL), more complex supervised approaches have been proposed to tackle binary classification of COPD. In this context, due to GPU memory constraints and large size of the images, different strategies to parcel a single 3D image as 2D slices [9,23] or 3D patches [19] have been pursued by supervised DL methods. Significant emphasis has been put on multiple instance learning (MIL) approaches [8,22,25], considering the spatial heterogeneity of COPD and that only a binary label is needed in case-finding scenarios. Typically, for a supervised model to learn good decision boundaries, the labeled training dataset needs good coverage of the appearances of all classes. However, good coverage of the diseased class can be difficult for low prevalence and heterogeneous diseases, making supervised models susceptible to fail on novel data points [13] (Fig. 1b). COPD fits exactly in this scenario, as its manifestations in the lung are diverse, in contrast to healthy individuals whose lungs are generally more uniform in appearance. This raises questions about the suitability of supervised models for COPD classification.Instead of attempting to learn all possible complex manifestations of the disease, we ask: Could COPD be more accurately detected if considered as an anomaly from the distribution of healthy lungs?As previously reported for anomaly detection [17,26], modeling the distribution of normal samples in the latent space, instead of in the voxel space, has shown to be both feasible and desirable. With this in mind, our contribution is two-fold:1. We show the benefit of reformulating COPD prediction as an anomaly detection task. Inspired by [16], we develop a generative model operating on the self-supervised representation space (Fig. 1c), learning the distribution of labeled healthy features and identifying unknown abnormal ones (stemming from COPD) as Out-of-Distribution (cOOpD). cOOpD outperforms all compared DL-based supervised methods on two distinct public datasets, whilst maintaining performance in a scenario using a simulated real-world prevalence training dataset. 2. We highlight the benefit of moving from voxels to representation space through a supervised method that leverages contrastive representations of lower dimensionality than voxels and outperforms voxel-based classifiers.To the best of our knowledge, this work is the first to investigate anomaly detection in the context of a heterogeneous lung disease classification and has the potential to be applied to a wide range of diffuse diseases affecting large body areas. "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.0,Method,"Our proposed method cOOpD aims at reformulating COPD classification as anomaly detection. It is a self-supervised anomaly detection framework, inspired by the strategy of [16], optimized for diffuse lung diseases covering 3D multichannel inputs, suitable augmentation strategies and patient-level aggregation of patch-level scores. During inference (Fig. 1d), a sequence of B 3D lung patches {x i } B i=1 from a single or paired CT scan X is extracted. Then, for each patch, a representation is obtained using a trained self-supervised contrastive encoder z i = f (x i ) (Sect. 2.1). Having learned the distribution of healthy patchrepresentations (Sect. 2.2), the patch-level latent representation is given to a generative model p(z), being attributed an anomaly score defined as the negative log-likelihood: s(x i ) = -log(p(f (x i ))). Several aggregation strategies S(x) of these scores to patient-level were tested (supplementary) including the mean, which was found to be the best performing and most conceptually meaningful strategy, as outlined in Eq. 1."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.1,Patch-Level Representations Using Contrastive Learning,"The latent representations of the encoder are learned with a self-supervised contrastive task, creating clusters based on semantic information. For this, we follow the contrastive training described in [16] based on SimCLR [7] with specific changes for medical images by providing more adequate mechanisms for 3D medical imaging. Context and spatial information were covered by enabling 3D multi-channel patches as input, where each patch is then used as a singular sample for the contrastive task (Fig. 1c). Our augmentation strategy follows the approach of [27] with the following transformations: Non-linear transformation based on the Bézier curve, local-pixel shuffling and in-and out-painting. These were specifically designed for diffuse lung diseases and should force the encoder to learn patch representations capturing shape, texture, boundaries and context information. Preliminary experiments found that using all patches available per patient can introduce redundancy and substantially increase the computational cost (supplementary). Therefore, a maximum of 100 patches per patient was set for training the self-supervised contrastive task. As an encoder, different 3D ResNet configurations (18 and 34) were tested."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.2,Generative Models Operating on Representation Space,"Once having extracted the latent representations, the distribution of normal representations is modeled, by fitting a generative model p(z) on the representations of purely normal patches. Patch normality is defined by % emphysema < 1% strictly applied to normal individuals, a very restrictive bound to guarantee that no intensity alterations could be present in the definition of normality. % emphysema is defined as the percentage of low attenuation areas less than a threshold of -950 Hounsfield units [4]. As generative models, Gaussian Mixture Model (GMM) and Normalizing Flow (NF) are employed. While both are density estimation methods used to model p(z), GMMs model the probability density function of the data as a weighted sum of Gaussian distributions, whereas our NF model uses the change of variable formula with a Gaussian prior. The implementation of the NF is identical to [16] consisting of fully connected affine coupling blocks and permutations based on the RealNVP architecture. We fit several GMM with κ ∈ 1, 2, 4, 8 and a NF on representations of the encoder from the purely normal patches of healthy patients from the training dataset without any transformations. The best performing generative model is selected based on the validation set performance."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,3.0,Experiment Setup,"Dataset & Preprocessing: Paired inspiratory and expiratory volumetric CT images were used from two nationwide multi-center studies (COPDGene [18] 1 and COSYCONET [12]), from which 5244 and 484 unique individuals were randomly selected, respectively (supplementary). Binary classes were defined based on the Global Initiative for Chronic Obstructive Lung Disease (GOLD), a discrete score between 0-4. The negative class (healthy) included never-smokers and individuals with a GOLD score of 0, while the positive class (diseased) included those with a GOLD score of 1 or higher. This resulted in the prevalence of the positive class being 57% for COPDGene and 85% for COSYCONET. All trainings were performed on COPDGene which was split randomly into training (50%), validation (25%) and test (25%) sets on the patient-level. COSYCONET was entirely used as an external test dataset. The data preparation process is illustrated in Fig. 1d and comprises the following sequential steps: Spatial Alignment of Paired Inspiratory and Expiratory CT Images: Considering the potential of adding the expiratory scan as an extra channel as an indirect measure of gas trapping [4], the paired images were geometrically aligned. Having the inspiratory image as the fixed image, an adaptation of [21] was performed.Lung Parenchyma Segmentation for Patch Extraction: Lung masks were generated on the inspiratory image space using a nnU-Net model [11] on YACTA [2] segmentation masks, a validated intensity-based method.Intensity Normalization: Inter-scanner variability was addressed by normalizing the intensity values to a scale between 0 (air) and 1 (tissue) [14]. Mean intensity values for air and tissue were derived from segmented tracheal and aortic regions, respectively, obtained using a pre-trained nnU-Net model (Task 055 SegTHOR). Additionally, all images were resampled to an isotropic resolution of 0.5 mm. Patch Extraction: Volumetric patches (50 3 voxels) containing > 70% of the lung were extracted from the lung parenchyma of aligned inspiratory and expiratory CT images. The chosen size covered the secondary pulmonary lobule, the basic unit of lung structure [24]. Two different patch overlapping strategies were implemented (0% and 20%) on inspiratory (1-channel) and inspiratory + registered expiratory (2-channels) images. Thus, four different configurations of input patches were tested.Baselines: State-of-the-art (SotA) baselines were applied to 2D slices and 3D patches. A 2D-CNN [9] was employed at the patient-level. An end-to-end 3D patch classifier with score aggregation (PatClass), an MIL approach with a Recurrent Neural Network as aggregation (MIL+RNN ) [5] and an Attentionbased MIL (MIL+Att) (similar to [22], adapted from [10]) were employed at the patch-level. Implementation was performed as described in the original works, with adaptations to 3D, when required (supplementary)."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Contrastive Representations Ablation:,"The contrastive latent representations' usefulness was evaluated with a supervised method (ReContrastive) that maps the latent representations back to their position in the original image, producing a 4D image, where the 4 th dimension is the length of the latent representation vector (Suppl). The produced image is then used as input for a CNN classifier. Training was performed for 500 epochs using the SGD Optimizer, a learning rate of 1e-2, Cosine Annealing [15] and a weight decay of 3e-5. A combination of random cropping, random scaling, random mirroring, rotations, and Gaussian blurring was employed as transformations.Evaluation Metrics: We used Area Under Receiver Operator Curve (AUROC) and Area Under Precision Recall Curve (AUPRC) as the default multi-threshold metric for classification. AUROC is used as the main evaluation metric since it is less sensitive to class balance changes.Final Method Configurations: These were chosen based on the highest AUROC on three experiment runs on the validation set. The best patch extraction configuration for all tested 3D methods was two-channel (inspiratory and registered expiratory) with 20% patch overlap. The best performance was always achieved with a ResNet34. For our proposed cOOpD method, GMM with κ = 4 was found to be the best performing generative model."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Real-World Prevalence Ablation:,"Given the global prevalence of COPD at 10.3% [3], we further evaluated the top two performing approaches in scenarios designed to approximate this real world prevalence. To better reflect these conditions, the diseased class in the COPDGene training set was undersampled to 5%, 10.3% and 15% while keeping all samples from the normal class, limiting the diversity of the diseased class in the training set (instead of oversampling the normal class)."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,4.0,Results,"As shown in Table 1, cOOpD outperforms all SotA supervised methods, achieving statistically significant improvements in terms of AUROC of 8.2% compared to the best method on COPDGene (PatClass+RNN), and 7.7% on COSY-CONET (MIL+RNN). ReContrastive, as a supervised ablation for assessing the advantage of using representations, also outperformed all the other voxel-based supervised strategies on the internal test set, by an AUROC difference of 3.8%  [5] 73.0 ± 0.6 ** 84.5 ± 0.5 ** 60.2 ± 4.2 * 95.7 ± 0.4 * MIL + Att [10,22] 65.8 ± 1.2 ** 80.9 ± 0.8 ** 57. but shows a large performance drop leading to the worst AUROC on the external test set. In the real-world ablation, as seen in Fig. 2a, the best performing supervised method (ReContrastive) performance decreases with the diseased class prevalence, reaching a drop of 6.5% compared to cOOpD."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,5.0,Discussion,"Final Method Configurations: The best working method configurations reflect the following properties of the task: Using both inspiratory and expiratory images provides information about pulmonary vascular alterations and airway wall thickness not visible on the inspiratory scan alone, as in line with [6]. Finer grained information is captured using overlapping patches, which tended to work better in conjunction with deeper encoders. Regarding our proposed method cOOpD, we note the following: We hypothesize that the latent space's complexity level is low, being easily covered with a simple generative model. As for the aggregation strategy, considering the spatial distribution of COPD, it can happen that only a small part of the lung is diseased. As the negative-loglikelihood has a lower bound but not an upper bound, a single patch having a high score leads to a high overall score when using mean aggregation, which is the desired behavior.Should COPD Binary Classification be Formulated as Anomaly Detection? cOOpD performance shows to be significantly superior compared to all tested methods, on the COPDGene (internal) and COSYCONET (independent external) test sets. The lower performance in the external test set was consistent with all other methods. There are several potential explanations for this. Besides being a highly imbalanced dataset, all patients in COSYCONET have a diagnosis of COPD and only 15% are categorized into GOLD 0 due to normal lung function. We hypothesize that these 15% ""healthy"" individuals have early signs of disease that are not captured by voxel-based methods but are being encoded by the latent representations. Considering that cOOpD was trained only on healthy representations from the COPDGene dataset, whose normal class consisted of never-smokers and GOLD 0 subjects, it can still outperform all the other methods, since the unseen traits of the disease are seen as anomalies. The advantage of solely modeling the healthy distribution is further highlighted by the real-world experiments (Fig. 2a), where cOOpD performance remains unaffected, when compared to the supervised ablation (ReContrastive). Identifying people at risk for disease worsening is paramount for COPD management. The anomaly score per patient fulfills this risk assessment need, by exhibiting a clear relation to the exact GOLD stage (Fig. 2b), even though it was never explicitly given the GOLD stage as a multi-class label. Further, the lung region scores enable spatial localization of anomalies, giving interpretability to the method (Fig. 2c). These findings support our approach of reformulating COPD binary classification as an anomaly detection task.Are Self-supervised Patch-Level Latent Representations Advantageous over Voxels? Both methods working on the representation space (cOOpD and ReContrastive) outperform all voxel-based baselines on the internal test set. Although for ReContrastive this improvement is no longer seen for the external test set, being the worst performing method, the early signs of disease for the healthy class of COSYCONET are likely being encoded by the latent representations, as mentioned earlier. We hypothesize that this performance drop stems from the problem of supervised models depicted in Fig. 1b. Combined with the cOOpD findings, this still supports the hypothesis that patch-level latent representations provide meaningful information and reduce the complexity of the problem."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,6.0,Conclusion,"Our proposed reformulation of COPD binary classification into an anomaly detection task (cOOpD) demonstrated superior performance compared to SotA methods. Additionally, the advantage of using latent representations was demonstrated. The cOOpD approach also demonstrated stability in performance when trained on datasets with simulated real-world class imbalance. Future work should focus on further validation on larger and more diverse datasets, longitudinal evaluation, and exploring its application to other heterogeneous diseases where annotated diseased data is scarce and access to healthy data is abundant."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Table 1 .,** 72.0 ± 1.5 ** 57.0 ± 8.0 ** 84.6 ± 1.4 ** 3D patch PatClass + RNN 76.1 ± 0.2 ** 86.3 ± 0.1 ** 56.2 ± 0.7 ** 95.3 ± 0.1 * MIL + RNN
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 4.
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,1.0,Introduction,"Viral or metabolic chronic liver diseases that cause liver fibrosis impose great challenges on global health. Accurate staging for the severity of liver fibrosis is essential in the diagnosis of various liver diseases. Current deep learning-based methods [24,25] mainly use abdominal MRI and computed tomography (CT) data for liver fibrosis staging. Usually, a square sub-region of the liver instead of the whole image is cropped as input features, since the shape of the liver is irregular and unrelated anatomies in the abdominal image could disturb the training of deep learning models. To automatically extract the region of interest (ROI), a recent work [8] proposes to use slide windows to crop multiple image patches around the centroid of the liver for data augmentation. However, it only uses one patch as input at each time, which only captures a sub-view of the liver. To exploit informative features across the whole liver, we formulate this task as a multi-view learning problem and consider each patch as a view.The aim of multi-view learning is to exploit complementary information from multiple features [23]. The central problem is how to integrate features from multiple views properly. In addition to the naive method that concatenates features at the input level [5], feature-level fusion strategies seek a common representation between different views through canonical correlation analysis [12,22] or maximizing the mutual information between different views using contrastive learning [1,21]. In terms of decision-level fusion, the widely used methods are decision averaging [18], decision voting [14], and attention-based decision fusion [9]. However, in the methods above, the weighting of multi-view features is either equal or learned implicitly through model training, which undermines the interpretability of the decision-making process. Besides, they are not capable of quantifying uncertainties, which could be non-trustworthy in healthcare applications.To enhance the interpretability and reliability of multi-view learning methods, recent works have proposed uncertainty-aware decision-level fusion strategies. Typically, they first estimate uncertainties through Bayesian methods such  as Monte-Carlo dropout [20], variational inference [19], ensemble methods [4], and evidential learning [17]. Then, the predictions from each view are aggregated through explicit uncertainty-aware combination rules [7,20], as logic rules are commonly acknowledged to be interpretable in a complex model [26]. However, the predictions before the combination are made based on each independent view. Cross-view features are not captured to support the final prediction. In our task, global features could also be informative in the staging of liver fibrosis.In this work, we propose an uncertainty-aware multi-view learning method with an interpretable fusion strategy of liver fibrosis staging, which captures both global features across views and local features in each independent view. The road map for this work is shown in Fig. 1(a). The uncertainty of each view is estimated through the evidential network and subjective logic to improve reliability. Based on the uncertainties, we apply an explicit combination rule according to Dempster-Shafer's evidence theory to obtain the final prediction, which improves explainability. Moreover, we incorporate an additional global view to model the cross-view representation through the data-efficient transformer.Our contribution has three folds. First, we are the first to formulate liver fibrosis staging as a multi-view learning problem and propose an uncertaintyaware framework with an interpretable fusion strategy based on Dempster-Shafer Evidence Theory. Second, we propose to incorporate global representation in the multi-view learning framework through the data-efficient transformer network. Third, we evaluate the proposed framework on enhanced liver MRI data. The results show that our method outperforms existing multi-view learning methods and yields lower calibration errors than other uncertainty estimation methods."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.0,Methods,"The aim of our method is to derive a distribution of class probabilities with uncertainty based on multiple views of a liver image. The pipeline for view extraction is shown in Fig. 1(b). A square region of interest (ROI) is cropped based on the segmentation of the foreground. Then nine sub-views of the liver are extracted in the ROI through overlapped sliding windows. The multi-view learning framework is shown in Fig. 2. Our framework mainly consists of three parts, i.e., evidential network, subjective logic, and combination rule. The evidential networks encode local views and the whole ROI as global view to evidence vectors e. For local views, the networks are implemented with the convolutional structure. While for the global view, a data-efficient vision transformer with shifted patch tokenization (SPT) and locality self-attention (LSA) strategy is applied. Subjective logic serves as a principle that transforms the vector e into the parameter α of the Dirichlet distribution of classification predictions, and the opinion D with uncertainty u. Then, Dempster's combination rule is applied to form the final opinion with overall uncertainty, which can be transformed into the final prediction. The details of subjective logic, Dempster's combination rule, the data-efficient transformer, and the training paradigm are discussed in the following sections."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.1,Subjective Logic for Uncertainty Estimation,"Subjective logic, as a generalization of the Bayesian theory, is a principled method of probabilistic reasoning under uncertainty [10]. It serves as the guideline of the estimation of both uncertainty and distribution of predicted probabilities in our framework. Given an image1 , e k 2 , ..., e k C ] with non-negative elements for C classes is estimated through the evidential network, which is implemented using a classification network with softplus activation for the output.According to subjective logic, the Dirichlet distribution of class probabilities Dir(p k |α k ) is determined by the evidence. For simplicity, we follow [17] and derive the parameter of the distribution by α k = e k + 1. Then the Dirichlet distribution is mapped to an opinionwhere and predicted probabilities pk can be derived in an end-to-end manner."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.2,Combination Rule,"Based on opinions derived from each view, Dempster's combination rule [11] is applied to obtain the overall opinion with uncertainty, which could be converted to the distribution of the final prediction. Specifically, given opinionswhere N = 1i =j b 1 i b 2 j is the normalization factor. According to Eq. ( 2), the combination rule indicates that the combined belief b c depends more on the opinion which is confident (with small u). In terms of uncertainty, the combined u is small when at least one opinion is confident.For opinions from K local views and one global view, the combined opinion could be derived by applying the above rule for K times, i.e., D = D 1 "
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.3,Global Representation Modeling,"To capture the global representation, we apply a data-efficient transformer as the evidential network for the global view. We follow [13] and improve the performance of the transformer on small datasets by increasing locality inductive bias, i.e., the assumption about relations between adjacent pixels. The standard vision transformer (ViT) [3] without such assumptions typically require more training data than convolutional networks [15]. Therefore, we adopt the SPT and LSA strategy to improve the locality inductive bias.As shown in Fig. 2, SPT is different from the standard tokenization in that the input image is shifted in four diagonal directions by half the patch size, and the shifted images are concatenated with the original images in the channel dimension to further utilize spatial relations between neighboring pixels. Then, the concatenated images are partitioned into patches and linearly projected as visual tokens in the same way as ViT.LSA modifies self-attention in ViT by sharpening the distribution of the attention map to pay more attention to important visual tokens. As shown in Fig. 2, diagonal masking and temperature scaling are performed before applying softmax to the attention map. Given the input feature X, The LSA module is formalized as,where q, k, v are the query, key, and value vectors obtained by linear projections of X. M is the diagonal masking operator that sets the diagonal elements of qk T to a small number (e.g.,-∞). τ ∈ R is the learnable scaling factor."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.4,Training Paradigm,"Theoretically, the proposed framework could be trained in an end-to-end manner.For each view k, we use the integrated cross-entropy loss as in [17],where ψ is the digamma function and y k is the one-hot label. We also apply a regularization term to increase the uncertainty of misclassified samples,where λ is the balance factor which gradually increases during training and αk = y k + (1 -y k ) α k . The overall loss is the summation of losses from all views and the loss for the combined opinion,where L Combined and L Global are losses of the combined and global opinions, implemented in the same way as L k . In practice, we pre-train the evidential networks before training with Eq. ( 6). For local views, we use the model weights pre-trained on ImageNet, and the transformer is pre-trained on the global view images."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.1,Dataset,"The proposed method was evaluated on Gd-EOB-DTPA-enhanced [25] hepatobiliary phase MRI data, including 342 patients acquired from two scanners, i.e., Siemens 1.5T and Siemens 3.0T. The gold standard was obtained through the pathological analysis of the liver biopsy or liver resection within 3 months before and after MRI scans. Please refer to supplementary materials for more data acquisition details. Among all patients, 88 individuals were identified with fibrosis stage S1, 41 with S2, 40 with S3, and 174 with the most advanced stage S4. Following [25], the slices with the largest liver area in images were selected. The data were then preprocessed with z-score normalization, resampled to a resolution of 1.5 × 1.5 mm 2 , and cropped to 256 × 256 pixel. For multi-view extraction, the size of the ROI, window, and stride were 160, 96, 32, respectively. For all experiments, a four-fold cross-validation strategy was employed, and results of two tasks with clinical significance [25] were evaluated, i.e., staging cirrhosis (S4 vs S1-3) and identifying substantial fibrosis (S1 vs S2-4). To keep a balanced number of samples for each class, we over-sampled the S1 data and under-sampled S4 data in the experiments of staging substantial fibrosis."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.2,Implementation Details,"Augmentations such as random rescale, flip, and cutout [2] were applied during training. We chose ResNet34 as the evidential network for local views. For configurations of the transformer, please refer to supplementary materials. The framework was trained using Adam optimizer with an initial learning rate of 1e -4 for 500 epochs, which was decreased by using the polynomial scheduler. The balance factor λ was set to increase linearly from 0 to 1 during training.The transformer network was pre-trained for 200 epochs using the same setting.The framework was implemented using Pytorch and was run on one Nvidia RTX 3090 GPU."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.3,Results,"Comparison with Multi-view Learning Methods. To assess the effectiveness of the proposed multi-view learning framework for liver fibrosis staging, we compared it with five multi-view learning methods, including Concat [5], DCCAE [22], CMC [21], PredSum [18], and Attention [9]. Concat is a commonly used method that concatenates multi-view images at the input level. DCCAE and CMC are feature-level strategies. PredSum and Attention are based on decision-level fusion. Additionally, SingleView [8] was adopted as the baseline method for liver fibrosis staging, which uses a single patch as input.As shown in Table 1, our method outperformed the SingleView method by 10.3% and 12% in AUC on the two tasks, respectively, indicating that the proposed method could exploit more informative features than the method using single view. Our method also set the new state of the art, when compared with other multi-view learning methods. This could be due to the fact that our method was able to capture both the global and local features, and the uncertainty-aware fusion strategy could be more robust than the methods with implicit fusion strategies.Comparison with Uncertainty-Aware Methods. To demonstrate reliability, we compared the proposed method with other methods. Specifically, these methods estimate uncertainty using Monte-Carlo dropout (Dropout) [20], variational inference (VI) [19], ensemble [4], and softmax entropy [16], respectively. Following [6], we evaluated the expected calibration error (ECE), which measures the gap between model confidence and expected accuracy.  Table 2 shows that our method achieved better results in ACC and AUC for both tasks than the other uncertainty-ware multi-view learning methods. It indicates that the uncertainty in our framework could paint a clearer picture of the reliability of each view, and thus the final prediction was more accurate based on the proposed scheme of rule-based combination. Our method also achieved the lowest ECE, indicating that the correspondence between the model confidence and overall results was more accurate.Interpretability. The proposed framework could explain which view of the input image contains more decisive information for liver fibrosis staging through uncertainties. To evaluate the quality of explanations, we compared the estimated uncertainties with annotations from experienced physicians. Views that contain more signs of fibrosis are supposed to have lower uncertainties. According to Fig. 3, the predicted uncertainties are consistent with annotations in local views of the S4 sample (a). In the S1 sample (b), the uncertainty of global view is low. It is reasonable since there are no visible signs of fibrosis in this stage. The model needs to capture the entire view to discriminate the S1 sample. Ablation Study. We performed this ablation study to investigate the roles of local views and global view, as well as to validate the effectiveness of the data-efficient transformer. Table 3 shows that using the global view solely achieved the worst performance in the staging of cirrhosis. This means that it could be difficult to extract useful features without complementary information from local views. This is consistent with Fig. 3(a), where the uncertainty derived from the global view is high, even if there are many signs of fibrosis. While in Fig. 3(b), the uncertainty of the global view is low, which indicates that it is easier to make decisions from the global view when there is no visible sign of fibrosis. Therefore, we concluded that the global view was more valuable in identifying substantial fibrosis. Compared with the method that only used local views, our method gained more improvement in the substantial fibrosis identification task, which further confirms the aforementioned conclusion. Our method also performed better than the method that applied a convolution neural network (CNN) for the global view. This demonstrates that the proposed data-efficient transformer was more suitable for the modeling of global representation than CNN."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,4.0,Conclusion,"In this work, we have proposed a reliable and interpretable multi-view learning framework for liver fibrosis staging. Specifically, uncertainty is estimated through subjective logic to improve reliability, and an explicit fusion strategy is applied which promotes interpretability. Furthermore, we use a data-efficient transformer to model the global representation, which improves the performance."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 18.
