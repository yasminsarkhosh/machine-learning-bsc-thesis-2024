Paper Title,Header Number,Header Title,Text
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,1.0,Introduction,"Deep learning-based systems show remarkable predictive performance in many computer vision tasks, including medical image analysis, and are often comparable to human performance. However, the complexity of this technique makes it challenging to extract model knowledge and understand model decisions. This limitation is being addressed by the field of Explainable AI, in which significant progress has been made in recent years. An important line of research is the use of inherently explainable models, which circumvent the need for indirect, errorprone on-top explanations [14]. A common misconception is that the additional explanation comes with a decrease in performance. However, Rudin et al. [14] and others have already pointed out that this can be avoided by designing algorithms that build explainability into the core concept, rather than just adding it on top. Our work proves this once again by providing a powerful and explainable solution for medical image classification.A promising approach for interpretability is the use of Privileged Information, i.e. information that is only available during training [19,20]. Besides using the additional knowledge to improve performance, it can also help to increase explainability, as has already been shown using the LIDC-IDRI dataset [3]. In addition to the malignancy of the lung nodules, which is the main goal of the prediction task, the radiologists also marked certain nodule characteristics such as sphericity, margin or spiculation. Shen et al. [16] used the attributes with a hierarchical 3D CNN approach, demonstrating the potential of using this privileged information. LaLonde et al. [11] extended this idea using capsule networks, a technique for learning individual, encapsulated representations rather than general convolutional layers [1,15]. This method was used to jointly learn the predefined attributes in the capsules and their associations with the classification target, i.e. malignancy. Explainability is enabled by providing additional attribute values that are essential to the model output. However, the predicted, possibly incorrect scores for the attributes must be trusted, which raises the question of whether there is a way to validate the predictions.Prototype Networks are another line of research implementing the idea that the representations of images cluster around a prototypical representation for each class [17]. The goal is to find embedded prototypes (i.e. examples) that best separate the images by their classes [5]. This idea has been applied to various methods, such as unsupervised learning [13], few-and zero-shot learning [17,18,22], as well as for capsule networks [21], however without the use of privileged information. A successful approach is prototypical models with case-based reasoning, which justify their prediction by showing prototypical training examples similar to the input instance [4,12]. This idea can be used for region-wise prototypical samples [6]. However, these networks can only tell which prototypical samples resemble the query image, not why. Similar to attention models, regional explanations are learned and provided [23,24]. It is up to the user to guess which features of the image regions are relevant to the network and are exemplified by the prototypes.Our method addresses the limitations of privileged information-based and prototype-based explanation by combining case-based visual reasoning through exemplary representation of high-level attributes to achieve explainability and high-performance. The proposed method is an image classifier that satisfies explainable-by-design with two elements: First, decisive intermediate results of a high-performance CNN are trained on human-defined attributes which are being predicted during application. Second, the model provides prototypical natural images to validate the attribute prediction. In addition to the enhanced explainability offered by the proposed approach, to our knowledge the proposed method outperforms existing studies on the LIDC-IDRI dataset.The main contributions of our work are:-A novel method that, for the first time to our knowledge, combines privileged information and prototype learning to provide increased explanatory power for medical classification tasks. -A prototype network architecture based on a capsule network that leverages the benefits of both techniques. -An explainable solution outperforming state-of-the-art explainable and nonexplainable methods on the LIDC-IDRI dataset.We provide the code with the model architecture and training algorithm of Proto-Caps on GitHub."
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,2.0,Methods,"The idea behind our approach is to combine the potential of attribute and prototype learning for a powerful and interpretable learning system. For this, we use a capsule network of attribute capsules from which the target class is predicted. As the attribute prediction can also be susceptible to error, we use prototypes to explain the predictions made for each attribute. Based on [11], our approach, called Proto-Caps, consists of a backbone capsule network. The network is trained using multiple heads. An attribute head is used to ensure that each capsule represents a single attribute, a reconstruction head learns the original segmentation, and the main target prediction head learns the final classification. The model is extended by a prototype layer that provides explanations for each attribute decision. The overall architecture of Proto-Caps is shown in Fig. 1.The backbone of our approach is a capsule network consisting of three layers: Features of the input image of size 1×32×32 are extracted by a 2D convolutional layer containing 256 kernels of size 9×9. We decided not to use 3D convolutional layers, as preliminary experiments showed only marginal differences (within std. dev. of results), but required significantly more computing time. The primary capsule layer then segregates low-level features into 8 different capsules, with each capsule applying 256 kernels of size 9 × 9. The final dense capsule layer consists of one capsule for each attribute and extracts high-level features, overall producing eight 16-dimensional vectors. These vectors form the starting point for the different prediction branches.The target head, a fully connected layer, combines the capsule encodings. The loss function for the malignancy prediction was chosen according to LaLonde et al. [11], where the distribution of radiologist malignancy annotations is optimized with the Kullback-Leibler divergence L mal to reflect the inter-observer agreement and thus uncertainty. The reconstruction branch to predict the segmentation mask of the nodule consists of a simple decoder with three fully connected layers with the output filters 512, 1024, and the size of the resulting image 1 × 32 × 32. The reconstruction loss L recon implements the mean square error between the output and the binary segmentation mask. It has been shown that incorporating reconstruction learning is beneficial to performance [11]. For the attribute head, we propose to use fully connected layers, instead of determining the attribute manifestation by the length of the capsule encoding, as was done previously [11]. Each capsule vector is processed by a separate linear layer to fit the respective attribute score. We formulate the attribute loss aswhere Y a is the ground truth mean attribute score by the radiologists, O a is the network score prediction for the a-th attribute, and b is a random binary mask allowing semi-supervised attribute learning. Two prototypes are learned per possible attribute class, resulting in 8-12 prototypes per attribute (i.e. capsule). During the training, a combined loss function encourages a training sample to be close to a prototype of the correct attribute class and away from prototypes dedicated to others, similar to existing approaches [6]. Randomly initialized, the prototypes are a representative subset of the training dataset for each attribute after the training. For this, a cluster cost reduces the Euclidean distance of a sample's capsule vector O a to the nearest prototype vector p j of group P as which is dedicated to its correct attribute score.In order to clearly distinguish between different attribute specifications, a separation loss is applied to increase the distance to the capsule prototypes that do not have the correct specification, limited by a maximum distance:Prototype optimization begins after 100 epochs. In addition to fitting the prototypes with the loss function, each prototype is replaced every 10 epochs by the most similar latent vector of a training sample. The original image of the training sample is stored and used for prototype visualization. During inference, the predicted attribute value is set to the ground truth attribute value of the closest prototype, ignoring the learned dense layers in the attribute head at this stage.The overall loss function is the following weighted sum, where λ recon = 0.512 was chosen according to [11], and the prototype weights were chosen empirically:"
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"Data. The proposed approach is evaluated using the publicly available LIDC-IDRI dataset consisting of 1018 clinical thoracic CT scans from patients with Non-Small Cell Lung Cancer (NSCLC) [2,3]. Each lung nodule with a minimum size of 3 mm was segmented and annotated with a malignancy score ranging from 1-highly unlikely to 5-highly suspicious by one to four expert raters. Nodules were also scored according to their characteristics with respect to predefined attributes, namely subtlety (difficulty of detection, 1-extremely subtle, 5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification (1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined, 5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). The pylidc framework [7] is used to access and process the data. The mean attribute annotation and the mean and standard deviation of the malignancy annotations are calculated. The latter was used to fit a Gaussian distribution, which serves as the ground truth label for optimization. Samples with a mean expert malignancy score of 3-indeterminate or annotations from fewer than three experts were excluded in consistency with the literature [8,9,11].Experiment Designs. To ensure comparability with previous work [8,9,11], the main metric used is Within-1-Accuracy, where a prediction within one score is considered correct. Five-fold stratified cross-validation was performed using 10 % of the training data for validation and the best run of three is reported.The algorithm was implemented using the PyTorch framework version 1.13 and CUDA version 11.6. A learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other learnable parameters. The batch size was set to 128 and the optimizer was ADAM [10]. With a maximum of 1000 epochs, but stopping early if there was no improvement in target accuracy within 100 epochs, the experiments lasted an average of three hours on a GeForce RTX 3090 graphics card. The code is publicly available at https://github.com/XRad-Ulm/Proto-Caps.Besides pure performance, the effect of reduced availability of attribute annotations was investigated. This was done by using attribute information only for a randomly selected fraction of the nodules during the training.To investigate the effect of prototypes on the network performance, an ablation study was performed. Three networks were compared: Proto-Caps (proposed) including learning and applying prototypes during inference, Proto-Caps w/o use where prototypes are only learned but ignored for inference, and Proto-Caps w/o learn using the proposed architecture without any prototypes."
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,4.0,Results,"Qualitative. Figure 2 shows examples of model output. The predicted malignancy score is justified by the closest prototypical sample of a certain attribute. The respective original image for each attribute prototype is being saved during the training process and used for visualization during inference. In case B, there are large differences between the margin and lobulation prototype and the sample. Similarly, in case C, the spiculation prediction is very different from the sample.During application, these discrepancies between the prototypes and the sample nodule raise suspicion, and help to assess the malignancy prediction. A quantitative evaluation of the relationship between correctness in attribute and in target prediction using logistic regression analysis shows a strong relationship between both with an accuracy of 0.93/0.1.Quantitative. Table 1 shows the results of our experiments compared to other state-of-the-art approaches, with results taken from original reports. The accuracy of the proposed method exceeds previous work in both the malignancy and almost all attribute predictions, while modelling all given attributes.Table 2 lists the results obtained when only fractions of the training samples come with attribute information. The experiments indicate that the performance of the given approach is maintained up to a fraction of 10 %. Using no attribute annotations at all, i.e. no privileged information, achieves a similar performance, but results in a loss of explainability, as the high-level features extracted in the capsules are not understandable to humans. This result suggests that privileged information here leads to an increase in interpretability for humans by providing attribute predictions and prototypes without interfering with the model performance. The ablation study shows no significant differences between the three models evaluated. For the malignancy accuracy, Proto-Caps w/o use and Proto-Caps w/o learn achieved μ = 93.9 % (σ = 0.8) and μ = 93.7 % (σ = 1.1), respectively. The average difference in attribute accuracy compared to the proposed methods is 1.7 % and 1.5 % better, respectively, and is more robust across experiments. The best result was obtained when the prototypes were learned but not used, possibly indicating that the prototypes may have a regularising effect during training, but further experiments are needed to confirm this due to the close results. To give an indication of the decoder performance, Proto-Caps w/o use achieved a dice score of 79.7 %. "
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,5.0,Discussion and Conclusion,"We propose a new method, named Proto-Caps, which combines the advantages of privileged information, and prototype learning for an explainable network, achieving more than 6 % better accuracy than the state-of-the-art explainable method. As shown by qualitative results (Fig. 2), the obtained prototypes can be used to detect potential false classifications. Our method is based on capsule networks, which allow prediction based on attribute-specific prototypes. Compared to class-specific prototypes, our approach is more specific and allows better interpretation of the predictions made. In summary, Proto-Caps outputs prediction results for the main classification task and for predefined attributes, and provides visual validation through the prototypical samples of the attributes.The experiments demonstrate that it outperforms state-of-the-art methods that provide less explainability. Our data reduction studies show that the proposed solution is robust to the number of annotated examples, and good results are obtained even with a 90% reduction in privileged information. This opens the door for application to other datasets by reducing the additional annotation overhead. While we did see a reduction in performance with too few labels, our results suggest that this is mainly due to inhomogeneous coverage of individual attribute values. In this respect, it would be interesting to find out how a specific selection of the annotated samples, e.g. with extremes, affects the accuracies, especially since our results show that the overall performance is robust even when the attributes are not explicitly trained, i.e. without additional privileged information. Another area of research would be to explore other types of privileged information that require less extra annotation effort, such as medical reports, to train the attribute capsules. It would also be worth investigating more sophisticated 3D-based capsule networks.In conclusion, we believe that the approach of leveraging privileged information with comprehensible architectures and prototype learning is promising for various high-risk application domains and offers many opportunities for further research."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,1.0,Introduction,"Consider a longitudinal brain connectome study where each participant goes through imaging protocol multiple times over the study period. Given a population of such subjects, analyzing them can be posed as a spatio-temporal graph analysis where each sample in a dataset is given as a set of longitudinal graphs of different cardinality. An exemplar sample corresponding to this task is shown in Fig. 1a that consists of graphs from T m time points with multi-variate node features (denoted in different colors). The fundamental goal of such longitudinal studies is to characterize progressive change patterns of time-varying graphs due to certain factors such as aging [32] and neurodegenerative diseases [19,22].There are several practical bottlenecks to extract meaningful results from the longitudinal brain connectome and region-wise imaging measures. The data are temporally sparse, i.e., the participants pay a different number of visits which can be very few. Also, each brain network has a different structure of white-matter fiber connections unlike regular lattice structure in images. Last but importantly, most neuroimaging datasets suffer from lack of samples as the data are expensive to acquire and process. These spatio-temporal heterogeneities and sample-size issue make longitudinal analyses of the brain network challenging, but it must be investigated to characterize the progressive disease-relevant variations.Therefore, it is necessary to develop an efficient prediction model for a ""set of longitudinal graphs"" and corresponding regional measures (i.e., node features) over sparse time-points. Most graph neural networks are designed for a fixed template graph for predicting node values [18] or graph-level labels [33,35], where the graph topology is used as a domain and predictions heavily rely on node features. Moreover, recent spatio-temporal graph methods for a stream of images (e.g., video) often include complex architectures that require a large-scale dataset to train [2,3], which cannot be easily adopted for medical applications due to the limited sample-size. Notice that the sample-size is a much bigger issue for a longitudinal study as a single label is given for a ""set"" of graphs, as opposed to a cross-sectional study where the label is given for each graph.We tackle the aforementioned issues by designing a flexible architecture with a ""mix"" of Multi-layer Perceptron (MLP) [30] to investigate time-varying graph structure and measurements on the nodes. We propose Spatio-Temporal Graph MLP (STGMLP) which integrates the following three mixing modules: 1) graph spatial mixer (GSM), 2) graph temporal mixer (GTM), and 3) spatio-temporal mixer (STM) that extract space, time, and spatio-temporal features, respectively. The features curated from the three components are fed into a downstream classifier to discriminate labels for the sets of longitudinal graphs. The core idea is to efficiently mix features along irregular space and time with simple MLPs: brain network structure guides spatial mixing as a graph, and temporal pooling extracts the most effective features from disjoint time-space across subjects."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,,Contributions of,"Our Work: our model 1) can be trained efficiently compared to existing spatio-temporal graph deep models with a significantly reduced number of parameters, 2) flexibly incorporates irregular space and time into prediction, 3) yields interpretable results that quantify the contribution of each node  contains F node features for N nodes and E i ∈ R N ×N is a weighted adjacency matrix whose elements denote connection strength between two nodes. Given a population of G m with C classes, STGMLP aims to classify the label of each G m by leveraging both temporal and spatial variations of the graph set from different groups. Note that the label of each sample (i.e., longitudinal graph set) is consistent over time."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,,Overview of STGMLP. STGMLP mixes graph features across space and time,"with Graph Spatial Mixer (GSM) and Graph Temporal Mixer (GTM), respectively. GSM performs a per-graph operation (i.e., node-mixing) and GTM accounts for cross-temporal operation (i.e., graph-mixing) between multiple graphs in a G m . Figure 1b and1c show inputs to node-mixing and graph-mixing MLPs. The node-mixing MLP projects node features from a local neighborhood of each node (i.e., local graph) onto a latent space. On the other hand, the graph-mixing MLP extracts hidden relationships between graphs across time by projecting spatially equivalent local graphs from multiple G i 's to the same latent space.The overall structure of STGMLP is shown in Fig. 2, which integrates GSMs, GTMs, a Spatio-Temporal Mixer (STM), and a downstream classifier. Due to the heterogeneous number of timepoints T m across samples, pooling operations are applied to the outputs of GSMs and GTMs to reduce them into a coherent dimension. Taking outcomes from the pooling layers, the STM fuses both spatial and temporal features. The fused feature is combined with its inputs through a skip-connection [14] to maximize the use of multi-level (i.e., space, time, and spatio-temporal) information extracted from the input. Finally, a downstream classifier takes the mixed features to predict labels for a given longitudinal graph set G m . The details of each module and variables are given below.Graph Spatial Mixer. GSM encodes node features and a graph structure of a graph G i with graph convolution. The node-mixing MLP (R F → R F ) acts on rows of X i , and it is shared across G m for all N × T m nodes. Let f (•) be an operation of node-mixing MLP which takes Ẽi and X i as inputs, where Ẽi is a normalized E i to ensure unbiased strength of the connectivity. It includes selfconnections I N (i.e., identity matrix) and computed as Ẽi = D- [1] is applied across all features to prevent biased learning from unbalanced node feature distributions. Including two fullyconnected (FC) layers and a GELU nonlinearity [15] σ(•), the MLP operates independently on each j-th node with two-layer graph convolutions [18] aswhere W 0 and W 1 are trainable weights and LN(•) is a layernorm function. The output (X i ) j ∈ R F accounts for a latent vector of local graph structure at node j. Stacking (X i ) j s up to the number of nodes N , an outcome X i ∈ R N ×F is derived for an input G i . In this way, a set of whole outputs from T m GSMs is derived as {X i } Tm i=1 ∈ R Tm×N ×F . Notice that the GSM can be stacked D times by iteratively taking the X i as an updated input to encode a wider range of local graph structures. After performing max pooling on T m and F dimensions of {X i } Tm i=1 , the condensation of spatial features across G m = {G 1 , ..., G Tm } is obtained as a N -dimensional vector S.Graph Temporal Mixer. GTM performs a cross-temporal operation on multiple ""pairs"" of graphs. This graph-mixing encodes the relations between graphs of different time-points. Given T m graphs from a subject, P pairs of graphs, each pair as a set {G i1 , G i2 }, are selected where P is a user parameter. For each pair for {i 1 , i 2 }, an averaged connectivity Ẽp =( Ẽi1 + Ẽi2 )/2 and X p ∈ R N ×2F as a concatenation of X i1 and X i2 are inputted into the graph-mixing MLP g(•). In our work, we choose to input pairs of temporally adjacent graphs together with the first-and-last graph pair to encode a temporal sequence. The g(•) acts on rows of X p , mapping R 2F → R F . It transforms the features of node j (i.e., (X p ) j ) into F -dimensional latent vector, and the projection is performed across the whole node pairs in parallel. Similar to the node-mixing MLP, graph-mixing MLP contains two FC layers with weights W 2 and W 3 and a GELU σ(•) as(2)As in the GSM, each (X p ) j is stacked N times to be a X p . For P GTMs, an output {X p } P p=1 ∈ R P ×N ×F is obtained and reduced into T ∈ R N by max pooling on P pairs and F node features. Note that, unlike GSMs, Ẽp is used only once in Eq. ( 2). Using Ẽp multiple times causes encoding of a wider range of local graph structures, unnecessarily encompassing graph spatial relations (i.e., non-adjacent neighbors) for extracting temporal relations of j-th node pairs. Spatio-Temporal Mixer. To capture comprehensive spatio-temporal relations across the whole graphs in G m , the spatial and temporal features, i.e., S and T , are embedded into a latent vector F ∈ R N in STM. To do so, the S and T are stacked as X f = [S, T ], where (X f ) j represents the spatial and temporal node features of the j-th node. A spatio-temporal mixing MLP h(•) is applied to (X f ) j for all j's in parallel with an averaged edge matrix Ẽf across G m aswhere W 4 and W 5 are weight matrices and σ(•) is a nonlinearity. With this STM, irregular space and time components can be flexibly integrated into a prediction.Longitudinal Graph Classifier. To take a full advantage from the extracted features, S and T are combined together with F via a skip connection. These features collected from diverse branching paths contain both low and high-level information extracted from the graphs, and their integration provides strong ensemble-like results [31]. Using a FC layer and softmax, a set of predicted label probabilities Ŷ is obtained for C classes aswhere W 6 is a set of trainable weights of the FC layer for class prediction. Given the ground truth Y , the cross-entropy loss is defined with 2 -regularization aswhere W is a set of trainable parameters and λ controls a regularization strength."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.0,Experiments,"In this section, we evaluate STGMLP on two independent datasets, i.e., ADNI and ABCD, whose demographics are given in Table 1. We discuss the quantitative results, model behavior, and neuroscientific interpretations below."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.1,Materials and Setup,"ADNI Dataset. The ADNI is the largest public AD dataset providing longitudinal and multimodal images such as magnetic resonance imaging (MRI) and positron emission tomography (PET). As node features, cortical thickness from MRI, standardized uptake value ratio (SUVR) from FDG-PET and Amyloid-PET at all ROIs were measured. Structural brain networks were obtained by inhouse probabilistic tractography on diffusion tensor images (DTI) on Destrieux ABCD Dataset. The ABCD dataset (v4.0) contains two timepoints with multivariate features: baseline data for children aged 8-10 and their 2-year follow-up measurements such as fractional anisotropy, mean diffusivity, and cortical thickness obtained via DTI and MRI. Morphometric similarity network [27] was used to construct a graph per subject. As in other works [5,9,24,34] studying the relationship between socioeconomic status (SES) and brain development on the ABCD, we categorized the longitudinal samples into Below-Poverty (BP) and Non-Poverty (NP) groups based on the annual household income. The poverty criterion from U.S. Census Bureau ($27,479) is used to set the BP group, and the NP group is set whose annual household income is $200k and greater.Setup. As baselines, we adopt various graph convolutional networks (GCNs) for spatio-temporal graph analysis such as ST-GCN [11], IT-GNN [16], infoGCN [4], ShiftGCN [3], and CTRGCN [2]. Also, typical machine learning (ML) methods such as Linear SVM, Linear Regression (LR) and MLP are used for comparisons. Along with the all node and edge features, the maximum time difference X Tm -X 1 is used to train these ML models. We applied early stopping via test loss with 5-fold cross validation (CV) for all methods including ours.To implement STGMLP, the learning rate, weight decay (λ), dropout rate, and depth D of GSM were set to 1e-2, 5e-4, 5%, and 3, respectively. For GTMs on the ADNI, total P =T m pairs are selected: T m -1 pairs for adjacent graphs in time, and one pair for the first and last (i.e., end-to-end) timepoints. For the ABCD, P is set to 1 as T m =2 for all samples. Note that, the combination of timepoints can be flexibly selected to include domain knowledge."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.2,Evaluation and Discussions on the Results,"Quantitative results (i.e., mean accuracy, precision, and recall) of all experiments and the number of trainable parameters are compared in Table 2. The results demonstrate that our model with a small computational cost outperformed baselines with vast parameters on both datasets. Also, our method showed no overfitting, as the mean training accuracies for ADNI and ABCD were 74.9% and 73.6%. Moreover, it is worth noting that the improvement in performance comes from the effectiveness of our method, not solely from the reduction in the number of parameters. On ADNI, our model even showed a 20.9%p accuracy margin over IT-GNN, which has a similar parameter scale (5k) as ours (1k).Preclinical vs. MCI vs. AD on ADNI Dataset. Here, STGMLP achieved 71.3% accuracy with 7.8∼24.0%p margin over baselines. Here, we provide clinically interpretable results by analyzing nodal contributions to classify each class via class-averaged Grad-CAM [28]. In Table 3, we reported the top 10 regions with the highest gradient activation for AD group classification, which are mostly distributed in the temporal and frontal regions. The ROI showing the highest activation is the right superior frontal gyrus, which is a majorly damaged area where atrophy of white matter is discovered in various AD studies [13,17,25]. Also, both sides of the superior temporal gyri and sulci were found, which are highly activated in auditory and verbal memory processing [20,36]. The visualization of averaged class-wise activations for all ROIs is shown in Fig 3.Below-Poverty vs. Non-poverty on ABCD Dataset. As shown in Table 3, ROIs that played a decisive role to classify the BP class are mostly distributed along the insular and occipital regions. For example, insular subregions such as the long insular gyrus and central sulcus of the insula and both left and right sides of the short insular gyri were identified, which are implicated to social decision making [23,26] and emotional processing [6,12]. These ROIs responsible for somatosensory are thought to be impacted by environmental exposures such as SES and play a key role in overall cognition in children [7,10]. Also, developmental changes in occipital subregions such as the superior occipital gyrus and the middle occipital sulcus are closely related to the parental SES [21,29], which appear to be consistent with the result of our experiment."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.3,Temporal Analysis on AD-Specific Activation,"In Fig. 3, we also investigated pairwise temporal features from the ADNI experiment. While the Preclinical group shows strong activations in the initial and end-to-end pairs, features of later timepoints are highly activated in MCI group, showing consistency to the neurodegenerative dynamics in AD. We also observed that the use of pairwise information is sufficient to analyze the entire time series, as the signs of trained weights (i.e., W 2 in GTM) were totally opposite between adjacent timepoints, i.e., averaged weights were -7.7e-2 (std:0.56) vs. +4.4e-2 (std:0.29). In this way, our method investigates network alterations (around 1-2 year) in a pairwise manner and captures the whole temporal variation via pooling, rather than directly looking at the whole changes (over several years). These results confirm that GTM can capture the key temporal features for a given pair and label, and the following nonlinear function intensifies the difference."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.4,Ablation Study on Hyperparamters,"Ablation studies on each module (i.e., GSM, GTM, and STM) and pooling methods were performed on ADNI. The results, reported in Table 4, show that using max pooling for both GSM and GTM performed best. This suggests that there exist particularly significant time-points (or time-points pairs) for classifying longitudinal brain networks and strongly reflecting these points in decision making is more useful than smoothing features for the entire time with average pooling."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,4.0,Conclusion,"In this work, we proposed a novel longitudinal graph mixer to investigate longitudinal variations of spatio-temporal graphs. The idea was driven by mixing features temporally and spatially along the topology of brain networks, and its structure was designed to deal with the heterogeneity of data with a significantly reduced number of parameters compared to deep graph convolutional models.Experiments validate the superiority of our framework, successfully identifying key ROIs in classifying different classes, suggesting a significant potential to be deployed for other longitudinal connectome analyses of various brain disorders."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible neurodegenerative disorder that affects millions of people worldwide [5]. In recent years, brain imaging genetics has emerged as a promising field for the diagnosis and prediction of AD and its prodromal stage -mild cognitive impairment (MCI). This approach largely focuses on using neuroimaging techniques, such as MRI and PET, to identify brain regions that are associated with specific genetic variants such as single nucleotide polymorphisms (SNPs). Such analyses have produced a wealth of research findings [23,26,28] that have demonstrated significant associations between imaging characteristics and genetics in AD, and have the great potential to identify new multimodal biomarkers affecting specific brain systems and provide an enormous impetus for drug discovery. and Y is the label information. DNNs first operate on each modality, generating hidden representations for each modality. These hidden representations go through a selfattention mechanism generating improved self-attention representations. At the same time, the hidden representations and label Y are multiplied by individual projection matrices U1, . . . , U4, Uy based on CCA, thus mapping them to a shared representation G. Finally, the disease prediction is calculated by self-attention representations with projection matrices and shared representation G.In the literature, various methods have been proposed to brain imaging genetics analysis [3,[9][10][11]13,19,27,29]. In particular, canonical correlation analysis (CCA) [12] is a powerful multivariate statistical technique for quantifying the associations between different sets of data. CCA and its variations have been widely applied in imaging genetics studies because of its advantages in biological interpretation. For example, Du et al. [8] proposed a joint multitask sparse canonical correlation analysis and classification (MTSCCALR) for identifying imaging genetics biomarkers of AD. Kim et al. [16] introduced a multi-task learning-based structured sparse canonical correlation analysis (MTS2CCA) for identifying brain imaging genetics related to sleep. Moon et al. [20] proposed a supervised deep generalized canonical correlation analysis (SDGCCA) for improving the classification of phenotypes and revealing biomarkers associated with phenotypes in the context of AD. Despite much progress made in this area, CCA-based traditional shallow models assume that the relationships between genetic and imaging data are linear. However, this may not always be the case, and nonlinear relationships may exist in brain imaging genetics data, leading to biased results. On the other hand, the existing CCA-based deep models do not provide a direct interpretation of the underlying biological mechanisms driving the observed associations between genetic and imaging data. Most of them explored post-hoc explanations as justifications for model predictions. This can limit the ability to translate findings into clinically relevant insights.In this paper, we propose a novel attentive deep canonical correlation analysis (ADCCA) model for diagnosing AD disease and discovering biomarkers using multimodal brain imaging genetics data. As illustrated in Fig. 1, the proposed framework comprises three key components: (i) deep neural network (DNN) modeling for generating latent representations of each modality to capture intramodality correlations; (ii) attention update mechanism for focusing on the most salient regions of input data; and (iii) nonlinear supervised CCA modeling for integrating multiple modalities to discriminate phenotypic groups. By combining the power of these techniques, the ADCCA approach effectively models nonlinear relationships among multimodal imaging genetics data and provides simultaneous predictions and interpretations. The model is trained end-to-end using a combination of classification and correlation losses.Through extensive experiments on the real-world ADNI dataset with three imaging modalities (VBM-MRI, FDG-PET, and AV45-PET) and genetic SNP data, we show that our model achieves outstanding performance for classifying AD vs. HC, AD vs. MCI, and MCI vs. HC groups. Also, it is demonstrated that the model explanation can reveal disorder-specific biomarkers coinciding with neuroscience findings. Last, we show that the combination of classification and correlation models can boost disease prediction performance."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,2.0,Method,"Suppose that the problem includes N subjects with M modalities. Let X m ∈ R N ×dm denote the m-th modality data, where d m represents the dimension of features in the m-th modality,the label information of all subjects. In this work, we seek to learn a disease prediction model that estimates Ŷ from {X m } M m=1 by making full use of all M modalities, as well as identify disease-specific biomarkers for clinical interpretation.The proposed ADCCA aims to combine the strengths of DNN, attention mechanism, and CCA to integrate and exploit the complementary information from multiple data modalities (Fig. 1). First, we use a separate DNN containing several fully-connected hidden layers to learn hidden representations for each modality, denoted as f m (X m ) ∈ R N ×lm , where l m represents the dimension of last layer of DNN corresponding to the m-th modality. Second, we employ the attention mechanism [25] on the basis of the DNN model. With the help of the attention mechanism, our method can explicitly capture the important features hidden in the input data. Specifically, we use self-attention, sometimes called intra-attention, which is regarded as an improvement in attention that focuses on internal links of features and reduces external data dependency to compute its representation. Suppose there are three linear transformation matrices for the mth modality:Mathematically, the self-attention representation of f m (X m ) can be calculated as:Third, following [20], we learn cross-modality features and incorporate the label information of samples for supervised learning based on CCA. The correlation loss function is defined as follows:where U 1 , • • • , U 4 , U y are projection matrices for each modality and label information, respectively. I denotes the identity matrix. According to Eq. ( 2), we haveThus, the label Y can be approximated as follows:, where U † y denotes the pseudo-inverse of U y . Then, we substitute self-attention representations that are more representative of each modality into the above equation and letFurther, the conventional supervised crossentropy loss [7] is used to enable the propagation of label information directly to the DNN of each modality.CrossEntropy Y, Softmax( Ŷm ) .(The final label prediction of ADCCA can be obtained using the following soft voting of the label presentation of each modality: Ŷ = Softmax(( M m=1 Ŷm )/M ). Overall, our final training objective can be defined as:where L cls is the supervised cross-entropy disease prediction loss, L cor is the correlation loss, and λ is a tunable hyperparameter that scales the numerical value of each loss item to the same order of magnitude to balance their influence. The solution on loss function L is similar to the SGDCCA method except for substituting the outputs of DNN models to their self-attention representations.3 Experiments and Results"
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.1,Data Acquisition and Preprocessing,"Brain imaging genetic data used in this study were obtained from the public ADNI database [22]. There is a total of 597 participants with both geno-type and brain imaging data, including 104 AD, 305 MCI, and 188 healthy control (HC) subjects. The image data consisted of three modalities including structural Magnetic Resonance Imaging (VBM-MRI), 18 F-fluorodeoxyglucose Positron Emission Tomography (FDG-PET), and 18 F-florbetapir PET (AV45-PET). These three imaging modalities allowed us to examine brain structure, glucose metabolism, and amyloid plaque deposition, respectively. Following the previous studies [2,30], we preprocessed neuroimaging data to extract ROI-based features. Specifically, the multi-modality imaging scans were aligned to each participant's same visit. All imaging scans were aligned to a T1-weighted template image, and segmented into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) maps. They were normalized to the standard Montreal Neurological Institute (MNI) space as 2 × 2 × 2 mm 3 voxels, being smoothed with an 8 mm FWHM kernel. We preprocessed the structural MRI scans with voxel-based morphometry (VBM) by using the SPM software [1], and registered the FDG-PET and AV45-PET scans to the MNI space by SPM. We subsampled the whole brain imaging and contained 90 ROIs (excluding the cerebellum and vermis) based on the AAL-90 atlas [24]. ROI-level measures were calculated by averaging all the voxel-level measures within each ROI.For genetic SNP data, according to the AlzGene database1 , only the SNPs belonging to top AD gene candidates were selected. The genetic data were genotyped by the Human 610-Quad or OmniExpress Array platform (Illumina, Inc., San Diego, CA, USA), and preprocessed following standard quality control and imputation procedures. There were 54 SNPs included which were collected from the neighbor of AD risk gene APOE according to the ANNOVAR annotation."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.2,Evaluation of Disease Classification Performance,"In our experiments, the whole data were separated into three groups, including AD vs. HC, AD vs. MCI, and MCI vs. HC. To quantitatively evaluate the performance of different methods, we considered four commonly-used evaluation metrics: accuracy (ACC), F1-score (F1), area under receiver operating characteristic curve (AUC), and Matthews correlation coefficient (MCC) [6]. Since the number of subjects was limited, we calculated the mean and standard deviation of all metrics using 5-fold cross-validation (CV). Many researchers have successfully adopted multimodal brain imaging data into CCA. We carefully choose five related methods for comparison: 1) vanilla DNN [18], 2) generalized CCA (GCCA) [15], 3) deep generalized CCA (DGCCA) [4], 4) MTSCCALR [8], and 5) SDGCCA [20]. Note that GCCA and DGCCA are unsupervised learning methods, and the others are supervised learning methods. The proposed model includes four DNNs, one for each modality, with three fully-connected layers and a Tanh activation function, which is trained with Adam optimizer with the learning rate set to 0.0001 and weight decay set to 0.001.Table 1 presents the classification results, where ± represents the standard deviation of evaluation scores across the 5 folds. From the results, it can be observed that the proposed ADCCA method significantly outperforms all other methods in terms of all four metrics. The higher AUC and MCC scores indicate that our method is able to accurately identify both positive and negative cases of AD. The smaller standard deviations of ADCCA illustrated the overall stability and reproducibility of the experiment. "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.3,The Most Discriminative Brain Regions and SNPs,"Identifying the most discriminative brain regions (i.e., ROIs) and SNPs is crucial for AD diagnosis. Here, we employed the integrated gradients interface provided by Captum [17] to assign importance scores to each feature of different modalities by analyzing the pre-trained model, which can provide a comprehensive explanation of how the input features of a deep learning model contribute to the model's output. The reason why not using the self-attention weights is that we use the self-attention to assign attention scores to hidden representations instead of the original features, thus it may not fully capture the importance of the original features in the input data. Figure 2(a-c) shows the top 20 discriminative ROIs identified by the proposed method from each individual brain imaging modality. Figure 2(d) shows the top 20 discriminative ROIs selected by the average importance scores of ROIs from the three modalities. We found that the hippocampal, amygdala, uncus, and gyrus regions are only identified by using the three modalities together. These selected regions are known to be highly related to AD and MCI in previous studies [21]. Besides, the result shows that the selected ROIs exhibited differences across different classification groups, indicating that our model can effectively differentiate the important ROIs for specific diseases. Figure 3 shows the most frequently selected SNPs with importance scores. The result indicates that rs6448453, rs3865444, and rs2718058 are the most discriminative SNPs which is consistent with previous evidence [14]. "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.4,Ablation Study,"The proposed ADCCA is trained using both correlation and classification losses.To understand the impact of each loss on classification, we conducted ablation studies by evaluating the performance of two additional models: the ADCCA model trained without the correlation loss (w/o L cor ) and without the classification loss (w/o L cls ). The results presented in Table 2 indicate that ADCCA outperforms the other two models for all evaluation metrics on all three classification tasks, suggesting that both correlation and classification losses contribute to ADCCA's improved performance. Removing either loss leads to decreased performance, and the impact will be particularly significant if the classification loss is eliminated. "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.5,Hyperparameter Analysis,"We investigated the impact of two important hyperparameters in the ADCCA model: λ, which appears in the loss function to balance the classification and correlation losses, and the dimension of the shared representation G. In order to explore the effects of these hyperparameters on the performance of the model, we conducted experiments using different values of λ and the shared representation dimensionality. Due to the space limit, we only report the classification results in AD vs. HC group, as shown in Fig. 4. The results in other groups can be found in the supplementary material. We observed that decreasing the value of λ generally leads to improved model performance across various tasks, but a lambda value of zero causes the model's performance to deteriorate. This may indicate that for the ADCCA model, L cls is more important than L cor . Furthermore, combining these two loss functions to jointly guide the model can lead to improved model performance. We also found that for the AD vs. HC group, the model achieves good performance even with a low-dimensional shared representation. However, for other groups, the impact of the shared representation dimension on the model's performance seems not significant. One explanation for this could be that the AD vs. HC group exhibits distinct feature differences, allowing the original features to be well represented even when mapped into a low-dimensional shared representation. "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,4.0,Conclusion,"In this work, we propose a novel deep canonical correlation analysis method for multimodal Alzheimer's disease diagnosis that leverages attention mechanisms to enhance interpretability and multimodal feature learning. Experimental results on the real-world imaging-genetics dataset demonstrate that our approach achieves better classification performance than the existing state-of-theart methods in terms of both classification accuracy and correlation between the modalities. In an exploratory analysis, we further show that the biomarkers identified by our model are closely associated with Alzheimer's disease. Our proposed approach is applicable to other diseases with multimodal data available. However, the limited size of medical datasets may restrict the effectiveness and generalization ability of such deep learning models. To address this issue, a potential future direction is to employ pre-training and transfer learning techniques that facilitate learning across datasets."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,,Table 1 .,AUC .570 ± .030 .540 ± .032 .574 ± .054 .637 ± .022 .796 ± .074 .816 ± .051 MCC .103 ± .058 .105 ± .075 .109 ± .100 .172 ± .045 .273 ± .110 .407 ± .073
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,1.0,Introduction,"One important task for the neuroscience community is to study atypical alterations in cortices associated with brain development, degeneration, or disorders. For this aim, recent approaches, namely interpretable and explainable deep learning, rely on the training of diagnostic or predictive deep learning models [6,12] with interpretable computations and explainable results. For the aspect of preterm birth, the classification task to differentiate between preterm and term-born infants can help distinguish fine-grained differences on brain cortical surfaces, providing valuable factors for better understanding featured infantile brain development patterns related to different factors.Although explainable deep learning methods are being actively studied in the machine learning community, they have two challenges when applying to neuroimaging data. First, existing methods typically adopt post-hoc techniques to explain a deep network [13], which is first trained for a specific classification task, and then the underlying (sparse) correlations between its input and output are analyzed offline, e.g., by backpropagating prediction gradients to the shallow layers [8]. Notably, such post-hoc approaches are established upon a common assumption that reliable explanations are the results caused by accurate predictions. This assumption could work in general applications that have largescale training data, while cannot always hold for neuroimaging and neuroscience research, where available data are typically small-sized and much more complex (e.g., high-resolution cortical surfaces containing noisy, highly redundant, and task-irrelevant information). Second, most of these methods works on gridded data (e.g., images) [2], and does not handle 3D meshes (e.g., brain cortical surfaces) [13]. For these type of data, advanced geometric deep learning methods or mapping original meshes onto a spherical surface [14] suggested promising accuracies in multiple tasks (e.g., parcellation [14], registration [9], and longitudinal prediction [4]), yet the learned models typically lack explainability.This paper presents an explainable geometric deep network, called NeuroExplainer, with applications to uncover altered infant cortical development patterns associated with preterm birth. NeuroExplainer adopts high-resolution cortical attributes as the input to develop a hierarchical attention-decoding architecture working in the sperical space. Distinct to existing post-hoc methods, the NeuroExplainer is constructed as an end-to-end framework, where finegrained explanation factors can be identified in a fully learnable fashion. Our network take advantage of the explainability to boost classification for the highdimensional neuroimaging data. Specifically, in the framework of weakly supervised discriminative localization, our NeuroExplainer is trained by minimizing general classification losses coupled with a set of constraints designed according to prior knowledge regarding brain development. These targeted regularizers drive the network to implicitly optimize the explainability metrics from multiple aspects (i.e., fidelity, sparsity, and stability), thus capturing fine-grained explanation factors to explicitly improve classification accuracies. Experimental results on the public dHCP benchmark suggest that our NeuroExplainer led to quantitatively reliable explanation results that are qualitatively consistent with representative neuroimaging studies, implying that it could be a practically useful AI tool for other related cortical surface-based neuroimaging studies."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.0,Method,"As the schematic diagram shown in Fig. 1, our NeuroExplainer works on the highresolution spherical surfaces of both brain hemispheres (each with 10, 242 vertices). The inputs are fundamental vertex-wise cortical attributes, i.e., thickness, mean curvature, and convexity. The architecture has two main parts, including an encoding branch to produce initial task-related attentions on down-sampled hemispheric surfaces, and a set of attention decoding blocks to hierarchically propagate such vertex-wise attentions onto higher-resolution spheres, finally capturing fine-grained explanation factors on the input high-resolution surfaces to boost the prediction task."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.1,Spherical Attention Encoding,"The starting components of the encoding branch are four spherical convolution blocks (i.e., EB-1 to EB-4 in Fig. 1), with the learnable parameters shared across two hemispheric surfaces. Each EB adopts 1-ring hexagonal convolution [14] followed by batch normalization (BN) and ReLU activation to extract vertexwise representations, which are then downsampled by hexagonal max pooling [14] (except in EB-4) to serve as the input of the subsequent layer. Based on the outputs from EB, we propose a learnable spherical attention mechanism to conduct weakly-supervised discriminative localization.Specifically, let F l and F r ∈ R 162×M0 be the vertex-wise representations (produced by EB-4) for the left and right hemispheres, respectively. We first concatenate them as a 324 × M 0 matrix, on which a self-attention operation [11] is applied to capturing cross-hemisphere long-range dependencies to refine the vertex-wise representations from both hemispheric surfaces, resulting in a unified feature matrix denoted as F 0 = [ Fl ; Fr ] ∈ R 324×M0 . As shown in Fig. 1, F 0 is further global average pooled (GAP) across all vertices to be a holistic feature vector f 0 ∈ R 1×M0 representing the whole cerebral cortex. Both F 0 and f 0 are then mapped by a same vertex-wise 1D convolution (i.e., W 0 ∈ R M0×2 , without bias) into the categorical space, denoted as A 0 = [A l 0 ; A r 0 ] ∈ R 324×2 and s 0 , respectively. Notably, s o is supervised by the one-hot code of subject's categorical label, by which A l 0 and A r 0 highlight discriminative vertices on the (down-sampled) left and right surfaces, respectively, considering that(1) where s o [i] (i = 0 or 1) in our study denote the prediction scores of preterm and fullterm, respectively, and 1 is an unit vector having the same row size with the subsequent matrix. Finally, we define the hemispheric attentions as, respectively, with values spatially varying and depending on the relevance to subject's category."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.2,Hierarchically Spherical Attention Decoding,"The explanation factors captured by the encoding branch are relatively coarse, as the receptive field of a cell on the downsampled surfaces (with 162 vertices after three pooling operations) is no smaller than a hexagonal region of 343 cells on the input surfaces (with 10, 242 vertices). To tackle this challenge, we design a spherical attention decoding strategy to hierarchically propagate coarse attentions (from lower-resolution spheres) onto higher-resolution spheres, based on which fine-grained attentions are finally produced to improve classification. Specifically, NeuroExplainer contains three consecutive decoding blocks (i.e., DB-1 to DB-3 in Fig. 1). Each DB adopts both the attention-gated discriminative representations from the preceding DB (except DB-1 that uses EB-4 outputs) and the local-detailed representations from the symmetric EB (at the same resolution) as the input. Let the attention-gated representations from the preceding DB beFr in , respectively, where each row of Fin has M in channels, and denotes element-wise dot product. We first upsample F l G and F r G to the spatial resolution of the current DB, by using hexagonal transposed convolutions [14] with learnable weights shared across hemispheres. Then, the upsampled discriminative representations from each hemisphere (say Fl G and Fr G ) are channel-wisely concatenated with the local representations from the corresponding EB (say F l E and F r E ), followed by an 1-ring convolution to produce a unified feature matrix, such aswhere C θ (•) denotes 1-ring conv parameterized by θ, and ⊕ stands for channel concatenation. In terms of F D , the attention mechanism described in (1) is further applied to producing refined spherical attentions and classification scores.Finally, as shown in Fig. 1, based on the fine-grained attentions over the input surfaces (each with 10, 242 vertices), we use GAP to aggregate the attentiongated representations and apply an 1D conv to output the classification score. "
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.3,Domain Knowledge-Guided Explanation Enhancement,"To perform task-oriented learning of explanation factors, we design a set of targeted regularization strategies by considering fundamental domain knowledge regarding infant brain development. Specifically, according to existing studies, we assume that human brains in infancy have generally consistent developments, while the structural/functional discrepancies between different groups (e.g., preterm and term-born) are typically rationalized [1,10]. Accordingly, we require the preterm-altered cortical development patterns captured by our NeuroExplainer to be discriminative, spatially sparse, and robust, which suggests the design of the following constraints that concurrently optimize fidelity, sparsity, and stability metrics [13] in deploying an explainable deep network.Explanation Fidelity-Aware Contrastive Learning. Given the spherical attention block at a specific resolution, we have A + i and A - j ∈ R V ×1 as the output attentions for a positive and negative subjects (i.e., preterm and fullterm infants in our study), respectively, and F + i and F - j ∈ R V ×M are the corresponding representation matrices. Based on the prior knowledge regarding infant brain development, it is reasonable to assume that A + i highlights atypically-developed cortical regions caused by preterm birth. In contrast, the remaining part of the cerebral cortex of a preterm infant (corresponding to 1 -A + i ) still growths normally, i.e., looking globally similar to the cortex of a term-born infant.Accordingly, as the illustration shown in Fig. 2(a), we design a fidelityaware contrastive penalty to regularize the learning of the attention maps and associated representations to improve their discriminative power. Letbe the holistic feature vector and its inverse for the ith (positive) sample, respectively. Similarly,denotes the holistic feature vector for the compared jth (negative) sample. By pushing f + i away from both f + i and f - j , while pulling f + i close to f - j , we define the respective loss aswhere i and j indicate any a pair of positive and negative cases from totally N training samples, and m is a margin setting as 1 in our implementation.Explanation Sparsity-Aware Regularization. According to the specified prior knowledge regarding infant brain development, the attention maps produced by our NeuroExplainer should have two featured properties in terms of sparsity. That is, the attention map for a preterm infant (e.g., A + i ) should be sparse, considering that altered cortical developments are assumed to be localized. In contrast, the attention map for a healthy term-born infant (e.g., A - j ) should not be spatially informative, as all brain regions growth typically without abnormality. To this end, we design a straightforward entropy-based regularization to enhance results' explainability, such aswhere i and j indicate a positive and a negative cases from totally N training samples, respectively, and 1 is an unit vector to sum up the values of all vertices.Explanation Stability-Aware Regularization. We enhance the explanation stability of our NeuroExplainer from two aspects. First, we require the spherical attention mechanisms to robustly decode from complex cortical-surface data finegrained explanation factors to produce accurate predictions. To this end, we randomize the surface coarsening step by quantifying a vertex's cortical attributes (on the downsampled surface) as the average of a random subset of the vertices from the respective hexagonal region of the highest-resolution surface, such as the examples summarized in Fig. 2(b). Considering that the network is trained to produce consistently accurate predictions for all these variants with perturbations, it inversely enhances the stability of learned explanation factors.Second, as described in Sect. 2.2, we design a cross-scale consistency regularization to refine the decoding branch. Specifically, let A l i and A h i be the spherical attentions from two different DB blocks. We simply minimizewhich encourages spherical attentions at different resolutions to be consistent.Implementation Details. In our implementation, the feature representations produced by EB-1 to EB-4 in Fig. 1 have 32, 64, 128, and 256 channels, respectively. Correspondingly, DB-1 to DB-3, and the final classification layer have 256, 128, 64, and 32 channels, respectively. The network was trained end-to-end by minimizing the cross-entropy classification losses defined at three different spatial resolutions (overall denoted as L CE ), coupled with the regularization terms introduced in Sec. 2.3, such aswhere the tuning parameters were empirically set as λ 1 = 0.2, λ 3 = 0.5, and λ 3 = 0.1. The network parameters were updated by using Adam optimizer for 500 epochs, with the initial learning rate setting as 0.001 and bath size as 20."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,3.0,Experiments,"Dataset and Experimental Setup. We conducted experiments on the dHCP benchmark [5].   For classification, our NeuroExplainer was compared with three representative geometric networks, including a spherical network based on 1-ring convolution (SphericalCNN) [14], a MoNet reimplementation working on spherical surfaces (SphericalMoNet) [9], and SubdivNet [3] working on original meshes. The classification performance was quantified in terms of accuracy (ACC), area under the ROC curve (AUC), sensitivity (SEN), and specificity (SPE).On the other hand, the explanation performance of our NeuroExplainer was compared with two representative feature-based explanation approaches, i.e., CAM [15] and Grad-CAM [7]. The explanation performance was quantitatively evaluated in terms of three metrics [13], i.e., Fidelity, Sparsity, and Stability. Please refer to [13] for more details regarding these metrics.Classification Results. The classification results obtained by different competing methods are summarized in Table 1, from which we can have at least two observations. 1) Our NeuroExplainer consistently led to better classification accuracies in terms of all metrics (especially SEN and AUC), suggesting that it can reliably identify featured development patterns associated with preterm birth to make accurate predictions in such an imbalanced learning task. 2) These results imply that our idea to capture fine-grained explanation factors in an endto-end fashion to boost discriminative representation extraction is beneficial for deploying an accurate classification model. 3) To check the efficacy of the prior-induced regularization strategies, we orderly removed them from the loss function (6) to quantify the respective influences. From Table 1, we can see that all the three regularizations demonstrated significant but different improvements on classification, implying their complementary roles in boosting explainable representation learning.Explanation Results. The quantitative explanation results are summarized in Table 2. Notably, the three metrics should be analyzed concurrently in evaluating a network's explainability [13], as the isolated quantification of a single metric could be biased. From Table 2, we can observe that our NeuroExplainer led to significantly better Fidelity and Stability, under reasonable Sparsity, suggesting that it can robustly identify rationalized preterm-altered cortical patterns from high-dimensional inputs for preterm infant recognition. Also, we visually compared the attention maps produced by different competing methods, with two typical examples presented in Fig. 3. From Fig. 3, we can see that, compared with post-hoc explanation methods, our end-to-end NeuroExplainer stably produced more reasonable attentions. For example, our NeuroExplainer led to group-wisely more consistent explanations across subjects. Also, it produced more consistent results across hemispheres, without using any related training constraints.Finally, we compared the individualized preterm-altered cortical development patterns uncovered by our NeuroExplainer with representative group-wise multimodal (dMRI and sMRI) quantitative analyses presented in [1]. As shown in Fig. 4, we can see that our observations in this paper are consistent with [1]. The discriminative cortical regions captured by our NeuroExplainer (using solely morphological features) are largely overlapped with the group-wise significantly different regions identified by [1] in terms of the mean diffusivity, neurite density, and cortical thickness, respectively. For example, they both highlighted some specific regions in the inferior parietal, medial occipital, and superior temporal lobe, and posterior insula, which is worth deeper evaluations in the future."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,4.0,Conclusion,"In the paper, we have proposed an geometric deep network, i.e., NeuroExplainer, to learn fine-grained explanation factors from complex cortical-surface data to boost discriminative representation extraction and accurate classification model construction. On the benchmark dHCP database, our NeuroExplainer achieved better performance than existing post-hoc approaches in terms of both explainability and prediction accuracy, in uncovering preterm-altered infant cortical development patterns. The proposed method could be a promising AI tool applied to other similar cortical surface-based neuroimage and neuroscience studies."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,,Competing Mehtods,ACC AUC SEN SPE SphericalCNN [14] 0.93 0.92 0.76 0.98 SphericalMoNet [9] 0.85 0.93 0.65 0.92 SubdivNet [3] 0 
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,1.0,Introduction,"Accurate segmentation of a variety of anatomical structures is a crucial prerequisite for subsequent diagnosis or treatment [28]. While recent advances in datadriven deep learning (DL) have achieved superior segmentation performance [29], the segmentation task is often constrained by the availability of costly pixel-wise labeled training datasets. In addition, even if static DL models are trained with extraordinarily large amounts of training datasets in a supervised learning manner [29], there exists a need for a segmentor to update a trained model with new data alongside incremental anatomical structures [24].In real-world scenarios, clinical databases are often sequentially constructed from various clinical sites with varying imaging protocols [19][20][21]23]. As well, labeled anatomical structures are incrementally increased with additional lesions or new structures of interest, depending on study goals or clinical needs [18,27]. Furthermore, access to previously used data for training can be restricted, due to data privacy protocols [17,18]. Therefore, efficiently utilizing heterogeneous structure-incremental (HSI) learning is highly desired for clinical practice to develop a DL model that can be generalized well for different types of input data and varying structures involved. Straightforwardly fine-tuning DL models with either new structures [30] or heterogeneous data [17] in the absence of the data used for the initial model training, unfortunately, can easily overwrite previously learned knowledge, i.e., catastrophic forgetting [14,17,30].At present, satisfactory methods applied in the realistic HSI setting are largely unavailable. F irst, recent structure-incremental works cannot deal with domain shift. Early attempts [27] simply used exemplar data in the previous stage. [5,18,30,33] combined a trained model prediction and a new class mask as a pseudo-label. However, predictions from the old model under a domain shift are likely to be unreliable [38]. The widely used pooled feature statistics consistency [5,30] is also not applicable for heterogeneous data, since the statistics are domain-specific [2]. In addition, a few works [13,25,34] proposed to increase the capacity of networks to avoid directly overwriting parameters that are entangled with old and new knowledge. However, the solutions cannot be domain adaptive. Second, from the perspective of continuous domain adaptation with the consistent class label, old exemplars have been used for the application of prostate MRI segmentation [32]. While Li et al. [17] further proposed to recover the missing old stage data with an additional generative model, hallucinating realistic data, given only the trained model itself, is a highly challenging task [31] and may lead to sensitive information leakage [35]. T hird, while, for natural image classification, Kundu et al. [16] updated the model for class-incremental unsupervised domain adaption, its class prototype is not applicable for segmentation.In this work, we propose a unified HSI segmentor evolving framework with a divergence-aware decoupled dual-flow (D 3 F) module, which is adaptively optimized via HSI pseudo-label distillation using a momentum MixUp decay (MMD) scheme. To explicitly avoid the overwriting of previously learned parameters, our D 3 F follows a ""divide-and-conquer"" strategy to balance the old and new tasks with a fixed rigidity branch and a compensated learnable plasticity branch, which is guided by our novel divergence-aware continuous batch renormalization (cBRN). The complementary knowledge can be flexibly integrated with the model re-parameterization [4]. Our additional parameters are constant in training, and 0 in testing. Then, the flexible D 3 F module is trained following the knowledge distillation with novel HSI pseudo-labels. Specifically, inspired by the self-knowledge distillation [15] and self-training [38] that utilize the previous prediction for better generalization, we adaptively construct the HSI pseudo-label with an MMD scheme to smoothly adjust the contribution of potential noisy old model predictions on heterogeneous data and progressively learned new model predictions along with the training. In addition, unsupervised self-entropy minimization is added to further enhance performance.Our main contributions can be summarized as follow:• To our knowledge, this is the first attempt at realistic HSI segmentation with both incremental structures of interest and diverse domains. • We propose a divergence-aware decoupled dual-flow module guided by our novel continuous batch renormalization (cBRN) for alleviating the catastrophic forgetting under domain shift scenarios. • The adaptively constructed HSI pseudo-label with self-training is developed for efficient HSI knowledge distillation.We evaluated our framework on anatomical structure segmentation tasks from different types of MRI data collected from multiple sites. Our HSI scheme demonstrated superior performance in segmenting all structures with diverse data distributions, surpassing conventional class-incremental methods without considering data shift, by a large margin."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.0,Methodology,"For the segmentation model under incremental structures of interest and domain shift scenarios, we are given an off-the-shelf segmentor f θ 0 : X 0 → Y 0 parameterized with θ 0 , which has been trained with the data {x 0 n , y 0 n } N 0 n=1 in an initial source domain D 0 = {X 0 , Y 0 }, where x 0 n ∈ R H×W and y 0 n ∈ R H×W are the paired image slice and its segmentation mask with the height of H and width of W , respectively. There are T consecutive evolving stages with heterogeneous target domains D t = {X t , S t } T t=1 , each with the paired slice set {x t n } N t n=1 ∈ X t and the current stage label set {s t n } N t n=1 ∈ S t , where x t n , s t n ∈ R H×W . Due to heterogeneous domain shifts, X t from different sites or modalities follows diverse distributions across all T stages. Due to incremental anatomical structures, the overall label space, across the previous t stages, Y t is expanded from Y t-1 with the additional annotated structuresfor delineating all of the structures Y T seen in T stages."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.1,cBRN Guided Divergence-Aware Decoupled Dual-Flow,"To alleviate the forgetting through parameter overwriting, caused by both new structures and data shift, we propose a D 3 F module for flexible decoupling and integration of old and new knowledge. Specifically, we duplicate the convolution in each layer initialized with the previous model f θ t-1 to form two branches as in [13,25,34]. The first rigidity branch f r θ t is fixed at the stage t to keep the old knowledge we have learned. In contrast, the extended plasticity branch f p θ t is expected to be adaptively updated 2 } with the model re-parameterization [4]. In fact, the dual-flow model can be regarded as an implicit ensemble scheme [9] to integrate multiple sub-modules with a different focus. In addition, as demonstrated in [6], the fixed modules will regularize the learnable modules to act as the fixed one. Thus, the plasticity modules can also be implicitly encouraged to keep the previous knowledge along with its HSI learning.However, under the domain shift, it can be sub-optimal to directly average the parameters, since f r θ t may not perform well to predict Y t-1 on X t . It has been demonstrated that batch statistics adaptation plays an important role in domain generalizable model training [22]. Therefore, we propose a continual batch renormalization (cBRN) to mitigate the feature statistics divergence between each training batch at a specific stage and the life-long global data distribution.Of note, as a default block in the modern convolutional neural networks (CNN) [8,37], batch normalization (BN) [11] normalizes the input feature of each CNN channel z ∈ R Hc×Wc with its batch-wise statistics, e.g., mean μ B and standard deviation σ B , and learnable scaling and shifting factors {γ, β} as zi = zi-μB σB • γ + β, where i indexes the spatial position in R Hc×Wc . BN assumes that the same mini-batch training and testing distribution [10], which does not hold in HSI. Simply enforcing the same statistics across domains as [5,30,33] can weaken the model expressiveness [36].The recent BRN [10] proposes to rectify the data shift between each batch and the dataset by using the moving average μ and σ along with the training:where η ∈ [0, 1] is applied to balance the global statistics and the current batch.In addition, γ = σB σ and β = μB -μ σ are used in both training and testing. Therefore, BRN renormalizes zi = zi-μ σ to highlight the dependency on the global statistics {μ, σ} in training for a more generalizable model, while limited to the static learning.In this work, we further explore the potential of BRN in the continuously evolving HSI task to be general for all of domains involved. Specifically, we extend BRN to cBRN across multiple consecutive stages by updating {μ c , σ c } along with all stages of training, which is transferred as shown in Fig. 1. The conventional BN also inherits {μ, σ} for testing, while not being used in training [11]. At the stage t, μ c and σ c are succeeded from t -1 stage, and are updated with the current batch-wise {μ r B , σ r B } and {μ p B , σ p B } in rigidity and plasticity branches:For testing, the two branches in final model f θ T can be merged for the lightweight implementation:Therefore, f T θ does not introduce additional parameters for deployment (Fig. 2)."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.2,HSI Pseudo-label Distillation with Momentum MixUp Decay,"The training of our developed f θ t with D 3 F is supervised with the previous model f θ t-1 and current stage data {x t n , s t n } N t n=1 . In conventional class incremental learning, the knowledge distillation [31] is widely used to construct the combined label y t n ∈ R H×W by adding s t n and the prediction of f θ t-1 (x t n ). Then, f θ t can be optimized by the training pairs of {x t n , y t n } N t n=1 . However, with heterogeneous data in different stages, f θ t-1 (x t n ) can be highly unreliable. Simply using it as ground truth cannot guide the correct knowledge transfer.In this work, we construct a complementary pseudo-label ŷt n ∈ R H×W with a MixUp decay scheme to adaptively exploit the knowledge in the old segmentor for the progressively learned new segmentor. In the initial training epochs, f θ t-1 could be a more reliable supervision signal, while we would expect f θ t can learn to perform better on predicting Y t-1 . Of note, even with the rigidity branch, the integrated network can be largely distracted by the plasticity branch in the initial epochs. Therefore, we propose to dynamically adjust their importance in constructing pseudo-label along with the training progress. Specifically, we MixUp the predictions of f θ t-1 and f θ t w.r.t. Y t-1 , i.e., f θ t (•)[: t -1], and control their pixel-wise proportion for the pseudo-label ŷt n with MMD:where i indexes each pixel, and λ is the adaptation momentum factor with the exponential decay of iteration I. λ 0 is the initial weight of f θ t-1 (x t n:i ), which is empirically set to 1 to constrain λ ∈ (0, 1]. Therefore, the weight of old model prediction can be smoothly decreased along with the training, and f θ t (x t n:i ) gradually represents the target data for the old classes in [: t -1]. Of note, we have ground-truth of new structure s t n:i under HSI scenarios [5,18,30,33]. We calculate the cross-entropy loss L CE with the pseudo-label ŷt n:i as self-training [15,38]. In addition to the old knowledge inherited in f θ t-1 , we propose to explore unsupervised learning protocols to stabilize the initial training. We adopt the widely used self-entropy (SE) minimization [7] as a simple add-on training objective. Specifically, we have the slice-level segmentation SE, which is the averaged entropy of the pixel-wise softmax prediction asIn training, the overall optimization loss is formulated as follows:where α is used to balance our HSI distillation and SE minimization terms, and I max is the scheduled iteration. Of note, strictly minimizing the SE can result in a trivial solution of always predicting a one-hot distribution [7], and a linear decreasing of α is usually applied, where λ 0 and α 0 are reset in each stage."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.0,Experiments and Results,"We carried out two evaluation settings using the BraTS2018 database [1], including cross-subset (relatively small domain shift) and cross-modality (relatively large domain shift) tasks. The BraTS2018 database is a continually evolving database [1] with a total of 285 glioblastoma or low-grade gliomas subjects,  comprising three consecutive subsets, i.e., 30 subjects from BraTS2013 [26], 167 subjects from TCIA [3], and 88 subjects from CBICA [1]. Notably, these three subsets were collected from different clinical sites, vendors, or populations [1]. Each subject has T1, T1ce, T2, and FLAIR MRI volumes with voxel-wise labels for the tumor core (CoreT), the enhancing tumor (EnhT), and the edema (ED). We incrementally learned CoreT, EnhT, and ED structures throughout three consecutive stages, each following different data distributions. We used subjectindependent 7/1/2 split for training, validation, and testing. For a fair comparison, we adopted the ResNet-based 2D nnU-Net backbone with BN as in [12] for all of the methods and all stages used in this work."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.1,Cross-Subset Structure Incremental Evolving,"In our cross-subset setting, three structures were sequentially learned across three stages: (CoreT with BraTS2013) → (EnhT with TCIA) → (ED with CBICA). Of note, we used a CoreT segmentator trained with BraTS2013 as our off-the-shelf segmentor in t = 0. Testing involved all subsets and anatomical structures. We compared our framework with the three typical structureincremental (SI-only) segmentation methods, e.g., PLOP [5], MargExcIL [18], and UCD [30], which cannot address the heterogeneous data across stages. As tabulated in Table 1, PLOP [5] with additional feature statistic constraints has lower performance than MargExcIL [18], since the feature statistic consistency was not held in HSI scenarios. Of note, the domain-incremental methods [17,32] cannot handle the changing output space. Our proposed HSI framework outperformed SI-only methods [5,18,30] with respect to both DSC and HD, by a large margin. For the anatomical structure CoreT learned in t = 0, the difference between our HSI and these SI-only methods was larger than 10% DSC, which indicates the data shift related forgetting lead to a more severe performance drop in the early stages. We set η = 0.01 and α 0 = 10 according to the sensitivity study in the supplementary material.For the ablation study, we denote HSI-D 3 F as our HSI without the D 3 F module, simply fine-tuning the model parameters. HSI-cBRN used dual-flow to avoid direct overwriting, while the model was not guided by cBRN for more generalized prediction on heterogeneous data. As shown in Table 1, both the dual-flow and cBRN improve the performance. Notably, the dual-flow model with flexible re-parameterization was able to alleviate the overwriting, while our cBRN was developed to deal with heterogeneous data. In addition, HSI-MMD indicates our HSI without the momentum MixUp decay in pseudo-label construction, i.e., simply regarding the prediction of f θ t-1 (x t ) is ground truth for Y t-1 . However, f θ t-1 (x t ) can be quite noisy, due to the low quantification performance of early stage structures, which can be aggravated in the case of the long-term evolving scenario. Of note, the pseudo-label construction is necessary as in [5,18,30]. We also provide the qualitative comparison with SI-only methods and ablation studies in Fig. 3."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.2,Cross-Modality Structure Incremental Evolving,"In our cross-modality setting, three structures were sequentially learned across three stages: (CoreT with T1) → (EnhT with T2) → (ED with T2 FLAIR). Of note, we used the CoreT segmentator trained with T1 modality as our off-the-shelf segmentor in t = 0. Testing involved all MRI modalities and all structures. With the hyperparameter validation, we empirically set η = 0.01 and α 0 = 10.In Table 2, we provide quantitative evaluation results. We can see that our HSI framework outperformed SI-only methods [5,18,30] consistently. The improvement can be even larger, compared with the cross-subset task, since we have much more diverse input data in the cross-modality setting. Catastrophic forgetting can be severe, when we use SI-only method for predicting early stage structures, e.g., CoreT. We also provide the ablation study with respect to D 3 F, cBRN, and MMD in Table 2. The inferior performance of HSI-D 3 F/cBRN/MMD demonstrates the effectiveness of these modules for mitigating domain shifts."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,4.0,Conclusion,"This work proposed an HSI framework under a clinically meaningful scenario, in which clinical databases are sequentially constructed from different sites/imaging protocols with new labels. To alleviate the catastrophic forgetting alongside continuously varying structures and data shifts, our HSI resorted to a D 3 F module for learning and integrating old and new knowledge nimbly. In doing so, we were able to achieve divergence awareness with our cBRN-guided model adaptation for all the data involved. Our framework was optimized with a self-entropy regularized HSI pseudo-label distillation scheme with MMD to efficiently utilize the previous model in different types of MRI data. Our framework demonstrated superior segmentation performance in learning new anatomical structures from cross-subset/modality MRI data. It was experimentally shown that a large improvement in learning anatomic structures was observed."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,1.0,Introduction,"Colonoscopy is a critical tool for identifying adenomatous polyps and reducing rectal cancer mortality. Deep learning methods have shown powerful abilities in automatic colonoscopy analysis, including polyp segmentation [5,22,26,27,29] and polyp detection [19,24]. However, the scarcity of annotated data due to high manual annotation costs results in poorly trained and low generalizable models. Previous methods have relied on generative adversarial networks (GANs) [9,25] or data augmentation methods [3,13,28] to enhance learning features, but these methods yielded limited improvements in downstream tasks. Recently, diffusion models [6,15] have emerged as promising solutions to this problem, demonstrating remarkable progress in generating multiple modalities of medical data [4,10,12,21]. "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Diffusion Sampler,"Fig. 1. Overview of the pipeline of our proposed approach, where details of ArSDM are described in Sect. 2.Despite recent progress in these methods for medical image analysis, existing models face two major challenges when applied to colonoscopy image analysis. Firstly, the foreground (polyp) of colonoscopy images contains rich pathological information yet is often tiny compared with the background (intestine wall) and can be easily overwhelmed during training. Thus, naive generative models may generate realistic colonoscopy images but those images seldom contain polyp regions. In addition, in order to generate high-quality annotated samples, it is crucial to maintain the consistency between the polyp morphologies in synthesized images and the original masks, which current generative models struggle to achieve.To tackle these issues and inspired by the remarkable success achieved by diffusion models in generating high-quality CT or MRI data [8,11,23], we creatively propose an effective adaptive refinement semantic diffusion model (ArSDM) to generate polyp-contained colonoscopy images while preserving the original annotations. The pipeline of the data generation and downstream task training is shown in Fig. 1. Specifically, we use the original segmentation masks as conditions to train a conditional diffusion model, which makes the generated samples share the same masks with the input images. Moreover, during diffusion model training, we employ an adaptive loss re-weighting method to assign loss weights for each input according to the size ratio of polyps and background, which addresses the overfitting problem for the large background. In addition, we fine-tune the diffusion model by minimizing the distance between the original ground truth masks and the prediction masks from synthesis images via a pretrained segmentation network. Thus the refined model could generate samples better aligned with the original masks.In summary, our contributions are three-fold: (1) Adaptive Refinement SDM: Based on the standard semantic diffusion model [21], we propose a novel ArSDM with the adaptive loss re-weighting and the prediction-guided sample refinement mechanisms, which is capable of generating realistic polyp-contained colonoscopy images while preserving the original annotations. To the best of our knowledge, this is the first work for adapting diffusion models to colonoscopy image synthesis. (2) Large-Scale Colonoscopy Generation: The proposed approach can be used to generate large-scale datasets with no/arbitrary annotations, which significantly benefits the medical image society, laying the foundation for large-scale pre-training models in automatic colonoscopy analysis. (3) Qualitative and Quantitative Evaluation: We conduct extensive experiments to evaluate our method on five public benchmarks for polyp segmentation and detection. The results demonstrate that our approach could help deep learning methods achieve better performances. The source code is available at https:// github.com/DuYooho/ArSDM."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.0,Method,"Background. Denoising diffusion probabilistic models (DDPMs) [6] are classes of deep generative models, which have forward and reverse processes. The forward process is a Markov Chain that gradually adds Gaussian noise to the original data. This process can be formulated as the joint distribution q (x 1:T | x 0 ):where q (x 0 ) is the original data distribution with x 0 ∼ q (x 0 ), x 1:T are latents with the same dimension of x 0 and β t is a variance schedule.The reverse process is aiming to learn a model to reverse the forward process that reconstructs the original input data, which is defined as:(2) where p (x T ) is the noised Gaussian transition from the forward process at timestep T . In this case, we only need to use deep-learning models to represent μ θ with θ as the model parameters. According to the original paper [6], the loss function can be simplified as:"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Re-Weighting Module,"Diffusion Loss ℒ Thus, instead of training the model μ θ to predict μt , we can train the model θ to predict ˜ , which is easier for parameterization and learning."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Refinement Loss ℒ,"In this paper, we propose an adaptive refinement semantic diffusion model, a variant of DDPM, which has three key parts, i.e., mask conditioning, adaptive loss re-weighting, and prediction-guided sample refinement. The overall illustration of our framework is shown in Fig. 2."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.1,Mask Conditioning,"Unlike the previous generative methods, our work aims to generate a synthetic image with an identical segmentation mask to the original annotation. To accomplish this, we adapt the widely used conditional U-Net architecture [21] in the reverse process, where the mask is fed as a condition. Specifically, for an input image x 0 ∈ R H×W ×C , x t can be sampled at any timestep t with the closed form:where ∼ N (0, I) , α t := 1β t and ᾱt := t s=1 α s . It will be fed into the encoder E of the U-Net, and its corresponding mask annotation c 0 ∈ R H×W will be injected into the decoder D. The model output can be formulated as:Thus, the U-Net model θ in Eq. 3 becomes θ (x t , t, c 0 ), and the loss function in Eq. 3 is changed to:Algorithm 1: One training iteration of ArSDM6 Take gradient descent step on ∇ θ L total"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.2,Adaptive Loss Re-weighting,"The polyp regions in the colonoscopy images differ from the background regions, which contain more pathological information and should be adequately treated to learn a better model. However, training the diffusion models using the original loss function ignores the difference between different regions, where each pixel shares the same weights when calculating the loss. This would lead to the model generating more background-like polyps since the large background region will easily overwhelm the small foreground polyp regions during training. A simple way to alleviate this problem is to apply a weighted loss function that assigns the polyp and background regions with different weights. However, most polyps vary a lot in size and shape. Thus assigning constant weights for all polyps exacerbated the imbalance problem. In this case, to tackle this problem, we propose an adaptive loss function that vests different weights according to the size ratio of the polyp over the background. Specifically, we define a pixel-wise weights matrix W λ ∈ R H×W with each entry w λ i,j to be:where p = 1 means the pixel p at (h, w) belongs to the polyp region and p = 0 means it belongs to the background region. Thus, the loss function becomes:"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.3,Prediction-Guided Sample Refinement,"The downstream tasks of polyp segmentation and detection require rich semantic information on polyp regions to train a good model. Through extensive experiments, we found inaccurate sample images with coarse polyp boundary that is not aligned properly with the original masks may introduce large biases and noises to the datasets. The model can be confused by several conflicting training images with the same annotation. To this end, we design a refinement strategy that uses the prediction of a pre-trained segmentation model on the sampled images to guide the training process and restore the proper polyp boundary information. Specifically, at each iteration of training, the output ˜ = θ (x t , t, c 0 ) will go into the sampler to generate sample image x0 . Then, we take the sample image as the input of the segmentation model to predict the pseudo masks c0 . We propose the following refinement loss based on IoU loss and binary cross entropy (BCE) loss between c0 and c 0 . The refinement loss is:where L = L IoU + L BCE is the sum of the IoU loss and BCE loss, c0 is the collection of the three side-outputs ( c3 , c4 , c5 ) and the global map cg as described in [5]. P(•) represents the PraNet model and S(•) is the DDIM [16] sampler. The detailed procedure of one training iteration is shown in Algorithm 1 and the overall loss function is defined as:3 Experiments"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.1,ArSDM Experimental Settings,"We conducted our experiments on five public polyp segmentation datasets: EndoScene [20], CVC-ClincDB/CVC-612 [1], CVC-ColonDB [18], ETIS [14] and Kvasir [7]. Following the standard of PraNet, 1,450 image-mask pairs from Kvasir and CVC-ClinicDB are taken as the training set. The evaluations are conducted on the five datasets separately to verify the learning and generalization capability. The training image-mask pairs are padded to have the same height and width and then resized to the size of 384 × 384. Experiments with predictionguided sample refinement are trained with around one-half NVIDIA A100 days, while others are trained with approximately one day for convergence. We use the DDIM sampler with a maximum timestep of 200 for sampling images."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.2,Downstream Experimental Settings,"We conduct the evaluation of our methods and the state-of-the-art counterparts on polyp segmentation and detection tasks. We generated the same number of samples as the diffusion training set using the original masks, and then combined them to create a new downstream training set. We employed PraNet [5], SANet [22], and Polyp-PVT [2] as baseline segmentation models with default settings, and evaluated them using mean Intersection over Union (IoU) and mean Dice metrics. For detection, we selected CenterNet [30], Sparse-RCNN [17], and Deformable-DETR [31] as baseline models with the same settings as the original papers, and evaluated them using Average Precision (AP) and F1-scores."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.3,Quantitative Comparisons,"The experimental results presented in Table 1 and 2 demonstrate the effectiveness of our proposed method in training better downstream models to achieve superior performance. Specifically, data generated by our approach assists the  significant improvements for each model in mDice and mIoU, with increases of 6.0% and 5.7% over PraNet, 2.1% and 2.7% over SANet, and 0.7% and 0.7% over Polyp-PVT. We also observe superior AP and F1-scores compared to CenterNet, Sparse-RCNN, and Deformable-DETR trained with original data, with gains of 9.1% and 5.3%, 2.7% and 5.8%, and 3.4% and 6.1%, respectively. Moreover, we conducted a comprehensive comparison with SOTA models, noting that these models were not specifically designed for colonoscopy images and may generate data that hinder the training process or lack the ability for effective improvement. Nevertheless, our experimental results confirm the superiority of our proposed method.Ablation Study. We conducted an ablation study to assess the importance of each proposed component. Table 3 and 4 report the overall accuracies on the test set. The results demonstrate both components contribute to the accuracy improvement of baseline models, indicating their essential roles in achieving the best final performance."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.4,Qualitative Analyses,"To further investigate the generative performance of our approach, we present visualization results in Fig. 3, which displays the generated samples and their corresponding masks, alongside the original images for reference. The generated samples demonstrate differences from the original images in both the polyp regions and the backgrounds while maintaining alignment with the masks. Additionally, we sought evaluations from medical professionals to assess the authenticity of the generated samples, and non-medical professionals to locate polyps in the images, which yielded positive feedback on the quality of the generated samples."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,4.0,Conclusion,"Automatic generation of annotated data is essential for colonoscopy image analysis, where the scale of existing datasets is limited by the expertise and time required for manual annotation. In this paper, we propose an adaptive refinement semantic diffusion model (ArSDM) for generating colonoscopy images while preserving annotations by introducing innovative adaptive loss re-weighting and prediction-guided sample refinement mechanisms. To evaluate our approach comprehensively, we conduct polyp segmentation and detection experiments on five widely used datasets, where experimental results demonstrate the effectiveness of our approach, in which model performances are greatly enhanced with little synthesized data."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Table 4 .,SamplesFig.3. Illustration of generated samples with the corresponding masks and original images for comparison reference.
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_32.
Synthetic Augmentation with Large-Scale Unconditional Pre-training,1.0,Introduction,"The recent advancements in medical image recognition systems have greatly benefited from deep learning techniques [15,28]. Large-scale well-annotated datasets are one of the key components for training deep learning models to achieve satisfactory results [3,17]. However, unlike natural images in computer vision, the number of medical images with expert annotations is often limited by the high labeling cost and privacy concerns. To overcome this challenge, a natural choice is to employ data augmentation to increase the number of training samples. Although conventional augmentation techniques [23] such as flipping and cropping can be directly applied to medical images, they merely improve the diversity of datasets, thus leading to marginal performance gains [1]. Another group of studies employ conditional generative adversarial networks (cGANs) [10] to synthesize visually appealing medical images that closely resemble those in the original datasets [36,37]. While existing works have proven effective in improving the performance of downstream models to some extent, a sufficient amount of labeled data is still required to adequately train models to generate decentquality images. More recently, diffusion models have become popular for natural image generation due to their impressive results and training stability [4,13,31]. A few studies have also demonstrated the potential of diffusion models for medical image synthesis [19,24].Although annotated data is typically hard to acquire for medical images, unannotated data is often more accessible. To mitigate the issue existed in current cGAN-based synthetic augmentation methods [8,[36][37][38], in this work, we propose to leverage the diffusion model with unlabeled pre-training to reduce the dependency on the amount of labeled data (see comparisons in Fig. 1). We propose a novel synthetic augmentation method, named HistoDiffusion, which can be pre-trained on large-scale unannotated datasets and adapted to smallscale annotated datasets for augmented training. Specifically, we first employ a latent diffusion model (LDM) and train it on a collection of unlabeled datasets from multiple sources. This large-scale pre-training enables the model to learn common yet diverse image characteristics and generate realistic medical images. Second, given a small labeled dataset that does not exist in the pre-training datasets, the decoder of the LDM is fine-tuned using annotations to adapt to the domain shift. Synthetic images are then generated with classifier guidance [4] in the latent space. Following the prior work [36], we select generated images based on the confidence of target labels and feature similarity to real labeled images. We evaluate our proposed method on a histopathology image dataset of colorectal cancer (CRC). Experiment results show that when presented with limited annotations, the classifier trained with our augmentation method outperforms the ones trained with the prior cGAN-based methods. Our experimental results show that once HistoDiffusion is well pre-trained using large datasets, it can be applied to any future incoming small dataset with minimal fine-tuning and may substantially improve the flexibility and efficacy of synthetic augmentation."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.0,Methodology,"Figure 2 illustrates the overall architecture of our proposed method. First, we train an LDM on a large-scale set of unlabeled datasets collected from multiple sources. We then fine-tune the decoder of this pretrained LDM on a small labeled dataset. To enable conditional image synthesis, we also train a latent classifier on the same labeled dataset to guide the diffusion model in LDM. Once the classifier is trained, we apply the fine-tuned LDM to generate a pool of candidate images conditioned on the target class labels. These candidate images are then passed through the image selection module to filter out any low-quality results. Finally, we can train downstream classification models on the expanded training data, which includes the selected images, and then use them to perform inference on test data. In this section, we will first introduce the background of diffusion models and then present details about the HistoDiffusion model."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.1,Diffusion Models,"Diffusion models (DM) [13,30,32] are probabilistic models that are designed to learn a data distribution. Given a sample from the data distribution z 0 ∼ q(z 0 ), the DM forward process produces a Markov chain z 1 , . . . , z T by gradually adding Gaussian noise to z 0 based on a variance schedule β 1 , . . . , β T , that is:where variances β t are constants. If β t are small, the posterior q(z t-1 |z t ) can be well approximated by diagonal Gaussian [21,30]. Furthermore, when the T of the chain is large enough, z T can be well approximated by standard Gaussian distribution N (0, I). These suggest that the true posterior q(z t-1 |z t ) can be estimated by p θ (z t-1 |z t ) defined as [22]:(2) The DM reverse process (also known as sampling) then generates samples z 0 ∼ p θ (z 0 ) by initiating a Markov chain with Gaussian noise z T ∼ N(0, I) and progressively decreasing noise in the chain of z T -1 , z T -2 , . . . , z 0 using the learnt p θ (z t-1 |z t ). To learn p θ (z t-1 |z t ), Gaussian noise is added to z 0 to generate samples z t ∼ q(z t |z 0 ), then a model θ is trained to predict using the following mean-squared error loss:where time step t is uniformly sampled from {1, . . . , T }. Then μ θ (z t ) and Σ θ (z t ) in Eq. 2 can be derived from θ (z t , t) to model p θ (z t-1 |z t ) [13,22]. The denoising model θ is typically implemented using a time-conditioned U-Net [27] with residual blocks [11] and self-attention layers [35]. Sinusoidal position embedding [35] is also usually used to specify the time step t to θ ."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.2,HistoDiffusion,"Model Architecture. Our proposed HistoDiffusion is built on Latent Diffusion Models (LDM) [26], which requires fewer computational resources without degradation in performance, compared to prior works [4,15,28]. LDM first trains a latent autoencoder (LAE) [16] to encode images as lower-dimensional latent representations and then learns a diffusion model (DM) for image synthesis by modeling the latent space of the trained LAE. Particularly, the encoder E of the LAE encodes the input image x ∈ R H×W ×3 into a latent representation z = E(x) ∈ R h×w×c in a lower-dimensional latent space Z. Here H and W are the height and width of image x, and h, w, and c are the height, width, and channel of latent z, respectively. The latent z is then passed into the decoder D to reconstruct the image x = D(z). Through this process, the compositional features from the image space X can be extracted to form the latent space Z, and we then model the distribution of Z by learning a DM. For the DM in LDM, both the forward and reverse sampling processes are performed in the latent space Z instead of the original image space X .Unconditional Large-scale Pre-training. To ensure the latent space Z can cover features of various data types, we first pre-train our proposed His-toDiffusion on large-scale unlabeled datasets. Specifically, we gather unlabeled images from M different sources to construct a large-scale set of datasets S = {S 1 , S 2 , . . . , S M }. We then train an LAE using the data from S with the following self-reconstruction loss to learn a powerful latent space Z that can describe diverse features: (4) where L rec is the loss measuring the difference between the output reconstructed image x and the input ground truth image x. Here we implement L rec with a combination of a pixel-wise L 1 loss, a perceptual loss [39], and a patch-base adversarial loss [6,7]. To avoid arbitrarily high-variance latent spaces, we also add a KL regularization term D KL [16,26] to constrain the variance of the latent space Z with a slight KL-penalty.After training the LAE, we fixed the trained encoder E and then train a DM with the loss L DM in Eq. 3 to model E's latent space Z. Here z 0 = E(x) in Eq. 3. Once the DM is trained, we can use denoising model θ in the DM reverse sampling process to synthesize a novel latent z0 ∈ R h×w×c and employ the trained decoder D to generate a new image x = D(z 0 ), which should satisfy the similar distribution as the data in S.Conditional Small-scale Fine-tuning. Using the LAE and DM pretrained on S, we can only generate the new image x following the similar distribution in S. To generalize our HistoDiffusion to the small-scale labeled dataset S collected from a different source (i.e., S ⊂ S), we further fine-tune HistoDiffusion using the labeled data from S . Let y be the label of image x in S . To minimize the training cost, we fix both the trained encoder E and trained DM model θ to keep latent space Z unchanged. Then we only fine-tune the decoder D using labeled data (x, y) from S with the following loss function: LD = Lrec(x, x) + λCELCE(ϕ(x), y) , (5) where L rec (x, x) is the self-reconstruction loss between the output reconstructed image x = D(E(x)) and the input ground truth image x. To enhance the correlation between the decoder output x and label y, we also add an auxiliary image classifier ϕ trained with (x, y) on the top of D and impose the cross-entropy classification loss L CE when fine-tuning D. λ CE is the balancing parameter. We annotate this fine-tuned decoder as D for differentiation."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,,Classifier-guided Conditional Synthesis.,"To enable conditional image generation with our HistoDiffusion, we further apply the classifier-guided diffusion sampling proposed in [4,29,30,33] using the labeled data (x, y) from small-scale labeled dataset S . We first utilize the trained encoder E to encode the data x from S as latent z 0 . Then we train a time-dependent latent classifier φ with paired (z t , y) using the following loss function: (6) where z t ∼ q(z t |z 0 ) is the noisy version of z 0 at the time step t during the DM forward process, and L CE is the cross-entropy classification loss. Based on the trained unconditional diffusion model θ , and a classifier φ trained on noisy input z t , we enable conditional diffusion sampling by perturbing the reverseprocess mean with the gradient of the log probability p φ (y|z t ) of a target class y predicted by the classifier φ as follows:where g is the guidance scale. Then the DM reverse process in HistoDiffusion can finally generate a novel latent z0 satisfying the class condition y through a Markov chain starting with a standard Gaussian noise z T ∼ N(0, I) using p θ,φ (z t-1 |z t , y) defined as follows: p θ,φ (zt-1|zt, y) = N (zt-1; μθ (zt|y), Σ θ (zt)) . (8) The final image x of class y can be generated by applying the fine-tuned decoder D , i.e., x = D ( z0 ).Selective Augmentation. To further improve the efficacy of synthetic augmentation, we follow [36] to selectively add synthetic images to the original labeled training data based on centroid feature distance. The augmentation ratio is defined as the ratio between the selected synthetic images and the original training images. More results are demonstrated later in Table 1."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,3.0,Experiments,"Datasets. We employ three public datasets of histopathology images during the large-scale pre-training procedure. The first one is the H&E breast cancer dataset [2], containing 312,320 patches extracted from the hematoxylin & eosin (H&E) stained human breast cancer tissue micro-array (TMA) images [18]. Each patch has a resolution of 224 × 224. The second dataset is PanNuke [9], a pan-cancer histology dataset for nuclei instance segmentation and classification. The PanNuke dataset includes 7,901 patches of 19 types of H&E stained tissues obtained from multiple data sources, and each patch has a unified size of 256×256 pixels. The third dataset is TCGA-BRCA-A2/E2 [34], a subset derived from the TCGA-BRCA breast cancer histology dataset [20]. The subset consists of 482,958 patches with a resolution of 256 × 256. Overall, there are 803,179 patches used for pre-training. As for fine-tuning and evaluation, we employ the NCT-CRC-HE-100K dataset that contains 100,000 patches from H&E stained histological images of human colorectal cancer (CRC) and normal tissue. The patches have been divided into 9 classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), nor-mal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM). The resolution of each patch is 224 × 224.To replicate a scenario where only a small annotated dataset is available for training, we have opted to utilize a subset of 5,000 (5%) samples for finetuning. This subset has been carefully selected through an even sampling without replacement from each tissue type present in the train set. It is worth noting that the labels for these samples have been kept, which allows the fine-tuning process to be guided by labeled data, leading to better predictions on the specific task or domain being trained. By ensuring that the fine-tuning process is representative of the entire dataset through even sampling from each tissue type, we can eliminate bias towards any particular tissue type. We evaluate the fine-tuned model on the official test set. The related data use declaration and acknowledgment can be found in our supplementary materials.Evaluation Metrics. We employ Fréchet Inception Distance (FID) score [12] to assess the image quality of the synthetic samples. We further compute the accuracy, F1-score, sensitivity, and specificity of the downstream classifiers to evaluate the performance gain from different augmentation methods.Model Implementation. All the patches are resized to 256 × 256 × 3 before being passed into the models. Our implementation of HistoDiffusion basically follows the LDM-4 [26] architecture, where the input is downsampled by a factor of 4, resulting in a latent representation with dimensions of 64 × 64 × 3. We use 1000 timesteps (T = 1000) for the training of diffusion model and sample with classifier-free guidance scale g = 1.0 and 200 DDIM steps. The latent classifier φ is constructed using the encoder architecture of the LAE and an additional attention pooling layer [25] added before the output layer.We use the same architecture for the auxiliary image classifier ϕ. For downstream evaluation, we implement the classifier using the ViT-B/16 architecture [5] in all experiments to ensure fair comparisons. The default hyper-parameter settings provided in their officially released codebases are followed.Comparison to State-of-the-Art. We compare our proposed HistoDiffusion with the current state-of-the-art cGAN-based method [36]. We employ Style-GAN2 [14] as the backbone generative model for cGAN-based synthesis. To ensure a fair comparison, all images synthesized by StyleGAN2 and HistoDiffusion model are further selected based on feature centroid distances [36]. More implementation details of our proposed HistoDiffusion, StyleGAN2, and baseline classifier can also be found in our supplementary materials.  3, where HistoDiffusion consistently generates more realistic images matching the given class conditions than SytleGAN2, especially for classes ADI and BACK. When augmenting the training dataset with different numbers of images synthesized from HistoDiffusion and StyleGAN2, one can observe that when increasing the ratio of synthesized data to 100%, the FID score of StyleGAN2 increases quickly and can become even worse than the one without using image selection strategy. In contrast, HistoDiffusion can keep synthesizing high-quality images until the augmentation ratio reaches 300%. Regarding classification performance improvement of the baseline classifier, the accuracy and F1 score of using His-toDiffusion augmentation are increased by up to 6.4% and 6.6%, respectively. Even when not using the image selection module to filter out the low-quality results (i.e., +random 50%), our HistoDiffusion can still improve the accuracy by 1.5%. The robustness and effectiveness of HistoDiffusion can be attributed to the unconditional large-scale pre-training, our specially-designed conditional fine-tuning, and classifier-guided generation, among others."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,4.0,Conclusions,"In this study, we have introduced a novel synthetic augmentation technique, termed HistoDiffusion, to enhance the performance of medical image recognition systems. HistoDiffusion leverages multiple unlabeled datasets for large-scale, unconditional pre-training, while employing a labeled dataset for small-scale conditional fine-tuning. Experiment results on a histopathology image dataset excluded from the pre-training demonstrate that given limited labels, HistoDiffusion with image selection remarkably enhances the classification performance of the baseline model, and can potentially handle any future incoming small dataset for augmented training using the same pre-trained model."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 71.
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,1.0,Introduction,"A must-have ingredient for training a deep neural network (DNN) is a large number of labelled data that is not always available in real-world applications. This challenge of data annotation becomes even worse for medical image segmentation tasks that require pixel-level annotation by experts. Data augmentation (DA) is a recognized approach to tackle this challenge. Common DA strategies create new samples by using predefined transformations such as rotation, translation, and colour jitter to existing data, where the performance gains heavily relies on the choice of augmentation operations and parameters [1].To mitigate this reliance, recent efforts have focused on learning optimal augmentation operations for a given task and dataset [3,8,11,15]. However, transformations learned from these methods are typically still limited to a predefined set of simple operations such as rotation, translation, and scaling. In the meantime, another direction of research has emerged that provides an alternative way of learning more expressive augmentations based on deformation-based transformations commonly used in image registration [6,12,16]. Instead of pre-specifying a list of operations such as rotation and scaling [3], these deformation-based transformations can describe more general spatial transformations. Moreover, they are perfectly suited for modelling an object's shape changes [16] that are crucial for image segmentation tasks. It thus provides an excellent candidate for learning shape variations of an object from the data, and via which to enable shape-based augmentations for medical image segmentation tasks. [12,16].However, to date, all existing approaches to learning deformable registrationbased DA assume a perfect alignment of image pairs to learn the transformations. In other words, the deformation-based transformations are learned globally for the image. This assumption is restrictive and associated with several challenges. First, the learning of a global image-level transformation requires image alignment that may be non-trivial in many scenarios, such as the alignment of tumours that can appear at different locations of an image, or alignment of images from different modalities. The learning of transformations itself is also complicated by the presence of other objects in the image and is best suited when the object of interest is always in the same (and often centre) location in all the images, i.e., images are globally aligned a priori [16]. Second, the application of the learned global transformations for DA is also restricted to images similar (and aligned) to those in training. It thus will be challenging to transfer the learned shape variations to even the same objects across different locations, orientations, or sizes in the image, let alone transferring across dataset (e.g., to transfer the learned shape variations of an organ from one image modality to another).Intuitively, object-centric transformations and augmentations have the potential to overcome the challenges associated with global image-level transformations. Recently, an object-centric augmentation method termed as TumorCP [13] showed that a simple object-level augmentation, via copy-pasting a tumour from one location to another, can yield impressive performance gains. However, the diversity of samples generated by TumorCP is limited to pasting tumours on different backgrounds with random distortions without further learned shapebased augmentation.Similarly, other existing works on object-level augmentation of lesions have mostly focused on position, orientation, and random transformations of the lesion on different backgrounds [14,17]. To date, no existing works have considered shape-based object-centric augmentations. Enriching object-centric DA with learned shape variations -a factor critical to object segmentation -can result in more diverse samples and thereby improve DNN training for medical image segmentation.In this paper, we present a novel approach for learning and transferring object-centric deformations for DA in medical image segmentation tasks. As illustrated in Fig. 1, this is achieved with two key elements:-A generative model of object-centric deformations -constrained to C1 diffeomorphism for better DNN training -to describe shape variability learned from paired patches of objects of interest. This allows the learning to focus on the shape variations of an object regardless of its positions and sizes in the image, thus bypassing the requirement for image alignment. -An online augmentation strategy to sample transformations from the generative model and to augment the objects of interest in place without distorting the surrounding content in the image. This allows us to add shape diversity to the objects of interest in an image regardless of their positions or sizes, eventually facilitating transferring the learned variations across datasets. We demonstrated the effectiveness of the presented object-centric diffeomorphic augmentation in kidney tumour segmentation, including using shape variations of kidney tumours learned from the same dataset (KiTS [7]), as well as transferring those learned from a larger liver tumour dataset (LiTS [2]). Experimental results showed that it can enrich the augmentation diversity of other techniques such as TumorCP [13], and improve kidney tumour segmentation [7] using shape variations learned either within or outside the same training data."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.0,Methods,We focus on DA for tumour segmentation because tumours can occur at different locations of an organ with substantially different orientations and sizes. It thus presents a challenging scenario where global image-level deformable transformations cannot apply. mentation approach comprises as outlined in Below we describe the two key methodological elements.
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.1,Object-Centric Diffeomorphism as a Generative Model,"The goal of this element is to learn to generate diffeomorphic transformation parameters θ that describe shape variations -in the form of deformable transformations T θ -that are present within training instances of tumour x. To realize this, we train a generative model G(.) for θ such that, when given two instances of tumours (x src , x tgt ), it is asked to generate θ from the encoded latent representations z in order to deform x src through T θ (x src ) to x tgt .Transformations: In order to model shape deformations between x src and x tgt , we need highly expressive transformations to capture rich shape variations in tumour pairs. We assume a spatial transformation T θ in the form of pixel-wise displacement field u as T θ (x) = x + u. Inspired from [4,6], we turn to C 1 diffeomorphisms to model our transformations. C 1 diffeomorphisms are smooth and invertible transformations that preserve differentiability up to the first derivative, making them a suitable choice to be embedded in a neural network for gradient-based optimization [4]. However, the set of all diffeomorphisms is an infinitely large Lie group. To overcome this issue, we focus on a specific finitedimensional subset of the Lie group that is large enough to capture the relevant variations in the tumours. For this, we make use of continuous piecewise-affinebased (CPAB) transformation based on the integration of CPA velocity field v θ proposed in [5]. Let Ω ⊂ R 2 denote the tumour domain and let P be triangular tesselation of Ω [6]. A velocity field that maps points from Ω to R 2 is said to be piecewise affine (PA) if it is affine when restricted to each triangle of P . The set V of v θ which are zero on the boundary of Ω can be shown to be finitedimensional linear space [5]. The dimensionality d of V is a result of how fine P is tessellated. It can be shown that V is parameterized by θ, i.e., any instance of V is a linear combination of d orthonormal CPA fields with weights θ [5]. A spatial transformation T θ can be derived by integrating a velocity field v θ [5] as:where the integration can be done via a specialized solver [5]. The solver chosen produces faster and more accurate results than a generic ODE solver. Specifically, the cost for this solver is O(C1)+O(C2 x Number of integration steps), where C1 is matrix exponential for the number of cells an image is divided into and C2 is the dimensionality of an image. The transformations T θ thus can be described by a generative model of θ. We also experimented with squaring and scaling layers for integration but that resulted in texture loss when learning transformations.Generative Modeling: The data generation process can be described as:where z is the latent variable assumed to follow an isotropic Gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and p(x tgt |θ, x src ) follows the deformable transformation as described in Equation ( 1).We define variational approximations of the posterior density as q ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two inputs x src and x tgt . Passing a tuple of x src and x tgt as the input helps the latent representations to learn the spatial difference between two tumour samples. Alternatively, the generative model as described can be considered as a conditional model where both the generative and inference model is conditioned on the source tumour sample x src .Variational Inference: The parameters ψ and φ are optimized by the modified evidence lower bound (ELBO) of the log-likelihood log p(x tgt |x src ):where the first term in the ELBO takes the form of similarity loss: L 2 norm on the difference between x tgt and xsrc = T θ (x src ) synthesized using the θ from G(z).The second KL term constrains our approximated posterior q ψ (z|x src , x tgt ) to be closer to the isotropic Gaussian prior p(z), and its contribution to the overall loss is scaled by the hyperparameter β. To further ensure that xsrc looks realistic, we discourage G(z) from generating overly-expressive transformations by adding a regularization term over the L 2 norm of the displacement field u with a tunable hyperparameter λ reg . The final objective function becomes:Object-Centric Learning: To learn object-centric spatial transformations, x src and x tgt are in the forms of image patches that solely contain tumours. Given an image and its corresponding tumour segmentation mask (X, Y ), we first extract a bounding box around the tumour by applying skimage.measure.regionprops from the scikit-image package to Y . We then use this bounding box to carve out the tumour x from the image X, masking out all the regions within the bounding box that do not belong to the tumour. All the tumour patches are then resized to the same scale, such that tumours of different sizes can be described by the same tesselation resolution. When pairing tumour patches, we pair each tumour with its K nearest neighbour tumours based on their Euclidean distance -this again avoids learning overly expressive transformation when attempting to deform between significantly different tumour shapes."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.2,Online Augmentations with Generative Models,"The goal of this element is to sample random object-centric transformations of T θ from G(z), to generate diverse augmentations of different instances of tumours in place. However, if we only transform the tumour and keep the rest of the image identical, the transformed tumour may appear unrealistic and out of place.To ensure that the entire transformed image appears smooth, we use a hybrid strategy to construct a deformation field for the entire image X that combines tumour-specific deformations with an identity transform for the rest of the image. Specifically, we fill a small region around the tumour with displacements of diminishing magnitudes, achieved by propagating the deformations from the boundaries of the deformation fields from G(z) to their neighbours with reduced magnitudes. Repeating this process ensures that the change at the boundaries is smooth and that the transformed region appears naturally as part of the image. "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.0,Experiments and Results,"We used two publicly available datasets, LiTS [2] and KiTS [7], for our experiments. LiTS [7]   1, ranging from using 11000 pairs from LiTS to only 3000 pairs when using only 25% samples from KiTS."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,,Model:,"The encoder of G(z) consisted of five convolutional layers and three fully connected layers, with a latent dimension of 12 for z. The decoder consisted of five fully connected layers to output the parameters θ for T θ . We trained the G(z) for a total of 400 epochs and a batch size of 16. We also implemented early stopping if the validation loss does not improve for 20 epochs. We used Adam optimizer [10] with a learning rate of 1e-4. We trained separate G(z)'s from KiTS and LiTS, respectively. We set β = 0.001 for both models but needed a high λ reg of 0.009 for the LiTS model compared to 0.004 for KiTS model. The tumours in the LiTS have higher intensity differences, which may explain why a higher value of λ reg was needed to ensure that transformed tumours did not become unrealistic.Results: We evaluated G(z) with two criteria. First, the model needs to be able to reconstruct x tgt by generating θ to transform x src . Second, the model needs to be able to generate diverse transformed tumour samples for a given tumour sample. Figure 2 presents visual examples of the reconstruction and generation results achieved by G(z). It can be observed that the reconstruction is successful in most cases, except when x src and x tgt were too different. This was necessary to ensure that T θ (x src ) did not produce unrealistic examples. The averaged L2-loss of transformed xsrc was 1.23 on the validation pairs. We also visually inspected validation samples after training to make sure that the deformed tumours were similar to the original tumours in appearance. The generated examples of tumours from a single source, as shown in Fig. 2(b), demonstrated that the generations were diverse yet realistic. "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.2,Deformation-Based da for Kidney Tumour Segmentation,"Data: We then used G(z) to generate deformation-based augmentations to increase the size and diversity of training samples for kidney tumour segmentation on KiTS. To assess the effect of augmentations on different sizes of labelled data, we considered training using 25%, 50%, 75%, and 100% of the KiTS training set. We considered two DA scenarios: augment with transformations learned from KiTS (within-data augmentation) versus from LiTS (cross-data augmentation).Models: For the base segmentation network, we adopted nnU-net [9] as it contains state of the art (SOTA) pipeline for medical image segmentation on most datasets. To make the segmentation pipeline compatible with G(z), we used the 2D segmentation module of nnU-net. For baselines, we considered 1) default augmentations such as rotation, scaling, and random crop in nnU-net as well as 2) TumorCP, all modified for 2D segmentation. Note that our goal is not to achieve SOTA results on KiTS, but to test the relative efficacy of the presented DA strategies in comparison with existing object-centric DA methods.Results: We use Sørensen-Dice Coefficient (Dice) to measure segmentation network performance. Dice measures the overlap between prediction and ground truth. As summarized in Table 1, when combined with TumorCP, the presented augmentations were able to generate statistically significant (paired t-test, p ≤ 0.05) improvements in all cases compared to TumorCP alone. This demonstrated the benefit of enriching simple copy-and-paste DA with shape variations. Interestingly, cross-data transferring of the learned augmentations (from LiTS) outperformed the within-data augmentation in the majority of the cases. Which we believe is because of two factors. Firstly, learning of the within-data augmentations is limited to the percentage of the training set used for segmentation. The number of objects to learn transformations from is thus greater in crossdata augmentation settings. Secondly, the transformations present in cross-data are completely unseen in the segmentation training network which helps in generating more diverse samples. Note that, as the transformations are learned as variations in object shapes, they can be transferred easily across datasets Surprisingly, the improvements achieved by the presented augmentation strategy were the most prominent when the segmentation was trained on 50% and 75% of the KiTS training set. This is contrary to the expectation that DA would be most beneficial when the labelled training set is small. This may be because smaller sample sizes do not provide sufficient initial tumor samples for shape transformations. This may also explain why the combination of TumorCP boosted the performance of our augmentation strategy, as the oversampling nature of TumorCP provided more tumour samples for the presented strategy to transform to further enrich the training set. It is also worth noting that in contrast to prior literature, random wrapping of objects does not come close to the learned augmentations. We speculate that while unrealistic transformations work for whole images, they may be problematic when only augmenting specific local objects in an image."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,4.0,Discussion and Conclusions,"In this work, we presented a novel diffeomorphism-based object-centric augmentation that can be learned and used to augment the objects of interest regardless of their position and size in an image. As demonstrated by the experimental results, this allows us to not only introduce new variations to unfixed objects like tumours in an image but also transfer the knowledge of shape variations across datasets. An immediate next step will be to extend the presented approach to learn and transfer 3D transformations for 3D segmentation tasks, and to enrich the shape-based transformation with appearance-based transformations. In the long term, it would be interesting to explore ways to transfer knowledge about more general forms of variations across datasets."
Few Shot Medical Image Segmentation with Cross Attention Transformer,1.0,Introduction,"Automatic segmentation of medical images is a fundamental step for a variety of medical image analysis tasks, such as diagnosis, treatment planning, and disease monitoring [1,2]. The emergence of deep learning (DL) has enabled the development of many medical image segmentation methods, which have achieved remarkable success [3,4,10,12,32]. Most of the existing methods follow a fullysupervised learning paradigm, which requires a considerable amount of labeled data for training. However, the manual annotation of medical images is timeconsuming and labor-intensive, limiting the application of DL in medical image segmentation. Specifically for the 3D volumetric medical images (e.g., CT, MRI), the manual annotation is even more challenging which requires the annotators to go through hundreds of 2D slices for each 3D scan.To address the challenge of manual annotation, various label-efficient techniques have been explored, such as self-supervised learning [15], semi-supervised learning [30,31], and weakly-supervised learning [11]. Despite leveraging information from unlabeled or weakly-labeled data, these techniques still require a substantial amount of training data [16,21], which may not be practical for novel classes with limited examples in the medical domain. This limitation encourages the few-shot learning paradigm [6,22,24,28] to be applied to medical image segmentation. Specifically, the few-shot learning paradigm aims to learn a model from a small number of labeled data (denoted as support) and then apply it to a new task (denoted as query) with only a few labeled data without any retraining. Considering the hundreds of organs and countless diseases in the human body, FSL brings great potential to the various medical image segmentation tasks where a new task can be easily investigated in a data-efficient manner.Most few-shot segmentation methods follow the learning-to-learn paradigm, which aims to learn a meta-learner to predict the segmentation of query images based on the knowledge of support images and their respective segmentation labels. The success of this paradigm depends on how effectively the knowledge can be transferred from the support prototype to the query images. Existing fewshot segmentation methods mainly focus on the following two aspects: (1) how to learn the meta-learner [14,17,26]; and (2) how to better transfer the knowledge from the support images to the query images [5,13,18,23,25,27]. Despite prototype-based methods having shown success, they typically ignore the interaction between support and query features during training. In this paper, as shown in Fig. 1(a), we propose CAT-Net, a Cross Attention Transformer network for few-shot medical image segmentation, which aims to fully capture intrinsic classes details while eliminating useless pixel information and learn an interdependence between the support and query features. Different from the existing FSS methods that only focus on the single direction of knowledge transfer (i.e., from the support features to the query features), the proposed CAT-Net can boost the mutual interactions between the support and query features, benefiting the segmentation performance of both the support and query images. Additionally, we propose an iterative training framework that feed the prior query segmentation into the attention transformer to effectively enhance and refine the features as well as the segmentation. Three publicly available datasets are adopted to evaluate our CAT-Net, i.e., Abd-CT [9], Abd-MRI [8], and Card-CT [33]. Extensive experiments validate the effectiveness of each component in our CAT-Net, and demonstrate its state-of-the-art performance.  To obtain the segmentation model for FSS, the commonly used episode training approach is employed [29]. Each trainig/testing episode (S i , Q i ) instantiates a N -way K-shot segmentation learning task. Specifically, the support set S i contains K samples of N classes, while the query set Q i contains one sample from the same class. The FSS model is trained with episodes to predict the novel class for the query image, guided by the support set. During inference, the model is evaluated directly on D test without any re-training. In this paper, we follow the established practice in medical FSS [7,15,20] that consider the 1-way 1-shot task."
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.2,Network Overview,The Overview of our CAT-Net is illustrated in Fig. 1(a). It consists of three main components: 1) a mask incorporated feature extraction (MIFE) sub-net that extracts initial query and support features as well as query mask; 2) a cross masked attention Transformer (CMAT) module in which the query and support features boost each other and thus refined the query prediction; and 3) an iterative refinement framework that sequentially applies the CMAT modules to continually promote the segmentation performance. The whole framework can be trained in an end-to-end fashion.
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.3,Mask Incorporated Feature Extraction,"The Mask Incorporate Feature Extraction (MIFE) sub-net takes query and support images as input and generates their respective features, integrated with the support mask. A simple classifier is then used to predict the segmentation for the query image. Specifically, we first employ a feature extractor network (i.e., ResNet-50) to map the query and support image pair I q and I s into the feature space, producing multi-level feature maps F q and F s for query and support image, respectively. Next, the support mask is pooled with F s and then expanded and concatenated with both F q and F s . Additionally, the segmentation mask of query image in MIFE is further concatenated with the query feature to strengthen the correlation between query and support features via a pixel-wise similarly map. Finally, the query feature is processed by a simple classifier to get the query mask. Further details of the MIFE architecture can be found in the supplementary material."
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.4,Cross Masked Attention Transformer,"As shown in Fig. 1(b), the cross masked attention Transformer (CMAT) module comprises three main components: 1) a self-attention module for extracting global information from query and support features; 2) a cross masked attention module for transferring foreground information between query and support features while eliminating redundant background information, and 3) a prototypical segmentation module for generating the final prediction of the query image.Self-Attention Module. To capture the global context information of every pixel in the query feature F q 0 and support features F s 0 , the initial features are first flattened into 1D sequences and fed into two identical self-attention modules. Each self-attention module consists of a multi-head attention (MHA) layer and a multi-perceptron (MLP) layer. Given an input sequence S, the MHA layer first projects the sequence into three sequences K, Q, and V with different weights. The attention matrix A is then calculated as:where d is the dimension of the input sequence. The attention matrix is then normalized by a softmax function and multiplied by the value sequence V to get the output sequence O:The MLP layer is a simple 1×1 convolution layer that maps the output sequence O to the same dimension as the input sequence S. Finally, the output sequence O is added to the input sequence S and normalized using layer normalization (LN) to obtain the final output sequence X. The output feature sequence of the selfattention alignment encoder is represented by X q ∈ R HW ×D and X s ∈ R HW ×D for query and support features, respectively.Cross Masked Attention Module. We utilize cross masked attention to incorporate query features and support features with respect to their foreground information by constraining the attention region in attention matrix with support and query masks. Specifically, given the query feature X q and support features X s from the aforementioned self-attention module, we first project the input sequence into three sequences K, Q, and V using different weights, resulting in K q , Q q , V q , and K s , Q s , V s , respectively. Taking the support features as an example, the cross attention matrix is calculated by:We expand and flatten the binary query mask M q to limit the foreground region in attention map. The masked cross attention (MCA) map is computed as:Similar to self-attention, the support feature is processed by MLP and LN layer to get the final enhanced query features F s 1 . Similarly, the enhanced query feature F q1 is obtained with foreground information from the query feature.Prototypical Segmentation Module. Once the enhanced query and support features are obtained, the prototypical segmentation is used to obtain the final prediction. First, a prototype of class c is built by masked average pooling of the support feature F s 1 as follows:where K is the number of support images, and m s (k,x,y,c) is a binary mask that indicates whether pixel at the location (x, y) in support feature k belongs to class c. Next, we use the non-parametirc metric learning method to perform segmentation. The prototype network calculates the distance between the query feature vector and the prototype P = {P c |c ∈ C}. Softmax function is applied to produce probabilistic outputs for all classes, generating the query segmentation:M q i,(x,y) = softmax αcos(F q i,(x,y) , p c ) • softmax(αcos(F q i,(x,y) , p c ))where cos(•) denotes cosine distance, α is a scaling factor that helps gradients to back-propagate in training. In our work, α is set to 20, same as in [29]. Additionally, we design a double threshold strategy to obtain query segmentation. Specifically, we set the first threshold τ to 0.5 to obtain the binary query mask M q , which is used to calculate the Dice loss and update the model. Then, the second threshold τ is set to 0.4 to obtain the dilated query mask M q , which is used to generate the enhanced query feature F q 2 in the next iteration. The second threshold τ is set lower than the first threshold τ to prevent some foreground pixels from being mistakenly discarded. The query segmentation mask M q and dilated mask M q are represented by:1, M q i,(x,y) > τ 0, M q i,(x,y) < τ"
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.5,Iterative Refinement Framework,"As explained above, the CMAT module is designed to refine the query and support features, as well as the query segmentation mask. Thus, it's natural to iteratively apply this sub-net to get the enhanced features and refine the mask, resulting in a boosted segmentation result. The result after the i-th iteration is represented by:The subdivision of each step can be specifically expressed as:where CMA(•) indicates the self-attention and cross masked attention module, and Proto(•) represents the prototypical segmentation module."
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.1,Dataset and Evaluation Metrics,"We evaluate the proposed method on three public datasets, i.e., Abd-CT [9], Abd-MRI [8], and Card-MRI [33]. Abd-CT contains 30 abdominal CT scans with annotations of left and right kidney (LK and RK), spleen (Spl), liver (Liv). Abd-MRI contains 20 abdominal MRI scans with annotations of the same organs as Abd-CT. Card-MRI includes 35 cardiac MRI scans with annotations of left ventricular blood pool (LV-B), left ventricular myocardium (LV-M), and right ventricle (RV). We use the Dice score as the evaluation metric following [15,20].To ensure a fair comparison, all the experiments are conducted under the 1-way 1-shot scenario using 5-fold cross-validation. We follow [15] to remove all slices containing test classes during training to ensure that the test classes are all unseen during validation. In each fold, we follow [7,15,20] that takes the last patient as the support image and the remaining patients as the query (setting I). We further propose a new validation setting (setting II) that takes every image in each fold as a support image alternately and the other images as the query. The averaged result of each fold is reported. It could evaluate the generalization ability of the model by reducing the affect of support image selection."
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.2,Implementation Details,"The proposed method is implemented using PyTorch. Each 3D scan is sliced into 2D slices and reshaped into 256×256 pixels. Common 3D image pre-processing techniques, such as intensity normalization and resampling, are applied to the training data. We apply episode training with 20k iterations. SGD optimizer is adopted with a learning rate of 0.001 and a batch size of 1. Each episode training takes approximately 4 h using a single NVIDIA RTX 3090 GPU. "
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.3,Comparison with State-of-the-Art Methods,"We compare the proposed CAT-Net with state-of-the-art (SOTA) methods, including SE-Net [19], PANet [29], ALP-Net [15], and AD-Net [7], and Q-Net [20]. PANet [29] are the typical prototypical FSS method in the natural image domain, SE-Net [19], ALP-Net [15], AD-Net [7], and Q-Net [20] are the most representative work in medical FSS task. Experiment results presented in Table 1 demonstrate that the proposed method outperforms SOTAs on all three datasets under both setting I and setting II. is able to generate more accurate and detailed segmentation results compared to SOTAs."
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.4,Ablation Study,We conduct an ablation study to investigate the effectiveness of each component in CAT-Net. All ablation studies are conducted on Abd-MRI under setting II.
Few Shot Medical Image Segmentation with Cross Attention Transformer,,Effectiveness of CMAT Block:,"To demonstrate the importance of our proposed CAT-Net in narrowing the information gap between the query and supporting images and obtaining enhanced features, we conducted an ablation study. Specifically, we compared the results of learning foreground information only from the support (S →Q) or query image (Q→S ) and obtaining a single enhanced feature instead of two (S ↔Q). It can be observed that using the enhanced query feature (S →Q) achieves 66.72% in Dice, outperforming only using the enhanced support feature (Q→S ) by 0.74%. With our CMAT block, the mutual boosted support and query feature (S ↔Q) could improve the Dice by 1.90%. Moreover, the iteration refinement framework consistently promotes the above three variations by 0.96%, 0.56%, and 2.26% in Dice, respectively (Table 2).  Influence of Iterative Mask Refinement Block: To determine the optimal number of iterative refinement CMAT block, we experiment with different numbers of blocks. In Fig. 3, we observe that increasing the number of blocks results in improved performance, with a maximum improvement of 2.26% in Dice when using 5 blocks. Considering the performance gain between using 4 and 5 CMAT blocks was insignificant, we hence opt to use four CMAT blocks in our final model to strike a balance between efficiency and performance."
Few Shot Medical Image Segmentation with Cross Attention Transformer,4.0,Conclusion,"In this paper, we propose CAT-Net, Cross Attention Transformer network for few-shot medical image segmentation. Our CAT-Net enables mutual interaction between the query and support features by the cross masked attention module, enhancing the representation abilities for both of them. Additionally, the proposed CMAT module can be iteratively applied to continually boost the segmentation performance. Experimental results demonstrated the effectiveness of each module and the superior performance of our model to the SOTA methods.In the future, we plan to extend our CAT-Net from 2D to 3D networks, explore the application of our model to other medical image segmentation tasks, as well as the extension of our model to other clinical applications, such as rare diseases and malformed organs, where data and annotations are scarce and costly."
Few Shot Medical Image Segmentation with Cross Attention Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_22.
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,1.0,Introduction,"Federated Learning (FL) [10][11][12]27] is a distributed learning approach that allows the collaborative training of machine learning models using data from decentralized sources while preserving data privacy. However, most current FL methods have limitations, including assuming fully annotated and homogeneous data distribution among local clients. In a practical scenario, like a multiinstitutional healthcare collaboration, the participating clients (i.e., medical institutions and hospitals) may not have the incentive or resources to annotate their data [16]. To address this, semi-supervised federated learning (SSFL) [4,16,28] methods have been proposed to utilize unlabeled data and integrate semi-supervised learning algorithms [2,[19][20][21]26] into federated settings.Based on the availability of labeled data, the existing SSFL studies can be classified into two main scenarios: (a) labels-at-client, with each client having some labeled and some unlabeled data [9,15], (b) labels-at-server, with each client possessing only unlabeled data and the server possessing some labeled data [4,7,9,28]. We argue that a more realistic SSFL scenario which is highly challenging but rarely explored in the literature is where some clients have labeled data, and others have completely unlabeled data [14,16,24].The classic federated averaging scheme aggregates weights of all labeled and unlabeled client models trained in parallel. The labeled clients typically use cross-entropy-based loss functions while the unlabeled clients primarily use consistency regularization loss [19] or pseudo-labeling-based [1,23] semi-supervised learning schemes. This results in high gradient diversity [28] between the supervised and unsupervised models particularly in heterogeneous client settings, as these are targeted to optimize separate objective functions. As a result, the aggregated global model is weak and unable to capture a strong representation of either group of clients. This, in turn, leads to the generation of noisy targets for unlabeled clients and hence the global model fails to converge. The situation is further aggravated under non-IID data distribution conditions where the labeled client class distribution varies greatly from that of unlabeled clients. This naturally poses the following important question: ""How can we effectively co-train supervised and unsupervised models under FL setting that aim to optimize separate objective functions at their respective heterogeneous labeled data or unlabeled data clients?""To address this question, we present a novel SSFL algorithm which we call IsoFed that effectively improves client training by isolating the model aggregation of labeled and unlabeled client groups while still leveraging one group of models to improve another. In summary, the primary contributions of this paper are:1. We propose IsoFed, a novel SSFL framework, that realizes isolated aggregation of labeled and unlabeled client models in the server followed by federated self-supervised pretraining of the global model in each individual site. 2. This is the first work to reformulate model aggregation for fully labeled and fully unlabeled clients under SSFL settings. To the best of our knowledge, we are the first to isolate the aggregation of labeled and unlabeled client models while switching between the two client groups. 3. This work bridges the gap between Federated Learning and Transfer Learning (TL) [22] by combining the best of both worlds for learning across sites. First, we conduct federated model aggregation among the labeled or unlabeled client groups. Next, we leverage Transfer Learning to allow knowledge transfer between the two groups. Therefore, we avoid the issue of averaging the supervised and unsupervised models with high gradient diversity in the context of SSFL while also being unaffected by catastrophic forgetting encountered in multi-domain transfer learning. 4. We, for the first time, extensively evaluate SSFL methods on multiple medical image benchmarks with a varying proportion of clients and degree of heterogeneity. Our results show that the proposed isolated aggregation followed by federated pretraining outperforms the state-of-the-art method, viz., RSCFed [14] by 6.91% in terms of accuracy and achieves near-supervised learning performance."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.1,Problem Description,"Assume a federated learning setting with m fully labeled clients denoted as {C 1 , C 2 , ..., C m } each possessing a labeled dataset D l = {(X l i , y l i )} N l i=1 and n fully unlabeled clients defined as {C m+1 , C m+2 , ..., C m+n } each possessing an unlabeled dataset D u = {(X u i )} N u i=1 . Our objective is to learn a global model θ glob via decentralized training."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.2,Local Training,"We adopt mean-teacher-based semi-supervised learning [12,14,20] to train each unlabeled client. At the beginning of each round, the global model W glob is used to initialize the teacher model W t . At the end of each communicating round, the student model W s is returned to the server as the local model. Each batch of images undergoes two types of augmentations. The teacher model receives weakly augmented data whereas the student model receives strongly augmented data in each local iteration. In order to decrease entropy of model output, the temperature of predictions is further increased via sharpening operation [2,3,5,14] as pt,i = Sharpen (p t , τ) i = pt,j where p t,i and pt,i denote each element in p t before and after sharpening, respectively. τ denotes the temperature parameter. The student model is trained on the local data (D u ) via consistency regularization with the teacher model output. The consistency regularization loss is defined as L MSE = pt -p s 2 2 where pt and p s are teacher and student predictions, respectively. . 2  2 denotes L2-norm. The student model weights are optimized via backpropagation whereas the teacher model weights are updated by exponential moving averaging (EMA) after each local iteration, as in Eq. 1:where α denotes momentum parameter. We optimize cross-entropy loss for local training on labeled clients defined as L CE = -y i log p i , where y i denotes labels."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.3,Isolated Federated Aggregation,"In this section, we explain the proposed isolated aggregation of labeled and unlabeled client models. Each communication round is composed of two consecutive substeps. First, the server initializes the global model W t glob and sends it to unlabeled clients (U i ). The global model is used to initialize the teacher model W t in each client. At this stage, only the unlabeled clients perform local training on the global model by minimizing the consistency regularization loss. The updated semi-supervised models obtained after running the local epochs are then uploaded to the server. We adopt a dynamically weighted Federated Averaging scheme [14] to aggregate the model parameters of all unlabeled clients W u at the server. For this, we first obtain the averaged model by performing Fed-Avg as in Eq. 2.where K is the total number of clients. n k is the number of samples in each client. The client models are then dynamically scaled using coefficients c k designed as functions of the individual distances from the averaged model as denoted in Eq. 3. The global model (W glob ) is updated by re-aggregating the client weights scaled by new coefficients c k . In Eq. 3, λ c is a hyperparameter.The updated global model parameters are then communicated to each labeled client which initializes its models using these weights and trains the local model via minimization of the standard cross-entropy loss. After a predefined number of local epochs, each labeled client uploads its local model to the server. The server then aggregates all the supervised models employing the aforementioned weighting scheme and the resultant global model W t+1 glob is then sent to each unlabeled client at the beginning of the next round. "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.4,Client-Adaptive Pretraining,"Motivated by the recent success of continued pretraining in Natural Language Processing [6,8,17], we present a client-adaptive pretraining strategy as the second part of our proposed method. If we view the isolated FL from a transfer learning perspective, the global model received in one group of clients from the server can be regarded as an averaged model pretrained on the other group of clients. To improve client-specific model performance, we conduct a second phase of in-client federated pretraining on the global model before initializing it as a teacher model.For self-supervised pretraining, we jointly learn the client-invariant features and client-specific classifier by optimizing an information-theoretic metric called information maximization (IM) loss denoted as L inf in Eq. 4. It acts as an estimate of the expected misclassification error of the global model for each client. Optimizing the IM loss makes the global model output predictions that are individually certain but collectively diverse. With the help of a diversity preserving regularizer (first component in Eq. 4), IM avoids the trivial solution of entropy minimization where all unlabeled data collapses to the same one-hot encoding. The joint optimization is done by reducing the entropy of the output probability distribution of global model (p i ) in conjunction with maximizing the mutual information between the data distribution and the estimated output distribution yielded by the global model.where N is the number of classes. x denotes any instance belonging to a dataset D. The entropy minimization leads to the least number of confused predictions whereas the regularizer avoids the degenerate solution where every data sample is assigned to the same class [13,18]. The pretrained model is then initialized as the teacher model to train the local student model in each round."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.1,Datasets and FL Settings,"To evaluate the performance and generalisability of the proposed method, we conduct experiments on four publicly available medical image benchmark datasets with different modalities [25], viz., BloodMNIST (microscopic peripheral blood cell images), PathMNIST (colon pathology), PneumoniaMNIST (chest X-ray), and OrganAMNIST (abdominal CT -axial view). Each image resolution is 28 × 28 pixels and is normalized before feeding it to the network. BloodMNIST contains a total of 17,092 images and is organized into 8 classes.PathMNIST has 107,180 images and has 9 types of tissues. PneumoniaMNIST is a collection of 5,856 images and the task is binary classification (diseased vs normal). OrganAMNIST is comprised of 58,850 images and the task is multi-class classification of 11 body organs. We split each training dataset between 4 clients to mimic a practical collaborative setting in healthcare. To testify the versatility of the models, we study two challenging non-IID data partition strategies with 0.5 and 0.8-Dirichlet (γ). As a result, the number of samples per class and per client widely vary from each other. Additionally, we show the impact of varying the proportion of labeled clients (75%, 50%, 25%) on model performance. See Suppl. Sec 1 for more details."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.2,Implementation and Training Details,"For all datasets, we employ a simple CNN comprising of two 5 × 5 convolution layers, a 2 × 2 max-pooling layer, and two fully-connected layers as the feature extraction backbone followed by a two-layer MLP and a fully-connected layer as the classification network. Our model is implemented with PyTorch. We follow the settings prescribed for a training RSCFed to enable a fair comparison. See Suppl. Sec 2 for more training details."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.3,Results and Discussion,"We use the standard metrics -accuracy, area under a ROC curve (AUC), Precision, and Recall to evaluate performance. We observe that the dynamically weighted version of Fed-Avg (discussed in Sect. 2.3) outperforms standard Fed-Avg and hence use it as a baseline in this paper instead of vanilla Fed-Avg.In order to fairly evaluate IsoFed, we compare with the following state-of-theart SSFL benchmarks: (a) MT+wFed-Avg: a combination of Mean Teacher and dynamically weighted Fed-Avg, (b) RSCFed: Random sampling consensusbased FL [14]. Since RSCFed has already been shown to significantly outperform FedIRM [16] and Fed-Consist [24] on multiple datasets, we exclude those methods from our comparative study due to space constraints. We consider fully-supervised FL as an upper bound and report the results for both the non-IID settings on each dataset. Tables 12show that overall, IsoFed outperforms RSCFed by 6.91%, 4.15%, 7.28%, and 6.71% in terms of average accuracy, AUC, Precision, and Recall respectively. Table 1 shows our method and our baselines on 8-class classification with BloodMNIST. L and U denote the number of labeled and unlabeled clients respectively. The average accuracy for fully-supervised FL is 79.51%. Among the baselines, MT+wFed-Avg has a higher overall accuracy score of 68.36% while RSCFed has an accuracy score of 63.41%. Particularly, we find RSCFed collapses under the most extreme case of γ = 0.5 and U=3. IsoFed improves the accuracy score to 73.83% and is stable for all evaluated conditions. Table 1 further reports performance on 9-class classification with PathMNIST. The fully-supervised FL achieves an overall accuracy of 69.71%. The baselines have very similar accuracy scores of 60.53% and 60.84% respectively. IsoFed improves it to 64.69%.Table 2 shows binary classification results on PneumoniaMNIST. The fullysupervised FL has an overall accuracy of 87.18%. MT+wFed-Avg and RSCFed  2. The upper bound accuracy is 69.61% and the baseline accuracies are 61.91% and 62.97% respectively. IsoFed achieves an overall accuracy score of 65.97%. In general, the performance of all methods decreases with γ changing from 0.8 to 0.5. It is expected as the clients become more label-skewed due to higher non-IID data partition. However, our approach is least affected by this which is reflected in its accuracy decrease by 2.19% as opposed to 4.45% and 2.94% incurred by baselines. As foreseen, performance also deteriorates with decrease in the number of labeled clients. For L:U = 3:1, 2:2, 1:3, the baseline accuracies degrade by 2.16%, 5.61%, 15.31% and 2%, 5.01%, 12.91% w.r.t. fully supervised FL setting. However, for IsoFed, the decrease in accuracy is only 0.55%, 3.09%, and 7.28%, respectively. This proves the near-supervised learning performance of the proposed training method. The superior performance of IsoFed over the baselines and closer performance to the upper bound demonstrates better learning and generalization. This is achieved by the isolated aggregation strategy and federated pretraining on all datasets."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.4,Ablation Study,"Owing to space constraints, we show ablation experiments only on OrganAM-NIST, which provides the most challenging classification task, to evaluate the impact of IsoFed components. (More results in Suppl. Sec 2). Table 2 demonstrates that client-adaptive pretraining improves model accuracy by 5.50% for the most extreme condition of γ = 0.5 and L:U = 1:3."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,4.0,Conclusion,"We have introduced a novel SSFL framework called IsoFed, an isolated federated learning technique, to address joint training of labeled and unlabeled clients in the context of decentralized semi-supervised learning. It opens a new research direction in learning across domains by unifying two dominant approaches -Federated Learning (among labeled or unlabeled clients) and Transfer Learning (between labeled and unlabeled clients). Our results challenge the conventional strategy of co-training fully labeled and fully unlabeled clients in SSFL. Experimental results on 4 different medical imaging datasets with varied proportion of labeled clients (25, 50, 75%) and varied non-IID distribution (0.5 & 0.8-Dirichlet) show that IsoFed achieves a considerable boost compared to current state-of-the-art SSFL method. IsoFed can be easily incorporated into other federated learning-based aggregation schemes as well as used in conjunction with any other semi-supervised learning framework in federated learning setting."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 39.
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,1.0,Introduction,"Recently, federated learning has emerged as a promising strategy for performing privacy-preserving, distributed learning for medical image segmentation. Among various methods, FedAvg [1] has been the de facto approach for federated learning, where the server maintains a global model which is dispatched to each client for updating locally on their own private data. After that, the updated local models are collected and averaged to produce a global model for the training of the next round. For FedAvg and its variants, a well-known issue is ""client drift"" caused by non-IID data distribution across different clients (i.e., data-level heterogeneity). To address this issue, a group of methods resort to designing proximal terms or reparametrization strategies [2,3] to restrain the client drift from the global model. However, these regularization terms inherently limit the local convergence potential, Other methods try to improve the local models' generalization ability without strict proximal restrictions on model parameters [4,5]. In [27], the authors proposed to learn compact local representations on each device and a global model across all devices, reducing both the intra-client and inter-client data variance. However, these methods perform local updates blindly, totally ignoring the feature distributions of other clients. In medical image segmentation, FedDG [6] was proposed to improve the local models' generalization ability via exchanging amplitude spectrum to transmit the distribution information across clients. However, the distribution perception step was processed offline, which was fixed upon finished, limiting its potential adaptability to various subsequent tasks.Different from the above-mentioned methods, in this paper, we aim to tackle the ""client drift"" problem by exploring a unified latent feature space for different clients in a privacy-preserving manner and by enhancing the feature discriminability of each client. Concretely, we propose to extract local prototypes to represent the feature distribution at each client. Since local prototypes are statistical characteristics, we can share them among different clients without the concern of privacy issues. Then performing cross-client pixel to local prototype matching can help not only to perceive the global feature distribution but also to explicitly align the cross-client features, leading to a more unified latent feature space. Besides, by performing pixel to local prototype matching at each client, we can directly shape and enhance the discriminability of the learned feature space at each client.Another well-acknowledged concern of federated networks is the ""straggler"" problem caused by system-level heterogeneity. FedAvg and some of its variants [2,3] directly average the local models weighted by their data amount ratio, which may lead to unexpected deterioration due to asynchronous learning process of local models. Based on the intuition that well-trained local models should contribute more to the global model, in this paper, we propose a simple yet effective process-aware model aggregation scheme, which is demonstrated to effectively suppress the influence of ""stragglers"".The contributions of our method can be summarized as follows:-We propose a novel FedContrast-GPA framework to simultaneously alleviate both data-level and system-level heterogeneity issues in federated optimization. -We propose an intra-client and inter-client local prototype based contrastive learning scheme, which not only enhances the feature discriminability of each client, but also explicitly performs cross-client feature distribution perception and alignment in a privacy-preserving manner. -We introduce a simple yet effective process-aware weighting scheme to suppress the influence of ""stragglers"" in global model aggregation. 2 MethodA typical federated learning process consists of two stages: local update at each client and global aggregation at the server side. In this paper, we propose the FedContrast-GPA framework (as shown in Fig. 1), which consists of the intraand inter-client local prototype based contrastive learning scheme (during local update) and the process-aware aggregation scheme (during global aggregation).Assuming there exist K clients in the federated network, we denote client k as S k . Then private data set on the k-th client can be denoted as "
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,2.1,Intra-and Inter-client Local-Prototype based Contrastive Learning,"Local Prototype Learning. Denote the bottleneck features of class c on S k aswhere N c k represents the number of pixels belonging to class c in the intermediate feature maps. In order to model the feature distribution of S k from a statistical view, we propose to generate the class-specific local prototypes to capture semantic-aware feature distribution. Considering that the spatial coverage and visual changes may vary dramatically across different classes, we extend the method introduced in [8] to allow learning different number of sub-clusters for different semantic classes. For detailed derivation of local prototype learning, please refer to [8]. Denote the learned local prototypes for class c aswhere T c refers to the number of sub-clusters for class c), and the pixel-to-local-prototype mapping as from all the semantic classes, where C is the total class number. Then contrastive learning is introduced to enforce compactness within a subcluster and separation among different sub-clusters. Specifically, the intra-client pixel-to-local-prototype contrastive loss is calculated as,where s  ands c ,t k represents the similarity score between f c,i k and the local prototype from the t-th sub-cluster of class c , where T c denotes the number of sub-clusters for class c , and Z is the normalization factor to average over all the pixels within a mini-batch. In our method, we adopt cosine similarity to get the similarity score,where <, > denotes the cosine similarity function. Please note, visual compactness is only imposed at the sub-cluster granularity, which means the local features should distribute faraway from not only sub-clusters of the other semantic classes, but also other sub-clusters of the same semantic class. Apart from the contrastive loss term, in Intra-LPCL, we also explicitly maximize the feature similarities between local features and their assigned local prototypes as,Then the final Intra-LPCL loss is calculated as,Inter-client Local-Prototype Based Contrastive Learning (Inter-LPCL) for Feature Alignment. The aim of Inter-LPCL is to perform distributed feature alignment across different clients in a privacy-preserving manner, such that the aggregated global model can generalize well across clients.Given the i-th local feature of class c from S k (i.e., f c,i k ), and the prototypes poolwe don't know the cross-client pixel-to-prototype assignments. Thus, instead of imposing strict restrictions on sub-cluster compactness as done in Intra-LPCL, we loosen the alignment restrictions to category level. Specifically, the local features from S k are supposed to distribute closer to one of the sub-clusters belonging to the same class in S k , and faraway from sub-clusters of the other semantic classes. Mathematically, the inter-LPCL loss is calcualted as,where max(•) returns the maximum value in the set, {s c ,t k,k , t ∈ {1, • • • , T c }} denotes the similarity set calculated between f c,i k and all the local prototypes from class c in S k , which is formulated as,The final Inter-LPCL loss of S k is then calculated by averaging over k in L inter k,k , which is formally defined as,Overall Objective for Local Update. The overall loss function for updating local model from S k is formulated as,where λ 1 , λ 2 are the hyper-parameters, L seg k is the segmentation loss,where CE(•) denotes the cross entropy loss."
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,2.2,Process-Aware Global Model Aggregation,"During each federated communication, FedAvg updates the global model as weighted average over local models, where α k is the aggregation weight for S k , which is commonly set as N k k N k (N k is the number of images in S k ). Instead of weighting the local models by its data amount ratio, in this paper, we argue that the aggregation weights should reflect the training process of each local model (i.e., well-trained model that generates good segmentation results should contribute more during aggregation). Specifically, denote the mean Dice Similarity Coefficient obtained on the training and validation data of S k as DSC k , then the normalized weights in our method are calculated as,By introducing the process-aware aggregation scheme, we can effectively detect the straggler, improving the robustness of aggregated global model."
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,3.0,Experiments and Results,"Datasets and Implementation Details. We validate our method on the challenging task of prostate segmentation from 3D MR images. T2-weighted MRI images used in our study are collected from 6 different data sources [24][25][26],where each source is treated as a client in our study. We follow [28] to preprocess the data. For data augmentation, both geometric transformations (including elastic deformation, translation, rotation and scaling) and intensity augmentations (including contrast and gaussian noise) are employed in our method. The local model is trained using Adam optimizer with a batch size of 64 and Adam momentum of 0.9 and 0.999. The learning rate is initialzed as 0.001 and multiplied by 0.9 after each round of federated communication. The local epoch in each federated round is empirically set as 1. The hyper-parameters λ 1 , λ 2 are empirically set as 0.03 and 0.001 respectively. The number of local prototypes for prostate and background are chosen by grid search, and empirically set as 3 and 6, respectively.Ablation Study on the Effectiveness of Each Component: We denote the process-aware global aggregation as GA p , then the detailed analysis on component effectiveness is presented in Table 1. One can see from this table that incorporating the ""Intra-LPCL"" and ""Inter-LPCL"" terms can bring +1.3% and +1.4% overall DSC performance gains respectively, validating the effectiveness of intra-client discriminability enhancement and inter-client feature perception and alignment. Albeit its simplicity, our process-aware global aggregation scheme can also boost the DSC segmentation performance by a large margin (i.e., a 1.7% increase over the baseline). Combined together, the proposed FedContrast-GPA framework can witness a gain of 3.4% on the overall DSC performance.To further demonstrate the advantage of our federated learning strategy, we conduct experiments to analyze the performance of centralized training and separate training, where centralized training is trained by updating the global model sequentially using private data from each client, while separate training is trained by updating each local client with only private data and no global communication. The average Dice performance for each client in centralized training is 73%, 77%, 84%, 72%, 86%, and 76%, respectively, while the average Dice performance for each client in separate training is 85%, 79%, 86%, 73%, 91%, and 27%, respectively. We can see that directly putting data together in centralized training does not bring performance gain due to data heterogeneity. Besides, in separate training, we can see a severe performance drop in some clients without enough learning data and knowledge from others.Analysis on the Straggler Mitigation Effect. To demonstrate the effectiveness of our method in straggler mitigation, we compare the client-specific DSC and HD95 performance between the baseline (FedAvg) and Ours. For clarity, we first define the ""stragglers"" in a federated learning network as follows: the clients whose performance from the baseline (FedAvg) rank among the worst half of all the clients. Note that the ""stragglers"" are recognized according to the performance of FedAvg, since our method aims to address the ""straggler"" problem in FedAvg.As shown in Fig. 2, for the prostate segmentation task, ""Client 2"", ""Client 4"" and ""Client 6"" are the ""stragglers"". Compared to the ""non-stragglers"", we can observe large performance gains on the ""stragglers"". In specific, the DSC performance gains on ""Client 2"", ""Client 4"" and ""Client 6"" are 2.0%, 3.3% and +11.0% (on average a 5.4% DSC gain), respectively. The HD95 performance  gains are -0.33 mm, -0.47 mm, and -1.3 mm, respectively (on average -0.7 mm gain in terms of HD95). Meanwhile, for the ""non-stragglers"", the improvements are respectively 2.2%, 0.6% and 1.5% (on average 1.4%) in terms of DSC and -0.0 mm, -0.0 mm, and -0.11 mm (on average -0.04 mm) in terms of HD95.From above analysis, we can see that the proposed method can achieve effective straggler mitigation by bringing larger performance gains over stragglers, and slightly boost the performance over the 'non-stragglers"".Comparison with State-of-the-Art Methods. We compare the performance of our method with five state-of-the-art (SOTA) methods, including FedAvg [1], FedAvg-LG [27], FedDG [6], FedProx [2] and MOON [4]. For fair comparison, all the SOTA methods were trained/tested on our own dataset splits. The base parameter settings are kept the same as ours, other hyperparameters are chosen by grid-search (In FedAvg-LG, the number of layers for global aggregation is set as 13, the hyper-parameters in FedDG are the same as the original paper, the weight for proxy term in FedProx is 2.5e-4, and the model contrastive coefficient in MOON is 0.01). In the following, we conduct both quantitative and qualitative comparisons with SOTA methods.To analyse the performance of our proposed FedContrast-GPA framework, we report the DSCs for all the distributed clients (i.e., S 1 -S 6 in Table 2). For a straightforward comparison with the SOTA methods, we also record the average DSC across all the clients. Detailed comparison results are illustrated in Table 2. As shown, overall, our proposed FedContrast-GPA framework achieves superior performance than the listed SOTA methods. Specifically, FedContrast-GPA outperforms the second best method by 1.8% in terms of average DSC, and generates the best DSC performance at each client, demonstrating favorable generalization ability of our method. Figure 3 demonstrates some sampled visualization results from different clients. As shown, listed SOTA methods may fail to obtain good segmentation results on some samples from different clients, while our approach can consistently generate reasonably good results, demonstrating the robustness and generalizability of our method."
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,4.0,Conclusions,"In this paper, we proposed a novel FedContrast-GPA framework to simultaneously address both the data-level heterogeneity and the system-level heterogeneity issues in federated networks. Extensive ablation studies and comparisons with the SOTA methods demonstrated the effectiveness of the proposed method."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,1.0,Introduction,"Deep learning methods provide state-of-the-art (SOTA) performance for a variety of medical image analysis tasks such as diabetic retinopathy grading [7], and chest X-ray diagnosis [10], to name a few. SOTA fully supervised methods have access to all classes as part of the training data whereas most real world clinical applications do not provide access to all classes which leads to unseen classes being wrongly diagnosed as one of the seen classes. Zero-Shot Learning (ZSL) aims to classify unseen test data by learning their plausible representations from seen class features, and in Generalized Zero-Shot Learning (GZSL) the model should accurately classify both seen and unseen classes during test time.Previous works on GZSL in medical images have focused on the single class scenario where an image is assigned a single disease class [18,21]. However, chest X-ray images have multiple labels and single-label methods do not work well in this setting. Hence we propose a multi-label GZSL approach that takes into account the semantic relationship between the multiple disease labels and learns a highly discriminative feature representation.GZSL for natural images [6,12,14,22] have the advantage of providing attribute vectors for all classes that enables a model to correlate between attribute vectors and corresponding feature representations of the seen classes. Defining unambiguous attribute vectors for medical images requires deep clinical expertise and time. This is more challenging for the multi-label scenario, where many disease conditions have similar appearances and textures. For example, in lung X-ray diagnosis, many conditions frequently co-occur with labels such as Atelectasis, Effusion, and Infiltration. An effective class attribute vector should be able to precisely identify individual labels and differentiate them from other co-occurring disease labels, which is very challenging to define. To overcome the above challenges, we make the following contributions:1. We propose a novel feature disentanglement method where a given image is decomposed into class-specific and class agnostic features. This enables better feature learning of different classes and subsequently contributes to better feature synthesis in the multi-label scenario. 2. We use text embedding similarities to learn the semantic relationships between different labels. This contributes to more accurate learning of multilabel interactions at a global scale and guide feature generation to synthesize feature vectors that are realistic and preserve the multi-label relationship between disease labels. 3. We solve the GZSL classification problem in terms of cluster assignment.Class specific feature disentanglement performs better for multi-label classification [11] and we use this concept to synthesize unseen class features and subsequently perform classification.Prior Work: GZSL's objective is to recognize images from known and unknown classes. Many works have shown promising results using GANs [23,26], and Intra-Class Compactness Enhancement [12]. Recent works on multi-label zero-shot learning (ML-ZSL) use information propagation [14], attention mechanisms [9] and co-occurrence statistics with weighted combinations of seen classes [19]. ZSL in medical image analysis is a much less explored topic with limited applications such as registration [13], segmentation [1], gleason grading [16] and artifact reduction [4]. [21] used multi-modal images and medical reports for GZSL of chest xray (CXR) images while [17,18] used saliency maps and GANs for GZSL using only CXRs.Recently, language models pre-trained on large corpora have also been considered for GZSL of CXRs [8]. However all the above works operate in the single label setting, while we solve the multi-label problem."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,2.0,Method,Method Overview: Given training data with seen classes we: 1) create a dictionary from the text embedddings; 2) disentangle the image into class specific and class agnostic features; 3) use class specific features to generate features of seen and unseen classes using the Mixup approach [28]; 4) for a given test image apply feature disentanglement and feature similarity analysis to identify the different class labels in the image.
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,,Embeddings:,"We generate embeddings of image class labels using BioBERT [15], a BERT [5]-like pre-trained model. BioBERT [15] is pre-trained on biomedical literature, more specifically the model available from Huggingface1 , which is a base and cased model. We consider a pooled set that produces a single 768 dimension vector for a label. We then calculate the cosine similarity between each of the labels and represent it as a matrix, which we refer to as Dict T extdictionary for text embeddings, shown in Table 1."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,2.1,Feature Disentanglement,"Our feature disentanglement method is inspired from [20] which decomposes the feature space into shape and texture for domain adaptation applications. We decompose the feature space of the seen class samples into 'class-specific' . This is achieved by having two heads instead of one (as in conventional architectures). Both vectors are combined and fed to the decoder, G n , which reconstructs the original input. The disentanglement network is trained using the following loss:Reconstruction Loss: L Rec , is the commonly used image reconstruction loss:It is a sum of the reconstruction losses from the class specific autoencoders. We train different autoencoders for images of each class in order to obtain class specific features and refer to them as 'Classspecific autoencoders'.Class Specific Loss: For given class l the class specific component z spec l i will have high similarity with samples from the same class and low similarity with the z spec k i of other classes k. These two conditions are incorporated as follows:where . denotes cosine similarity. The sum is calculated for all classes indexed by l and over all samples indexed by i, j.Class Agnostic Loss: Class agnostic features of different classes have similar semantic content and have high cosine similarity. L agn is defined asWe want class specific and class agnostic features of same-class samples to be mutually complementary and have minimal overlap in semantic content, i.e.,Since the above loss terms are minimized it helps us achieve our stated objectives.  Feature Generation Network: After disentangling the different seen class samples into their class specific components we create a distribution of each seen class feature. We generate synthetic class specific features of unseen classes using the following approach inspired by Mixup [28]:where z specU k is the class specific synthetic vector for unseen classes k( = l), z specS l is a feature sampled from the distribution of seen class l, Λ l is a random number drawn from a beta distribution. ŷ is a one-hot encoded vector and is a sum of the one-hot label vectors of individual classes. Hence we do not need a weight when combining the label vectors. The weights Λ l are such that l Λ l = 1. Generating unseen class features through Mixup without additional constraints can generate unrealistic features. We use the dictionary of text embeddings to guide the feature generation process. As synthetic features of the seen and unseen classes are generated we cluster them using the online self supervised learning based SwAV method [3] and calculate the centroids of each cluster. The semantic similarity of the centroid clusters should be such that their cosine similarity values are close to those obtained in Table 1, i.e., we define a loss:where Cent All refers to the changing matrix of cluster centroid similarities for all seen and unseen classes. N is the total number of classes. The final loss term for clustering all class samples is L Clust = L(x s , x t ) + λ 4 L ML-Cluster where L(x s , x t ) is the SwAV loss function defined in [3]. We add only those synthetic samples to classifier training data that reduce L Clust . This formulation ensures that the cluster output is well separated semantically and the cluster centroids follow the semantic relationship between all classes in Table 1.Training, Inference and Implementation: For a given test image we use the pre-trained L class specific autoencoders to get the class specific features.An input 256 × 256 image is passed through the Encoder having 3 convolution layers (64, 32, 32 3 × 3 filters ) each followed by max pooling. The Decoder is symmetric to the Encoder. z agn and z spec are 256-dimension vectors. We then calculate the cosine similarity of the class specific features with the corresponding class centroids. If the cosine similarity is above 0.5 then the sample is assigned to the class. Following standard practice for GZSL, average class accuracies are calculated for the seen (Acc S ) and unseen (Acc U ) classes, and also the harmonic mean H = 2×AccU ×AccS AccU +AccS ."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.0,Experimental Results,"Dataset Description. We demonstrate our method's effectiveness on the following chest xray datasets for multi-label classification tasks: 1.NIH Chest X-ray Dataset [24]: having 112, 120 expert-annotated frontal-view X-rays from 30, 805 unique patients and has 14 disease labels. Original images were resized to 224 × 224. Hyperparameter values are λ 1 = 1.1, λ 2 = 0.7, λ 3 = 0.9, λ 4 = 1.2. Comparison Methods: We compare our method's performance with multiple GZSL methods -single label and multi-label techniques -employing different feature generation approaches such as CVAE or GANs. Our method is denoted as ML-GZSL (Multi Label GZSL). Our benchmark is a fully supervised learning (FSL) based method of [27] which is the top ranked method for [10], where the ranking is based on AUC. It builds upon a DenseNet-121 trained for multi-label classification."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.1,Generalized Zero Shot Learning Results,"Classification results for medical images in Table 2 show our proposed method significantly outperforms all competing GZSL methods. Note that we use the cluster centroids in place of attribute vectors for these feature synthesis methods. This significant difference in performance can be explained by the fact that the complex architectures that worked for natural images will not be equally effective for medical images which have less information. Absence of attribute vectors for medical images is another contributing factor. The class attributes provide a rich source of information about natural images which can be leveraged using existing architectures. Since those are not available for medical images these methods do not perform equally well. Different combinations of 7 seen and unseen classes are taken, and for each combination we run our model 5 times and the final reported numbers are the average of different combinations. ML-GZSL's performance is almost equal to that of the benchmark fully supervised method FSL. Although GZSL methods generally perform inferior to FSL methods, our use of class specific features significantly improves performance. Additionally, the use of semantic relation between text embeddings significantly improves the performance due to better feature synthesis. The average accuracy is obtained by first calculating True Positive, False Positive, True Negative, False Negative values and using these values to get the global accuracy. Furthermore the AUC(and F1) values for CheXpert data are as follows: FSL-93.0(91.7), ML-GZSL-92.8(91.6), [18]-91.9(90.0), [8]-84.3(82.4)."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.2,Ablation Studies,"Table 3 shows results for ablation studies. We exclude each of the three loss terms related to feature disentanglement -L agn ,L spec and L agn-spec -and report the results as ML-GZSL wLagn , ML-GZSL wLspec , and ML-GZSL wLagn-spec . We also compare with the results of using image features obtained from a CNN based feature extractor (ResNet50 trained on Imagenet), which we denote as 'pretrain'. We observe that the class specific features has the greatest influence on the results and excluding it, ML-GZSL wLspec , results in significant performance degradation compared to ML-GZSL. ML-GZSL wLagn-spec and ML-GZSL wLagn also show significantly lower performance. These results highlight the importance of the class specific features and at the same time illustrate class agnostic features have an important influence on the method's performance.We also investigate the influence of L ML-Cluster (Eq. 6) in the clustering process. The numbers in Table 3 show that ML-GZSL wL ML-Cluster (which is essentially the original SwAV algorithm) performs much worse. This proves the significant contribution of the text embedding dictionary in our multi-label GZSL framework. Hyperparameter Selection: The λ's were varied between [0.4-1.5] in steps of 0.05 and the performance on a separate test set of 10, 000 images were monitored. We optimize Eq. 1 by setting λ 2 = λ3 = λ 4 = 1, and select the optimum value of λ 1 . After fixing λ 1 we determine optimal λ 2 , and subsequently λ 3 , λ 4 .Realism of Synthetic Features. We reconstruct the xray images from the synthetic feature vectors using the feature disentanglement autoencoders' decoder part. We select 1000 such synthetic images from 14 classes of the NIH dataset and ask two trained radiologists, having 12 and 14 years experience in examining chest xray images for abnormalities, to identify whether the images are realistic or not. Each radiologist was blinded to the other's answers. Results for ML-GZSL show one radiologist (RAD 1) identified 912/1000 (91.2%) images as realistic while RAD 2 identified 919 (91.9%) generated images as realistic. Both of them had a high agreement with 890 common images (89.0%) identified as realistic. Considering both RAD 1 and RAD 2 feedback, a total of 941 (94.1%) unique images were identified as realistic and 59/1000 (5.9%) images were not identified as realistic by any of the experts. ML-GZSL showed the highest agreement between RAD 1 and RAD 2."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,4.0,Conclusion,"Our experiments demonstrate that our approach of multi label GZSL is more accurate than using conventional approaches that solve the single-label scenario. We propose a novel feature disentanglement approach that obtains class specific and class agnostic features from the training images. Additionally, the relationship between text embeddings of disease labels is used to create a dictionary that guides clustering and feature synthesis. Classification results on multiple publicly available chest xray datasets demonstrate the improved performance obtained by using class specific features. The synthetic features obtained by our method are realistic since a major percentage of the corresponding reconstructed images are validated as realistic by trained clinicians."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 26.
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,1.0,Introduction,"Chest X-ray (CXR) plays a vital role in screening and diagnosis of thoracic diseases [19]. The effectiveness of deep-learning based computer-aided diagnosis has been demonstrated in disease detection [21]. However, one of the major challenges in training deep learning models for medical purposes is the need for extensive, high-quality clinical annotation, which is time-consuming and costly.Recently, CLIP [22] and ALIGN [10] have shown the ability to perform vision tasks without any supervision. However, vision-language pre-training (VLP) in the CXR domain still lacks sufficient image-text datasets because many public datasets consist of image-label pairs with different class compositions. Med-CLIP [26] attempted to a rule-based labler to use both image-text data and image-label data. However, it relies on the performance of the rule-based labeler and is not scalable to other diseases that the labeler cannot address.In this paper, we propose a training method, CXR-CLIP, that integrates image-text data with image-label data using class-specific prompts made by radiologists. Our method does not depend on a rule-based labeler and can be applied to any image-label data. Also, inspired by DeCLIP [13], we used Multi-View Supervision (MVS) utilizing multiple images and texts in a CXR study to make more image-text pairs for efficient learning. In addition, we introduce two contrastive loss functions, named image contrastive loss (ICL) and text contrastive loss (TCL), to learn study-level characteristics of the CXR images and reports respectively.The main contributions of this paper are summarized as follows. 1) We tackle the lack of data for VLP in CXR by generating image-text pairs from image-label datasets using prompt templates designed by radiologists and utilizing multiple images and texts in a study. 2) Two additional contrastive losses are introduced to learn discriminate features of image and text, improving image-text retrieval performances. 3) Performance of our model is validated on diverse datasets with zero-shot and few-shot settings."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,2.0,Related Work,"Data Efficient VLP. Recent studies [13,17] have proposed data-efficient VLP via joint learning with self-supervision. DeCLIP [13] suggested MVS that utilizes image and text augmentation to leverage positive pairs along with other selfsupervisions. In CXR domain, GloRIA [7] aligned words in reports and subregions in an image for label efficiency, and BioVIL [2] combined self-supervision for label efficiency. We modify MVS as two distinct images and texts from a study and present self-supervised loss functions, ICL and TCL for efficient learning.Self-supervision Within CXR Study. A CXR study could include several images in different views and two report sections: 'findings' and 'impression'. The impression section includes the differential diagnosis inferred from the findings section. BioVIL [2] enhanced the text encoder by matching two sections during language pre-training. MedAug [24] shows that self-supervised learning by matching images in a study is better than differently augmented images. We utilize both of multiple images and texts from a single study in VLP in an end-to-end fashion.Leveraging Image-Label Data in VLP. MedCLIP [26] integrated unpaired images, texts, and labels using rule-based labeler [8], which is less capable of retrieving the exact report for a given image due to the effect of decoupling image-text pairs. UniCL [28] suggested using prompts to leverage image-label dataset [4], considering the samples from the same label to be a positive pair. To our knowledge, this is the first work to utilize prompting for training in CXR domain. For the image-label data, two different prompts are generated from class labels as (t 1 , t 2 ). Using sampled pairs, the encoders are trained with three kinds of contrastive losses (MVS, ICL, and TCL)."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.0,Method,"CXR-CLIP samples image-text pairs from not only image-text data but also image-label data, and learns study-level characteristics with two images and two texts per study. The overview of the proposed method is illustrated in Fig. 1."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.1,Data Sampling,"We define a CXR study as s = {X, T }, where X is a set of images, and T is a set of ""findings"" and ""impression"" sections. The study of image-label dataset has a set of image labels Y instead of T . For the image-label dataset, we make promptbased texts T = Concat({p ∼ P (y)} y∈Y ), where p is a sampled prompt sentence, P (y) is a set of prompts given the class name and value y, and Concat(•) means concatenating texts. The set of prompts is used to generate sentences such as actual clinical reports, taking into account class labels and their values (positive, negative, etc.), unlike the previous prompt [7] for evaluation which randomly combines a level of severity, location, and sub-type of disease. Our prompts are available in Appendix.We sample two images (x 1 , x 2 ) in X if there are multiple images. Otherwise, we use augmented image A i (x 1 ) as x 2 , where A i is image augmentation. To leverage various information from different views in CXR (AP, PA, or lateral), we sample images from two distinct views as possible. Similarly, we sample two texts (t 1 , t 2 ) in T if there are both ""findings"" and ""impression"". Otherwise, we use augmented text A t (t 1 ) as t 2 , where A t is text augmentation. For the imagelabel data, we sample two prompt sentences as t 1 and t 2 from the constructed T = Concat({p ∼ P (y)} y∈Y )."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.2,Model Architecture,"We construct image encoder E i and text encoder E t to obtain global representations of image and text, and a projection layer f i and f t to match the size of final embedding vectors.Image Encoder. We have tested two different image encoders; ResNet-50 [6] and Swin-Tiny [14] as follow [7,26]. We extract global visual features from the global average pooled output of the image encoder. A linear layer is adopted to project the embeddings into the same size as text embeddings. The normalized visual embedding v is obtained by v = f i (E i (x)) / ||f i (E i (x))||. We denote a batch of the visual embeddings as V = {v} n i=1 , where n is a batch size. Text Encoder. We use BioClinicalBERT [1] model, which is the same architecture as BERT [5] but pre-trained with medical texts [11] as follow [7,26]. We use [EOS] token's final output as the global textual representation. Also, a linear projection layer is adopted the same as the image encoder. The normalized text embedding u is denoted as u = f t (E t (t)) / ||f t (E t (t))||. We denote a batch of the text embedding as U = {u} n i=1 and (v i , u i ) are paired."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.3,Loss Function,"In this section, we first describe CLIP loss [22] and then describe our losses (MVS, ICL, TCL) in terms of CLIP loss. The goal of CLIP loss is to pull image embedding and corresponding text embedding closer and to push unpaired image and text farther in the embedding space. The InfoNCE loss is generally adopted as a type of contrastive loss, and CLIP uses the average of two InfoNCE losses; image-to-text and text-to-image. The formula for CLIP loss is given by(1) where τ is a learnable temperature to scale logits.In DeCLIP [13], MVS uses four L CLIP loss with all possible pairs augmented views; (x, t), (x, A t (t)), (A i (x), t) and (A i (x), A t (t)). We modify DeCLIP's MVS to fit the CXR domain by the composition of the second example. DeCLIP only utilizes an augmented view of the original sample, but we sample a pair of the second image and text as described in Sect. 3.1. We denote the first and the second sets of image embeddings as U 1 , U 2 , and text embeddings as(2) The goal of ICL and TCL is to learn modality-specific characteristics in terms of image and text respectively. We design ICL and TCL as same as CLIP loss, but the input embeddings are different. ICL only uses image embeddings;ICL pulls image embeddings from the same study and pushes image embeddings from the different studies, so that, the image encoder can learn study-level diversity. Similarly, TCL pulls embeddings of ""findings"" and ""impression"" in the same study or diverse expressions of prompts from the same label and pushes the other studies' text embeddings, so that the text encoder can match diverse clinical expressions on the same diagnosis. Thereby, the final training objective consists of three contrastive losses balanced each component by λ I and λ T , formulated by"
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.1,Datasets,"We used three pre-trained datasets and tested with various external datasets to test the generalizability of models. The statistics of the datasets used are summarized in Table 1.MIMIC-CXR [12] consists of CXR studies, each with one or more images and free-form reports. We extracted ""findings"" and ""impression"" from the reports. We used the training split for pre-training and the test split for image-to-text retrieval.CheXpert [8] is an image-label data with 14 classes, obtained from the impression section by its rule-based labeler, and each class is labeled as positive, Note that only the reports of CheXpert5x200 are publicly available, but the reports of CheXpert are not. Following the previous works [7,26], we excluded CheXpert5x200 from the training set and used it for test. ChestX-ray14 [25] consists of frontal images with binary labels for 14 diseases. Prompts are generated by sampling 3 negative classes per study. We used 20% of the original training set for validation, and the remaining 80% for pretraining.RSNA pneumonia [23] is binary-labeled data as pneumonia or normal. We split train/valid/test set 70%, 15%, 15% of the dataset following [7] for the external classification task.SIIM Pneumothorax1 is also binary labeled as pneumothorax or normal. We split the train/valid/test set same ratio as RSNA pneumonia following [7] and used it for the classification task.VinDR-CXR [18] contains 22 local labels and 6 global labels of disease, which were obtained by experienced radiologists. We split the validation set from the original training set. Of 28 classes, ""other diseases"" and ""other lesions"" classes were excluded. Then, only 18 classes having 10 or more samples within the test set were evaluated for the binary classification of each class as follow [9].Open-I [3] is an image-text dataset. From each study, one of the report sections and one frontal-view image were sampled and used for image-to-text retrieval."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.2,Implementation Details,"We used augmentations A i and A t to fit medical images and reports. For A i , we resize and crop with scale [0.8, 1.1], randomly adapt CLAHE [20], and random color jittering; brightness, hue ratios from [0.9, 1.1] and contrast, saturation [0.8, 1.2]. For A t , to preserve clinical meaning, sentence swap and back-translation2 from Italian to English is used. The image size and final-embedding size are set to 224 and 512 respectively as in previous work [26]. We set λ I and λ T to 1.0, 0.5 for balancing total loss. Two encoders were trained for 15 epochs in a mixed-precision manner, early stopped by validation loss, and optimized by AdamW [16] with an initial learning rate 5e-5 and a weight decay 1e-4. We used cosine-annealing learning-rate scheduler [15] with warm-up for 1 epoch. A training batch consists of 128 studies with 256 image-text pairs. We implemented all experiments on PyTorch with 4 NVIDIA V100 GPUs."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.3,Comparison with State-of-the-Arts,"Zero-Shot and Few-Shot Classification. Table 2 shows performance on classification tasks of our models and state-of-the-art models. To evaluate zero- Table 3. Comparison with state-of-the-arts for image-to-text retrieval. The notations of datasets and models are same to Table 2.Model Name Pre-Train Dataset CheXpert5x200 MIMIC-CXR Open-I Total RSUM R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 shot classification fairly, we used evaluation prompts suggested from previous works [2,7,9]. The evaluation prompts are available in Appendix. We evaluate binary classification computed by Area Under ROC (AUC) and multi-class classification computed by accuracy (ACC). Our ResNet model trained with MIMIC-CXR outperforms GloRIA [7] except for CheXpert5x200, as GloRIA trained with image-text pair in CheXpert. Our SwinTiny model trained with MIMIC-CXR and CheXpert outperforms MedCLIP [26], which is the same architecture trained with the same datasets, in most of the metrics. Adding more pre-training datasets by prompting image-label datasets tends to improve performance for classifications, while the SwinTiny CXR-CLIP pre-trained with three datasets, performs the best for most of the metrics. More comparison with self-supervised models is available in Appendix.Image-to-Text Retrieval. We evaluated image-to-text retrieval computed by R@K, the recall of the exact report in the top K retrieved reports for a given image. (Table 3) While GloRIA [7] uses image-text pairs in CheXpert(C*) which is not available in public, CXR-CLIP uses image-text in MIMIC-CXR. So we adapt an external image-text dataset Open-I [3] for a fair comparison. GloRIA Table 4. Ablations and comparison with CLIP [22] and DeCLIP [13]. Our augmentations effectively preserves clinical meaning than EDA. Our full methodology (CXR-CLIP) outperforms DeCLIP."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,,Method,CheXpert 5x200 MIMIC-CXR Total RSUM ACC R@1 R@5 R@10 R@1 R@5 R@10 Vanila CLIP 58.9 4. 4  
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.4,Ablations,"For the ablation study, models with ResNet-50 [6] backbone were trained on MIMIC-CXR and CheXpert datasets and tested on zero-shot classification and image-to-text retrieval tasks with MIMIC-CXR and CheXpert5x200 datasets. We conducted two ablations shown in Table 4. First, we analyzed the effect of each component of CXR-CLIP by adding the components to vanilla CLIP [22] one by one. To validate our data sampling closer, we divided the sampling method into three parts 1) study-level sampling 2) data augmentations 3) Multi-view and Multi-text sampling (MVS). Our study-level sampling strategy improves performance compared to vanilla CLIP, which uses a naive sampling method bringing an image and corresponding report. Additionally, the modified data augmentation to fit the CXR domain contributes to performance increment of classification, the similar performance on retrieval. MVS slightly improves performances in both classification and image-text retrieval. Adding more supervision (ICL and TCL) improves performance by utilizing better multi-views and multi-text inputs. However, TCL drops the performance of recalls in CheXpert5x200, TCL could be hard to optimize variation of the radiologic report and prompt not diverse as images.In the second ablation study, CXR-CLIP was compared to DeCLIP [13] to confirm that our MVS using two image-text pairs per study is better than the MVS of DeCLIP which uses naively augmented images and texts. We show that our text augmentation outperforms DeCLIP's text augmentation named EDA [27] in terms of image-to-text recall, which implies our text augmentation preserves clinical meaning. The superiority of our MVS over DeCLIP's MVS confirms that using multiple images and texts from one study is better than using images and texts from augmented examples. Also, our full methodology (CXR-CLIP) outperforms DeCLIP, suggesting that our method efficiently learns in the CXR domain more than DeCLIP."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,5.0,Conclusion,"We presented a framework enlarging training image-text pair by using imagelabel datasets as image-text pair with prompts and utilizing multiple images and report sections in a study. Adding image-label datasets achieved performance gain in classification tasks including zero-shot and few-shot settings, on the other hand, lost the performance of retrieval tasks. We also proposed loss functions ICL and TCL to enhance the discriminating power within each modality, which effectively increases image-text retrieval performance. Our additional loss functions are designed to efficiently learn CXR domain knowledge along with image-text contrastive learning."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_10.
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,1.0,Introduction,"Model generalizability is one of the main challenges of AI, especially in high stake applications such as healthcare. While NN models achieve state-of-the-art (SOTA) performance in disease classification [9,17,24], they are brittle to small shifts in the data distribution [7] caused by a change in acquisition protocol or scanner type [22]. Fine-tuning all or some layers of a NN model on the target domain can alleviate this problem [2], but it requires a substantial amount of labeled data and be computationally expensive [12,21]. In contrast, radiologists follow fairly generalizable and comprehensible rules. Specifically, they search for patterns of changes in anatomy to read abnormality from an image and apply logical rules for specific diagnoses. This approach is transparent and closer to an interpretable-by-design approach in AI. We develop a method to extract a mixture of interpretable models based on clinical concepts, similar to radiologists' rules, from a pre-trained NN. Such a model is more data-and computationefficient than the original NN for fine-tuning to a new distribution.Standard interpretable by design method [18] finds an interpretable function (e.g., linear regression or rule-based) between human-interpretable concepts and final output [14]. A concept classifier [19,26] detects the presence or absence of concepts in an image. In medical images, previous research uses TCAV scores [13] to quantify the role of a concept on the final prediction [3,6,23], but the conceptbased interpretable models have been mostly unexplored. Recently Posthoc Concept Bottleneck models (PCBMs) [25] identify concepts from the embeddings of BB. However, the common design choice amongst those methods relies on a single interpretable classifier to explain the entire dataset, cannot capture the diverse sample-specific explanations, and performs poorly than their BB variants.Our Contributions. This paper proposes a novel data-efficient interpretable method that can be transferred to an unseen domain. Our interpretable model is built upon human-interpretable concepts and can provide sample-specific explanations for diverse disease subtypes and pathological patterns. Beginning with a BB in the source domain, we progressively extract a mixture of interpretable models from BB. Our method includes a set of selectors routing the explainable samples through the interpretable models. The interpretable models provide First-order-logic (FOL) explanations for the samples they cover. The remaining unexplained samples are routed through the residuals until they are covered by a successive interpretable model. We repeat the process until we cover a desired fraction of data. Due to class imbalance in large CXR datasets, early interpretable models tend to cover all samples with disease present while ignoring disease subgroups and pathological heterogeneity. We address this problem by estimating the class-stratified coverage from the total data coverage. We then finetune the interpretable models in the target domain. The target domain lacks concept-level annotation since they are expensive. Hence, we learn a concept detector in the target domain with a pseudo labeling approach [15] and finetune the interpretable models. Our work is the first to apply concept-based methods to CXRs and transfer them between domains."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.0,Methodology,"Notation. Assume f 0 : X → Y is a BB, trained on a dataset X ×Y ×C, with X , Y, and C being the images, classes, and concepts, respectively; f 0 = h 0 •Φ, where Φ and h 0 is the feature extractor and the classifier respectively. Also, m is the number of class labels. This paper focuses on binary classification (having or not having a disease), so m = 2 and Y ∈ {0, 1}. Yet, it can be extended to multiclass problems easily. Given a learnable projection [4,5], t : Φ → C, our method learns Fig. 1. Schematic view of our method. Note that f k (.) = h k (Φ(.)). At iteration k, the selector routes each sample either towards the expert g k with probability π k (.) or the residual r k = f k-1 -g k with probability 1-π k (.). g k generates FOL-based explanations for the samples it covers. Note Φ is fixed across iterations.three functions: (1) a set of selectors (π : C → {0, 1}) routing samples to an interpretable model or residual, (2) a set of interpretable models (g : C → Y), and (3) the residuals. The interpretable models are called ""experts"" since they specialize in a distinct subset of data defined by that iteration's coverage τ as shown in SelectiveNet [16]. Figure 1 illustrates our method."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.1,Distilling BB to the Mixture of Interpretable Models,"Handling Class Imbalance. For an iteration k, we first split the given coverage τ k to stratified coverages per class as, where w m denotes the fraction of samples belonging to the m th class; N m and N are the samples of m th class and total samples, respectively. Learning the Selectors. At iteration k, the selector π k routes i th sample to the expert (g k ) or residual (r k ) with probability π k (c i ) and 1 -π k (c i ) respectively. For coverages {τ k m , ∀m}, we learn g k and π k jointly by solving the loss:where θ * s k , θ * g k are the optimal parameters for π k and g k , respectively. R k is theis the empirical mean of samples of m th class selected by the selector for the associated expert g k . We define L k (g k ,π k ) in the next section. The selectors are neural networks with sigmoid activation. At inference time, π k routes a sample to g k if and only if π k (.) ≥ 0.5."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Learning the Experts.,"For iteration k, the loss L k (g k ,π k ) distills the expert g k from f k-1 , BB of the previous iteration by solving the following loss:is the cumulative probability of the sample covered by the residuals for all the previous iterations from 1, • • • , k -1 (i.e., k-1 j=1 1 -π j (c i ) ) and the expert g k at iteration k (i.e., π k (c i )).Learning the Residuals. After learning g k , we calculate the residual as,of logits). We fix Φ and optimize the following loss to update h k to specialize on those samples not covered by g k , effectively creating a new BB f k for the next iteration (k + 1):We refer to all the experts as the Mixture of Interpretable Experts (MoIE-CXR). We denote the models, including the final residual, as MoIE-CXR+R. Each expert in MoIE-CXR constructs sample-specific FOLs using the optimization strategy and algorithm discussed in [4]."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.2,Finetuning to an Unseen Domain,"We assume the MoIE-CXR-identified concepts to be generalizable to an unseen domain. So, we learn the projection t t for the target domain and compute the pseudo concepts using SSL [15]. Next, we transfer the selectors, experts, and final residual ({π k s , g k s } K k=1 and f K s ) from the source to a target domain with limited labeled data and computational cost. Algorithm 1 details the procedure. Algorithm 1. Finetuning to an unseen domain. "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,3.0,Experiments,"We perform experiments to show that MoIE-CXR 1) captures a diverse set of concepts, 2) does not compromise BB's performance, 3) covers ""harder"" instances with the residuals in later iterations resulting in their drop in performance, 4) is finetuned well to an unseen domain with minimal computation. Experimental Details. We evaluate our method using 220,763 frontal images from the MIMIC-CXR dataset [11]. We use Densenet121 [8] as BB (f 0 ) to classify cardiomegaly, effusion, edema, pneumonia, and pneumothorax, considering each to be a separate binary classification problem. We obtain 107 anatomical and observation concepts from the RadGraph's inference dataset [10], automatically generated by DYGIE++ [20]. We train BB following [24]. To retrieve the concepts, we utilize until the 4 th Densenet block as feature extractor Φ and flatten the features to learn t. We use an 80%-10%-10% train-validation-test split with no patient shared across splits. We use 4, 4, 5, 5, and 5 experts for cardiomegaly, pneumonia, effusion, pneumothorax, and edema. We employ ELL [1] as g. Further, we only include concepts as input to g if their validation auroc exceeds 0.7. Refer to Table 1 in the supplementary material for the hyperparameters. We stop until all the experts cover at least 90% of the data cumulatively.Baseline. We compare our method with 1) end-to-end CEM [26], 2) sequential CBM [14], and 3) PCBM [25] baselines, comprising of two parts: a) concept predictor Φ : X → C, predicting concepts from images, with all the convolution blocks; and b) label predictor, g : C → Y, predicting labels from the concepts. We create CBM + ELL and PCBM + ELL by replacing the standard classifier with the identical g of MOIE-CXR to generate FOLs [1] for the baseline.MoIE-CXR Captures Diverse Explanations. Figure 2 illustrates the FOL explanations. Recall that the experts (g) in MoIE-CXR and the baselines are ELLs [1], attributing attention weights to each concept. A concept with high attention weight indicates its high predictive significance. With a single g, the baselines rank the concepts in accordance with the identical order of attention weights for all the samples in a class, yielding a generic FOL for that class. In Fig. 2, the baseline PCBM + ELL uses left pleural and pleural unspec to identify effusion for all four samples. MoIE-CXR deploys multiple experts, learning to specialize in distinct subsets of a class. So different interpretable models in MoIE assign different attention weights to capture instance-specific concepts unique to each subset. In Fig. 2 expert2 relies on right pleural and pleural unspec, but expert4 relies only on pleural unspec to classify effusion. The results show that the learned experts can provide more precise explanations at the subject level using the concepts, increasing confidence and trust in clinical use.Table 1. MoIE-CXR does not compromize the performance of BB. We provide the mean and standard errors of AUROC over five random seeds. For MoIE-CXR, we also report the percentage of test set samples covered by all experts as ""Coverage"". We boldfaced our results and BB."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Model,Effusion  
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,MoIE-CXR does not Compromise BB's Performance. Analysing MoIE-CXR:,"Table 1 shows that MoIE-CXR outperforms other models, including BB. Recall that MoIE-CXR refers to the mixture of all interpretable experts, excluding any residuals. As MoIE-CXR specializes in various subsets of data, it effectively discovers sample-specific classifying concepts and achieves superior performance. In general, MoIE-CXR exceeds the interpretable-by-design baselines (CEM, CBM, and CBM + ELL) by a fair margin (on average, at least ∼ 10% ↑), especially for pneumonia and pneumothorax where the number of samples with the disease is significantly less (∼ 750/24000 in the testset)."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Analysing MoIE-CXR+R:,"To compare the performance on the entire dataset, we additionally report MoIE-CXR+R, the mixture of interpretable experts with the final residual in Table 1. MoIE-CXR+R outperforms the interpretable-by-design models and yields comparable performance as BB. The residualized PCBM baseline, i.e., PCBM-h, performs similarly to MoIE-CXR+R. PCBM-h rectifies the interpretable PCBM's mistakes by learning the residual with the complete dataset to resemble BB's performance. However, the experts and the final residual approximate the interpretable and uninterpretable fractions of BB, respectively. In each iteration, the residual focuses on the samples not covered by the respective expert to create BB for the next iteration and likewise. As a result, the final residual in MoIE-CXR+R covers the ""hardest"" examples, reducing its overall performance relative to MoIE-CXR.Identification of Harder Samples by Successive Residuals. Figure 3 (a-c) reports the proportional AUROC of the experts and the residuals per iteration. The proportional AUROC is the AUROC of that model times the empirical coverage, ζ k , the mean of the samples routed to the model by the respective selector (π k ). According to Fig. 3a in iteration 1, the residual (black bar) contributes more to the proportional AUROC than the expert1 (blue bar) for effusion with both achieving a cumulative proportional AUROC ∼ 0.92. All the final experts collectively extract the entire interpretable component from BB f 0 in the final iteration, resulting in their more significant contribution to the cumulative performance. In subsequent iterations, the proportional AUROC decreases as the experts are distilled from the BB of the previous iteration. The BB is derived from the residual that performs progressively worse with each iteration. The residual of the final iteration covers the ""hardest"" samples. Tracing these samples back to the original BB f 0 , f 0 underperforms on these samples (Fig. 3 (d-f)) as the residual.Applying MoIE-CXR to the Unseen Domain. In this experiment, we utilize Algorithm 1 to transfer MoIE-CXR trained on MIMIC-CXR dataset to Stanford Chexpert [9] dataset for the diseases -effusion, cardiomegaly and edema. Using 2.5%, 5%, 7.5%, 10%, and 15 % of training data from the Stanford Chexpert dataset, we employ two variants of MoIE-CXR where we (1) train only the selectors (π) without finetuning the experts (g) (""No finetuned"" variant of MoIE-CXR in Fig. 4), and (2) finetune π and g jointly for only 5 epochs (""Finetuned"" variant of MoIE-CXR and MoIE-CXR + R in Fig. 4). Finetuning π is essential to route the samples of the target domain to the appropriate expert. As later experts cover the ""harder"" samples of MIMIC-CXR, we only transfer the experts of the first three iterations (refer to Fig. 3). To ensure a fair comparison, we finetune (both the feature extractor Φ and classifier h 0 ) BB: f 0 = h 0 • Φ of MIMIC-CXR with the same training data of Stanford Chexpert for 5 epochs. Throughout this experiment, we fix Φ while finetuning the final residual in MoIE+R as stated in Eq. 3. Figure 4 displays the performances of different models and the computation costs in terms of Flops. The Flops are calculated as, Flop of (forward propagation + backward propagation) × (total no. of batches) × (no of training epochs). The finetuned MoIE-CXR outperforms the finetuned BB (on average ∼ 5% ↑ for effusion and cardiomegaly). As experts are simple models [1] and accept only low dimensional concept vectors compared to BB, the computational cost to train MoIE-CXR is significantly lower than that of BB (Fig. 4 (d-f)). Specifically, BB requires ∼ 776T flops to be finetuned on 2.5% of the training data of Stanford CheXpert, whereas MoIE-CXR requires ∼ 0.0065T flops. As MoIE-CXR discovers the sample-specific domain-invariant concepts, it achieves such high performance with low computational cost than BB."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,4.0,Conclusion,"This paper proposes a novel iterative interpretable method that identifies instance-specific concepts without losing the performance of the BB and is effectively fine-tuned in an unseen target domain with no concept annotation, limited labeled data, and minimal computation cost. Also, as in the prior work, MoIEcaptured concepts may not showcase a causal effect that can be explored in the future."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,,"1: Input: s (Φs). Source data: Ds = {Xs, Cs, Ys}. t represents MoIE-CXR and MoIE-CXR + R for the target domain."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 59.
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,1.0,Introduction,"The absence of highly accurate and noninvasive diagnostics for risk-stratifying benign vs malignant solitary pulmonary nodules (SPNs) leads to increased anxiety, costs, complications, and mortality [22,26]. The use of noninvasive methods to discriminate malignant from benign SPNs is a high-priority public health initiative [8,9]. Deep learning approaches have shown promise in classifying SPNs from longitudinal chest computed tomography (CT) [1,5,12,21], but approaches that only consider imaging are fundamentally limited. Multimodal models generally outperform single modality models in disease diagnosis and prediction [24], and this is especially true in lung cancer which is heavily contextualized through non-imaging risk factors [6,23,30]. Taken together, these findings suggest that learning across both time and multiple modalities is important in biomedical predictive modeling, especially SPN diagnosis. However, such an approach that scales across longitudinal multimodal data from comprehensive representations of the clinical routine has yet to be demonstrated [24]. Related Work. Directly learning from routinely collected electronic health records (EHRs) is challenging because observations within and between modalities can be sparse and irregularly sampled. Previous studies overcome these challenges by aggregating over visits and binning time series within a Bidirectional Encoder Representations from Transformers (BERT) architecture [2,14,20,25], limiting their scope to data collected on similar time scales, such as ICU measurements, [11,29], or leveraging graph guided transformers to handle asynchrony [33]. Self-attention [31] has become the dominant technique for learning powerful representations of EHRs with trade-offs in interpretability and quadratic scaling with the number of visits or bins, which can be inefficient with data spanning multiple years. In contrast, others address the episodic nature of EHRs by converting non-imaging variables to continuous longitudinal curves that provide the instantaneous value of categorical variables as intensity functions [17] or continuous variables as latent functions [16]. Operating with the hypothesis that distinct disease mechanisms manifest independently of one another in a probabilistic manner, one can learn a transformation that disentangles latent sources, or clinical signatures, from these longitudinal curves. Clinical signatures learned in this way are expert-interpretable and have been well-validated to reflect known pathophysiology across many diseases [15,18]. Given that several clinical risk factors have been shown to independently contribute to lung cancer risk, these signatures are well poised for this predictive task. Despite the wealth of studies seeking to learn comprehensive representations of routine EHRs, these techniques have not been combined with longitudinal imaging."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,,Present Work.,"In this work, we jointly learn from longitudinal medical imaging, demographics, billing codes, medications, and lab values to classify SPNs. We converted 9195 non-imaging event streams from the EHR to longitudinal curves to impute cross-sections and synchronize across modalities. We use Independent Component Analyses (ICA) to disentangle latent clinical signatures from these curves, with the hypothesis that the disease mechanisms known to be important for SPN classification can also be captured with probabilistic independence. We leverage a transformer-based encoder to fuse features from both longitudinal imaging and clinical signature expressions sampled at intervals ranging from weeks to up to five years. Due to the importance of time dynamics in SPN classification, we use the time interval between samples to scale self-attention with the intuition that recent observations are more important to attend to than older observations. Compared with imaging-only and a baseline that aggregates longitudinal data into bins, our approach allowed us to incorporate additional modalities from routinely collected EHRs, which led to improved SPN classification."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,2.0,Methods,"Latent Clinical Signatures via Probabilistic Independence. We obtained event streams for billing codes, medications, and laboratory tests across the full record of each subject in our EHR cohorts (up to 22 years). After removing variables with less than 1000 events and mapping billing codes to the SNOMED-CT ontology [7], we arrived at 9195 unique variables. We transformed each variable to a longitudinal curve at daily resolution, estimating the variable's instantaneous value for each day [18]. We used smooth interpolation for continuous variables [4] or a continuous estimate of event density per time for event data. Previous work used Gaussian process inference to compute both types of curves [16,17]. For this work we traded approximation for computational efficiency. To encode a limited memory into the curve values, each curve was smoothed using a rolling uniform mean of the past 365 d (Fig. 1,left).We use an ICA model to estimate a linear decomposition of the observed curves from the EHR-Pulmonary cohort to independent latent sources, or clinical signatures. Formally, we have dataset D EHR-Pulmonary = {L k | k = 1, . . . , n} with longitudinal curves denoted as L k = {l i |i = 1, . . . , 9195}. We randomly sample l i ∀i ∈ [1,9195] at a three-year resolution and concatenate samples across all subjects as x i ∈ R m . For D EHR-Pulmonary , m was empirically found to be 630037. We make a simplifying assumption that x i is a linear mixture of c latent sources, s, with longitudinal expression levels e ∈ R m : The linear mixture is then X = SE with x i forming the rows of X, S ∈ R 9195×c denoting the independent latent sources and E ∈ R c×m denoting the expression matrix. We set c = 2000 and estimated S in an unsupervised manner using FastICA [13]. Given longitudinal curves for another cohort, for instance D Image-EHR = {X k | k = 1, . . . , n}, we obtain expressions of clinical signatures for subject k via E k = S -1 X k (Fig. 1, left).Longitudinal Multimodal Transformer (TDSig). We represent our multimodal datasets D Image-EHR and }, where T is the maximum sequence length. We set T = 3 and added a fixed padding embedding to represent missing items in the sequence. Embeddings that incorporate positional and segment information are computed for each item in the sequence (Fig. 1, right). Token embeddings for images are a convolutional embeddings of five concatenated 3D patches proposed by a pretrained SPN detection model [21]. We use a 16-layer ResNet [10] to compute this embedding. Likewise, token embeddings for clinical signature expressions are linear transformations to the same dimension as imaging token embeddings. The sequence of embeddings are then passed through a multi-headed Transformer. All embeddings except the nodule detection model are co-optimized with the Transformer. We will refer to this approach as TDSig.Time-Distance Self-attention. Following [5,19,32], we intuit that if medical data is sampled as a cross-sectional manifestation of a continuously progressing phenotype, we can use a temporal emphasis model (TEM) emphasize the importance of recent observations over older ones. Additionally, self-attention is masked for padded embeddings, allowing our approach to scale with varying sequence lengths across subjects. Formally, if subject k has a sequence of T images at relative acquisition days t 1 . . . t T , we construct a matrix R of relative times with entries R i,j = |t T -t i | where t i is the acquisition day of tokens êk,i and ĝk,i , or 0 if they are padded embeddings. We map the relative times in R to a [0,1] value in R using a TEM of the form:This is a flipped sigmoid function that monotonically decreases with the relative time from the most recent observation. Its slope of decline and decline offset are governed by learnable non-negative parameters b and c respectively. A separate TEM is instantiated for each attention head, with the rationale that separate attention heads can learn to condition on time differently. The transformer encoder computes query, key, and value matrices as linear transformations of input embedding H = { Ê Ĝ} at attention head pTEM-scaled self-attention is computed via element-wise multiplication of the query-key product and R:where M is the padding mask [31] and d is the dimension of the query and key matrices. ReLU gating of the query-key product allows the TEM to adjust the attention weights in an unsigned direction.Baselines. We compared against a popular multimodal strategy that aggregates event streams into a sequence of bins as opposed to our method of extracting instantaneous cross-sectional representations. For each scan, we computed a TF-IDF [27] weighted vector from all billing codes occurring up to one year before the scan acquisition date. We passed this through a published Word2Vec-based medical concept embedding [3] to compute a contextual representation ∈ R 100 . This, along with the subject's scans, formed a sequence that was used as input to a model we call TDCode2vec. Our search for contextual embeddings for medications and laboratory values did not yield any robust published models that were compatible with our EHR's nomenclature, so these were not included in TDCode2vec. We also performed experiments using only image sequences as input, which we call TDImage. Finally, we implemented single cross-sectional versions of TDImage, TDCode2vec, and TDSig, CSImage, CSCode2vec, and CSSig respectively, using the scan date closest to the lung malignancy diagnosis for cases or SPN date for controls. All baselines except CSImage, which employed a multi-layer perceptron directly after the convolutional embedding, used the same architecture and time-distance self-attention as TDSig. The transformer encoders in this study were standardized to 4 heads, 4 blocks, input token size of 320, multi-layer perception size of 124, self-attention weights of size 64. This work was supported by Pytorch 1.13.1, CUDA 11.7."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"Datasets. This study used an imaging-only cohort from the NLST [28] and three multimodal cohorts from our home institution with IRB approval (Table 1). For the NLST cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a biopsy-confirmed diagnosis of lung malignancy and controls who had a positive screening result for an SPN but no lung malignancy. We randomly sampled from the control group to obtain a 4:6 case control ratio. Next, EHR-Pulmonary was the unlabeled dataset used to learn clinical signatures in an unsupervised manner. We searched all records in our EHR archives for patients who had billing codes from a broad set of pulmonary conditions, intending to capture pulmonary conditions beyond just malignancy. Additionally, Image-EHR was a labeled dataset with paired imaging and EHRs. We searched our institution's imaging archive for patients with three chest CTs within five years. In the EHR-Image cohort, malignant cases were labeled as those with a billing code for lung malignancy and no cancer of any type prior. Importantly, this case criteria includes metastasis from cancer in non-lung locations. Benign controls were those who did not meet this criterion. Finally, Image-EHR-SPN was a subset of Image-EHR with the inclusion criteria that subjects had a billing code for an SPN and no cancer of any type prior to the SPN. We labeled malignant cases as those with a lung malignancy billing code occurring within three years after any scan and only used data collected before the lung malignancy code. All data within the five-year period were used for controls. We removed all billing codes relating to lung malignancy. A description of the billing codes used to define SPN and lung cancer events are provided in Supplementary 1.2. Training and Validation. All models were pretrained with the NLST cohort after which we froze the convolutional embedding layer. While this was the only pretraining step for image-only models (CSImage and TDimage), the multimodal models underwent another stage of pretraining using the Image-EHR cohort with subjects from Image-EHR-SPN subtracted. In this stage, we randomly selected one scan and the corresponding clinical signature expressions for each subject and each training epoch. Models were trained until the running mean over 100 global steps of the validation loss increased by more than 0.2. For evaluation, we performed five-fold cross-validation with Image-EHR-SPN, using up to three of the most recent scans in the longitudinal models. We report the mean AUC and 95% confidence interval from 1000 bootstrapped samples, sampling with replacement from the pooled predictions across all test folds. A two-sided Wilcoxon signed-rank test was used to test if differences in mean AUC between models were significant.Reclassification Analysis. We performed a reclassification analysis of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65, which are the cutoffs used to guide clinical management. Given a baseline comparison, our approach reclassifies a subject correctly if it predicts a higher risk tier than the baseline in cases, or a lower risk tier than the baseline in controls (Fig. 2)."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,4.0,Results,"The significant improvement with TDSig over CSSig demonstrates the advantage of longitudinally in the context of combining images and clinical signatures (Table 2). There were large performance gaps between TDSig and TDCode2vec, as well as between CSSig and CSCode2vec, demonstrating the advantage of  clinical signatures over a binned embedding strategy. Cross-sectional embedded billing codes did not significantly improve performance over images alone (CSCode2vec vs CSImage, p = 0.56), but adding clinical signatures did (CSSig vs CSImage, p < 0.01; TDSig vs TDImage, p < 0.01) and the greatest improvement in longitudinal data over single cross sections occurred when clinical signatures were included. For control subjects, TDSig correctly/incorrectly reclassified 40/18 from TDCode2vec, 54/8 from TDImage, 12/18 from CSSig, 104/7 from CSCode2vec, and 125/5 from CSImage. For case subjects, TDSig correctly/incorrectly reclassified 13/10 from TDCode2vec, 17/8 from TDImage, 12/2 from CSSig, 23/16 from CSCode2vec, and 29/16 from CSImage (Fig. 2). Full reclassification matrices are reported in Supplementary 6.1. On qualitative inspection of a control subject, clinical signatures likely added clarity to benign imaging findings that were difficult for baseline approaches to classify (Fig. 3)."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,5.0,Discussion and Conclusion,"This work presents a novel transformer-based strategy for integrating longitudinal imaging with interpretable clinical signatures learned from comprehensive multimodal EHRs. We demonstrated large performance gains in SPN classification compared with baselines, although calibration of our models is needed to assess clinical utility. We evaluated on clinically-billed SPNs, meaning that clinicians likely found these lesions difficult enough to conduct a clinical workup. In this setting, we found that adding clinical context increased the performance gap between longitudinal data and single cross-sections. Our clinical signatures incorporated longitudinality and additional modalities to build a better representation of clinical context than binned embeddings. We release our implementation at https://github.com/MASILab/lmsignatures.The lack of longitudinal multimodal datasets has long been a limiting factor [24] in conducting studies such as ours. One of our contributions is demonstrating training strategies in a small-dataset, incomplete-data regime. We were able to overcome our small cohort size (Image-EHR-SPN) by leveraging unsupervised learning on datasets without imaging (EHR-Pulmonary), pretraining on public datasets without EHRs (NLST), and pretraining on paired multimodal data with noisy labels (Image-EHR) within a flexible transformer architecture.Our approach of sampling cross-sections where clinical decisions are likely to be made scales well with long, multi-year observation windows, which may not be true for BERT-based embeddings [20,25]. We did not compare against these contextual embeddings because none have been publically released, but integrating these with longitudinal imaging is an area of future investigation."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,1.0,Introduction,"In recent years, Deep Learning (DL) based methods have achieved considerable success in the medical domain for tasks including disease diagnosis and clinical feature segmentation [20,28]. However, their progress is often constrained as they require large labelled datasets. Labelling medical image data is a labour-intensive and time-consuming process that needs the careful attention of clinical experts. Active learning (AL) can benefit the iterative improvement of any intelligent diagnosis system by reducing the burden of extensive annotation effort [19,25].Ophthalmologists use the segmentation of ocular Optical Coherence Tomography (OCT) images to diagnose, and treatment of eye diseases such as Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) [6]. Here, we propose a novel Edge estimation-based Active Learning EdgeAL framework for OCT image segmentation that leverages prediction uncertainty across the boundaries of the semantic regions of input images. The Edge information is one of the image's most salient features, and it can boost segmentation accuracy when integrated into neural model training [13]. We formulate a novel acquisition function that leverages the variance of the predicted score across the gradient surface of the input to measure uncertainty. Empirical results show that EdgeAL achieves state-of-the-art performance with minimal annotation samples, using a seed set as small as 2% of unlabeled data."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,2.0,Related Work,"Active learning is a cost-effective strategy that selects the most informative samples for annotation to improve model performance based on uncertainty [11], data distribution [22], expected model change [4], and other criteria [1]. A simpler way to measure uncertainty can be realized using posterior probabilities of predictions, such as selecting instances with the least confidence [9,11], or computing class entropy [14]. Some uncertainty-based approaches have been directly used with deep neural networks [24]. Gal et al. [7] propose dropout-base Monte Carlo (MC) sampling to obtain uncertainty estimation. It uses multiple forward passes with dropout at different layers to generate uncertainty during inference. Ensemble-based methods also have been widely used where the variance between the prediction outcomes from a collection of models serve as the uncertainty [18,23,27].Many AL methods have been adopted for segmentation tasks [8,15,18]. Gorriz et al. [8] propose an AL framework Melanoma segmentation by extending Cost-Effective Active Learning (CEAL) [26] algorithm where complimentary samples of both high and low confidence are selected for annotation. Mackowiak et al. [15] use a region-based selection approach and estimate model uncertainty using MC dropout to reduce human-annotation cost. Nath et al. [18] propose an ensemble-based method where multiple AL frameworks are jointly optimized, and a query-by-committee approach is adopted for sample selection. These methods do not consider any prior information to estimate uncertainty. Authors in [24] propose an AL framework for multi-view datasets [17] segmentation task where model uncertainty is estimated based on Kullback-Leibler (KL) divergence of posterior probability distributions for a disjoint subset of prior features such as depth, and camera position.However, viewpoint information is not always available in medical imaging. We leverage edge information as a prior for AL sampling based on previous studies where edge information has improved the performance of segmentation tasks [13]. To our knowledge, there has yet to be any exploration of using image edges as an a priori in active learning.There has not been sufficient work other than [12] related to Active Learning for OCT segmentation. Their approach requires foundation models [10] to be pre-trained on large-scale datasets in similar domains, which could be infeasible to collect due to data privacy. On the other hand, our method requires a few samples (∼2%) for initial training, overcoming the limitation of the need for a large dataset. "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.0,Methodology,"Figure 1 shows that our active learning technique consists of four major stages. First, we train the network on a subset of labeled images, usually a tiny percentage of the total collection (e.g., 2%). Following that, we compute uncertainty values for input instances and input areas. Based on this knowledge, we select superpixels to label and obtain annotations from a simulated oracle."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.1,Segmentation Network,"We trained our OCT semantic segmentation model using a randomly selected small portion of the labeled data D s , seed set, keeping the rest for oracle imitation. We choose Y-net-gen-ffc (YN * ) without pre-retrained weight initialization as our primary architecture due to its superior performance [6]."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.2,Uncertainty in Prediction,"EdgeAL seeks to improve the model's performance by querying uncertain areas on unlabeled data D u after training it on a seed set D s . To accomplish this, we have created a novel edge-based uncertainty measurement method. We compute the edge entropy score and edge divergence score -to assess the prediction ambiguity associated with the edges. Figure 2 depicts examples of input OCT, measured edge entropy, and edge kl-divergence corresponding to the input.Entropy Score on Edges. Analyzing the edges of raw OCT inputs yields critical information on features and texture in images. They may look noisy, but they summarize all the alterations in a picture. The Sobel operator can be used to identify edges in the input image [13]. Let us define the normalized absolute value of edges of an image I i by S i . |∇I i | is the absolute gradient.To determine the probability that each pixel in an image belongs to a particular class c, we use the output of our network, denoted as P (m,n) i (c). We adopt Monte Carlo (MC) dropout simulation for uncertainty sampling and average predictions over |D| occurrence from [7]. Consequently, an MC probability distribution depicts the chance of a pixel at location (m, n) in picture I i belonging to a class c, and C is the set of segmentation classes. We run MC dropouts |D| times during the neural network assessment mode and measure P (m,n) i (c) using Eq. 1.Following Zhao et al. [30], we apply contextual calibration on P (m,n) i (c) by S i to prioritize significant input surface variations. Now, S i is linked with a probability distribution, with φ (m,n) i (c) having information about the edges of input. This formulation makes our implementation unique from other active learning methods in image segmentation.We name φ m,n i (c) as contextual probability and define our edge entropy by following entropy formula of [14].Divergence Score on Edges. In areas with strong edges/gradients, edge entropy reflects the degree of inconsistency in the network's prediction for each input pixel. However, the degree of this uncertainty must also be measured. KLdivergence is used to measure the difference in inconsistency between P (m,n) i and φ (m,n) i for a pixel (m, n) in an input image based on the idea of self-knowledge distillation I i [29]. The edge divergence ED m,n i score can be formalized using Eq. 1 and 2. EDwheremeasures the difference between model prediction probability and contextual probability for pixels belonging to edges of the input (Fig. 2c)."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.3,Superpixel Selection,"Clinical images have sparse representation, which can be beneficial for active learning annotation [15]. We use a traditional segmentation technique, SEEDS [2], to leverage the local structure from images for finding superpixels. Annotating superpixels and regions for active learning may be more beneficial to the user than annotating the entire picture [15].We compute mean edge entropy EE r i and mean edge divergence ED d i for a particular area r within a superpixel. Where |r| is the amount of pixels in the superpixel region. We use regional entropy to find the optimal superpixel for our selection strategy and pick the one with the most significant value based on the literature [24].(i, r) = arg max (j,s) EE s j (6) Following [24], we find the subset of superpixels in the dataset with a 50% overlap (r, i). Let us call it set R. We choose the superpixels with the largest edge divergence to determine the ultimate query (sample) for annotation.(p, q) = arg maxAfter each selection, we remove the superpixels from R. The selection process runs until we have K amount of superpixels being selected from R.After getting the selected superpixel maps, we receive the matching ground truth information for the selected superpixel regions from the oracle. The model is then freshly trained on the updated labeled dataset for the next active learning iteration."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.0,Experiments and Results,"This section will provide a detailed overview of the datasets and architectures employed in our experiments. Subsequently, we will present the extensive experimental results and compare them with other state-of-the-art methods to showcase the effectiveness of our approach. We compare our AL method with nine well-known strategies: softmax margin (MAR) [9], softmax confidence (CONF) [26], softmax entropy (ENT) [14], MC dropout entropy (MCDR) [7], Core-set selection (CORESET) [23], (CEAL) [8], and regional MC dropout entropy (RMCDR) [15], maximum representations (MAXRPR) [27], and random selection (Random)."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.1,Datasets and Networks,"To test EdgeAL, we ran experiments on Duke [3], AROI [16], and UMN [21] datasets in which experts annotated ground truth segmentations. Duke contains 100 B-scans from 10 patients, AROI contains 1136 B-scans from 24, and UMN contains 725 OCT B-scans from 29 patients. There are nine, eight, and two segmentation classes in Duke, AROI, and UMN, respectively. These classes cover fluid and retinal layers. Based on convention and dataset guidelines [6,16], we use a 60:20:20 training: testing: validation ratio for the experiment without mixing one patient's data in any of the splits. Further, we resized all the images and ground truths to 224 × 224 using Bilinear approximation. Moreover, we run a 5fold cross-validation (CV) on the Duke dataset without mixing individual patient data in each fold's training, testing, and validation set. Table 1 summarizes the 5-fold CV results. We run experiments using Y-net(YN) [6], U-net (UN) [10], and DeepLab-V3 (DP-V3) [24] with ResNet and MobileNet backbones [10]. We present the results in Table 2. No pre-trained weights were employed in the execution of our studies other than the ablation study presented in Table 2. We apply mixed loss of dice and cross-entropy and Adam as an optimizer, with learning rates of 0.005 and weight decay of 0.0004, trained for 100 epochs with a maximum batch size of 10 across all AL iterations. We follow the hyperparameter settings and evaluation metric (dice score) of [6], which is the baseline of our experiment."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.2,Comparisons,"Figure 3 compares the performance of EdgeAL with other contemporary active learning algorithms across three datasets. Results show EdgeAL outperforms other methods on all 3 datasets. Our method can achieve 99% of maximum model performance consistently with about 12% (∼8 samples), 2.3% (∼16 samples), and 3% (∼14 samples) labeled data on Duke, AROI, and UNM datasets.Other AL methods, CEAL, RMCDR, CORESET, and MAR, do not perform consistently in all three datasets. We used the same segmentation network YN * and hyperparameters (described in Sect. 3) for a fair comparison.Our 5-fold CV result in Table 1 also concludes similarly. We see that after training on a 2% seed set, all methods have similar CV performance; however, Furthermore, to scrutinize if EdgeAL is independent of network architecture and weight initialization, we run experiments on four network architectures with default weight initialization of PyTorch (LeCun initialization)1 and imagenet weight initialization. Table 2 presents the test performance after training on 12% of actively selected data. These results also conclude that EdgeAL's performance is independent of the architecture and weight choices, while other active learning methods (RMCDR, MAXRPR) only perform well in pre-trained models (Table 2)."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,5.0,Conclusion,"EdgeAL is a novel active learning technique for OCT image segmentation, which can accomplish results similar to full training with a small amount of data by utilizing edge information to identify regions of uncertainty. Our method can reduce the labeling effort by requiring only a portion of an image to annotate and is particularly advantageous in the medical field, where labeled data can be scarce. EdgeAL's success in OCT segmentation suggests that a significant amount of data is not always required to learn data distribution in medical imaging. Edges are a fundamental image characteristic, allowing EdgeAL to be adapted for other domains without significant modifications, which leads us to future works."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 8.
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,1.0,Introduction,"Automatic multi-organ segmentation (MOS) plays a vital role in computeraided diagnosis and treatment planning. Recently, deep learning based methods have made remarkable progress in solving MOS tasks. However, they typically require a large amount of expert-level accurate, densely-annotated data for training, which is laborious and time consuming to collect. Therefore, existing fully labeled datasets (termed as FLDs) are very few and often low in sample size [1]. While there exist many publicly available partially labeled datasets (PLDs) [2,3], each with one or a few out of the many organs annotated. This has motivated the development of various Partially-Supervised Multi-Organ Segmentation (PSMOS) methods that aim to learn a unified model from a union of such datasets. For example, Dmitriev and Kaufman proposed the conditional U-Net to enable PSMOS using a single unified network [4]. Co-training between two models with consistency constraints on soft pseudo labels [6], and multi-scale features learned in a pyramid-input and pyramid-output network [7] were both explored for PSMOS. Other researchers resorted to prior knowledge to guide the training process. In PaNN [8], the average organ size distributions on the PLDs were constrained to resemble the prior statistics obtained from the FLD. Another method was introduced in [9] where the non-overlapping characteristics between different organs were exploited to design the exclusion loss.Although witnessed great progress in PSMOS, existing methods are faced with the following challenges: 1) Shortage in sufficiently labeled samples for supervised learning, since voxel-level labels are only available for a subset of organs in PLDs; 2) Significant cross-site appearance variations caused by different imaging protocols or subject cohorts. Different from existing methods, we propose a novel framework to explicitly tackle the above-mentioned challenges.To handle the label-scarcity problem in PLDs, we propose a novel Affinityaware Consistency Learning (ACL) scheme to incorporate voxel-to-organ affinity in the embedding space into consistency learning. Although consistency learning is frequently used for leveraging unlabeled data in label-efficient learning [10][11][12], it is mostly deployed in the label space [13][14][15], while little attention has been paid to exploring consistency in the latent feature space. Zheng et al. [16] proposed to adopt auxiliary student-teacher networks to utilize the features for consistency learning, which introduced more parameters, thus were computationally expensive. By incorporating voxel-to-organ affinity in the embedding space into consistency learning, our ACL scheme is plug-and-play and can capture rich context information in the embedding space.To tackle the data discrepancy problem [17], based on the assumption that a well trained joint model should generate consistent feature distributions across different sites, we propose a novel Cross-Site Feature Alignment (CSFA) module, where two terms are introduced to attend to both the organ-specific and interorgan statistics in the latent feature space. Concretely, for each PLD, we restrain the organ-specific prototypes calculated in each mini-batch to be close to the corresponding prototypes generated on the small-sized FLD. To further reduce the data discrepancy problem, we constrain the affinity relationships across different organ-specific prototypes to be consistent among different sites. By doing this, we transfer not only the single-class centroid, but also the inter-organ affinity learned from the small-sized FLD to PLDs, allowing for knowledge propagation at multiple granularity levels. Our contributions can be summarized as follows:-We propose a novel affinity-aware consistency learning scheme to incorporate voxel-to-organ affinity in the embedding space into a consistency learning framework, which can capture semantic context in the latent feature space. -We design a novel cross site feature alignment module to calibrate feature distributions of PLDs with distribution priors learned from a small-sized FLD, alleviating the cross-site data discrepancy. -We demonstrate on five datasets collected from different sites that our method can effectively learn a unified MOS model from multi-source datasets, achieving superior performance over the state-of-the-art (SOTA) methods. A schematic illustration of our framework. ""Aug"" refers to perturbations with data augmentations. In the CSFA module, hollow shapes refer to the features belonging to unlabeled organs in the PLDs, while solid ones refer to labeled organs. The affinity matrix is calculated according to Eq. 10 and Eq. 11. Lseg is the segmentation loss."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.0,Methodology,"To learn a unified model from a small-sized FLD and a number of PLDs, we propose a novel framework to address the issues of label-scarcity and cross-site data discrepancy. The overall workflow of our method is presented in Fig. 1.During training, in each batch, we sample 3D patches from both the FLD and one of the PLDs, where the teacher-student scheme [14] is adopted to impose consistency constraints on the unlabeled voxels of the PLD. In our method, apart from the label space consistency, we introduce the ACL scheme to explore consistency in the embedding space. We further leverage the CSFA module to perform feature alignment between the FLD and the PLD. Please note that consistency constraints are only imposed on the unlabeled voxels of PLDs. The label space consistency loss is omitted in Fig. 1 for brevity."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.1,Preliminaries,"Denote Y full as the full label set, i.e., Y full = {0, 1, 2, • • • , C}, where 0 refers to the background class, and {1, • • • , C} are one-to-one mappings to the target organs, C is the number of target organs. Given a small-sized FLD D f and a number of PLDswhere N is the number of PLDs. Each dataset can then be formally defined as either, where I f j,i is the i-th pixel of the j-th image in the FLD D f , and y f j,i is its corresponding label. Similarly, (I n j,i , y n j,i ) is the i-th pixel-label pair of the j-th image in the n-th PLD D n p . Please note that each D n p contains only a subset of the full label set, i.e., Y n p = unique({y n j,i }) Y full , where unique(•) returns the unique values in the label set. The task of PSMOS aims to learn the mapping function ϕ = f • g to project the 3D image patch I j ∈ R h×w×z to its corresponding semantic labels, where f is the feature extractor, g is the segmentation head, and • means sequentially executing f and g, (h, w, z) are the 3D patch size. Since foreground organ in one PLD may be labeled as background in another dataset, such a background ambiguity brings challenges to joint training on multiple PLDs. To address this issue, we follow [7,9] to calculate the marginal cross entropy and marginal Dice loss as the baseline segmentation loss L seg ."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.2,Prototype Generation,"In our proposed framework, the calculation of both the pixel-to-prototype predictions (in ACL) and the feature alignment loss (in CSFA) are based on organspecific prototypes. In each mini-batch, denote the organ-specific prototypes for the FLD as {q c }, c ∈ {0, • • • , C} and prototypes for the n-th PLD as {q n c }, c ∈ {0, • • • , C}, then they are generated as follows. On the FLD, we generate the prototypes in an exponential moving average scheme. Specifically, the feature prototype of the t-th iteration is calculated as (for brevity, we omit the iteration superscript t),where q update c is the average feature of the c-th class in current mini-batch of the FLD and α is the weighting coefficient. Given the feature maps F = {f i } and their related labels {y i }, where f i represents the i-th pixel in the feature maps of current mini-batch, the feature center of the c-th class is then calculated as,where Z c is the number of pixels belonging to the c-th class in current mini-batch. On the n-th PLD, we directly adopt the feature centers calculated in each mini-batch as the organ-specific prototypes. In specific, for the labeled organs, the prototypes {q n c }, c ∈ Y n p are calculated according to Eq. 2, with feature maps generated on 3D patches from the n-th PLD. While on the unlabeled organs, only reliable features are used for calculating the pseudo feature centers as,where p n i is the normalized prediction score generated from the teacher model, y i denotes the corresponding pseudo label, τ is the confidence threshold, Z c is the number of reliable predictions in class c, and 1[•] returns 1 if the inside condition is True, otherwise, returns 0."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.3,Affinity-Aware Consistency Learning,"In this paper, we propose to incorporate the voxel-to-organ affinity into consistency learning. Specifically, instance-to-prototype matching is calculated to capture the voxel-to-organ affinity. The affinities are then transformed into normalized scores for calculating the consistency constraint on two perturbed inputs. We adopt the teacher-student scheme [14] for consistency learning on the unlabeled data. Formally, denote I t , I s as the perturbed versions of the same sampled 3D patch for the teacher branch and the student branch respectively. In the teacher branch, denote φ i = f tea (I t,i ) ∈ R d as the extracted feature for the i-th pixel of 3D image patch I t . Given the prototypes generated on the FLDwhere < •, • > calculates the cosine similarity between the two terms.Similarly, in the student branch, denote ψ i as the i-th featureSince p t,i , p s,i model the voxel-to-organ affinities in the embedding space, constraining consistency on them introduces rich context information for training on the unlabeled data, which is formulated as,where 1is the normalization factor to get the mean KL-Divergence in the feature embedding space. Denote ϕ tea = f tea • g tea , ϕ stu = f stu • g stu as the teacher and student segmentation model respectively, the logits from the student and the teacher branch can be calculated as l s,i = ϕ stu (I s,i ), l t,i = ϕ tea (I t,i ). Then the consistency loss in the label space is calculated as,where 1is the normalization factor. The overall affinity-aware consistency loss is finally formulated as,"
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.4,Cross-Site Feature Alignment (CSFA) Module,"The CSFA module is proposed to calibrate feature distributions across different sites. Specifically, given the learned prototypes from current mini-batch of the n-th PLD ({q n c }, c ∈ {0, • • • , C}), they can be regarded as the organ-specific cluster centers in the embedding space. Then, compactness loss is introduced to calibrate D n p with the cluster centers learned from the FLD as,where |Y n p | returns the number of labeled organs in D n p . To further take into consideration the inter-organ affinity relationships during feature distribution alignment, we first model inter-organ affinity relationships on the FLD by calculating the affinity matrix A = {a ij } ∈ R (C+1)×(C+1) as shown in Fig. 1,Similarly, we can obtain the affinity matrixThen the affinity relationship aware feature alignment loss is calculated as,where a c , a n p,c refer to the c-th row of A and A n p respectively. The overall cross-site alignment loss is then calculated as the sum of the compactness loss and the affinity relationship aware calibration loss,The overall training objective is finally formulated as,where λ a is the tradeoff parameter."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,3.0,Experiments and Results,"Datasets and Implementation Details. We use five abdominal CT datasets (MALBCVWC [1], Decathlon Spleen [3], KiTS [2], Decathlon Liver [3] and Decathlon Pancreas [3] datasets respectively) to evaluate the effectiveness of our method [1][2][3]. The spatial resolution of all these datasets are resampled to (1 × 1 × 3)mm 3 . We randomly split each dataset into training (60%), validation (20%) and testing (20%). We adopt 3D U-Net [18] as our backbone model. The patch size (h, w, z) is set to (160, 160, 96). The hyper-parameters α and τ are empirically set to 0.9, and 0.8, respectively. λ a is initialized as 0.01 and linearly decreased to 1e-3 at 20000 iterations. We use SGD optimizer to train the model and the initial learning rate is set to 0.01. We adopt Dice similarity coefficient (DSC) as metric to evaluate the performance of different methods.  1, the ""baseline+ACL"" setting reports the results with our proposed affinity-aware consistency learning scheme. Comparing to the baseline, it brings a 1.1% performance gain in terms of DSC. By introducing the CSFA module, the ""baseline+ACL+CSFA"" setting can further boost the performance by 0.5% in terms of DSC. We further study the effectiveness of the CSFA module in alleviating crosssite data discrepancy. Concretely, we measure the feature distribution discrepancy between the FLD and each PLD by calculating the Maximum Mean Discrepancy (MMD) using gaussian kernel [19], which was designed to quantify domain discrepancy. We conduct ""full vs partial"" MMD analysis on the following two settings: ""Ours w/CSFA"" and ""Ours wo/CSFA"", where ""Ours w/CSFA"" is the proposed framework, while ""Ours wo/CSFA"" setting refers to removing the CSFA module from our framework. In the MMD calculation, for each dataset, we first generate features from the penultimate layer. Then we randomly select 2000 features in each class for MMD calculation. Please note, for each PLD, we adopt the pseudo labels for feature selection. Detailed comparison results are illustrated in Table 2. As shown, by introducing the CSFA module, the feature distribution discrepancy in terms of MMD can be effectively alleviated across all the ""full vs partial"" dataset pairs.Comparison with the State-of-the-Art (SOTA) Methods. We compare with four SOTA methods, including PaNN [8], PIPO [7], Marginal Loss [9], and DoDNet [5]. For fair comparison, all the SOTA methods were trained/tested on our own dataset splits. We also implemented our method taking the nnUNet as the backbone to compare with Marginal Loss [9] and PaNN [8]. We reported the DSC values for each organ across test sets from all the datasets. For a straightforward comparison with the SOTA, we also recorded the average DSC over all the organs. Detailed results are illustrated in Table 3. As shown, our method achieves the best performance. Specifically, our method outperforms the secondbest method PaNN [8] with a 1.2% DSC gain using the same nnUNet backbone. And our method when taking 3D-UNet as the backbone also outperforms the listed SOTA methods. We further conduct paired t-test to compare the difference between ours and other SOTA methods, the p-values are 2E-8 (PIPO), 2E-5 (DoDNet), 2E-4 (Marginal Loss), 0.037 (PaNN), respectively. As all p-values are smaller than 0.05, the differences between ours and other SOTA methods are statistically significant. In practice, some organs are much harder to be well-segmented than others due to their relatively small organ sizes. Therefore, we pay more attention to the performance on those hard organs (in our datasets, Pancreas and Kidneys are deemed to be more difficult due to their relatively small sizes). From the last column of Table 3, we can see that the segmentation performance gains of our method are more pronounced on hard organs (on average a 1.8% DSC gain). Figure 2 demonstrates the qualitative visualization results on some hard samples. As shown in this figure, our method can generate better segmentation results than other SOTA methods. Besides, the reasonable performance on segmenting kidney with tumors (row 2 in Fig. 2) makes our method promising in clinical practice."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,4.0,Conclusion,"In this paper, we designed a novel Affinity-aware Consistency Learning scheme (ACL) to model voxel-to-organ affinity context in the feature embedding space into consistency learning. Meanwhile, the CSFA module was designed to perform feature distribution alignment across different sites, where both organ-specific cluster centers and the inter-organ affinity relationships were propagated from the small-sized FLD to PLDs for cross-site feature alignment. Extensive ablation studies validated effectiveness of each component in our method. Quantitative and Qualitative comparison results with other SOTA methods demonstrated superior performance of our method."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,,Table 3 .,Fig. 2. 3D visualized results of some hard samples.
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,1.0,Introduction,"There is a growing interest in leveraging brain imaging data to predict non-brain-imaging phenotypes in individual participants, since brain functional activity could intrinsically serve as an ""objective"" observer of a subject given that the emergence of behavior and cognition were widely attributed to the orchestration of local and remote cortical areas by means of a densely connected brain network [1]. In this context, the movie-watching paradigm has been widely demonstrated to provide richer life-like scenarios [2][3][4][5], better subject compliance in contrast to rest and task states, and were suggested to be better at predicting emotional/cognitive reaction [6][7][8][9] that could be more easily and saliently evoked by naturalistic input load, making movie watching the upper bound of current paradigms.However, in recent works [8,10,11], the individual trait prediction from moviewatching paradigm fMRI (mfMRI) induced brain activity/functional connectivity can only achieve an accuracy around 0.40 (Pearson/Spearman's r). The accuracy drops even dramatically when dataset size is small. Regarding the reality that it is a challenging task to increase mfMRI data size, at least two strategies can be intuitively proposed to improve the performance on the basis of limited number of subjects: 1) Incorporation of other data modalities, such as behavior. Conceptually, a joint use of what a subject ""thinks"" and what the subject ""reacts"" to a stimulus could help to increase the accuracy of prediction of ""who"" he/she is. Eye movement behaviors [12], as one example, have been related to subjects' cognitive and phenotypical measures [12][13][14], and might supplement the fMRI derived brain activities in monitoring subjects' attention and task compliance and observing subjects' subconscious traits [15]; 2) Increase the number of different video clips watched by the same group of subjects.Integration of multimodal data has been realized by using a graph to present the relation between subjects, where subjects are defined as nodes, node feature is fMRI connectivity, and edge is estimated by thresholding the similarity of behaviors, including eye movement [16]. This graph convolution networks could realize an embedding of a cohort of subjects' brain activity features according to their behavior similarity, and estimate a mapping of these embedded features to cognitive scores, and further propagate the mapping to other nodes as a prediction of their scores. However, behavior in this model only provides the topology of the graph but are not fully involved in the process of embeddings. Also, different definitions of edges, such as eye trajectory and pupil size in this work, yield a set of graphs with different topologies on the same set of nodes. Therefore, we aim to solve the following two technique problems: 1) how to integrate the different edge features to node ones and learn graph embedding for both node and edge; and 2) how to integrate the embeddings of heterogeneous graphs with the same set of nodes to fulfill classification or regression. Based on Convolution with Edge-Node Switching graph neural network (CensNet) [17], we proposed Attention-CensNet (A-CensNet for short), where subjects are nodes, with the mfMRI derived functional connectivity as nodal features. Eye tracking derived gaze trajectory and temporal pupil size variation were respectively used to measure the similarity between subjects and to construct a set of heterogenous edges. Each of these heterogenous graphs was taken as an independent channel, where CensNet was used to alternatively learn both node embeddings and edge embeddings. Then, Squeeze-and-Excitation attention module (SENet) [18] was used to integrate the node-edge embeddings from multiple channels into one hybrid graph on which the final round of node embedding was performed. Note that mfMRI and eye tracking data from the same cohort exposed to different movies inputs also yield additional channels in this work.In the following sections, we firstly introduce the dataset and preprocessing steps. Basics of CensNet and SENet, and proposal of A-CensNet are introduced followed by its application to our task. Comparative and ablation studies regarding to prediction accuracy (AUC) were presented in the Results to demonstrate the effectiveness of multiple channel integration strategy, and the better performance of A-CensNet than others."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.1,Dataset,"In the Human Connectome Project (HCP) 7T release [19], movie-watching fMRI and resting-state fMRI data were acquired on a 7 T Siemens Magnetom scanner [20]. Among the four scan sessions (MOVIE 1~4), MOVIE 2&3 were used as a testbed in this work. Important imaging parameters are as follows: TR = 1000 ms, TE = 22.2 ms, flip angle = 45 deg, FOV = 208 × 208 mm, matrix = 130 × 130, spatial resolution = 1.6mm 3 , number of slices = 85, multiband factor = 5. During MOVIE 2 runs, 4 video clips (818 time points, TRs), separated by five 20s rest sessions (100 time points in total), were presented to subjects. In MOVIE 3 runs, 5 video clips (789 time points, TRs), separated by five 20s rest sessions (120 time points in total), were presented to subjects. Eye tracking data were acquired during MOVIE runs using an EyeLink S1000 system with a sampling rate of 1000 Hz. HCP provides many phenotypic measures from a variety of domains. As suggested by [8], we focus on measures in the cognition domain in this work. After a quality control of data modalities, 81 subjects are selected."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.2,Preprocessing,"FMRI data have been preprocessed by the minimal preprocessing pipeline for the Human Connectome Project [20,21]. The signals were mapped to the grayordinate system, which includes 64k vertices on the reconstructed cortical surface plus 30k subcortical voxels for an individual. The within-subject cross-modal registration and cross-subject registration are adopted to warp the grayordinate vertices and voxels to the same space, such that the associated fMRI signals have cross-subject correspondence.In this study, subcortical regions are not our major interest and not included. Destrieux atlas [22] is applied to cortical surfaces to yield 75 cortical areas on each hemisphere. We compute the mean fMRI signal averaged over vertices within a cortical area, and construct a 150-by-150 functional connectivity matrix, by means of Pearson correlation between these average signals (blue panel in Fig. 1(a)). We zero the negative correlation and 90% of the lowest positive correlation. The upper triangular matrix is converted to a vector and used as the functional feature.For eye tracking data, time stamps are used to extract the effective data points and synchronize the eye behavior features across subjects. Blink session is not considered. Since many phenotypic measures within a domain could be correlated with one another, we perform principal components analysis (PCA) to measures classified in ""cognition"" domains [8]. The first principal component is used to classify the subjects to four groups by their scores. Subject number balance among groups is considered. Each group is assigned a label l ∈ L."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.3,Classification of Population via Attention-CensNet,"Basics of CensNet. Supposing we have a dataset of M subjects, our objective is to assign each subject a cognitive group label l. We construct a graph G = {V, E, A} to represent the entire cohort as shown in Fig. 1(a), where v ∈ V is a node of the graph, the subjects in this work. Edges E s as well as the adjacent matrix A encode the similarity between subjects. The implementation of CensNet was detailed elsewhere [17], and we provide a summarized version as follows:For spectral graph convolution, normalized graph Laplacian of a graph G = {V, E, A} is computed: L = I N -D -1/2 AD -1/2 where I N is the identity matrix and D is the diagonal degree matrix. One of the important steps is the layer-wise propagation rule based on an approximated graph spectral kernel as follows:where Ã = A + I N and D is the degree matrix, H l and W l are the hidden feature matrix and learnable weight of the l th layer. On this basis, the CensNet is proposed to have both node and edge convolution layers. For convenience, the graph (white box) in Fig. 1 (a) consists of a node-center version (yellow box) and an edge-center one (green box). In the node convolution layer, the embedding of nodes in the white box is updated, while the edge adjacency matrix and edge features in the green box are intact. Then, a similar update is implemented in the edge layer. Such a node-edge switching is implemented by the following equations:wheree , T ∈ R Nv×Ne is a binary matrix that indicates whether an edge connects a node. P e is a learnable weight vector, denotes the diagonalization operation. denotes the element-wise product. The loss function is defined as:where Y L is the subset of nodes with labels, M is the softmax results of the last node layer where node feature map has F dimensions.Squeeze-and-Excitation Attention Block. The SENet [18] is introduced to integrate multiple graphs that share the same nodes but have different node features and edges. Note that a typical CensNet includes 1) the node-and-edge switching embedding plus 2) an additional round of node embedding (for node classification). The SENet is inserted between 1) and 2) (lower panel in Fig. 1(a)). The process (node as same as edge) is expressed by:It is important to note that previous works [9] adopted regression scheme to predict cognitive scores, and the prediction accuracy was quantified by Pearson/Spearman correlation. However, the small dataset size significantly reduces the prediction accuracy [9], especially when sample size is below 100. We followed the suggestion in [16] to use the classification strategy instead and the accuracy of class label prediction was quantified by the AUC. Accordingly, the regression layer of some of state-of-the-art methods [8,9,16,23] are replaced by classification one. We set the dimension of the final output to 4 (the number of classes) and pass it through logsoftmax function. The loss function and the AUC calculation is the same as that used in A-CensNet, while keeping the other layers by their default configuration."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.1,Implementation Details,"It was demonstrated in He et al., 2020 that the behavior score prediction accuracy via regression drops dramatically when subject number is below 100 (no more than r = 0.1 via Spearman correlation). Since we only have 81 subjects, a low regression accuracy cannot be a trustworthy to be used to compare with state-of-the-arts. As a compromise solution, we adopted classification scheme to demonstrate the effectiveness of our proposed framework.In our application, subjects are divided to four cognitive groups (around 20 subjects in each one, a total of 81 subjects). Then, we randomly split the dataset to training, validation and testing sets, respectively. We randomly selected 10 subjects from each group, a total of 40 subjects (about 50% of the total) to form the training set. Among the remaining 41 people, we randomly selected 20 people to be the verification group and 21 people to be the testing group (each accounting for 25% of the total). Such a random division of training/validation/testing subsets was repeated 100 times, independently. The results (AUCs) were the average of 100 independent replicates.We experiment on preserving {10%, 15%, 20%} top graph and edges by their weights, and find that preserving 10% node feature and 10% edges yields the best prediction performance. We try different settings of learning rate from {0.05, 0.01, 0.005, 0.001}, dropout {0.2, 0.3, 0.4, 0.5}, hidden {16, 32, 64, 128, 512, 1024}, and found that the best performance is yielded by learning rate to 0.005, dropout to 0.2, hidden to 1024. We implement the A-CensNet structure by adding the Attention mechanism based on CensNet, which empirically produce the best performance. AUC is adopted for each independent replication experiment to evaluate the prediction performance."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.2,Ablation Study,"The prediction accuracy measured by AUC of 100 repeated experiments are reported in Table 1. As a comparison, attention module is removed, as shown in the ""Attention-No"" rows in Table 1. In our method, the attention block is added after the node-edge updates in each channel (see Fig. 1(a)). The results are reported in ""Attention-Middle"" section. We also move the attention module ahead of CensNet. The results are reported in ""Attention-Before"" section. It is noted that data from different datasets are taken as different channels for attention module. To evaluate whether multiple stimulus loads to the same subjects enhance the performance, we only use two channels (trajectory and pupil variation as two sets of edges) within a single movie. The results are in the first row in ""Attention-Top"" and ""Attention-Middle"" sections. We use the format {Node, Edge} to describe the graph structure.In the non-attention algorithms, CensNet on movie2 dataset ({mfMRI2, Pupil2}) yields the best performance. Concatenation of node feature or edge feature from two movie datasets even decreases the accuracy. When attention module is added (Attention-Middle), two-channel models (gray rows) do not significantly increase the accuracy. Within one movie dataset, when node feature is fixed and eye trajectory and pupil size variation are taken as two channels, the performance is not better than that on single-channel model on movie3 dataset. When two movie datasets are considered (car-neose&green rows), they are integrated by means of channels when models with attention are used. In contrast, in non-attention models, the two movie datasets are integrated by means of feature concatenation. It is seen that the channel integration outperforms feature concatenation when pupil size is used as edges (green). But this does not hold when eye trajectory was used as edges (carneose). When both trajectory and pupil size are both considered as two channels, but features from two dataset are concatenated (white rows), the prediction performance is worst (46.21 ± 0.55) in all attention models. Finally, when both trajectory and pupil size from two dataset are used as four channels (blue row) in our algorithm, the best performance was yielded.These comparisons suggest that integration of edge features by channel attention does not always improves the performance than single use of an edge (gray rows), while integration of datasets by channel attention significantly outperforms the integration by feature concatenation. This observation still holds for ""Attention-Before"" models. But their accuracy is not as high as that via ""Attention-Middle"" models (see the crosscomparison between blue rows and gray rows), suggesting that a node-edge embedding could yield latent features more sensitive to individual variations, such that a channel attention works better at this deep feature space than being applied immediately after the original shallow features (""Attention-Before""). Finally, in addition to the 4-group classification, we evenly divided the subjects into 6 and 8 groups, respectively. AUCs via our model are 56.34 ± 0.65 and 56.95 ± 0.48, demonstrating the robustness of the algorithm to class numbers."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.3,Comparison with State-of-the-Arts,"We compare our results with the ones by state-of-the-art methods listed in Table 2. The results via linear model and GCN were applied on Movie2 and have been reported in [16], and is thus listed for reference. Note that the linear, FNN and BrainNetCNN are not GNN-related methods. They have no edges but only rely on mfMRI features. "
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,,Models,"Data AUC Edge Node Linear [8]  -mfM RI2 41.94±0.81 GCN [16]  Trj2 mfM RI2 48.51±0.94 CensNet [17]  mfM RI2 49.75±0.65CensNet [17]  Trj2 mfM RI2 50.36±0.71 Ppl2 52.91±0.73 FNN [9]  - All nonlinear deep neural networks yield significant improvement in contrast to the linear method (41.94 ± 0.81). A concatenation of mfMRI features (mfMRI2 +3 rows) does not improve the prediction accuracy in contrast to that on a single dataset, suggesting the importance of the strategy selection for concatenating features from multiple datasets. Within a single movie dataset (unshaded rows), models integrating mfMRI and eye tracking outperform the models (FNN and BrainNetCNN) that used single modality (mfMRI). After integrating mfMRI and eye behavior from multiple datasets, our results outperform all the-state-of-arts. These results demonstrate the effectiveness of integration of brain activity and eye behavior to one framework in cognition prediction. Also, given the limited subjects, multiple loads of stimuli integrated via attention modules could significantly improve the prediction performance."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,4.0,Conclusion,"We propose A-CensNet to predict subjects' cognitive scores, with subjects taken as nodes, mfMRI derived functional connectivity as node feature, different eye tracking features are used to compute similarity between subjects to construct heterogeneous graph edges. These graphs from different dataset are all taken as different channels. The proposed model integrates graph embeddings from multiple channels into one. This model outperforms the one using single modality, single channel and state-of-the-art methods. Our results suggest that the brain functional activity patterns and the behavior patterns might complement each other in interpreting trait-like phenotypes, and might provide new clues to studies of diseases with cognitive abnormality [24]."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"Globally, cancer is a leading cause of death and the burden of cancer incidence and mortality is rapidly growing [1]. In cancer diagnosis, treatment, and management, pathologydriven information plays a pivotal role. Cancer grade is, in particular, one of the major factors that determine the treatment options and life expectancy. However, the current pathology workflow is sub-optimal and low-throughput since it is, by and large, manually conducted, and the large volume of workloads can result in dysfunction or errors in cancer grading, which have an adversarial effect on patient care and safety [2]. Therefore, there is a high demand to automate and expedite the current pathology workflow and to improve the overall accuracy and robustness of cancer grading.Recently, many computational tools have shown to be effective in analyzing pathology images [3]. These are mainly built based upon deep convolutional neural networks (DCNNs). For instance, [4] used DCCNs for prostate cancer detection and grading, [5] classified gliomas into three different cancer grades, and [6] utilized an ensemble of DCNNs for breast cancer classification. To further improve the efficiency and effectiveness of DCNNs in pathology image analysis, advanced methods that are tailored to pathology images have been proposed. For example, [7] proposed to incorporate both local and global contexts through the aggregation learning of multiple context blocks for colorectal cancer classification; [8] extracted and utilized multi-scale patterns for cancer grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer classification in pathology images as both categorical and ordinal classification problems. Built based upon a shared feature extractor, a categorical classification branch, and an ordinal classification branch, it simultaneously conducts both categorical and ordinal learning for colorectal and prostate cancer grading; a hybrid method that combines DCCNs with hand-crafted features was developed for mitosis detection in breast cancer [10]. Moreover, attention mechanisms have been utilized for an improved pathology image analysis. For instance, [11] proposed a two-step framework for glioma sub-type classification in the brain, which consists of a contrastive learning framework for robust feature extractor training and a sparse-attention block for meaningful multiple instance feature aggregation. Such attention mechanisms have been usually utilized in a multiple instance learning framework or as self-attention for feature representations. To the best of our knowledge, attention mechanisms have not been used for feature representations of class centroids.In this study, we propose a centroid-aware feature recalibration network (CaFeNet) for accurate and robust cancer grading in pathology images. CaFeNet is built based upon three major components: 1) a feature extractor, 2) a centroid update (Cup) module, and 3) a centroid-aware feature recalibration (CaFe) module. The feature extractor is utilized to obtain the feature representation of pathology images. Cup module obtains and updates the centroids of class labels, i.e., cancer grades. CaFe module adjusts the input embedding vectors with respect to the class centroids (i.e., training data distribution). Assuming that the classes are well separated in the feature space, the centroid embedding vectors can serve as reference points to represent the data distribution of the training data. This indicates that the centroid embedding vectors can be used to recalibrate the input embedding vectors of pathology images. During inference, we fix the centroid embedding vectors so that the recalibrated embedding vectors do not vary much compared to the input embedding vectors even though the data distribution substantially changes, leading to improved stability and robustness of the feature representation. In this manner, the feature representations of the input pathology images are re-calibrated and stabilized for a reliable cancer classification. The experimental results demonstrate that CaFeNet achieves the state-of-the-art cancer grading performance in colorectal cancer grading datasets. The source code of CaFeNet is available at https://github.com/col in19950703/CaFeNet."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.0,Methodology,The overview of the proposed CaFeNet is illustrated in Fig. 1. CaFeNet employs a deep convolutional neural network as a feature extractor and an attention mechanism to produce robust feature representations of pathology images and conducts cancer grading with high accuracy. Algorithm 1 depicts the detailed algorithm of CaFeNet. 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.1,Centroid-Aware Feature Recalibration,"Let {x i , y i } N i=1 be a set of pairs of pathology images and ground truth labels where N is the number of pathology image-ground truth label pairs, x i ∈ R h×w×c is the i th pathology image, y i ∈ {C 1 , . . . , C M } represents the corresponding ground truth label. h, w, and c denote the height, width, and the number of channels, respectively. M is the cardinality of the class labels. Given x i , a deep neural network f maps x i into an embedding space, producing an embedding vector e i ∈ R d . The embedding vector e i is fed into 1) a centroid update (Cup) module and 2) a centroid-aware feature recalibration (CaFe) module. Cup module obtains and updates the centroid of the class label in the embedding space E C ∈ R M ×D . CaFe module adjusts the embedding vector e i in regard to the embedding vectors of the class centroids and produces a recalibrated embedding vector e R i . e i and e R i are concatenated together and is fed into a classification layer to conduct cancer grading. from the centroid embedding vectors E C by using a linear layer. Then, attention scores are computed via a dot product between Q E and K C followed by a softmax operation. Multiplying the attention scores by V C , we obtain the recalibrated feature representation E R for the input embedding vectors E. The process can be formulated as follows:Finally, CaFe concatenates E and E R and produces them as the output."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.2,Network Architecture,"We employ EfficientNet-B0 [12] as a backbone network. EfficientNet is designed to achieve the state-of-the-art accuracy on computer vision tasks while minimizing computational costs through a compound scaling method. EfficientNet-B0 is composed of one convolution layer and 16 stages of mobile inverted bottleneck blocks, of which each with a different number of layers and channels. Each mobile inverted bottleneck block comprises one pointwise convolution (1 × 1 convolution for the channel expansion), one depth-wise separable convolution with a kernel size of 3 or 5, and one project pointwise convolution (1 × 1 convolution for the channel reduction). 3 Experiments and Results"
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.1,Datasets,Two publicly available colorectal cancer datasets [9] were employed to evaluate the effectiveness of the proposed CaFeNet. 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.2,Comparative Experiments,"We conducted a series of comparative experiments to evaluate the effectiveness of CaFeNet for cancer grading, in comparison to several existing methods: 1) three DCNNbased models: ResNet [13], DenseNet [14], EfficientNet [12], 2) two metric learningbased models: triplet loss (Triplet) [15] and supervised contrastive loss (SC) [16], 3) two transformer-based models: vision transformer (ViT) [17] and swin transformer (Swin) [18], and 4) one (pathology) domain-specific model (M MAE-CE o ) [9], which demonstrates the state-of-the-art performance on the two colorectal cancer datasets under consideration. For Triplet and SC, EfficientNet was used as a backbone network. We trained CaFeNet and other competing networks on C Train and selected the best model usingC Validation . Then, the chosen model of each network was separately applied to C TestI andC TestII . The results of M MAE-CE o were obtained from the original literature."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.3,Implementation Details,"We initialized all models using the pre-trained weights on the ImageNet dataset, and then trained them using the Adam optimizer with default parameter values (β 1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs. We employed cosine anneal warm restart schedule with initial learning rates of 1.0 e -3 , η min = 1.0 e -3 , and T 0 = 20. After data augmentation, all patches, except for those used in ViT [17] and Swin [18] models, were resized to 512 × 512 pixels. For ViT and Swin, the patches were resized to 384 × 384 pixels. We implemented all models using the PyTorch platform and trained on a workstation equipped with two RTX 3090 GPUs. To increase the variability of the dataset during the training phase, we applied several data augmentation techniques, including affine transformation, random horizontal and vertical flip, image blurring, random Gaussian noise, dropout, random color saturation and contrast conversion, and random contrast transformations. All these techniques were implemented using the Aleju library (https:// github.com/aleju/imgaug). "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.4,Result and Discussions,"We evaluated the performance of colorectal cancer grading by the proposed CaFeNet and other competing models using five evaluation metrics, including accuracy (Acc), precision, recall, F1-score (F1), and quadratic weighted kappa (κ w ). Table 2 demonstrates the quantitative experimental results on C TestI . The results show that CaFeNet was one of the best performing models along with ResNet, Swin, and M MAE-CE o .Were the best performing models. Among DCNN-based models, ResNet was superior to other DCNN-based models. Metric learning was able to improve the classification performance. EffcientNet was the worst model among them, but with the help of triplet loss (Triplet) or supervised contrastive loss (SC), the overall performance increased by ≥2.8% Acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 F1, and ≥0.047 κ w . Among the transformer-based models, Swin was one of the best performing models, but ViT showed much lower performance in all evaluation metrics. Moreover, we applied the same models to C TestII to test the generalizability of the models. We note that C TestI originated from the same set with C Train and C Validation and C TestII was obtained from different time periods and using a different slide scanner. Table 3 depicts the quantitative classification results on C TestII . CaFeNet outperformed other competing models in all evaluation metrics except Triplet for recall. In a headto-head comparison of the classification results between C TestI and C TestII , there was a consistent performance drop in the proposed CaFeNet and other competing models. This is ascribable to the difference between the test datasets (C TestI and C TestII ) and the training and validation datasets (C Train and C Validation ). In regard to such differences, it is striking that the proposed CaFeNet achieved the best performance on C TestII . CaFeNet, ResNet, Swin, and M MAE-CE o were the four best performing models on C TestI . However, ResNet, Swin, and M MAE-CE o showed a higher performance drop in all evaluation metrics. CaFeNet had a minimal performance drop except EfficientNet. EfficientNet, however, obtained poorer performance on both C TestI and C TestII . These results suggest that CaFeNet has the better generalizability so as to well adapt to unseen histopathology image data.We conducted ablation experiments to investigate the effect of the CaFe module on cancer classification. The results are presented in Table 4. The exclusion of the CaFe module, i.e., EfficientNet, resulted in much worse performance than CaFeNet. Using only the recalibrated embedding vectors E R , a substantial drop in performance was observed. These two results indicate that the recalibrated embedding vectors complement to the input embedding vectors E. Moreover, we examined the effect of the method that merges the two embedding vectors. Using addition, instead of concatenation, there was a consistent performance drop, indicating that concatenation is the superior approach for combining the two embedding vectors together. In addition, we compared the model complexity of the proposed CaFeNet and other competing models. Table 5 demonstrates the number of parameters, floating point operations per second (FLOPs), and training and inference time (in milliseconds). The proposed CaFeNet was one of the models that require a relatively small number of parameters and FLOPs and a short amount of time during training and inference. DenseNet, EfficientNet, Triplet, SC, and M MAE-CE o contain the smaller number of parameters than that of CaFeNet, but these models show either the higher number of FLOPs or longer time during training and/or inference. Similar observations were made for ResNet, ViT, and Swin. These models require much larger number of parameters and FLOPs and longer training time. These results confirm that the proposed CaFeNet is computational efficient and it does not achieve its superior learning capability and generalizability at the expense of the model complexity."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,4.0,Conclusions,"Herein, we propose an attention mechanism-based deep neural network, called CaFeNet, for cancer classification in pathology images. The proposed approach proposes to improve the feature representation of deep neural networks by re-calibrating input embedding vectors via an attention mechanism in regard to the centroids of cancer grades. In the experiments on colorectal cancer datasets against several competing models, the proposed network demonstrated that it has a better learning capability as well as a generalizability in classifying pathology images into different cancer grades. However, the experiments were only conducted on two public colorectal cancer datasets from a single institute. Additional experiments need to be conducted to further verify the findings of our study. Therefore, future work will focus on validating the effectiveness of the proposed network for other types of cancers and tissues in pathology images."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,,Table 1,"shows the details of the datasets. Both datasets provide colorectal pathology images with ground truth labels for cancer grading. The ground labels are benign (BN), well-differentiated (WD) cancer, moderatelydifferentiated (MD) cancer, and poorly-differentiated (PD) cancer."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,1.0,Introduction,"Artificial intelligence (AI) research has evolved rapidly, and unprecedented breakthroughs have been made in many fields. Applications of AI products can be witnessed in our daily life, such as autonomous driving, computer-aided diagnosis, automatic voice customer service, etc. The development of AI is undoubtedly a revolution in the course of human history.The most effective and commonly used AI model is the one based on deep neural networks [1]. However, with continuous research being held in methodologies, DL models are becoming more and more complicated. Researchers found that the deeper and more complex DL models are, the better the performance they could achieve for the tasks that traditional AI algorithms could not work well. The complexity of DL models reduces interpretability and transparency substantially; therefore, the current DL models are ""black-box"" [2]. It is difficult to find how the model works in a way that humans can understand. Because of that, what researchers can do is only to prepare enough data and spend time training a model to obtain a high performance. Therefore, researchers or users can hardly find the reason why a DL model made a wrong decision.XAI is an old area in AI research, but was named relatively recently [3,4], focusing now on the explainability of DL models. The final goal of XAI is to develop methods for revealing a basis for the decision made by a DL model and how the decision was made by the model to let users understand and trust the decision and model. Many XAI methods have been proposed to explain a trained DL model (i.e., post-hoc methods). Representative XAI methods include class activation mapping (CAM) [5], grad-CAM, layer-wise relevance propagation (LRP) [6], DL important features (DeepLIFT) [7], local interpretable model-agnostic explanations (LIME) [8], and SHapley additive explanations (SHAP) [9]. These XAI methods offer post-hoc explanations that indicate which areas in a given input image the trained model focuses on and identify which areas in the image have a positive or negative impact on the model decision. In other words, those XAI methods are ""instance-based"" and limited to the visual explanation of model's attentions in a given input image (i.e., an instance). However, they do not offer explanations of the learned functions of the network.In this study, we developed and presented an original XAI approach that can reveal the learned functions of groups of neurons in a neural network, which we call ""functional explanations"" and define as explanations of the model behavior by a combination of functions, as opposed to the visualization of a pattern to which a neuron responds. To our knowledge, there is no XAI method that offers functional explanations. Thus, our method is a post-hoc method that offers both instance-based and model-based functional explanations. We applied our XAI method to an MTANN model to emphasize the explainability and trustability of the MTANN, so that users can trust the MTANN."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.1,MTANN Deep Learning,"In the field of image processing, supervised nonlinear filters and edge enhancers based on an artificial neural network (ANN) [10] have been investigated for the reduction of the quantum noise in angiograms and supervised semantic segmentation of the left ventricles in angiography [11], which are called neural filters and neural edge enhancers, respectively. By extending the neural filter and edge enhancer, massive-training artificial neural networks (MTANNs) have been developed to reduce false positives in the computerized detection of lung nodules in computed tomography (CT) [12]. The MTANNs have also shown promising performance in pattern recognition and classification tasks [13,14].An MTANN is a deep learning model consisting of linear-output artificial neural network regression model that directly operates on pixels in an input image, as shown in Fig. 1. A large number of patches are extracted from input images; and corresponding pixels at the same positions in desired output images, named as teaching images, are extracted for the MTANN to learn. This patch-based training leads to the fact that the MTANN can be trained with only a small number of input and teaching images. "
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.2,Sensitivity-Based Structure Optimization,"The numbers of hidden layers and their units in an MTANN model are adjustable hyperparameters. A relatively large structure is used to ensure that the model performs well on a specific task. A trained large model, however, may contain redundant units, and functions of neurons for the task would be ""distributed and diluted"" in many neurons in the model. This makes the analysis of the functions of neurons very difficult [15].To address this issue, we applied our sensitivity-based structure optimization algorithm [16] to a trained large MTANN model to ""consolidate"" the diluted functions of neurons in the MTANN model. With this algorithm, redundant hidden units of the model are gradually removed; and a compact model with equivalent performance is obtained. The algorithm is described as the following steps: Train on until the loss value converges the loss value of on other necessary evaluation metrics of on (like PSNR, dice coefficient, etc., which depend on the task) (Initialize the maximum loss value after removing a hidden unit from , and the loss value is supposed to be between 0 and 1) (Initialize the index of the hidden layer where the hidden unit belongs)(Initialize the index of the hidden layer until in the -th hidden layer)for in do (Go through each hidden layer) if do (This layer has only one unit which cannot be removed) Skip to the next iteration for in do (Go through each hidden unit in the -th hidden layer) Remove the -th hidden unit in the -th hidden layer from temporarily the loss value of on if doPut the -th hidden unit in the -th hidden layer back to ← (Copy current model's weights and structure) Remove the -th hidden unit in the -th hidden layer from permanently return With the proposed optimization algorithm, the hidden units of MTANN could be gradually removed until the performance drops greatly when any of the rest unit is deleted."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.3,Calculation of Weighted Function Maps,"After applying the structure optimization algorithm, every hidden unit in the compact model is expected to have an essential function for the target task. To understand the functions of the hidden units, function maps were obtained by performing the MTANN convolution of a hidden unit over a given input image. For better discrimination between enhancement and suppression, the function maps were normalized and then multiplied by the sign of the weight between the hidden units and the output unit. Weighted function maps were finally generated by shifting the range of the function map by 0.5. Namely, for a given hidden unit, in the weighted function map, a pixel value >0.5 means enhancement of patterns in the input image, whereas a pixel value <0.5 means suppression."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.4,Unsupervised Hierarchical Clustering,"To group similar functions of the hidden units of the MTANN, we applied an unsupervised hierarchical clustering algorithm [17] to the weighted functional visualization maps. With this algorithm, the hidden units were automatically divided into several groups based on the following distance function between the weighted function maps of the hidden units:where SSIM is the structural similarity index, and NMRSE is the normalized root mean square error. With the unsupervised hierarchical clustering algorithm, we visualize the function maps of the hidden units group by group to explain the behavior of each group of the hidden units."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"Our XAI technique was applied to explain the MTANN model's decision in a liver tumor segmentation task [20]. Dynamic contrast-enhanced liver CT scans consisting of 42 patients with 194 liver tumors in the portal venous phase from the LiTS database [21] were used in this study. Each slice of the CT volumes in the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of 0.60-1.00 mm and thicknesses of 0.20-0.70 mm. The dataset consists of the original hepatic CT image with the liver mask and the ""gold-standard"" liver tumor region manually segmented by a radiologist, as illustrated in Fig. 2. Firstly, to have the same physical scale on spatial coordinates, bicubic interpolation was applied on the original hepatic CT images together with the corresponding liver mask and ""gold-standard"" tumor segmentation to obtain isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . Then, to unify the image size into the same size, the isotropic image was cropped to obtain the liver region volume of interest (VOI) with an in-plane matrix size of 512 × 512. An anisotropic diffusion filter was applied to reduce the quantum noise, which could substantially reduce the noise while major structures such as tumors and vessels maintained [22]. Finally, a Z-score normalization was applied to unify complex histograms of tumors in different cases. The final pre-processed CT images were used as the input images.In addition, since most liver tumors' shape is ellipsoidal, the liver tumors can also be enhanced by the Hessian-based method and utilized in the model to improve the performance [23,24]. Hence, the model consisted of these two input channels: segmented liver CT image and its Hessian-enhanced image. Also, the patches were extracted from input images from both channels: a 5 × 5 × 5 sized patch in the same spatial position was extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.Seven cases and 24 cases in the dynamic contrast-enhanced CT scans dataset were used for training and testing, respectively. 10,000 patches were randomly selected from the liver mask region in each case, summing up to a total of 70,000 training samples for training. The number of input units in the MTANN model with one hidden layer was 250. The structure optimization process started with 80 hidden units in the hidden layer. The binary cross-entropy (BCE) loss function was used to train the model. The MTANN model classified the input patches into tumor or non-tumor classes, and the output pixels represented the probability of being a tumor class. During the structure optimization process, the F1 score on the training patches and the Dice coefficient on the training images were also calculated as the reference to select a suitable compact model that performed equivalently to the original large model.As observed in the four evaluation metric curves in Fig. 3, as the number of hidden units was reduced from 80 to 9, the performance of the model fluctuated up and down, and after it was reduced below 9, the performance of the model dropped dramatically. Therefore, we chose a number of hidden units of 9 as the optimized structure.Then, we applied the unsupervised hierarchical clustering algorithm to the weighted function maps from the optimized compact model with 9 hidden units. Figure 4 shows that the 9 hidden units are clearly divided into 3 different groups. We denote hidden units 3, 4, and 7 as group A, hidden units 2, 6, 1, and 8 as group B, and hidden units 0 and 5 as group C. The hidden units in the same group should have a similar function, and the function maps from each group should show the function of the group.  As illustrated in Fig. 5, the low-intensity areas in the function maps of hidden units 0 and 5 in group C match the high-intensity areas in the Hessian-enhanced input image, which means they suppress the high-intensity areas. Likewise, group A enhances the liver area, and group B suppresses the non-tumor area. We also understood that groups A and B worked together to enhance the tumor area, and group C suppressed the liver's boundary as well as reduced the false enhancements inside the liver. Thus, our XAI method was able to reveal the learned functions of groups of neurons in the neural network, which we call ""functional explanations"" and define as the explanations of the model behavior by a combination of functions. Our method is a post-hoc method that offers both instance-based and model-based functional explanations."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,4.0,Conclusion,"In this study, we proposed a novel XAI approach to explain the functions and behavior of an MTANN model for semantic segmentation of liver tumors in CT. Our structure optimization algorithm refined the structure and made every hidden unit in the model have a clear, meaningful function by removing redundant hidden units and ""condensing"" the functions into fewer hidden units, which solved the issue of unstable XAI results with conventional XAI methods. The unsupervised hierarchical clustering algorithm in our XAI approach grouped the hidden units with a similar function into one group so as to explain their functions by group. Through the experiments, we successfully proved that the MTANN model was explainable by functions."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,1.0,Introduction,"Vision Transformers (ViTs) are self-attention based neural networks that have achieved state-of-the-art performance on various medical imaging tasks [8,24,30]. Since ViTs are capable of encoding long range dependencies between input F. Almalik and N. Alkhunaizi-Equal contribution sequences [16], they are more robust against distribution shifts and are wellsuited for handling heterogeneous distributions [5]. However, training ViT models typically requires significantly more data than traditional Convolutional Neural Network (CNN) models [16], which limits their application in domains such as healthcare, where data scarcity is a challenge. One way to overcome this challenge is to train such models in a collaborative and distributed manner, where large amounts of data can be leveraged from different sites without the need for sharing private data [9,11]. Federated learning and split learning are two well-known approaches for collaborative model training.Federated Learning (FL) enables clients to collaboratively learn a global model by aggregating locally trained models [14]. Since this can be accomplished without sharing raw data, FL mitigates risks related to private data leakage. Several aggregation rules such as FedAvg [20] and FedProx [19] have been proposed for FL. However, it has been demonstrated that most FL algorithms are vulnerable to gradient inversion attacks [13], which dilute their privacy guarantees. In contrast, Split Learning (SL) divides a deep neural network into components with independently accessible parameters [10]. Since no participant in SL can access the complete model parameters, it has been claimed that SL offers better data confidentiality compared to FL. In particular, the U-shaped SL configuration, where each client has its own feature extraction head and a task-specific tail [27] can further improve client privacy, as it circumvents the need to share the data or labels. Recently, SL frameworks have been proposed for various medical applications such as tumor classification [3] and chext x-ray classification [23].Recent studies [21,22] have demonstrated that both FL and SL can be combined to effectively train ViTs. In [22], a framework called FeSTA was proposed for medical image classification. The FeSTA framework involves a hybrid ViT architecture with U-shaped SL configuration -each client has its own CNN head and a multilayer perceptron (MLP) tail, while the shared ViT body resides on a central server. This architecture can be trained using both SL and FL in a potentially task-agnostic fashion, leading to better performance compared to other distributed learning methods. The work in [21] focuses on privacy and incorporates differential privacy with mixed masked patches sent from the ViT on the server to the clients to prevent any potential data leakage.In this work, we build upon the FeSTA framework [22] for collaborative learning of ViT. Despite its success, FeSTA requires pretraining the ViT body on a large dataset prior to its utilization in the SL and FL training process. In the absence of pretraining, limited training data availability (a common problem in medical imaging) leads to severe overfitting and poor generalization. Furthermore, the FeSTA framework exploits only the final cls token produced by the ViT body and ignores all the other intermediate features of the ViT. It is well-known that intermediate features (referred to as patch tokens) also contain discriminative information that could be useful for the classification task [4].To overcome the above limitations, we propose a framework called Federated Split learning of Vision transformer with Block Sampling (FeSViBS). Our primary novelty is the introduction of a block sampling module, which randomly selects an intermediate transformer block for each client in each training round, extracts intermediate features, and distills these features into a pseudo cls token using a shared projection network. The proposed approach has two key benefits: (i) it effectively leverages intermediate ViT features, which are completely ignored in FeSTA, and (ii) sampling these intermediate features from different blocks, rather than relying solely on an individual block's features or the final cls token, serves as a feature augmentation strategy for the network, enhancing its generalization. The contributions of this work can be summarized as follows: i. We propose the FeSViBS framework, a novel federated and split learning framework that leverages the features learned by intermediate ViT blocks to enhance the performance of the collaborative system. ii. We introduce block sampling at the server level, which acts as a feature augmentation strategy for better generalization."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,2.0,Methodology,"We first describe the working of a typical split vision transformer before proceeding to describe FeSViBS., where N c is the number of training samples available at client c, x represents the input data, and y is the class label. Following [22], we assume U-shaped split learning setting, with each client having two local networks called head (H θc ) and tail (T ψc ), where θ c and ψ c are client-specific head and tail parameters, respectively. The server consists of a ViT body (B Φ ), which includes a stack of L transformer blocks denoted asHere, Φ l represents the parameters of the l th transformer block anddenotes the complete set of parameters of the transformer body.During training, the client performs a forward pass of the input data through the head to produce an embedding h c = H θc (x c ) ∈ R 768×M of its local data, which is typically organized as M patch tokens representing different patches of the input image. These embeddings (smashed representations) are then sent to the server. The ViT appends an additional token called the class token (cls ∈ R 768×1 ) and utilizes the self-attention mechanism to obtain a representation b c = B Φ (h c ) ∈ R 768×1 , which is typically the cls token resulting from the last transformer block. This cls token is returned to the client for further processing. The tail at each client projects the received class token representation b c into a class probability distribution to get the final prediction ŷc = T ψc (b c ). This marks the end of the forward pass. Subsequently, the backpropagation starts with computing loss c (y c , ŷc ), where c (.) represents the client's loss function between the true labels y c and predicted labels ŷc . The gradient of this loss is propagated back in the reverse order from the client's tail, server's body, to the client's head. We refer to this setting as Split Learning of Vision Transformer (SLViT), where each client optimizes the following objective in each round:In FeSTA [22], an additional federation step was introduced. After every few SL rounds, the local (client-specific) heads and tails are aggregated in a unifying round using FedAvg [20] to produce global parameters θ and ψ. Note that the above framework completely ignores all the intermediate features (patch tokens) extracted from various ViT blocks. In [4], it was demonstrated that these patch tokens are also discriminative and valuable for classification tasks. Hence, we aim to exploit these intermediate features to further enhance the performance."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,2.1,FeSViBS Framework,"The proposed FeSViBS method is illustrated in Fig. 1 and detailed in Algorithm 1. The working of the FeSViBS framework is very similar to FeSTA, except for one key difference. During the forward pass of SLViT and FeSTA, the server always returns the cls token from the last ViT block. In contrast, a FeSViBS server samples an intermediate block l ∈ {1, 2. . . . , L} for each client c in each round and extracts the intermediate features z c,l from the chosen l th block as follows:where z c,l ∈ R 768×M . The server then projects the extracted intermediate features into a lower dimension using a projection network R (shared across all blocks) to obtain the final representation b c,l = R π (z c,l ), where b c,l ∈ R 768×1 . This final representation b c,l can be considered as a pseudo class token and the role of the projection network is to distill the discriminative information contained in the intermediate features into this pseudo class token. The primary motivation for block sampling is to effectively leverage intermediate ViT features that are better at capturing local texture information (but are lost when only the final cls token is used). Stochasticity in the block selection serves as a feature augmentation strategy, thereby aiding the generalization performance.The architecture of the projection network is shown in Fig. 1 and it resembles a simple ResNet [12] block with skip connection. The pseudo class token is then sent to the client's tail to obtain the final prediction ŷc = T ψc (b c,l ) and complete the forward pass. Each client uses ŷc along with the true labels y c to compute the loss c (y c , ŷc ). The gradients of the client's tail are then calculated and sent back to the server, which then carries out the back-propagation through the projection network and relevant blocks of the ViT body (only those blocks involved in the corresponding forward pass). Next, the server sends the gradients back to the client to propagate them through the head and end the back-propagation step. Hence, the client's optimization problem is:In the FeSViBS framework, the heads and tails of all the clients are assumed to have the same network architecture. Within each collaboration round, all the clients perform the forward and backward passes. While the parameters of the relevant head and tail as well as the shared projection network are updated after every backward pass, the parameters of the ViT body are updated only at the end of a collaboration round after aggregating updates from all the clients. The above protocol until this step is referred to as SViBS, because there is still no federation of the heads and tails. Similar to FeSTA, we also perform aggregation of the local heads and tails periodically in unifying rounds, resulting in the final FeSViBS framework. While in SViBS, the clients can initialize their heads and tails independently, FeSviBS requires a common initialization by the server and sharing of aggregated head and tail parameters after a unifying round."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,3.0,Experimental Setup,"Datasets. We conduct our experiments on three medical imaging datasets. The first dataset is HAM10000 [26], a multi-class dataset comprising of 10, 015 dermoscopic images from diverse populations. HAM10000 includes (end if 21: end for categories of pigmented lesions; we randomly perform 80%/20% split for training and testing, respectively. The second dataset [2] termed ""BloodMNIST"" is a multi-class dataset consisting of 17, 092 blood cell images for 8 different imbalanced cell types. We followed [29] and split the dataset into 70% training, 10% validation, and 20% testing. Finally, the Fed-ISIC2019 dataset consists of 23, 247 dermoscopy images for 8 different melanoma classes. This dataset was prepared by FLamby [25] from the original ISIC2019 dataset [6,7,26] and the data was collected from 6 centers, with significant differences in population characteristics and acquisition systems, representing real-world domain shifts. We use 80%/20% split for training and testing, respectively. The training samples in all datasets are divided among 6 clients, whereas the testing set is shared among them all. The distribution of each dataset is depicted in Fig. 2. Note that Fed-ISIC2019 and BloodMNIST are non-IID, whereas HAM10000 is IID.Server's Network. For the server's body, we chose the ViT-B/16 model from timm library [28] which includes L = 12 transformer blocks, embedding dimension D = 768, 12 attention heads, and divides the input image into patches each of size 16 × 16 with M = 196 patches. We limit the block sampling to the first 6 ViT blocks. Additionally, the projection network has two convolution layers with a skip connection, which takes an input of dimension 768 × 196 and projects it into a lower dimension of 768. Clients' Networks. Each client has two main networks: head and tail. We followed timm library's implementation of Hybrid ViTs (h-ViT) to design each client's head, which is a ResNet-50 [12] with a convolution layer added to project the features extracted by ResNet-50 to a dimension of 768 × 196. The tail is a linear classifier. Also, we unify the clients' networks (head and tail) every 2 rounds using FedAvg. We conduct our experiments for 200 rounds with Adam optimizer [17], a learning rate of 1 × 10 -4 , and 32 batch size with a cross-entropy loss calculated at the tail. The code was implemented using PyTorch 1.10 and the models were trained using Nvidia A100 GPU with 40 GB memory."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,4.0,Results and Analysis,"Following [25], we used balanced accuracy in all experiments to evaluate the performance of the classification task across all datasets. This metric defines as the average recall on each class. In Table 1, we compare the performance of FeS-ViBS and SViBS frameworks with other SOTA methods. FeSViBS consistently outperforms other methods on the three datasets with both IID and non-IID settings. More specifically, for HAM10000 (IID), FeSViBS outperforms all other methods with a 4.4% gain in performance over FeSTA and approximately 11% over FedAvg and FedProx (μ = 0.006). In the non-IID settings with both Blood-MNIST and Fed-ISIC2019, FeSViBS maintains a high performance compared to other methods. Under extreme non-IID settings (Fed-ISIC2019), our approach demonstrated a performance improvement of 10.4% compared to FeSTA and 5.8% over FedAvg and FedProx, demonstrating the robustness of FeSViBS.  We investigate the impact of sampling intermediate blocks in SViBS, by analysing the individual performance of intermediate features from specific blocks during training. The results in Fig. 3 demonstrate that the majority of individual blocks outperform the vanilla split learning setting (SLViT), which is dependent on the cls token. On the other hand, SViBS shows dominant performance across datasets, where the sampling of ViT blocks provides augmented representations of the input images at different rounds and improves the generalizability. From Table 1, we also observe that the variance of the accuracy achieved by FeSViBS due to stochastic block sampling during inference is very low."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,5.0,Ablation Study,"Set of ViT Blocks. To study the impact of ViT blocks from which the intermediate features are sampled on the overall performance of FeSViBS, we carry out experiments choosing different sets of blocks. The results depicted in Fig. 4 (left) show consistent performance for different sets of blocks across different datasets. This indicates that implementing FeSViBS with the first 6 ViT blocks would reduce the computational cost without compromising performance.FeSViBS with Differential Privacy. Differential Privacy (DP) [1] is a widelyused approach that aims to improve the privacy of local client's data by adding noise. We conduct experiments where we add Gaussian noise to the client's head output (h c ). In such a scenario, DP makes it more challenging for a malicious/curious server to infer the client's input from the smashed representations. With different values, the results in Fig. 4 (right) show that FeSViBS maintains its performance even under a small value ( = 0.1), while also outperforming FeSTA under the same constraints.Number of Unifying Rounds. We investigated the impact of reducing communication rounds (unifying rounds) on FeSViBS performance. However, our results showed that performance was maintained even with decreasing the number of communication rounds.Computational and Communication Overhead. Except for MOON and SCAFFOLD, all methods in Table 1 share the same h-ViT architecture, resulting in similar computational costs. SViBS and FeSViBS require training an additional projection network but avoid needing a complete ViT forward/backward pass. Centralized and local training methods have no communication cost. For other methods, the communication cost per client per collaboration round: (i) FedAvg/FedProx: ∼ 97M, (ii) SLViT/SViBS: ∼ 197M values for HAM10000 dataset, and (iii) FeSTA/FeSViBS: ∼ 197M values +12M parameters per client per unifying round. Thus, the proposed method has a marginally higher communication overload than SL and twice the communication burden as FL."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,6.0,Conclusion and Future Directions,"We proposed a novel Federated Split Learning of Vision Transformer with Block Sampling (FeSViBS), which utilizes FL, SL and sampling of ViT blocks to enhance the performance of the collaborative system. We evaluate FeSViBS framework under IID and non-IID settings on three real-world medical imaging datasets and demonstrate consistent performance. In the future, we aim to (i) extend our work and evaluate the privacy of FeSViBS under the presence of malicious clients/server, (ii) evaluate FeSViBS in the context of natural images and (iii) extend the current framework to multi-task settings."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,1.0,Introduction,"The human brain is a complex and economically organized system, consisting of interconnected regions that form a hierarchical brain network [1]. Understanding the topology of these networks is crucial for gaining insight into brain function and behavior [2]. Like many other real networks, brain network exhibits characteristics of a small-world and free-scale organization, where there are a small number of hub nodes that are densely connected to other peripheral regions [3,4]. Recent studies show that hub nodes play a central role in adapting brain network connectivity to meet task demands [2,5]. It has been also observed that most neurodegenerative and neuropsychiatric diseases are associated with alterations in dynamic functional connectivity (dFC) that occur selectively on hub nodes [6][7][8][9]. Therefore, accurate identification of hub nodes from brain networks, particularly in a dynamic scenario, is essential for understanding brain development and the neurodegenerative process.In network science, hub nodes are often classified as provincial or connector hubs based on the information of network modules [4]. Provincial hubs have high connectivity within a single module, while connector hubs link multiple communities and play a more critical role in the overall network organization [4,10]. With the consensus that damage to connector hubs can result in a wider disruption of the brain than damage to provincial hubs [11][12][13], identifying connector hubs is location-wise and functionwise more important. Multiple hub identification methods have been proposed and can be grouped into univariate-sorting-based [4,14,15] and multivariate-learning-based approaches [16]. The former involves selecting the top nodes ranked by certain nodal centrality, while the latter jointly identifies a set of critical nodes by learning topological features represented by low-dimensional graph embeddings. Although various methods exist for identifying hub nodes in brain networks, there is a notable gap in effective approaches for dynamic scenarios. One common strategy is to treat each network as a separate static network and then use existing hub identification methods for each sliding window independently to obtain a set of spatiotemporal hub nodes, as illustrated in Fig. 1(a). Where a dynamic sequence of time-varying networks is generated applying a sliding window technique to BOLD signals in fMRI imaging data. While efficient, this approach lacks an in-depth exploration of how the topological evolution is linked to hub regions, and it often produces low-consistency hubs due to a low signal-to-noise ratio, as depicted in Fig. 1(a).To address these challenges, we propose a novel learning-based spatiotemporal hub identification method. Unlike existing approaches that treat each temporal network as an independent static network, our method jointly identifies a set of temporal hub nodes using a dynamic graph embedding. Specially, due to dynamic graph embedding vectors [17][18][19] are the underlying representation of time-varying brain network, we can easily cast the network-to-network evolution over time as a total variation of dynamic graph embedding with respect to time, where each temporal graph embedding are not independent. Furthermore, in Fig. 1(b), as each temporal graph embedding vector is an instance residing on a Grassmannian manifold [19], we leverage the Grassmannian manifold optimization scheme to learn the dynamic graph embeddings. Overall, our spatiotemporal hub identification method ensures the temporal consistency of hub nodes over time while aligning the temporal hubs with the dynamic connectivity evolution in real time. We evaluate our proposed method on both synthetic and real brain network data, and results show that it outperforms conventional approaches."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,2.1,Dynamic Graph Embedding Learning,"Dynamic Brain Network. Suppose we observe a time-varying brain network consisting of N brain regions, we define the network over time as a dynamic graph G = (V, E, T ), where V = {V(t)} t∈T is the node set over time, E = {E(t)} t∈T is a collection of edges over time, and T is the time span. For each temporal point t ∈ T = [0, T ], there is a graph snapshot G t of N nodes with the node-to-node connectivity degrees encoded in an N × N adjacency matrix, denoted asTemporal Hub Identification. Given a temporal network G t , our goal is to find K hubs in each temporal network. The locations of these temporal hubs are indexed by a binary diagonal matrix, where s i = 0 indicates the i th node is a hub, and s i = 1 otherwise. To achieve this, we require the latent temporal graph embedding F(t) ∈ R N ×P (P < N , F(t) T F(t) = I P×P , ) for each temporal network W(t) should yield a distinct separation between connector hub nodes and the peripheral nodes. To link the learning of F(t) with the optimization of hub selection indicator s(t), we adopt the following objective function: arg minwhereis the diagonal matrix. The trace norm,, measures the smoothness of graph embedding in the context of the network topology governed by L s (t), where F i (t) and F j (t) are the temporal embedding on nodes v i and v j . Suppose v i is the temporal hub node and v j is the linked non-hub node. Specifically, we want the distance term F i (t) -F j (t) 2  2 to be as large as possible to separate hub and peripheral nodes, while expecting that connector hubs, which have a higher connectivity degree than peripheral nodes, will have a large weight w ij . Thus, identifying hub is searching for a solution that excluding all K hub nodes (let s i = 0) from the objective function to minimize the trace norm.Physics-Based Network Evolution Model. Since a temporal network evolves from the previous temporal state, the physics network-to-network evolution model can be mathematically described as follows:where τ is the time interval and ∂F(t) ∂t denotes the network-to-network evolution rate. In the context of neurobiological signals, the evolution of network connectivity is a smooth process rather than a mutational change. This means that as the time interval between two consecutive network states approaches zero, the 2-norm difference between two consecutive networks (F(t) and F(t -τ )) approaches a finite value, i.e., which includes two terms: the identification of temporal hub sets S(t) over time using the learned dynamic graph embedding F(t), and the regularization term that enforces the smoothness of F(t) evolution over time. The scalar parameter α controls the trade-off between the two terms."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,2.2,Optimization on Grassmannian Manifold,"Equation ( 3) involves an integral-differential form with respect to (w.r.t) F(t) and is hard to solve directly. To address this issue, we divide the entire time series into M segments with an interval of t = T M , where lim M →∞ T M = 0. This allows us to decompose Eq. ( 3) into a linear discrete model based on the integration definition: arg minSince Eq. ( 4) is not a convex function, we adopt an alternative approach to jointly optimize S(t m ) and F(t m ) in the following.Optimizing Dynamic Graph Embedding on Grassmannian Manifold. According to [19], each temporal graph embedding F(t m ) can be represented as an instance on the Grassmannian manifold I(P, N ) ∈ R N ×P , which makes the classic Euclidean space unsuitable for measuring the variation of F(t m ). In the context of physics,signifies the evolving variation from the temporal state t m-1 to t m , see Fig. 1"
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,,"(b). In the Grassmannian manifold space, F(t m ) 2 2 can be accurately measured by the squared geodesic distance Log F(t m ) (F(t m-1 )) = P-tr(F(t m )F(t m ) T F(t m-1 )F(t m-1 ) T ).","Therefore, the optimization of Eq. ( 4) w.r.t F(t m ) is as follows: (5) where β = α t 2 is a scaler parameter. Following the optimization framework in [19], we calculate the Grassmannian gradient F(t m ) of each temporal F(t m ) by projecting the Euclidean gradient) onto the tangent space via the orthogonal projection [19]:Given F(t m ) , we update the modified F(t m ) using an exponential mapping operation [19]: (7) where U, , and V are derived from the compact singular value decomposition (SVD) of -Fm , i.e., -Fm = U V T . ε is a scalar parameter controlling the step size of optimization.Optimizing Temporal Hub Node Set. After updatingF(t m ), the energy function of Eq. ( 4) can be rearranged as Eq. ( 8) with S(t m ) = diag(s(t m )):whererepresents the distance between the temporal graph embedding F(t m ) at the i th and j th nodes. The optimal set of temporal hub nodes s at the t m temporal point can be achieved using the convex optimization scheme proposed in [16]."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.0,Experiments and Results,"We assess the performance of our spatiotemporal hub identification method on both simulated and real network data. Our proposed method, named Dynamic-Graph-Embedding-based method, not only learns the topological features of each temporal network but also jointly learns the evolving consistency pattern on the entire dynamic sequence. We compare our method with conventional approaches: the classic sortingbased hub identification method that uses nodal betweenness centrality [20] (referred to as Sorting-Betweenness-based method), and the multivariate-learning-based hub identification that learns the topological property independently for each temporal network ( Graph-Embedding-based method [16])."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.1,Accuracy and Robustness on Synthesized Network Data,"Data Preparation. A set of synthesized time-variant networks were generated by the folling steps: (1) initialize a network with a specified number of nodes and connections;(2) set an evolution ratio (updated/total) to simulate the gradual process of network evolution. This involves keeping a fixed proportion of connections in the final state, while updating the remaining connections to generate the evolved network. Figure 2 shows toy examples of the synthesized time-varying network. Robustness. To further quantify the robustness of our proposed spatiotemporal hub identification method, we varied the complexity of network evolution and the network topology by changing the evolution ratio and hub number. Each time-varying network underwent 50 evolutions. The performance was evaluated by changing the evolution ratio from 40% to 70% and increasing the hub number in each temporal network while fixing other variables. We repeated the process 50 times and reported the overlap ratio between the identified temporal hubs and the ground truth across the entire dynamic sequence. Figure 3 shows that our proposed method consistently outperformed conventional methods, demonstrating its superior reliability."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.2,Evaluation of Hub Identification on Real Brain Networks,"Data Preparation. A total of 125 subjects consisting of 63 normal control (NC) and 62 obsessive-compulsive disease (OCD) were selected from an obsessive-compulsive  Consistency of Hub Topology. We employed conventional engineering methods and our proposed spatiotemporal hub identification technique on each sliding window across subjects, as depicted in Fig. 4. The emerging evidence suggests that hubs remain stable even when switching brain states between tasks [5,11]. Thus, it is also reasonable for us to hypothesize that hubs remain stable underlying a resting-state environment. Our proposed Dynamic-Graph-Embedding-based method yielded the highest consistency of hub locations across temporal states (6 common hubs), followed by the Graph-Embeddingbased (5 common hubs) and Sorting-Betweenness-based methods (4 common hubs).To further quantify the similarity, we calculated the covariance of the count histogram between the last and current temporal states, and our method exhibited the highest similarity (Fig. 4(c)). Statistical Power of the Identified Hubs. As mounting evidence suggests that certain neurodegeneration and neuropsychiatric diseases selectively damage hub regions, we performed a two-sample t-test to assess the statistical power of the identified hub. We identified hubs for each subject at each temporal point and then voted out the eight most frequently selected nodes as the common hubs across the entire temporal series, as shown in Fig. 5. The significance was indicated by red stars. Our proposed spatiotemporal hub identification method yielded more hub nodes manifesting significant differences specific to OCD compared to the other two methods (Fig. 5(a))."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,4.0,Conclusion,This paper introduces a novel spatiotemporal hub identification method. Our approach integrates the evolution model of network connectivity to ensure the consistency of dynamic graph embedding over time. The results on both simulated and real data are promising and suggest the great potential for investigating the role of hubs in the evolution of both task-based and resting-state-based networks.
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"Liver cancer is one of the most deadly cancers and has the second highest fatality rate [17]. Focal liver lesions (FLLs) are the most common lesions found in liver cancer, yet FLLs are challenging to diagnose because they can be either benign lesions, such as focal nodular hyperplasia (FNH), hepatic abscess (HA), hepatic hemangioma (HH), and hepatic cyst (HC) or malignant tumors, such as intrahepatic cholangiocarcinoma (ICC), hepatic metastases (HM), and hepatocellular carcinoma (HCC). Accurate early diagnosis of FLLs is thus critical to increasing the 5-year survival rate, a task that remains challenging as of today. Dynamic contrast-enhanced CT is a common technique for liver cancer diagnosis, where four different phases of imaging, namely, non-contrast (NC), arterial (ART), portal venous (PV), and delayed (DL) provide complementary information about the liver. Different types of FLLs acquired in the four phases are shown in Fig. 1. With the development of deep learning, computer-aided liver lesion diagnosis has attracted much attention [5,8,16] in recent years. Romero et al. [16] presented an end-to-end framework based on Inception-V3 and InceptionResNet-V2 to discriminate liver lesions between cysts and malignant tumors. Heker et al. [8] combined liver segmentation and classification using transfer learning and joint learning to increase the performance of CNN. As a manner to elevate the accuracy of CNNs, Frid-Adar et al. [5] designed a GAN-based network to generate synthetic liver lesion images, improving the classification performance based on CNN. It is reported in many studies [9,18] that using multi-phase data, like most professionals do in practice, can help the network get a more accurate result, which also acts in liver lesion classification [15,23,24]. Yasaka et al. [24] proposed multi-channel CNN to extract features from multi-phase liver CT by concatenation. Roboh et al. [15] proposed an algorithm based on CNNs to handle 3D context in liver CTs and utilized clinical context to assist the classification. Xu et al. [23] constructed a knowledge-guided framework to integrate liver lesion features from three phases using self-attention and fused them with a cross-feature interaction module and a cross-lesion correlation module.A single-phase lesion annotation means the annotation of both lesion position and its class. In hospitals, collected multi-phase CTs are normally grouped by patients rather than lesions, which makes single-phase lesion annotation insufficient for feature fusion learning. However, the number of lesions inside a single patient can vary from one to dozens and they can be of different types in realistic cases. Multi-phase CTs are also not co-registered in most cases, therefore, it is necessary to make sure the lesions extracted from different phases are somehow aligned for feature fusion, which is called as multi-phase lesion annotation. Moreover, while most works have attached much importance to liver lesion segmentation [2], its outcome is usually organized at a single-phase level. Additional effort will be needed when consolidating segmentation and multi-phase classification.Self-attention based transformers [19] have shown strong capability in natural language processing tasks. Meanwhile, vision transformers (ViT) [4] have been shown to replace CNN with a transformer encoder in computer vision tasks and can achieve obvious advantages on large-scale datasets. To the best of our knowledge, we find no study using ViT backbone network in liver lesion classification. The reason for this is twofold. First, pure ViT has several limitations itself [6], including ignoring local information within each patch, extracting only single-scale features, and lacking inductive bias. Second, no complete open liver lesion classification datasets exist. Most relevant studies are based on private datasets, which tend to be small in size and cause overfitting in learning models.In this paper, we construct a hybrid framework with ViT backbone for liver lesion classification, TransLiver. We design a pre-processing unit to reduce the annotation cost, where we obtain lesion area on multi-phase CTs from annotations marked on a single phase. To alleviate the limitations of pure transformers, we propose a multi-stage pyramid structure and add convolutional layers to the original transformer encoder. We use additional cross phase tokens at the last stage to complete a multi-phase fusion, which can focus on cross-phase communication and improve the fusion effectiveness as compared with conventional modes. While most multi-phase liver lesion classification studies use datasets with no more than three phases (without DL phase for its difficulty of collection) or no more than six lesion classes, we validate the whole framework on an in-house dataset with four phases of abdominal CT and seven classes of liver lesions. Considering the disproportion of axial lesion slice number and the relatively small scale of the dataset, we adopt a 2-D network in classification part instead of 3-D in pre-processing part and achieve a 90.9% accuracy."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.0,Method,"Figure 2 illustrates the overall architecture of TransLiver, where activation layers and batch normalization layers are omitted. Multi-phase liver lesion CTs are converted from single-phase annotation to multi-phase annotation by a preprocessing unit including a registration network and a lesion matcher.For each phase, a convolutional encoder extracts preliminary lesion features on axial slices. As the backbone of the whole framework, transformer encoder employs a 4-stage pyramid structure extracting multi-scale features, with each stage connected by a convolutional down-sampler. There are two types of transformer blocks, single-phase liver transformer block (SPLTB) and multi-phase liver transformer block (MPLTB). The former is phase-specific, while the latter is in charge of multi-phase fusion. Extracted features from different phases are averaged and classified by two successive fully connected networks. A voting strategy about slices is applied to decide the classification results. "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.1,Pre-processing Unit,"The single-phase annotated lesion has the position and class labels in all phases but they are not aligned, so we could have difficulty finding out which lesions in different phases are the same with 2 or more lesions in one patient. To reduce errors caused by unregistered data and address the situation that one patient has multiple lesions of different types, we pre-process the multi-phase liver CTs registered and grouped by lesions.The registration network is based on Voxelmorph [1], with a U-Net learning registration field and moving data transformed by the field. We also use auxiliary Dice loss function between fixed image lesion masks and moved image lesion masks to help the registration field learning. In [1], the network needs to specify an atlas image, otherwise, pairs of images will be registered to each other. But in our work, we need to register the original data in a cross-phase form. We choose an atlas phase ART as suggested by clinicians and other phases of CTs are registered to the ART phase of every patient.After registration, a lesion matcher finds the same lesions in different phases. We generate a minimum circumscribed cuboid with padding as the lesion window for each lesion to keep the surrounding information. The windows are then converted to 0-1 masks to calculate Dice coefficient. Lesions with the maximal window Dice coefficient that is no less than a set threshold are considered the same. Only lesions completely found in all phases will be used in the following classification network."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.2,Convolutional Encoder and Convolutional Down-Sampler,"In pure vision transformer, input images are converted to tokens by patch embedding and added with positional encoding to keep the positional information. Patch embedding consists of a linear connected layer or a convolutional layer, which does not enable to construct local relation [13]. Absolute positional encoding destroys the translation variance [10] that keeps the rotation and shift operations from altering the final results [6].So, we construct a convolutional encoder without absolute positional encoding to replace the original embedding layer. For an input image X ∈ R B×H×W ×1 , B is the batch size, and H × W is the size of the input. The module contains four convolutional layers playing different roles. The first layer, Conv 1 , with a kernel size of 3, stride of 2, and output channels of 32, reduces the size to H 2 × W 2 . Next two layers, Conv 2 and Conv 3 , each with a kernel size of 3, stride of 1, and the same output channel as Conv 1 , extract local information. Conv 1 , Conv 2 , and Conv 3 each is followed by a GeLU activation layer and a batch normalization. Considering the design of PVTv2 [21], an overlapped convolutional layer, Conv 4 , with a kernel size of 7, stride of 2, and output channels of 64, is used to strengthen the connection among patches. It is followed by layer normalization.to finish the tokenization of transformer.We add convolutional down-samplers between stages of transformer encoder so that they can produce hierarchical representation like CNN structure. Each convolutional down-sampler contains a residual structure with a 3×3 depthwise convolution to increase the locality of our model. We also utilize a convolutional layer with a kernel size of 2 and stride of 2, which halves the image resolution and doubles the number of channels."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.3,Single-Phase Liver Transformer Block,"Vision Transformers can get excellent performance on large-scale datasets such as ImageNet [4], but they are also prone to overfit on small datasets such as private hospital datasets. We adopt the spatial reduction structure proposed in PVT [20] to largely reduce the computational overhead by reducing the size of K and V using depthwise convolution. Following [6,12], a learnable relative positional encoding is added here to replace the absolute positional encoding. The self-attention module can be written as:where Q, K, V are the same with original ViT, d h is the head dimension, and P is the relative positional encoding. Spatial reduction SR consists of a k × k depthwise convolution with a stride of k and a batch normalization, where k is the spatial reduction ratio set in each stage. Feed forward network (FFN) is designed for a better capacity of representation. We use the module designed in [6] IRFFN (Inverted Residual FFN) with three convolutions instead of two linear layers in the initial vision transformer. The first and third convolutions are pointwise for dimension translation, which has a similar effect to the original linear layers. The second convolution with a shortcut connection extracts local information in a higher dimension and improves the gradient propagation ability across layers [6]. The structure also has two GeLU activation layers between convolutional layers and three batch normalizations after the GeLUs and the last convolutional layer for better performance."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.4,Multi-phase Liver Transformer Block,"Single-phase liver transformer block (SPLTB) is phase-specific, which means the model parameters of each phase are independent. It can fully extract features in different phases before fusion. Inspired by [14], in stage 4, we design a multi-phase liver transformer block (MPLTB) for communication between phases. MPLTB introduced some new parameters that are not in the original transformer. These parameters are randomly initialized, concatenated with the corresponding phase tokens respectively, and updated in phase-specific SPLTB. Then, they are separated and averaged for the next layer. The whole module is defined as:where X l i is phase tokens of the ith phase and the lth layer and t l is cross phase tokens of the lth layer. Because of the phase-specific SPLTB, t l i represents the corresponding cross phase tokens output of the ith phase in the lth layer. Cross phase tokens need negligible extra cost and can force the information to concentrate inside the tokens [14]. Compared to the direct fusion of input images or output features like average and concatenation, cross phase tokens can also reduce fusion granularity to sufficiently explore the relationship among phases. It is worth noting that these tokens will be removed provisionally when reshaping the phase tokens in SPLTB for right execution. The fusion is conducted in deep layers because the semantic concepts are learned in higher layers which benefits the cross phase connection."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.1,Liver Lesion Classification,"Dataset. The employed single-phase annotated dataset is collected from Sir Run Run Shaw Hospital (SRRSH), affiliated with the Zhejiang University School of Medicine, and has received the ethics approval of IRB. The collection process can be found in supplementary materials. The size of each CT slice is decreased to 224×224 using cubic interpolation. After the pre-processing unit with window Dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases of CTs, seven types of lesions (13.2% of HCC, 5.3% of HM, 11.3% of ICC, 22.6% of HH, 31.1% of HC, 8.7% of FNH, and 7.8% of HA), and totally 4820 slices. To handle the imbalance of dataset, we randomly select 586 lesions as the training and validation set with no more than 700 axial slices in each lesion type, and the rest 175 lesions constitute the test set. Lesions from the same patient are either assigned to the training and validation set or the test set, but not both.Implementations. The training and validation set is randomly divided with a 4:1 ratio. The data is augmented by flip, rotation, crop, shift, and scale. We initialize the backbone network using pre-trained weights of CMT-S [6]. Our models are implemented by Pytorch1.12.1 and Timm0.6.13 [22]. Then, they are trained on four NVIDIA Tesla A100 GPUs for 200 epochs using cross-entropy loss function with label smoothing and SGD optimizer with learning rate warmup and cosine annealing. The batch size is 32 and the learning rate is 1e-3. We measured performance by precision (Pre.), sensitivity (Sen.), specificity (Spe.), F1-score (F1), area under the curve (AUC), and accuracy (Acc.).Results. We first compare the class-wise accuracy of our model against other advanced methods applying different architectures in multi-phase liver lesion classification with more than four lesion types [3,11,15,23,24]. TransLiver gets the highest overall accuracy of 90.9% classifying the most lesion types of seven (HCC 90.9%, HM 62.5%, ICC 73.7%, HH 91.7%, HC 100.0%, FNH 100.0%, and HA 93.3%). In the results of our method, HM has a relatively low performance of 62.5%, mainly due to its low proportion in our dataset. The details can be found in supplementary materials.Because the sources of data are different among the methods compared above and to the best of our knowledge, no relevant study based on transformers was found, we further train some SOTA normal classification models on our dataset. Considering the fairness, all the models below are initialized with pre-trained weights and adopt 2-D structures using the same slice-level classification strategy. For completeness, we concatenate the multi-phase features to execute the fusion. As illustrated in Table 1, our proposed TransLiver model gets better performance than other models in all metrics. Behind our model, CMT-S achieves the best performance, indicating the effect of convolutional structures in transformer. "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.2,Ablation Study,"To verify the improvement of our modules, we conduct three baseline experiments for comparison. Here convolutional encoder, convolutional down-sampler, and SPLTB as a whole is called c-SPLTB. Baseline 0 does not use c-SPLTB or cross phase tokens in MPLTB but replaces them with pure vision transformer and output feature concatenation respectively. Baseline 1 adds the c-SPLTB and Baseline 2 adds the cross phase tokens. A 3-D version of Baseline 2 utilizing 3-D patch embedding is also studied in Baseline 3 to validate the advantage of our 2-D model. The result shown in Fig. 3 demonstrates that our design choice is appropriate. It is worth mentioning that the 2-D structure is prone to redundancy between axial slices and ignores the relation between slices compared with the 3-D structure but gets observably higher accuracy. We suppose the reason is twofold. Most of lesions in our dataset having few slices weakens the redundancy between slices in 2-D pipeline, while the number of slices is still obviously larger than the number of lesions, alleviating the overfitting issue. Furthermore, vision transformers are mostly pretrained in 2-D images, causing poor performance when transferring to 3-D pipeline.We also evaluate the model performance under different phase combinations by cutting the branch of certain phases. It shows that information from various phases can significantly influence the classification performance. A missing phase can cause an accuracy drop of about 10% and complete four-phase model outperforms single-phase model by nearly 20%. Figure 4 contains average results of phase number and details with all phase combinations can be found in supplementary materials.  "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,4.0,Conclusion,"We have presented a hybrid architecture for multi-phase liver lesion classification in this paper. The lesion features are extracted by transformer backbone with several auxiliary convolutional modules. Then, we fuse the features from different phases through cross phase tokens to enhance their information exchange.To handle the issues in realistic cases, we design a pre-processing unit to acquire multi-phase annotated lesions from single-phase annotated ones. We report performance of an overall 90.9% classification accuracy on a four-phase seven-class dataset through quantitative experiments and show obvious improvement compared with SOTA classification methods. In future work, we will extend classification to instance segmentation and provide an end-to-end effective model for liver lesion diagnosis."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 31.
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,1.0,Introduction,"Semantic segmentation on histological whole slide images (WSIs) allows precise detection of tumor boundaries, thereby facilitating the assessment of metastases [3] and other related analytical procedures [17]. However, pixel-level annotations of gigapixel-sized WSIs (e.g. 100, 000 × 100, 000 pixels) for training a segmentation model are difficult to acquire. For instance, in the CAMELYON16 breast cancer metastases dataset [10], 49.5% of WSIs contain metastases that are smaller than 1% of the tissue, requiring a high level of expertise and long inspection time to ensure exhaustive tumor localization; whereas other WSIs have large tumor lesions and require a substantial amount of annotation time for boundary delineation [18]. Identifying potentially informative image regions (i.e., providing useful information for model training) allows requesting the minimum amount of annotations for model optimization, and a decrease in annotated area reduces both localization and delineation workloads. The challenge is to effectively select annotation regions in order to achieve full annotation performance with the least annotated area, resulting in high sampling efficiency.We use region-based active learning (AL) [13] to progressively identify annotation regions, based on iteratively updated segmentation models. Each region selection process consists of two steps. First, the prediction of the most recently trained segmentation model is converted to a priority map that reflects informativeness of each pixel. Existing studies on WSIs made extensive use of informativeness measures that quantify model uncertainty (e.g., least confidence [8], maximum entropy [5] and highest disagreement between a set of models [19]). The enhancement of priority maps, such as highlighting easy-to-label pixels [13], edge pixels [6] or pixels with a low estimated segmentation quality [2], is also a popular area of research. Second, on the priority map, regions are selected according to a region selection method. Prior works have rarely looked into region selection methods; the majority followed the standard approach [13] where a sliding window divides the priority map into fixed-sized square regions, the selection priority of each region is calculated as the cumulative informativeness of its constituent pixels, and a number of regions with the highest priorities are then selected. In some other works, only non-overlapping or sparsely overlapped regions were considered to be candidates [8,19]. Following that, some works used additional criteria to filter the selected regions, such as finding a representative subset [5,19]. All of these works selected square regions of a manually predefined size, disregarding the actual shape and size of informative areas.This work focuses on region selection methods, a topic that has been largely neglected in literature until now, but which we show to have a great impact on AL sampling efficiency (i.e., the annotated area required to reach the full annotation performance). We discover that the sampling efficiency of the aforementioned standard method decreases as the AL step size (i.e., the annotated area at each AL cycle, determined by the multiplication of the region size and the number of selected regions per WSI) increases. To avoid extensive AL step size tuning, we propose an adaptive region selection method with reduced reliance on this AL hyperparameter. Specifically, our method dynamically determines an annotation region by first identifying an informative area with connected component detection and then detecting its bounding box. We test our method using a breast cancer metastases segmentation task on the public CAMELYON16 dataset and demonstrate that determining the selected regions individually provides greater flexibility and efficiency than selecting regions with a uniform predefined shape and size, given the variability in histological tissue structures. Results show that our method consistently outperforms the standard method by providing a higher sampling efficiency, while also being more robust to AL step size choices. Additionally, our method is especially beneficial for settings where a large AL step size is desirable due to annotator availability or computational restrictions."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.1,Region-Based Active Learning for WSI Annotation,"We are given an unlabeled pool U = {X 1 . . . X n }, where X i ∈ R Wi×Hi denotes the i th WSI with width W i and height H i . Initially, X i has no annotation; regions are iteratively selected from it and annotated across AL cycles. We denote the j th annotated rectangular region in, where (c ij x , c ij y ) are the center coordinates of the region and w ij , h ij are the width and height of that region, respectively. In the standard region selection method, where fixedsize square regions are selected, w ij = h ij = l, ∀i, j, where l is predefined.Figure 1 illustrates the workflow of region-based AL for WSI annotation. The goal is to iteratively select and annotate potentially informative regions from WSIs in U to enrich the labeled set L in order to effectively update the model g. To begin, k regions (each containing at least 10% of tissue) per WSI are randomly selected and annotated to generate the initial labeled set L. The model g is then trained on L and predicts on U to select k new regions from each WSI for the new round of annotation. The newly annotated regions are added to L for retraining g in the next AL cycle. The train-select-annotate process is repeated until a certain performance of g or annotation budget is reached.The selection of k new regions from X i is performed in two steps based on the model prediction P i = g(X i ). First, P i is converted to a priority map M i using a per-pixel informativeness measure. Second, k regions are selected based on M i using a region selection method. The informativeness measure is not the focus of this study, we therefore adopt the most commonly used one that quantifies model uncertainty (details in Sect. 3.2). Next we describe the four region selection methods evaluated in this work."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.2,Region Selection Methods,"Random. This is the baseline method where k regions of size l × l are randomly selected. Each region contains at least 10% of tissue and does not overlap with other regions. Standard [13] M i is divided into overlapping regions of a fixed size l × l using a sliding window with a stride of 1 pixel. The selection priority of each region is calculated as the summed priority of the constituent pixels, and k regions with the highest priorities are then selected. Non-maximum suppression is used to avoid selecting overlapping regions. Standard (non-square) We implement a generalized version of the standard method that allows non-square region selections by including multiple region candidates centered at each pixel with various aspect ratios. To save computation and prevent extreme shapes, such as those with a width or height of only a few pixels, we specify a set of candidates as depicted in Fig. 2. Specifically, we define a variable region width w as spanning from 1  2 l to l with a stride of 256 pixels and determine the corresponding region height as h = l 2 w . Adaptive (proposed) Our method allows for selecting regions with variable aspect ratios and sizes to accommodate histological tissue variability. The k regions are selected sequentially; when selecting the j th region R ij in X i , we first set the priorities of all pixels in previously selected regions (if any) to zero. We then find the highest priority pixel (c ij x , c ij y ) on M i ; a median filter with a kernel size of 3 is applied beforehand to remove outliers. Afterwards, we create a mask on M i with an intensity threshold of τ th percentile of intensities in M i , detect the connected component containing (c ij x , c ij y ), and select its bounding box. As depicted in Fig. 3, τ is determined by performing a bisection search over [98, 100] th percentiles, such that the bounding box size is in range. This size range is chosen to be comparable to the other three methods, which select regions of size l 2 . Note that Standard (non-square) can be understood as an ablation study of the proposed method Adaptive to examine the effect of variable region shape by maintaining constant region size."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.3,WSI Semantic Segmentation Framework,"This section describes the breast cancer metastases segmentation task we use for evaluating the AL region selection methods. The task is performed with patch-wise classification, where the WSI is partitioned into patches, each patch is classified as to whether it contains metastases, and the results are assembled.Training. The patch classification model h(x, w) : R d×d -→ [0, 1] takes as input a patch x and outputs the probability p(y = 1|x, w) of containing metastases, where w denotes model parameters. Patches are extracted from the annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels. Following [11], a patch is labeled as positive if the center 128 × 128 pixels area contains at least  one metastasis pixel and negative otherwise. In each training epoch, 20 patches per WSI are extracted at random positions within the annotated area; for WSIs containing annotated metastases, positive and negative patches are extracted with equal probability. A patch with less than 1% tissue content is discarded. Data augmentation includes random flip, random rotation, and stain augmentation [12]. Inference. X i is divided into a grid of uniformly spaced patches (40× magnification, d = 256 pixels) with a stride s. The patches are predicted using the trained patch classification model and the results are stitched to a probability map P i ∈ [0, 1] W i ×H i , where each pixel represents a patch prediction. The patch extraction stride s determines the size of P i (W i = Wi s , H i = Hi s )."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.1,Dataset,We used the publicly available CAMELYON16 Challenge dataset [10] 
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.2,Implementation Details,"Training Schedules. We use MobileNet v2 [15] initialized with ImageNet [14] weights as the backbone of the patch classification model. It is extended with two fully-connected layers with sizes of 512 and 2, followed by a softmax activation layer. The model is trained for up to 500 epochs using cross-entropy loss and the Adam optimizer [7], and is stopped early if the validation loss stagnates for 100 consecutive epochs. Model selection is guided by the lowest validation loss. The learning rate is scheduled by the one cycle policy [16] with a maximum of 0.0005. The batch size is 32. We used Fastai v1 [4] for model training and testing. The running time of one AL cycle (select-train-test) on a single NVIDIA Geforce RTX3080 GPU (10GB) is around 7 h.Active Learning Setups. Since the CAMELYON16 dataset is fully annotated, we perform AL by assuming all WSIs are unannotated and revealing the annotation of a region only after it is selected during the AL procedure. We divide the WSIs in U randomly into five stratified subsets of equal size and use them sequentially. In particular, regions are selected from WSIs in the first subset at the first AL cycle, from WSIs in the second subset at the second AL cycle, and so on. This is done because WSI inference is computationally expensive due to the large patch amount, reducing the number of predicted WSIs to one fifth helps to speed up AL cycles. We use an informativeness measure that prioritizes pixels with a predicted probability close to 0.5 (i.e., M i = 1-2|P i -0.5|), following [9]. We annotate validation WSIs in the same way as the training WSIs via AL.Evaluations. We use the CAMELYON16 challenge metric Free Response Operating Characteristic (FROC) score [1] to validate the segmentation framework.To evaluate the WSI segmentation performance directly, we use mean intersection over union (mIoU). For comparison, we follow [3] to use a threshold of 0.5 to generate the binary segmentation map and report mIoU (Tumor), which is the average mIoU over the 48 test WSIs with metastases. We evaluate the model trained at each AL cycle to track performance change across the AL procedure."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.3,Results,"Full Annotation Performance. To validate our segmentation framework, we first train on the fully-annotated data (average performance of five repetitions reported). With a patch extraction stride s = 256 pixels, our framework yields an FROC score of 0.760 that is equivalent to the Challenge top 2, and an mIoU (Tumor) of 0.749, which is higher than the most comparable method in [3] that achieved 0.741 with s = 128 pixels. With our framework, reducing s to 128 pixels improves both metastases identification and segmentation (FROC score: 0.779, mIoU (Tumor): 0.758). However, halving s results in a 4-fold increase in inference time. This makes an AL experiment, which involves multiple rounds of WSI inference, extremely costly. Therefore, we use s = 256 pixels for all following AL experiments to compromise between performance and computation costs. Because WSIs without metastases do not require pixel-level annotation, we exclude the 159 training and validation WSIs without metastases from all following AL experiments. This reduction leads to a slight decrease of full annotation performance (mIoU (Tumor) from 0.749 to 0.722).  All experiments (except for Random) use uncertainty sampling. When using region selection method Standard, the sampling efficiency advantage of uncertainty sampling over random sampling decreases as AL step size increases. A small AL step size minimizes the annotated tissue area for a certain high level of model performance, such as an mIoU (Tumor) of 0.7, yet requires a large number of AL cycles to achieve full annotation performance (Fig. 4 (a-d)), Table 1. Annotated tissue area (%) required to achieve full annotation performance. The symbol ""/"" indicates that the full annotation performance is not achieved in the corresponding experimental setting in Fig. 4.  resulting in high computation costs. A large AL step size allows for full annotation performance to be achieved in a small number of AL cycles, but at the expense of rapidly expanding the annotated tissue area (Fig. 4(e), (f), (h) and (i)). Enabling selected regions to have variable aspect ratios does not substantially improve the sampling efficiency, with Standard (non-square) outperforming Standard only when the AL step size is excessively large (Fig. 4(i)). However, allowing regions to be of variable size consistently improves sampling efficiency. Table 1 shows that Adaptive achieves full annotation performance with fewer AL cycles than Standard for small AL step sizes and less annotated tissue area for large AL step sizes. As a result, when region selection method Adaptive is used, uncertainty sampling consistently outperforms random sampling. Furthermore, Fig. 4(e-i)) shows that Adaptive effectively prevents the rapid expansion of annotated tissue area as AL step size increases, demonstrating greater robustness to AL step size choices than Standard. This is advantageous because extensive AL step size tuning to balance the annotation and computation costs can be avoided. This behavior can also be desirable in cases where frequent interaction with annotators is not possible or to reduce computation costs, because the proposed method is more tolerant to a large AL step size. We note in Fig. 4(h) that the full annotation performance is not achieved with Adaptive within 15 AL cycles; in Fig. S1 in the supplementary materials we show that allowing for oversampling of previously selected regions can be a solution to this problem. Additionally, we visualize examples of selected regions in Fig. 5 and show that Adaptive avoids two region selection issues of Standard : small, isolated informative areas are missed, and irrelevant pixels are selected due to the region shape and size restrictions."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,4.0,Discussion and Conclusion,"We presented a new AL region selection method to select annotation regions on WSIs. In contrast to the standard method that selects regions with predetermined shape and size, our method takes into account the intrinsic variability of histological tissue and dynamically determines the shape and size for each selected region. Experiments showed that it outperforms the standard method in terms of both sampling efficiency and the robustness to AL hyperparameters. Although the uncertainty map was used to demonstrate the efficacy of our approach, it can be seamlessly applied to any priority maps. A limitation of this study is that the annotation cost is estimated only based on the annotated area, while annotation effort may vary when annotating regions of equal size. Future work will involve the development of a WSI dataset with comprehensive documentation of annotation time to evaluate the proposed method and an investigation of potential combination with self-supervised learning."
Aneurysm Pose Estimation with Deep Learning,1.0,Introduction,"Intracranial aneurysms are abnormal focal dilations of cerebral blood vessels. Their rupturing accounts for 85% of Subarachnoid Hemorrhages (SAH), and is associated with high morbidity and mortality rates [23]. Early detection and monitoring of Unruptured Intracranial Aneurysms (UIA) has become a problem of increasing clinical importance. Due to its non-invasive nature, 3D time-of-flight Magnetic Resonance Angiography (TOF-MRA) is the most suitable imaging technique for screening. However, detecting aneurysms in TOF-MRA volumes is a costly process that requires radiologists to scroll through different cut planes [7]. Therefore, an automated method to detect aneurysms and provide immediate appropriate visualization would be a valuable tool to assist radiologists in their clinical routine. We envision a dynamic browsing of cut planes rotating around the main axis of the aneurysm to facilitate the analysis of the aneurysm and the surrounding angioarchitecture. This requires estimating the location and orientation, i.e. the pose, of the aneurysm. This pose has been related to the risk of rupture [13] and could also be used for image registration [17].Automated methods for detecting UIAs range from traditional Computer-Aided Detection (CAD) systems using image filtering techniques [1,27], to advanced deep learning methods based on Convolutional Neural Networks (CNNs). Although 2D and 2.5D methods have been proposed [19,24,26], most recent methods are fully 3D patch-based approaches. In 2020, the Aneurysm Detection and SegMentation (ADAM) challenge [25] compared various detection methods using TOF-MRA data. The class imbalance problem caused by the scarcity of aneurysm voxels inside an image volume, was addressed through loss functions and/or data augmentation. The top-performing method was nnDetection [3], which relies on a 3D bounding box representation. nnUNet [9], ranked third, uses the UNet [6] semantic segmentation architecture. Both methods consider large patches as input, which requires significant computing power and large databases for reliable sample modeling. Detection provides localization, but aneurysm orientation is challenging to estimate, due to the noise/artifacts in medical images, annotation burden, with inter-and intra-observer variability, and small size and shape diversity which imply more uncertainty than for larger objects.Estimating the pose of organs has been investigated in the literature as a slice positioning problem. A set of slices must be optimally selected relative to the pose of the knee [5,14,28,29], shoulder [29], or brain [4,10,12]. These organs are single instances of large objects with specific shapes, standard positions and orientations within the images. On the contrary, aneurysms are very small pathologies with unspecific shapes, undefined locations and numbers. These algorithms can be categorized into registration-based and learning-based methods. Registrationbased methods [5,12] rely on rigid transformations, which limits their application to (quasi-)rigid body parts. Learning-based methods [10,20,28,29] typically consist of a two-stage pipeline. The first stage detects the Regions-of-Interest (ROIs), while the second stage regresses/estimates the object orientation for each ROI. For instance, faster-RCNN [22] and V-Net [18] architectures were employed in [29] to localize and segment the orientation plane, defined by a center location and two unit vectors. However, existing methods are mainly intended for low-resolution images like MR scout scans, and their computational demands increase with high-resolution images. One-stage approaches, such as YOLO [21], demonstrate promising performance in object detection with greater flexibility than two-stage approaches.In this paper, we introduce a novel one-stage method to simultaneously localize, and estimate the size and the orientation of aneurysms from 3D TOF-MRA images. A fast and approximate annotation is used. To address the class imbalance problem, a small patch approach is combined with dedicated data sampling and generation strategies. We follow a landmark approach to estimate the aneurysm pose, while avoiding rotation discontinuity problems associated with Euler angles and quaternions [30]. Furthermore, we propose a 3D extension of YOLO architecture, using a cosine similarity loss for the orientation."
Aneurysm Pose Estimation with Deep Learning,2.1,Datasets and Data Annotation,"In this work, two TOF-MRA aneurysm datasets were used. The first dataset includes 132 exams (75 female, 57 male) collected at our medical institution between 2015 and 2021 according to the following inclusion criteria: diagnosed unruptured saccular aneurysms smaller than 20 mm, no pre-treated aneurysm or fusiform aneurysm. A single exam was included per patient (i.e. no follow-up exams). All images were acquired using a 3T scanner (GE Discovery MR750w). Acquisition parameters included TR = 28 ms, TE = 3.4 ms, slice thickness= 0.8 mm, and 4 slabs (54 slices/slab), resulting in 512 × 512 × 254 volumes with a 0.47×0.47×0.4mm 3 voxel size. Each DICOM data was anonymized on the clinical site before processing. As per the charter of our university hospital, the anonymous use of imaging data acquired in clinical practice is authorized for research purposes, in accordance with the principle of non-opposition of the patient. Each image contained from one (84/132) to five aneurysms (4 subjects), totaling 206 aneurysms with a mean diameter of 3.97 ± 2.32 mm (range: 0.96-19.63 mm). Most aneurysms were small, with 81 below 3 mm and 77 between 3-5 mm.The second dataset is the public aneurysm dataset [7], which comprises 412 images. After applying the same inclusion criteria as the in-house dataset, 270 images were selected for analysis. Two expert neuroradiologists reviewed the dataset, identifying 7 additional aneurysms and removing 5 aneurysms as they were simple irregularities on the vessel surface. The resulting images contains 164 aneurysms with similar statistics to the first dataset: mean diameter of 3.74 ± 2.17 mm (range: 1.37-13.64 mm), 66 below 3 mm and 72 between 3-5 mm. Each image contained from 0 (130 healthy subjects) to 3 (3 subjects) aneurysms.Previous works on aneurysm detection and segmentation relied on voxel-wise labeling, which is time-consuming and susceptible to intra-and inter-rater variability. To address these limitations, weak annotations using spheres have been recently investigated [2,7]. Similar to [2], our annotation involves labeling each aneurysm using two points: the center of the neck (i.e. ostium) and another point along its main axis (i.e. dome). This method provides information about aneurysms location, size, and their orientation (see Fig. 1). To simplify the placement of the two points in the volume rendering view, we developed a Python extension for the 3D Slicer software [8], which provides a real-time visualization of the sphere in the canonical cut planes."
Aneurysm Pose Estimation with Deep Learning,2.2,Data Sampling and Generation,"Accurate modeling of aneurysm and background properties is crucial for pose estimation tasks. We use small 96 × 96 × 96 voxel patches with an isotropic voxel size of 0.4 mm, resulting in 38.4 mm side length patches. This approach is computationally efficient compared to larger patch methods, such as nnDetection [3] and nnUNet [9]. It also allows for the extraction of multiple non-intersecting negative (aneurysm-free) patches from each image for more training data and reliable background modeling. However, this approach introduces a class imbalance problem, as there is only a single positive patch for each aneurysm. To overcome this, we used adapted data sampling strategies. Our first strategy duplicates each positive patch 50 times and applies random distortions at each epoch to synthesize a variety of aneurysm shapes: each control point on a 3 × 3 × 3 lattice enclosing the patch, except the central point, is moved randomly by 3 mm in all 3 space directions, and the distortion field is interpolated using cubic spline interpolation. To guide the model to discriminate between healthy vessels and aneurysms, our second strategy pre-selects 40 non-intersecting negative patches per image, 30 of which are centered on blood vessels by iteratively choosing the brightest voxels as patch centers. Each training epoch used a set composed of all positive patches, completed with random negative patches equally drawn among images (15% of the training set). Random rotations (0 to 180 • ), shifts (0 to 10 mm) and horizontal flips were applied as data augmentation."
Aneurysm Pose Estimation with Deep Learning,2.3,Neural Network Architecture,"Inspired by YOLO [21], we present a one-stage neural network architecture for aneurysm pose estimation in 3D images. As shown in Fig. 2, our architecture follows a grid-based approach and divides the input 3D patch (96 × 96 × 96 voxels) into 12 × 12 × 12 = 1728 cells of 8 × 8 × 8 voxels.To encode the input patch into feature maps, we use residual convolutional blocks and down-sampling operations. The Localization and Orientation Head splits the encoded feature maps into a grid of 1728 cells using two consecutive convolutional blocks followed by three parallel convolutions. The first convolution generates a confidence probability score indicating whether the cell contains an aneurysm center. For positive cells (i.e. containing an aneurysm center), the second convolution, followed by sigmoid function, predicts the aneurysm center coordinates C = (C x , C y , C z ) relative to the cell size, while the third convolution estimates the aneurysm size and its orientation by calculating the axis vector "
Aneurysm Pose Estimation with Deep Learning,2.4,Loss Function,"Due to the grid-based nature of our architecture, there is a high imbalance between the number of negative cells and a very small number of positive cells. Inspired by [21], a weighted loss function was employed, which encompasses the sum of terms pertaining to confidence, localization, and orientation estimation.To optimize the detection confidence, we used the binary cross-entropy (BCE) loss function for both positive (BCE P ) and negative (BCE N ) cells. To prioritize identifying aneurysms over background, we weighted the negative cell term by half the number of positive cells (#P ) in the batch (Eq. 1). Aneurysm localization and dimensions are assessed using mean squared error (MSE) (Eq. 2). Orientation estimation is enforced through the cosine similarity of v (Eq. 3). These last two terms are only computed on positive cells with a weight of 5 to account for the limited number of such cells."
Aneurysm Pose Estimation with Deep Learning,2.5,Implementation Details,"We implemented our method using PyTorch framework (1.10.0). The model has approximately 28 million parameters, that were optimized using the stochastic gradient descent algorithm. The hyper-parameters were determined using a subset of the in-house dataset: 200 epochs, balanced batch sampling technique between negative and positive patches, batch size of 32, and initial learning rate of 10 -2 . Each input volume was normalized using z-score normalization. Training and inference were performed on an NVIDIA RTX A6000 GPU with 48 GB of memory. During inference, a patch reconstruction technique is used to predict the location and orientation of aneurysms in the entire volume. The original volume is split into patches with an isotropic voxel resolution of 0.4 mm. To mitigate border effects caused by convolutions, a 16 voxel overlap is considered between adjacent patches. Predictions are made for each patch and converted back to the original volume resolution: a pose is kept only if the predicted center is inside the central 64 × 64 × 64 part of the patch. Non-Maximum Suppression (NMS) is used to eliminate overlapping predictions, considered as spheres (see Sect. 2.3)."
Aneurysm Pose Estimation with Deep Learning,2.6,Evaluation Metrics,"For the pose estimation task, our method was evaluated based on two standard metrics. First, the Euclidean distance (in mm) was measured between the predicted aneurysm center (C) and its corresponding ground truth (GT). The second metric computes the angular difference (in degrees) between the predicted aneurysm orientation vector ( v) and its corresponding GT.For the detection task, our evaluation was based on the Intersection-over-Union (IoU) between the predicted and GT spheres at a threshold of 10% [3,16]. A GT sphere with an IoU score above 10% was tagged as a true positive (TP), else it was a false negative (FN). A false positive (FP) was counted for each predicted sphere with no IoU score above 10%. We report the Average Precision metric (AP 0.1 ), as well as the sensitivity score (Sensitivity 0.5 ) and the number of false positives per case (FPs/case 0.5 ), both at a default 50% confidence threshold."
Aneurysm Pose Estimation with Deep Learning,3.1,Pose Estimation,"We conducted 5-fold cross-validation separately on two large datasets (see Sect. 2.1) to evaluate the performance of our method for aneurysm pose estimation. Each dataset was randomly split into five subsets, with 25 or 26 patients per subset for the in-house dataset and 54 patients per subset for dataset [7]. The number of aneurysms and mean aneurysm size for each subset were as follows: (In-house) aneurysms: 47, 32, 45, 38, and 44; size: 4.15 mm, 3.61 mm, 3.88 mm, 3.85 mm, and 4.25 mm; (Dataset [7]) aneurysms: 32, 33, 43, 28, and 28; size: 3.56 mm, 3.19 mm, 4.05 mm, 4.21 mm, and 3.70 mm. We trained five models for each dataset, using four subsets for training and one subset for testing. This resulted in, for each fold, around 9655 training patches for the in-house dataset and 7595 training patches for dataset [7].The results on both datasets are shown in Table 1. In the in-house dataset, the median (mean ± std) errors were 0.49 mm (0.54 mm±0.32) for the aneurysm  center location; and 11.91 • (15.26 • ±10.92) for its orientation. In dataset [7], the median errors were 0.48 mm (0.51 mm±0.26) for the center location; and 12.27 • (14.58 • ±10.53), for the orientation. Figure 3 illustrates that the pose computed by our method is sufficiently accurate for clinical use. It was used to display a cut plane through aneurysms with diverse shapes and sizes. Figure 3a reports on the case of a small aneurysm (size 1.97 mm). The pose was estimated with 8.20 • orientation error and 0.82 mm center location error. This accuracy, especially on the location, makes it possible to infer a cut plane through the aneurysm that is fit for immediate clinical analysis. Similarly, the case of a larger, spherical-shaped aneurysm (size 7.69 mm) is shown in Fig. 3b. Our method estimated the pose with an orientation error of 10.62 • and center location error of 0.72 mm. Larger orientation errors occurred in rare cases like the aneurysm in Fig. 3c (size 3.52 mm). We related such errors (here 41.54 • ) to the complex shape of the aneurysm, that implied annotation uncertainty for the axis orientation. Besides, our method was able to detect some aneurysms that were missed in the initial annotation by radiologists. Figure 3d shows such an aneurysm detection (size 3.50 mm)."
Aneurysm Pose Estimation with Deep Learning,3.2,Object Detection,"We also evaluated the effectiveness of our method on the classical detection task by comparing it with two public and fully-automated state-of-the-art baselines, nnDetection [3] and nnUNet [9]. nnDetection is based on an improved RetinaNet architecture, which has demonstrated superior performance compared to SSD Table 2. The results (mean ± std) of aneurysm detection task using dataset [7]. The results of our method on the in-house dataset are added for comparison."
Aneurysm Pose Estimation with Deep Learning,,Methods,"AP0.1 (%) Sensitivity0.5 (%) FPs/case0. and Faster RCNN [15]. We used 5-fold cross-validation on the public dataset [7] to guarantee the reproducibility of the results. The 5 models trained in Sect. 3.1 were used to assess the performance of our method. To ensure a fair comparison, we converted the output of nnDetection and nnUNet to spherical representations. Specifically, for nnDetection, we transformed the predicted 3D bounding boxes into spheres using the largest extent of the box as the sphere diameter. For nnUNet, we fitted one sphere on each connected component from the segmented voxel image. The diameter was computed as the maximum distance between two voxel locations, and the confidence score as the maximum predicted voxel value.As shown in Table 2, our method exhibited competitive performance compared to the two baselines achieving an AP 0.1 score of 76.60% (nnDetection: 73.68% and nnUNet: 72.46%). Additionally, our method demonstrated a good trade-off between sensitivity and FP/case, with a Sensitivity 0.5 score of 82.93% associated with 0.44 FPs/case 0.5 . In comparison, based on Free-response Receiver Operating Characteristic (FROC) curves, nnUNet achieves a maximum sensitivity of 81.90% with a higher FP/case of 1.04, while nnDetection achieves the same sensitivity of 82.93% but with a higher FP/case of 0.51."
Aneurysm Pose Estimation with Deep Learning,4.0,Conclusion,"In this paper, we proposed a novel one-stage deep learning approach for aneurysm pose estimation from TOF-MRA images, which can also be used for the classical detection task. It was evaluated using two large datasets, including a public one [7]. The results demonstrate the effectiveness of our proposed method in both tasks.In the pose estimation task, our method achieved good and similar performance on both datasets, accurately estimating the pose of aneurysms with diverse shapes and sizes. Rare errors in orientation were primarily due to small aneurysms and sometimes complex aneurysm shapes, leading to weak and uncertain GT annotations. Specifically, on the public dataset [7], the median orientation error was 14.79 • for small aneurysms (<3 mm), 11.49 • for medium-sized aneurysms (3-5 mm), and 10.69 • for large aneurysms. A current work consists in giving a better clinical definition of the aneurysm axis to reduce this error.In the aneurysm detection task, our proposed method exhibited promising performance compared to two state-of-the-art baselines, nnDetection [3] and nnUNet [9], with an average precision score of 76.60% and a good balance between sensitivity and FPs/case scores. Besides, these baselines are more computationally demanding compared to our method, which is based on small nonintersecting patches. Out of the 164 aneurysms in dataset [7], half of the 28 FN aneurysms had a size below 3 mm. Part of these misses are related to the annotation uncertainty on such aneurysms, which are difficult to diagnose in TOF-MRA [11]. Nevertheless, our future work will address this specific class of aneurysms, including the management of multiple annotators for finer uncertainty modeling.Our method represents a promising step towards automated aneurysm pose estimation and detection, offering several advantages over existing approaches. It demonstrated multi-task learning capabilities by simultaneously localizing, and estimating the size and the orientation of aneurysms in a single forward pass. Preliminary qualitative tests are hopeful indicators for its clinical utility."
Localized Questions in Medical Visual Question Answering,1.0,Introduction,"Visual Question Answering (VQA) models are neural networks that answer natural language questions about an image [2,8,12,21]. The capability of VQA models to interpret natural language questions is of great appeal, as the range of possible questions that can be asked is vast and can differ from those used to train the models. This has led to many proposed VQA models for medical applications in recent years [7,9,14,16,[23][24][25]. These models can enable clinicians to probe the model with nuanced questions, thus helping to build confidence in its predictions.Recent work on medical VQA has primarily focused on building more effective model architectures [7,20,23] or developing strategies to overcome limitations in medical VQA datasets [4,15,18,19,23]. Another emerging trend is to enhance VQA performance by addressing the consistency of answers produced [22], particularly when considering entailment questions (i.e., the answer to ""Is the image that of a healthy subject?"" should be consistent with the answer to ""Is there a fracture in the tibia?""). Despite these recent advances, however, most VQA models restrict to questions that consider the entire image at a time. Specifically, VQA typically uses questions that address content within an image without specifying where this content may or may not be in the image. Yet the ability to ask specific questions about regions or locations of the image would be highly beneficial to any user as it would allow fine-grained questions and model probing. For instance, Fig. 1 illustrates examples of such localized questions that combine content and spatial specifications. In the medical field, posing localized questions can significantly enhance the diagnostic process by providing second opinions to medical experts about suspicious regions. Additionally, this approach can improve trustworthiness by assessing the consistency between answers to both global and localized questions.To this day, few works have addressed the ability to include location information in VQA models. In [17], localization information is posed in questions by constraining the spatial extent to a point within bounding boxes yielded by an object detector. The model then focuses its attention on objects close to this point. However, the method was developed for natural images and relies heavily on the object detector to limit the attention extent, making it difficult to scale in medical imaging applications. Alternatively, the approach from [23] answers questions about a pre-defined coarse grid of regions by directly including region information into the question (e.g., ""Is grasper in (0,0) to (32,32)?""). This method relies on the ability of the model to learn a spatial mapping of the image and limits the regions to be on a fixed grid. Localized questions were also considered in [22], but the region of interest was cropped before being presented to the model, assuming that the surrounding context is irrelevant for answering this type of question.To overcome these limitations, we propose a novel VQA architecture that alleviates the mentioned issues. At its core, we hypothesize that by allowing the VQA model to access the entire images and properly encoding the region of interest, this model can be more effective at answering questions about regions. To achieve this, we propose using a multi-glimpse attention mechanism [3,22,23] restricting its focus range to the region in question, but only after the model has considered the entire image. By doing so, we preserve contextual information about the question and its region. We evaluate the effectiveness of our approach by conducting extensive experiments on three datasets and comparing our method to state-of-the-art baselines. Our results demonstrate performance improvements across all datasets."
Localized Questions in Medical Visual Question Answering,2.0,Method,"Our method extends a VQA model to answer localized questions. We define a localized question for an image x as a tuple (q, m), where q is a question, and m is a binary mask of the same size as x that identifies the region to which the question pertains. Our VQA model p θ , depicted in Fig. 2, accepts an image and a localized question as input and produces a probability distribution over a finite set A of possible answers. The final answer of the model â is the element with the highest probability, â = arg max a∈A p θ (a | q, x, m).(The model proceeds in three stages to produce its prediction: input embedding, localized attention, and final classification.Input Embedding. The question q is first processed by an LSTM [11] to produce an embedding q ∈ R Q . Similarly, the image x is processed by a ResNet-152 [10] to produce the feature map x ∈ R C×H×W .Localized Attention. An attention mechanism uses the embedding to determine relevant parts of the image to answer the corresponding question. Unlike previous attention methods, we include the region information that the mask defines. Our localized attention module (Fig. 2 right) uses both descriptors and the mask to produce multiple weighted versions of the image feature map, x = att(q, x, m). To do so, the module first computes an attention map g ∈ R G×H×W with G glimpses by applying unmasked attention [13,23] to the image feature map and the text descriptor. The value of the attention map at location (h, w) is computed as,where the index :hw indicates the feature vector at location (h, w), W (x) ∈ R C ×C , W (q) ∈ R C ×Q , and W (g) ∈ R G×C are learnable parameters of linear transformations, and is the element-wise product. In practice, the transformations W (x) and W (g) are implemented with 1 × 1 convolutions and all linear transformations include a dropout layer applied to its input. The image feature maps x are then weighted with the attention map and masked with m as, where c and g are the indexes over feature channels and glimpses, respectively, (h, w) is the index over the spatial dimensions, and m ↓ H×W denotes a binary downsampled version of m with the spatial size of x. This design allows the localized attention module to compute the attention maps using the full information available in the image, thereby incorporating context into them before being masked to constrain the answer to the specified region.Classification. The question descriptor q and the weighted feature maps x from the localized attention are vectorized and concatenated into a single vector of size C • G + Q and then processed by a multi-layer perceptron classifier to produce the final probabilities.Training. The training procedure minimizes the standard cross-entropy loss over the training set updating the parameters of the LSTM encoder, localized attention module, and the final classifier. The training set consists of triplets of images, localized questions, and the corresponding ground-truth answers. As in [2], the ResNet weights are fixed with pre-trained values, and the LSTM weights are updated during training."
Localized Questions in Medical Visual Question Answering,3.0,Experiments and Results,We compare our model to several baselines across three datasets and report quantitative and qualitative results. Additional results are available in the supplementary material.
Localized Questions in Medical Visual Question Answering,3.1,Datasets,"We evaluate our method on three datasets containing questions about regions which we detail here. The first dataset consists of an existing retinal fundus VQA dataset with questions about the image's regions and the entire image.The second and third datasets are generated from public segmentation datasets but use the method described in [23] to generate a VQA version with region questions. [22]. 679 fundus images containing questions about entire images (e.g., ""what is the DME risk grade?"") and about randomly generated circular regions (e.g., ""are there hard exudates in this region?""). The dataset comprises 9'779 question-answer (QA) pairs for training, 2'380 for validation, and 1'311 for testing. RIS-VQA. Images from the 2017 Robotic Instrument Segmentation dataset [1]."
Localized Questions in Medical Visual Question Answering,,DME-VQA,"We automatically generated binary questions with the structure ""is there [instrument] in this region?"" and corresponding masks as rectangular regions with random locations and sizes. Based on the ground-truth label maps, the binary answers were labeled ""yes"" if the region contained at least one pixel of the corresponding instrument and ""no"" otherwise. The questions were balanced to maintain the same amount of ""yes"" and ""no"" answers. Figure 3 shows the distribution of questions in the three datasets."
Localized Questions in Medical Visual Question Answering,3.2,Baselines and Metrics,"We compare our method to four different baselines, as shown in Fig. 4:No mask: no information is provided about the region in the question.Region in text [23]: region information is included as text in the question.Crop region [22]: image is masked to show only the queried region, with the area outside the region set to zero. Draw region: region is indicated by drawing its boundary on the input image with a distinctive color.We evaluated the performance of our method using accuracy for the DME-VQA dataset and the area under the receiver operating characteristic (ROC) curve and Average Precision (AP) for the RIS-VQA and INSEGCAT-VQA datasets. Implementation Details: Our VQA architecture uses an LSTM [11] with an output dimension 1024 to encode the question and a word embedding size of 300.We use the ResNet-152 [10] with ImageNet weights to encode images of size 448 × 448, generating feature maps with 2048 channels. In the localized attention block, the visual and textual features are projected into a 512-dimensional space before being combined by element-wise multiplication. Following [6,13], the number of glimpses is set to G = 2 for all experiments. The classification block is a multi-layer perceptron with a hidden layer of 1024 dimensions. A dropout rate of 0.25 and ReLU activation are used in the localized attention and classifier blocks. We train our models for 100 epochs using an early stopping condition with patience of 20 epochs. Data augmentation consists of horizontal flips. We use a batch size of 64 samples and the Adam optimizer with a learning rate of 10 -4 , which is reduced by a factor of 0.1 when learning stagnates. Models implemented in PyTorch 1.13.1 and trained on an Nvidia RTX 3090 graphics card."
Localized Questions in Medical Visual Question Answering,3.3,Results,"Our method outperformed all considered baselines on the DME-VQA (Table 1), the RIS-VQA, and the INSEGCAT-VQA datasets (Table 2), highlighting the importance of contextual information in answering localized questions. Context proved to be particularly critical in distinguishing between objects of similar appearance, such as the bipolar and prograsp forceps in RIS-VQA, where our method led to an 8 percent point performance improvement (Table 3). In contrast, the importance of context was reduced when dealing with visually distinct objects, resulting in smaller performance gains as observed in the INSEGCAT-VQA dataset. For example, despite not incorporating contextual information, the baseline crop region still benefited from correlations between the location of the region and the instrument mentioned in the question (e.g., the eye retractor typically appears at the top or the bottom of the image), enabling it to achieve competitive performance levels that are less than 2 percent points lower than our method (Table 2, bottom). Similar to our method, the baseline draw region incorporates contextual information when answering localized questions. However, we observed that drawing regions on the image can interfere with the computation of guided attention maps, leading to incorrect predictions (Fig. 5, column 4). In addition, the lack of masking of the attention maps often led the model to wrongly consider areas beyond the region of interest while answering questions (Fig. 5, column 1).When analyzing mistakes made by our model, we observe that they tend to occur when objects or background structures in the image look similar to the object mentioned in the question (Fig. 5, column 3). Similarly, false predictions were observed when only a few pixels of the object mentioned in the question were present in the region.  "
Localized Questions in Medical Visual Question Answering,4.0,Conclusions,"In this paper, we proposed a novel VQA architecture to answer questions about regions. We compare the performance of our approach against several baselines and across three different datasets. By focusing the model's attention on the region after considering the evidence in the full image, we show how our method brings improvements, especially when the complete image context is required to answer the questions. Future works include studying the agreement between answers to questions about concentric regions, as well as the agreement between questions about images and regions."
Localized Questions in Medical Visual Question Answering,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 34.
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,1.0,Introduction,"Carotid intima-media (CIM) segmentation has been widely applied in clinical practice, providing a diagnostic basis for atherosclerotic disease (one of the complications of obesity). To identify the contour of the intima-media, i.e., the structure between the lumen-intima (LI) and the media-adventitia (MA), one of the available solutions is deep learning-based medical image segmentation for CIM. Currently, this CIM segmentation approach faces the challenges of lack of largequantity images, high-quality labels from ultrasound experts, and a mixture of clear and ambiguous CIM areas in carotid ultrasound images.Semi-supervised learning recently applies novel frameworks to a general segmentation task [1][2][3][4]. In particular, the combination of consistency regularization and pseudo-labeling utilizes unlabeled data to partially address the lack-of-label issue [5]. A different strategy to efficiently utilize labeling effort is active learning (AL), which can iteratively select a subset of unlabeled data for annotation by experts, but still reach a model performance otherwise requiring a much larger training set. AL has been widely applied to image classification [6][7][8], semantic segmentation [9,10] and medical image segmentation [11,12]. These methods have effectively improved accuracy through experts' involvement. However, carotid ultrasound images are user-end protocol dependent, and with high variability in quality, real-world labels on ultrasound images generally share the same characteristics in high variability. Therefore, after testing several state-of-the-art AL methods, we would like to incorporate methodologies from semi-supervised learning designed to extract predictive information from unlabeled data, and between labeled and unlabeled data, for AL.In this work, we propose pseudo-label divergence-based active learning (PLD-AL) to obtain accurate CIM segmentation contributing to the clinical diagnosis of obesity and atherosclerotic disease. As shown in Fig. 1, unlike the conventional AL framework that utilizes one machine learning model, PLD-AL is composed by two networks: the student network is fully trained on the current labeled pool, and the teacher network is weighted upon previous itself and the student one. We use divergence, which measures the distance between two model predictions, to select data for annotation. Furthermore, we use the teacher network to refine the labels to reduce the noise of labels and improve the effectiveness of the next network optimization stage.Our contributions are as follows: we propose PLD-AL, which aims to train segmentation models using a gradually enlarged and refined labeled pool. First, we automatically select and annotate large divergence data between the current and previous AI models, facilitating fast convergence of the AL model to most sound data in the unlabeled pool. Second, we propose a strategy to refine the labels in the labeled pool alternatingly with the proposed label-divergence-based AL algorithm, which improves the robustness compared to the conventional AL approach. We conducted experiments to demonstrate that our method yielded competitive performance gains over other AL methods. Finally, we applied the trained model to a real-world in-house hospital dataset with noisy labels and obtain accurate CIM segmentation results. We release our code at https:// github.com/CrystalWei626/PLD AL."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.0,Method,"Section 2.1 establishes mathematical formulation on the main task of CIM segmentation in our AL framework. Our proposed AL approach has two loops: in Sect. 2.2, the outer loop implements progressive annotation on the automatically selected unlabeled pool; in Sect. 2.3, the inner loop trains the neural networks on the labeled pool and subsequently refines it through a feedback routine."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.1,Mathematical Notations and Formulation,"Denote x ∈ R I×J a carotid ultrasound image and y ∈ R I×J the corresponding CIM mask. Let D L = X L ×Y L and X U be the initial labeled and unlabeled pools, where X L is the carotid ultrasound image set, and Y L is the corresponding label set. We aim to improve generalization ability of the AI model by selecting the most informative data in X U and delivering them to an expert for annotation.We propose a novel AL framework: PLD-AL for CIM segmentation, as illustrated in Fig. 1 and Algorithm 1. First, AI models are trained on D L and used to refine Y L . Then, the AI models select data from X U for expert to annotate, forming a new set of labeled data. Finally, we update D L and X U and use new D L to train the same AI models."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,,Algorithm 1: PLD-AL,"1 Input: Initial labeled pool DL = XL × YL; Unlabeled pool XU ; Judgment threshold τ ; Refining threshold λ; 2 Initialize θS and θT ;Fit the mIoU curve In each AL iteration, we use a mean-teacher architecture as the backbone of AL. The student and the teacher networks, respectively parameterized by θ S and θ T , share the same neural network architecture F , which maps the carotid ultrasound image x ∈ R I×J to the extended three-dimensional CIM mask probability p ∈ R I×J×2 , whose 3rd-dimensional component p ij ∈ R 2 denotes the softmax probability output for binary classification at the pixel (i, j). We use the divergence between pseudo-labels generated by student and teacher networks to assist in selecting data for the expert to annotate."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.2,Outer Loop: Divergence Based AL,"The outer loop is an AL cycle that selects data for the expert to annotate according to the divergence between the predictions of the student and teacher networks. First, we initialize θ S and θ T . We complete the inner loop proposed in Sect. 2.3, and obtain the trained parameters for the student and teacher networks. Then, we select n data from X U for the expert to annotate. We suggest using the Kullback-Leibler (KL) divergence to assist in selecting data, as shown in Eq. ( 1):We consider data prediction uncertainty as a decisive metric for data selection.It is deduced that the KL divergence between the output of the primary and the auxiliary models in a dual-decoder architecture can approximate the prediction uncertainty [13,14]. We compute the KL divergence scores d(x) = mean i,j Div KL (x(i, j), θ S , θ T ) of the data in X U . Let X A be the subset that contains data x in X U corresponding to the top-n largest d(x) values (denoted by TOP n d(x)). With this, we can next obtain the label set Y A in terms of X A by means of the expert's annotates and the required post-processing step; see Sect. 3.1 for details. Lastly, we add the selected dataset with its label set X A × Y A into D L and delete X A from X U . We repeat the above steps until reaching the maximum number of AL iterations."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.3,Inner Loop: Network Optimization and Label Refinement,"The inner loop trains the neural networks by the labeled pool and refines noisy labels through a feedback routine. In the k th epoch of the inner loop, we first use the last labeled pool D L to optimize the training parameter θ (k) S by minibatch stochastic gradient descent. The loss function consists of a supervised loss L sup between labels and predictions of the student model, and a consistency loss L con between the predictions of the student and the teacher models. These can be implemented using the cross-entropy loss and the mean squared error loss, respectively. Then, we update θ (k)T by exponential moving average (EMA) with a decay rate α as Eq. (2):We refine noisy labels based on the idea that the fitness soars sharply at first but slows down after the model begins to fit noise [15]. We interrupt the model training before it begins to fit noise, then refine the labels utilizing the current network output. We calculate the model fitness via a series of the intersection over union (mIoU) [16] scores sampled at every training epoch. To estimate the ratio of change of the model fitness, we fit the mIoU curve M(k) via e.g., the exponential regression formed in Eq. (3) when the length of mIoU series is larger than a designated parameterwhere a, b, and c are the fitting parameters to be determined by least squared estimate. Then we calculate the ratio of change of the model fitness γ k via the derivative of the mIoU curve M (k):When training stops at this epoch k satisfying γ k < τ (τ is a judgment threshold), we lastly predict the CIM mask probability p ij = F (x(i, j)|θT ) via the teacher network for each pixel at (i, j) and update the noisy label y(i, j) in Y L if max p ij > λ (λ is a refining threshold)."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.1,Experiment Settings,"Implementation Details. We used the PyTorch platform (version 1.13.1) to implement our method. And we adapted the same UNet++ [17] structures as the encoding-decoding structures for the student and the teacher networks. We implemented 1000 training iterations with a total mini-batch size of 14 and initial batch size of labeled data of 2 on an Nvidia GeForce RTX 3070 GPU with 8192 MB of memory (Nvidia, Santa Clara, CA, United States). Since the number of labeled data increases after completing each AL iteration, the batch size of labeled data should increase by 2 synchronously to keep the total epoch num unchanged. We used stochastic gradient descent (SGD) as the optimizer with the parameter settings: momentum (0.9) and weight decay (0.0001). We set EMA decay rate α = min{1 -1/(iter + 1), 0.99}, where iter is the current training iteration number. 2021 regions of interest (ROI) of size 256 × 128 were cropped from original carotid ultrasound images for model training using template matching technique [18]. We set the number of AL iterations, fixed labeling budget, initial labeled and unlabeled data, and the test data as 5, 200,159, 1857, and 1204, respectively. During each annotation phase, experts manually marked the CIM boundaries with scatters and we subsequently generated the complete CIM masks via the Akima interpolation method [19]. θ S and θ T was initialized by the pre-train model1 on ImageNet [20]. At our best practice, we chose the hyper-parameters λ = 0.8, τ = 0.005 and K 1 = 1.Dataset. We employed the publicly available Carotid Ultrasound Boundary Study (CUBS) dataset2  [21] and the in-house dataset acquired at Children's Hospital, Zhejiang University School of Medicine. The CUBS dataset contains ultrasound images of the left and right carotid arteries from 1088 patients across two medical centers and three manual annotations of LI and MA boundaries by experts. According to the description of these annotations in the dataset specification, the analytic hierarchy process (AHP) [22] was adapted to weigh the three expert's annotations to obtain accurate labels for testing. We randomly performed morphological transformations (dilation and erosion) by OpenCV [23] on the accurate labels to generate noisy labels for training. The in-house dataset comes from 373 patients aged 6-12, with 2704 carotid ultrasound images. We picked 350 images with visible CIM areas and applied the model trained on CBUS to CIM segmentation. The data acquisition and the experimental protocol have been approved by the institutional review board of Children's Hospital, Zhejiang University School of Medicine.Table 1. Quantitative results of performance comparison, the metrics were calculated over the test dataset and took the mean. Bold font highlights the optimal performance except for the upper limit. The asterisk * denotes p < 0.001 compared with the rest methods. Evaluation Metrics. We utilized dice coefficient (Dice) [24], intersection over union (IoU) [16], average surface distance (ASD), 95% covered Hausdorff distance (95HD) [25], and the average training time of 5 AL iterations as evaluation metrics of the CIM segmentation performance compared to the generated ground truth on the unseen test set."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.2,Performance Comparison,"We evaluated the performance of AL methods on the CIM segmentation task using the CUBS dataset.Baselines. We compared our method to other AL methods, including AL methods with query strategy based on random selection (Random), entropy increase (Entropy) [26], prediction confidence (Confidence) [12], CoreSet [27] and predicted probability diverse contexts (CDAL) [28]. All of the backbones of these baseline methods are fully supervised models. Furthermore, we trained a supervised model by the fully labeled pool with accurate labels yielding an upper limit of generalization ability. We compared this upper limit to the performance of all the methods.Table 1 illustrates the quantitative results of different methods on the test dataset. It shows that our method based on the KL divergence query strategy improves the mean generalization metrics (Dice, IoU, ASD, and 95HD) compared with other AL methods. In particular, it significantly (two-tailed Wilcoxon signed-rank test with p < 0.001) outperforms the others in terms of any metric."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.3,Ablation Study,"We conducted ablation study on the CUBS dataset to demonstrate the importance of the label refinement module proposed in Sect. 2.3. We canceled the label refinement module and substituted the label refinement module with confidence learning (CL) for noise label correction [29].Table 2. Quantitative results of ablation study, the metrics were calculated over the test dataset and took the mean. The abbreviations, Refine and CL, represent the label refinement module and confidence learning [29], respectively. Bold font highlights the optimal performance except for the upper limit. The asterisk * denotes p < 0.001 compared with the rest methods.  Table 2 illustrates the results of ablation study experiment. Our method substantially outperforms the method without the label refinement module and slightly outperforms the method with CL. In particular, it significantly (twotailed Wilcoxon signed-rank test with p < 0.001) outperforms the others in terms of all the metrics. Moreover, the training time of our method is significantly reduced compared to CL since CL needs to estimate the uncertainty during training to correct the noisy data smoothly, which leads to more computational cost."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.4,Application on In-house Dataset,"We applied the teacher network trained in Sect. 3.2 to the in-house dataset acquired at a pediatric hospital. Figure 2 visualizes three example images with different CIM area qualities (clear, mildly ambiguous, severely ambiguous). Qualitatively, the generalization ability of the model trained by our method is much better than those trained by other methods, regardless of image quality. Moreover, as shown in Fig. 2, Random over-estimates the CIM area, while CoreSet, CDAL, and our method produces more conservative results but lost continuity in the severely ambiguous image. Quantitatively, the mean Dice, IoU, ASD, and 95HD of our method are 79.20%, 66.99%, 1.92 voxels, and 6.12 voxels, respectively, indicating a small but rational generalization loss on the in-house data."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,4.0,Conclusion,"We propose a novel AL framework PLD-AL, by training segmentation models using a gradually enlarged and refined labeled pool to obtain accurate and efficient CIM segmentation. Compared with other AL methods, it achieves competitive performance gains. Furthermore, we applied the trained model to an in-house hospital dataset and obtained accurate CIM segmentation results. In the future, we will extend our approach to subsequently calculate CIM thickness and roughness for clinical evaluation of obesity or atherosclerotic disease. We will also investigate the robustness of the proposed method in terms of inter-expert variations and noisy annotation labels. Our approach merely involves one expert in the loop, which may potentially be sensitive to the expert's experience. Multiple experts may consider minimizing inter-reader differences during human-AI interactive labeling [30]."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,1.0,Introduction,"In the past decade, numerous deep learning-based methods for DR staging have been explored and achieved promising results [10,11,19,27]. However, most current studies focus on centralized learning, which necessitates data collection from multiple institutions to a central server for model training. This approach poses significant data privacy security risks. Additionally, in clinical practice, different institutions may have their own DR staging criteria [3]. Consequently, it is difficult for the previous centralized DR staging method to utilize data of varying DR staging criteria to train a unified model.Federated learning (FL) is a collaborative learning framework that enables training a model without sharing data between institutions, thereby ensuring data privacy [15,21]. In the FL paradigm, FedAvg [24] and its variants [1,4,9,16,18,22,23] are widely used and have achieved excellent performance in various medical tasks. However, these FL methods assign each client a static weight for model aggregation, which may lead to the global model not learning sufficient knowledge from clients with large heterogeneous features and ignoring the reliability of each client. In clinical practice, the data distributions of DR datasets between institutions often vary significantly due to medical resource constraints, population distributions, collection devices, and morbidity [25,29]. This variation poses great challenges for the exploration of federated DR staging methods. Moreover, most existing DR staging methods and FL paradigms mainly focus on performance improvement and ignore the exploration of the confidence of the prediction. Therefore, it is essential to develop a new FL paradigm that can provide reliable DR staging results while maintaining higher performance. Such a paradigm would reduce data privacy risks and increase user confidence in AI-based DR staging systems deployed in real-world clinical settings.To address the issues, we propose a novel FL paradigm, named FedUAA, that employs a personalized structure to handle collaborative DR staging among multiple institutions with varying DR staging criteria. We utilize uncertainty to evaluate the reliability of each client's contribution. While uncertainty is a proposed measure to evaluate the reliability of model predictions [12,14,28,30], it remains an open topic in FL research. In our work, we introduce a temperaturewarmed evidential uncertainty (TWEU) head to enable the model to generate a final result with uncertainty evaluation without sacrificing performance. Additionally, based on client uncertainty, we developed an uncertainty-aware weighting module (UAW) to dynamically aggregate models according to each client's uncertainty score distribution. This can improve collaborative DR staging across multiple institutions, particularly for clients with large data heterogeneity. Finally, we construct a dataset for federated DR staging based on five public datasets with different staging criteria from various institutions to satisfy the real non-iid condition. The comprehensive experiments demonstrate that FedUAA provides outstanding DR staging performance with a high degree of reliability, outperforming other state-of-the-art FL approaches. "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.0,Methodology,"Figure 1 (a) shows the overview of our proposed FedUAA. During training, local clients share the encoder (ϕ) to the cloud server for model aggregation, while the TWEU (ψ) head is retained locally to generate DR staging results with uncertainty evaluation based on features from the encoder to satisfy local-specific DR staging criteria. The algorithm of our proposed FedUAA is detailed in Supplementary A. Therefore, the target of our FedUAA is:where £ is the total loss for optimizing the model, f i is the model of i -th client, while X i and Y i are the input and label of i -th client. Different from previous personalized FL paradigms [2,4], our FedUAA dynamically adjusts the weights for model aggregation according to the reliability of each client, i.e., the client with larger distributional heterogeneity tends to have larger uncertainty distribution and should be assigned a larger weight for model aggregation to strengthen attention on the client with data heterogeneity. Besides, by introducing TWEU, our FedUAA can generate a reliable prediction with an estimated uncertainty, which makes the model more reliable without losing DR staging performance."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.1,Temperature-Warmed Evidential Uncertainty Head,"To make the model more reliable without sacrificing DR staging performance, we propose a novel temperature-warmed evidence uncertainty head (TWEU), which can directly generate DR staging results with uncertainty score based on the features from the encoder. The framework of TWEU is illustrated in Fig. 1 (b). Specifically, we take one of the client models as an example and we assume that the staging criteria of this client is K categories. Correspondingly, given a color fundus image input, we can obtain its K +1 non-negative mass values, whose sum is 1. This can be defined as, where b i ≥ 0 is the probability of i -th category, while u represent the overall uncertainty score. Specifically, as shown in Fig. 1 (b), a local fully connected layer (FC) is used to learn the local DR category-related features F V , and the Softplus activation function is adopted to obtain the evidence E = [e 1 , ..., e K ] of K staging categories based on F V , so as to ensure that its feature value is greater than 0. Then, E is re-parameterized by Dirichlet concentration [5], as: is the Dirichlet intensities. Therefore, the probability assigned to category k is proportional to the observed evidence for category k. Conversely, if less total evidence is obtained, the greater the uncertainty score will be. As shown in Fig. 1  "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.2,Uncertainty-Aware Weighting Module,"Most existing FL paradigms aggregate model parameters by assigning a fixed weight to each client, resulting in limited performance on those clients with large heterogeneity in their data distributions. To address this issue, as shown in Fig. 1 (a), we propose a novel uncertainty-aware weighting (UAW) module that can dynamically adjust the weights for model aggregation based on the reliability of each client, which enables the model to better leverage the knowledge from different clients and further improve the DR staging performance. Specifically, at the end of a training epoch, each client-side model produces an uncertainty value distribution (U ), and the ground truth for incorrect prediction of U GT also can be calculated based on the final prediction P by,where P i and Y i are the final prediction result and ground truth of i -th sample in local dataset. Based on U and U GT , we can find the optimal uncertainty score θ, which can well reflect the reliability of the local client. To this end, we calculate the ROC curve between U and U GT , and obtain all possible sensitivity (Sens)and specificity (Spes) values corresponding to each uncertainty score (u) used as a threshold. Then, Youden index (J) [7] is adopted to obtain the optimal uncertainty score θ by:More details about Youden index are given in Supplementary B. Finally, the optimal uncertainty scores Θ = [θ 1 , ..., θ N ] of all clients are sent to the server, and a Softmax function is introduced to normalize Θ to obtain the weights for model aggregation as w i = e θi / N i=1 e θi . Therefore, the weights for model aggregation are proportional to the optimal threshold of the client. Generally, local dataset with larger uncertainty distributions will have a higher optimal uncertainty score θ, indicating that it is necessary to improve the feature learning capacity of the client model to further enhance its confidence in the feature representation, and thus higher weights should be assigned during model aggregation."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,3.0,Loss Function,"As shown in Fig. 1 (b), the loss function of client model is:where  (5) where Φ(•) is the digamma function, while β (α) is the multinomial beta function for the Dirichlet concentration parameter α. Meanwhile, the KL divergence function is introduced to ensure that incorrect predictions will yield less evidence:where Γ (•) is the gamma function, while α = y + (1y) α represents the adjusted parameters of the Dirichlet distribution which aims to avoid penalizing the evidence of the ground-truth class to 0. In summary, the loss function L Uce for the model optimization based on the features that were parameterized by Dirichlet concentration is as follows: where λ is the balance factor for L KL . To prevent the model from focusing too much on KL divergence in the initial stage of training, causing a lack of exploration for the parameter space, we initialize λ as 0 and increase it gradually to 1 with the number of training iterations. And, seen from Sect. 2.1, Dirichlet concentration alters the original feature distribution of F v , which may reduce the model's confidence in the category-related evidence features, thus potentially leading to a decrease in performance. Aiming at this problem, as shown in Fig. 1 (b), we introduce temperature coefficients to enhance confidence in the belief masses, and the loss function L T ce to guide the model optimization based on the temperature-warmed belief features b T is formalized as:"
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,4.0,Experimental Results,"Dataset and Implementation: We construct a database for federated DR staging based on 5 public datasets, including APTOS (3,662 samples)1 , Messidor (1,200 samples) [6], DDR (13,673 samples) [20], KaggleDR (35,126 samples) (DRR)2 , and IDRiD (516 samples) [26], where each dataset is regarded as a client, More details of datasets are given in Supplementary C.We conduct experiments on the Pytorch with 3090 GPU. The SGD with a learning rate of 0.01 is utilized. The batch size is set to 32, the number of epochs is 100, and the temperature coefficient τ is empirically set to 0.05. To facilitate training, the images are resized to 256 × 256 before feeding to the model. Performance for DR Staging: Table 1 shows the DR staging AUC for different FL paradigms on different clients. Our FedUAA achieves the highest AUC scores on all clients, with a 1.48% improvement in average AUC compared to FedBN [23], which achieved the highest average AUC score among the compared methods. Meanwhile, most FL based approaches achieve higher DR staging performance than SingleSet, suggesting that collaborative training across multiple institutions can improve the performance of DR staging with high data privacy security. Moreover, as shown in Table 1, FL paradigms such as FedDyn [1] and SCAFFOLD [16] exhibit limited performance in our collaborative DR staging task due to the varying staging criteria across different clients, as well as significant differences in label distribution and domain features. These results indicate that our FedUAA is more effective than other FL methods for collaborative DR staging tasks. Furthermore, although all FL methods achieve comparable performance on APTOS and DDR clients with distinct features, our FedUAA approach significantly improves performance on clients with small data volumes or large heterogeneity distribution, such as DRR, Messidor, and IDRiD, by 1.27%, 1.33%, and 1.29% over suboptimal results, respectively, which further demonstrates the effectiveness of our core idea of adaptively adjusting aggregation weights based on the reliability of each client. In addition, we also conduct experiments demonstrate the statistical significance of performance improvement. As shown in Supplementary D, most average p-values are smaller than 0.05. These experimental results further prove the effectiveness of our proposed FedUAA.Reliability Analysis: Providing reliable evaluation for final predictions is crucial for AI models to be deployed in clinical practice. As illustrated in Fig. 2 (b), the model without introducing uncertainty (Backbone) assigns high probability values for incorrect staging results without any alert messages, which is also a significant cause of low user confidence in the deployment of AI models to medical practices. Interestingly, our FedUAA can evaluate the reliability of the final decision through the uncertainty score. For example, for the data with obvious features (Fig. 2 (a)), our FedUAA produces a correct prediction result with a low uncertainty score, indicating that the decision is reliable. Conversely, even if our FedUAA gives an incorrect decision for the data with ambiguous features (Fig. 2 (b)), it can indicate that the diagnosis result may be unreliable by assigning a higher uncertainty score, thus suggesting that the subject should seek a double-check from an ophthalmologist to avoid mis-diagnosis. Furthermore, as shown in Fig. 2 (c), we degraded the quality of the input image by adding different levels of Gaussian noise σ 2 to further verify the robustness of FedUAA. Seen from Fig. 2 (c), the performance of all methods decreases as the level of added noise increases, however, our FedUAA still maintains a higher performance than other comparison methods, demonstrating the robustness of our FedUAA.Ablation Study: We also conduct ablation experiments to verify the effectiveness of the components in our FedUAA. In this paper, the pre-trained ResNet50 [13] is adopted as our backbone (BC) for SingleSet DR staging, while employing FedBN [23] as the FL BC. Furthermore, most ensemble-based [17] and MC-dropout-based [8] uncertainty methods are challenging to extend to our federated DR staging task across multiple institutions with different staging criteria. Therefore, we compare our proposed method with the commonly used evidential based uncertainty approach (EU (L Uce )) [12].For training model with SingleSet, as shown in Table 2, since Dirichlet concentration alters the original feature distribution of the backbone [12], resulting in a decrease in the model's confidence in category-related evidence, consequently, a decrease in performance when directly introducing EU (BC+EU (L Uce )) for DR staging. In contrast, our proposed BC+TWEU (L Uce +L T ce ) achieves superior performance compared to BC and BC+EU (L Uce ), demonstrating that TWEU (L Uce +L T ce ) enables the model to generate a reliable final decision without sacrificing performance. For training model with FL, as shown in Table 2, BC+FL outperforms SingleSet, indicating that introducing FL can effectively improve the performance for DR staging while maintaining high data privacy security. Besides, FL+EU (L Uce ) and FL+TWEU (L Uce +L T ce ) also obtain a similar conclusion as in SingleSet, further proving the effectiveness of TWEU. Meanwhile, the performance of our FedUAA (FL+TWEU (L Uce +L T ce )+UAW) achieves higher performance than FL+TWEU (L Uce +L T ce ) and FL backbone, especially for clients with large data distribution heterogeneity such as DRR, Messidor, and IDRiD. These results show that our proposed UAW can further improve the performance of FL in collaborative DR staging tasks."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,5.0,Conclusion,"In this paper, focusing on the challenges in the collaborative DR staging between institutions with different DR staging criteria, we propose a novel FedUAA by combining the FL with evidential uncertainty theory. Compared to other FL methods, our FedUAA can produce reliable and robust DR staging results with uncertainty evaluation, and further enhance the collaborative DR staging performance by dynamically aggregating knowledge from different clients based on their reliability. Comprehensive experimental results show that our FedUAA addresses the challenges in collaborative DR staging across multiple institutions, and achieves a robust and reliable DR staging performance."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_21.
Debiasing Medical Visual Question Answering via Counterfactual Training,1.0,Introduction,"Medical visual question answering (Med-VQA) has attracted considerable attention in recent years. It seeks to discover the plausible answer by evaluating the visual information of a medical image and a clinical query regarding the image. The Med-VQA technology can considerably enhance the efficiency of medical professionals and fulfill the growing demand for medical resources [15,25]. However, numerous researches have found that general VQA models are significantly influenced by superficial linguistic correlations in training set, lacking adequate visual grounding [9,14,26]. Since most of the existing Med-VQA datasets [12,16] are manually spitted and annotated, the spurious over-reliant bias factor also exists in Med-VQA, as the Fig. 1 shown. Recent general VQA works dedicate to reducing the language priors through enhancing the visual information [23,27] or data balancing [11,13], there is bare attempt to prevent the language priors in medical domain. Current Med-VQA works [4,5,15,17] devote to construct effective models and most Med-VQA datasets [12,16] simply balance the medical images to mitigate the inherent bias. These works all neglect the cheating factors that the Med-VQA models typically resort to linguistic distributions priors, consequently ignoring the semantic clinic objects. This problem accordingly leads to disastrous results in clinic application consequences.Therefore, we propose a novel unbiased and interpretable Med-VQA model and preliminarily construct a bias-sensitive Med-VQA dataset to address the problems mentioned above. First, with the aim of forcing the model to focus on clinic objects rather than superficial correlations, we prepare the counterfactual samples by masking clinic words with ""[MASK]"" tokens and meanwhile assign the irrelevant answers for implicit bias-weaken. Further, for explicitly reducing the linguist bias, we treat the language bias as the causal effect of the clinic Train What organ system is pictured?What organ system is pictured? Baseline: Chest DeBCF: Neck Answer: Chest Fig. 1. The baseline generates incorrect answer ""Chest"" relying on the majority prior ""Chest"" in train-set of the publicly available SLAKE [16] dataset rather than real semantic image objects. The proposed DeBCF overcomes the language priors and generates the reliable answer with correct semantic parts. question on the generated answer and then subtract it from the total causal effect for counterfactual training. It is noted that both the original data and generated counterfactual data will be used for counterfactual training. In this way, the model may not tend to provide answers over-rely on the largest proportions of candidate answers in train-set when tested, thus concentrating on entanglement of the visual objects and language information.Additionally, we conduct a bias-sensitive Med-VQA dataset Semantically-Labeled Knowledge-Enhanced-Changing Priors (SLAKE-CP) for evaluating the ability of disentangling the memorized linguist priors and semantic visual information. Qualitative and quantitative experimental results illustrate that our proposed model is superior to the state-of-the-art Med-VQA models on the two public benchmarks and can obtain more obvious improvements on the newly constructed SLAKE-CP."
Debiasing Medical Visual Question Answering via Counterfactual Training,2.0,Methodology,Figure 2 illustrates the proposed Med-VQA method which consists of implicit and explicit counterfactual debiased stages: the counterfactual training data preparation stage to improve the sensitivity of the critical clinic objects for implicit bias-weaken. Along with the counterfactual causal effect training stage to directly migrate the language priors.
Debiasing Medical Visual Question Answering via Counterfactual Training,2.1,Counterfactual Training Data Preparation,"To implicitly weaken the language bias, we follow CSS [3] to prepare counterfactual training samples for improving the sensitivity of clinic objects. First, we extract the question type (e.g. ""Where"" in Fig. 2) of each question and calculate the importance s of the remaining words w i in clinical question q to the label a as: s(a, w i ) = S(P (a|q, v),where P (a|q, v) represents the probability of predicting answer a through Med-VQA model with image v and question q, ∇ is the gradient operator, S is the cosine similarity, and 1 is the all-ones vector. The top-K clinic words with the highest importance s are defined as critical words. Then, we construct counterfactual samples Q -by replacing the critical words with ""[MASK]"". We also assign the Q -with an answer A -, and the detailed assigning procedure is as follows. We first generate the probability of predicting answer P + (a) with the question Q + which replaces the marginal words with ""[MASK]"" (all but the question type labels and the critical words), and then pick up top-N candidate answers with the highest probability as A + . The rest answers are denoted as"
Debiasing Medical Visual Question Answering via Counterfactual Training,2.2,Counterfactual Cause Effect Training Procedure,"For explicitly subtracting the language priors, following [24], we introduce casual effect [21] to translate priors into quantified expressions. The causal effects can directly reflect the comparisons between the outputs with different treatments (e.g. X = x represents with-treatment and X = x * represents the counterfactual situation where is without the treatment). The total effect (TE) of X = x on Y can be defined as two different conditions that with or without the input:where M is the mediator between the variables X and Y . Note that the total effect can be composed of the natural direct effect (NDE) and total indirect effect(TIE). Between them, the NDE concentrates the exclusive effect of X = x on Y and prohibit the effect through M :Thus TIE can reflect the reduction of language bias by subtracting the NDE from the TE:Based on the above definition, we translate the Med-VQA task into a causal effect graph as Fig. 2 (c)(d) shown, aiming to directly formulate the language bias and subtract it. The answer set A = {a} is caused by direct effect from medical image V = v and clinic question Q = q, also the indirect effect of fusion knowledge K(Q = q, V = v) through the cross-modal fusion module. We define the notations that:Through subtracting NDE of Q = q on A from the TE of V = v, Q = q and K = k on the answer, we can explicitly capture language bias and remove it via TIE, which is defined below. In the inference stage, we choose the answer with the maximum TIE as the prediction.where k * = K(V = v * , Q = q * ), v * and q * is the counterfactual situation where model is without v, q as inputs. The Y q ,v ,k = log σ(Z q + Z v + Z k ), where the [20]:where L V QA , L QA and L V A are corss-entropy losses over Y q ,v ,k , Z q and Z v .The complete objective of our method is optimized to minimize the L DeBCF which combines the L cls over both the original and the counterfactual data:where α is the hyperparameter which control the ratio of counterfactual samples."
Debiasing Medical Visual Question Answering via Counterfactual Training,3.0,SLAKE-CP: Construction and Analysis,"For further evaluating the debiasing ability of Med-VQA, we follow VQA-CP [1] to create a bias-sensitive Med-VQA dataset which can be called SLAKE-CP. The SLAKE-CP can be further adopted by future debiased Med-VQA researches.Grouping. We first construct all image-question-answer samples of train-set and test-set in SLAKE [16] into a whole set together. We start by labeling each question with a question type (first few words). If the samples have the same question type and answer, then these samples can be divided into same group.Re-Splitting. We re-split the SLAKE [16] dataset to construct disparate distribution as Fig. 2 (a) shows. In detail, we first assign 1 group to the test-set. Among the remaining groups, if there is a group with a different question type or answer from the groups in test-set, this group will be assigned to test-set otherwise to train-set, aiming to vary the prior distributions of the train and test while remaining unchanged distributions of the images. The iteration stops when the test-set approximately reaches 1/7rd of the whole set, and the remaining are added to the train-set. We ensure the newly constructed test-set and train-set cover the majority of question types (""Is"", ""What"", ""Where"", ""Which"", etc.) after these procedures. Most of the data attributes of SLAKE-CP are consistent with SLAKE, such as the train-test splitting, and open-close type splitting. "
Debiasing Medical Visual Question Answering via Counterfactual Training,4.1,Datasets and Implementation Details,"Datasets. SLAKE [16] is a knowledge-augmented Med-VQA dataset, consisting of 642 images and 7033 question-answer samples. The VQA-RAD [12] is a manually annotated dataset validated by clinicians, which contains 315 radiographic images and 3,515 question-answer samples. We followed the original data partition, where questions are divided into closed-ended and open-ended types.Implementation Details. For implementation, we apply Pytorch library with 6 NVIDIA TITAN 24 GB Xp GPUs. We employ the MEVF [19] as baseline.  The vision encoders are initialized by MAML [7] and CDAE [18], and LSTM is adopted as question encoder. The BAN [10] is adopted as the fusion module E F . The medical images are resized into 224 × 224, and questions are cut to 12 words and then embed into 300 dimensions through Golve. The proposed model is trained for 200 epochs with 64 batch size and optimized with Adam whose learning rate is 1e -3 . In Sect. 2.1, we choose top-1 candidate answer as A + and mask top-1 critical clinic word, the hyperparameter α is set to 0.6."
Debiasing Medical Visual Question Answering via Counterfactual Training,4.2,Experimental Results,"Comparison with State-of-the-Art Methods. We compare our DeBCF with 10 state-of-the-art Med-VQA models on the SLAKE [16] and VQA-RAD [12] public benchmarks as the Table 1 shown. The proposed model obviously outperform the existing advanced models, attaining 82.6% and 71.6% mean accuracy respectively. Specifically, the results of our proposed model have prominent improvements over the attention-based models MFB [29], SAN [28], BAN [10]. Further, the improvements over MEVF+BAN [30] and CPRD+BAN [15] which adopt the same fusion model BAN [10] as ours are 4.0%, 1.5% overall accuracy on SLAKE respectively. In particular, the proposed model conspicuously improved the overall accuracy by 2.5% and compared with the advanced CLIPQCR [6]. Moreover, our model has significant superior with other debiasing models, including RUBi [2] and LPF [13], GGE [8]. Although these works can effectively reduce language bias, they reckon without visuallinguist explicable information and contrarily weaken the inference ability. For ours, we explicitly subtract the language bias through causal effect and generate counterfactual samples to implicitly improve the sensitivity of clinical words and visual objects for inference.Discussion of SLAKE-CP. Table 2 illustrates the superiority of the DeBCF on the newly constructed SLAKE-CP datasets which is the linguistic-bias sensitive evaluation. In particular, the DeBCF yields 34.2% mean overall accuracy on SLAKE-CP datasets. The performance of all the models has prominently dropped in the newly unbiased SLAKE-CP datasets compared with the SLAKE. It is obviously observed that the DeBCF significantly outperforms the baselines [10,19,28,29] and the debiasing methods [2,8,13]. The proposed model is also superior to the advanced models CLIPQCR [6] and CPRD+BAN [15], over-passing 4.2% and 3.8% overall accuracy. Within the bias-sensitive benchmarks, the comparisons demonstrate that our model may have the superiority to overcome the linguistic priors and force the model to generate more creditable answers rather than taking the superficial linguistic correlations as a shortcut.Ablation Analysis. Table 3 demonstrates the ablation study which verifies the effectiveness of devised methods. We adopt MEVF+BAN [10] as the baseline in index 1. The baseline equipped with the counterfactual data preparation stage gains 1.5% and 0.6% overall accuracy on SLAKE-CP and SLAKE datasets. This illustrates that masking critical clinical objects contributes to the implicit suppression of linguistic bias in Med-VQA. In addition, the comparison between index 3 and 1 demonstrates that subtracting the cause-effect of the question can explicitly weaken the prior and modifies the model to focus on intrinsically meaningful objects rather than superficial counterfactual correlations. Moreover, the model which combines the counterfactual masking samples into the counterfactual causal effect training procedure in index 4 obtains significant gains by up to 5.2% and 2.8% overall accuracy on SLAKE-CP and SLAKE, illustrating that we have built a robust unbiased Med-VQA model to overcome language priors."
Debiasing Medical Visual Question Answering via Counterfactual Training,,Influence of Hyperparameters.,"The influence results of the top-K and the hyperparameter α are conducted in Table 4 and 5, which reveal that choosing top-1 critical words and α = 0.6 achieves the best performance respectively. Crucially, masking top-1 critical clinic word can disentangle the linguistic bias and redundancy masking may result in interference.Quantitative Analysis. As Fig. 3 shown, we conduct a quantitative comparison analysis to illustrate the ability to disengage the language prior to our proposed model through Grad-CAM maps [22]. For example 1 in row 1, the proposed DeBCF sensitively recognizes the precise critical keywords ""Where, is, liver"" and corresponding visual image objects to predict the correct answer with the highest probability score, while the advanced model MEVF+BAN [19] is subjected to the language prior that generate the wrong answer according to the superficial context ""Where is"" and ignore the reliable visual objects. Additionally, we also conduct the comparison of sensitivity to the visual grounds in Fig. 4.Given the same question but different medical images and answers, the proposed model correctly predict the various answers while the MEVF+BAN [19] fails. The detailed comparisons illustrate the debiased ability of the proposed model to overcome language priors and ingeniously grasp the critical parts (clinic keywords and visual objects) for a precise explanation."
Debiasing Medical Visual Question Answering via Counterfactual Training,5.0,Conclusion,"In this paper, we propose a novel debiasing Med-VQA model that prepares the counterfactual data by masking critical clinic words and combines it into the counterfactual training stage which subtracting the causal effect of language priors directly, aiming to migrate the linguistic-bias in Med-VQA. Additionally, we construct a linguistic-bias sensitive Med-VQA dataset SLAKE-CP by disintegrating the language priors from training. Experimental results demonstrate the superior debiasing and interpretive performance of the proposed model. It's the first attempt to construct a preliminary bias-sensitive Med-VQA dataset, which will be elaborated in our future work. The codes will be released."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,1.0,Introduction,"Federated learning (FL), allowing decentralized data sources to train a unified deep learning model collaboratively without data sharing, has drawn great attention in medical imaging due to its privacy-preserving properties [13,22,25,40]. Existing studies of FL mainly focus on data heterogeneity across clients [19,20,31], while ignoring the widely-existed class imbalance problem in medical scenarios. In clinical practice, the number of samples for different diseases may vary greatly due to varying incidence rates in the population. When conducting FL on cooperative medical institutions with global class-imbalanced data, the global model may suffer from significant performance degradation, which typically manifests as the recognition accuracy of minority classes (e.g. rare diseases) being lower than that of majority classes (e.g. common diseases) [34]. Deploying such a biased global/federated model is fatal, especially for misdiagnosing a rare disease [15,42]. Therefore, addressing class imbalance in federated learning is of great value.Several FL frameworks have been proposed to tackle imbalanced data [9,41]. Following re-weighting [7], Wang et al. [39] presented a weighted form of cross entropy loss named ratio loss depending on a balanced auxiliary dataset for the server to calculate weights. Sarkar et al. [33] introduced focal loss [24] to up-weight hard samples. CLIMB [35] assigned larger weights to clients more likely to own minority classes via a meta-algorithm. Inspired by decoupling [17], CReFF [34] retrained a new classifier with balanced synthetic features in the server. All these methods aim to balance classes from the classifier perspective without exploring better representations with class-imbalanced data for performance improvement.In this paper, we formulate the effect of class imbalance in FL into the attribute bias and the class bias [37]. The attribute bias means minority classes have more imbalanced background attributes in their class-specific attributes compared to majority classes, making them less distinguishable. The class bias represents the difference in prior probabilities across classes, resulting in biased predictions toward majority classes. To handle the two biases, we present a new class-balancing FL method named FedIIC from two perspectives: feature learning and classifier learning. The key idea of FedIIC is to alleviate the two biases through the calibration of the feature extractor and the classifier. Specifically, two-level supervised contrastive learning [18], i.e. intra-and inter-client contrastive learning, is built to calibrate the feature extractor for better feature learning. For classifier learning, difficulty-aware logit adjustment is adopted to calibrate the classifier dynamically for better decision boundaries. Extensive comparison experiments on both real-world and simulated multi-source data validate FedIIC's effectiveness.The main contributions are summarized as follows. (1) A new viewpoint of realistic medical FL scenarios where global training data is class-imbalanced. (2) A novel privacy-preserving framework FedIIC for balanced federated learning.(3) Superior performance in dealing with class imbalance under both real-world and simulated multi-source decentralized settings."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.1,Preliminaries and Overview,"Considering a typical FL scenario for multi-class image classification with K participants, each participant is assumed to own a private datasetwhere N k is the data amount of D k , and denote each image-label pair as). The goal of FL is to Assuming each image has two kinds of latent attributes, i.e. Z c and Z a , representing the class-specific attributes (determining the category of the image, e.g. texture, color, etc.) and the variant background attributes (e.g. brightness, contrast, etc.) respectively [37], based on the Bayes theorem, the posterior probability of classification can be formulated aswhere the last two items represent the attribute bias and the class bias respectively, which widely exist in class-imbalanced data and affect the posterior probability. For robust FL with class-imbalanced data, the key idea is to alleviate the two biases simultaneously, instead of focusing on the latter as [34]. Hence, we propose FedIIC to address class imbalance from the two perspectives as illustrated in Fig. 1. Details are presented in the following."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.2,Intra-Client Contrastive Learning,"Limited local data affects data diversity (i.e., limited (Z c , Z a ) combinations), especially for minority classes, making Z c less distinguishable. To emphasize more on the learning of Z c , supervised contrastive learning (SCL), proven to be effective for representation learning [16,21,27,45], is introduced in local training.The basic loss function of SCL can be formulated aswhere I denotes the index set of the multi-view batch generated by different augmentations (e.g. the two views in Fig. 1 ), |•| measures the number of elements in a set, A(i) = I\{i}, P (i) = {s ∈ A(i)|y s = y i }, τ represents the temperature, and z denotes the l 2 -normalized embedding of a sample x. Note that in this paper, we use a 2-layer MLP h(•) to obtain z before it is normalized as [3], i.e.. In the multi-view batch, L SCL keeps the embeddings of the same class closer while pushing the embeddings of different classes further away, which helps the model learn better Z c of each class due to richer Z a . However, SCL can not perfectly address class imbalance as the majority classes would benefit more from Eq. 2 following traditional training losses (e.g. the cross entropy loss).To overcome this problem, we propose to employ a dynamic temperature τ := P τ = (p i p j ) t τ in Eq. 2 inspired by [16,45], where p i is the prior probability of class i in the local dataset and t is a parameter set as 0.5 by default. Hence, the loss function is rewritten asnamed intra-client contrastive learning. Through P , sample pairs of the minority classes are up-weighted compared to those of the majority classes, leading to better balance."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.3,Inter-client Contrastive Learning,"Given limited local data under FL, the effectiveness of intra-client contrastive learning may be bounded. How to better utilize cross-client data from the global perspective is crucial for further performance improvement. Inspired by learning from prototypes [4,12,31], we propose inter-client contrastive learning. Assuming a set of shared class-wise prototypes V = {v 1 , v 2 , ..., v L } across clients, the local model can be trained bywhere y i is the label of sample i. When minimizing L Inter , the embedding of each sample will get closer to the prototype of the same class while farther from the prototypes of different classes, encouraging local models to learn common attributes (i.e. class-specific attributes) for samples with the same classes.To this end, how to produce high-quality prototypes is the key to interclient contrastive learning. In previous studies, one common method to generate prototypes is uploading and aggregating local information. For example, Mu et al. [31] and Chen et al. [4] uploaded features to the server directly to generate prototypes. However, it may cause privacy leakage under well-designed attacks and will introduce extra communication costs. Different from these methods, in FedIIC, we propose a new method to generate global prototypes without uploading extra information. Considering that the essence of linear classification is similarity calculation based on vector inner product, the weights of a welltrained linear classifier are nearly co-linear with the feature vectors of different classes [11,32,45]. Therefore, the weights of a linear classifier denoted as W = {w 1 , w 2 , ..., w L }, can represent the corresponding features of L classes learned by the feature extractor g(•) to some extent. Specifically, given a global model [f g (•), g g (•), h g (•)] after model aggregation in the server, the weights of g g (•) are fed to h g (•) to calculate the initial prototypes V = { v 1 , v 2 , ..., v L } as shown in Fig. 1. Considering that features of different classes should have low inter-class similarity, we further fine-tune V via gradient descent byIn this way, the cosine similarity of any ( v i , v j ) pair in V is minimized to be equal, resulting in V with lower inter-class similarity. This operation is called orthogonalization. Finally, the class-wise prototypes V are defined as the element-wise l 2 -normalization of V and are sent to clients for inter-client contrastive learning."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.4,Difficulty-Aware Logit Adjustment,"After calibrating the feature extractor g(•), one common method to calibrate the linear classifier f (•) is logit adjustment (LA) [2,30] to alleviate the impact of class imbalance in local training. Specifically, Zhang et al. [43] proposed to add per-class margins to logits and re-compute the cross entropy (CE) loss bywhere δ y denotes the positive per-class margin and is inversely proportional to the local class frequency p(y). In this way, during local training, the logits of minority classes will increase to compensate for the item, which in turn trains the model to emphasize more on minority classes. However, the frequency-dependent margin may not be appropriate for medical data. For instance, some disease types/classes may have large intra-class variations and are difficult to diagnose even with a large amount of data, which may result in even smaller per-class margins. To address this, in FedIIC, the per-class margin is calculated based on not only the class frequency but also difficulties inspired by [44]. Specifically, we define δ y := log([l ce (y)] q /p(y)), where l ce (y) is the average CE loss of all samples belonging to class y in any round and q is a hyper-parameter set as 0.25 by default. l ce (y) is calculated as follows. At any round r, the total sample number of class y, denoted as N y r , belonging to clients of communication is first calculated. After receiving the global model from the server and before local training, each client i uploads l i ce (y), i.e. the total loss of class y, to the server. Finally, l ce (y) is calculated as 1 N y r i l i ce (y). This process to calculate average loss value can be privacy-preserving under the existing secure multiparty computation framework based on homomorphic encryption [35]. Based on the newly defined δ y , Eq. 6 is renamed as L DALA . Note that the calculation of L DALA does not rely on the multi-view batch like L Intra and L Inter . For a fair where k 1 and k 2 are trade-off hyper-parameters. After minimizing L during the local training phase of each client, the global model is updated by FedAvg [28]."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.0,Experiments,"Datasets. Three FL scenarios with class-imbalanced global data are used for evaluation, which are described as follows:1. Real Multi-Source Dermoscopic Image Datasets (denoted as Real ) consisting of five data sources from three datasets, including PH 2 [29], Atlas [1], and HAM10000 [38] where each source is treated as an individual client. For evaluation, we construct a separate test set by randomly sampling from the training set of ISIC 2019 [5,38] and ensure that the test set has no overlap with the above five data sources. 2. Intracranial Hemorrhage Classification (denoted as ICH ). The RNSA ICH dataset [10], containing five ICH subtypes, is adopted for experiments. The same pre-processing strategies in [14,26] are adopted, and images with only one single hemorrhage type are selected. Following [14,26], data is split according to 7:1:2 for training, validation, and testing respectively. To simulate heterogeneous multi-source data, following [34], Dirichlet distribution, i.e. Dir(α = 1.0), is used to divide the training set to 20 clients. "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.0,Skin Lesion Classification (denoted as ISIC ).,"The training data of ISIC 2019 [5,38], containing eight classes, is used for evaluation. Following [14,26], we split the dataset by 7:1:2 for training, validation, and testing respectively. Similarly, Dirichlet distribution, i.e. Dir(α = 1.0), is used to generate highly heterogeneous data partitions of 10 clients.Data distributions of the three training settings are illustrated in Fig. 2, and imbalance ratios are 35.43, 19.59 and 57.60, respectively.Implementation Details. EfficientNet-B0 [36], pre-trained by ImageNet [8], is adopted as the backbone trained by an Adam optimizer with betas as 0.9 and 0.999, a weight decay as 5e-4, constant learning rates of 1e-4 for Real and 3e-4 for both ICH and ISIC, and a batch size of 32. For ICH, the multi-view batch for contrastive learning is generated by following [14,26]. For both Real and ISIC, the multi-view batch is generated by 1) RandAug [6] and 2) SimAugment [3]. The hyper-parameters k 1 and k 2 in Eq. 7 are set as 2.0. For federated training, the local training epoch is set as 1 and the global training round is set as 200 for ICH and ISIC and 30 for Real. At each round, all clients (i.e., 100%) are included for model aggregation."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.1,Comparison with State-of-the-Art Methods,"Ten related approaches are included for comprehensive comparison, including FedAvg [28], FedProx [20] addressing data heterogeneity, MOON [19] and Fed-Proc [31] utilizing contrastive learning in FL, FedFocal [33] utilizing focal loss  [24] for balancing, FedRS [23] addressing the class-missing problem, FedLC [43] applying frequency-dependent logits adjustment in FL, PRR-Imb [4] training personalized models with heterogeneous and imbalanced data, and CLIMB [35] and CReFF [34] addressing class-imbalance global data in FL. All the methods share the same experimental details described above for a fair comparison. More implementation details and visualization results can be found in supplemental materials. Following the ISIC 2019 competition, balanced accuracy (BACC) is used as the primary metric for class-imbalanced testing sets. Two key metrics in classification, i.e. F1 score (F1) and accuracy (ACC) are also employed for evaluation. Comparison results are summarized in Table 1. As can see, FedIIC achieves the best performance against all previous methods across the three metrics, outperforming the second-best approach (CReFF) by 3.99%, 7.32%, and 2.01% in BACC on Real, ISIC, and ICH respectively."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.2,Ablation Study,"To validate the effectiveness of each component in FedIIC, a series of ablation studies are conducted on ISIC and ICH following the same experimental details described in Sect. 3. Quantitative results are summarized in Table 2. Under severe global imbalance, FedAvg is struggling. With the introduction of DALA, the performance is improved in BACC but degraded in F1. It is consistent with the quantitative results between CReFF and FedAvg on ICH in Table 1, indicating the limitation of only eliminating class bias through classifier calibration while ignoring attribute bias. The above results validate the necessity of addressing the imbalance in feature learning for performance improvement. Therefore, introducing either intra-or inter-client contrastive learning for better representation learning under class imbalance is beneficial in both BACC and F1. By combining all the components, FedIIC achieves the best overall performance, outperforming FedAvg with large margins.Ablation studies of hyper-parameters in FedIIC are conducted on ISIC as stated in Table 3. Setting t = 0 encounters noticeable performance degradation, indicating the necessity of dynamic temperatures based on class priors in intraclient contrastive learning. Meanwhile, the performance gap between the initial prototypes V with and without orthogonalization validates the effectiveness of reducing inter-class similarity in prototypes. When introducing difficulty to logit adjustment (i.e., d = 0.25), we observe an increase in BACC and a decrease in F1, which is consistent with the above analysis in Table 1 (i.e., CReFF vs. FedAvg)."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,4.0,Conclusion,"This paper discusses a more realistic federated learning (FL) setting in medical scenarios where global data is class-imbalanced and presents a novel framework FedIIC. The key idea behind FedIIC is to calibrate both the feature extractor and the classification head to simultaneously eliminate attribute biases and class biases. Specifically, both intra-and inter-client contrastive learning are introduced for balanced feature learning, and difficulty-aware logit adjustment is deployed to balance decision boundaries across classes. Experimental results on both real-world and simulated medical FL scenarios demonstrate FedIIC's superiority against the state-of-the-art FL approaches. We believe that this study is helpful to build real-world FL systems for clinical applications."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_65.
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,1.0,Introduction,"When using a Machine Learning (ML) model during intraoperative tissue characterisation, it is vital that the surgeon is able to assess how reliable a model's prediction is [8]. For the surgeon to trust the output predictions of the model, the model must be able to explain itself reliably in a clinical scenario [2]. To assess an explainability method we consider five metrics of performance: speed, usability, generalisability, trustworthiness and ability to localise semantic features. The explanation of a model's predictions is trustworthy if small perturbations in the input or model parameters, results in a similar output explanation. One form of explainability in the image classification domain is pixel attribution (PA) mapping. PA maps aim to highlight the ""most important"" pixels to the classification. PA maps can be used to visually highlight whether a model is poorly extracting semantic features [32] and/or that the model is misinformed due to spurious correlations within the data that it was trained on [16]. To efficiently process image data, these methods mainly rely on Convolutional Neural Networks (CNNs) and achieve state-of-the-art (SOTA) performance. One of the first PA methods proposed for CNNs was class activation maps (CAM) [33]. CAM uses one forward pass of the model to find the channels in the last convolutional layer that contributed most to the prediction. One of CAM's limitations is its reliance on global average pooling (GAP) [21] after the last convolutional layer as it dramatically reduces the number of architectures that can use CAM. To improve on this, Grad-CAM [30] generalises to all CNN architectures which are differentiable from the output logit layer to the chosen convolutional layer. However, Grad-CAM often lacks sharpness in object localisation, as noted and improved on in Grad-CAM++ [6] and SmoothGrad-CAM++ [24]. These extensions of Grad-CAM have good semantic feature localisation but they are unable to be deployed for use in surgery [5]. Both Score-CAM [31] and Recipro-CAM [5] also generalise to all CNN architectures but are deployable. Score-CAM improves on object localisation within the visual PA map without losing the class specific capabilities of Grad-CAM by masking out regions of the image and measuring the change in the output score. This is similar to perturbation methods like RISE [26], LIME [28] and other perturbation techniques [3,32]. On the other hand, Recipro-CAM focuses on the speed of PA map computation whilst maintaining comparable SOTA performance. By utilising the CNN's receptive field, Recipro-CAM generates a number of spatial masks and then measures the effect on the output score much like Score-CAM.Despite being speedy, easy to deploy and able to localise semantic features, the above methods lack trustworthiness due to the training strategy of their underlying model. Deep learning (DL) models trained with empirical risk minimisation (ERM) are overconfident in prediction [12] and vulnerable to adversarial attacks [13]. Bayesian Neural Networks (BNNs) [23] bring improved regularisation and output uncertainty estimates. Unfortunately, the non-linearity and number of variables within NNs make Bayesian inference a computationally intensive task. For this reason, variational methods [15,18] are used to approximate Bayesian inference. More recently, the variational method Bayes by Backprop [4] used Dropout [19] to approximate Bayesian inference. Dropout is a regularisation technique which has also been noted to improve salient feature extraction. Although Bayes by Backprop is trustworthy, it often fails to scale to the complex architectures of SOTA models. To improve on this lack of generalisability, another variational method called Monte Carlo (MC) Dropout [12]  proposes that a model trained with Dropout is equivalent to a probabilistic deep Gaussian process [7,11]. With this assumption, an estimated output distribution is computed after a number of forward passes with Dropout have been applied. This output distribution is used in practice to indicate risk in the model's predictions. Surgeons in practice can use this risk during diagnosis to trust the model for decision making [14]. Using Dropout to perturb a model is a computationally cheap method of model averaging [19]. It is worth noting though that this method's validity as a Bayesian Inference approximation was later questioned [10]. However, this does not affect the use of this method for risk estimation. So far, model explainability and risk estimation have mostly been used separately to assess models' suitability for surgical applications. DistDeepSHAP [20] computed the uncertainty of Shapley values to show uncertainty in explainability maps. However, DistDeepSHAP is a model-agnostic interpretability method that shows the global effect of perturbing inputs, instead of providing an insight to the model's learned representations. The aim of this paper is to show that the fusion of MC Dropout and PA methods leads to improved explainability.In this paper, we propose the first approach which incorporates risk estimation into a PA method. A classification model is trained with Dropout and a PA method is used to generate a PA map. At test time, the classification model is employed with the Dropout enabled. In this work, we propose to repeat this process for a number of iterations creating a volume of PA maps. This volume is used to generate a pixel-wise distribution of PA values from which we can infer risk. More specifically, we introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. This provides an improved explanation of the model's prediction by clearly presenting to the surgeon which salient areas to trust in the model's enhanced PA map. In this work, we focus on the explainability of the classification of brain tumours using probe-based Confocal Laser Endomicroscopy (pCLE) data. Performance evaluation on pCLE data shows that our improved explainability method outperforms the SOTA. "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,2.0,Methodology,"The aim of the proposed method is to produce an improved PA map of a classification model, while providing risk estimation of the model's explainability to enhance trustworthiness in decision making during intraoperative tissue characterisation.In our method, any CNN classification model trained with Dropout can be used. Let Ŷ be the output logits of the CNN model, where Dropout is enabled at test time, with input image X ∈ R height×width×channels . Any PA method can be used to generate a PA map using the output logits S = f s ( Ŷ ) ∈ R height×width where f s (.) is the PA method. We propose to repeat the above process for T iterations to create a volume of PA maps S = {S 1 , ..., S T } ∈ R height×width×T . A visual representation of how the volume is generated is show in Fig. 2. The aim is to use this volume to generate a pixel-wise distribution of PA values from which we can infer risk. To achieve this, we compute the expectation and variance values of the volume along the third dimension as:where, i, j represent the pixel's row and column coordinates, respectively. The expectation E(S i,j ) of each pixel (i, j) is used to generate an enhanced PA map of size height × width. The intuition is that the above distribution of PA values can produce less noisy and risky estimations of a pixel's contribution to the final explainability map compared to a single estimate.As well as advancing SOTA PA methods, our method also estimates the trustworthiness of the enhanced PA map generated above. For risk estimation, it is important to consider that different pixels in the PA map correspond to different semantic features which contribute differently to the output logits. This makes the pixel-wise distributions have different scales. For this reason, the coefficient of variation (CV) is used to estimate pixel-wise risk, as it allows us to compare pixel-wise variances despite their different scales. This is mathematically defined as:Our proposed method improves trustworthiness of explainability as it allows visualisation of both the explainability of the classification model (provided by the enhanced PA map) together with the pixel-wise risk of this map (provided by the CV map). For instance, salient areas on the PA map should not be trusted unless the CV values are low. An example of the enhanced PA and risk maps generated with the proposed method are shown in Fig. 3. This shows that the proposed method not only improves explainability but also provides associated risk information which improves trustworthiness."
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"Dataset. The developed explainability framework has been validated on an in vivo and ex vivo pCLE dataset of meningioma, glioblastoma and metastases of an invasive ductal carcinoma (IDC). All studies on human subjects were performed according to the requirements of the local ethic committee and in agreement with the Declaration of Helsinki (No. CLE-001 Nr: 2014480). The Cellvizio c by Mauna Kea Technologies, Paris, France has been used in combination with the mini laser probe CystoFlex c UHD-R. The distinguishing characteristic of the meningioma is the psammoma body with concentric circles that show various degrees of calcification. Regarding glioblastomas, the pCLE images allow for the visualization of the characteristic hypercellularity, evidence of irregular nuclei with mitotic activities or multinuclear appearance with irregular cell shape. When examining metastases of an IDC, the tumor presents as egg-shaped cells with uniform evenly spaced nuclei. Our dataset includes 38 meningioma videos, 24 glioblastoma and 6 IDC. Each pCLE video represents one tumour type and corresponds to a different patient. The data has been curated to remove noisy images and similar frames. This resulted in a training dataset of 2500 frames per class (7500 frames in total) and a testing dataset of the same size. The dataset is split into a training and testing subset, with the division done on the patient level.Implementation. To implement the DL models we use the open-source framework PyTorch [25] and a NVIDIA Geforce RTX 3090 graphics card for parallel computation. To show our method generalises we trained two lightweight models: ResNet-18 [17] with a learning rate of 0.01 and MobileNetV2 [29] with a learning rate of 0.001. Both were trained using the Adam-W [22] optimiser with a weight decay of 0.01 and Dropout probability 0.1. We report the model's Top-1 accuracy for Resnet18 as 94.0% and for MobileNet as 86.6%. At test time, we set T = 100 to create a fair distribution of PA maps. PA methods were implemented with the help of TorchCAM [9] and ReciproCAM was implemented using the authors' source code.Evaluation Metrics. Evaluating a PA method is not a trivial task because a PA map may not need to be inline with what a human deems ""reasonable"" [1]. Segmentation scores like intersection over union (IoU) may be used with caution to compare thresholded PA maps to ground truth maps with annotated salient regions. By doing so, we can measure how informed the model is about a particular class. To quantify how misinformed a model is, we can estimate at its average drop [6]:where, X = X f s ( Ŷ (X). The above equation measures the effect on the output score of the classification model if we only include the pixels which the PA method scored highly. A minimum average drop is desired.As average drop was found to not be sufficient on its own, the unified method ADCC [27] was introduced which is the harmonic mean of average drop, coherency and complexity, defined as:Coherency is the Pearson Correlation Coefficient which ensures that the remaining pixels after dropping are still important, defined as:where, Cov(., .) is the covariance and σ is the standard deviation. A higher coherency is better. Complexity is the L1 norm of the output PA map.Complexity is used to measure how cluttered a PA map is. For a good PA map, complexity should be a minimum. As it has been shown in the literature, the metrics in Eqs. ( 3), ( 5) and ( 6), can not be used individually to evaluate a PA method [27]. ADCC combined with computation time gives us a reliable overall metric of how a PA method is performing. Performance Evaluation. The proposed method has been compared to combinations of ResNet18 and MobileNetV2 with SOTA PA methods. At test time, Dropout is not enabled for these standard methods, it is only enabled for our method. In Table 1, we show that our method outperforms all the compared CNN-PA method combinations on ADCC. The Dropout version of ScoreCAM is too computationally expensive and therefore is not included in our comparison. We believe that the better performance of our method is because of the random dropping of features taking place during Dropout at test time which helps to suppress noise in the estimated enhanced PA map. The combination of Recipro-CAM with our proposed method improves performance (increases ADCC) at the expense of increasing the computational complexity. We believe that this could be reduced using a batched implementation of Recipro-CAM. We attribute slow down in SmoothGradCAM++ when Dropout is applied during test time to the perturbations it adds on top of the PA method. Our validation study shows that Grad-CAM, Grad-CAM++ and Recipro-CAM are often leading in terms of speed as expected from the literature. In Fig. 1, we can see our proposed method reduces noise in the PA map around the salient region. The distinguishing characteristic of the meningioma is the psammoma body which is highlighted by all the PA methods. Risk estimations from Eq. ( 2) are also displayed and provide an added visualisation for a surgeon to trust the model. As it can be seen, areas of low CV match the areas of high PA values which verifies the trustworthiness of our method. We believe that the proposed explainability method could be used to support the surgeon intraoperatively in diagnosis and decision making during tumour resection. The enhanced PA map extracted with our method highlights the areas which were the most important to the model's prediction. When these areas correlate with clinically relevant areas, it shows that the model has learned to robustly classify the different tissue classes. Hence, it can be trusted by the surgeon for diagnosis."
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,4.0,Conclusion,"In this work we have introduced the first combination of risk in an explainability method. Using our proposed framework we not only improve on all the tested SOTA PA method's ADCC performances but also produce an estimation of risk on the output PA values. The proposed method can clearly present to the surgeon areas of the explainability map that are more trustworthy. From this work we hope to encourage trust between the surgeon and DL models. For future work, we plan to reducing the computation time of our method and deploy the proposed framework for use in surgery."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,1.0,Introduction,"Lifelong learning [31,34] is the process of sequential learning from a series of nonstationary data distributions through acquiring novel concepts while preserving alreadylearned knowledge. However, Deep Neural Networks (DNNs) exhibit a significant drop in performance on previously seen tasks when trained in continual learning settings. This phenomenon is often called catastrophic forgetting [27,28,35]. Furthermore, the unavailability of sufficient training data in medical imaging poses an additional challenge in tackling catastrophic forgetting of a DNN model.In a lifelong learning scenario, maintaining a robust embedding space and preserving geometrical structure is crucial to mitigate performance degradation and catastrophic forgetting of old tasks [23]. However, the absence of samples from prior tasks has been identified as one of the chief reasons for catastrophic forgetting. Therefore, to address the problem, a small memory buffer has been used in the literature to store a subset of samples from already seen tasks and replayed together with new samples [18,32]. Nevertheless, imbalances in data (e.g., between current tasks and the classes stored in the memory) make the model biased towards the current task [18]. Knowledge distillation [4,9,16,31] has been widely used in the literature to preserve the previous knowledge while training on a novel data distribution. This approach applies constraints on updating the weight of the current model by mimicking the prediction of the old model. To maintain the old embedding structure intact in the new model, feature distillation strategies have been introduced in the literature [9,18]. For instance, LUCIR [18] emphasizes on maximizing the similarity between the orientation of old and new embedding by minimizing the cosine distance between old and new embedding. While effective, feature distillation applies strong constraints directly on the lower-dimensional embedding extracted from the old and new models, reducing the plasticity of the model. This is not ideal for adopting novel concepts while preserving old knowledge. Lower-dimensional embedding spaces, typically used for distillation, may not preserve all the latent information in the input data [19]. As a result, they may not be ideal for distillation in lifelong learning scenarios. Furthermore, DNNs often operate on zero-curvature (i.e., Euclidean) spaces, which may not be suitable for modeling and distilling complex geometrical structures in non-stationary biomedical image distributions with various modalities and discrepancies in imaging protocols and medical equipment. On the contrary, hyperbolic spaces have been successfully used to model hierarchical structure in input data for different vision tasks [13,21].In this paper, we propose to perform distillation in a Reproducing Kernel Hilbert Space (RKHS), constructed from the embedding space of multiple fixed-curvature spacess. This approach is inspired by the ability of kernel methods to yield rich representations in higher-dimensional RKHS [10,19]. Specifically, we employ a Radial Basis Function (RBF) kernel on a mixed-curvature space that combines embeddings from hyperbolic (negative curvature), and Euclidean (zero curvature), using a decomposable Riemannian distance function as illustrated in Fig. 1. This mixed-curvature space is robust and can maintain a higher quality geometrical formation. This makes the space more suitable for knowledge distillation and tackling catastrophic forgetting in lifelong learning scenarios for medical image classification. Finally, to ensure a similar geometric structure between the old and new models in L3, we propose minimizing the distance between the new embedding and the subspace constructed using the old embedding in RKHS. Overall, our contributions in this paper are as follows:-To the best of our knowledge, this is the first attempt to study mixed-curvature space for the continual medical image classification task. -We propose a novel knowledge distillation strategy to maintain a similar geometric structure for continual learning by minimizing the distance between new embedding and subspace constructed using old embedding in RKHS. -Quantitative analysis shows that our proposed distillation strategy is capable of preserving complex geometrical structure in embedding space resulting in significantly less degradation of the performance of continual learning and superior performance compared to state-of-the-art baseline methods on BloodMNIST, PathMNIST, and OrganaMNIST datasets."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,2.0,Preliminaries,"Lifelong Learning (L3). L3 consists of a series of T tasks T t ∈ {T . We assume that a fixedsize memory M is available to store a subset of previously seen samples to mitigate catastrophic forgetting in L3.Mixed-Curvature Space. Mixed-Curvature space is formulated as the Cartesian product of fixed-curvature spaces and represented asHere, M i can be a Euclidean (zero curvature), hyperbolic (constant negative curvature), or spherical (constant positive curvature) space. Furthermore, × denotes the Cartesian product, and d i is the dimensionality of fixed-curvature space M i with curvature c i . The distance in the mixed-curvature space can be decomposed as d M (x, y) := C i=1 d Mi (x i , y i ). Hyperbolic Poincaré Ball. Hyperbolic space is a Riemannian manifold with negative curvature. The Poincare ball with curvature -c, c > 0, D n c = {x ∈ R n : c x < 1} is a model of n-dimensional hyperbolic geometry. To perform vector operations on H n , Möbius Gyrovector space is widely used. Möbius addition between x ∈ D n c and y ∈ D n c is defined as followsUsing Möbius addition, geodesic distance between two input data points, x and y in D n c is computed using the following formula.Tangent space of data point x ∈ D n c is the inner product space and is defined aswhich comprises the tangent vector of all directions at x. Mapping hyperbolic embedding to Euclidean space and vice-versa is crucial for performing operations on D n . Consequently, a vector x ∈ T x D n c is embedded onto the Poincaré ball D n c with anchor x using the exponential mapping function and the inverse process is done using the logarithmic mapping function log c v that maps x ∈ D n c to the tangent space of v as followswhere λ c v is conformal factor that is defined asIn practice, anchor v is set to the origin. Therefore, the exponential mapping is expressed as"
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,3.0,Proposed Method,"In our approach, we emphasize on modeling complex latent structure of medical data by combining embedding representation of zero-curvature Euclidean and negativecurvature hyperbolic space. To attain richer representational power of RKHS [17], we embed the low-dimensional fixed-curvature embedding onto higher-dimensional RKHS using the kernel method. and2. for any given n ∈ N, we have"
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,Definition 1. Positive Definite Kernel,"Popular kernel functions (e.g., the Gaussian RBF) operate on flat-curvature Euclidean spaces. In R n , the Gaussian RBF kernel method is defined asHowever, using the geodesic distance in a hyperbolic space along with an RBF function similar to Eq. (4) (i.e., replacing z i -z j 2 with the geodesic distance) does not lead to a valid positive definite kernel. Theoretically, a valid RBF kernel is impossible to obtain for hyperbolic space using geodesic distance [11,12]. Therefore, we use the tangent plane of hyperbolic space and employ log cto embed hyperbolic data to RKHS via the following valid pd kernel (see [10] for the proof of positive definiteness):Now, in L3 setting, we have two models h t and h t-1 at our hand at time t. We aim to improve h t while ensuring the past knowledge incorporated in h t-1 is kept within h t . Assume Z t and Z t-1 are the extracted feature vectors for input X using current and old feature extractor, h t feat and h t-1 feat , respectively. Unlike other existing distillation methods, we employ an independent 2-layer MLP for each fixed-curvature space to project extracted features to a new lower-dimensional embedding space on which we perform further operations. This has two benefits, (i) it relaxes the strong constraint directly applied on Z t and Z t-1 and (ii) reduce the computation cost of performing kernel method. Since we are interested in modeling embedding structure in zero-curvature Euclidean and negative-curvature hyperbolic spaces, we have two MLP as projection modules attached to feature extractors, namely g e and g h .Our Idea. Our main idea is that, for a rich and overparameterized representation, the data manifold is low-dimensional. Our algorithm makes use of RKHS, which can be intuitively thought of as a neural network with infinite width. Hence, we assume that the data manifold for the model at time t-1 is well-approximated by a low-dimensional hyperplane (our data manifold assumption). Let = minIn Eq. ( 7), φ is the implicit mapping to the RKHS defined by the Gaussian RBF kernel, i.e. k e . The benefit of formulation Eq. ( 7) is that it has a closed-form solution asIn Eq. ( 8), K ZZ ∈ R m×m is the Gram matrix of Z e t-1 , and k zZ is an m-dimensional vector storing the kernel values between z e t and elements of z e t-1 . We provide the proof of equivalency between Eq. ( 7) and Eq. ( 8) in the supplementary material due to the lack of space. Note that we could use the same form for the hyperbolic projection module g h to distill between the model at time t and t -1, albeit this time, we employ the hyperbolic kernel k h . Putting everything together, KD (Z t ) := E zt δ e (z e t , Z e t-1 ) + βE zt δ h (z h t , Z h t-1 ) . (Here, β is a hyper-parameter that controls the weight of distillation between the Euclidean and hyperbolic spaces. We can employ Eq. ( 9) at the batch level. Note that in our formulation, computing the inverse of an m × m matrix is required, which has a complexity of O(m 3 ). However, this needs to be done once per batch and manifold (i.e., Euclidean plus hyperbolic). KD is differentiable with respect to Z t , which enables us to update the model at time t. We train our lifelong learning model by combining distillation loss, KD , together with standard cross entropy loss. Please refer to the overall steps of training lifelong learning model using our proposed distillation strategy via mixed-curvature space in Algorithm 1."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,3.1,Classifier and Exemplar Selection,"We employ herding based exemplar selection method that selects examples that are closest to the class prototype, following iCARL [32]. In this section, we describe Mixedcurvature space and L3 methods to tackle catastrophic forgetting.Constant-Curvature and Mixed-Curvature Space. Constant-curvature spaces have been successfully used in the literature to realize the intrinsic geometrical orientation of data for various downstream tasks in machine learning. Flat-curvature Euclidean space is suitable to model grid data [37] while positive and negative-curvature space is better suited for capturing cyclical [2] and hierarchical [25] structure respectively. Hyperbolic representation has been used across domains ranging from image classification [26] and natural language processing [29,30] to graphs [6]. However, a constantcurvature space is limited in modeling the geometrical structure of data embedding as it is designed with a focus on particular structures [14]. Kernel Methods. A Kernel is a function that measures the similarity between two input samples. The intuition behind the kernel method is to embed the low-dimensional input data into a higher, possibly infinite, dimensional RKHS space. Because of the ability to realize rich representation in RKHS, kernel methods have been studied extensively in machine learning [17].L3 Using Regularization with Distillation. Regularization-based approaches impose constraints on updating weights of L3 model to maintain the performance on old tasks. LwF mimics the prediction of the old model into the current model but struggles to maintain consistent performance in the absence of a task identifier. Rebuff et al. in [32] store a subset of exemplars using a herding-based sampling strategy and apply knowledge distillation on output space like LwF [24]. Distillation strategy on feature spaces has also been studied in the literature of L3. Hou et al. in [18] proposes a less-forgetting constraint that controls the update of weight by minimizing the cosine angle between old and new embedding representation."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,5.0,Experimental Details,"Datasets. In our experiments, we use four datasets (e.g., BloodMNIST [1], PathM-NIST [20], OrganaMNIST [3]) and TissueMNIST [3] from MedMNIST collection [38] for the multi-class disease classification. BloodMNIST, PathMNIST, OrganaMNIST and TissueMNIST have 8, 9, 11, and 8 distinct classes, respectively that are split into 4 tasks with non-overlapping classes between tasks following [8]. For cross-domain continual learning experiments, we present 4 datasets sequentially to the model. Implementation Details. We employ ResNet18 [15] as the backbone for feature extraction and a set of task-specific fully connected layers as the classifier to train all the baseline methods across datasets. To ensure fairness in comparisons, we run each experiment with the same set of hyperparameters as used in [8] for five times with a fixed set of distinct seed values, 1, 2, 3, 4, 5 and report the average value. Each model is optimized using Stochastic Gradient Decent (SGD) with a batch of 32 images for 200 epochs, having early stopping options in case of overfitting. Furthermore, we use gradient clipping by enforcing the maximum gradient value to 10 to tackle the gradient exploding problem.Evaluation Metrics. We rely on average accuracy and average forgetting to quantitatively examine the performances of lifelong learning methods as used in previous approaches [7,32]. Average accuracy is computed by averaging the accuracy of all the previously observed and current tasks after learning a current task t and defined as: Acc t = 1 t t i=1 Acc t,i , where Acc t,i is the accuracy of task i after learning task t. We measure the forgetting of the previous task at the end of learning the current task t using:, where at task t, forgetting on task i is defined as the maximum difference value previously achieved accuracy and current accuracy on task i."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,6.0,Results and Discussion,"In our comparison, we consider two regularization-based methods (i.e., EWC [22], and LwF [24]) and 5 memory-based methods (e.g., EEIL [5], ER [33], Bic [36], LUCIR [18] and iCARL [32]). We employ the publicly available code1 of [8] in our experiments to produce results for all baseline methods on BloodMNIST, PathMNIST, and OrganaM-NIST datasets and report the quantitative results in Table 1. The results suggest that the performance of all methods improves with the increase in buffer size (e.g., from 200 to 1000). We observe that our proposed distillation approach outperforms other baseline methods across the settings. The results suggest that the regularization-based methods, e.g., EWC and LwF perform poorly in task-agnostic settings across the datasets as those methods are designed for task-aware class-incremental learning. Our proposed method outperforms experience replay, ER method by a significant margin in both evaluation metrics (i.e., average accuracy and average forgetting) across datasets. For instance, our method shows around 30%, 30%, and 20% improvement in accuracy compared to ER while the second best method, iCARL, performs about 4%, 2%, and 8% worse than our method on BloodMNIST, PathMNIST, and OrganaMNIST respectively with 200 exemplars. Similarly, with 1000 exemplars, our proposed method shows consistent performances and outperforms iCARL by 4%, 6%, and 2% accordingly on BloodMNIST, PathMNIST, and OrganaMNIST datasets. We also observe that catastrophic forgetting decreases with the increase of exemplars. Our method shows about 2% less forgetting phenomenon across the datasets with 1000 exemplars compared to the second best method iCARL. Table 2 presents the experimental results (e.g., average accuracy) on relatively complex cross-domain incremental learning setting where datasets (BloodMNIST, PathMNIST, OrganaMNIST, and TissueMNIST) with varying modalities from different institutions are presented at each novel task. Results show an unmatched gap between regularization-based methods (e.g., Lwf and EWC) and our proposed distillation method. CL3DMC outperforms ER method by around 16% on both task-aware and task-agnostic settings. Similarly, CL3DMC performs around 3% better than the second best method, iCARL."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,7.0,Conclusion,"In this paper, we propose a novel distillation strategy, L3DMC on mixed-curvature space to preserve the complex geometric structure of medical data while training a DNN model on a sequence of tasks. L3DMC aims to optimize the lifelong learning model by minimizing the distance between new embedding and old subspace generated using current and old models respectively on higher dimensional RKHS. Extensive experiments show that L3DMC outperforms state-of-the-art L3 methods on standard medical image datasets for disease classification. In future, we would like to explore the effectiveness of our proposed distillation strategy on long-task and memory-free L3 setting."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,,"e (z e t , Z e t-1 ) := φ(z e t ) -span{φ(z e t-1,i )} m i=1 2"
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,,"c=1,...,t h t feat (X)μ c 2 . Input: Dataset D 0 , D 1 , ..., D T , and Memory M Output: The new model at time t with parameters Θ t 1: Randomly Initialize Θ 0 ; h 0 Θ = h 0 feat • h 0 cls 2: Train Θ 0 on D 0 using CE 3: for t in {1, 2, ..., T } do"
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,"Machine Learning -Explainability,","Bias, and Uncertainty I"
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,1.0,Introduction,"Gaze information is a widely used behavioral measure to study attentional focus [7], cognitive control [19], memory traces [23] and decision making [28]. The most commonly used gaze estimation technique in laboratory settings is the infrared eye tracker, which detects gaze position by emitting invisible near-infrared light and then capturing the reflection from the cornea [6]. While infrared eye tracker still remains the most accurate and reliable solution for the gaze estimation, these systems have several limitations, including individual differences in the contrast of the pupil and iris and the need for time-consuming setup and calibration before each scanning session [3,11].Recently, Electroencephalogram (EEG) has been explored as an alternative method to estimate eye movements by recording electrical activity from the brain non-invasively with high temporal resolution [16]. The growing body of literature has shown that Deep Learning architectures could be significantly effective for many EEG-based tasks [4,26]. Nevertheless, with the advantages that Deep Learning brings, new challenges arise. Most of these models applied to electroencephalography (EEG) data tend to lack interpretability, making it difficult to understand the underlying reasons for their predictions, which subsequently leads to a decrease in the acceptability of advanced technology in neuroscience [25]. However, a potential solution already exists, in the form of the attention mechanism [29]. The attention mechanism has the potential to provide a more transparent and understandable way of analyzing EEG data, enabling us to comprehend the relationships between different brain signals better and make more informed decisions based on the results. With the development and implementation of these techniques, we can look forward to a future where EEG data can be utilized more effectively and efficiently in various applications.Attention mechanisms have recently emerged as a powerful tool for processing sequential data, including time-series data in various fields such as natural language processing, speech recognition, and computer vision [5,24,29]. In the context of EEG signal analysis, attention mechanism has shown promising results in various applications, including sleep stage classification, seizure detection, and event-related potential analysis [8,13,17]. Since different electrodes record the brain activity from the different brain areas and functions, the information density from each electrode can vary for different tasks [15].In this study, we introduce a new deep learning framework for analyzing EEG signals applying attention mechanisms. For the method evaluation, we used the EEGEyeNet dataset and benchmark [16], which includes concurrent EEG and infrared eye-tracking recordings, with eye tracking data serving as a ground truth. Our method incorporates attention modules to assign weights to individual electrodes based on their importance, allowing the network to prioritize relevant information in the signal. Specifically, we demonstrate the ability of our framework to accurately predict gaze position and saccade direction, achieving superior performance compared to previously benchmarked methods. Furthermore, we provide visualizations of model's interpretability through case studies."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,2.1,Motivation,"In this study, our primary goal was to build a model sensitive to different electrodes. The motivation for this goal is two-fold. Firstly, with regards to interpreting the model, the electrodes can be considered the smallest entity as they record signals from specific regions of the brain. Therefore, the electrodebased explanation is a reasonable approach considering human understanding. Second, in the context of model learning, incorporating adaptive weighting of electrodes within a neural network can potentially enhance the accuracy and reliability of gaze estimation systems. This is because electrodes are functionally connected to cognitive behaviors. Specifically, in tasks such as gaze estimation, electrodes positioned near the eyes can capture electrical signals from the orbicularis oculi muscles [2], thereby making the pre-frontal brain areas more crucial for precise estimation [15]. Additionally, the noise of EEG recordings could be induced by broken wire contacts, too much or dried gel, or loose electrodes [27], the influence of such electrodes should be reduced in the network under ideal circumstances. As shown in Fig. 1, our model design focuses on enhancing an existing deep learning architecture with an electrode-sensitive component. This component first extracts electroderelated information, and then utilizes this information for two purposes:(1) emphasizing the reliable electrodes and diminishing the influence of suspicious electrodes, while simultaneously (2) providing explanations for each prediction."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,2.2,Attention-CNN,"Following the idea from the previous section, we propose the Attention-CNN model, where the attention blocks are used as the electrode-sensitive component. As shown in Fig. 2, the Attention-CNN model is structured by adding an attention block after each convolution block in every layer and an additional single attention block before the final prediction block (the blocks in blue). A convolution block contains a convolution layer, a batch-norm layer [14], a leaky ReLU [18] and a max-pooling layer. In addition, the residual [10] techniques are applied in the CNN framework. The convolution layer operates only in the time dimension. The attention blocks, acting as an electrode-sensitive component, can be carried out by Squeeze-and-Excitation Block (SE Block) [12] and/or Self-Attention Block (SA Block) [29]. In the attention blocks, the retrieved electrode importance is used to weigh the features in each layer. Additionally, the same weights can provide explanations for the predictions of the model. In the prediction block, the features are flattened and then fed into the fully connected layer to finally obtain the predictions. While the SA Block is only required once in the process, the SE Blocks are added in every residual block. In order to keep the same scale for the same sample, the parameters of the SE Blocks are shared for the whole process. All building blocks are trained end-to-end, including the weights for the electrode importance used in the attention blocks.Squeeze and Excitation Block: the SE block involves two principle operations. The Squeeze operation compresses features u ∈ R T ×J into electrodewise vectors z ∈ R J by using global average pooling. Here, T denotes the feature size, and J is the number of electrodes. More precisely, the j-th element of z is calculated byThe Excitation operation first computes activation s by employing the gating mechanism with sigmoid activation:, where σ refers to the sigmoid function, δ represents the ReLU [20] function, and W are learnable weights. The final output of SE block weigh each channel adaptively by re-scaling U with s: xj = F scale (u j , s j ) = s j • u j . In contrast to the original implementation [12] which deals with 3-dimensional data, the input data in our setup has only 2 dimensions (electrodes and time)."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,,Self Attention Block:,"The self-attention mechanism [22] was first used in the field of Natural language processing (NLP), aiming at catching the attention of/between different words in a sentence or paragraph. The attention is obtained by letting the input data interact with themselves and determining which features are more important. This was implemented by introducing the Query, Key, Value technique, which is defined as, where U denotes the input of self-attention block and φ(•, •) represents linear transformation.Then, Attention Weights are computed using Query and Key:where d k stands for the dimensions of the Key, and √ d k works as a scaling factor. The softmax function was applied to adjust the range of the value in attention weights (M att ) to [0, 1].Unlike the transformer model, the attention weights are first compressed into a one-dimensional vector by a layer of global average pooling (ψ) and normalized by a sigmoid function. More precisely, we compute Z att = sigmoid(ψ(M att )). Finally, the output of SA Block X is computed by : X = κ(Z att , V ), where κ denotes the electrode-wise production."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.1,Materials and Experimental Settings,"EEGEyeNet Dataset: For our experiments, we utilized the EEGEyeNet dataset [16], which includes synchronized EEG and Eye-tracking data. The EEG signals were collected using a high-density, 128-channel EEG Geodesic Hydrocel system sampled at a frequency of 500 Hz. Eye-tracking data, including eye position and pupil size, were gathered using an infrared video-based eye tracker (Eye-Link 1000 Plus, SR Research), also operating at a sampling rate of 500 Hz. The recorded EEG and eye-tracking information was pre-processed, synchronized and segmented into 1-second clips based on eye movements. The infrared eye tracking recordings were used as ground truth. In this paper, the processed dataset we utilized contains two parts: the Position Task and Direction Task, which correspond to two types of eye movements: fixation, i.e., the maintaining of the gaze on a single location, and saccade, i.e. the rapid eye movements that shift the centre of gaze from one point to another. While Position Task estimates the absolute position from fixation, Direction Task estimates the relative changes during saccades, involving two sub-tasks, i.e., the prediction of amplitude and angle. The statistics and primary labels of these two parts are shown in Table 1. To ensure data integrity and prevent data leakage, the dataset was split into training, validation, and test sets across subjects, with 70 % of the subjects used for training, and 15% each for validation and testing. This procedure ensures that no data from the same subject appears in both the training and validation/testing phases, thereby avoiding potential subject-related patterns from being learned by the model during training and tested on in validation/testing. For more details of this dataset, please refer to [16]."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,,Implementation Details:,"The experiments are implemented with PyTorch [21]. When training the Attention-CNN model, the batch size is set to 32, the number of epochs is 50, and the learning rate is 1e -4 . There are 12 convolution blocks, and the residual operation repeats every three convolution blocks. The feature length of the hidden layer is set as 64, and the kernel size is 64. The number of convolutional layers, kernel size and hidden feature length, are selected based on validation performance. We conducted experiments with three configurations: the SE Block and the SA Block together, only one of the attention blocks, or no attention blocks at all. For the angle prediction in Direction Task, we use angle loss l angle = |(atan(sin(pt), cos(pt))|, where p denotes the predicted results, and t denotes the targets. For Position Task and Amplitude prediction in the Direction Task, the loss function is set to smooth-L1 [9].Evaluation: For Position task, Euclidean distance is applied as the evaluation metric in both pixels and visual angles. Compared to pixel distance, visual angles depend on both object size on the screen and the viewing distance, thus enabling the comparison across varied settings. The performance of Direction Task is measured by the square root of the mean squared error (RMSE) for the angle (in radians) and the amplitude (in pixels) of saccades. In order to avoid the error caused by the repeatedness of angles in the plane (i.e. 2π and 0 rad represents the same direction), atan(sin(α), cos(α)) is applied, just like in angle loss."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.2,Performance of the Attention-CNN,"Table 2 shows the quantitative performance of the Attention-CNN in this work. For the Position Task, CNN with SE block has an average performance with the RMSE of 109.58 pixels. Likewise, the CNN model with both SE block and the SA block has a similar performance (110.05 pixels). Similar to Position Task, in amplitude prediction of Direction Task, the attention blocks aid the prediction evidently, heightening the performance by 5 pixels. Here, the model with both attention blocks has a lower variance. For angle prediction, the CNN model with both SE block and SA block has the best performance among all with the RMSE of 0.1707 rad.We can conclude that the CNN model with both attention blocks consistently outperforms the CNN model alone by 5 to 10 percent across all tasks, indicating that electrode-wise attention assists in the learning process of the models.  "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.3,Model Interpretability by Case Studies,"To provide a more detailed analysis of the interpretability of our proposed Attention-CNN model, as well as to further investigate the underlying reasons for the observed accuracy improvement, we conducted a visual analysis of the model performance, with a particular focus on the role of the attention block.Our analysis yielded two key findings, which are as follows: Firstly, the attention blocks were able to detect the electrical difference between the right and left pre-frontal area in case of longer saccades, i.e. rapid eye movements from one side of the screen to the other; see the saccades (d) and (e) in Fig. 3. We present the sequence of saccades and observed the EEG signals as well as the electrode importance from proposed models in Fig. 3. The attention block effectively captured this phenomenon by highlighting the electrodes surrounding the prominent signals (saccades (d) and (e) in Fig. 3). Conversely, in cases where the saccade was of a shorter distance (other saccades in Fig. 3), attention was more widely distributed across the scalp rather than being concentrated in specific regions. This is justifiable as the neural network aims to integrate a more comprehensive set of information from all EEG channels.Additionally, the attention block effectively learned to circumvent the interference caused by noisy electrodes and redirected attention towards the frontal region. Figure 4 illustrates a scenario where problematic electrodes were situated around both ears, exhibiting abnormal amplitudes (±100 µV). Using Layer-wise Relevance Propagation [1] to elucidate the CNN model's predictions, the result depicted in Fig. 4b revealed that the most significant electrodes were located over the left ear, coinciding with the noisy electrodes. In contrast, as shown in Fig. 4c, the Attention-CNN model effectively excluded the unreliable electrodes and allocated greater attention to the frontal region of the brain. "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.4,Explainability Quantification,"We further examine the validity in explainability of the proposed method by comparing the distribution of learned attention of noisy and non-noisy electrodes in the Direction Task. The attention block's effectiveness is demonstrated by its ability to assign lower weights to these noisy electrodes in contrast to the non-noisy ones. Within all samples in the Direction Task that feature at least one noisy electrode, only 19% of the non-noisy electrodes had normalized attention weights below 0.05. In contrast, 42% of the noisy electrodes exhibited this trait, implying the attention block's ability to reduce weights of abnormal electrodes. We direct readers to the Supplementary materials for a distribution plot showcasing the difference between noisy and non-noisy electrodes, along with additional details. It's important to note that quantifying explainability methods for signal-format data, such as EEG, presents a significant challenge and has limited existing research. Therefore, additional investigations in this field are anticipated in future studies."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,4.0,Conclusion,"In this study, we aimed to address the issue of the lack of interpretability in deep learning models for EEG-based tasks. Our approach was to leverage the fact that EEG signal noise or artifacts are often localized to specific electrodes. We accomplished this by incorporating attention modules as electrode-sensitive components within a neural network architecture. These attention blocks were used to emphasize the importance of specific electrodes, resulting in more accurate predictions and improved interpretability through the use of scaling.Moreover, our proposed approach was less susceptible to noise. We conducted comprehensive experiments to evaluate the performance of our proposed Attention-CNN model. Our results demonstrate that this model can accurately classify EEG and eye-tracking data while also providing insights into the quality of the recorded EEG signals. This contribution is significant as it can lead to the development of new decoding techniques that are less sensitive to noise.In summary, our study underscores the importance of incorporating attention mechanisms into deep learning models for analyzing EEG and eye-tracking data. This approach opens up new avenues for future research in this area and has the potential to provide valuable insights into the neural basis of cognitive processes."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,,Table 2 .,CNN + both 0.1707 ± 0.011 52.2782 ± 1.169 110.0523 ± 0.670 (2.28 ± 0.010)
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,1.0,Introduction,"Deep learning methods for automatic lung lesion segmentation from CT volumes have the potential to alleviate the burden on clinicians in assessing lung damage and disease progression in COVID-19 patients [20][21][22]. However, these methods require large amounts of manually labeled data to achieve the level of robustness required for their clinical application [5,8,23,27]. Manual labeling of CT volumes is time-consuming and may increase the workload of clinicians. Additionally, applying deep learning-based segmentation models to data from new unseen sources can result in suboptimal lesion segmentation due to unseen acquisition devices/parameters, variations in patient pathology, or future coronavirus variants resulting in new appearance characteristics or new lesion pathologies [16]. To address this challenge, interactive segmentation methods that can quickly adapt to such changing settings are needed. These can be used either by end-users or algorithm developers to quickly expand existing annotated datasets and enable agile retraining of automatic segmentation models [4]."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,,Related Work:,"Interactive segmentation methods for Artificial Intelligence (AI) assisted annotation have shown promising applications in the existing literature [14,18,25,26]. BIFSeg [26] utilizes a bounding box and scribbles with convolutional neural network (CNN) image-specific fine-tuning to segment potentially unseen objects of interest. MIDeepSeg [14] incorporates user-clicks with the input image using exponential geodesic distance. However, BIFSeg, MIDeepSeg, and similar deep learning-based methods exploit large networks that do not adapt rapidly to new data examples in an online setting due to the elevated computational requirements.Due to their quick adaptability and efficiency, a number of existing online likelihood methods have been applied as interactive segmentation methods [2,3,24]. DybaORF [24] utilizes hand-crafted features with dynamically changing weights based on interactive labels' distribution to train a Random Forest classifier. ECONet [2] improves online learning with a shallow CNN that jointly learns both features and classifier to outperform previous online likelihood inference methods. While ECONet is, to the best of our knowledge, the only online learning method that addresses COVID-19 lung lesion segmentation, it is limited to learning from user scribbles only. This means that it requires a significant amount of user interaction to achieve expert-level accuracy. Additionally, the model uses a single convolution for feature extraction, limiting its accuracy to a specific scale of pathologies. For each CT volume, the model is trained from scratch, resulting in lack of prior knowledge about lesions. Contributions: To overcome limitations of existing techniques, we propose adaptive multi-scale online likelihood network (MONet) for AI-assisted interactive segmentation of lung lesions in CT volumes from COVID-19 patients. Our contributions are three-fold, we propose: i. Multi-scale online likelihood network (MONet), consisting of a multi-scale feature extractor, which enables relevant features extraction at different scales for improved accuracy; ii. Adaptive online loss that uses weights from a scaled negative exponential geodesic distance from user-scribbles, enabling adaptive learning from both initial segmentation and user-provided corrections (Fig. 1); iii. Probability-guided pruning approach where uncertainty from initial segmentation model is used for pruning ambiguous online training data.MONet enables human-in-the-loop online learning to perform AI-assisted annotations and should not be mistaken for an end-to-end segmentation model. We perform expert evaluation which shows that adaptively learned MONet outperforms existing state-of-the-art, achieving 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score evaluated."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.0,Method,"Given an input image volume, I, a pre-trained CNN segmentation model generates an automatic segmentation C with associated probabilities P . When using data from a new domain, the automated network may fail to properly segment foreground/background objects. To improve this, the user provides scribblesbased interaction indicating corrected class labels for a subset of voxels in the image I. Let S = S f ∪ S b represent these set of scribbles, where S f and S b denote the foreground and background scribbles, respectively, and S f ∩ S b = ∅. Figure 2 (a) shows scribbles S, along with the initial segmentation C and probabilities P ."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.1,Multi-scale Online Likelihood Network,"Our proposed multi-scale online likelihood network (MONet), shown in Fig. 2 (b), uses a multi-scale feature extractor that applies a 3D convolution at various kernel sizes to capture spatial information at different scales. The output of each scale is concatenated and fed to a fully-connected classifier, which infers the likelihood for background/foreground classification of the central voxel in the input patch. Each layer in MONet is followed by batch normalization and ReLU activation."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.2,Adaptive Loss for Online Learning,"The scribbles S only provide sparse information for online learning. However, these corrections are likely also applicable to neighboring voxels with similar appearance features, thereby providing an extended source of training information. Concurrently, the initial automated segmentation C will often provide reliable results away from the scribbles. To extend the influence of the scribbles S while preserving the quality of the initial segmentation C, we propose a spatially-varying adaptive online loss:where i is a voxel index, L C and L S are individual loss terms for learning from the automated segmentation C and the user-provided correction scribbles S respectively. W are spatially-varying interaction-based weights defined using the geodesic distance D between voxel i and the scribbles S:where the temperature term τ controls the influence of W in I. The geodesic distance to the scribbles is defined as D(i, S, I) = min j∈S d(i, j, I) where d(i, j, I) = min p∈Pi,j 1 0 ∇I(p(x)) • u(x) dx and P i,j is the set of all possible differentiable paths in I between voxels i and j. A feasible path p is parameterized by x ∈ [0, 1]. We denote u(x) = p (x)/ p (x) the unit vector tangent to the direction of the path p. We further let D = ∞ for S = ∅. Dynamic Label-Balanced Cross-Entropy Loss: User-scribbles for online interactive segmentation suffer from dynamically changing class imbalance [2]. Moreover, lung lesions in CT volumes usually occupy a small subset of all voxels, introducing additional label imbalance and hence reducing their impact on imbalanced online training. To address these challenges, we utilize a label-balanced cross-entropy loss [2,10,11], with dynamically changing class weights from segmentations and scribbles distribution. Given an online model with parameters θ, the foreground likelihood from this model is p i = P (s i = 1|I, θ). Then, the segmentations-balanced and scribbles-balanced cross-entropy terms are:where α and β are class weights for labels C and scribbles S that are defined by labels and scribbles distributions during online interaction as: The patch-based training approach from [2] is used to first extract K×K×K patches from I centered around each voxel in S and C and train MONet using Eq. ( 1). Once learned, efficient online inference from MONet is achieved by applying it to the whole input CT volumes as a fully convolutional network [12]."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.3,Improving Efficiency with Probability-Guided Pruning,"MONet is applied as an online likelihood learning method, where the online training happens with an expert human annotator in the loop, which makes online training efficiency critical. We observe that the automatic segmentation models provide dense labels C which may significantly impact online training and inference performance. C may contain ambiguous predictions for new data, and a number of voxels in C may provide redundant labels. To improve online efficiency while preserving accuracy during training, we prune labels as C * = M C where: M i is set to 1 if P i ≥ ζ and U i ≥ η and 0 otherwise. ζ ∈ [0, 1] is the minimum confidence to preserve a label, U i ∈ [0, 1] is a uniformly distributed random variable, and η ∈ [0, 1] is the fraction of samples to prune."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.0,Experimental Validation,"Table 1 outlines the different state-of-the-art interactive segmentation methods and their extended variants that we introduce for fair comparison. We compare our proposed MONet with ECONet [2] and MIDeepSeg [14]. As our proposed Eq. ( 2) is inspired by the exponential geodesic distance from MIDeepSeg [14], we introduce MIDeepSegTuned, which utilizes our proposed addition of a temperature term τ . Moreover, to show the importance of multi-scale features, we include MONet-NoMS which uses features from a single 3D convolution layer. We utilize MONAI Label to implement all online likelihood methods [7]. For methods requiring an initial segmentation, we train a 3D UNet [6] using MONAI [17] with features [32,32,64,128,256,32]. Output from each method is regularized using GraphCut optimization. We also compare against a baseline interactive Graph-Cut (IntGraphCut) implementation, that updates UNet output with scribbles based on [3] and then performs GraphCut optimization. The proposed method is targeted for online training and inference, where quick adaptability with minimal  set using cosine annealing scheduler [13]. Dropout of 0.3 was used for all fullyconnected layers in online models. Each layer size in ECONet and MONet-NoMS was selected by repeating line search experiments from [2]: (i) input patch/3D  convolution kernel size of K = 9, (ii) 128 input 3D convolution filters and (iii) fully-connected sizes of 32×16×2. For MONet, we utilize four input 3D convolution with multi-scale kernel sizes K = [1, 3, 5, 9] with each containing 32 filters (i.e., a total of 128 filters, same as (ii)). We utilize the same fully-connected sizes as in (iii) above. Parameters ζ = 0.8 and η = 0.98 are selected empirically.We utilize τ = 0.3 for MONet, MIDeepSegTuned and MONet-NoMS. We use GraphCut regularization, where λ = 2.5 and σ = 0.15 [3]. Search experiments used for selecting τ , λ, σ are shown in Fig. 1 and 2 in supplementary material."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.1,Quantitative Comparison Using Synthetic Scribbler,"We employ the synthetic scribbler method from [2,25] where mis-segmented regions in the inferred segmentations are identified by comparison to the ground truth segmentations. Table 2 and Fig. 3 present quantitative comparison of methods using synthetic scribbler. They show that MONet outperforms all existing state-of-the-art in terms of accuracy with the least number of synthetic scribbled voxels. In particular, MONet outperforms both MIDeepSeg [14] and MIDeepSeg-Tuned, where adaptive online learning enables it to quickly adapt and refine segmentations. In terms of efficiency, online training and inference of the proposed MONet takes around 6.18 s combined, which is 22.4% faster as compared to 7.97 s for MIDeepSeg. However, it is slower than ECONet and ISeg. MIDeepSeg performs the worst as it is unable to adapt to large variations and ambiguity within lung lesions from COVID-19 patients, whereas by utilizing our proposed Eq. ( 2) in MIDeepSegTuned, we improve its accuracy. When comparing to online learning methods, MONet outperforms MONet-NoMS, where the accuracy is improved due to MONet's ability to extract multi-scale features. Existing stateof-the-art online method ECONet [2] requires significantly more scribbled voxels as it only relies on user-scribbles for online learning."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.2,Performance and Workload Validation by Expert User,"This experiment aims to compare the performance and perceived subjective workload of the proposed MONet with the best performing comparison method MIDeepSegTuned based on [14]. We asked an expert, with 2 years of experience in lung lesion CT from Radiology Department, Oxford University Hospitals NHS Foundation Trust, to utilize each method for labelling the following pathologies as lung lesions in 10 CT volumes from UESTC-COVID-19 expert set [27]: ground glass opacity, consolidation, crazy-paving, linear opacities. One CT volume is used by the expert to practice usage of our tool. The remaining 9 CT volumes were presented in a random order, where the perceived workload was evaluated by the expert at half way (after 5 segmentations) and at the end. We use the National Aeronautics and Space Administration Task Load Index (NASA-TLX) [9] as per previous interactive segmentation studies [15,19,28]. The NASA-TLX asks the expert to rate the task based on six factors, being performance, frustration, effort, mental, physical and temporal demand. The weighted NASA-TLX score is then recorded as the expert answers 15 pair-wise questions rating factors based on importance. In addition, we also recorded accuracy metrics (Dice and ASSD) against ground truth labels in [27], time taken to complete annotation and whether the expert was able to successfully complete their task within 10 min allocated for each volume. Table 3 presents an overview for this experiment, where using the proposed MONet, the expert was able to complete 100% of the labelling task, whereas using MIDeepSegTuned they only completed 33.33% within the allocated time. In addition, MONet achieves better accuracy with lower time for complete annotation and less overall perceived workload with NASA-TLX of 52.33% as compared to 77.00% for MIDeepSegTuned. Table 4 shows the individual scores that contribute to overall perceived workload. It shows that using the proposed MONet, the expert perceived reduced workload in all sub-scale scores except temporal demand. We believe this is due to the additional online train- ing/inference overhead for MONet application. Figure 4 visually compares these results where MONet results in more accurate segmentation as compared to MIDeepSegTuned. We also note that MONet's ability to apply learned knowledge on the whole volume enables it to also infer small isolated lesions, which MIDeepSegTuned fails to identify."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,4.0,Conclusion,"We proposed a multi-scale online likelihood network (MONet) for scribblesbased AI-assisted interactive segmentation of lung lesions in CT volumes from COVID-19 patients. MONet consisted of a multi-scale feature extractor that enabled extraction of relevant features at different scales for improved accuracy. We proposed an adaptive online loss that utilized adaptive weights based on user-provided scribbles that enabled adaptive learning from both an initial automated segmentation and user-provided label corrections. Additionally, we proposed a dynamic label-balanced cross-entropy loss that addressed dynamic class imbalance, an inherent challenge for online interactive segmentation methods. Experimental validation showed that the proposed MONet outperformed the existing state-of-the-art on the task of annotating lung lesions in COVID-19 patients. Validation by an expert showed that the proposed MONet achieved on average 5.86% higher Dice while achieving 24.67% less perceived NASA-TLX workload score than the MIDeepSegTuned method [14].This project utilized scribbles-based interactive segmentation tools from opensource project MONAI Label (https://github.com/Project-MONAI/MONAILabel) [7]."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,,Table 1 .,"ods in this context would result in a considerable decrease in online efficiency, rendering the method impractical for online applications[2]. We utilize a GPUbased implementation of geodesic distance transform[1] in Eq. (2), whereas MIDeepSeg uses a CPU-based implementation. We use NVIDIA Tesla V100 GPU with 32 GB memory for all our experiments. Comparison of accuracy for each method is made using Dice similarity (Dice) and average symmetric surface distance (ASSD) metrics against ground truth annotations[2,14]. Moreover, we compare performance using execution time (Time), including online training and inference time, average full annotation time (FA-Time), and number of voxels with scribbles (S) needed for a given accuracy.Data:To simulate a scenario where the automatic segmentation model is trained on data from a different source than it is tested on, we utilize two different COVID-19 CT datasets. The dataset from the COVID-19 CT lesion segmentation challenge[21] is used for training and validation of 3D UNet for automatic segmentation task and patch-based pre-training of MONet/MONet-NoMS/ECONet. This dataset contains binary lung lesions segmentation labels for 199 CT volumes (160 training, 39 validation). We use UESTC-COVID-19[27], a dataset from a different source, for the experimental evaluation of interactive segmentation methods (test set)."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 53.
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,1.0,Introduction,"Deep neural networks have shown expert-level performance in various disease diagnoses [35,36]. In practice, a deep neural network is often limited to the diagnosis of only a few diseases, partly because it is challenging to collect enough training data of all diseases even for a specific body tissue or organ. One possible solution is to enable a deployed intelligent diagnosis system to continually learn new diseases with collected new training data later. However, if old data are not accessible due to certain reasons (e.g., challenge in data sharing), current intelligent systems will suffer from catastrophic forgetting of old knowledge when learning new diseases [17].Multiple approaches have been proposed to alleviate the catastrophic forgetting issue. One approach aims to determine part of the model parameters which are crucial to old knowledge and tries to keep these parameters unchanged during learning new knowledge [5,15,18]. Another approach aims to preserve old knowledge by making the updated model imitate the behaviour (e.g., output at certain layer) of the old model particularly with the help of knowledge distillation technique [8,17,34]. Storing a small amount of old data or synthesizing old data relevant to old knowledge and using them together with training data of new knowledge can often help significantly alleviate forgetting of old knowledge [2,3,19,22]. Although the above approaches can help the updated model keep old knowledge to some extent, they often fall into the dilemma of model plasticity (for new knowledge learning) and stability (for old knowledge preservation). In order to resolve this dilemma, new model components (e.g., neurons or layers in neural networks) can be added specifically for learning new knowledge, while old parameters are largely kept unchanged for old knowledge [20,26,30]. While this approach has shown state-of-the-art continual learning performance, it faces the problem of rapid model expansion and effective fusion of new model components into the existing ones. To alleviate the model expansion issue and meanwhile well preserve old knowledge, researchers have started to explore the usage of a pretrained and fixed feature extractor for the whole process of continual learning [14,27,28,32], where the challenge is to discriminate between different classes of knowledge with limited learnable parameters.In this study, inspired by recent advances in transfer learning in natural language processing [7,12], we propose adding a light-weight learnable module called adapter to a pretrained and fixed convolutional neural network (CNN) for effective continual learning of new knowledge. For each round of continual learning, the CNN model will be updated to learn a set of new classes (hereinafter also called learning a new task). The learnable task-specific adapters are added between consecutive convolutional stages to help the pretrained CNN feature extractor more effectively extract discriminative features of new diseases. To the best of our knowledge, it is the first time to apply the idea of CNN adapter in the continual learning field. In addition, to keep extracted features discriminative between different tasks, a special task-specific classifier head is added when learning each new task, in which all previously learned old classes are considered as the 'out-of-distribution' (OOD) class and correspond to an additional output neuron in each task-specific classifier head. A simple yet effective finetuning strategy is applied to calibrate outputs between multiple task-specific heads. Extensive empirical evaluations on three image datasets show that the proposed method outperforms existing continual learning methods by a large margin, consistently supporting the effectiveness of the proposed method."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.0,Method,"This study aims to improve continual learning performance of an intelligent diagnosis system. At each learning round, following previous studies [2,3] which show that rehearsal with old samples can significantly improve continual learning performance, the system will be updated based on the training data of new diseases and preserved small subset for each previously learned disease. During inference, the system is expected to accurately diagnose all learned diseases, without knowing which round (i.e., task) the class of any test input is from."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.1,Overall Framework,"We propose an Adapter-based Continual Learning framework called ACL with a multi-head training strategy. With the motivation to make full use of readily available pretrained CNN models and slow down the speed of model expansion that appears in some state-of-the-art continual learning methods (e.g., DER [30]), and inspired by a recently developed transfer learning strategy Delta tuning for downstream tasks in natural language process [7], we propose adding a learnable light-weight adapter between consecutive convolutional stages in a pretrained and fixed CNN model when learning new classes of diseases at each learning round (Fig. 1). Each round of continual learning as a unique task is associated with task-specific adapters and a task-specific classifier head. Model update at each round of continual learning is to find optimal parameters in the newly added task-specific adapters and classifier head. During inference, since multiple classifier heads exist, the correct head containing the class of a given input is expected to be selected. In order to establish the potential connections between tasks and further boost the continual learning performance, a two-stage multi-head learning strategy was proposed by including the idea of out-of-distribution (OOD) detection."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.2,Task-Specific Adapters,"State-of-the-art continual learning methods try to preserve old knowledge by either combining old fixed feature extractors with the newly learned feature extractor [30], or by using a shared and fixed pretrained feature extractor [32]. However, simply combining feature extractors over rounds of continual learning would rapidly expand the model, while using a shared and fixed feature extractor could largely limit model ability of learning new knowledge because only the model head can be tuned to discriminate between classes. To resolve this dilemma, we propose adding a light-weight task-specific module called adapter into a single pretrained and fixed CNN feature extractor (e.g., with the ResNet backbone), such that the model expands very slowly and the adapter-tuned feature extractor for each old task is fixed when learning a new task. In this way, old knowledge largely stored in the feature extractor and associated taskspecific adapters will be well preserved when the model learns new knowledge at subsequent rounds of continual learning. Formally, suppose the pretrained CNN feature extractor contains K stages of convolutional layers (e.g., 5 stages in ResNet), and the output feature maps from the k-th stage is denoted by z k , k ∈ {1, . . . , K -1}. Then, when the model learns a new task at the t-th round of continual learning, a task-specific adapter A t,k is added between the k-th and (k + 1)-th stages as follows,where the adapter-tuned output ẑt,k will be used as input to the (k +1)-th stage.The light-weight adapter can be flexibly designed. In this study, a simple twolayer convolution module followed by a global scaling is designed as the adapter (Fig. 1, Right). The input feature maps to the adapter are spatially downsampled by the first convolutional layer and then upsampled by the second layer. The global scaling factor α is learned together with the two convolutional layers. The proposed task-specific adapter for continual learning is inspired by Delta tuning [7] which adds learnable 2-layer perceptron(s) into a pretrained and fixedTransformer model in natural language processing. Different from Delta tuning which is used as a transfer learning strategy to adapt a pretrained model for any individual downstream task, the proposed task-specific adapter is used as a continual learning strategy to help a model continually learn new knowledge over multiple rounds (i.e., multiple tasks) in image processing, with each round corresponding to a specific set of adapters. Also note that the proposed adapter differs from existing adapters in CLIP-Adapter (CA) [9] and Tip-Adapter (TA) [33]. First, in structure, CA and TA use 2-layer MLP or cache model, while ours uses a 2-layer convnet with a global scaling factor. Second, the number and locations of adapters in model are different. CA and TA use adapter only at output of the last layer, while ours appears between each two consecutive CNN stages. Third, the roles of adapters are different. Existing adapters are for few-shot classification, while ours is for continual learning. It is also different from current prompt tuning. Prompts appear as part of input to the first or/and intermediate layer(s) of model, often in the form of learnable tokens for Transformer or image regions for CNNs. In contrast, our adapter appears as an embedded neural module for each two consecutive CNN stages, in the form of sub-network."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.3,Task-Specific Head,"Task-specific head is proposed to alleviate the potential feature fusion issue in current state-of-the-art methods [30] which combine task-specific features by a unified classifier head. In particular, if feature outputs from multiple taskspecific feature extractors are simply fused by concatenating or averaging followed by a unified 1-or 2-layer percepton (as in [30]), discriminative feature information appearing only in those classes of a specific task could become less salient after fusion with multiple (possibly less discriminative) features from other task-specific feature extractors. As a result, current state-of-the-art methods often require storing relatively more old data to help train a discriminative unified classifier head between different classes. To avoid the possible reduction in feature discriminability, we propose not fusing features from multiple feature extractors, but a task-specific classifier head for each task. Each task-specific head consists of one fully connected layer followed by a softmax operator. Specially, for a task containing C new classes, one additional class absorbing all previously learned old classes ('others' output neuron in Fig. 1) is also included, and therefore the number of output elements from the softmax will be C +1. The 'others' output is used to predict the probability of the input image being from certain class of any other task rather than from the current task. In other words, each task-specific head has the ability of out-of-distribution (OOD) ability with the help of the 'others' output neuron. At the t-round of continual learning (i.e., for the t-th task learning), the task-specific adapters and the task-specific classifier head can be directly optimized, e.g., by cross-entropy loss, with the C new classes of training data and the 'others' class of all preserved old data.However, training the task-specific classifier head without considering its relationship with existing classifier heads of previously learned tasks may cause the head selection issue during model inference. For example, a previously learned old classifier head may consider an input of latterly learned class as one of the old classes (correspondingly the 'others' output from the old head will be low). In other words, the 'others' outputs from multiple classifier heads cannot not be reliably compared (i.e., not calibrated) with one another if each classifier head is trained individually. In this case, if all classifier heads consider a test input as 'others' class with high confidence or multiple classifier heads consider a test input as one of their classes, it would become difficult to choose an appropriate classifier head for final prediction. To resolve the head selection issue, after initial training of the current task's adapters and classifier head, all the tasks' heads are fine-tuned together such that all 'others' outputs from the multiple heads are comparable. In short, at the t-th round of continual learning, the t task-specific classifier heads can be fine-tuned by minimizing the loss function L,where L c s is the cross-entropy loss for the s-th classifier head. Following the finetuning step in previous continual learning studies [4,30], training data of the current t-th task are sub-sampled such that training data in the fine-tuning step are balanced across all learned classes so far. Note that for each input image, multiple runs of feature extraction are performed, with each run adding adapters of a different task to the original feature extractor and extracting the feature vector for the corresponding task-specific head. Also note that in the fine-tuning step, adapters of all tasks are fixed and not tuned. Compared to training the adapters of each task with all training data of the corresponding task, fine-tuning these adapters would likely cause over-fitting of the adapters to the sub-sampled data and therefore is avoided in the fine-tuning step.Once the multi-head classifier is fine-tuned at the t-th round of continual learning, the classifier can be applied to predict any test data as one of all the learned classes so far. First, the task head with the smallest 'others' output probability (among all t 'others' outputs) is selected, and then the class with the highest output from the selected task head is selected as the final prediction result. Although unlikely selected, the 'others' class in the selected task head is excluded for the final prediction."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,3.1,Experimental Setup,"Four datasets were used to evaluate the proposed ACL (Table 1). Among them, Skin8 is imbalanced across classes and from the public challenge organized by the International Skin Imaging Collaboration (ISIC) [24]. Path16 is a subset of publicly released histopathology images collated from multiple publicly available datasets [1,6,13,25,29,35], including eleven diseases and five normal classes (see Supplementary Material for more details about dataset generation). These data In all experiments, publicly released CNN models which are pretrained on the Imagenet-1K dataset were used for the fixed feature extractor. During continual learning, the stochastic gradient descent optimizer was used for task-specific adapter learning, with batch size 32, weight decay 0.0005, and momentum 0.9. The initial learning rate was 0.01 and decayed by a factor of 10 at the 70th, 100th and 130th epoch, respectively. The adapters were trained for up to 200 epochs with consistently observed convergence. For fine-tuning classifier heads, the Adam optimizer was adopted, with initial learning rate 0.001 which decayed by a factor of 10 at the 55th, and 80th, respectively. The classifier heads were finetuned for 100 epochs with convergence observed. Unless otherwise mentioned, ResNet18 was used as the backbone, the size of memory for storing old images was 40 on Skin8, 80 on Path16, 2000 on CIFAR100 and 200 on MedMNIST.In continual learning, the classifier sequentially learned multiple tasks, with each task a small number of new classes (e.g., 2, 10, 20). After learning each task, the mean class recall (MCR) over all classes learned so far is used to measure the classifier's performance. Note that MCR is equivalent to classification accuracy for class-balanced test set. For each experiment, the order of classes is fixed, and all methods were executed three times with different initialization. The mean and standard deviation of MCRs over three runs were reported."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,3.2,Result Analysis,"Effectiveness Evaluation: In this section, we compare ACL against stateof-the-art baselines, including iCaRL [19], DynaER [30], DER++ [3], WA [34], PODNet [8], and UCIR [11]. In addition, an upper-bound result (from a classifier which was trained with all classes of training data) is also reported. Similar amount of effort was taken in tuning each baseline method. As shown in Fig. 2, our method outperforms all strong baselines in almost all settings, no matter whether the classifier learn continually 2 classes each time on Skin8 (Fig. 2, first column), in two different task orders on Path16 (Fig. 2, second column), in 10 or  20 classes each time on CIFAR100 (Fig. 2, last column), or in 4 different domains on MedMNIST [31] (Fig. 1 in Supplementary Material). Note that performance of most methods does not decrease (or even increase) at the last two or three learning rounds on Path16, probably because most methods perform much better on these tasks than on previous rounds of tasks.Ablation Study: An ablation study was performed to evaluate the performance gain of each proposed component in ACL. Table 2 (first four rows) shows that the continual learning performance is gradually improved while more components are included, confirming the effectiveness of each proposed component. In addition, when fusing all the task features with a unified classifier head (Table 2, last row), the continual learning performance is clearly decreased compared to that from the proposed method (fourth row), confirming the effectiveness of task-specific classifier heads for class-incremental learning. Generalizability Study: The pretrained feature extractor with different CNN backbones were used to evaluate the generalization of ACL. As shown in Table 3, ACL consistently outperforms representative strong baselines with each CNN backbone (ResNet18 [10], EfficientNet-B0 [23] and MobileNetV2 [21]) on Skin8, supporting the generalizability of our method."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,4.0,Conclusion,"Here we propose a new adapter-based strategy for class-incremental learning of new diseases. The learnable light-weight and task-specific adapters, together with the pretrained and fixed feature extractor, can effectively learn new knowledge of diseases and meanwhile keep old knowledge from catastrophic forgetting. The task-specific heads with the special 'out-of-distribution' output neuron within each head helps keep extracted features discriminative between different tasks. Empirical evaluations on multiple medical image datasets confirm the efficacy of the proposed method. We expect such adapter-based strategy can be extended to other continual learning tasks including lesion detection and segmentation."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 7.
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,1.0,Introduction,"Differential privacy (DP) has emerged as a promising technique to safeguard the privacy of sensitive data in federated learning (FL) [2,12,13,29,34], offering privacy guarantees in a mathematical format [7,25,33]. However, introducing noise to ensure DP often comes at the cost of performance. Some recent studies have noticed that the noise added to the gradient impedes optimization [6,14,27].For critical medical applications requiring low error tolerance, such performance degradation makes the rigorous privacy guarantee diminish [5,22]. Therefore, it is imperative to maintain high performance while enhancing privacy, i.e., optimizing the privacy-performance trade-off. Unfortunately, despite its significance, such trade-off optimization in FL has not been sufficiently investigated to date.Several studies have examined the trade-off in the centralized scenario. For instance, Li et al. [18] proposed enhancing utility by leveraging public data or data statistics to estimate gradient geometry. Amid et al. [3] utilized the loss on public data as a mirror map to improve performance. Li et al. [17] suggested constructing less noisy preconditioners using historical gradients. In contrast to these studies, we concentrate on promoting the trade-off in FL, where public dataset is limited and sharing side information may not be feasible [12,29]. Specifically, we aim to ensure that clients are differentially private. Our objective is not to protect a single data point, but rather to achieve that a learned model does not reveal whether a client participated in decentralized training. This ensures that a client's entire dataset is safeguarded against differential attacks from third parties. This is particularly crucial in medical imaging, where sensitive patient information is typically kept within each hospital. Nevertheless, in medical imaging, the number of participants (silos) is usually much smaller than in other domains, such as mobile devices [12]. This cross-silo situation necessitates adding a considerable amount of noise to protect client privacy, making the optimization of the trade-off uniquely challenging [20].To improve the trade-off of privacy protection and performance, the key point is to mitigate the noise added to the client during gradient updates. Our idea is inspired by the observation in DP-FedAvg [24], which suggests that the utility of DP can be improved by utilizing a sufficiently large dataset with numerous users. Through an analysis of the DP accountant, we identified that the noise is closely related to the gradient clip bound and the number of participants. In this regard, we propose to split the original client into disjoint sub-clients, which act as intermediaries for exchanging information between the hospital and the server. This strategy increases the number of client updates against queries, thereby consequently reducing the magnitude of noise. However, finding an optimal splitting is not straightforward due to the non-identical nature of data samples. Splitting a client into more sub-clients may increase the diversity of FL training, which can adversely harm the final performance. Thus, there is a trade-off between noise level and training diversity. Our objective is to explore the relationships among clients, noise effects, and training diversities to identify a balance point that maximizes the trade-off between privacy and performance.In this paper, we present a novel adaptive intermediary method to optimize the privacy-performance trade-off. Our approach is based on the interplay relationships among noise levels, training diversities, and the number of clients. Specifically, we observe a reciprocal correlation between the noise level and the number of intermediaries, as well as a linear correlation between the training diversity and the intermediary number. To determine the optimal number of intermediaries, we introduce a new term called intermediary ratio, which quantifies the ratio of noise level and training diversity. Our theoretical analysis demonstrates that splitting the original clients into more intermediaries achieves DP with the same privacy budget and DP failure probability. Furthermore, we show that when sample-level DP and client-level DP have equivalent noise levels, the variance of the difference between noisy and original model diverges exponentially with more training steps, leading to poor performance. We evaluate our method on both classification and segmentation tasks, including the intracranial hemorrhage diagnosis with 25,000 CT slices, and the prostate MRI segmentation with heterogeneous data from different hospitals. Our method consistently outperforms various DP optimization methods on both tasks and can serve as a lightweight add-on with good compatibility. In addition, we conduct comprehensive analytical studies to demonstrate the effectiveness of our method."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.1,Preliminaries,"In this work, we consider client-level differential privacy. We first introduce the definition of DP as follows: Definition 1. (( , δ)-Differential Privacy [7,8]) For a randomized learning mechanism M : X → R, where X is the collection of datasets it can be trained on, and Y is the collection of model it can generate, it is ( , δ)-DP if:where denotes the privacy budget, and δ represents the probability that -DP fails in this mechanism. Note that the smaller the value is, the more private the mechanism is. Our aim of applying DP is to protect the collection of ""datasets"" X , which are client model updates in every communication round in the context of FL. The protection can be done by incorporating a DP-preserving randomization mechanism into the learning process. One commonly used method is the Gaussian mechanism, which involves bounding the contribution (l 2 -norm) of each client update followed by adding Gaussian noise proportional to that bound onto the aggregate [24]. Specifically, suppose there are N clients, denote the gradients of each client as Δ i , the server model θ t+1 at round t+1 is updated by adding the Gaussian mechanism approximating the sum of updates as follows:where C is the gradient clipping threshold, and z is the noise multiplier determined by the privacy accountant with given , δ, and training steps. The noise multiplier z indicates the amount of noise required to reach a particular privacy budget. To privatize the participation of clients in FL, the noise added for client-level DP typically correlates with the number of clients. This incurs a large magnitude of noise in cross-silo FL in the medical field, which can significantly deteriorate the final server model performance."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.2,Adaptive Intermediary for Improving Client-Level DP,"The key to optimizing the privacy-performance trade-off lies in mitigating the effects of noise without compromising privacy protection. Based on the noise calculation in Eq. ( 1), we propose to study the final effects of noise on the server model, which can be denoted as ζ ∼ N (0, σ 2 I), where σ = zC /N. Note that the final noise (ζ) is determined by σ, which relates to the noise multiplier z, clip threshold C, and the number of clients N . In DP, the clip threshold and the noise multiplier are usually pre-assigned. Therefore, the noise level can be reduced by increasing the number of clients N . To this end, we propose to reduce the noise by splitting the original clients into non-overlapping sub-clients, which serve as intermediaries to communicate with the server (see Fig. 1 (a)). We validate our hypothesis by studying the feasibility and analyzing the relationships between the intermediary number, noise, and performance.Feasibility. We demonstrate the feasibility by showing the use of intermediary preserves privacy. For X the collection of possible datasets from extant clients, denote D i ∈ X the dataset of client i, we randomly split D i into v disjoint subsets D i,1 , ..., D i,v , so that j D i,j = D i . We define the dataset D i,j of client i as the intermediary j. Then we show that partitioning extant clients into multiple intermediaries is capable of maintaining DP. Denote the collection of all possible datasets formed by the intermediaries as Y, and note that X ⊆ Y. We have:This indicates that partitioning the original client into intermediaries keeps the same DP regime. The proof can be found in Appendix C. We also analyze the reverse relation in the appendix section to complete the overall relationship.Privacy-Performance Trade-Off Analysis. With the above basis, we further investigate the privacy-performance trade-off by varying the number of intermediaries. According to the noise calculation of σ = zC /N, we can reduce noise by splitting clients into intermediaries to increase N . However, increasing the number of intermediaries causes each intermediary to hold fewer samples. This may affect the aggregation direction and harms final performance consequently.There is a trade-off behind intermediary splitting. To investigate the trade-off, we design and study two highly related metrics, i.e., noise level ξ and client update diversity level ϕ. Denoting clipped gradients as Δi , we define the noise level and diversity level as:By varying the number of intermediaries, we obtain different values for noise levels and diversities (see Fig. 1 (b)). By fitting the relations between noise level (client update diversity) and the number of intermediaries for each client (denoted as v), we surprisingly find the relations that:where ξ v and ϕ v denote the value when each client is split into v intermediaries. By defining the intermediary ratio as λ = ξ /ϕ, we can use this ratio to quantify the relations between noise level and diversity, which helps identify the optimal number of intermediaries to generate.Adaptive Intermediary Generation. We can generate the intermediary based on the defined intermediary ratio λ. We experimentally investigated the relationships between the final performance and the number of intermediaries and found the optimal ratio lies in the range of 1 /N. Therefore, for each client, the number of intermediaries is v = 2 N • ξ /ϕ. Considering the extreme case of lim N →∞ ξ = 0, we can also infer the ratio λ = 0, which further validates the rationality and consistency with our empirical findings. For the practical application, we can initialize the number of intermediaries via the first round results. Then, for each round, we will re-calculate the ratio using ξ and ϕ from the last round, and then adaptively split clients to make sure the new ratio lies around 1 /N."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.3,Cumulation of Sample-Level DP to Client-Level,"We further investigate the relationships between client-level DP and sample-level DP, by cumulating sample-level DP mechanism to a client level. In DP-SGD [1], denote the standard deviation of Gaussian noise as σ = z( , δ)c/K with K being the batch size, c being the sample-level gradient clip bound and z being the noise multiplier determined by privacy accountant with ( , δ). Noise is added to each batch gradient before taking a descent, so that each step is ( , δ)-DP.Note that z can take different forms, the form provided by moment accountant [1] is z( , δ) = O( ln(1/δ)/ 2 ). Through the use of the moment accountant and sensitivity cumulation, we can calculate the standard deviation of cumulated noise in T steps as  T ). With regards to performance, we prove in Appendix C that the variance of the difference between the noisy model and the original model diverges with a rate of O((1 -2ηβ + η 2 μ 2 ) T ) for μ-convex, β-smooth loss functions. This shows that increasing T also increases the probability of obtaining a model which diverges further from the original model, resulting in poorer performance.On the Client-Level. For client-level noise, we can compute the standard deviation as σ c = z( c , δ c )C, where C is the clip bound of client update. The clip bound is typically set to the median among l2-norms of all client updates. Assuming an identical distribution across clients and samples, we have C = O(T c). As a result, we have z c = O(z T ), indicating that the cumulation of sample-level noise in DP-SGD gives the same DP level up to a constant, which is equivalent to adding noise directly to the client level through the moment accountant. Regarding the performance, we note that by leveraging the noisy models from several clients that hold identically distributed datasets, we can reduce the probability of getting a significantly drifted model without additional privacy leakage."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.1,Experimental Setup,"Datasets. We evaluate our method on two tasks: 1) intracranial hemorrhage (ICH) classification, and 2) prostate MRI segmentation. For ICH classification, we use the RSNA-ICH dataset [9] and follow [15] to relieve the class imbalance across ICH subtypes and perform the binary diseased-or-healthy classification. We randomly sample 25,000 slices and split them into 20 clients, where each client data is split into 60%, 20%, and 20% for training, validation, and testing. We resize images to 224 × 224 and perform data augmentation with random affine and horizontal flip. For prostate segmentation, we adopt a multi-site T2weighted MRI dataset [21] which contains 6 different data sources from 3 public datasets [16,19,26]. We regard each data source as one client, resize images to 256 × 256, and use 50%, 25%, and 25% for for training, validation and testing.Table 1. Performance comparison of different DP optimization methods and ours. We report mean and standard deviation across three independent runs with different seeds."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,,Method,"No Privacy z = 0.5 z = 1.0 z = 1.5 Privacy Setup. We use the Opacus' [32] implementation of privacy loss random variables (PRVs) accountant [10] for the Gaussian mechanism for our privacy accounting. We restrict the total number of training rounds and then account for any privacy overheads with various privacy levels controlled by the noise multiplier z, where a higher z indicates a higher privacy regime . Adaptive clipping [4] is employed to bound each client's contribution in the federation. Following [33], we report the results by exploring effects of different noise multiplier z values. We set z in the range of {0.  -k where k is the smallest integer that satisfies 10 -k ≤ 1/n for the client number n as suggested by [17].Implementation Details. We use Adam optimize, set the local update epoch to 1, and set total communication rounds to 100. We use DenseNet121 [11] for classification, the batch size is 16 and the learning rate is 3 × 10 -4 . We use UNet [30] for segmentation, the batch size of 8, and the learning rate is 10 -3 ."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.2,Empirical Evaluation,"First, we present experimental results using different global optimizers on the server with client-level DP. Then, we demonstrate how our adaptive intermedi-ary strategy benefits privacy-performance trade-offs. We consider four popular private server optimizers: DP-FedAvg [24] which adds client-level privacy protection to FedAvg [23], DP-FedAdam which is a differentially private version of the optimizer FedAdam [28], DP-FedNova which we equip the global solver FedNova [31] for client-level DP, and DP 2 -RMSProp [17] which is a very recent private optimization framework and we deploy it as the global optimizer in FL.We perform validation with different noise multiplier values. Non-private FL is also provided as a performance upper bound. Note that our method has the same performance ascompared methods in non-private settings, because there are no noises to harmonize. As can be observed from Table 1, severe performance degradation occurs in the private cross-silo FL setting, especially for high-privacy regimes (e.g., z = 0.7 for prostate segmentation). There are no significant differences among different global optimizers, which shows that the optimizers carefully designed for non-private FL are unable to address the noisy gradient issue in DP settings. However, our method relieves the gradient corruption and consistently and substantially boosts performance even with large noises (e.g., 44.55% Dice boost on prostate segmentation with z = 0.7). We also identify that the influences on performance introduced by DP may vary across different tasks and client numbers. For example, the segmentation task with fewer clients is more seriously damaged compared with the classification task with more clients. "
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.3,Analytical Studies,"Effects of Optimizing Privacy-Performance Trade-Offs. We present the dynamic behavior of our method regarding variations of the intermediary ratio λ across different rounds in Fig. 2 (a). Compared with DP-FedAvg [24], where λ shows a significant increase with the rise of noise multiplier z, our method harmonizes this trend with more centralized distributions by the adaptive intermediary for better privacy-performance trade-offs. In Fig. 2 (b), we also study the standard deviation of similarities, which is another metric for quantifying gradient diversity between local and global gradients. Our method shows more stable optimization directions with less variance among clients. Moreover, we observe a decline in gradient diversities as the privacy regime rises for DP-FedAvg [24]. To interpret, we speculate that local optimization may be dominated by greater noises for more common gradient de-corruption.Client Scalability Analysis. As the noise level is highly dependent on client numbers (see Eq. ( 1) and Table 1), we investigate the scalability of DP-FedAvg [24] and our method by varying number of clients. Figure 2 (c) presents the results on prostate segmentation with different training clients (z = 0.3). Notably, we keep test data unchanged for fair comparisons. We observe a dramatic drop in performance of DP-FedAvg [24] due to excessive noise when the number of clients shrinks. However, our method performs stably even under extreme conditions, e.g., the federation only has two participants."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,,Stability of Adaptive Intermediary,"Estimation. Finally, we analyze the historical variation of our adaptive intermediary strategy in Fig. 2 (d), where we present the intermediary numbers during the training progress. We expect that more intermediaries are required to balance the privacy-performance trade-off with a greater noise multiplier z. Besides, we verify the reliability and stability of our adaptive intermediary estimation by showing that the variation during the training does not exceed one, except for a single instance when z = 0.7."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,4.0,Conclusion,"In this paper, we propose a novel adaptive intermediary method to promote privacy-performance trade-offs in the context of client-level DP in FL. We have comprehensively studied the relations among number of intermediaries, noise levels and training diversities in our work. We also investigate relations between sample-level and client-level DP. Our proposed method outperforms compared methods on both medical image diagnosis and segmentation tasks and shows good compatibility with existing DP optimizers. For future work, it is promising to investigate our method for clients with imbalanced class distributions, where the intermediary may not have all labels."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,1.0,Introduction,"Semantic segmentation of organs, anatomical structures, or anomalies in medical images (e.g. CT or MRI scans) remains one of the fundamental tasks in medical image analysis. Volumetric medical image segmentation (MIS) helps healthcare professionals to diagnose conditions more accurately, plan medical treatments, and perform image-guided procedures. Although deep neural networks (DNNs) have shown remarkable improvements in performance for different vision tasks, A model trained on voxel-domain adversarial attacks is vulnerable to frequency-domain adversarial attacks. In our proposed adversarial training method, we generate adversarial samples by perturbing their frequency-domain representation using a novel module named ""Frequency Perturbation"". The model is then updated while minimizing the dice loss on clean and adversarially perturbed images. Furthermore, we propose a frequency consistency loss to improve the model performance.including volumetric MIS, their real-world deployment is not straightforward particularly due to the vulnerabilities towards adversarial attacks [26]. An adversary can deliberately manipulate input data by crafting and adding perturbations to the input that are imperceptible to the human eye but cause the DNN to produce incorrect outputs [10]. Adversarial attacks pose a serious security threat to DNNs [1], as they can be used to cause DNNs to make incorrect predictions in a wide range of applications, including DNN-based medical imaging systems. To mitigate these threats, various techniques have been explored, including adversarial training, input data transformations, randomization, denoising auto-encoders, feature squeezing, and robust architectural changes [1]. Although significant progress has been made in adversarial defenses, however, this area is still evolving due to the development of attacks over time [3].Ensuring the adversarial robustness of the models involved in safety-critical applications such as, medical imaging and healthcare is of paramount importance because a misdiagnosis or incorrect decision can result in life-threatening implications. Moreover, the weak robustness of deep learning-based medical imaging models will create a trust deficit among clinicians, making them reluctant to rely on the model predictions. The adversarial robustness of the medical imaging models is still an open and under-explored area [6,20]. Furthermore, most adversarial attacks and defenses have been designed for 2D natural images and little effort has been made to secure volumetric (3D) medical data [20].In the context of 2D natural images, it has been recently observed that frequency-domain based adversarial attacks are more effective against the defenses that are primarily designed to ""undo"" the impact of pixel-domain adversarial noise in natural images [7]. Motivated by this observation in 2D natural images, here we explore the effectiveness of frequency-domain based adversarial attacks in the regime of volumetric medical image segmentation and aim to obtain a volumetric MIS model that is robust against adversarial attacks. To achieve this goal, we propose a min-max objective for adversarial training of volumetric MIS model in frequency-domain. For maximization step, we introduce Volumetric Adversarial Frequency Attack -VAFA (Fig. 1, Sect. 2.1) which operates in the frequency-domain of the data (unlike other prevalent voxeldomain attacks) and explicitly takes into account the 3D nature of the volumetric medical data to achieve higher fooling rate. For minimization step, we propose Volumetric Adversarial Frequency-domain Training -VAFT (Fig. 1, Sect. 2.2) to obtain a model that is robust to adversarial attacks. In VAFT, we update model parameters on clean and adversarial (obtained via VAFA) samples and further introduce a novel frequency consistency loss to keep frequency representation of the logits of clean and adversarial samples close to each other for a better accuracy tradeoff. In summary, our contributions are as follows:-We propose an approach with a min-max objective for adversarial training of volumetric MIS model in the frequency domain. In the maximization step, we introduce a volumetric adversarial frequency attack (VAFA) that is specifically designed for volumetric medical data to achieve higher fooling rate. Further, we introduce a volumetric adversarial frequency-domain training (VAFT) based on a frequency consistency loss in the minimization step to produce a model that is robust to adversarial attacks. -We conduct experiments with two different hybrid CNN-transformers based volumetric medical segmentation methods for multi-organ segmentation.Related Work: There are three main types of popular volumetric MIS model architectures: CNN [23], Transformer [13] and hybrid [12,24]. Research has shown that medical machine learning models can be manipulated in various ways by an attacker, such as adding imperceptible perturbation to the image, rotating the image, or modifying medical text [8]. Adversarial attack studies on medical data have primarily focused on classification problems and voxel-domain adversaries. For example, Ma et al. [20] have used four types of pixel-domain attacks [4,10,16,21] on two-class and multi-class medical datasets. Li et al. [19] and Daza et al. [6] have focused on single-step and iterative adversarial attacks [5,10,15] on the volumetric MIS. In constant to voxel-domain adversarial attacks, our approach works in the frequency-domain."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.0,Frequency Domain Adversarial Attack and Training,"We aim to train a model for volumetric medical segmentation that is robust against adversarial attacks. Existing adversarial training (AT) approaches rely on min-max optimization [10,16,21] and operate in the input space. They find adversaries by adding the adversarial perturbation to the input samples by Compared to different voxel-domain attacks (PGD [21], FGSM [10], BIM [16] and GN [14]), our attack (VAFA) achieves higher fooling rate (highlighted in red bounding box) while maintaining comparable perceptual similarity. Best viewed zoomed in.maximizing the model loss (e.g., dice loss in segmentation). The loss function is then minimized on such adversaries to update the model parameters. In this work, we propose a frequency-domain adversarial attack that takes into account the 3D nature of the volumetric medical data and performs significantly better than the other voxel-domain as well as 2D frequency domain attacks (Table 1). Based on our attack, we then introduce a novel frequency-domain adversarial training to make the model resilient to adversarial attacks. Additionally, we observe that our approach improves/retains the performance of the robust model on clean samples when compared to the non-robust model. Our approach optimizes adversarial samples by perturbing the 3D-DCT coefficients within the frequency domain using our frequency perturbation module (Fig. 1) and adversarial guidance from the segmentation loss (Sect. 2.1). We find adversarial samples with high perceptual quality by maximizing the structural similarity between clean and adversarial samples. Using clean and adversarial samples, we propose updating the model parameters by simultaneously minimizing the segmentation loss (i.e. Dice loss) and the frequency consistency loss (Eq. 4) between the clean and adversarial outputs of the segmentation model.3D Medical Segmentation Framework: Deep learning-based 3D medical segmentation generally uses encoder-decoder architectures [18]. The encoder produces a latent representation of the input sample. A segmentation map of the input sample is generated by the decoder using the latent feature representation. The decoder usually incorporates skip connections from the encoder to preserve spatial information [12]. Next, we describe our proposed volumetric frequencydomain adversarial attack in Sect. 2.1 and then training in Sect. 2.2."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.1,Volumetric Adversarial Frequency Attack (VAFA),"Generally, adversarial attacks operate in the voxel domain by adding an imperceptible perturbation to the input data. In contrast, our attack perturbs the 3D-DCT coefficient to launch a frequency-domain attack for 3D medical image segmentation. Our Frequency Perturbation Module (FPM) transforms voxel-domain data into frequency-domain by using discrete cosine transforms (DCTs) and perturbs the DCT coefficients using a learnable quantization. It then takes an inverse DCT of the perturbed frequency-domain data and returns voxel-domain image.We keep the model in a ""frozen"" state while maximizing the dice loss [25] for segmentation and minimizing structural similarity loss [27] for perceptual quality.We represent a 3D (volumetric) single channel clean sample by X ∈ R 1×H×W ×D and its ground-truth binary segmentation mask by Y ∈ {0, 1} NumClass×H×W ×D , where ""NumClass"" is the number of classes. We split X into n 3D patches i.e. X → {x i } n i=1 , whereWe apply our frequency perturbation module to each of these patches.Frequency Perturbation Module: We apply a 3D discrete cosine transform (DCT), represented as D(•), to each patch x i . The resulting DCT coefficients are then processed through a function ϕ(•), which performs three operations: quantization, differentiable rounding (as described in [9]), and subsequent de-quantization. ϕ(•) utilizes a learnable quantization table q ∈ Z h×w×d to modify the DCT coefficients, setting some of them to zero. In particular, ϕ(D(x), q) := D(x) q q, where DCT coefficients of a patch (i.e. D(x)) are element-wise divided by quantization table q. After the division operation, the result undergoes rounding using a differentiable rounding operation [9], resulting in some values being rounded down to zero. The de-quantization step involves element-wise multiplication of D(x) q with the same quantization table q. This step allows us to reconstruct the quantized DCT coefficients. Since quantization table is in the denominator of the division operation, therefore, higher quantization table values increase the possibility of more DCT coefficients being rounded down to zero. To control the number of DCT coefficients being set to zero, we can constrain the values of the quantization table to a maximum threshold (constraint in Eq. 2). In other words, ϕ(•) performs a 3D adversarial lossy compression on input through a learnable quantization table. Finally, a 3D inverse DCT (IDCT) is performed on the output of ϕ(•) in order to obtain an adversarially perturbed voxel-domain representation, denoted by x . We show our frequency perturbation module in Eq. 1 as follows:x → D(x) → ϕ(D(x), q) quantization, rounding and de-quantizationWe repeat the above mentioned sequence of transformations for all patches and then merge {x i } n i=1 to form adversarial image X ∈ R H×W ×D . Quantization Constraint: We learn quantization table q by maximizing the L dice while ensuring that q ∞ ≤ q max . Quantization threshold q max controls the Algorithm 1. Volumetric Adversarial Frequency Attack (VAFA)"
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,,". , n}","Initialize all quantization tables with ones. 5:for t ← 1 to T do 6:Merge all adversarial patches to form X 9:L(X, X , Y) = L dice (M θ (X ), Y) -Lssim(X, X ) 10:end for 13: end function 14: Return X Algorithm 2. Volumetric Adversarial Frequency Training (VAFT)Freq. Attack on clean images. 7:Backward pass and update M θ 9:end for 10: end for 11: M ← M θ AT robust model after training completion. 12: Return M extent to which DCT coefficients are perturbed. The higher the value of q max , the more information is lost. The drop in perception quality of the adversarial sample and the accuracy of the model are directly proportional to the value of q max . To increase the perceptual quality of adversarial samples, we also minimize the structural similarity loss [27] between clean and adversarial samples, denoted by L ssim (X, X ), in optimization objective. Our attack optimizes the following objective to fool a target model M θ :where L ssim (X, X ) = 1 -1 n n i=1 SSIM(x i , x i ) is structural similarity loss [27]. Algorithm 1 presents our volumetric adversarial frequency attack (VAFA). An overview of the attack can be found in maximization step of Fig. 1."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.2,Volumetric Adversarial Frequency Training (VAFT),"The model parameters are then updated by minimizing the segmentation loss on both clean and adversarial samples (Eq. 3). Since our attack disrupts the frequency domain to find adversaries, we develop a novel frequency consistency loss (Eq. 4) to encourage frequency domain representation of the model's output (segmentation logits) for the clean sample close to the adversarial sample. Our frequency consistency loss not only boosts the robustness of the model against adversarial attacks but also improves/retains the performance of the robust model on clean images (Sect. 3). We present our volumetric adversarial frequency training (VAFT) in Algorithm 2.where X = VAFA(X, Y) and D(•) is 3D DCT function. An overview of the adversarial training can be found in minimization step of Fig. 1. Figure 2 presents a qualitative results of adversarial examples under different attacks on the standard UNETR model. We highlight areas by red bounding box in Fig. 2 to show the impact of each attack on the model performance, when compared with prediction on clean sample. Our attack (VAFA) achieves higher fooling rate as compared to other voxel-domain attacks, while maintaining comparable perceptual similarity."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,3.0,Experiments and Results,"Implementation Details: We demonstrate the effectiveness of our approach using two medical segmentation models: UNETR [12], UNETR++ [24] and two datasets: Synapse (18-12 split) [17], and ACDC [2]. Using pre-trained models from open-source Github repositories by the corresponding authors, we launch different adversarial attacks and conduct adversarial training with default parameters. We use the Pytorch framework and single NVIDIA A100-SXM4-40GB GPU for our experiments. For a pixel/voxel range [0, 255], we create l ∞ adversarial examples under perturbation budgets of ∈ {4, 8} for voxel-domain attacks following [7] and compare it with our attack VAFA. Unless otherwise specified, all attacks are run for a total of 20 optimization steps. More details about the parameters of the attacks used in different experiments can be found in Appendix. We use mean Dice Similarity Score (DSC), mean 95% Hausdorff Distance (HD95). We also report perceptual similarity between clean and adversarial sample (LPIPS) [28].Results: For each evaluation metric, we take mean across all classes (including background) and test images. In each table (where applicable), green values show DSC and HD95 on clean images. Table 1 shows comparison of voxel-domain attacks (e.g. PGD [21], FGSM [10], BIM [16], GaussianNoise(GN) [14]) with VAFA-2D (2D DCT in FPM applied on each scan independently) and VAFA on UNETR model (Synapse). VAFA achieves a higher fooling rate as compared to other attacks with comparable LPIPS. We posit that VAFA-2D on volumetric MIS data is sub-optimal and it does not take into account the 3D nature of the data and model's reliance on the 3D neighborhood of a voxel to predict its class. Further details are provided in the supplementary material. We show impacts of different parameters of VAFA e.g. quantization threshold (q max ), steps, and patch size (h × w × d) on DSC and LPIPS in Table 2, 3 and 4 respectively. DSC and LPIPS decrease when these parameters values are increased. Table 5 shows a comparison of VAFA (patch size = 32 × 32 × 32) with other voxeldomain attacks on UNETR and UNETR++ models. For adversarial training experiments, we use q max = 20 (for Synapse), q max = 10 (for ACDC) and patchsize of 32 × 32 × 32 (chosen after considering the trade-off between DSC and LPIPS from Table 4) for VAFA. For voxel-domain attacks, we use = 4 (for Synapse) and = 2 (for ACDC) by following the work of [11,22]. Table 6 presents a comparison of the performance (DSC) of various adversarially trained models against different attacks. M VAFA-FR , M VAFA denote our robust models which were adversarially trained with and without frequency consistency loss (L fr , Eq. 4) respectively. In contrast to other voxel-domain robust models, our approach demonstrated robustness against both voxel and frequency-based attacks."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,4.0,Conclusion,"We present a frequency-domain based adversarial attack and training for volumetric medical image segmentation. Our attack strategy is tailored to the 3D nature of medical imaging data, allowing for a higher fooling rate than voxelbased attacks while preserving comparable perceptual similarity of adversarial samples. Based upon our proposed attack, we introduce a frequency-domain adversarial training method that enhances the robustness of the volumetric segmentation model against both voxel and frequency-domain based attacks. Our training strategy is particularly important in medical image segmentation, where the accuracy and reliability of the model are crucial for clinical decision making."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_43.
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,1.0,Introduction,"The brain consumes copious amounts of energy to sustain its activity, resulting in a skewed energetic budget per mass compared to the rest of the body (about 25% utilized by about 3%, see [2,16] for an elaborate review of energy utilization). Given this disproportionate need, resources are allocated on a need-basis: active areas signal to the nearby blood vessel to dilate and increase blood flow, bringing a surplus of resources, to that area. This fundamental physiological process is called neurovascular coupling. It is non-trivial to model and different types of neuronal activity have been shown to elicit opposite vascular responses.Neurovascular coupling is a cornerstone of proper brain function and also underpins the ability to observe and study the human brain in action. Imaging methods based on blood oxygenated level dependent (BOLD) approaches rely on it [13], as do methods that are based on rheological properties, such as blood volume and flow speed. Since these methods do not directly measure neuronal activity per-se, but a physiological proxy, i.e. the resulting change in vascular dynamics and oxygen levels, it is of utmost importance to know the precise transform function linking neuronal activity to the observed vascular dynamics. Given the differential response to neuronal activity (see [5] for a timely review), obtaining a cellular and population level hemodynamic response function (HRF) remains an unmet need in this field, that would finally unlock the ability to infer neuronal activity directly from blood flow dynamics [20].The initial characterization of the hemodynamic response function (HRF) was performed at the system level, where system refers to large cortical regions encompassing tens of thousands of neurons of different types, without taking into account the fine details of different vascular compartments (see [29] for a succinct review on the original works). At this level, a canonical response function was derived from extensive work on sensory-evoked somatosensory responses. This HRF has become widely accepted and used in the interpretation of BOLD signals. This function consists of three components: an initial dip (its existence and physiological origin are much debated), a prolonged and very pronounced overshoot, followed by a shallower and much shorter undershoot. The initial dip occurs within one second of the sensory stimulus, the overshoot peaks around five seconds later, overshoot and return to baseline level occurs within 15-20 s post stimuli. It should be noted that vascular reactivity is much faster than the collective behavior described by the canonical HRF, with reports showing sensoryevoked vascular responses observed after just 300ms. Recently, more advanced imaging and analysis methods have pushed the formulation of an HRF at the single cell to single blood vessel (capillary) level, pointing to a rather narrow family of possible functions. Importantly, this work also established that the HRF derived at the microscopic level can be partially translated to macroscopic imaging approaches. Nevertheless, single neuron to single vessel responses fail to capture the more complex and varied neuronal population level responses that could be integrated across the extensive vascular network that surrounds them. Here, we exploit a unique dataset, in which neuronal and vascular responses (changes in diameter) were recorded in a volumetric fashion and with relevant temporal resolution, allowing us to establish a novel pipeline to uncover/formulate a many-to-many HRF.Our model needs to combine neuron firing and blood vessel data and employs a multi-modal transformer. There are three types of multi-modal transformers: (i) a multi-modal Transformer where the two modalities are concatenated and separated by the [SEP] token [18,19], and self-attention is used, (ii) coattention-based model modules that contextualize each modality with the other modality [21,26], and (iii) generative models containing an encoder that uses self-attention on the input and a decoder that uses both the encoded data and data from the decoder's domain as inputs [3,17,23,[30][31][32]34]. Our model is of the third type and presents two distinctive properties: pulling from multiple time points and an attention mechanism that is modulated based on distance.Our results show that the new transformer model can predict the state of blood vessels better than the baseline models. The utility of neuronal data in the prediction is demonstrated by an ablation study. By analyzing the learned model, we obtain insights into the link between neuronal and vascular activities."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,2.0,Data,"All procedures were approved by the Ethics Committee of Tel Aviv University for Animal Use and Welfare and followed pertinent Institutional Animal Care and Use Committee (IACUC) and local guidelines. Neuronal activity was monitored in female C57BL/6J transgenic mice expressing Thy1-GCaMP6s. Vascular dynamics were tracked using a Texas Red fluorescent dye, which was conjugated to a large polysaccharide moiety (2 mega Dalton dextran) and retro-orbitally bolus injected under brief isoflurane sedation at the beginning of the imaging day.425 quasi-linear vascular segments and 50 putative neuronal cell bodies were manually labeled within a volume of 490×500×300μm 3 , which was continuously imaged across two consecutive 1850-second long sessions at an imaging rate of 30.03 volumes per second [9][10][11][12]. For neuronal activity estimation, we selected a cuboid volume of interest around each neuronal cell body and summed the fluorescence within it following an axial intensity normalization corresponding to an uneven duty cycle of our varifocal lens.For vascular diameter estimation we used the Radon transform, [1,[6][7][8][9]22,24,28] as its resilience to rotation and poor contrast are particularly useful for our application. Specifically, Gao and Drew have formerly found that thresholding the vascular intensity profile in Radon space is more resilient to noise than other thresholding methods [8]. Based on their observation, we used the timecollapsed imagery to determine a threshold in Radon space, which was then applied separately for each frame in time.This unique ability to rapidly track neuronal and vascular interactions across a continuous brain volume bears several important advantages [9,10]. In particular, a greater proportion of the vascular ensemble that reacts to a given neuronal metabolic demand can be accounted for."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,3.0,Method,"The HRF learning problem explored in this work is defined as the prediction of current blood flow rates at different vessel segments, given the previous neuronal spikes as well as previous blood flow rates. We propose to design a parameterized deep neural network f θ for scalar regression of blood flow rates at different vessel segments, such that at a given time t we havewhere the matrix S t ∈ R ts×n denotes the n neurons' spikes at the t s previous samples, while the matrix F t ∈ R tv×m denotes the blood flow of the m vessel segment at the previous t v time samples. X S ∈ R n×3 and X F ∈ R m×3 are the three-dimensional positions of the neurons and vessel segments, respectively. HRF predictions should satisfy fundamental symmetries and invariance of physiological priors and of experimental bias, such as invariance to rigid spatial transformation (rotation and translation). Therefore, a positional input X u is transformed to inter-elements Euclidean distancesThus, the learning problem is refined as, where D S , D F , D SF represents the Euclidean distance matrix between neurons, vessel segments, and neurons to vessel segments, respectively. We do not include any further auxiliary features or prior in the input.We model f θ using a new variant of the Transformer family. The proposed model consists of an encoder and a decoder. The encoder embeds the neurons at both spatial and temporal levels. The decoder predicts vessel segment flow by utilizing both the past flow values and the spatial information of the vessel segments, along with the neuronal activity via the cross-attention mechanism."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Transformers.,"The self-attention mechanism introduced by Transformers [30] is based on a trainable associative memory with (key, value) vector pairs, where a query vector q ∈ R d is matched against a set of k key vectors using scaled inner products, as followswhere Q ∈ R N ×d , K ∈ R k×d and V ∈ R k×d represent the packed N queries, k keys and values tensors respectively. Keys, queries and values are obtained using linear transformations of the sequence's elements. A multi-head self-attention layer is defined by extending the self-attention mechanism using h attention heads, i.e. h self-attention functions applied to the input, reprojected to values via a dh × D linear layer."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Neuronal Encoding.,"To obtain the initial Spatio-Temporal Encoding, for the prediction at time t, we project each neuron to a high d dimensional embedding φ s t ∈ R ts×n×d by modulating it with its spike value such that, where W ∈ R d denotes the neuronal encoding. The embedding is modulated by the magnitude of the spike, such that higher neuronal activities are projected farther in the embedding space.The temporal encoding is defined using sinusoidal encoding [30] applied on φ and augmented with a learnable embedding such that φ s t ← φ s t + p t • p where p t and p represent the sinusoidal time encoding and the learned vector, respectively. We emphasize the fact that, contrary to traditional transformers, the embedding tensor φ t has an additional spatial dimension such that the tensor is threedimensional, enabling both spatial and temporal attention.In order to incorporate the spatial information of the neurons, we propose to insert spatial encoding by importing the pairwise information directly into the self-attention layer. For this, we multiply the distance relation by the similarity tensor as followswith denoting the Hadamard product, and ψ S (D S ) : R + → R + an elementwise learnable parameterized similarity function. This way, the similarity function scales the self-attention map according to the distance between the elements (in our case the neurons)."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Vascular Decoding.,"The spatio-temporal encoding of the vascular data is similar to the embedding performed by the encoder. The information on each vascular segment is embedded in a high-dimensional vector φ F t ∈ R tv×m×d to be further projected by the temporal encoding. The spatial geometric information is incorporated via the pairwise vascular segments' distance matrix D F via the decoder's self-attention module A F .The most important element of the decoder is the cross-attention module, which incorporates neuronal information for vascular prediction. Given the final neuronal embeddings φ s t , the cross-attention module performs cross-analysis of the neuronal embeddings such thatwhere Q F and K S represent the affine transform of φ F t and φ s t , respectively. Here also, the (non-square) cross-attention map is modulated by the neuronvessel distance matrix D SF .The spatio-temporal map is of dimensions A SF ∈ R tv×ts×h×m×n where h denotes the number of attention heads. Thus, we perform aggregation by averaging over the neuronal time dimension, in order to remain invariant to the temporal neuronal embedding and to gather all past neuronal influence on blood flow rates. This way, one can observe that the proposed method is not limited to any spatial or time constraint. The model can be deployed in different spatiotemporal settings at test time, thanks to both the geometric spatial encoding and the Transformer's sequential processing ability. Finally, the output module reprojects the last time vessel embedding into the prediction space."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Architecture and Training.,"The initial encoding defines the model embedding dimension d = 64. The encoder and the decoder are defined as the concatenation of L = 3 layers, each composed of self-attention and feed-forward layers interleaved with normalization layers. The decoder also contains N additional cross-attention modules. The output layer is defined by a fully connected layer that projects the last vascular time embedding into the objective dimension m. An illustration of the model is given in Fig. 1.The dimension of the feed-forward network is four times that of the embedding [30]. It is composed of GEGLU layers [25], with layer normalization set to the pre-layer norm setting, as in [15,33]. We use an eight-head self-attention module in all experiments. The geometric filtering first augments the distance using Fourier features [27] and the module is a fully connected neural network with two 50-dimensional hidden layers and GELU non-linearities, expanded to all the heads of the self-attention module. We provide the module with the element-wise inverse of the distance matrix instead of the regular Euclidean matrix, both in order to reduce the dynamic range and since closer elements may have a higher impact.The training objective is the Mean Squared Error lossThe Adam optimizer [14] is used with 32 samples per minibatch, for 300 epochs. We initialized the learning rate to 5 • 10 -5 coupled with a cosine decay scheduler down to 1 • 10 -6 at the end of the training. The dataset of the first data collection session has been split by 85%, 7.5% and 7.5% for the training, validation, and testing set, respectively. Training time is approximately 20 h for time windows t s = t v = 10, on an NVIDIA RTX A600. Testing time is approximately 0.25 ms per sample."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,4.0,Experiments,"We compare the proposed method, dubbed Hemodynamic Response Function Transformer (HRFT), with several popular statistical and machine-learning models: (i) naive persistence model, which predicts the previous time step's vascular input, (ii) linear regression, which concatenates all the input (blood flow and neuronal data) from all times stamps before performing the regression, and (iii) a Recurrent Neural Network composed of two stacked GRU [4] layers. All the methods are experimented with using the same inputs and the models have similar capacity (∼0.5 M parameters). In order to understand the impact of neuronal information, we also compare our method with the HRFT encoder only applied to the vascular input, referred to as HRFT-S. The only difference between this model and the full HRFT is the cross-attention module. If the neuronal input is irrelevant or the link is too weak to improve the prediction, HRFT is expected not to outperform HRFT-S, which makes the comparison pertinent.We present both MSE and Normalized Root MSE. Because of computational constraints, we randomly subsample 55 vessel segments among the 425. We trained the model with temporal windows of size t s = t v = 10, equivalent to 300 ms according to the original data acquisition's 30.03 Hz sampling rate.In addition to the original sampling rate, we also present results for prediction based on lower frequencies, in order to check the ability of the models to capture longer-range dependencies. We note that the error in these cases is expected to be larger, since the time gap between the last measurement and the required prediction is larger.In order to check the generalization abilities of the methods, we test the trained models on a second dataset obtained 30 min after the sampling of the original dataset (that includes training, validation, and the first test set). The results are presented in Table 1. As can be seen, the HRFT method outperforms all baselines, including the HRFT-S variant, for 6 Hz and 15 Hz. At the original sampling rate, the performance of HRFT and HRFT-S is similar and better than the baselines. This is expected since at this frame rate the history of t v = 10 we employ spans only 300 ms, which is at the limit of the shortest known neurovascular response reported in the literature [29]. It is reassuring that error levels for HRFT remain similar for samples taken 30min after the training set (and the first test set) were collected.To gain insights into the HRF, we examine the HRFT model. The learned distance function ψ SF of the 1st cross attention layer is depicted in Fig. 2(a) (other layers are similar). The plot shows the learned function in blue and the actual samples in red. Evidently, this prior on the attention is monotonically decreasing with the distance between the neuron and the blood vessel. Panel (b) shows the cross-attention in the same layer. We note that some neurons have little influence, and the rest of the attention is scattered relatively uniformly. Panel (c) considers the derivative of the prediction vector F t+1 by each of the neuron data, summed over all test samples of the 2nd session at 6 Hz, and all neurons and vessels. There are two negative peaks (contractions) that occur at 333 ms and 1333 ms, which is remarkably consistent with current knowledge [29]. There is also a dilation effect at 666 ms. The 15 Hz data with t v = 10 captures shifts of 0-700 ms and the 300 ms peak is clearly visible in that model as well."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,5.0,Conclusions and Future Work,"We present the first local HRF model. While for the baseline methods, the performance is at the same level with and without neuronal data (omitted from the tables), the transformer we present supports an improved prediction capability using neuronal firing rates (ablation) and also gives rise to interesting insights regarding the behavior of the hemodynamic response function.Limitations. Our main goal is to verify the ability to model HRF by showing that using neuronal data helps predict blood flow beyond the history of the latter. The next challenge is to scale the model in order to be able to model more vessels (without subsampling) and longer historical sequences (larger t v , t s ). With transformers being used for very long sequences, this is a limitation of our resources and not of our method."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Acknowledgements,". The authors thank David Kain for conducting the mouse surgery. This project has received funding from the ISRAEL SCIENCE FOUNDA-TION (grant No. 2923/20) within the Israel Precision Medicine Partnership program. It was also supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD). It was also supported by the European Research Council, grant No 639416, and the Israel Science Foundation, grant No 2342/21. The contribution of the first author is part of a PhD thesis research conducted at Tel Aviv University."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 35.
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,1.0,Introduction,"Accurately segmenting cardiac images is crucial for diagnosing and treating cardiovascular diseases. Recently, deep learning methods have greatly advanced cardiac image segmentation. However, most state-of-the-art segmentation models require a large scale of training samples with pixel-wise dense annotations, which are expensive and time-consuming to obtain. Thus, researchers are active in exploring other labour-efficient forms of annotations for effective training. For example, semi-supervised learning (SSL) [14,20,22,[26][27][28][29]31] is one such approach that attempts to propagate labels from the limited labeled data to the abundant unlabeled data, typically via pseudo-labeling. However, due to limited diversity in the restricted labeled set, accurately propagating labels is very challenging [15]. As another form, weakly supervised learning (WSL), i.e., our focused scenario, utilizes sparse labels such as scribbles, bounding boxes, and points for effective training, wherein scribbles have gained significant attention due to their ease of annotation and flexibility in labeling irregular objects. Yet, an intuitive challenge is that the incomplete shape of cardiac in scribble annotations inherently lacks sufficient structural information, as illustrated in Fig. 1, which easily leads to (i) poor local prediction (e.g., poor boundary prediction with high 95% Hausdorff Distance) because no structural priors are provided during training; (ii) poor generalizability due to less-compact class feature distributions learned from extremely sparse supervision. Effectively training a cardiac segmentation model using scribble annotations remains an open challenge. Related Work. A few efforts, not limited to medical images, have been made in scribble-supervised segmentation [4,5,9,11,13,15,19,32]. For example, Tang et al. [19] introduced a probabilistic graphical model, conditional random field (CRF), to regularize the spatial relationship between neighboring pixels in an image. Kim et al. [9] proposed another regularization loss based on level-set [17] to leverage the weak supervision. S2L [11] leverages label filtering to improve the pseudo labels generated by the scribble-trained model. USTM [13] adapts an uncertainty-aware mean-teacher [31] model in semi-supervised learning to leverage the unlabeled pixels. Zhang et al. [32] adapted a positive-unlabeled learning framework into this problem assisted by a global consistency term. Luo et al. [15] proposed a dual-decoder design where predictions from two slightly different decoders are randomly mixed to provide more reliable auxiliary pseudolabel supervision. Despite its effectiveness to some extent, the aforementioned two challenges still have not received adequate attention.In this paper, we propose SC-Net, a new scribble-supervised approach that combines Superpixel-guided scribble walking with Class-wise contrastive regularization. The basic framework is built upon the recent dual-decoder backbone design [15]. Besides the sparse supervision (using partial cross-entropy loss) from scribbles, predictions from two slightly different decoders are randomly mixed to provide auxiliary pseudo-label supervision. This design helps to prevent the model from memorizing its own predictions and falling into a trivial solution during optimization. Then, we tackle the aforementioned inherent challenges with two schemes. Firstly, we propose a specialized mechanism to guide the scribbles to walk towards unlabeled pixels based on superpixel connectivity and image content, in order to augment the structural priors into the labels themselves. As such, better local predictions are achieved. Secondly, we propose a class-wise contrastive regularization term that leverages prototype contrastive learning to disconnect the feature manifolds of different classes, which addresses the issue of less-compact class feature distributions due to sparse supervision. We evaluate our approach on the public cardiac dataset ACDC and show that it achieves promising results, especially better boundary predictions, compared to recent scribble-supervised and semi-supervised methods with similar labeling efforts."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.1,Preliminaries and Basic Framework,"In the scribble-supervised setting, the dataset includes images and their corresponding scribble annotations. We denote an image as X with the scribble annotation S = {(s r , y r )}, where s r is the pixel of scribble r, and y r ∈ {0, 1, ..., C -1} denotes the corresponding label with C possible classes at pixel s r . As shown in Fig. 1, our framework is built upon a one-encoder-dual-decoder design [15], where the encoder (θ enc ) is shared and two decoders are independent and slightly different. Here, we denote the decoder 1 as θ Dec1 and the auxiliary decoder 2 as θ Dec2 . Compared to θ Dec1 , θ Dec2 introduces the dropout layer (ratio = 0.5) before each convolutional block to impose perturbations. In this framework, the supervised signals consist of a scribble-supervised loss and a pseudo-supervised self-training loss. For the former one, we adopt the commonly used partial crossentropy loss for those scribble-containing pixels [11,19], formulated as:where p c 1(i) and p c 2(i) are the predicted probability of pixel i belonging to class c from the two decoders θ Dec1 and θ Dec2 , respectively. For the self-training loss, this dual-decoder framework randomly mix the predictions from the two different decoders to generate the ensemble pseudo label as: ŷML = argmax[α × p 1 + (1α) × p 2 , where α = random(0, 1). Such dynamically mixing scheme can increase the diversity of pseudo labels, which helps to prevent the model from memorizing its own single prediction and falling into a trivial solution during optimization [8]. As such, the self-training loss can be formulated as:(Despite its effectiveness, this framework still overlooks the aforementioned fundamental limitations of sparse scribble supervision: (i) although the mixed pseudo labels provide dense supervision, they still stems from the initial sparse guidance, making it difficult to provide accurate local structural information. Thus, we propose superpixel-guided scribble walking strategy (Sect. 2.2) to enrich structural priors for the initial supervision itself. (ii) Extremely sparse supervision inevitably leads to less-compact class feature distributions, resulting in poor generalizability to unseen test data. Thus, we further propose class-wise contrastive regularization (Sect. 2.3) to enhance the compactness of class embeddings."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.2,Superpixel-Guided Scribble Walking,"In order to enhance the structural information in our initial supervision, we utilize the superpixel of the image as a guide for propagating scribble annotations to unlabeled pixels, considering that it effectively groups pixels with similar characteristics within the uniform regions of an image and helps capture the class boundaries [30]. Specifically, we employ the simple linear iterative clustering (SLIC) algorithm [1] to generate the superpixels. The algorithm works by first dividing the image into a grid of equally-sized squares, then selecting a number of seed points within each square based on the desired number K of superpixels.Next, it iteratively assigns each pixel to the nearest seed point based on its color similarity and spatial proximity (distance). This process is repeated until the clustering converges or reaches a predefined number of iterations. Finally, the algorithm updates the location of the seed points to the centroid of the corresponding superpixel, and repeats until convergence. As such, the image is coarsely segmented into K clusters. To balance accuracy and computational efficiency, the number of iterations is empirically set to 10. K is set to 150. An example of superpixel is depicted in Fig. 1. Then, guided by the obtained superpixel, the scribbles walk towards unlabeled pixels with the following mechanisms: (i) if the superpixel cluster overlaps with a scribble s r , the label y r of s r walks towards to the pixels contained in this cluster; (ii) yet, if the superpixel cluster does not overlap any scribble or overlaps more than one scribble, the pixels within this cluster are not assigned any labels. As such, we denote the set of the superpixel-guided expanded label as {(x sp , ŷsp )}, where x sp represents the pixel with the corresponding label ŷsp ∈ {0, 1, ..., C -1}. An expansion example can be also found in Fig. 1. Although we use strict walking constraints to expand the labels, superpixels are primarily based on color similarity and spatial proximity to seed points. However, magnetic resonance imaging has less color information compared to natural images, and different organs often share similar intensity, leading to some inevitable label noises. Therefore, to alleviate the negative impact of the label noises, we adopt the noise-robust Dice loss [24] to supervise the models, formulated as:where N is the number of label-containing pixels. ŷsp is converted to one-hot representation. p 1(i) and p 2(i) are the predicted probabilities of pixel i from θ Dec1 and θ Dec2 , respectively. Following [24], = 10 -5 and γ = 1.5. Note that when γ = 2, this loss will degrade into the typical Dice loss."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.3,Class-Wise Contrastive Regularization,"When using extremely sparse supervision, it is difficult for the model to learn compact class feature distributions, leading to poor generalizability. To address this, we propose a class-wise contrastive regularization term that leverages prototype contrastive learning to disconnect the feature manifolds of different classes, as illustrated in Fig. 1. Specifically, using the additional non-linear projection head, we derive two sets of projected features, namely F 1 and F 2 , from decoder 1 and decoder 2, respectively. Then, we filter the projected features by comparing their respective categories with that of the mixed pseudo label ŷML and the current predictions from the two decoders. Only features that have matching categories are retained and denoted as Ḟ c 1 and Ḟ c 2 , where superscript c indicates that such feature vectors correspond to class c. Then, we use C attention modules [2] to obtain ranking scores to sort the retained features and then the top-k features are selected as the class prototypes, where the class-c prototypes are denoted as Z c 1 = {z c 1 } and Z c 2 = {z c 2 }. Note that we extract feature prototypes in an online fashion instead of retaining cross-epoch memories as in [2], since the latter can be computationally inefficient and memory-intensive. Then, we extract the features of each category f c 1 ∈ F 1 and f c 2 ∈ F 2 using the current predictions and encourage their proximity to the corresponding prototypes z c 1 and z c 2 . We adopt the cosine similarity to measure the proximity between the class features and the class prototypes. Taking decoder 1 as example, we define its class-wise contrastive regularization loss L Dec1 CR as:where w ij is obtained by normalizing the learnable attention weights (detailed in [2]). N z c 1 or N f c 1 is the number of prototypes or projected features of c-th class, respectively. Similarly, we obtain such regularization loss for decoder 2, denoted as L Dec2 CR . As such, the overall class-wise contrastive regularization loss is formulated as:Overall, the final loss of our SC-Net is summarized as:where λ MLS , λ sN R and λ CR are the trade-off weights. λ MLS is set to 0.5, following [15]. λ sN R is set to 0.005. λ CR is scheduled with an iteration-dependent ramp-up function [10] with the maximal value of 0.01 suggested by [25]."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,3.0,Experiments and Results,"Dataset. We evaluate our method on the public ACDC dataset [3], which consists of 200 short-axis cine-MRI scans from 100 patients. Each patient has two annotated scans from end-diastolic (ED) and end-systolic (ES) phases, where each scan has three structure labels, including right ventricle (RV), myocardium (Myo) and left ventericle (LV). The scribbles used in this work are manually annotated by Valvano et al. [21]. Considering the large thickness in this dataset, we perform 2D segmentation rather than 3D segmentation, following [15,21].Implementation and Evaluation Metrics. The framework is implemented with PyTorch using an NVIDIA RTX 3090 GPU. We adopt UNet [18] as the backbone with extension to dual-branch design [15]. All the 2D slices are normalized to [0, 1] and resized to 256×256 pixels. Data augmentations, including random rotation, flipping and noise injection, are applied. The SGD optimizer is utilized with the momentum of 0.9 and weight decay is 10 -4 , the poly learning rate strategy is employed [16]. We train the segmentation model for 60,000 iterations in total with a batch size of 12. During inference, the encoder in combination with the primary decoder (Dec 1) is utilized to segment each scan slice-by-slice and stack the resulting 2D slice predictions into a 3D volume. We adopt the commonly used Dice Coefficient (DSC) and 95% Hausdorff Distance (95HD) as the evaluation metrics. Five-fold cross-validation is employed. The code will be available at https://github.com/Lemonzhoumeng/SC-Net.Comparison Study. We compare our proposed SC-Net with recent state-ofthe-art alternative methods for annotation-efficient learning. Table 1 presents the quantitative results of different methods. All methods are implemented with the same backbone to ensure fairness. According to [15,21], the cost of scribble annotation for the entire ACDC training set is similar to that of dense pixel-level annotation for 10% of the training samples. Thus, we use 10% of the training samples (8 patients) as labeled data and the remaining 90% as unlabeled data to perform semi-supervised learning (SSL). Here, we compare popular semisupervised approaches, including AdvEnt [23], DAN [34], MT [20] and UAMT [31], as well as the supervised-only (Sup) baseline (using 10% densely labeled data only). As observed, SC-Net achieves significantly better performance than  the competing SSL methods, showing that when the annotation budget is similar, using scribble annotations can lead to better outcomes than pixel-wise annotations. Furthermore, we compare SC-Net with weakly-supervised learning (WSL) approaches on scribble annotated data, including pCE only [12] (lower bound), RW [6] (using random walker to produce additional label), USTM [13] (uncertainty-aware self-ensembling and transformation-consistent model), S2L [11] (Scribble2Label), MLoss [9] (Mumford-shah loss), EM [7] (entropy minimization) and DBMS [15] (dual-branch mixed supervision). Besides, the upper bound, i.e., supervised training with full dense annotations, is also presented. It can be observed that SC-Net achieves more promising results compared to existing methods. In comparison to DBMS, SC-Net exhibits a slight improvement in DSC, but a significant decrease in the 95HD metric (p<0.05). Furthermore, our method achieves slightly lower performance in DSC compared to the upper bound, but even slightly better results in 95HD. This indicates that our approach effectively addresses the inherent limitations of sparse supervision. Figure 2 presents exemplar qualitative results of our SC-Net and other methods. It can be seen that the prediction of our SC-Net fit more accurately with the ground truth, especially in local details. Thanks to the more compact feature distributions, our method reduces false-positive predictions, as indicated in the red box.Ablation Study. We perform an ablation study to investigate the effects of the two key components of our SC-Net. The results are also shown in Table 1.We found that the two components need to work together. When we remove L sN R , the performance degrades to some extent. This may be because it is difficult for the model to generate high-quality local pseudo-labels with only sparse supervision provided by scribbles, and class-wise contrastive regularization relies heavily on pseudo labels to separate class features. When we remove L CR , the performance also drops slightly. This is mainly because the generated superpixels inevitably contain errors, which can misguide the scribble walking. Yet, using L CR can regularize the feature distribution between classes, reducing the impact of these errors. Meanwhile, the structure prior strengthened by superpixel guidance helps to provide higher-quality local pseudo labels to assist class-wise contrastive regularization. The two components complement each other, resulting in the best performance of our complete SC-Net.Sensitivity Analysis. The superpixel-guided scribble walking plays an important role in our SC-Net. Thus, we conduct further assessments on the sensitivity of λ sN R , which is used to weight L sN R , and the cluster number K used for superpixel generation. The results obtained by five-fold cross validation are presented in Fig. 3. As observed, increasing λ sN R from 0.001 to 0.005 leads to better results in terms of both metrics. When λ sN R is set to 0.01, the result exhibits only a slight decrease compared to 0.005 (0.872 vs. 0.867 in term of DSC). These observations show that our method is not so sensitive to λ sN R within the empirical range. In practice, the optimal value of K depends on the characteristics of the input image, such as object complexity and texture. We find that our method is also not highly sensitive to K, but the optimal results are achieved when K = 150 for the cardiac MR images in our study."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,4.0,Conclusion,"In this work, we proposed the SC-Net towards effective weakly supervised medical image segmentation using scribble annotations. By combining superpixelguided scribble walking with class-wise contrastive regularization, our approach alleviates two inherent challenges caused by sparse supervision, i.e., the lack of structural priors during training and less-compact class feature distributions.Comprehensive experiments on the public cardiac dataset ACDC demonstrated the superior performance of our method compared to recent scribble-supervised and semi-supervised methods with similar labeling efforts."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,1.0,Introduction,"Humans inherently learn in an incremental manner, acquiring new concepts over time without forgetting previous ones. In contrast, deep learning models suffer from catastrophic forgetting [10], where learning from new data can override previously acquired knowledge. In this context, the class-incremental continual learning problem was formalized by Rebuffi et al. [23], where new classes are observed in different stages, restricting the model from accessing previous data.The medical domain faces a similar problem: the ability to dynamically extend a model to new classes is critical for multiple organ and tumor segmentation, wherein the key obstacle lies in mitigating 'forgetting.' A typical strategy involves retaining some previous data. For instance, Liu et al. [13] introduced a memory module to store the prototypical representation of different organ categories. However, such methods, reliant on an account of data and annotations, may face practical constraints as privacy regulations could make accessing prior data and annotations difficult [9]. An alternative strategy is to use pseudo labels generated by previously trained models on new data. Ozdemir et al. [18,19] extended the distillation loss to medical image segmentation. A concurrent study of ours [7] mainly focused on architectural extension, addressing the forgetting problem by freezing the encoder and decoder and adding additional decoders when learning new classes. While these strategies have been alleviating the forgetting problem, they led to tremendous memory costs for model parameters.Therefore, we identify two main open questions that must be addressed when designing a multi-organ and tumor segmentation framework. Q1: Can we relieve the forgetting problem without needing previous data and annotations? Q2: Can we design a new model architecture that allows us to share more parameters among different continual learning steps?To tackle the above questions, in this paper, we propose a novel continual multi-organ and tumor segmentation method that overcomes the forgetting problem with little memory and computation overhead. First, inspired by knowledge distillation methods in continual learning [11,14,15,17], we propose to generate soft pseudo annotations for the old classes on newly-arrived data. This enables us to recall old knowledge without saving the old data. We observe that with this simple strategy, we are able to maintain a reasonable performance for the old classes. Second, we propose image-aware segmentation heads for each class on top of the shared encoder and decoder. These heads allow the use of a single backbone and easy extension to new classes while bringing little computational cost. Inspired by Liu et al. [12], we adopt the text embedding generated by Contrastive Language-Image Pre-training (CLIP) [22]. CLIP is a large-scale image-text co-training model that is able to encode high-level visual semantics into text embeddings. This information will be an advantage for training new classes with the class names known in advance.We focus on organ/tumor segmentation because it is one of the most critical tasks in medical imaging [6,21,27,28], and continual learning in semantic segmentation is under-explored in the medical domain. We evaluate our continual learning method using three datasets: BTCV [8], LiTS [1] and JHH [25] (a private dataset at Johns Hopkins Hospital) 1 . On the public datasets, the learning trajectory is to first segment 13 organs in the BTCV dataset, then learn to Fig. 1. An overview of the proposed method. An encoder (Enc) processes the input image to extract its features, which are then reduced to a feature vector (fimage) by a global average pooling layer. This feature vector is subsequently concatenated with a CLIP embedding (ω class ), calculated using the pre-trained CLIP model. Through a series of Multi-Layer Perceptron (MLP) layers, we derive class-specific parameters of convolution kernels (θ class ). These kernels, when applied to the decoder (Dec) feature, yield the mask for the respective class.segment liver tumors in the LiTS dataset. On the private dataset, the learning trajectory is to first segment 13 organs, followed by continual segmentation of three gastrointestinal tracts and four cardiovascular system structures. In our study, we review and compare three popular continual learning baselines that apply knowledge distillation to predictions [11], features [17], and multi-scale pooled features [3], respectively. The extensive results demonstrate that the proposed method outperforms existing methods, achieving superior performance in both keeping the knowledge of old classes and learning the new ones while maintaining high memory efficiency."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.0,Methodology,"We formulate the continual organ segmentation as follows: given a sequence of partially annotated datasets {D 1 , D 2 , . . . , D n } each with organ classes {C 1 , C 2 , . . . , C n }, we learn a single multi-organ segmentation model sequentially using one dataset at a time. When training on the i-th dataset D t , the previous datasets {D 1 , . . . , D t-1 } are not available. The model is required to predict the accumulated organ labels for all seen datasets {D 1 , . . . , D t }:(1)where j is a voxel index, X is an image from D t , P is the probability function that the model learns and Ŷ is the output segmentation mask."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.1,Pseudo Labels for Multi-organ Segmentation,"In the context of continual organ segmentation, the model's inability to access the previous dataset presents a challenge as it often results in the model forgetting the previously learned classes. In a preliminary experiment, we observed that a segmentation model pre-trained on some organ classes will totally forget the old classes when fine-tuned on new ones. We found the use of pseudo-labeling can largely mitigate this issue and preserve the existing knowledge. Specifically, we leverage the output prediction from the previous learning step t -1, denoted as Ŷt-1 , which includes the old classes C t-1 , as the pseudo label for the current step's old classes. For new classes, we still use the ground truth label. Formally, the label Lc t for class c in current learning step t can be expressed as:where L c t represents the ground truth label for class c in step t obtained from dataset D t . By utilizing this approach, we aim to maintain the original knowledge and prevent the model from forgetting the previously learned information while learning the new classes. The following proposed model is trained only with pseudo labeling of old classes without any other distillation or regularization."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.2,The Proposed Multi-organ Segmentation Model,"In the following, we introduce the proposed multi-organ segmentation model for continual learning. Figure 1 illustrates the overall framework of the proposed model architecture. It has an encoder-decoder backbone, a set of image-aware organ-specific output heads, and text-driven head parameter generation.Backbone Model: For continual learning, ideally, the model should be able to learn a sufficiently general representation that would easily adapt to new classes. We use Swin UNETR [4] as our backbone since it exhibits strong performance in self-supervised pre-training and the ability to transfer to various medical image segmentation tasks. Swin UNETR has Swin Transformer [16] as the encoder and several deconvolution layers as the decoder."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Image-Aware Organ-Specific Heads:,"The vanilla Swin UNETR has a Softmax layer as the output layer that predicts the probabilities of each class. We propose to replace the output layer with multiple image-aware organ-specific heads. We first use a global average pooling (GAP) layer on the last encoder features to obtain a global feature f of the current image X. Then for each organ class k, a multilayer perceptron (MLP) module is learned to map the global image feature to a set of parameters θ k :where E(X) denotes the encoder feature of image X. An output head for organ class k is a sequence of convolution layers that use parameters θ k as convolution kernel parameters. These convolution layers are applied to the decoder features, which output the segmentation prediction for organ class k:where E is the encoder, D is the decoder, σ is the Sigmoid non-linear layer and P (Y k j = 1) denotes the predicted probability that pixel j belongs to the organ class k. The predictions for each class are optimized by Binary Cross Entropy loss. The separate heads allow independent probability prediction for newly introduced and previously learned classes, therefore minimizing the impact of new classes on old ones during continual learning. Moreover, this design allows multi-label prediction for cases where a pixel belongs to more than one class (e.g., a tumor on an organ)."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Text Driven Head Parameter Generation:,"We further equip the segmentation heads with semantic information about each organ class. With the widespread success of large-scale vision-language models, there have been many efforts that apply these models to the medical domain [2,5,26]. It is suggested that vision-language models could be used for zero-shot learning in the medical domain and recognize novel classes with well-designed prompts [20]. We propose to use CLIP [22] to generate text embeddings for the target organ names. Specifically, we produce the organ name embedding by the pre-trained CLIP text encoder and a medical prompt (e.g., ""a computerized tomography of a [CLS]"", where [CLS] is an organ class name). Then we use the text embeddings ω together with the global image feature f to generate parameters for the organ segmentation heads:where ω k is the text embedding for organ class k. CLIP embeddings carry highlevel semantic meanings and have the ability to connect correlated concepts. Therefore, it guides the MLP module to generate better convolution parameters for each organ class. More importantly, the fixed-length CLIP embedding allows us to adapt the pre-trained model to open-vocabulary segmentation and extend to novel classes. [12]: For the purpose of continual learning, we improve the original design of Universal Model in the MLP module."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Difference from Universal Model,"Unlike Liu et al. [12], who utilized a single MLP to manage multiple classes, we allocate an individual and independent MLP to each class. This design significantly mitigates interference among different classes."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.3,Computational Complexity Analysis,"Another key contribution of our work is the reduction of computational complexity in continual segmentation. We compare our proposed model's FLOPs (floating-point operations per second) with the baseline model, Swin UNETR [4]. Our model's FLOPs are just slightly higher than Swin UNETR's, with 661.6 GFLOPs and 659.4 GFLOPs, respectively. This is because we used lightweight output convolution heads with a small number of channels. Ji et al. [7] proposed a state-of-the-art architecture for medical continual semantic segmentation, which uses a pre-trained and then frozen encoder coupled with incrementally added decoders in each learning step. However, subsequent continual learning steps using this architecture introduce massive computational complexity. For example, Swin UNETR's decoder alone has 466.08 GFLOPs, meaning that every new learning step adds an additional 466.08 GFLOPs. In contrast, our model only needs to add a few image-aware organ-specific heads for new classes of the new task, with each head consuming only 0.12 GFLOPs. As a result, the computational complexity of our model nearly remains constant in continual learning for segmentation, while that of the architecture of Ji et al. [7] increases linearly to the number of steps."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,3.0,Experiment and Result,"Datasets: We empirically evaluate the proposed model under two data settings: in one setting, both training and continual learning are conducted on the inhouse JHH dataset. It has multiple classes annotated, which can be categorized into three groups: the abdominal organs (in which seven classes are learned in step 1: spleen, right kidney, left kidney, gall bladder, liver, postcava, pancreas), the gastrointestinal tract (in which three classes are learned in step 2: stomach, colon, intestine), and other organs (in which four classes are learned in step 3: aorta, portal vein and splenic vein, celiac truck, superior mesenteric artery). The categorization is in accordance with TotalSegmentator [24]. In the other setting, we first train on the BTCV dataset and then do continual learning on the LiTS dataset. The BTCV dataset contains 47 abdominal CT images delineating 13 organs. The LiTS dataset contains 130 contrast-enhanced abdominal CT scans for liver and liver tumor segmentation. We use 13 classes (spleen, right kidney, left kidney, gall bladder, esophagus, liver, stomach, aorta, inferior vena cava, portal vein and splenic vein, pancreas, right adrenal gland, left adrenal gland) from BTCV in step 1 learning and the live tumor from LiTS in step 2 learning."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Baselines and Metrics:,"For a fair comparison, all the compared methods use the same Swin UNETR [4] as the backbone, which is the state-of-the-art model in a bunch of medical image segmentation tasks. We compare with three popular continual learning baseline methods that apply knowledge distillation, including LwF [11], ILT [17] and PLOP [3]. We compare the proposed method with different baseline models using the commonly used Dice score (DSC) metric (the Sørensen-Dice coefficient). In each learning step, we report the average DSC for the classes that are used at the current step as well as the previous steps (e.g., in step 2 of the JHH dataset, we report the average dice of the gastrointestinal tracts and the abdominal organs). The dice score at old classes reveals a model's ability to retain its previous knowledge, and the score for the current step classes indicates the model's ability to acquire new knowledge under the regularization of old ones."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Implementation Details:,"The proposed model architecture is trained on new classes with pseudo labeling of old classes. No other distillation techniques are used. We use a lightweight design for the image-aware organ-specific heads. Each head consists of three convolution layers. The number of kernels in the first two layers is 8, and in the last layer is 1. All the compared models are trained using the AdamW optimizer for 100 epochs with a cosine learning rate scheduler. We use a batch size of 2 and a patch size of 96 × 96 × 96 for the training. The initial "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Results:,"The continual segmentation results using the JHH dataset and public datasets are shown in Tables 1 and2, respectively. Notably, by simply using the pseudo labeling technique (LwF), we are able to achieve reasonably good performance in remembering the old classes (Dice of 0.777 in step 2 and 0.767 in step 3 for abdominal organs in the JHH dataset; Dice of 0.770 in step 2 for BTCV organs). Class-wise DSC scores are in Appendix Tables 4567. All the compared methods use prediction-level or feature-level distillation as regularization. Among them, the proposed method achieves the highest performance in most learning steps. Specifically, the proposed method exhibits the least forgetting in old classes and a far better ability to adapt to new data and new classes.To evaluate the proposed model designs, we also conduct the ablation study on the JHH dataset, shown in Table 3. Specifically, we ablate the performance improvement introduced by the organ-specific segmentation heads as well as the CLIP text embeddings. The first line in Table 3 shows the performance of the baseline Swin UNETR model learned with pseudo labeling (LwF). The second row introduces the organ-specific segmentation heads, but uses one-hot embeddings rather than the CLIP text embeddings for each organ. The third row gives the performance of the full method. The results show that by adapting the model to use organ-specific heads as segmentation outputs, we are able to achieve improvement of a large margin (e.g., 0.144 in step 2 and 0.179 in step 3 for gastrointestinal tracts). With the application of CLIP text embeddings, we are able to further improves the performance (e.g., by a margin of 0.019 in step 2 and 0.027 in step 3 for gastrointestinal tracts). This study validates the effectiveness of the proposed organ-specific segmentation heads and the CLIP text embeddings in the continual organ segmentation task. Finally, we show the qualitative segmentation results of the proposed method together with the best baseline method ILT on the JHH dataset. We show the results of learning steps 2 and 3 in Fig. 2, one case per column and two cases for each step. The visualization demonstrates that the proposed method successfully segments the correct organs while the best baseline method fails throughout the continual learning process."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,4.0,Conclusion,"In this paper, we propose a method for continual multiple organ and tumor segmentation in 3D abdominal CT images. We first empirically verified the effectiveness of high-quality pseudo labels in retaining previous knowledge. Then, we propose a new model design that uses organ-specific heads for segmentation, which allows easy extension to new classes and brings little computational cost in the meantime. The segmentation heads are further strengthened by utilizing the CLIP text embeddings that encode the semantics of organ or tumor classes. Numerical results on an in-house dataset and two public datasets demonstrate that the proposed method outperforms the continual learning baseline methods in the challenging multiple organ and tumor segmentation tasks."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 4.
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,1.0,Introduction,"Black-box neural network classifiers offer enormous potential for computer-aided diagnosis and prediction in medical imaging applications but, unfortunately, they also have a strong tendency to learn spurious correlations in the data [11]. For the development and safe deployment of machine learning (ML) systems it is essential to understand what information the classifiers are basing their decisions on, such that reliance on spurious correlations may be identified.Spurious correlations arise when the training data are confounded by additional variables that are unrelated to the diagnostic information we want to predict. For instance, older patients in our training data may be more likely to present with a disease than younger patients. A classifier trained on this data may inadvertently learn to base its decision on image features related to age rather than pathology. Crucially, such faulty behavior cannot be identified using classification performance metrics such as area under the ROC curve (AUC) if the testing data contains the same confounding information as the training data, since the classifier predicts the right thing, but for the wrong reason. If undetected, however, such spurious correlations may lead to serious safety implications after deployment. Fig. 1. Overview. We train classifiers on datasets with three types of artificially added confounders highlighted by arrows. We then evaluate the ability of explanation techniques to correctly identify reliance on these confounders (shown Attri-Net [24]).Interpretable ML approaches may be used as a powerful tool to detect spurious correlations during development or after deployment of an ML system. Currently, the most widely used explanation modality are visual explanations, which highlight the pixels in the input image that are responsible for a particular decision. Common strategies include methods which leverage the gradient of the prediction with respect to the input image [7,19,22,23,25], explain the predictions by counterfactually generating an image of the opposite class [9,18,20,24], interpret the feature map of the last layer before the classification [8,10,27], or methods that build a local approximation of the decision function such as LIME [16], or SHAP [15].The majority of visual explanation methods are post-hoc techniques, meaning a heuristic is applied to any trained model (e.g. a ResNet [13]) to approximately understand the decision mechanism for a given data point. However, post-hoc techniques are by definition only approximations and many techniques have been found to suffer from serious limitations [4,12,26]. Inherently interpretable techniques on the other hand build custom architectures that are designed to directly reveal the reasoning of the classifier to the user without the need for approximations. This class of methods does not suffer from the same limitations as post-hoc methods, and it has been argued that inherently interpretable approaches should be preferred in high-stakes applications such as medical image analysis [17]. For instance, if a classifier bases its decision on a spurious signal, an inherently interpretable classifier should by definition reveal this relationship.Inherently interpretable visual explanation approaches are much less widely explored than post-hoc techniques, but there has recently been an increased interest in the topic. Two recently proposed methods in this category are the attribution network (Attri-Net) [24], and convolutional dynamic alignment networks (CoDA-Nets) [6]. Attri-Net first produces human-interpretable feature attribution maps for each disease category using a GAN-based counterfactual generator [24]. Then makes the final prediction with simple logistic regression classifiers based on those feature attribution maps. CoDA-Nets express neural networks as input dependent linear transformation [6]. Both approaches produce explanations on the pixel level of the input images."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Related Work on Comparing Explanation Techniques.,"A number of works have studied the quality of post-hoc explanation techniques. The vast majority of work focuses exclusively on gradient-based approaches (e.g. [5,21]).In their landmark study, Adebayo et al. [1] find that commonly used gradientbased explanation techniques do not pass some basic sanity checks. Arun et al. [5] extends this work to weakly supervised localisation in one of the few papers in this domain focusing on medical data. Both papers, however, do not consider other types of commonly used approaches such as counterfactual methods, or local function approximations such as LIME or SHAP.A small number of works specifically investigate explanations' sensitivity to spurious correlations. In closely related work to ours, Adebayo et al. [3] explore a large library of post-hoc explanation techniques including LIME and SHAP, for detecting spurious image backgrounds in a bird versus dog classification task and find that many techniques are in fact able to detect the spurious background. In subsequent work, the same authors explore the usefulness of four post-hoc gradient-based explanation methods for identifying spurious correlations in hand and knee radiographs [2] and come to the conclusion that the examined methods are ineffective at identifying spurious correlations. We note that prior work is inconclusive on the usefulness of explanation techniques for identifying spurious correlations. In particular, in the medical context it is still unclear if commonly used explanation techniques are suitable for the detection of spurious correlations. Moreover, there is, to our knowledge, no evidence for the supposition that inherently interpretable techniques are better suited for this task.Contributions. We present a rigorous evaluation of post-hoc explanations and inherently interpretable techniques for the identification of spurious correlations in a medical imaging task. Specifically, we focus on the task of diagnosing cardiomegaly from chest x-ray data with three types of synthetically generated spurious correlations (see Fig. 1). To identify whether an explanation correctly identifies a model's reliance on spurious correlations, we propose two quantitative metrics which are highly reflective of our qualitative findings. In contrast to the majority of prior work we focus on a wide range of different explanation approaches including counterfactual techniques and local function approximations, as well as post-hoc techniques and an inherently interpretable approach. Our analysis yields actionable insights which will be useful for a wide audience of ML practitioners."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.0,A Framework for Evaluating Explanation Techniques,"In the following, we introduce our evaluation strategy and proposed evaluation metrics, the studied confounders, as well as the evaluated explanation techniques. The strategy and evaluation metrics are generic and can also be applied to different problems. The confounders are engineered to correspond to realistic image artifacts that can appear in chest x-ray imaging1 ."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.1,Evaluation Strategy,"We assume a setting in which the development data for a binary neural network based classifier contains an unknown spurious correlation with the target label. To quantitatively study this setting, we create training data with artificial spurious correlations by adding a confounding effect (e.g. a hospital tag) in a percentage of the cases with a positive label, where we vary the percentage p ∈ {0, 20, 50, 80, 100}. E.g., for p = 100% all of the positive images in the training set will have an artificial confounder, and for p = 0% there is no spurious signal. With increasing p the reliance on a spurious signal becomes more likely. The images with a negative label remain untouched.In the evaluation, we consider a scenario in which the test data contain the same confounder type with the same proportion p used in the respective trainings. In this case, we can not tell if a classifier relies on the confounded features from classification performance. Our aim, therefore, is to investigate whether explanation techniques can identify that the classifier predicts the right thing for the wrong reason.We perform all experiments on chest x-ray images from the widely used CheXpert dataset [14], where we focus on the binary classification task on disease cardiomegaly. We divided our dataset into a training (80%), validation (10%) test (10%) set."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.2,Studied Confounders,"We study three types of confounders inspired by real-world artefacts. Firstly, we investigate a hospital tag placed in the lower left corner of the image (see Fig. 1a). Secondly, we add vertical lines of hyperintense signal that can be caused by foreign materials on the light path assembly (see Fig. 1b). Lastly, we consider an oblique occlusion of the image in the lower part of the image, which is an artefact that we observed for many images in the CheXpert dataset (see Fig. 1c)."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.3,Evaluation Metrics for Measuring Confounder Detection,We propose two novel metrics which reflect an explanation's ability to correctly identify spurious correlations.
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Confounder Sensitivity (CS).,"Firstly, the explanations should be able to correctly attribute the confounder if classifier bases its decision on it. We assess this property by summing the number of true positive attributions divided by the total number of confounded pixels for each test image. We consider a pixel a true positive if it is part of the pixels affected by the confounder and in the top 10% attributed pixels according to a visual explanation. Thus the maximum sensitivity of 1 is obtained if all confounded pixels are in the top 10% of the attributions. Note that we do not penalise attributions outside of the confounding label as those can still also be correct. To guarantee that we only evaluate on samples for which the prediction is actually influenced by the confounder, we only include images for which the prediction with and without the confounding label is of the opposite class. To reduce computation times we use a maximum of 100 samples for each evaluation. An optimal explanation methods should obtain a CS score of 0 if the data contains p = 0% confounded data points, since in that case the spurious signal should not be attributed. For increasing p the confounder sensitivity should increase, i.e. the explanation should reflect the classifiers increasing reliance on the confounder."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Sensitivity to Prediction Changes via Explanation NCC.,"Secondly, the explanations should not be invariant to changes in classifier prediction. That is, if the classifier's prediction for a specific image changes when adding or removing a confounder, then the explanations should also be different. We measure this property using the average normalised cross correlation (NCC) between explanations of test images when confounders were either present or absent.Again, we only evaluate on images for which the prediction changes when adding the confounder as in these cases, we know the classifier is relying on confounders, and we evaluate a maximum of 100 samples. An optimal explanation method should obtain a high NCC score if the training data contains p = 0% confounded data points, since in that case the explanation with and without the confounder should be similar. For increasing p the NCC score should decrease to reflect the classifiers increasing reliance on the confounder."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.4,Evaluated Explanation Methods,"We evaluated five post-hoc techniques with representative examples from the approaches mentioned in the introduction: Guided Backpropgation [23] and Grad-CAM [19] (gradient-based), Gifsplanation (counterfactual), and LIME [16] and SHAP partition explainer [15] (local linear approximations). All post-hoc techniques were applied to a standard black-box ResNet50 model. We furthermore investigated the interpretable visual explanation method Attri-Net [24]. We used the default parameters for all methods. We found CoDA-Nets [6] required lengthy hyperparameter tuning for each type of experiment, and decided to exclude it in this paper."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,3.0,Results,"We first established the classifiers' performance in the presence of confounders, then compared all techniques in their ability to identify such confounders.Classification Performance. Both investigated classifiers, the ResNet50 and the inherently interpretable Attri-Net, performed similarly in terms of classification AUC (first row of Fig. 2). For all three confounders, classification AUC consistently increased with increasing contamination p of the training dataset. This indicated that the classifiers increasingly relied on the spurious signal. For p = 100% contamination, where the confounder was present on all positive training examples, both classifiers reached almost a perfect classification AUC of 1.Explanations. We analysed the explanations' ability to identify confounders by reporting confounder sensitivity (CS, middle row in Fig. 2) and explanation NCC (bottom row in Fig. 2). Out of the investigated methods Attri-Net and SHAP were closest to the ideal behaviour of high confounder sensitivity and low explanation NCC for p > 0%. We found that SHAP performed extremely well in detecting tag confounders, but struggled with hyperintensities confounders. This can be explained by the fact that the tag confounder is relatively small and thus is more likely to be completely covered by the superpixels in SHAP. Overall, the inherently interpretable Attri-Net technique achieved the best balance. In agreement with related literature we found that gradient-based explanation methods performed poorly. In particular, Guided Backpropagation displayed similar CSscores no matter if the classifier relies on a spurious signal (p > 0%) or not (p = 0%). Note that some results for p = 100% were missing because no data points fulfilled the criterion of the prediction being flipped with and without the confounders.Figures 3, 4 & 5 contain examples explanations for the hyperintensity, tag, and edge confounder, respectively. Our qualitative analysis of the results confirms the quantitative findings, with SHAP and Attri-Net providing the most intuitive explanations. In particular, in the challenging hyperintensities scenario (see Fig. 3) AttriNet was the only method able to highlight the confounders in a human-interpretable fashion. We note that in all examples when a confounder was present, SHAP tended to highlight only the confounder, while Attri-Net also highlighted features related to Cardiomegaly. This may reflect the different decision mechanisms of the ResNet50 and the Attri-Net. The explanation techniques' ability to identify confounders in terms of confounder sensitivity (middle row) and explanation NCC (bottom row, lower is better)."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,4.0,Discussion,"In this paper, we proposed an evaluation strategy to assess the ability of visual explanations to correctly identify a classifier's reliance on a spurious signal. We specifically focused on the scenario where the classifier is predicting the right thing, but for the wrong reason, which is highly significant for the safe development of ML-basd diagnosis and prediction systems. Using this strategy, we assessed the performance of five post-hoc explanation techniques and one inherently interpretable technique with three realistic confounding signals. We found that the inherently interpretable Attri-Net technique, as well as the post-hoc SHAP technique performed the best, with Attri-Net yielding the most balanced performance. Both techniques are suitable for finding false reliance on a spurious signals. We also observed that the variation in the explanations' sparsity makes them perform differently in detecting spurious signals of different sizes and   shapes. In agreement with prior work, we found that gradient based techniques performed less robustly in our experiments.From our experiments we draw two main conclusions. Firstly, practitioners looking to check for spurious correlations in a trained black-box model such as a ResNet should give preference to SHAP which provided the best performance out of the post-hoc techniques in our experiments. Secondly, an inherently interpretable technique, namely Attri-Net, performed the best in our experiments providing evidence to the supposition by Rudin et al. [17] that inherently interpretable techniques may provide a fruitful avenue for future work.A major limitation of our study is the limited number of techniques we examined. Thus a primary focus of future work will be to scale our experiments to a wider range of techniques. Future work will also focus on human-in-the-loop experiments, as we believe, this will be the ultimate assessment of the usefulness of different explanation techniques."
Efficient Subclass Segmentation in Medical Images,1.0,Introduction,"In recent years, the use of deep learning for automatic medical image segmentation has led to many successful results based on large amounts of annotated training data. However, the trend towards segmenting medical images into finergrained classes (denoted as subclasses) using deep neural networks has resulted in an increased demand for finely annotated training data [4,11,21]. This process requires a higher level of domain expertise, making it both time-consuming and demanding. As annotating coarse-grained (denoted as superclasses) classes is generally easier than subclasses, one way to reduce the annotation cost is to collect a large number of superclasses annotations and then labeling only a small number of samples in subclasses. Moreover, in some cases, a dataset may have already been annotated with superclass labels, but the research focus has shifted towards finer-grained categories [9,24]. In such cases, re-annotating an entire dataset may not be as cost-effective as annotating only a small amount of data with subclass labels.Here, the primary challenge is to effectively leverage superclass annotations to facilitate the learning of fine-grained subclasses. To solve this problem, several works have proposed approaches for recognizing new subclasses with limited subclass annotations while utilizing the abundant superclass annotations in classification tasks [6,8,18,25]. In general, they assume the subclasses are not known during the training stage and typically involve pre-training a base model on superclasses to automatically group samples of the same superclass into several clusters while adapting them to finer subclasses during test time.However, to the best of our knowledge, there has been no work specifically exploring learning subclasses with limited subclass and full superclass annotations in semantic segmentation task. Previous label-efficient learning methods, such as semi-supervised learning [7,17,26], few-shot learning [10,15,19] and weakly supervised learning [13,27], focus on either utilize unlabeled data or enhance the model's generalization ability or use weaker annotations for training. However, they do not take into account the existence of superclasses annotations, making them less competitive in our setting.In this study, we focus on the problem of efficient subclass segmentation in medical images, whose goal is to segment subclasses under the supervision of limited subclass and sufficient superclass annotations. Unlike previous works such as [6,8,18,25], we assume that the target subclasses and their corresponding limited annotations are available during the training process, which is more in line with practical medical scenarios.Our main approach is to utilize the hierarchical structure of categories to design network architectures and data generation methods that make it easier for the network to distinguish between subclass categories. Specifically, we propose 1) a Prior Concatenation module that concatenates predicted logits from the superclass classifier to the input feature map before subclass segmentation, serving as prior knowledge to enable the network to focus on recognizing subclass categories within the current predicted superclass; 2) a Separate Normalization module that aims to stretch the intra-class distance within the same superclass, facilitating subclass segmentation; 3) a HierarchicalMix module inspired by GuidedMix [23], which for the first time suggests fusing similar labeled and unlabeled image pairs to generate high-quality pseudo labels for the unlabeled samples. However, GuidedMix selects image pairs based on their similarity and fuses entire images. In contrast, our approach is more targeted. We mix a certain superclass region from an image with subclass annotation to the corresponding superclass region in an unlabeled image without subclass annotation, avoiding confusion between different superclass regions. This allows the model to focus on distinguishing subclasses within the same superclass. Our experiments on the Brats 2021 [3] and ACDC [5] datasets demonstrate that our model, with sufficient superclass and very limited subclass annotations, achieves comparable accuracy to a model trained with full subclass annotations."
Efficient Subclass Segmentation in Medical Images,2.0,Method,"Problem Definition. We start by considering a set of R coarse classes, denoted by Y c = {Y 1 , ..., Y R }, such as background and brain tumor, and a set of N training images, annotated with Y c , denoted by D c = {(x l , y l )|y l i ∈ Y c } N l=1 . Each pixel i in image x l is assigned a superclass label y l i . To learn a finer segmentation model, we introduce a set of fine subclasskR }, such as background, enhancing tumor, tumor core, and whole tumor. We assume that only a small subset of n training images have pixel-wise subclass labels z ∈ Y f denoted byOur goal is to train a segmentation network f (x l ) that can accurately predict the subclass labels for each pixel in the image x l , even when n N . Without specification, we consider R = 2 (background and foreground) and extend the foreground class to multi subclass in this work.Prior Concatenation. One direct way to leverage the superclass and subclass annotations simultaneously is using two 1×1×1 convolution layers as superclass and subclass classification heads for the features extracted from the network. The superclassification and subclassification heads are individually trained by superclass P c (x l ) labels and subclass labels P f (x l ). With enough superclass labels, the feature maps corresponding to different superclasses should be well separated. However, this coerces the subclassification head to discriminate among K subclasses under the mere guidance from few subclass annotations, making it prone to overfitting.Another common method to incorporate the information from superclass annotations into the subclassification head is negative learning [14]. This technique penalizes the prediction of pixels being in the wrong superclass label, effectively using the superclass labels as a guiding principle for the subclassification head. However, in our experiments, we found that this method may lead to lower overall performance, possibly due to unstable training gradients resulting from the uncertainty of the subclass labels.To make use of superclass labels without affecting the training of the subclass classification head, we propose a simple yet effective method called Prior Concatenation (PC): as shown in Fig. 1 (a), we concatenate predicted superclass logit scores S c (x l ) onto the feature maps F (x l ) and then perform subclass segmentation. The intuition behind this operation is that by concatenating the predicted superclass probabilities with feature maps, the network is able to leverage the prior knowledge of the superclass distribution and focus more on learning the fine-grained features for better discrimination among subclasses.Separate Normalization. Intuitively, given sufficient superclass labels in supervised learning, the superclassification head tends to reduce feature distance among samples within the same superclass, which conflicts with the goal of increasing the distance between subclasses within the same superclass. To alleviate this issue, we aim to enhance the internal diversity of the distribution within the same superclass while preserving the discriminative features among superclasses.To achieve this, we propose Separate Normalization(SN) to separately process feature maps belonging to hierarchical foreground and background divided by superclass labels. As a superclass and the subclasses within share the same background, the original conflict between classifiers is transferred to finding the optimal transformations that separate foreground from background, enabling the network to extract class-specific features while keeping the features inside different superclasses well-separated.Our framework is shown in Fig. 1 (b). First, we use Batch Norm layers [12] to perform separate affine transformations on the original feature map. The transformed feature maps, each representing a semantic foreground and background, are then passed through a convolution block for feature extraction before further classification. The classification process is coherent with the semantic meaning of each branch. Namely, the foreground branch includes a superclassifier and a subclassifier that classifies the superclass and subclass foreground, while the background branch is dedicated solely to classify background pixels. Finally, two separate network branches are jointly supervised by segmentation loss on superand subclass labels. The aforementioned prior concatenation continues to take effect by concatenating predicted superclass logits on the inputs of subclassifier.HierarchicalMix. Given the scarcity of subclass labels, we intend to maximally exploit the existent subclass supervision to guide the segmentation of coarsely labeled samples. Inspired by GuidedMix [23], which provides consistent knowledge transfer between similar labeled and unlabeled images with pseudo labeling, we propose HierarchicalMix(HM) to generate robust pseudo supervision. Nevertheless, GuidedMix relies on image distance to select similar images and performs a whole-image mixup, which loses focus on the semantic meaning of each region within an image. We address this limitation by exploiting the additional superclass information for a more targeted mixup. This information allows us to fuse only the semantic foreground regions, realizing a more precise transfer of foreground knowledge. A detailed pipeline of HierarchicalMix is described below.As shown in Fig. 2, for each sample (x, y) in the dataset that does not have subclass labels, we pair it with a randomly chosen fine-labeled sample (x , y , z ). First, we perform an random rotation and flipping T on (x, y) and feed both the original sample and the transformed sample Tx into the segmentation network f . An indirect segmentation of x is obtained by performing the inverse transformation T -1 on the segmentation result of Tx. A transform-invariant pseudo subclass label map z pse is generated according to the following scheme: Pixel (i, j) in z pse is assigned a valid subclass label index (z pse ) i,j = f (x) i,j only when f (x) i,j agrees with [T -1 f (Tx)] i,j with a high confidence τ as well as f (x) i,j and x i,j both belong to the same superclass label.Next, we adopt image mixup by cropping the bounding box of foreground pixels in x , resizing it to match the size of foreground in x, and linearly overlaying them by a factor of α on x. This semantically mixed image x mix has subclass labels z = resize(α • z ) from the fine-labeled image x . Then, we pass it through the network to obtain a segmentation result f (x mix ). This segmentation result is supervised by the superposition of the pseudo label map z pse and subclass labels z, with weighting factor α:The intuition behind this framework is to simultaneously leverage the information from both unlabeled and labeled data by incorporating a more robust supervision from transform-invariant pseudo labels. While mixing up only the semantic foreground provides a way of exchanging knowledge between similar foreground objects while lifting the confirmation bias in pseudo labeling [1]."
Efficient Subclass Segmentation in Medical Images,3.0,Experiments,"Dataset and Preprocessing. We conduct all experiments on two public datasets. The first one is the ACDC1 dataset [5], which contains 200 MRI images with segmentation labels for left ventricle cavity (LV), right ventricle cavity (RV), and myocardium (MYO). Due to the large inter-slice spacing, we use 2D segmentation as in [2]. We adopt the processed data and the same data division in [16], which uses 140 scans for training, 20 scans for validation and 40 scans for evaluation. During inference, predictions are made on each individual slice and then assembled into a 3D volume. The second is the BraTS20212 dataset [3], which consists of 1251 mpMRI scans with an isotropic 1 mm 3 resolution. Each scan includes four modalities (FLAIR, T1, T1ce, and T2), and is annotated for necrotic tumor core (TC), peritumoral edematous/invaded tissue (PE), and the GD-enhancing tumor (ET). We randomly split the dataset into 876, 125, and 250 cases for training, validation, and testing, respectively. For both datasets, image intensities are normalized to values in [0, 1] and the foreground superclass is defined as the union of all foreground subclasses for both datasets.Implementation Details and Evaluation Metrics. To augment the data during training, we randomly cropped the images with a patch size of 256 × 256 for the ACDC dataset and 96 × 96 × 96 for the BraTS2021 dataset. The model loss L is set by adding the losses from Cross Entropy Loss and Dice Loss. The weighing factor α in HierarchicalMix section is chosen to be 0.5, while τ linearly decreases from 1 to 0.4 during the training process.We trained the model for 40,000 iterations using SGD optimizer with a 0.9 momentum and a linearly decreasing learning rate that starts at 0.01 and ends with 0. We used a batch size of 24 for the ACDC dataset and 4 for the BraTS2021 dataset, where half of the samples are labeled with subclasses and the other half only labeled with superclasses. More details can be found in the supplementary materials. To evaluate the segmentation performance, we used two widely-used metrics: the Dice coefficient (DSC) and 95% Hausdorff Distance (HD 95 ). The confidence factor τ mentioned in HierarchicalMix starts at 1 and linearly decays to 0.4 throughout the training process, along with a weighting factor α sampled according to the uniform distribution on [0.5, 1].Performance Comparison with Other Methods. To evaluate the effectiveness of our proposed method, we firstly trained two U-Net models [20] to serve as upper and lower bounds of performance. The first U-Net was trained on the complete subclass dataset {(x l , y l , z l )} N l=1 , while the second was trained on its subset {(x l , y l , z l )} n l=1 . Then, we compared our method with the following four methods, all of which were trained using n subclass labels and N superclass labels: Modified U-Net (Mod): This method adds an additional superclass classifier alongside the subclass classifier in the U-Net. Negative Learning (NL): This method incorporates superclass information into the loss module by introducing a separate negative learning loss in the original U-Net. This additional loss penalizes pixels that are not segmented as the correct superclass. Cross Pseudo Supervision (CPS) [7]: This method simulates pseudo supervision by utilizing the segmentation results from two models with different parameter initializations, and adapts their original network to the Modified U-Net architecture. Uncertainty Aware Mean Teacher (UAMT) [26]: This method modifies the classical mean teacher architecture [22] by adapting the teacher model to learn from only reliable targets while ignoring the rest, and also adapts the original network to the Modified U-Net architecture.Table 1. Mean Dice Score (%, left) and HD95 (mm, right) of different methods on ACDC and BraTS2021 datasets. Sup. and Sub. separately represents the number of data with superclass and subclass annotations in the experiments. '_' means the result of our proposal is significantly better than the closet competitive result (p-value < 0.05). The standard deviations of each metric are recorded in the supplementary materials. The quantitative results presented in Table 1 reveal that all methods that utilize additional superclass annotations outperformed the baseline method, which involved training a U-Net using only limited subclass labels. However, the methods that were specifically designed to utilize superclass information or explore the intrinsic structure of the subclass data, such as NL, CPS, and UAMT, did not consistently outperform the simple Modified U-Net. In fact, these methods sometimes performed worse than the simple Modified U-Net, indicating the difficulty of utilizing superclass information effectively. In contrast, our proposed method achieved the best performance among all compared methods on both the ACDC and BraTS2021 datasets. Specifically, our method attained an average Dice score of 87.3% for ACDC and 75.4% for BraTS2021, outperforming the closest competitor by 5.0% and 1.4%, respectively. Ablation Studies. In this study, we performed comprehensive ablation studies to analyze the contributions of each component and the performance of our method under different numbers of images with subclass annotations. The performance of each component is individually evaluated, and is listed in Table 2. Each component has demonstrated its effectiveness in comparison to the naive modified U-Net method. Moreover, models that incorporate more components generally outperform those with fewer components. The effectiveness of the proposed HierarchicalMix is evident from the comparisons made with models that use only image mixup or pseudo-labeling for data augmentation, while the addition of Separate Normalization consistently improves the model performance. Furthermore, our method was competitive with a fully supervised baseline, achieving comparable results with only 6.5% and 3.4% subclass annotations on ACDC and BraTS2021."
Efficient Subclass Segmentation in Medical Images,4.0,Conclusion,"In this work, we proposed an innovative approach to address the problem of efficient subclass segmentation in medical images, where limited subclass annotations and sufficient superclass annotations are available. To the best of our knowledge, this is the first work specifically focusing on this problem. Our approach leverages the hierarchical structure of categories to design network architectures and data generation methods that enable the network to distinguish between subclass categories more easily. Specifically, we introduced a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superclass classifier, a Separate Normalization module that stretches the intra-class distance within the same superclass to facilitate subclass segmentation, and a HierarchicalMix model that generates high-quality pseudo labels for unlabeled samples by fusing only similar superclass regions from labeled and unlabeled images. Our experiments on the ACDC and BraTS2021 datasets demonstrated that our proposed approach outperformed other compared methods in improving the segmentation accuracy. Overall, our proposed method provides a promising solution for efficient fine-grained subclass segmentation in medical images."
Efficient Subclass Segmentation in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_25.
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,1.0,Introduction,"Federated learning (FL) has emerged as a promising methodology for harnessing the power of private medical data without necessitating centralized data governance [6,7,22,25]. However, recent study [28] has identified a significant issue in current FL algorithms, namely, the trade-off between local and global performance when encountering distribution shifts. This issue is particularly prevalent in medical scenarios [12,16,17], where medical images may undergo shifts of varying degrees due to differences in imaging device vendors, parameter settings, and the patient demographics. Personalized FL (PFL) techniques are typically utilized to address the data heterogeneity problem by weighing more on in-distribution (ID) data of each client. For instance, FedRep [5] learns the entire network during local updates and keeps part of the network from global synchronization. However, they have a risk of overfitting to local data [23], especially when client local data is limited, and have poor generalizability on outof-distribution (OOD) data. Another line of work has studied the heterogeneity issue by regularizing the updates of local model. For instance, FedProx [15] constraints local updates to be closer to the global model. An effective way to evaluate FL's generalizability is to investigate its performance on the joint global distribution following [28], which refers to testing the FL models on {D i }, where D i indicates client i's distribution 1 . Unfortunately, the existing works have not found the sweet spot between personalized (local) and consensus (global) models.In this regard, we study a practical problem of enhancing personalization and generalization jointly in cross-silo FL for medical image classification when faced data heterogeneity. To this end, we aim to address the following two questions in FL: What could be the causes that result in local and global trade-off ? and How to achieve better local and global trade-off ? First, we provide a new angle to understand the trade-off. We reveal that over-personalization in FL can cause overfitting on local data and trap the model into a sharp valley of loss landscape (highly sensitive to parameter perturbation, see detailed definition in Sec. 2.2), thus limiting its generalizability. An effective strategy for avoiding sharp valleys in the loss landscape is to enforce models to obtain flat minima. In the centralized domain, weight interpolation has been explored as a means of seeking flat minima as its solution is moved closer to the centroid of the high-performing models, which corresponds to a flatter minimum [3,6,11,24]. However, research on these interpolation methods has been overlooked in FL.With the above basis, we propose to track both local and global models during the federated training and perform model interpolation to seek the optimal balance. Our insight is drawn from the model soup method [27], which shows that averaging weights of multiple trained models with same initial parameters can enhance model generalization. However, the original model soup method requires training substantial models with varying hyper-parameters, which can be prohibitively time-consuming and costly in terms of communication during FL. Given the high communication cost and the inability to restart training from scratch in FL, we leverage global models at different time points within a single training session as the ingredients to adapt the model soup method [27] to FL.In this paper, we propose a novel federated model soup method (FedSoup) to produce an ensembled model from local and global models that achieve better local-global trade-off. We refer the 'soup' as a combo of different federated models. Our proposed FedSoup includes two key modules. The first one is temporal model selection, which aims to select suitable models to be combined into one. The second module is Federated model patching [10], which refers to a finetuning technique that aims to enhance personalization without compromising the already satisfactory global performance. For the first module, temporal model selection, we utilize a greedy model selection strategy based on the local validation performance. This avoids incorporating models that could be located in a different error landscape basin than the local loss landscape (shown in Fig. 1). Consequently, each client possesses their personalized global model soups, consisting of a subset of historical global models that are selected based on their local validation sets. As for the second module In summary, our key contributions are as follows: (i) A novel FL method called Federated Model Soups (FedSoup) is proposed to improve generalization and personalization by promoting smoothness and seeking flat minima. (ii) A new temporal model selection mechanism is designed for FL, which maintains a client-specific model soups with temporal history global model to meet personalization requirements while not incurring additional training costs. (iii) An innovative federated model patching method between local and global models is introduced in federated client training to alleviate overfitting of local limited data."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.1,Problem Setup,"Consider a cross-silo FL setting with N clients. Let D := {D i } N i=1 be a set of N training domain, each of which is a distribution over the input space X . For each client, we have access to n training data points in the form of (x i j , y i j ) n j=1 ∼ D i , where y i j denotes the target label for input x i j . We also define a set of unseen target domains T := {T i } N i in a similar manner, where N is the number of target domains and is typically set to one. The goal of personalization (local performance) is to find a model f ( "
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.2,Generalization and Flat Minima,"In practice, ERM in deep neural networks, i.e., arg min θ E D (θ), can yield multiple solutions that offer comparable training loss, but vastly different levels of generalizability [3]. However, without proper regularization, models are prone to overfit the training data and the training model will fall into a sharp valley of the loss surface, which is less generalizable [4].One common reason for failures in ERM is the presence of variations in the data distribution (D i = D), which can cause a shift in the loss landscape. As illustrated in Fig. 1, the sharper the optimized minima, the more sensitive it is to shifts in the loss landscape. This results in an increase in generalization error. In cross-silo FL, each client may overfit their local training data, leading to poor global performance. This is due to the distribution shift problem, which creates conflicting objectives among the local models [23]. Therefore, when the local model converges to a sharp minima, the higher the degree of personalization (local performance) of the model, the more likely it is to have poor generalization ability (global performance).From the domain generalization formalization in [2,3], the test loss E T (θ) can be bounded by the robust empirical loss E D (θ) as follows:where the sup A |P Di (A) -P T (A)| is a divergence between domain D i and T , A is the set of measurable subsets under D i and T , and ξ is the confidence bound term related to the radius and the number of the training samples.From the Equation (1), we can infer that minimizing sharpness and seeking flat minima is directly related with the generalization performance on the unseen target domain."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.3,Our Solution: FedSoup,"After analyzing the aforementioned relationship between sharpness and generalization, we expound on the distinctive challenges of seeking flat minima and mitigating the trade-off between local and global performance in FL. Consequently, we introduce two refined modules as the ingredient of our proposed FedSoup solution. FedSoup only needs to modify the training method of the client, and the algorithm implementation is shown in Algorithm 1.Temporal Model Selection. Stochastic Weight Averaging (SWA) [11] and Sharpness-Aware Minimization (SAM) [8] are two commonly used flat minima optimizers, which seek to find parameters in wide low-loss basins. In contrast to SAM, which incurs extra computational cost to identify the worst parameter perturbation, SWA is a more succinct and effective approach for implicitly favoring the flat minima by averaging weights. The SWA algorithm is motivated by the observation that SGD often finds high-performing models in the weight space but rarely reaches the central points of the optimal set. By averaging the parameter values over iterations, the SWA algorithm moves the solution closer to the centroid of this space of points. Nevertheless, when it comes to cross-silo FL training, the discrepancy between clients is significant, and models might lie in different basins. Merging all these models haphazardly is not effective and might hinder generalization. Recently, a selective weight averaging method called model soups [27] was introduced to enhance the generalization of fine-tuned models. The original model soups is not applicable to the FL setting, requiring high communication costs and training compute. We adapt the idea to a new approach by leveraging global models trained at different time points in one pass of FL training. Additionally, considering the heterogeneity of data distribution in cross-silo FL and the requirement of personalization, we propose a model selection strategy where each client utilizes the performance of its local validation set as a monitoring indicator. We called this module temporal model selection (see Algorithm 1 Line 7-8) Federated Model Patching. According to previous analysis on the loss landscape, there is a loss landscape offset between different FL clients due to their domain discrepancy. Thus, simply integrating a global model can damage the model's personalization. To address this, we introduce the use of the model patching [10]  without a large barrier of linear connectivity. [19]. We called this module federated model patching. In summary, the update rule of FedSoup is implemented as follows:where θ g is global model, θ l is local model, k is the number of selected global models. This update rule corresponds to Algorithm 1 Line 9.Algorithm 1. FedSoup It is important to note that our proposed FedSoup algorithm requires only one carefully tuned hyper-parameter, namely the interpolation start epoch. To mitigate the risk of having empty global model soups when the start epoch is too late and to prevent potential performance degradation when the start epoch is too early, we have set the default interpolation start epoch to be 75% of the total training epochs, aligning with the default setting of SWA. Furthermore, it is worth mentioning that the modified model soup and model patching modules in our proposed FedSoup framework are interdependent. Model patching, which is a technique based on our modified model soup algorithm, provides an abundance of models to explore flatter minima and enhance performance."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.1,Experimental Setup,"Dataset. We validate the effectiveness of our proposed method, FedSoup, on two medical image classification tasks. The first task involved the classification of pathology images from five different sources using Camelyon17 dataset [1], and each source is viewed as a client. The pathology experiment consists of a total of 4, 600 images 2 , each with a resolution of 96 × 96. The second task involved retinal fundus images from four different institutions [9,21,26], and each institute is viewed as a client. The retinal fundus experiment consists of a total of 1, 264 images, each with a resolution of 128 × 128. The objective of both datasets is to identify abnormal images from normal ones. We also maintained an equal number of samples from each client to prevent clients with more data from having a disproportionate influence on the global performance evaluation. Evaluation Setup. For each client, we take 75% of the data as the training set.To assess the generalization ability and personalization of our model, we have constructed both local and global testing sets. Following [28], in our experimental setting, we first create a held-out global testing set by randomly sampling an equal number of images per source/institute, so its distribution is different from either of the client. The local testing dataset for each FL client is the remaining sample from the same source as its training set. The number of local testing set per client is approximately the same as that of the held-out global testing set. For the pathology dataset, as each subject can have multiple samples, we have ensured that data from the same subject only appeared in either the training or testing set. To align with the cross-validation setting for subsequent out-of-domain evaluations, we conducted a five-fold leave-one-client-data crossvalidation, with three repetitions using different random seeds in each fold. The results of the repeated experiments without hold-out client data are provided in the appendix. For PFL methods, we report the performance by averaging the results of each personalized models."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,Models and Training,"Hyper-Paramters. We employ the ResNet-18 architecture as the backbone model. Our approach initiates local-global interpolation at the 75% training phase, consistent with the default hyper-parameter setting of SWA. We utilize the Adam optimizer with learning rate of 1e-3, momentum coefficients of 0.9 and 0.99 and set the batch size to 16. We set the local training epoch to 1 and perform a total of 1, 000 communication rounds."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.2,Comparison with State-of-the-Art Methods,"We compare our method with seven common FL and state-of-the-art PFL methods. Results in Table 1 demonstrate that our FedSoup method achieves competitive performance on local evaluation sets while significantly improving generalization with respect to global performance. Furthermore, FedSoup exhibits greater stability with lower variance of performance across multiple experiments. Comparing the performance improvement of FedSoup across different datasets, we observed that FedSoup had a more substantial effect on the smaller retinal fundus dataset compared to the larger pathology dataset. In terms of performance gap compared to the second-best methods (FedBABU on Retina and FedProx on Pathology), our approach demonstrates a larger advantage on the Retina dataset. This observation suggests that our proposed method can mitigate the negative impact of local overfitting caused by small local datasets, thus improving the model's generalization ability. Sharpness Quantification. The sharpness measure used in this study calculates the median of the dominating Hessian eigenvalue across all training set batches using the Power Iteration algorithm [29]. This metric indicates the maximum curvature of the loss landscape, which is often used in the literature on flat minima [13] to reflect the sharpness. The median of the dominating Hessian eigenvalue of all clients in the retinal fundus dataset was measured in this part. Based on the Fig. 2(a) presented, it is evident that the proposed method leads to flatter minima as compared to the other methods.Trade-off at Different Personalized Levels. Following the evaluation in [28], we conducted an experiment comparing the local and globalperformance of different models at various levels of personalization using the retinal fundus datasets. We control the personalization level for the PFL methods by varying the number of iterations that the model undergoes fine-tuning using only the local training set after federated training. As we increase the number of finetuning iterations, we consider the level of personalization to be higher. We choose to show model performance after fine-tuning iterations 1, 7, and 15. The results in Fig. 2(b) indicated that existing FL methods often have a trade-off between local and global performance. As the number of fine-tuning iterations increases, local performance typically improves but global performance decreases. Compared to other methods, our appraoch maintains high local performance while also preventing a significant drop in global performance, which remains much higher than other methods."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.3,Unseen Domain Generalization,"We show the additional benefits of FedSoup on unseen domain generalization.Setup. To evaluate the generalization of our method beyond the participating domains, we utilize one domain that did not take part in the distributed training and used its data as the evaluation set for unseen domain generalization. To this end, we perform leave-one-out cross-validation by having one client as the tobe-evaluated unseen set each time. To ensure a reliable results of unseen domain generalization, we conducted experiments on the Camelyon17 dataset, which has a larger number of samples.Results. Overall, our proposed method demonstrates an advantage in terms of unseen domain generalization capabilities (see Fig. 2(c)). In comparison to FedAvg, Our approach resulted in a 2.87-point increase in the AUC index for generalization to unseen domains on the pathology dataset."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,4.0,Conclusion,"In this paper, we demonstrate the trade-off between personalization and generalization in the current FL methods for medical image classification. To optimize this trade-off and achieve flat minima, we propose the novel FedSoup method. By maintaining personalized global model pools in each client and interpolating weights between local and global models, our proposed method enhances both generalization and personalization. FedSoup outperforms other PFL methods in terms of both generalization and personalization, without incurring any additional inference or memory costs."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,,1: l ← average(soup ∪ {θ l }){Module 2: Federated Model Patching} 10: return θ l
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,Table 1 .,FedSoup 85.71 (0.37) 92.47 (0.31) 72.87 (1.35) 81.45 (1.40) 90.92 (0.50) 96.00 (0.43) 78.64 (0.90) 86.24 (0.86)
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 30.
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,1.0,Introduction,"Predictive modeling of the individual-level cognitive development during infancy is of great importance in advancing our understanding of the subject-specific relationship between the cognitive ability and early brain structural and functional development and their underlying neural mechanisms. It is also critical for early identifying cognitive delays and developing more effectively and timely personalized therapeutic interventions for at-risk infants. However, this is a very challenging task due to the complex and rapid development of brain structure, function and cognition during the first years of life [1,2].Recently, a few methods have been explored for predicting infant cognition using either resting-state functional MRI (rs-fMRI) [2][3][4] or structural MRI (sMRI) [5][6][7]. Although encouraging preliminary results have been achieved, two unaddressed major issues hinder the precise prediction of the individual-level cognitive development during infancy. 1) Spatial information loss: Previous works [3][4][5][6][7][8] typically rely on region-level features or inter-region connectivity features after parcellation of the brain cortex into a set of regions. Consequently, these features largely ignore fine-grained spatial patterns on cortical surfaces, which encode subject-specific rich information critical for cognitive prediction. 2) Modality-information loss: Previous methods use either functional features or structural features, and thus the complementary information between them and their underlying relationship are not leveraged for cognition development. Indeed, it is believed that the spontaneous neuronal activity is related to the intrinsic human brain functional organizations supported by the underlying structural substrates [2], which gives emphasis to understanding the underlying individual structure-functional profile during infancy. Therefore, an effective unified framework that can automatically learn the complementary and spatially fine-grained information from structural and functional data for cognition development prediction is critically desired.To address the above issues, we propose a novel cortical surface-based multimodal learning framework (CSML), to enable learning of the fine-grained spatial patterns and complementary information from structural and functional MRI data for precise prediction of the individual-level cognitive development. Specifically, 1) to learn detailed spatial patterns of both functional connectivity and structural information, we propose to leverage the strong feature learning and representation ability of spherical surface networks [9] to automatically extract task-related features on cortical surfaces. 2) To effectively fuse structural and functional information, we propose a dual-branch surface network to simultaneously extract structural morphologic features and functional connectivity features on cortical surfaces, and further fuse their complementary information in a feature disentanglement module. 3) To enable precise prediction of cognitive outcome, we leverage the prior knowledge that the cognition function develops with age growing by jointly predicting age and cognition scales. To our best knowledge, this is the first work to leverage the multimodal, fine-grained spatial information on cortical surface explicitly for cognition development prediction. The experimental results based on a longitudinal infant dataset not only validate the superiority of our proposed model but also imply the tight association between the individual cognition development and the fine-grained cortical information."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.0,Method,"In this section, we present the details of CSML (Fig. 1), including three steps: 1) surface-based fine-grained information representation (Fig. 1(a)); 2) modality-specific information learning (Fig. 1(b)); and 3) multi-modality information fusion (Fig. 1(c)). Fig. 1. Overview of our framework for cortical surface-based multimodal fine-grained information learning. Given the sMRI and fMRI of an infant, its structural and functional feature representations z s and z f are first extracted. Then, the modality shared (Com(z)) and specific (Spe(z)) information are disentangled and further fused by a modality fusion block F. After that, we constrain the fused latent variable m s,f to be age-irrelevant by the age predictor P a , and finally obtain the predicted cognitive scores from the predictor P c ."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.1,Surface-Based Fine-Grained Information Representation,"The input of the network framework consists of two branches for encoding cortical structural information and functional connectivity information, respectively. To integrate multi-modal MRI data together for cognition development prediction, we map all modality data to a common space, i.e., the cortical surface registered to UNC 4D infant surface atlas [10] and further resampled with 40,962 vertices, following the well-established pipelines [11][12][13][14][15]. To capture the spatially fine-grained information in structural MRI, the structural branch contains a set of surface maps of biologically meaningful cortical properties, including cortical thickness, surface area, cortical volume, sulcal depth, mean curvature, and average convexity. To preserve the fine-grained spatial patterns of functional connectivity, we leverage an infant-dedicated cortical functional parcellation map [15]. Specifically, for each parcel, we first calculate the Pearson's correlation coefficient between the averaged functional time series of all vertices within this parcel and the functional time series of each cortical vertex to build the parcel-specific cortical functional connectivity (FC) map and then perform Fisher's r-to-z transformation. Finally, we use these cortical FC maps from all parcels, which characterize rich spatially detailed FC information, as the input of the functional branch."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.2,Modality-Specific Encoder,"For the multi-modality input, we employ two modality-specific encoders E s and E f to describe its feature representation, respectively. To be specific, we regard each modality comprised of multiple feature channels, while each channel could be interpreted as an observation of the data from a certain view. Therefore, we process each view separately asI is the number of morphological features we used; j [1, J ], J is the number of parcels we used in building FC maps. We implement E s and E f as the Spherical Res-Net [9,16,17], which is composed of stacks of spherical convolutional layers and spherical pooling layers to extract the fine-grained spatial patterns and generates the view-related feature representations of v i s and v j f . Considering the different number of views in each modality, two Transformer layers [18] T s and T f are then adopted to fuse the multi-view feature representations for each modality separately. Herein, following the previous work [19], we prepend two learnable embeddings x s and x f as the first token for the sequences of view-related feature representations {v i s |i [1, I ]} and {v j f |j [1, J ]}, respectively. Within the Transformer layers, for the structural-related features, x s interact with and fuse the view-related feature representation {v i s |i [1, I ]} through the self-attention mechanism as follows,∼where z s is the aggregated representation for the structural data, Q s (•), K s (•), U s (•), and W s (•) are four multi-layer perceptrons (MLP), const is a constant for normalization. Similarly, we can obtain the functional-related variable z f by feeding x f and the functional view-related representations {v"
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.3,Modality-Fusion Block,"To better learn the complementary information between the two modalities, we further decompose the modality-specific latent variables z s and z f into two parts: Com(z n ) and Spe(z n ), where n {s, f }, standing for the structure (s) and function (f ) related variables, respectively. Com(z n ) is the common code representing the shared information among modalities, while Spe(z n ) is the specific code representing the complementary information that differentiates one modality from the other. The basic requirements of this disentanglement are: (1) The concatenation of Com(z n ) and Spe(z n ) equals z n ; (2) Com(z s ) and Com(z f ) should be as similar as possible; (3) Spe(z s ) differs from Spe(z f ) as much as possible. Accordingly, L 1 Disen is defined as:Since the latent variable of each modality has been disentangled into Com(z n ) and Spe(z n ), the combined information is formed as the concatenation of the common code and specific codes as follows: z s,f = Spe(z s ), Common, Spe(z f ) , where Common = 0.5(Com(z s ) + Com(z f ))."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.4,Cognitive Scores Prediction,"Given the combined multimodal information z s,f , it is intuitive to regress the cognitive scores directly. However, considering that cognitive functions develop rapidly during the first years of life [1], the regressor would be prone to learn the age-related information instead and thus cannot differentiate the individualized development discrepancy between subjects within the same age group. Therefore, we fuse the combined multimodal information through a MLP F as follows, m s,f = F(z s,f ), and further disentangle the age-related variance Age(m s,f ) and the individual-related invariance Ind (m s,f ) from m s,f to precisely evaluate the cognition development level. The basic requirements of this disentanglement are: (1) The concatenation of Age(m s,f ) and Ind (m s,f ) equals m s,f ;(2) Age(m s,f ) is capable of age estimation through an age predictor P a ; (3) Ind (m s,f ) is incapable of age estimation through P a . Accordingly, L 2  Disen is defined as:where t is the ground truth of age. Then, we can use the identity-related features Ind (m s,f ) containing subject-specific structure-function profile to predict the cognitive scores through a cognitive score predictor P c under the guidance of the corresponding age feature Age(m s,f ). The loss function to train P c is defined as:where y is the ground truth of cognitive scores. Specifically, we implement P a and P c as two sets of MLP. Finally, the overall objective function to optimize the neural network is written as:where λ 1 and λ 2 are trade-off parameters to balance the multiple losses."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.1,Dataset,"We verified the effectiveness of the proposed CSML model for infant cognition development prediction on a public high-resolution dataset including 318 pairs of sMRI and rs-fMRI scans acquired at different ages ranging from 88 to 1040 days in the UNC/UMN Baby Connectome Project [20]. All structural and functional MR images were preprocessed following state-of-the-art infant-tailored pipelines [11][12][13][14][15]. Cortical surfaces were reconstructed and aligned onto the public UNC 4D infant surface atlas [10,11]. For each cortical vertex on the middle cortical surface, its representative fMRI time-series was extracted [13][14][15]. An infant-dedicated fine-grained functional parcellation map [15] with 432 cortical ROIs per hemisphere in UNC 4D infant surface atlas was warped onto each individual cortical surface.To quantify the cognition development level of each participant, four Mullen cognitive scores [21] were collected at their corresponding scan ages, i.e., Visual Receptive Scale (VRS), Fine Motor Scale (FMS), Receptive Language Scale (RLS), and Expressive Language Scale (ELS). These four cognitive scales were respectively normalized into the [0, 1] range using the minimum and maximum values for the training efficiency."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.2,Experimental Settings,"In order to validate our methods, a 5-fold cross-validation strategy is employed, and each fold consists of 190 training samples, 64 validating samples, and 64 testing samples. To quantitatively evaluate the performance, the Pearson's correlation coefficient (PCC) and root mean square error (RMSE) between the ground truth and predicted values were calculated. In the testing phase, the mean and standard deviation of the 5-fold results were reported.The encoders E s and E f in CSML constitutes 5 Res-blocks with the dimensions of {32, 32, 64, 64, 128}, respectively. The modality fusion block F, age predictor P a , and cognitive score predictor P c were designed as two-layer MLP with the ReLU activation function and the dimension of {192, 128}, {64, 1}, and {128, 1}, respectively. We implemented the model with PyTorch and used Adam as optimizer with the weight decay of 10 -4 and the learning rate cyclically tuned within [10 -6 , 10 -3 ]. The batch size was set to 1. The maximum training epoch is 500. After comparison, we empirically set the hyperparameters as λ 1 =0.05 and λ 2 =0.01."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.3,Results,"We first show the results of some ablated models of our method in Table 1, where w/o Structure and w/o Function denote for the variants using functional and structural features only. w/o Age denotes the variant with single task of cognition prediction. It can be observed that, the overall performance on four cognitive tasks has been extensively improved by jointly leveraging the structural and functional information. The disentanglement mechanism successfully separates the shared and complementary information amongst modalities and further removes the redundancy with the loss L 1  Disen . Moreover, the joint age prediction and cognitive estimation also brings further improvement by differentiating the age-related and identity-related variables with L 2 Disen . The scatter plots of predicted cognitive scores in five testing folds are depicted in Fig. 2(a), demonstrating that the scores are well predicted.We also comprehensively compared with various traditional and state-of-the-art functional connectivity-based methods, including KNN, random forest (RF), SVR, gaussian process regression (GPR), GCN [22], GAT [23], and UniMP [24]. As shown in Table 2, our algorithm outperforms the previous methods by a large margin. Of note, the proposed method demonstrates better performance even with the functional information only, which highlights the importance to preserve the fined-grained FC information. Additionally, based on our proposed model CSML, the prediction accuracy of infant cognition development is over 0.85 on average, suggesting that the model may observe plausible biomarkers for cognition development during infancy. Based on the welltrained models, we explored the explainability and interpretability of the proposed method by investigating the weights of the Transformers. Since the Transformer layers T s and T f fuse the multi-view representations v i s and v j f into z s and z f for further cognitive prediction, by analyzing the attention value A i n of each view v i n in the Transformer, we can infer which regions for functional data and which morphological features for structural data are more important for cognition prediction. The results are shown in Fig. 2 (b) and Fig. 2 (c), in line with the reports in related studies to some extent [25][26][27][28], demonstrating the scientific value of our method. For example, the left lateral prefrontal cortex involved in higher executive functions [25,26] demonstrates high importance in functional data. Moreover, previous researchers [27,28] have observed the close relevance between the visual cortex and the early cognitive process, which also confirms the result of our method. "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,4.0,Conclusion,"In this work, we develop an innovative cortical surface-based multimodal learning framework (CSML) to address the infant cognition prediction problem. Specifically, we unprecedentedly propose to explicitly leverage the surface-based feature representations to preserve the fine-grained, spatially detailed multimodal information for cognition prediction. In addition, by disentangling the modality-shared and complementary information, our model successfully captures the individualized cognition development patterns underlying the dramatic brain development. With its superior performance compared to state-of-the-art methods, our proposed CSML suggests that the informative clues for brain-cognitive relationship are hidden in the multimodal fine-grained details and validates itself as a potentially powerful framework for simultaneously learning effective representations from sMRI and rs-fMRI data."
Category-Independent Visual Explanation for Medical Deep Network Understanding,1.0,Introduction,"Medical application is a field that has high requirements of model reliability, trustworthiness, and interpretability. According to the act proposed by the European Commission on AI system regulation [4], medical AI systems are categorized as high-risk systems. Five sets of requirements are listed: (1) high quality of data, (2) traceability, (3) transparency, (4) human oversight, (5) robustness, accuracy, and cybersecurity. These requirements impose a potential challenge for deep learning models where such a model is often used as a black-box system. To increase a model's explainability, many visualization methods are proposed to generate the region of interest (ROI) heatmap based on the output of the deep learning model [7,9]. This ROI heatmap highlights the region that deep learning algorithms focus on. This region often contains cues for researchers to investigate the algorithm's decision making process which would help doctors to gain confidence in the AI assisted products. For example, when doctors see a model make a correct prediction and at the same time highlight the right ROI, then it would help this model to gain more trust from doctors.The state-of-art algorithms mostly focus on providing visualization during training where the categorical label is available. It becomes problematic at product deployment stage when no label is available. Without supplying the ground truth categorical labels, the false categorical labels would mislead the visualization algorithm to highlight wrong regions for cues. A sample is shown in Fig. 1. GradCAM [23] visualization is used widely on a deep learning network that classifies multiple diseases. Three different categorical labels are supplied (Fig. 1 (bd)) which leads GradCAM to generate three distinguishable ROI heatmaps. To address this issue, we propose a method called Hessian-Category Independent Activation Maps (Hessian-CIAM), which utilizes the Hessian matrix as an activation weighting function to eliminate the need for categorical labels to compute the ROI heatmap. Then a polarity checking process is added to the post process which corrects the polarity error from the SVD based smoothing function. Figure 1 (e) shows the visualization from our category-independent method. We benchmark our algorithm against seven state-of-art algorithms on the Chestx-ray8 dataset which demonstrated the superior performance of our algorithm. Additionally, we demonstrate a clinical use case in glaucoma detection from retinal images which shows the flexibility of our algorithm."
Category-Independent Visual Explanation for Medical Deep Network Understanding,2.0,Related Works,"The visual explanation for deep networks is an essential task for researchers to interpret and debug deep networks where an ROI heat map is one of the most popular tools. This field is pioneered by Oquab et al. [18] which additional Global Max Pooling (GMP) layers are added to extract the attention region from a trained convolutional network. It is later improved by CAM [27] by attaching a Global Average Pooling (GAP) layer to the existing model. The GAP identifies the extent of the object while GMP only finds one discriminative part. One drawback of Oquab's method and CAM is the requirement of modifying the original network to output visualizations. This requirement is eliminated by a gradientbased approach GradCam [23]. In this algorithm, the activation weights from the last convolutional layer of the deep network are extracted and weighed by a gradient from the back-propagation to generate the ROI heat map. This method is later improved by GradCAM++ [3] and LayerCAM [12]. An alternative way to generate an ROI heatmap is perturbation-based methods. It removes the requirement of the gradient calculation by iteratively perturbating different parts of the activations weight [22] or image [20,24] to identify the region on the image that has the highest impact on the prediction result. One major drawback of such an approach is its speed as it requires iteratively running the deep learning model. The gradient-based and perturbation-based methods deliver high-quality ROI heatmap when a categorical label is supplied. It is a useful visualization tool to help researchers interpret the deep network during the development stage. It becomes a different story when it comes to deployment. During the deployment, there is no such luxury of having a ground truth categorical label that is supplied to the visualization algorithm. One solution to relax this problem is using the prediction result as a target label, but this solution often generates a wrong visualization as when the deep learning algorithm outputs incorrect prediction. Muhammad [17] proposed a method to eliminate the dependence on the ground truth categorical label by directly applying SVD on the 2D activations and using its first principle component as the ROI heat map. The first principle component's polarity is bi-directional which could potentially highlight the non-interest region instead. Visualization techniques such as slot attention [15], SCOUTER [13], and SHAP [16] require modification on the original network and training to generate an ROI heatmap. It is not the main scope of our paper and we will not further discuss it here.Medical applications have high requirements for model reliability, trustworthiness, and interpretability. The visualization tools such as GradCAM and Fig. 2. Overview of our algorithm, the Hessian matrix, and activation weight from the last convolutional layer is used to create an ROI heatmap followed by a post process.GradCAM++ are widely applied to medical applications such as retina imaging [21], X-ray [10], CT [6], MRI [26], and ultrasound [11]. However, those visualization algorithms require categorical labels to generate visual explanations. This requirement limits the usage of algorithms to the training stage where the ground truth category label is available. Generating high quality visual explanations without relying on the category label at the deployment stage remains a challenge. In this work, we propose a category-independent visual explanation method to solve this problem."
Category-Independent Visual Explanation for Medical Deep Network Understanding,3.0,Method,"Our algorithm generates an ROI heatmap to indicate the region on the image that the deep learning algorithms focus on when making classification decisions. Our method does not require any modification or additional training on target deep networks. The overview flow of our algorithm is illustrated in Fig. 2. Input images feed into the deep network where the activation weights from the last convolution layer are weighted by the Hessian matrix followed by a post-process to output a clean ROI heatmap.It is well known that the Hessian matrix appears in the expansion of gradient about a point in parameter space [19], as:where ω is a point in parameter space, Δω is a perturbation of ω, Δω is the gradient and H is the hessian matrix. In order to approximate the Hessian matrix H, we let Δω = rv, where v is the identity matrix, and r is a small number which leads the O(r) term to become insignificant. So we can further simplify the equation into:Our goal is to apply the Hessian matrix as a weighting function to indicate the significance of each activation function output in the CNN. So we applied an L2 normalization on the Hv, here v is an identity matrix, so we can get the normalized Hessian matrix Ĥ = |Hv| Hv 2. In the CNN we denote A k as the feature activation map from the kth convolution layer. Ĥk denotes the normalized Hessian matrix in the kth layer. We calculate the Hadamard product between Ĥk and A k , then apply ReLU to obtain the new activation map. n is the depth of the activation map. The ROI heatmap L H = n k=1 ReLu( Ĥk A k ). The ROI heatmap L H can be noisy, we follow Muhammad's approach [17] to smooth out the L H which applies SVD on A k H = ReLu( Ĥk A k ) = UΣV T where U denotes a M × M matrix. Σ denotes a diagonal matrix with size of M × N . V denotes a N × N matrix. The column of U and V are the left singular vectors. The V 1 denotes the first component in V which is a weight function to create a smoothed ROI heatmapOne drawback of Muhammad's approach [17] is the polarity of V 1 is not considered as the Eigenvectors from SVD are bidirectional. It could lead the algorithm to output non-ROI regions. To solve this problem, we revise the algorithm to calculate the correlation between the smoothed version L HS and the original ROI heatmap L H . If the correlation appears negative, we will reverse the ROI heatmap, as:"
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.1,Experiment Setup,"We conduct experiments on lung disease classification Chestx-ray8 [25] to evaluate the performance of our algorithm. The Chestx-ray8 dataset contains 100,000x-ray images with 19 disease labels. It is a significantly imbalanced dataset with some categories having as few as 7 images. To demonstrate our visualization techniques, we simplified the dataset by selecting 6 diseases with a higher number of images. After the selection, our training set contains images from atelectasis (3135 images), effusion (2875 images), infiltration (6941 images), mass (1665 images), nodule (2036 images), and pneumothorax (1485 images). Additionally, we randomly selected 7000 images from healthy people. 20% of images in the training set were set aside as validation sets for parameter tuning. This dataset contains 881 test images with bounding boxes that indicate the location of the diseases which 644 images were in the 6 diseases we selected. We utilize the pre-trained ResNet50 [8] as the backbone. The cross-entropy loss is used as a loss function; the learning rate is set to 0.00001; the batch size is 64. The training cycle is set to 100 epochs. Our workstation is equipped with 2 Nvidia 3090 GPU (24 GB RAM), Intel Xeon CPU (3.30 GHz), and 128 GB RAM."
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.2,Quantitative Evaluation,"The algorithm is evaluated following the method proposed by Cao et al. [2]. The union of intersection (IoU) between the bounding box and ROI is measured. The  foreground of ROI is extracted based on applying thresholds to find the area that covers 95%, 90%, 85%, 80%, and 75% of energy from the heatmap. The gradient and perturbation-based methods require ground truth labels to generate an ROI heatmap but, at the inference time, the ground truth label is not available.To simulate the deployment scenario, we conduct two sets of evaluations. In the first evaluation, the prediction results (our ResNet model delivers 42.6% prediction accuracy) from the deep learning model are used as a label feed into the visualization methods. One drawback of this approach is the prediction result is not always reliable and the incorrect prediction could mislead the algorithm to output the wrong ROI. As a comparison, in the second evaluation, we supply ground truth labels to visualization methods. The quantitative evaluation of different visualization methods is shown in Table 1.Three groups of visualization algorithms are evaluated in our experiment. The gradient based group contains the algorithm that relies on the gradient from the label to generate the ROI. In this group, GradCAM achieved the highest at 0.175 IoU at the 75% threshold. The pertubation based group makes small perturbations in the input image or activation weight to find the ROI that has the highest impact. In this group, the ScoreCAM achieved the highest 0.168 IoU at the 75% threshold. The category independent group contains algorithms that do not require a label to generate ROI. Our method scored the highest IoU at 0.271 IoU at the 75% threshold. When ground truth labels are supplied, the IoU for gradient based methods is improved in the range of 5% to 20%. For perturbation based methods, supplying ground truth data reduced the performance of AblationCAM and had minimal impact on RISE.Next, we split the test set into two categories which are samples with wrong and correct predictions (shown in Fig. 3 (left)). The 75% threshold is used to calculate IoU. The samples with correct prediction consistently scored higher IoU across all visualization methods. Our method shows the highest performance in both wrong and correct prediction categories. The perturbation based methods consistently scored lower than other methods indicating this group is not suitable for X-ray image classification applications.To further investigate the efficiency of our algorithm, we extract the IoU on each disease (Fig. 3 (right)) where a 75% threshold is applied to calculate the mean IoU. The evaluation shows our algorithm is positively correlated with the size of the ground truth bounding box. It indicates the disease with a larger infection area is easier to visualize by our algorithm."
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.3,Qualitative Evaluation,Sample images comparing our algorithm with five state-of-art algorithms are shown in Fig. 4. Our algorithm has a cleaner heatmap. The gradient methods generate a heatmap that contains a higher level of noise that covers a large area of the non-lung regions such as the shoulder. The perturbation based methods deliver the worst visualization in our evaluation. The AlbationCAM and ScoreCAM are only able to highlight the whole lung area but it does not provide any clinical value to pinpoint the disease locations. The RISE [20] method delivers multiple clusters of highlight regions that are not feasible to provide human-readable information. The last row of Fig. 4 shows the worst case in our evaluation which is a representative case to illustrate the failure mode of our algorithm. The deep learning algorithm may fail to detect the small size lesions which leads to the wrong ROI for visualization methods. More comparison is available in the supplementary material.
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.4,Clinical Application,"Our algorithm has the potential to apply to many clinical applications. We conducted an additional experiment on the glaucoma retinal image database [14]   with 3,144 negative and 1,712 positive glaucoma samples1 Each sample contains a saliency map annotated by ophthalmologists by using mouse clicks to simulate the human visual attention process. Since our goal is to evaluate the explainability of our visualization algorithm, we decided to use all images to train the glaucoma classification model. We follow the work from Bylinskii et al. [1] to apply similarity (histogram intersection), cross-correlation, and Judd AUC to measure the performance of our algorithm. The 95% energy of the ROI heatmap was used as a threshold to clean our heatmap. Our algorithm achieved 0.618 ± 0.0024 in similarity, 0.755 ± 0.0033 in cross-correlation, and 0.703 ± 0.0013 in Judd AUC. The complete evaluation is available in the supplementary material (Fig. 5)."
Category-Independent Visual Explanation for Medical Deep Network Understanding,5.0,Conclusion,"In this study, we propose a novel category-independent deep learning visualization algorithm that does not rely on categorical labels to generate visualizations. Our evaluation demonstrates that our algorithm outperforms seven state-of-theart algorithms by a significant margin on a multi-disease classification task using X-ray images. This indicates that our algorithm has the potential to enhance model explainability and facilitate its deployment in medical applications. Additionally, we demonstrate the flexibility of our algorithm by showing a clinical use case on retinal image glaucoma detection. Overall, our proposed Hessian-CIAM algorithm represents a promising tool for improving our understanding of deep learning models and enhancing their interpretability, particularly in medical applications."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,1.0,Introduction,"Carotid stenosis grading (CSG) represents the severity of carotid atherosclerosis, which is highly related to stroke risk [11]. In clinical practice, sonographers need to first visually locate the frame with the largest degree of vascular stenosis (i.e., minimal area of remnant vessels) in a dynamic plaque video clip based on B-mode ultrasound (US), then manually delineate the contours of both vessels and remnant vessels on it to perform CSG. However, the two-stage pipeline is time-consuming and the diagnostic results heavily rely on operator experience and expertise due to ambiguous plaque boundaries and temporal variation (see Fig. 1). Fully-supervised segmentation models can automatize this procedure, but require numerous pixel-level masks laboriously annotated by sonographers and face the risk of training failure due to unreliable annotation. Hence, tackling this task via weak supervision, i.e., video classification, is desired to avoid the requirement of tedious and unreliable annotation.Achieving accurate automatic CSG with US videos is challenging. First, the plaque clips often have extremely high intra-class variation due to changeable plaque echo intensity, shapes, sizes, and positions (Fig. 1(d-e)). The second challenge lies in the inter-class similarity of important measurement indicators (i.e., diameter and area stenosis rate) for CSG among cases with borderlines of mild and severe, which makes designing automatic algorithms difficult (Fig. 1(c-d)).A typical approach for this video classification task is CNN-LSTM [7]. Whereas, such 2D + 1D paradigm lacks interaction with temporal semantics of input frames in the early stage. Instead, a more efficient way is to build 3D networks that handle spatial and temporal (ST) information simultaneously [21].There are several types of 3D networks that have been widely used in visual tasks: (1) Pure 3D convolution neural networks (3D CNNs) refer to capturing local ST features using convolution operations [4,10,21]. However, most current 3D CNNs suffer from the lack of good initialization and capacity for extracting global representations [16]. (2) Pure 3D transformer networks (3D Trans) aim to exploit global ST features by applying self-attention mechanisms [3,9,15,20]. However, their ability in extracting local ST information is weaker than 3D CNNs. Moreover, such designs have not deeply explored lightweight crossdimensional attention mechanisms to gain refined fused features for classification.Recently, Wang et al. [18] first introduced the self-attention mechanism in 3D CNN for video classification. [16,19] then proposed Convolution-Transformer hybrid networks for image classification. Li et al. [12] further extended such hybrid design to 3D for video recognition by seamlessly integrating 3D convolution and self-attention. Thanks to both operations, such networks can fully exploit and integrate local and global features, and thus achieve state-of-the-art results. However, current limited 3D hybrid frameworks are designed in cascade, which may lead to semantic misalignment between CNN-and Transformer-style features and thus degrade accuracy of video classification.In this study, we present the first video classification framework based on 3D Convolution-Transformer design for CSG (named CSG-3DCT). Our contribution is three-fold. First, we propose a novel and effective video classification network for weakly-supervised CSG, which can avoid the need of laborious and unreliable mask annotation. Second, we adopt an inflation strategy to ease the model training, where pre-trained 2D convolution weights can be adapted into the 3D counterpart. In this case, our network can implicitly gain the pre-trained weights of existing large models to achieve an effective warm start. Third, we propose a novel play-and-plug attention-guided multi-dimension fusion (AMDF) transformer encoder to integrate global dependencies within and across ST dimensions. Two lightweight cross-dimensional attention mechanisms are devised in AMDF to model ST interactions, which merely use class (CLS) token [8] as Query. Extensive experiments show that CSG-3DCT achieve state-of-the-art performance in CSG task."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,2.0,Methodology,"Figure 2(a) shows the pipeline of our proposed framework. Note that the proposed CSG-3DCT is inflated from the 2D architecture. Thus, it can implicitly gain the pre-trained weights of current large model for effective initialization. In CSG-3DCT, given a video clip, vessel regions are first detected by the pre-trained detection model [14] for reducing redundant background information. Then, the cropped regions are concatenated to form a volumetric vessel and input to the 3D CNN and Transformer encoders. Specifically, to better model the global knowledge, ST features are decoupled and fused by the proposed AMDF transformer encoder. CNN-and Transformer-style features are integrated by the 3D feature coupling unit (3D FCU) [16] orderly. Finally, by combining the CNN features and the CLS token, the model will output the label prediction.3D Mix-Architecture for Video Classification. CNN and Transformer have been validated that they specialize in extracting local and global features, respectively. Besides, compared to the traditional 2D video classifiers, 3D systems have shown the potential to improve classification accuracy due to their powerful capacity of encoding multi-dimensional information. Thus, in CSG-3DCT, we propose to leverage the advantages of both CNN and Transformer and extend the whole framework to a 3D version.The meta-architecture of our proposed CSG-3DCT follows the 2D Convolution-Transformer (Conformer) model [16]. It mainly has 5 stages (termed c1-c5 ). Extending it to 3D represents that both CNN and Transformer should be modified to adapt the 3D input. In specific, we tend to inflate the 2D k ×k convolution kernels to 3D ones with the size of t × k 2 by adding a temporal dimension, which is similar to [4]. Such kernels can be implicitly pre-trained on ImageNet through bootstrapping operation [4]. While translating the 2D transformer only requires adjusting the token number according to the input dimension.Inflation Strategy for 3D CNN Encoder. We devise an inflation strategy for the 3D CNN encoder to relieve the model training and enhance the representation ability. For achieving 2D-to-3D inflation, a feasible scheme is to expand all the 2D convolution kernels at temporal dimension with t>1 [4]. However, multi-temporal (t>1) 3D convolutions are computationally complex and hard to train. Thus, we only select part of the convolution kernels for inflating their temporal dimension larger than 1, while others restrict the temporal dimension to 1. By adapting pre-trained 2D convolution weights into the 3D counterpart, our network can achieve good initialization from existing large model. Moreover, we notice that performing convolutions at a temporal level in early layers may degrade accuracy due to the over-neglect of spatial learning [10]. Therefore, instead of taking a whole-stage temporal convolution, we only perform it on 3D Conv blocks of the last three stages (i.e., c3-c5 ). We highlight that our temporal convolutions are length-invariant, which indicates that we will not down-sample at the temporal dimension. It can benefit the maintenance of both video fidelity and time-series knowledge, especially for short videos. See supplementary material for more details.Transformer Encoder with Play-and-plug AMDF Design. Simply translating the 2D transformer encoder into the 3D standard version mainly has two limitations: (1) It blindly compares the similarity of all ST tokens by selfattention, which tends to inaccurate predictions. Moreover, such video-based computation handles t× tokens simultaneously compared to image-based methods, leading to much computational cost. ( 2) It also has no ability to decide which information is more important during different learning stages. Thus, we propose to enhance the decoupled ST features and their interactions using different attention manners. The proposed encoder can improve computational efficiency, and can be flexibly integrated into 2D or 3D transformer-based networks.Before the transformer encoder, we first decompose the feature maps X produced by the stem module into t × n 2 embeddings without overlap. A CLS token X cls ∈ R d is then added in the start position of X to obtain merged embeddings Z ∈ R d×(t×n 2 +1) . n 2 and d denote the number of spatial patch tokens and hidden dimensions, respectively. Then, the multiple AMDF Trans blocks in the transformer encoder drive Z to produce multi-dimensional enhanced representations. Specifically, the AMDF block has the following main components.1) Intra-dimension ST Learning Module. Different from the cascade structure in [3], CSG-3DCT constructs two parallel branches to learn global ST features, respectively. As shown in Fig. 2(b), the proposed module is following ViT [8], which consists of a multi-head self-attention (MHSA) module and a feed-forward network (FFN). Query-Key-Value (QKV) projection after Layer-Norms [2] is conducted before each MHSA module. Besides, the residual connections are performed in MHSA module and FFN. Taking token embeddings as input, the two branches can extract the ST features well by parallel spatial and temporal attention (see Fig. 2(c) for visualization of computation process).2) Inter-dimension ST Fusion Module. To boost interactions between S and T dimensions, we build the inter-dimension fusion module after the intradimension learning module. The only difference between the two types of modules is the calculation mode of attention. As shown in Fig. 3, we consider the following two methods to interact the ST features: (i) Switched Attention (SWA) Fusion and (ii) Cross Attention (CA) Fusion. Here, we define one branch as the target dimension and the other branch as the complementary dimension. For example, when the temporal features are flowing to the spatial features, the spatial branch is the target and the temporal branch is the complementary one. SWA is an intuitive way for information interaction. It uses the attention weights (i.e., generated by Q and K) as the bridge to directly swap the information. For computing the target features in CA, K and V are from the complementary one, and Q is from its own. Intuition behind CA is that the target branch can query the useful information from the given K and V [6]. Thus, the querying process in CA can better encourage the knowledge flowing.To improve computing efficiency in CA, Chen et al. [5] proposed to adopt the CLS token of the target branch to compute the CLS-Q to replace the common Q from token embeddings. Then, they transferred the target CLS token to the complementary branch to obtain the K and V and perform CA. However, such a design may lead to overfitting due to the query-queried feature dependency. Motivated by [5], we introduce a simple yet efficient attention strategy in interdimension ST fusion module. Specifically, the target dimension adopts its CLS token as a query to mine rich information, and this CLS token will not be inserted into the complementary dimension. Besides, using one token only can reduce the computation time quadratically compared to all tokens attention.3) Learnable Mechanism for Adaptive Updating. Multi-dimensional features commonly have distinct degrees of contribution for prediction. For example, supposing the size of carotid plaque does not vary significantly in a dynamic segment, the spatial information may play a dominant role in making the final diagnosis. Thus, we introduce a learnable parameter to make the network adaptively adjust the weights of different branches and learn the more important features (see Fig. 2(b)). We highlight that this idea is easy to implement and general to be equipped with any existing feature-fusion modules."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,3.0,Experimental Results,"Dataset and Implementations. We validated the CSG-3DCT on a large inhouse carotid transverse US video dataset. Approved by the local IRB, a total Table 1. Quantitative results of methods. ""MTV(B/2+S/8)"" means to use the larger ""B"" model to encode shorter temporal information (2 frames), and the smaller ""S"" model to encode longer temporal information (8 frames) [20]. † denotes random initialization. * indicates removing the learnable mechanism from AMDF encoder."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,,Methods,"Accuracy F1-score Precision Recall I3D [4] 78.8% 78.8% 81.0% 80.8% SlowFast [10] 70.3% 70.2% 70.3% 70.8% TPN [21] 78 of 200 videos (63225 images with size 560×560 and 380×380) were collected from 169 patients with carotid plaque. In clinic, sonographers often focus on a relatively narrow short plaque video clip instead of the long video. Thus, we remade the dataset by using the key plaque video clips instead of original long videos. Specifically, sonographers with 7-year experience manually annotated 8/16 frames for a plaque clip and labeled the corresponding stenosis grading (mild/severe) using the Pair annotation software package [13]. The final dataset was split randomly into 318, 23, and 118 plaque clips with 8 frames or into 278, 23, and 109 ones with 16 frames for training, validation, and independent testing set at the patient level with no overlap.In this study, we implemented CSG-3DCT in Pytorch, using an NVIDIA A40 GPU. Unless specified, we trained our model using 8-frame input plaque clips. All frames were resized to 256 × 256. The learnable weights of QKV projection and LayerNorm weights in spatial dimension branch of intra-dimension ST learning module were initialized with those from transformer branch in Conformer [16], while other parameters in AMDF transformer encoder performed random initialization. We trained CSG-3DCT using Adam optimizer with the learning rate (lr ) of 1e-4 and weight decay of 1e-4 for 100 epochs. Batch size was set as 4. Inspired by [18], CSG-3DCT with 16-frame inputs was initialized with 8-frame model and fine-tuned using an initial lr of 0.0025 for 40 epochs. Quantitative and Qualitative Analysis. We conducted extensive experiments to evaluate CSG-3DCT. Accuracy, F1-score, precision and recall were evaluation metrics. Table 1 compares CSG-3DCT with other 8 strong competitors, including 3D CNNs, 3D Trans, and 3D Mix-architecture. Note that ""-Base"" is directly inflated from Conformer [16]. Among all the competitors, -Base achieves the best results on accuracy and f1-score. It can also be observed that our proposed CSG-3DCT achieves state-of-the-art results (at least 4.3% improvement in accuracy).Figure 4 visualizes feature maps of different typical networks using Grad-CAM [17]. We use models without temporal downsampling (i.e., TimeSformer [3] and the ""fast"" branch of SlowFast [10]) to observe attention changes along temporal dimension. Both models ignore capturing equally important local and global ST features simultaneously, resulting in imprecise and coarse attention to the key object, i.e., the plaque area. Compared to both cases, CSG-3DCT can progressively learn the ST contexts in an interactive fashion. As a result, the attention area is more accurate and complete, indicating the stronger discriminative ability of the learned features by CSG-3DCT, which proves the efficacy of our framework.Ablation Study. We performed ablation experiments in the last 6 rows of Table 1. ""-SWA * "" uses SWA in AMDF transformer encoder, while ""-CA * "" uses CA instead. ""-Base-16"" denotes our ""-Base"" model with 16-frame inputs.1) Effects of Different Key Components of Our Model Design. We compared CSG-3DCT with three variants (i.e., -Base, -SWA * , and -CA * ) to analyze the effects of different key components. Compared with -Base, each of our proposed modules and their combination can help improve the accuracy. We adopt CA in our final model for its good performance.2) Effects of Plaque Clip Length. We only investigated the effects of our model on 8-frame and 16-frame input clips due to limited GPU memory. We can find in Table 1 that longer input clips slightly degrade the performance. This is reasonable since the frame-extracting method has been applied in the original videos, causing the covered range of plaque from a longer plaque clip is relatively wider, which is not beneficial to stenosis grading.3) Effectiveness of Initialization with ImageNet. We evaluated the value of training models starting from ImageNet-pretrained weights compared with scratch. It can be seen in Table 1 that model with pretraining significantly boosts +4.2% Acc., demonstrating the efficacy of good initialization."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,4.0,Conclusion,"We propose a novel and effective video classification network for automatic weakly-supervised CSG. To the best of our knowledge, this is the first work to tackle this task. By adopting an inflation strategy, our network can achieve effective warm start and make more accurate predictions. Moreover, we develop a novel AMDF Transformer encoder to enhance the feature discrimination of the video with reduced computational complexity. Experiments on our large inhouse dataset demonstrate the superiority of our method. In the future, we will explore to validate the generalization capability of CSG-3DCT on more large datasets and extend two-grade classification to four-grade of carotid stenosis."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_48.
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,1.0,Introduction,"Despite the success of artificial intelligence (AI) in aiding diagnosis, its application to medical education remains limited. Trainee physicians require several years of experience with a diverse range of clinical cases to develop sufficient skills and expertise. However, designing educational materials solely based on real-world data poses several challenges. For example, although small but significant disease characteristics (e.g., depth of cancer invasion) can sometimes alter diagnosis and treatment, collecting pairs with and without these characteristics is cumbersome. Another major challenge is longitudinal tracking of pathological progression over time (e.g., from the early stage of cancer to the advanced stage), which is difficult to understand because medical images are often snapshots. Privacy is also a concern since images of educational materials are widely distributed. Therefore, medical image editing that allows users to generate their intended disease characteristics is useful for precise medical education [3].Image editing can synthesize low-or high-level image contents [11]. Our goal is to develop high-precision medical image editing according to the fine-grained characteristics of individual diseases, rather than at the level of disease categories. For example, even if two diseases belong to the same disease category of ""lung tumor,"" the impression of benign or malignant will differ depending on fine-grained characteristics, such as whether the margins are ""smooth"" or ""spiculated."" In this case, our approach is to edit the tumor margins to be smooth or spiculated. These fine-grained characteristics consist of low-to mid-level image features to distinguish the substructures of organs and diseases, which we call anatomical elements.Several types of image editing techniques for medical imaging have been introduced, mainly using generative adversarial networks [5] and, more recently, diffusion models [2]. Nevertheless, editing specific anatomical elements remains a challenge [1,11]. Latent space manipulation generates images by controlling latent feature axes [4,14], but the editable attributes are often global rather than fine-grained. Conditional generation can precisely edit image content by using class or segmentation labels. However, it requires manually provided labels [15] or virtual models [18], which are labor-intensive. Additionally, accurately modeling certain fine-grained characteristics, such as the textual variations of disease, can be a daunting task. Image interpolation [17] requires actual images with targeted content, which limits its applicability.Here, we propose a novel framework for image editing called U3-Net that allows the generation of anatomical elements with precise conditions. The core technique is self-supervised segmentation, which aims to achieve pixel-wise clustering without manually annotated labels [6,7]. As shown in Fig. 1a,U3-Net converts an input image into a segmentation map corresponding to the anatomical elements. Once the user has completed editing, U3-Net synthesizes an image in which the targeted anatomical element has been modified. As a result, our synthesized medical images can highlight hypothetical pathological changes and significant clinical differences in a single image. For example, Fig. 1b shows that whether or not rectal cancer invades the muscularis propria (i.e., b-2 vs. b-3) affects cancer staging (i.e., T1 vs. T2) as well as treatment strategy (i.e., endoscopic resection vs. surgery). The distinction between mucinous and nonmucinous rectal cancers (see Fig. 1c) is also important to estimate the better or worse prognosis of the disease. These synthetic images can help trainees intuitively comprehend clinically significant findings and alleviate privacy concerns. Five expert physicians evaluated the edited images from a clinical perspective using two datasets: a pelvic MRI dataset and chest CT dataset."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Contributions: Our contributions are as follows:,"-We propose a novel image-editing algorithm, U3-Net, to synthesize images for medical education via self-supervised segmentation. -U3-Net can faithfully synthesize intended anatomical elements according to the editing operation on the segmentation labels. -Evaluation by five expert physicians showed that the edited images were natural as medical images with the intended features."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.0,Methodology,"U3-Net consists of three neural networks: encoder, decoder, and discriminator (see Fig. 2). The encoder achieves self-supervised segmentation with a feature extraction (FE) module and a pixel-wise clustering (CL) module. We perform pixel-wise clustering under the constraint of invariance to photometric and geometric transformations [6], with the assumption that these transformations should not change the clinical interpretation of the anatomical elements. Given a pair of differently transformed images, the FE module produces embedding maps corresponding to the input images. The CL module then performs Kmeans clustering on the embedding maps to produce two interchangeable outputs: segmentation maps and corresponding quantized embedding maps. These outputs are trained to be consistent between the two views. The decoder then estimates the corresponding images from the quantized embedding maps, while the discriminator forces the decoder to produce more realistic images."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.1,First Training Stage for Self-supervised Segmentation,"The training process for U3-Net is two-stage. First, we train the encoder and decoder (excluding the discriminator) to conduct K-class self-supervised segmentation. To achieve pixel-wise clustering that is consistent between two transformed views of the input images, we introduce four constraints: intra-cluster pull force, inter-cluster push force, cross-view consistency, and reconstruction loss."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Random Image Transformation:,"We consider a sequence of image transformations [t 1 , . . . , t n ] specified by the type (e.g., image rotation) and magnitude (e.g., degree of rotation) of each transformation:  Cluster Assignment and Update: In the CL module, K-means clustering in the first iteration initializes K mean vectors µ k ∈ R D . Then, the embedding vector of the i-th pixel e i∈{1,...,H×W } ∈ R D in the embedding maps, E 1 and E 2 , is assigned to the cluster with the nearest mean vector as follows:, where y i is the cluster index of the i-th pixel. By replacing embedding vectors with their respective mean vectors, quantized embedding maps, E q1 and E q2 , are generated g(E) = E q = [µ y1 , . . . , µ yH×W ] ∈ R D×H×W . The cluster indices form the segmentation maps S = [y 1 , . . . , y H×W ] ∈ R H×W , S 1 and S 2 . The mean vectors µ k are updated by using the exponential moving average [9].Intra-cluster Pull Force: For transformation-invariant pixel-wise clustering, we define four loss terms. The first term, cluster loss, forces the embedding vectors to adhere to the associated mean vector (see Fig. 3), as defined:Inter-cluster Push Force: The second term, distance loss, pushes the distance between the mean vectors above a margin parameter m (see Fig. 3), as defined:where k A and k B indicate two different cluster indices."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Cross-view Consistency:,"The segmentation maps from the different views, S 1 and S 2 , should overlap after re-transforming to align the coordinates. Such a re-transform is composed of inverse and forward geometric transformations:The inverse transformations of the photometric transformations are not considered. Using the re-transformed segmentation maps, we impose a third term, cross-view consistency loss, which forces the embedding vectors of one view to match the mean vector of the other (see Fig. 3), as defined:Reconstruction Loss: Without user editing, the decoder reconstructs the input images from quantized embedding maps h(E q ) = R ∈ R C×H×W . We thus employ reconstruction loss, which minimizes the mean squared error between the reconstructed and input images.Learning Objective: The weighted sum of the loss functions is set to be minimized:"
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.2,Second Training Stage for Faithful Image Synthesis,"In the second stage, we train the decoder and discriminator (excluding the encoder) to produce naturally appearing images from the quantized embedding maps. Learning Objective: We impose generator loss L gen for the decoder to produce more faithful images by deceiving the discriminator, and discriminator loss L dis for the discriminator to judge the real or fake of the images as the perpixel feedback [16]. We also add cutmix augmentation L cutmix and consistency regularization L cons to the latter [16]. In this stage, the decoder and discriminator are trained by alternately minimizing the following competing objectives:"
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.3,Inference Stage for Medical Image Editing,"After training, the encoder can output a segmentation map from a testing image.As shown in Fig. 1a, when a user edits the segmentation map S → S by changing the cluster indices y i → y i , the quantized embedding map is subsequently updated E q → E q by reassigning the mean vectors according to the edited indices µ yi → µ y i . Finally, the decoder converts the quantized embedding map into a synthetic image with the intended disease characteristics h(E q ) = R ∈ R C×H×W . SSIM, and PSNR were 1.41 × 10 -2 ± 1.04 × 10 -2 , 7.40 × 10 -1 ± 0.57 × 10 -1 , and 22.5 ± 2.7 in the pelvic MRI testing dataset and 5.03 × 10 -4 ± 3.03 × 10 -4 , 9.08 × 10 -1 ± 0.34 × 10 -1 , and 38.6 ± 1.7 in the chest CT testing dataset. Subsequently, segmentation maps from the testing images were edited to generate images with the intended characteristics (see Fig. 4cd). Five expert physicians (two diagnostic radiologists, two colorectal surgeons, and one thoracic surgeon) assessed them from a clinical perspective. First, we tested whether the evaluators could identify real or synthesized images from 20 images, which include ten real images and ten synthesized images. The accuracies (i.e., the ratio of images correctly identified as real or synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic MRI and chest CT testing datasets, respectively. Note that when the synthetic images cannot be distinguished at all, the accuracy should be 0.5. Second, we presented image captions explaining the radiological features, which also represented the editing intention for the synthetic images. We asked the evaluators to rate each presented image from A to C. A: The image is natural as a medical image, and the caption is consistent with the image. B: The image is natural as a medical image, but the caption is NOT consistent with the image. C : The image is NOT natural as a medical image. This test was conducted after informing the evaluators of the assumption that all 20 images could be synthetic, without indicating which image was real or synthetic. As a result, the ratio of synthetic images (vs. that of real images) categorized as A, B, and C were 0.80 ± 0.15 (vs. 0.78 ± 0.20), 0.02 ± 0.04 (vs. 0.08 ± 0.07), and 0.18 ± 0.11 (vs. 0.14 ± 0.13) for the pelvic MRI testing dataset, and 0.74 ± 0.28 (vs. 0.76 ± 0.30), 0.08 ± 0.09 (vs. 0.12 ± 0.15), and 0.18 ± 0.21 (vs. 0.12 ± 0.14) for the chest CT testing dataset. There were no significant differences between real and synthetic images (t-test: p > 0.05). Consequently, the majority of the edited images were natural-looking medical images with accurately reproduced disease features."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,4.0,Conclusion,"In this study, we propose a medical image-editing framework to edit fine-grained anatomical elements. The self-supervised segmentation extracted low-to midlevel content of medical images, which corresponded well to the clinically meaningful substructures of organs and diseases. The majority of the edited images with intended characteristics were perceived as natural medical images by several expert physicians. Our medical image editing method can be applied to medical education, which has been overlooked as an application of AI. Future challenges include improving scalability with fewer manual operations, validating segmentation maps from a more objective perspective, and comparing our proposed algorithm with existing methods, such as those based on superpixels [10].Data use declaration and acknowledgment: The pelvic MRI and chest CT datasets were collected from the National Cancer Center Hospital. The study, data use, and data protection procedures were approved by the Ethics Committee of the National Cancer Center, Tokyo, Japan (protocol number 2016-496).Our implementation and all synthesized images will be available here: https:// github.com/Kaz-K/medical-image-editing."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,,"Appearance Loss: Appearance loss combines mean squared loss L mse , focal frequency loss L ffl [8], perceptual loss L lpips [19], and intermediate loss L int , as follows: L app = w mse L mse + w ffl L ffl + w lpips L lpips + w int L int , where intermediate loss L int refers to the L2 distance of the intermediate features of the discriminator between the reconstructed and input images."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_38.
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,3.0,Experiments and Results,"Implementation and Datasets: All neural networks were implemented in Python 3.8 using the PyTorch library 1.10.0 [12] on an NVIDIA Tesla A100 GPU running CUDA 10.2. The encoder, decoder, and discriminator were implemented based on U-Net [13] (see Supplementary Information for details). The pelvic MRI dataset with rectal cancer contained 289 image series for training and 100 image series for testing. For each image series, the min-max normalization converted the pixel values to [-1, 1]. The chest CT dataset with lung cancer contained 500 image series for training and 100 image series for testing. The CT values in the range [-2048, 2048] were normalized to [-1, 1]. Both were in-house datasets collected from a single hospital. Every image series comprises two-dimensional (2D) consecutive slices, and we applied our algorithm on a per 2D slice basis.Self-supervised Medical Image Segmentation: We began by optimizing the hyperparameters to achieve self-supervised segmentation. Appropriate transformations were selected from six candidate functions: t 1 , Random HorizontalFlip, t 2 , RandomAffine, t 3 , ColorJitter, t 4 , RandomGaussianBlur, t 5 , RandomPosterize, t 6 , RandomGaussianNoise. Because anatomical elements, including the substructures of organs and diseases, are too detailed for human annotators to segment, it was difficult to create ground-truth labels. Therefore, the training configuration was selected based on the consensus of two expert radiologists with domain knowledge. By comparing different settings on the pelvic MRI training dataset (see Supplementary Information), the number of segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate magnitude, the weakly imposed reconstruction loss, and a certain value of the margin parameter were considered suitable for self-supervised segmentation. In particular, we found that reconstruction loss is essential for obtaining segmentation maps corresponding to anatomical elements, although such a loss term was not included in previous studies [6,7]. A similar configuration was applied to the chest CT training dataset. The resultant segmentation maps are shown in Fig. 4ab. The anatomical substructures, including the histological structure of the colorectal wall and subregions within the lung, corresponded well with the segmentation maps in both the pelvic MRI and chest CT testing datasets. Because our self-supervised segmentation extracts low-to mid-level image content, a semantic object (e.g., rectum or lung cancer) typically consists of multiple segmentation classes shared with other objects (see the magnified images in Fig. 4ab). These anatomical elements may be too detailed for humans to annotate, demonstrating the necessity of self-supervised segmentation for highprecision medical-image editing."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Evaluation of the Synthesized Images:,"We measured the quality of image reconstruction using mean square error (MSE), structural similarity (SSIM), and peak signal-to-noise ratio (PSNR). The mean ± standard deviations of MSE,"
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,1.0,Introduction,"Multi-organ segmentation is a crucial step in medical image analysis that enables physicians to perform diagnosis, prognosis, and treatment planning. However, manual segmentation of large volume computed tomography (CT) and magnetic resonance (MR) images is time-consuming and prone to high inter-rater variability [30]. In recent years, deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on a wide range of segmentation tasks for natural images [12,16]. However, in the medical domain, there is often a lack of labeled examples to optimally train a deep neural network from scratch. Since unlabeled medical images are comparatively easier to obtain in larger quantities, an alternative strategy is to perform self-supervised learning and generate pre-trained models from unlabeled datasets. Self-supervised learning involves automatically generating a supervisory signal from the data itself and learning a representation by solving a pretext task.In computer vision, current self-supervised learning methods can be broadly divided into discriminative modeling and generative modeling. In earlier times, discriminative self-supervised pretext tasks are designed as rotation prediction [15], jigsaw solving [18], and relative patch location prediction [6], etc. Recently, contrastive learning achieves great success, whose core idea is to attract different augmented views of the same image and repulse augmented views of different images. Based on this, MoCo [11] is proposed, which greatly shrink the gap between self-supervised learning and fully-supervised learning. More advanced techniques have emerged recently [3,9]. Contrastive learning frameworks have also shown promising results in the medical domain, achieving good performance with few labeled examples [1]. Generative modeling also provides a feasible way for self-supervised pre-training [21,35]. Recently, He et al. propose MAE [28] and yield a nontrivial and meaningful generative self-supervisory task, by masking a high proportion of the input image. Transfer learning performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior. In medical image domain, Model Genesis [36] uses a ""painting"" operation to generate a new image by modifying the input image. Several selfsupervised learning approaches have also achieved state-of-the-art performance in the medical domain on both classification and segmentation tasks while significantly reducing annotation cost [1,2,14,20,29,[31][32][33][34] However, most self-supervised pre-training strategies are image [11] or patch [1] level, which are not capable of capturing the detailed feature representations required for accurate medical segmentation. To address this issue, in this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation.Our proposed framework leverages Felzenszwalb's algorithm [8] to formulate local regions and defines a novel contrastive sampling loss to perform localized contrastive learning. Our main contributions include -We propose a standalone localized contrastive learning module that can be integrated into most existing pre-training strategy to boost multi-organ segmentation performance by learning localized feature representations. -We introduce a novel localized contrastive sampling loss for dense selfsupervised pre-training on local regions.-We conduct extensive experiments on three multi-organ segmentation benchmarks and demonstrate that our method consistently outperforms current supervised and unsupervised pre-training approaches."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.0,Methodology,"Figure 1 illustrates our complete framework, which comprises two stages: the contrastive pre-training stage and the fine-tuning stage. Although LRC can be integrated with most of the current popular pre-training strategies, for the purpose of illustration, in this section, we demonstrate how to integrate our LRC module with the classical global (image-level) contrast pre-training strategy MoCo [11], using both its original global contrast and our localized contrastive losses during the contrastive pre-training stage. During the fine-tuning stage, we simply concatenate the local and global contrast models and fine-tune the resulting model on a small target dataset. Further details about each stage are discussed in the following subsections."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.1,Pre-training Stage,"In the pre-training stage, for each batch an image x q is randomly chosen from B images as a query sample, and the rest of the images x n ∈ {x 1 , x 2 , ..., x B } are considered as negative key samples, where n = q. To formulate a positive key sample x p , elastic transforms are performed on the query sample x q .Global Contrast. To explore global contextual information, we train a latent encoder E g following the contrastive protocol in [11]. Three sets of latent embeddings z q , z p , z n are extracted by E g from x q , x p , x n respectively. Using dot product as a measure of similarity, a form of a contrastive loss function called InfoNCE [19] is considered:, where τ g is the global temperature hyper-parameter per [27]. Note that in the global contrast branch, we only pre-train E g ."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,,Local Region,"Contrast. Unlike global contrast, positive and negative pairs for local contrast are only generated from input image x q and its transform x p . We differentiate local regions and formulate the positive and negative pairs by using Felzenszwalb's algorithm. For an input image x, Felzenszwalb's algorithm provides K local regions R = {r 1 , r 2 , .., r K }, where r k is the k-th local region cluster for image x. We then perform elastic transform for both the query image x q and its local regions R q so that we have the augmented image x p = T e (x q ) and its local regions R p = {r 1 p , r 2 p , .., r Kp p }, where r k p = T e (r k q ). Note that K q = K p always holds since R p is a one-to-one mapping from R q . Following the widely used U-Net [22] model design, the query image x q and augmented image x p are then forwarded to a randomly initialized U-Net variant, which includes a convolutional encoder E l and a convolutional decoder D l . We get corresponding feature maps f q and f p with the same spatial dimensions as x q and x p and D channels from the last convolutional layer of D l . Afterwards, we sample N vectors with dimension D from the local region r k q in f q , and formulate the sample mean, where f k,n q is the n-th vector sampled from feature map f q within the k-th local region r k q . Our sampling strategy is straightforward: we sample random points with replacement following a uniform distribution. We simply refer to this as ""random sampling"". Similarly, for feature map f p , its sample mean f k p can be provided following the same random sampling process. Each local region pair of f k q and f k p is considered a positive pair. For the negative pairs, we sample both f q and f p from the rest of the local regions {r 1 , r 2 , ..., r k-1 , r k+1 , ..., r K }. The local contrastive loss can be defined as follows:where τ l is the local temperature hyper-parameter. Compared to the global contrast branch, in local contrastive learning, we pre-train both E l and D l ."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.2,Fine-Tuning Stage,"In the former pre-training stages, E g , E l , and D l are pre-trained with global and local contrast strategy accordingly, with a large number of unlabelled images. In the fine-tuning stage, we fine-tune the model with a limited number of labelled images x f ∈ {x 1 , x 2 , ..., x F }, where F is the size of the fine-tuning dataset.Besides the two pre-trained encoders and one decoder, a randomly initialized decoder D g is appended to the pre-trained E g to ensure that the embeddings have the same dimensions prior to concatenation. We combine local and global contrast models by concatenating the output of D g and D l 's last convolutional layer, and fine-tune on the target dataset in an end-to-end fashion. Different levels of feature maps from encoders are concatenated with corresponding layers of decoders through skip connections to provide alternative paths for the gradient. Dice loss is applied as in usual multi-organ segmentation tasks."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.1,Pre-training Dataset,"During both global and local pre-training stages, we pre-train the encoders on the Abdomen-1K [17] dataset. It contains over one thousand CT images which equates to roughly 240,000 2D slices. The CT images have been curated from 12 medical centers and include multi-phase, multi-vendor, and multi-disease cases.Although segmentation masks for liver, kidney, spleen, and pancreas are provided in this dataset, we ignore these labels during pre-training since we are following the self-supervised protocol."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"During the fine-tuning stage, we perform extensive experiments on three datasets with respect to different regions of the human body. ABD-110 is an abdomen dataset from [25] that contains 110 CT images from patients with various abdominal tumors and these CT images were taken during the treatment planning stage. We report the average DSC on 11 abdominal organs (large bowel, duodenum, spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney, stomach and gallbladder).Thorax-85 is a thorax dataset from [5] that contains 85 thoracic CT images. We report the average DSC on 6 thoracic organs (esophagus, trachea, spinal cord, left lung, right lung, and heart).HaN is from [24] and contains 120 CT images covering the head and neck region. We report the average DSC on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right optical nerve, left parotid, right parotid, left submandibular gland, and right submandibular gland)."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.3,Implementation Details,"All images are re-sampled to have spacing of 2.5 mm × 1.0 mm × 1.0 mm, with respect to the depth, height, and width of the 3D volume. In the pre-training stage, we apply elastic transform to formulate positive samples. In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [13] (for MAE [10], we use ViT-base [7].) encoder E g for 200 epochs. In the local contrast branch, we use the Adam optimizer to pre-train both encoder E l and decoder D l for 30 epochs. The dimension of sampled vectors D is 64 since f q and f p have 64 channels. In the fine-tuning stage, we use the Adam optimizer to train the whole framework in an end-to-end fashion. All optimizers in both pre-training and fine-tuning stages are set to have momentum of 0.9 and weight decay of 10 -4 . "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.4,Quantitative Results,"In Table 1, we select 9 self-supervised pre-trained with 1 ImageNet supervised pre-trained networks and combine with our proposed localized region contrast (LRC). Through extensive experiments on 3 different datasets, we demonstrate LRC is capable of enhancing these pre-training algorithms in a consistent way. We use Sørensen-Dice coefficient (DSC) to measure our experimental results.  "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.5,Qualitative Results,"In Fig. 2, we show segmentation results on ABD-110, Thorax-85, and HaN datasets respectively. All the results are provided by models trained with target dataset size |X T | = 10. By comparing (c) with (g) and (d) with (h), our method shows significant improvement, particularly on the challenging HaN dataset. However, due to limited space, we are only capable of demonstrating selected global pre-training methods.   "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.7,Ablation Study,"Effect of Additional Parameters. Additional parameters do bring performance enhancement in machine learning. However, in Number of Samples N . In Table 3, we explore the effect of different number of samples N to the contrastive sampling loss. When the sample mean f k is only averaged from a small number of vectors, the capability of representing certain region level can be limited. In the opposite, when the number of samples N is large, the sampling bias can be high, since the number of pixels can be smaller than N . Therefore, we need a proper choice of N . With N = 50, our method demonstrates the best DSC score of 0.732. "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a contrastive learning framework, which integrates a novel localized contrastive sampling loss and enables the learning of finegrained representations that are crucial for accurate segmentation of complex structures. Through extensive experiments on three multi-organ segmentation datasets, we demonstrated that our approach consistently boosts current supervised and unsupervised pre-training methods. LRC provides a promising direction for improving the accuracy of medical image segmentation, which is a crucial step in various clinical applications. Overall, we believe that our approach can significantly benefit the medical image analysis community and pave the way for future developments in self-supervised learning for medical applications."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,,Table 1 .,w/ or wo/ LRC w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ Random init 68.8 70.9 76.0 78.0 85.9 87.8 89 89.4 50.9 52.6 77.8 78.0
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,1.0,Introduction,"Parkinson's Disease (PD) is an age-related neurodegenerative disease with complex symptomology that significantly impacts the quality of life, with nearly 90,000 people diagnosed each year in North America [29]. Recent research has shown that gait difficulty and postural impairment symptoms of PD are highly correlated with alterations in various brain networks, including the motor, cerebellar, and cognitive control networks [25]. Understanding brain functional networks associated with an individual's gait impairment severity is essential for developing targeted interventions, such as physical therapy or brain stimulation techniques. However, most prior works have either focused only on a binary diagnosis (PD vs. Control) [16] (ignoring the progression and heterogeneity of the disease symptoms) or only used sensor-and vision-based technologies [8,18] to quantify PD symptoms (abstaining from identifying brain networks associated with gait impairment severity).Graph Neural Networks (GNNs) have been highly successful in inferring neural activity patterns in resting-state fMRI (rs-fMRI) [23]. These models represent functional connectivity matrices as weighted graphs, where each node is a brain region of interest (ROI), and the edges between them capture the magnitude of connectivity, i.e., interactions, as weights. Changes in the connectivity strengths can reflect intrinsic representations in a high-dimensional space that correlate with symptom or disease severity. Assuming that edges with higher weights exert greater functional connectivity (and vice versa), GNNs can encode how ROIs and their neighbors across various individuals can possess similar attributes. GAT [26] is a well-known GNN model that encodes pairwise interactions (edges) into an attention mechanism and uses eigenvectors and eigenvalues of each node as positional embeddings for local structures. However, since each node or ROI in a brain network has the same degree and connects to every other node, standard graph representations are limited in modeling functional connectivity differences in a high-dimensional space that can be used for inter-subject functional covariance comparison. Riemannian geometry [14] is another robust, mathematical framework for rs-fMRI analysis that projects a functional, connectivity matrix in a manifold of symmetric positive-definite (SPD) matrices, making it possible to model high-dimensional, edge interactions and dependencies. It has been applied to analyzing gait patterns [20] in Parkinson's disease and to functional brain network analysis in other neurological disorders (e.g., Mild Cognitive Impairment [7] and autism [30]).Addressing the problem of identifying brain functional network alterations related to the severity of gait impairments presents several challenges: (i) clinical datasets are often sparse or highly imbalanced, especially for severely impaired disease states; (ii) although substantial progress has been made in modeling functional connectomes using graph theory, few studies exist that capture the individual variability in disease progression and they often fall short of generating clinically relevant explanations that are symptom-specific.In this work, we propose a novel, explainable, geometric weighted-graph attention network (xGW-GAT) that embeds functional connectomes in a learnable, graph structure that encodes discriminative edge attributes used for attention-based, transductive classification tasks. We train the model to predict a gait impairment rating score (MDS-UPDRS Part 3.10) for each PD participant. To mitigate limited clinical data across all different classes of gait impairment and data imbalance (challenge i), we propose a stratified, learning-based sample selection method that leverages non-Euclidean, centrality features of connectomes to sub-select training samples with the highest predictive power. To provide clinical interpretability (challenge ii), xGW-GAT innovatively produces individual and global attention-based, explanation masks per gait category and soft assigns nodes to functional, resting-state brain networks. We apply the proposed framework on our dataset of 35 clinical participants and compare it with existing methods. We observe significant improvements in classification accuracy while enabling adequate clinical interpretability.In summary, our contributions are: (1) we propose a novel, geometric attention-based model, xGW-GAT, that uses edge-weights to depict neighborhood influence from local node embeddings during dynamic, attention-based learning; (2) we develop a multi-classification pipeline that mitigates sparse and imbalanced sampling with stratified, learning-based sample selection during training on real-world clinical datasets; (3) we provide an explanation generator to interpret attention-based, edge explanations that highlight salient brain network interactions for gait impairment severity states; (4) we establish a new benchmark for PD gait impairment assessment using brain functional connectivities."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Problem definition.,"Assume a set of functional connectomes, G n ∈ R d×d , G 2 , . . . , G N are given, where N is the number of samples and d is the number of ROIs. Each connectome is represented by a weighted, undirected graph G = (V, E, W), where V = {v i } d i=1 is the set of nodes, E ⊆ V × V is the edge set, and W ∈ R |V|×|V| denotes the matrix of edge weights. The weight w ij of an edge e ij ∈ E represents the strength of the functional connection between nodes v i and v j , i.e., the Pearson correlation coefficient of the time series of the pair of the nodes. Each G n contains node attributes X n and edge attributes H n . We develop a model that predicts a gait impairment score, Y n and outputs an individual explanation mask M c ∈ R d×d per class c to assign ROIs to functional brain networks."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.1,Connectomes in a Riemannian Manifold,"Functional connectivity matrices belong to the manifold of symmetric positivedefinite (SPD) matrices [31]. We leverage Riemannian geometry to perform principled comparisons between different connectomes, such as prior work [24]. To highlight connections between adjacent nodes, each weight matrix W n ∈ R d×d can be represented as a symmetric, adjacency matrix with zero, non-negative eigenvalues, where each element of the adjacency matrix is the edge weight, w ij between nodes i and j. We then consider W n to be a point, S n , in the manifold of SPD matrices Sym + d that locally looks like a topological Euclidean space. However, Sym + d does not form a vector space; thus, we project each SPD matrix S n onto a common tangent space using parallel transport. Given a reference point S i ∈ Sym + d , we transport a tangent vector v ∈ T Sj from S j to S i along the geodesic connecting S j and S i (see Fig. 1-B). This process is performed for each subject n = 1, 2, . . . , N, yielding a set of tangent vectors in a common tangent space that can be analyzed using traditional Euclidean methods.To calculate the geodesic distance between two SPD matrices S i and S j ∈ Sym + d , we adopt the Log-Euclidean Riemannian Metric (LERM) [1] distance, d LERM as follows:where • 2 F is the Frobenius norm. LERM is invariant to similarity transformations (scaling and orthogonality) and is computationally efficient for highdimensional data. See the Supplementary Material for results with other distance metrics."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.2,Stratified Learning-Based Sample Selection,"Data availability and dataset imbalance are re-occurring challenges with realworld clinical datasets, often leading to bias and overfitting during model train-ing. We address this by expanding a learning-based sample selection method [11] to weight per-class distributions. We assume that similar brain connectivity networks are correlated with disease severity whereas connectomes that vary in topological patterns might elicit different gait impairment scores. Our subsampling technique selects training samples containing the highest representative power, i.e., contributing the least amount of pairwise differences for predicting a gait score. We divide our training samples into subgroups: train-in, n s , and holdout, n t using N -fold cross-validation. For each pair of symmetric d-by-d tangent matrices, S s,s i,j ∈ T I SPD(d), we encode the pairwise differences between the connectomes from the train-in, n s , to obtain a set of n s (n s -1)/2 tangent matrices in T I SPD(d). Each tangent matrix represents the ""difference"" between two connectomes. We affix a threshold of k samples to be selected from each class c to identify l central training samples with the highest expected predictive power, i.e., the lowest average difference in target gait impairment scores per class, ŷc , between samples j from the train-in and holdout group. We select degree, closeness, and eigenvector centrality as our topological features that encode information on changes in node connectivity. We train a linear regression mapping f on the Riemannian geometric distances D le (S s i , S s j ) between the connectomes from n s using the vectorized upper triangular portion (including the diagonal) of the tangent matrices. The absolute difference in target score, per class, between samples i and j from the train-in group n s is denoted by |ŷ s c,jŷs c,i | (see Fig. 1-C). The top-k samples per class with the highest predictive power are sub-selected from the total training set, oversampled for class imbalance with RandomOver-Sampler [15], and used for training xGW-GAT layers (see Fig. 1-D)."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.3,Dynamic Graph Attention Layers,"Attention. We employ Graph Attention Network version 2 (GATv2) [2], a GAT [26] variant to perform dynamic, multi-head, edge-weight attention message passing for classifying each S n . We assume that every node i ∈ V has an initial representation h (0) i ∈ R d0 . GATv2 updates each node representation, h based on the features of neighboring nodes and the edge weights between nodes by computing attention scores α ij for every edge (i, j) by normalizing attention coefficients e(h i , h j ). α ij measures the importance of node j's features to node i's at layer l by performing a weighted sum over the neighboring nodes j ∈ N i :where a (l) ∈ R 2F and Θ (l) are trainable parameters and learned, hi ∈ R F is the embedding for node i, σ represents a non-linearity activation function, and denotes vector concatenation. As conventional graph attention mechanisms for transductive tasks typically do not incorporate edge attributes, we introduce an attention-based, message-passing mechanism incorporating edge weights, similar to [6]. The algorithm uses a message vector m ij ∈ R F by concatenating node features of neighboring nodes i, j, and edge weight W i,j :where MLP 1 is a Multi-Layer Perceptron. Accordingly, an update of each ROI representation is influenced by its neighboring regions weighted by their connectivity strength. After stacking L layers, a readout function summarizing all node embeddings is employed to obtain a graph-level embedding g:Loss Function. xGW-GAT layers (Fig. 1-F) are trained with a supervised, weighted negative log-likelihood loss function to mitigate class imbalance across classes, C, defined as:where r q is the rescaling weight for the q-th class, y pq is the q-th element of the true label vector y p for the p-th sample, and ŷpq is the predicted label vector."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.4,Individual-And Global-Level Explanations,"We define an attention explanation mask for each sample, n ∈ 1, 2, . . . , N and for each class c ∈ 1, 2, . . . , C that identifies the most important node/ROI connections contributing to the classification of subjects. We return a set of attention coefficients α n = [α n 1 , α n 2 , . . . , α n S ] for each sample n, where S is the number of attention heads. We aggregate trained, attention coefficients per sample used for predicting each ŷ using a max operation that returns α n max ∈ R d×d . An explanation mask per class, M c , or per sample, M n , can be derived using the max attention coefficients, α max (Fig. 1-G):M can be soft-thresholded to retain the top-L most positively attributed attention weights to the mask as follows:where Top-L(M) represents the set of top-L elements in M."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,3.0,Experiments,"Dataset. We obtained data from a private dataset (n = 35, mean age 69 ± 7.9) defined in [19], which contains MDS-UPDRS exams from all participants. Following previously published protocols [21], all participants are recorded during the off-medication state. Participants were evaluated by a board-certified movement disorders specialist on a scale from 0 to 4 based on MDS-UPDRS Sect. 3.10 [10]. The dataset includes 22 participants with a score 1, 4 participants with a score 2, 4 participants with a score of 3, 4 participants with a score 4, and 1 participant with a score 0 on MDS-UPDRS item 3.10. The single score-0 participant (normal) was combined with the score-1 participants (minor gait impairment) to adjust for severe class imbalance. We pre-processed functional connectivity matrices and corrected them for possible motion artifacts using the CONN toolbox [28]. The FC matrices were obtained using a combined Harvard-Oxford and AAL parcellation atlas [28] with 165 ROIs, where each entry in row i and column j in the matrix is the Pearson correlation between the average rs-fMRI signal measured in ROI i and ROI j. We imputed any missing ROI network scores with the mean score per column and Z-transformed FC matrices [μ = 0, σ = 1]. This dataset (like other clinical datasets in practice) poses highly imbalanced distributions for classes with severe impairment, which makes it useful to demonstrate our method's capability in an imbalanced and limited-data scenario. In addition, most existing studies focus on differentiating participants from controls, while the severity of specific impairments is understudied (our focus)."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Software.,"All experiments were implemented in Python 3.10 and ran on Nvidia A100 GPU runtimes. We used PyTorch Geometric [9], PyTorch, and Scikit-learn for machine learning methods. We used the SPD class from the Morphometrics package to compute Riemannian geometrics and NetworkX to extract the topological features of the graphs from the tangent matrices. Hyper-parameters are tuned automatically with the open-source AutoML toolkit NNI (https://github. com/microsoft/nni).Setup. We used the mean, connectivity profile, i.e., W [5], as the node feature for xGW-GAT layers, a weighted ADAM optimizer, a learning rate of 1e -4, a batch size of 2 for training and 1 for test, and 100 training epochs. We used 2 GATv2 layers, dropout rate = 0.1, hidden dim = 8, heads= 2, and a global mean pooling layer. We used 4-fold cross-validation to partition training and holdout sets and selected k = 4 as the optimal number of selected training samples between 2 and 15. We report weighted, macro average scores for F 1 , area under the ROC curve (AUC), precision (Pre), and recall (Rec) over 100 trials."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,3.1,Results,"We perform a multi-class classification task of Slight (1), Mild (2), Moderate (3), Severe(4) gait impairment severity. To benchmark our method, we compare our "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Method,"Pre Rec F1 AUC GCN* [13] 0.46 0.48 0.47 0.54 PNA* [4] 0.52 0.54 0.53 0.56 BrainNetCNN* [12] 0.62 0.71 0.66 0.57 BrainGNN* [17] 0.66 0.53 0.59 0.62 GAT* [26] 0 results with several state-of-the-art classifiers: GAT [26], GCN [13], PNA [4], and two state-of-the-art deep models design for brain networks: BrainNetCNN [12] and BrainGNN [17]. We also perform an ablation study on sample selection and the type of topological features used in training our method: (!ss) no [stratified, learning-based] sample selection, (dc) node degree centrality, (cc) node closeness centrality, and (ec) eigenvector centrality. Results for the highest-performing settings of xGW-GAT are displayed in Table 1 and node feature descriptions are included in the Supplementary Material.The results (Table 1) show that xGW-GAT yields significant improvement in performance over SOTA graph-based models, including models designed for brain network analysis. xGW-GAT with our stratified, learning-based selection method combined with the RandomOverSampler technique to temper the effects of class imbalance outperforms a standard xGW-GAT by 42%. Compared with SOTA deep models like GCN and PNA, our model also outperforms them by large margins, with up to 29% improvement for AUC. These predictions are promising for an explainable analysis of PD gait impairment while also minimizing random uncertainties introduced in individual participant graphs."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,4.0,Discussion,"Brain Networks Mapping. As shown in Fig. 2, we aid interpretability for clinical relevance by partitioning the ROIs into nine ""networks"" based on their functional roles: Default Mode Network (DMN), SensoriMotor Network (SMN), Visual Network (VN), Salience Network (SN), Dorsal Attention Network (DAN), FrontoParietal Network (FPN), Language Network (LN), Cerebellar Network (CN), and Bilateral Limbic Network (BLN) are colored accordingly, while edges across different systems are colored gray. Edge widths here are the attention weights. Salient ROIs. We provide per-class and individual-level interpretations for understanding how ROIs contribute to predicting gait impairment scores. We build the node and edge files with the thresholded attention explanation masks, M per PD participant or per class and plot glass brains using BrainNet Viewer ((https://www.nitrc.org/projects/bnv/)).We observe that rich interactions decrease significantly for the Mild class, Fig. 2(b), within the CN, primarily associated with coordinating voluntary movement, the SN, responsible for thought, cognition, and planning behavior, and the VN, the center for visual processing during resting and task states. These observations are consistent with existing neuroimaging findings, which support that PD is positively associated with the severity of cognitive deficits and neuromotor control for inter-network and intra-network interactions within the salience network, cerebellar lobules, and visual network [22,32]. Similarly, there are significantly lower connections within CN and VN and sparser connections within the SMN for the Moderate and Severe classes, Fig. 2(c)-(d). Existing studies show functional connectivity losses within the sensorimotor network (SMN) [3] are correlated with disruptions in regional and global topological organization for SMN areas for people with PD, resulting in loss of motor control. For Mild, Moderate, and Severe PD participants, abrupt connectivity is also observed for the frontoparietal network, FPN, known for coordinating behavior and associated with connectivity alterations correlated with motor deterioration [27]."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,5.0,Conclusion,"This study showcases a novel benchmark for using an explainable, geometric-weighted graph attention network to discover patterns associated with gait impairment. The framework innovatively integrates edge-weighted attention encoding and explanations to represent neighborhood interactions in functional brain connectomes, providing interpretable functional network clustering for neurological analysis. Despite a small sample size and imbalanced settings, the lightweight model offers stable results for quick inference on categorical PD neuromotor states. Future work includes new experiments, an expanded, multi-modal dataset, and sensitivity and specificity analysis to discover subtypes associated with the severity of PD gait impairment."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Table 1 .,F. Nerrise et al.
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Acknowledgements,". This work was partially supported by NIH grants (AA010723, NS115114, P30AG066515), Stanford School of Medicine Department of Psychiatry and Behavioral Sciences Jaswa Innovator Award, UST (a Stanford AI Lab alliance member), and the Stanford Institute for Human-Centered AI (HAI) Google Cloud credits. FN is funded by the Stanford Graduate Fellowship and the Stanford NeuroTech Training Program Fellowship."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 68.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1.0,Introduction,"Segmentation is among the most common medical image analysis tasks and is critical to a wide variety of clinical applications. To date, data-driven deep learning (DL) methods have shown prominent segmentation performance when trained on fully-annotated datasets [8]. However, data annotation is a significant bottleneck for dataset creation. First, annotation process is tedious, laborious and time-consuming, especially for 3D medical images where dense annotation with voxel-level accuracy is required. Second, medical images typically need to be annotated by medical experts whose time is limited and expensive, making the annotations even more difficult and costly to obtain. Active learning (AL) is a promising solution to improve annotation efficiency by iteratively selecting the most important data to annotate with the goal of reducing the total number of annotated samples required. However, most deep AL methods require an initial set of labeled samples to start the active selection. When the entire data pool is unlabeled, which samples should one select as the initial set? This problem is known as cold-start active learning , a low-budget paradigm of AL that permits only one chance to request annotations from experts without access to any previously annotated data.Cold-start AL is highly relevant to many practical scenarios. First, cold-start AL aims to study the general question of constructing a training set for an organ that has not been labeled in public datasets. This is a very common scenario (whenever a dataset is collected for a new application), especially when iterative AL is not an option. Second, even if iterative AL is possible, a better initial set has been found to lead to noticeable improvement for the subsequent AL cycles [4,25]. Third, in low-budget scenarios, cold-start AL can achieve one-shot selection of the most informative data without several cycles of annotation. This can lead to an appealing 'less is more' outcome by optimizing the available budget and also alleviating the issue of having human experts on standby for traditional iterative AL.Despite its importance, very little effort has been made to address the coldstart problem, especially in medical imaging settings. The existing cold-start AL techniques are mainly based on the two principles of the traditional AL strategies: (1) Uncertainty sampling [5,11,15,18], where the most uncertain samples are selected to maximize the added value of the new annotations. (2) Diversity sampling [7,10,19,22], where samples from diverse regions of the data distribution are selected to avoid redundancy. In the medical domain, diversity-based cold-start strategies have been recently explored on 2D classification/segmentation tasks [4,24,25]. The effectiveness of these approaches on 3D medical image segmentation remains unknown, especially since 3D models are often patch-based while 2D models can use the entire image. A recent study on 3D medical segmentation shows the feasibility to use the uncertainty estimated from a proxy task to rank the importance of the unlabeled data in the cold-start scenario [14]. However, it fails to compare against the diversity-based approaches, and the proposed proxy task is only limited to CT images, making the effectiveness of this strategy unclear on other 3D imaging modalities. Consequently, no comprehensive cold-start AL baselines currently exist for 3D medical image segmentation, creating additional challenges for this promising research direction.In this paper, we introduce the COLosSAL benchmark, the first cold-start active learning benchmark for 3D medical image segmentation by evaluating on six popular cold-start AL strategies. Specifically, we aim to answer three important open questions: (1) compared to random selection, how effective are the uncertainty-based and diversity-based cold-start strategies for 3D segmentation tasks? (2) what is the impact of allowing a larger budget on the compared strategies? (3) can these strategies work better if the local ROI of the target organ is known as prior? We train and validate our models on five 3D medical image segmentation tasks from the publicly available Medical Segmentation Decathlon (MSD) dataset [1], which covers two of the most common 3D image modalities and the segmentation tasks for both healthy tissue and tumor/pathology.Our contributions are summarized as follows:• We offer the first cold-start AL benchmark for 3D medical image segmentation. We make our code repository, data partitions, and baseline results publicly available to facilitate future cold-start AL research. • We explore the impact of the budget and the extent of the 3D ROI on the cold-start AL strategies. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.0,COLosSAL Benchmark Definition,"Formally, given an unlabeled data pool of size N , cold-start AL aims to select the optimal m samples (m ≪ N ) without access to any prior segmentation labels. Specifically, the optimal samples are defined as the subset of 3D volumes that can lead to the best validation performance when training a standard 3D segmentation network. In this study, we use m = 5 for low-budget scenarios."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,3D Medical Image Datasets,"We use the Medical Segmentation Decathlon (MSD) collection [1] to define our benchmark, due to its public accessibility and the standardized datasets spanning across two common 3D image modalities, i.e., CT and MRI. We select five tasks from the collection appropriate for the 3D segmentation tasks, namely tasks 2-Heart, 3-Liver, 4-Hippocampus, 7-Pancreas, and 9-Spleen. Liver and Pancreas tasks include both organ and tumor segmentation, while the other tasks focus on organs only.  "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Cold-Start AL Scenarios,"In this study, we investigate the cold-start AL strategies for 3D segmentation tasks in three scenarios, as illustrated in Fig. 1.1. With a low budget of 5 volumes (except for Heart, where 3 volumes are used because of the smaller dataset and easier segmentation task), we assess the performance of the uncertainty-based and diversity-based approaches against the random selection. 2. Next, we explore the impact of budgets for different cold-start AL schemes by allowing a higher budget, as previous work shows inconsistent effectiveness of AL schemes in different budget regimes [7]. 3. Finally, we explore whether the cold-start AL strategies can benefit from using the uncertainty/diversity from only the local ROI of the target organ, rather than the entire volume. This strategy may be helpful for 3D tasks especially for small organs, whose uncertainty/diversity can be outweighted by the irrelevant structures in the entire volume, but needs to be validated.Evaluation Metrics. To evaluate the segmentation performance, we use the Dice similarity coefficient and 95% Hausdorff distance (HD95), which measures the overlap between the segmentation result and ground truth, and the quality of segmentation boundaries by computing the 95 th percentile of the distances between the segmentation and the ground truth boundary points, respectively."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.3,Baseline Cold-Start Active Learners,"We provide the implementation for the baseline approaches: random selection, two variants of an uncertainty-based approach named ProxyRank [14], and three diversity-based methods, namely ALPS [22], CALR [10], and TypiClust [7].Random Selection. As suggested by prior works [3,4,7,12,20,26], random selection is a strong competitor in the cold-start setting, since it is independent and identically distributed (i.i.d.) to the entire data pool. We shuffle the entire training list with a random seed and select the first m samples. In our experiments, random selection is conducted 15 times and the mean Dice score is reported.Uncertainty-Based Selection. Many traditional AL methods use uncertainty sampling, where the most uncertain samples are selected using the uncertainty of the network trained on an initial labeled set. Without such an initial labeled set, it is not straightforward to capture uncertainty in the cold-start setting.Recently, Nath et al. [14] proposed a proxy task and then utilized uncertainty generated from the proxy task to rank the unlabeled data. By selecting the most uncertain samples, this strategy has shown superior performance to random selection. Specifically, pseudo labels were generated by thresholding the CT images with an organ-dependent Hounsfield Unit (HU) intensity window. These pseudo labels carry coarse information for the target organ, though they also include other unrelated structures. The uncertainty generated by this proxy task is assumed to represent the uncertainty of the actual segmentation task.However, this approach [14] was limited to CT images. Here, we extend this strategy to MR images. For each MR image, we apply a sequence of transformations to convert it to a noisy binary mask: (1) z-score normalization, (2) intensity clipping to the [1 st , 99 th ] percentile of the intensity values, (3) intensity normalization to [0, 1] and (4) Otsu thresholding [16]. We visually verify that the binary pseudo label includes the coarse boundary of the target organ.As in [14], we compute the model uncertainty for each unlabeled data using Monte Carlo dropout [6]: with dropout enabled during inference, multiple predictions are generated with stochastic dropout configurations. Entropy [13] and Variance [21] are used as uncertainty measures to create two variants of this proxy ranking method, denoted as ProxyRank-Ent and ProxyRank-Var. The overall uncertainty score of an unlabeled image is computed as the mean across all voxels. Finally, we rank all unlabeled data with the overall uncertainty scores and select the most uncertain m samples.Diversity-Based Selection. Unlike uncertainty-based methods which require a warm start, diversity-based methods can be used in the cold-start setting. Generally, diversity-based approaches consist of two stages. First, a feature extraction network is trained using unsupervised/self-supervised tasks to represent each unlabeled data as a latent feature. Second, clustering algorithms are used to select the most diverse samples in latent space to reduce data redundancy. The major challenge of benchmarking the diversity-based methods for 3D tasks is to have a feature extraction network for 3D volumes. To address this issue, we train a 3D auto-encoder on the unlabeled training data using a self-supervised task, i.e., image reconstruction. Specifically, we represent each unlabeled 3D volume as a latent feature by extracting the bottleneck feature maps, followed by an adaptive average pooling for dimension reduction [24]. Afterwards, we adapt the diversity-based approaches to our 3D tasks by using the same clustering strategies as proposed in the original works, but replacing the feature extraction network with our 3D version. In our benchmark, we evaluate the clustering strategies from three state-of-the-art diversity-based methods. 1. ALPS [22]: k -MEANS is used to cluster the latent features with the number of clusters equal to the query number m. For each cluster, the sample that is the closest to the cluster center is selected. 2. CALR [10]: This approach is based on the maximum density sampling, where the sample with the most information is considered the one that can optimally represent the distribution of a cluster. A bottom-up hierarchical clustering algorithm termed BIRCH [23] is used and the number of clusters is set as the query number m. For each cluster, the information density for each sample within the cluster is computed and the sample with the highest information density is selected. The information density is expressed as, where X c = {x 1 , x 2 , ...x j } is the feature set in a cluster and cosine similarity is used as sim(⋅). 3. TypiClust [7]: This approach also uses the points density in each cluster to select a diverse set of typical examples. k -MEANS clustering is used, followed by selecting the most typical data from each cluster, which is similar to the ALPS strategy but less sensitive to outliers. The typicality is calculated as the inverse of the average Euclidean distance of x to its K nearest neighbors KNN(x), expressed as:. K is set as 20 in the original paper but that is too high for our application. Instead, we use all the samples from the same cluster to calculate typicality."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.4,Implementation Details,"In our benchmark, we use the 3D U-Net as the network architecture. For uncertainty estimation, 20 Monte Carlo simulations are used with a dropout rate of 0.2. As in [14], a dropout layer is added at the end of every level of the U-Net for both encoder and decoder. The performance of different AL strategies is evaluated by training a 3D patch-based segmentation network using the selected data, which is an important distinction from the earlier 2D variants in the literature. The only difference between different experiments is the selected data. For CT pre-processing, image intensity is clipped to [-1024, 1024] HU and rescaled to [0, 1]. For MRI pre-processing, we sequentially apply z-score normalization, intensity clipping to [1 st , 99 th ] percentile and rescaling to [0, 1]. During training, we randomly crop a 3D patch with a patch size of 128 × 128 × 128 (except for hippocampus, where we use 32 × 32 × 32) with the center voxel of the patch being foreground and background at a ratio of 2 ∶ 1. Stochastic gradient descent algorithm with a Nesterov momentum (µ = 0.99) is used as the optimizer and L DiceCE is used as the segmentation loss. An initial learning rate is set as 0.01 and decayed with a polynomial policy as in [9]. For each experiment, we train our model using 30k iterations and validate the performance every 200 iterations.A variety of augmentation techniques as in [9] are applied to achieve optimal performance for all compared methods. All the networks are implemented in PyTorch [17] and MONAI [2]. Our experiments are conducted with the deterministic training mode in MONAI with a fixed random seed=0. We use a 24G NVIDIA GeForce RTX 3090 GPU.For the global vs. local experiments, the local ROIs are created by extracting the 3D bounding box from the ground truth mask and expanding it by five voxels along each direction. We note that although no ground truth masks are accessible in the cold-start AL setting, this analysis is still valuable to determine the usefulness of local ROIs. It is only worth exploring automatic generation of these local ROIs if the gold-standard ROIs show promising results."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.0,Experimental Results,"Impact of Selection Strategies. In Fig. 2, with a fixed budget of 5 samples (except for Heart, where 3 samples are used), we compare the uncertainty-based and diversity-based strategies against the random selection on five different segmentation tasks. Note that the selections made by each of our evaluated AL strategies are deterministic. For random selection, we visualize the individual Dice scores (red dots) of all 15 runs as well as their mean (dashed line). HD95 results (Supp. Tab. 1) follow the same trends.Our results explain why random selection remains a strong competitor for 3D segmentation tasks in cold-start scenarios, as no strategy evaluated in our benchmark consistently outperforms the random selection average performance.However, we observe that TypiClust (shown as orange) achieves comparable or superior performance compared to random selection across all tasks in our benchmark, whereas other approaches can significantly under-perform on certain tasks, especially challenging ones like the liver dataset. Hence, Typi-Clust stands out as a more robust cold-start selection strategy, which can achieve at least a comparable (sometimes better) performance against the mean of random selection. We further note that TypiClust largely mitigates the risk of 'unlucky' random selection as it consistently performs better than the low-performing random samples (red dots below the dashed line).Impact of Different Budgets. In Fig. 3(a), we compare AL strategies under the budgets of m = 5 vs. m = 10 (3 vs. 5 for Hearts). We visualize the performance under each budget using a heatmap, where each element in the matrix is the difference of Dice scores between the evaluated strategy and the mean of random selection under that budget. A positive value (warm color) means that the AL strategy is more effective than random selection. We observe an increasing amount of warm elements in the higher-budget regime, indicating that most cold-start AL strategies become more effective when more budget is allowed. This is especially true for the diversity-based strategies (three bottom rows), suggesting that when a slightly higher budget is available, the diversity of the selected samples is important. HD95 results (Supp. Tab. 1) are similar.Impact of Different ROIs. In Fig. 3(b), with a fixed budget of m = 5 volumes, we compare the AL strategies when uncertainty/diversity is extracted from the entire volume (global) vs. a local ROI (local). Each element in this heatmap is the Dice difference of the AL strategy between global and local; warm color means global is better than local. The hippocampus images in MSD are already cropped to the ROI, and thus are excluded from this comparison. We observe different trends across different methods and tasks. Overall, we can observe more warm elements in the heatmap, indicating that using only the local uncertainty or diversity for cold-start AL cannot consistently outperform the global counterparts, even with ideal ROI generated from ground truth. HD95 results (Supp. Tab. 2) follow the same trends.Limitations. For the segmentation tasks that include tumors ( 4th and 5th columns on Fig. 3(a)), we find that almost no AL strategy is very effective, especially the uncertainty-based approaches. The uncertainty-based methods heavily rely on the uncertainty estimated by the network trained on the proxy tasks, which likely makes the uncertainty of tumors difficult to capture. It may be nec-essary to allocate more budget or design better proxy tasks to make cold-start AL methods effective for such challenging tasks. Lastly, empirical exploration of cold-start AL on iterative AL is beyond the scope of this study and merits its own dedicated study in future."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.0,Conclusion,"In this paper, we presented the COLosSAL benchmark for cold-start AL strategies on 3D medical image segmentation using the public MSD dataset. Comprehensive experiments were performed to answer three important open questions for cold-start AL. While cold-start AL remains an unsolved problem for 3D segmentation, important trends emerge from our results; for example, diversitybased strategies tend to benefit more from a larger budget. Among the compared methods, TypiClust [7] stands out as the most robust option for cold-start AL in medical image segmentation tasks. We believe our findings and the open-source benchmark will facilitate future cold-start AL studies, such as the exploration of different uncertainty estimation/feature extraction methods and evaluation on multi-modality datasets."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,1.0,Introduction,"The demand for precise medical data analysis has led to the widespread use of deep learning methods in the medical field. However, accompanied by the promulgation of data acts and the strengthening of data privacy, it has become increasingly challenging to train models in large-scale centralized medical datasets. As one of the solutions, federated learning provides a new way out of the dilemma and attracts significant attention from researchers.Federated learning (FL) [1,2] is a distributed machine learning paradigm in which all clients train a global model collaboratively while preserving their data locally. As a crucial core of them, the aggregation algorithm plays an important role in releasing data potential and improving global model performance. FedAvg [1], as pioneering work, was a simple and effective aggregation algorithm, which makes the proportions of local datasets size as the aggregation weights of local models. But in the real world, not only the numbers of datasets held by clients is different, but also their data distribution may be diverse, which leads to the fact that the data in the federated learning is non-Independent Identically Distribution (non-IID). The naive aggregation algorithms maybe have worse performance because of the non-IID data [3][4][5][6][7][8]. In medical image segmentation, [9] and [10] took the lead in discussing the application and safety of federated learning in brain tumor segmentation (BraTS). To solve the non-IID challenges of FL in the medical image field, FedDG [11] and FedMRCM [12] were proposed to address the domain shift issue between the source domain and the target domain, but the sharing of latent features may cause privacy concerns. Auto-FedRL [13] and Auto-FedAvg [14] were proposed to deal with the non-IID problem by using an optimization algorithm to learn super parameters and aggregate weights. IDA [15] introduced the Inverse Distance of local models and the average model of all clients to handle non-IID data. The work [16][17][18][19] proposed corresponding aggregation methods from the perspectives of clustering, frequency domain, Bayesian, and representation similarity analysis. More than this, the first computational competition on federated learning, Federated Tumor Segmentation (FeTS) Challenge1  [20] was held to measure the performance of different aggregation algorithms on glioma segmentation [21][22][23][24]. Leon et al. [25] proposed FedCostWAvg get a notable improvement compared to FedAvg by including the cost function decreased during the last round and won the challenge. However, most of these methods improve the performance by adding other regular terms to the aggregation method, without considering all factors as a whole, which may limit the performance of the global model. Different from the above methods, inspired by the concept of the law of universal gravitation in physics, in this paper, we propose a novel aggregation strategy, FedGrav, which unifies the differences in sample size and the discrepancies of local models among clients by defining the concept of model affinity. Specifically, we take the client sample size as the mass of the local model, and the discrepancies among the local models as their distance, which is quantified from the topological perspective of neural networks. Last, the formula 1 is employed to calculate the affinity and explore the internal correlation between the local models. The proposed method promotes a more effective aggregation of local models by unifying the difference between sample size and local model between clients.The primary contributions of this paper can be summarized as: (1) We propose FedGrav, a novel aggregation strategy that unifies the difference both in sample size and local model among clients by defining the concept of model affinity; (2) We propose Model Graph Distance, a new method to quantify model differences from the perspective of neural network topology. (3) We propose an aggregation algorithm that introduces the concept of affinity and graph into federated learning, and the aggregation weights can be adjusted adaptively; (4) The superior performance is achieved by the proposed method, on the public CIFAR-10 and FeTS challenge datasets."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,2.1,Overview,"Suppose K clients with private data cooperate to train a global model and share the same neural network structure, 3D-Unet [26], which is provided by the FeTS challenge and kept unchanged. For the clients, every client trains a local model w i for local E epochs and then delivers the local model to the server. The server aggregates local models to a global model by computing the aggregation weights with the proposed FedGrav and assigns it to all clients. Specifically, given K local models, we first make graph mapping to map the network model to the topology graph, and then the graph distance is obtained after the graph pruning and comparison. For the model affinity computation, FedGrav takes the sample size of every client as the mass of the local model and combines the given graph distance to calculate the affinity between models according to the formula 1. After that, a symmetric Model Affinity Matrix A ∈ R K×K is analyzed to compute aggregation weights. Last, The server aggregates local models to a global model according to the aggregation weights and assigns it to all clients. Repeat and until T rounds or other limits. An overview of the method is shown in Fig. 1."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,2.2,FedGrav,"Model Affinity. Inspired by the law of universal gravitation, we assume that there is similar gravitation between any two local models. We define it as model affinity in federated learning. It can be described that the affinity between two local models is proportional to the sample size of the client corresponding to the local model, and inversely proportional to the distance between two models. The equation for model affinity takes the form:where A ik is the affinity between i-th and k-th local models, n i and n k are the sample size of i-th and k-th client, and d ik is the distance between two local models, which is quantified from the perspective of neural network topology and will be described in the following section. M is the affinity constant, it can be simplified in the subsequent analysis, so this paper will not set specific values for it. The model affinity depicts the internal correlation between two local models, which lays the foundation for accurate aggregation weights.Graph Distance. The distance is defined to quantify model differences. The differences in local models reflect the discrepancies in the distribution of client data to a certain extent. If the differences in local models can be accurately measured, the more appropriate aggregation weights will be assigned to local models to aggregate a better global model. The key motivation is to measure the internal correlations of local models as accurately as possible. We explore the model distance from the perspective of neural network topology in this paper and define it as model graph distance. In FedGrav, the computation of graph distance goes through the following steps:(1) Graph Mapping. Suppose the server has received local models trained by local data, and we map them into the topological graph. Inspired by [27], take the j-th convolutional layer of k-th local model with 3D-Unet structure as an example, whose kernel dimension is 3  27 . And then, we make every node W as scalar by averaging or summing, which can be formulated as:It can be mapped into a graph whose structure is similar to the full connection layer after the scalarization of the convolutional layer. Given a 3×3×3×C in ×C out convolutional layer, the dimensions of its input and output are C in and C out respectively. So, we obtain a weight matrix W t ∈R Cin×Cout after averaging or summing the weights of convolution kernel. We take the C in and C out as the number of nodes, and the weight summation w sum is the edge weight.(2) Graph Pruning. The server collects local models from clients and makes the graph mapping on them to get K graphs which have the same structure except for the edge weights. These graphs contain all the information of local models, including the part of universality and the part of characteristics of the client data. To make the graphs more distinctive, the graph pruning is conducted.In detail, we differentiated these graphs by setting an adaptive threshold δ, where the edge will be removed if the weight difference of each layer between the local models and global model in the last round is less than the threshold, otherwise, the edge will exist. It can be simplified as:otherwise.(3)where in Eq. 3, 0 denotes the edge is removed, w t kj denotes edge weight of the j-th layer from the k-th graph in the t-th round, also the weight summation of the j-th layer from the k-th local model in the t-th round, w t-1 gj is the weight summation of the j-th layer from the global model in (t -1)-th round. The threshold δ varies adaptively with the weights of local models, and λ is the pruning ratio which is responsible for adjusting the degree of pruning. After that we get K discriminative graphs(3) Graph Comparison. In order to measure the degree of correlation between two graphs, we measure the similarity between pairs of graphs by computing matching between their sets of embeddings, where the Pyramid Match Graph Kernel [28] is employed. We take the reciprocal of the correlation degree as the distance between them. The distance is defined as follows:Aggregation Weights. According to the above process, the Affinity Matrix A is obtained, which reports the correlation among local models and is symmetric. The element A ik in matrix A denotes the affinity of G i and G k . The elements in Table 1. Comparisons with other state-of-the-art methods on the CIFAR-10 dataset."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,,Method Accuracy (%),"FedAvg [1] 88.37 ± 0.04 FedProx [6] 87.93 ± 0.19 FedNova [29] 88.68 ± 0.26 Auto-FedAvg [14]  In federated learning, clients send the updated local models back to the server each round. In round t, α k is represented as α t k . The global model w t+1 g is aggregated by the server:then, the server assigns the global model w t g to all clients. Repeat and until T rounds or other limits."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.1,Datesets and Settings,"CIFAR-10. The first dataset to verify the validity of our algorithm is CIFAR-10. We partition the training set into 8 clients with heterogeneous data by sampling from a Dirichlet distribution (α = 0.5) as in [10] to simulate the non-IID distribution, and the test set in CIFAR-10 is considered as the global test set to evaluate the performance of different algorithms. VGG-9 [30] is employed for image classification, and the other detailed settings are as follows: initial learning rate of 1e -2; total rounds of 100; local epochs of 20; batch size of 64; SGD optimizer for clients."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,,MICCAI FeTS2021,"Training Data. The real-world dataset used in experiments is provided by the FeTS Challenge organizer, which is the training set of the whole dataset about brain tumor segmentation. In order to evaluate the performance of FedGrav, we partition the dataset composed of 341 data samples "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.2,Results,"Experiment Results on the CIFAR-10. We first validate the proposed method on the CIFAR-10 dataset. Table 1 shows the quantitative results of the state-of-the-art FL methods in terms of the average accuracy, such as FedAvg [1], FedProx [6], FedNova [29], and Auto-FedAvg [14]. As can be seen from the table, the proposed FedGrav method outperforms the other competing FL aggregation methods including Auto-FedAvg, a learning-based aggregation method, which indicates the potential and superiority of FedGrav.Experiment Results on MICCAI FeTS2021 Training Dataset. In order to verify the robustness of our method and its performance in real-world data, we conduct the experiment on the MICCAI FeTS2021 Training dataset. We evaluate the performance of our algorithm by comparing six indicators: the Dice Similarity Coefficient(DSC) and Hausdorff Distance-95th percentile(HD95) of whole tumor(WT), enhancing tumor(ET), and tumor core(TC). As is shown in Table 2, we list the average results of FedAvg, FedCostWAvg(shortened to FCW), the champion method of FeTS Challenge 2021, and the proposed Fed-Grav. Different from the original FedCostWAvg which changed the activation function of networks, our re-implemented version made the network unchanged to ensure a fair comparison. Through the quantitative comparison in Table 2, we can find that the proposed method FedGrav has achieved the best results in all indicators except the HD95 TC. Moreover, compared with FedCostWAvg, FedGrav has significantly improved the evaluation of segmentation performance, especially in the enhancing tumor segmentation.The visualization results are shown in Fig. 2. It can be seen that our Fed-Grav achieves better segmentation results, even in the hard example, compared to FedCostWAvg and FedAvg. The results proved that the proposed method FedGrav can explore the correlations of local models better and achieved more excellent aggregation performance compared with other methods.  "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.3,Ablation Study,"To evaluate the effectiveness and find the better configuration of FedGrav, we conduct the ablation study on the FeTS datasets, and the results are shown in Fig. 3. As we can see, the mean DSC shows a trend of rising first and then falling, because more irrelevant and redundant information will be saved in the model when pruning is not performed. The different values of λ denote the loose degree of graphs, with the gradual increase of λ, the redundant information in local models is gradually eliminated, and the unique information of each local model is preserved. While, when the pruning ratio λ increases to a certain extent, the models lack key information, which makes the model affinity inaccurate, resulting in a decline in segmentation performance."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,4.0,Conclusion,"In this paper, we introduced FedGrav, a novel aggregation strategy inspired by the law of universal gravitation in physics. FedGrav improves local model aggregation by considering both the differences in sample size and discrepancies among local models. It can adaptively adjust the aggregation weights and explore the internal correlations of local models more effectively. We evaluated our method on CIFAR-10 and real-world MICCAI Federated Tumor Segmentation Challenge (FeTS) datasets, and the superior results demonstrated the effectiveness and robustness of our FedGrav."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,1.0,Introduction,"The electrocardiogram (ECG), is one of the most widely used methods to analyze cardiac morphology and function, by measuring the electrical signal from the heart with multiple electrodes. ECG data is used by clinicians for both diagnostic and monitoring purposes in various cardiac syndromes. A 12-lead ECG is routinely obtained in patients to diagnose and monitor disease development. However, for the interpretation of the ECG signal, the knowledge of an expert is required. Physicians usually analyze the ECG through the recognition of specific patterns, known to be associated with disease. This however requires substantial expertise, and potentially additional relevant information exists in a 12-lead ECG missed by human interpretation. Deep learning has already proven its usefulness in the interpretation of the ECG signal in multiple classification challenges [1,3] and more recently also in feature discovery by means of explainable AI algorithms [2,7,9,10,15]. The explainablity of AI algorithms is especially valued in medical settings, where trusting a black box AI algorithm is undesirable [1].VAEs and in particular β-VAEs have been used as unsupervised explainable ECG feature generators in the explainable AI algorithms mentioned above [6]. It was shown that a β-VAE, trained on reconstruction of the ECG signal, is able to extract features from the ECG signal that can be made more interpretable by visualization of reconstructed latent space samples with the decoder of the β-VAE [10]. This is a first step towards an explainable deep learning pipeline for ECG analysis. However, the features generated by a β-VAE when only trained to minimized reconstruction loss, are likely not optimal for task specific predictions.The aim of this paper is to explore further improvement of the latent features by improving their explainability and prediction performance. This is clinically relevant but unexplored for the post myocardial infarction setting. We propose to improve explainability by reducing the dimension of the latent space to a level more manageable for human assessment, while encouraging outcome specific information to be captured in a small part of the latent space, and while maintaining ECG reconstruction performance for visual assessment. To achieve this, we propose a novel method to jointly optimize the β-VAE with a combination of a task specific prediction loss for a subset of the latent space, and KL-divergence and reconstruction loss for the entire latent space. The task chosen to optimize here is left ventricular function (LVF), one of the most important determinants of prognosis in patients with cardiac disease. Current assessment of LVF requires advanced imaging methods and interpretation by a trained professional. The ECG, on the other hand, can be obtained by a patient at home. In combination with automated analysis this would facilitate remote monitoring of LVF in patients."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.1,Data,"To train the models for both reconstruction and LVF prediction, two datasets were used: i) A non-labeled dataset consisting of 119,886 raw 10 s 12-lead ECG signals taken at 500 Hz from 7255 patients diagnosed with acute coronary syndrome between 2010 and 2021 at the Leiden University Medical Center, the Netherlands; ii) A labeled dataset of 33,610 ECGs from 2736 patients of the same cohort. This dataset was labeled by visual assessment of an echocardiogram performed within 3 days before or after the ECG. The label categories, normal, mild, moderate and severe impairment were binarized for model training. When the ECG was taken within two weeks after cardiac intervention a 1-day margin was used. If a cardiac intervention was performed between ECG and echocardiography, the case was excluded. 11.5% of the ECGs were labeled with a moderate to severe impaired LVF. The institutional review board approved the study protocol (nWMODIV2 2022006) and waived the obligation to obtain informed consent. The raw ECG signals were first split into separate heartbeats (400ms before and after the R-peak, the largest peak in the ECG, that represents depolarization of the ventricles) with a peak detection method inspired by RPNet, a U-Net structured CNN with inception blocks, that was trained on manually labeled peak locations [16]. The heartbeats were then filtered with a magnitude and an autocorrelation filter. The magnitude filter removed heartbeats with an average magnitude below a set threshold. The autocorrelation filter removed signals where both the mean and maximum autocorrelation between the heartbeats were below a set threshold. These two criteria were used, since ECG signals showing multiple rhythms can result in low mean autocorrelation, but, if not noisy, will not result in low maximum autocorrelation. The remaining heartbeats were then averaged per ECG lead. The μ and σ of the intervals of the subsequent R peaks were used as an additional feature for LVF prediction."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.3,Model Overview,"To investigate a general improvement to the VAE feature extraction pipeline [7,9,10,15], the proposed method was tested with two architectures: i) A small VAE with 300k parameters consisting of an encoder and a mirrored decoder. Both parts contained 7 2D convolutional layers, of which 3 were residual layers, with respective channel sizes of [8,16,32,64,64,64,64] and a kernel size of 5; ii) A second larger VAE from the FactorECG pipeline as proposed by Van de Leur et al. (2022) [10] with 50M parameters. The VAEs were both extended at the bottleneck (the latent space, of size L), with a single fully connected layer for output prediction, in this case the LVF label, see Fig. 1. The μ and σ of the RR intervals (time between two subsequent R peaks), were added to the input of the prediction layer, since the information represented by these features is lost in averaging the heartbeats. To maintain explainability of the extracted features, only one fully connected layer is used, as otherwise the features will become weighted combinations of the latent space values, which makes visualization with the decoder and subsequent interpretation complex. However, for pure prediction performance, additional fully connected layers may have been helpful. The extracted features, again with the μ and σ of the RR interval, were subsequently analyzed with logistic regression using regularized l1 and l2 penalties on the LVF prediction task, ignoring the output of the prediction layer in the VAE. The VAEs were build and trained in the PyTorch 1.12 framework and trained on a Quadro RTX 6000 GPU with CUDA 11.4 [12,13], while for logistic regression we used the Scikit-learn toolbox [14]. The implementation of our models will be made publicly available via GitHub at https://github.com/ViktorvdValk/Task-Specific-VAE."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.4,Model Training,"The β-VAE was first pretrained in a self-supervised manner with the mean heartbeats of all filtered ECG signals, minimizing i) the mean squared reconstruction error (MSE) between the input and output ECG, and ii) the KL-divergence between the output of the encoder and the standard normal distribution. The KL-divergence loss was weighted with a β factor, like in the original paper [6]. This pretrained VAE was then fine-tuned in two-steps, first the encoder and decoder were fixed, and only the prediction layer was trained, then all layers were trained end-to-end. This training scheme was used to ensure more stable training. For these fine-tuning steps, the loss function was complemented with a binary cross-entropy loss, which was weighted with a γ factor. The task naive VAE resulting from pretraining was compared to the task specific VAE resulting from both fine-tuning steps. For pretraining, both datasets were combined and split in a training (85%) and a test set (15%). 5-fold cross validation was done with the training set with again an 85%:15% ratio between training and validation set. For fine-tuning, the same procedure was used on just the labeled dataset, making sure labeled ECGs were in the same set in both cases. All data splits were grouped by patient and stratified by label in case of the labeled data splits. Both pretraining and fine-tuning were done until convergence, i.e. until the loss on the validation set stopped improving for 25 epochs. This was done to prevent the advantage of additional training of the task specific network. To prevent overfitting, balanced sampling and regularization by means of drop out layers and the Adam optimizer with weight decay were used, this was especially necessary in the fine-tuning phase. To prevent gradient explosion, gradient clipping and He initialization were used [5]."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.5,Feature Evaluation,"The differences between the features from the task naive and task specific VAEs, were compared w.r.t. reconstruction and prediction. For reconstruction, both MSE and correlation between input and output ECG, and for prediction the Area Under the Receiver Operator Characteristic Curve (AUROC) and the macroaveraged F1 score were used. Significant difference between AUROC scores was calculated as proposed in Hanley & McNeil (1983)  [4]. For visualization of the representation of a latent space feature f in a so called factor traversal, all features except f were sampled at their mean, while f was sampled in a range between μ -3σ and μ +3σ. Using these samples as input for the decoder, creates a representation of that feature, which can give insight in ECG features that are important for LVF prediction."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.6,Baseline Methods,"As a baseline method, a principal component analysis (PCA) was performed on the preprocessed ECGs, to extract features. PCA can be considered an ordered task naive linear feature extractor that focuses on the axis of the largest variance, in contrast to the VAEs which are non-ordered non-linear feature extractors, that are optimized for reconstruction. A logistic regression predictor with just sex and age as input was used as an additional baseline."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.1,Experiments,"The proposed pipeline contains several hyper-parameters, of which the latent space size L was optimized in this study. The influence of the β parameter was also briefly addressed. L was optimized for its importance in the explainability and the reconstruction and prediction quality of the model. A higher L increases the complexity of the model, and consequently decreases its explainability. An L that is too low, on the other hand, restricts the capacity of the model for reconstruction and prediction. The PCA baseline method was considered to give an upper bound of L, since the number of principal components, the PCA analog for L, indicates how many values would be needed to capture sufficient information."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.2,Hyperparameter Optimization,"The influences of γ on prediction and reconstruction performance was small and was therefore fixed to 500. The influence of L on prediction quality can be seen in Fig. 2. The PCA baseline performs more or less equal to the task naive networks for all L. For the task specific networks, the F1 scores are higher than their task naive counterparts and the PCA baseline, especially for lower L. The task specific VAEs already reached their best prediction performance starting at L = 2, as compared to the task naive VAEs and the PCA baseline that reach their best prediction performance from L = 30. The influence of L on reconstruction can be seen in Fig. 2a andb. All networks perform equal to the PCA baseline for low latent dimensions. The reconstruction for the small VAE and the FactorECG VAE does not seems to improve any further for respectively L >20 and L > 15, where the PCA baseline reconstruction keeps improving with L. However, setting β to 0 and thereby ablating the variational nature of the VAEs prevents this stagnation of reconstruction performance. The task specific networks perform equally well as their task naive counterparts, which suggests that the additional joint optimization does not have a major negative impact on reconstruction. The optimization shows that the relevant information for LVF prediction in the ECG signal can be captured in just two features by both VAEs. Reconstruction, on the other hand, requires at least 10/15 features for the VAEs to reach maximum performance. Therefore, in another experiment, a split task VAE was trained, in which 8 of the latent space features where only optimized for reconstruction and only 2 also for prediction.  "
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.3,Results on the Test Set,"Table 1 shows the results on the test set for L = 2 and L = 10. The (split) task specific networks significantly outperform their task naive counterparts, the PCA baseline, and the sex and age benchmark w.r.t. LVF prediction. Figure 3a, 3b and 3c show that the split task, in contrast to the task naive small VAE with β = 0 and β = 4 can be used to encode the ECG signals to a landscape that visually separates the signals based on LVF status reasonably well. The factor traversals in Fig. 3e andd show an example of the interpretation of the latent features. Setting β to 0, creates features that appear visually less informative. "
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,4.0,Discussion,"Joint optimization of a β-VAE successfully generated features that contain more information about LVF, without hampering reconstruction of the ECG signal. We hypothesize that the β-VAEs have multiple optima for ECG reconstruction of which only some generate features that are relevant for LVF prediction. This study shows that joint optimization will favor this desired subset of optima, and that this is true for different architectures. In addition, we showed that jointly optimizing only a subset of the latent space features for prediction, results in aggregation of the predictive information, thereby improving explainability.The AUROC score of the FactorECG VAE prediction is similar when compared to van der Leur et al. (2022) [10] (AUROC≈0.9 for L = 36). However, the proposed small VAE achieved equal if not better reconstruction and prediction performance with less than 1% of the parameters as shown in Fig. 2.The F1 score is considered more robust than the AUROC score with data imbalance, which is the case here [8]. From Fig. 2d we can therefore conclude that the task specific networks outperform the task naive networks for any L. The differences between the task specific networks and their task naive versions in prediction, at similar reconstruction, indicate that the ECG signal can be summarized with a set of latent features of which only a subset is important for LVF prediction. The joint optimization promotes the extraction of this subset especially when L is small. Figure 2a andb show that the PCA baseline outperforms both VAEs in reconstruction for L > 20 when β = 4, but not for β = 0. This indicates that the VAEs are restricted in reconstruction by the KLdivergence loss. This loss was shown to promote feature disentanglement and a gradient in the latent space [11]. Figure 3d and e show that without this loss (β = 0) the latent features are more complex to interpret. This could be explained as a reduction of the disentanglement of the features resulting from the absence of the KL-divergence loss. However, Fig. 3b andc both show a gradient in the latent space, which suggests that the prediction loss on its own also promotes a gradient in the latent space. Moreover, Fig. 3c shows dependence, and thus a lack of disentanglement, between the latent features even when β = 4. This complex interplay between the three losses used in the joint optimization, is very relevant for the explainability aspect of this method, but beyond the scope of the current study. We aim to examine the complex interplay in future work. In conclusion, the proposed joint optimization improves both explainability and prediction performance of VAEs by extraction of a smaller set of LVF specific features from the ECG signal. This could reduce the need of more advanced imaging methods, currently needed to measure the LVF. This opens the way for remote monitoring of left ventricular function in patients."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,1.0,Introduction,"Coronary artery segmentation is crucial for clinical coronary artery disease diagnosis and treatment [4]. Coronary-computed tomography angiography (CCTA), as a non-invasive technique, has been certified and recommended as established technology in the cardiological clinical arena [15]. Thus, automatic coronary artery segmentation on CCTA images has become increasingly sought after as a means to enhance diagnostic efficiency for clinicians. In recent years, the performance of deep learning-based methods have surpassed that of conventional machine learning approaches (e.g. region growing) in coronary artery segmentation [4]. Nevertheless, most of these deep learning-based methods highly depend on accurately labeled datasets, which need labor-intensive annotations. Therefore, there is a growing demand for relevant label-efficient learning algorithms for automatic coronary artery segmentation on CCTA images.Label-efficient learning algorithms have garnered considerable interest and research efforts in natural and medical image processing [5,6,16], while research on label-efficient coronary artery segmentation for CCTA images is slightly lagging behind. Although numerous label-efficient algorithms for coronary artery segmentation in X-ray angiograms have been proposed [19,20], only a few researches focus on CCTA images. Qi et al. [13] proposed an elabrately designed EE-Net to achieve commendable performance with limited labels. Zheng et al. [22] transformed nnU-Net into semi-supervised segmentation field as the generator of Gan, having achieved satisfactory performance on CCTA images. Most of these researches use incomplete supervision, which labels a subset of data. However, other types of weak supervision (e.g. inexact supervision), which are widely used in natural image segmentation [16], are seldom applied to coronary artery segmentation on CCTA images.Different types of supervision are utilized according to the specific tasks. The application of various types of weak supervision are inhibited in coronary artery segmentation on CCTA images by the following challenges. 1) Difficult labeling (Fig. 1(a)). The target regions are scattered, while manual annotation is drawn slice by slice on the planes along the vessels. Also, boundaries of branches and peripheral vessels are blurred. These make the annotating process timeconsuming and expertise-required. 2) Complex topology (Fig. 1(b)). Coronary artery shows complex and slender structures, diameter of which ranges from 2 mm to 5 mm. The tree-like structure varies individually. Based on these challenges and the insight that vessels share local feature (Fig. 1(b)), we propose partial vessels annotation and our framework as following.Given the above, we propose partial vessels annotation (PVA) (Fig. 1(c)) for CCTA images. While PVA is a form of partial annotation (PA) which has been adopted by a number of researches [2,7,12,18], our proposed PVA differs from the commonly used PA methods. More specifically, PVA labels vessels continuously from the proximal end to the distal end, while the labeled regions of PA are typically randomly selected. Thus, our proposed PVA has two merits. 1) PVA balances efficiency and informativity. Compared with full annotation, PVA only requires clinicians to label vessels within restricted regions in adjacent slices, rather than all scattered target regions in each individual slice. Compared with PA, PVA keep labeled vessels continuous to preserve local topology information.2) PVA provides flexibility for clinicians. Given that clinical diagnosis places greater emphasis on the trunks rather than the branches, PVA allows clinicians to focus their labeling efforts on vessels of particular interest. Therefore, our proposed PVA is well-suited for clinical use.In this paper, we further propose a progressive weakly supervised learning framework for PVA. Our proposed framework, using PVA (only 24.29% vessels labeled), achieved better performance than the competing weakly supervised methods, and comparable performance in trunk continuity with the full annotation (100% vessels labeled) supervised baseline model. The framework works in two stages, which are local feature extraction (LFE) stage and global structure reconstruction (GSR) stage. 1) LFE stage extracts the local features of coronary artery from the limited labeled vessels in PVA, and then propagates the knowledge to unlabeled regions. 2) GSR stage leverages prediction consistency during the iterative self-training process to correct the errors, which are introduced inevitably by the label propagation process. The code of our method is available at https://github.com/ZhangZ7112/PVA-CAS.To summarize, the contributions of our work are three-fold:-To the best of our knowledge, we proposed partial vessels annotation for coronary artery segmentation for the first time, which is in accord with clinical use. First, it balances efficiency and informativity. Second, it provides flexibility for clinicians to annotate where they pay more attention. -We proposed a progressive weakly supervised learning framework for partial vessels annotation-based coronary artery segmentation. It only required 24.29% labeled vessels, but achieved comparable performance in trunk continuity with the baseline model using full annotation. Thus, it shows great potential to lower the label cost for relevant clinical and research use.-We proposed an adaptive label propagation unit (LPU) and a learnable plugand-play feature prototype analysis (FPA) block in our framework. LPU integrates the functions of pseudo label initialization and updating, which dynamically adjusts the updating weights according to the calculated confidence level. FPA enhances vessel continuity by leveraging the similarity between feature embeddings and the feature prototype."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.0,Method,"As shown in Fig. 2, our proposed framework for partial vessels annotation (PVA) works in two stages. 1) The LFE stage(Sect. 2.1) extracts and learns vessel features from PVA locally. After the learning process, it infers on the training set to propagate the learned knowledge to unlabeled regions, outputs of which are integrated with PVA labels to initialize pseudo labels.2) The GSR stage (Sect. 2.2) utilizes pseudo labels to conduct self-training, and leverages prediction consistency to improve the pseudo labels. In our proposed framework, we also designed an adaptive label propagation unit (LPU) and a learnable plug-and-play feature prototype analysis (FPA) block. LPU initialize and update the pseudo labels; FPA block learns before testing and improves the final output during testing. "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.1,Local Feature Extraction Stage,"In LFE stage, our hypothesis is that the small areas surrounding the labeled regions hold valid information. Based on this, a light segmentation model S l is trained to learn vessel features locally, with small patches centering around the labeled regions as input and output. In this manner, the negative impact of inaccurate supervision information in unlabeled regions is also reduced.Pseudo Label Initialization in LPU. After training, S l propagates the learned knowledge of local feature to unlabeled regions. For each image of shape H × W × D, the corresponding output logit ŷ1 ∈ [0, 1] H×W ×D of S l provides a complete estimate of the distribution of vessels, albeit with some approximation. Meanwhile, the PVA label y P V A ∈ {0, 1} H×W ×D provides accurate information on the distribution of vessels, but only to a limited extent. Therefore, LPU integrate ŷ1 and y P V A to initialize the pseudo label y P L (Eq. 1), which will be utilized in GSR stage and updated during iterative self-training.(1)"
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.2,Global Structure Reconstruction Stage,"The GSR stage mainly consists of three parts: 1) The segmentation model S g to learn the global tree-like structure; 2) LPU to improve pseudo labels; 3) FPA block to improve segmentation results at testing. Through initialization (Eq. 1), the initial pseudo label y (t=0) P L contains the information of both PVA labels and the knowledge of local features in S l . Therefore, at the beginning of this stage, S g learns from y (t=0) P L to warm up. After this, logits of S g are utilized to update the pseudo labels during iterative self-training.Pseudo Label Updating in LPU. The principle of this process is that more reliable logit influences more the distribution of the corresponding pseudo label. Based on this principle, first we calculate the confidence degree η (t) ∈ [0, 1] for ŷ(t)2 . Defined by Eq. 2, η (t) numerically equals to the average of the logits in labeled regions. This definition makes sense since the expected logit equals to ones in vessel regions and zeros in background regions. The closer ŷ(t) 2 gets to the expected logit, the higher η (t) (confidence degree) will be.Then, a quality control test is performed to avoid negative optimization as far as possible. As the confidence degree η (t) assesses the quality of predictions, which means low-confidence predictions are more likely to generate errors, our quality control test rejects low-confidence predictions to reduce the risk of error accumulation. If η (t) is higher than all elements in the set {η (i) } t-1 i=1 , the current logit is trustworthy to pass the test to improve the pseudo label. Then, y (t) P L is updated by the exponentially weighted moving average (EWMA) of the logits and the pseudo labels (Eq. 3). This process is similar to prediction ensemble [11], which hase been adopted to filter pseudo labels [9]. However, different from their methods, where the factor η (t) is a fixed hyperparameter coefficient and the pseudo labels are updated each or every several epoches, η (t) in our method is adaptive. Our EWMA gradually diminishes the negative influence of existing errors through the weighted average of predictions across multiple phases.Feature Prototype Analysis Block. Inspired by [21], which generates class feature prototype ρ c (Eq. 4) from the embeddings z l i of labeled points in class c, we inherit the idea but further transform the mechanism into the proposed learnable plug-and-play block, FPA block. Experimental experience finds that the output of FPA block has good continuity, for which the FPA output are utilized to enhance the continuity of convolution output at testing.In the penultimate layer of the network, which is followed by a 1 × 1 × 1 convolutional layer to output logits, we parallelly put the feature map Z ∈ R C×H×W ×D into FPA. The output similarity map O ∈ R 1×H×W ×D is calculated by Eq. 5, where Z(h, w, d) ∈ R C denotes the feature embeddings of voxel (h, w, d), and ρ θ ∈ R C the kernel parameters of FPA.The learning process of FPA block is before testing, during which the whole model except FPA gets frozen. To reduce the additional overhead, ρ θ is initialized by one-time calculated ρ c and fine-tuned with loss L fpa (Eq. 6), where only labeled voxels will take effect in updating the kernel.3 Experiments and Results"
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.1,Dataset and Evaluation Metrics,"Experiments are implemented on a clinical dataset, which includes 108 subjects of CCTA volumes (2:1 for training and testing). The volumes share the size of 512 × 512 × D, with D ranging from 261 to 608. PVA labels of the training set are annotated by clinicians, where only 24.29% vessels are labeled. The metrics used to quantify the results include both integrity and continuity assessment indicators. Integrity assessment indicators are Mean Dice Coefficient (Dice), Relevant Dice Coefficient (RDice) [13], Overlap (OV) [8]; continuity assessment indicators are Overlap util First Error (OF) [14] on the three main trunks (LAD, LCX and RCA)."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.2,Implementation Details,"3D U-Net [3] is set as our baseline model. Experiments were implemented using Pytorch on GeForce RTX 2080Ti. Adam optimizer was used to train the models with an initial learning rate of 10 -4 . The patch sizes were set as 128 × 128 × 128 and 512 × 512 × 256 respectively for S l and S g . When testing, sliding windows were used with a half-window width step to cover the entire volume."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.3,Comparative Test,"To verify the effectiveness of our proposed method, it is compared with both classic segmentation models (3D U-Net [3], HRNet [17], Transunet [1]) and partial annotation-related weakly supervised frameworks (EWPA [12], DMPLS [10]). The quantitative results of different methods are summarized in Table 1, which shows that our proposed method outperforms the competing methods under PVA. The competing frameworks (EWPA and DMPLS) had achieved the best results in their respective tasks under partial annotation, but our proposed method achieved better results for PVA-based coronary artery segmentation. It is worth mentioning that the performance in trunk continuity (measured by the indicator OF) of our proposed method using PVA (24.29% vessels labeled) is comparable to that of the baseline model using full annotation (100% vessels labeled).The qualitative visual results verify that our proposed method outperforms the competing methods under PVA. Three cases are shown in Fig. 3. All the cases show that the segmentation results of our method have good overall topology integrity, especially on trunk continuity. "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.4,Ablation Study,"Ablation experiments were conducted to verify the importance of the components in our proposed framework (summarized in Table 2). The performance improvement verifies the effectiveness of pseudo label initialization (PLI) and updating (PLU) mechanisms in the label propagation unit (LPU). PLI integrates the information of PVA labels with the propagated knowledge, and PLU improves the pseudo labels during self-training. With the help of FPA block, the segmentation results gain further improvement, especially on the continuity of trunks. "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,4.0,Conclusion,"In this paper, we proposed partial vessels annotation (PVA) for coronary artery segmentation on CCTA images. The proposed PVA is convenient for clinical use for the two merits, providing flexibility as well as balancing efficiency and informativity. Under PVA, we proposed a progressive weakly supervised learning framework, which outperforms the competing methods and shows comparable performance in trunk continuity with the full annotation supervised baseline model. In our framework, we also designed an adaptive label propagation unit (LPU) and a learnable plug-and-play feature prototype analysis(FPA) block. LPU integrates the functions of pseudo label initialization and updating, and FPA improves vessel continuity by leveraging the similarity between feature embeddings and the feature prototype. To conclude, our proposed framework under PVA shows great potential for accurate coronary artery segmentation while requiring significantly less annotation effort."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_28.
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"Magnetic Resonance Imaging (MRI) of the brain is an essential imaging modality to accurately diagnose various neurological diseases ranging from inflammatory T. Pinetz and A. Effland-are funded the German Research Foundation under Germany's Excellence Strategy -EXC-2047/1 -390685813 and -EXC2151 -390873048 and R. Haase is funded by a research grant (BONFOR; O-194.0002.1). T. Pinetz and E. Kobler-contributed equally to this work. lesions to brain tumors and metastases. For accurate depictions of said pathologies, gadolinium-based contrast agents (GBCA) are injected intravenously to highlight brain-blood barrier dysfunctions. However, these contrast agents are expensive and may cause nephrogenic systemic fibrosis in patients with severely reduced kidney function [31]. Moreover, [17] reported that Gadolinium accumulates inside patients with unclear health consequences, especially after repeated application. The American College of Radiology recommends administering the lowest GBCA dose to obtain the needed clinical information [1].Driven by this recommendation, several research groups have recently published dose-reduction techniques focusing on maintaining image quality. Complementary to the development of higher relaxivity contrast agents [28], virtual contrast [3,8] -replacing a large fraction of the GBCA dose by deep learninghas been proposed. These approaches typically acquire a contrast-enhanced (CE) scan with a lower GBCA dose along with non-CE scans, e.g., T1w, T2w, FLAIR, or ADC. These input images are then processed by a deep neural network (DNN) to replicate the corresponding standard-dose scan. While promising, virtual contrast techniques have not been integrated into clinical practice yet due to falsepositive signals or missed small lesions [3,23]. As with all deep learning-based approaches, the availability of large datasets is essential, which is problematic in the considered case since the additional CE low-dose scan is not acquired in clinical routine exams. Hence, there are no public datasets to easily benchmark and compare different algorithms or evaluate their performance. In general, the enhancement behavior of pathological tissues at various GBCA dosages has barely been researched due to a lack of data [12].In recent years, generative models have been used to overcome data scarcity in the computer vision and medical imaging community. Frequently, generative adversarial networks (GANs) [9] are applied as state-of-the-art in image generation [30] or semantic translation/interpolation [5,18,21]. In a nutshell, the GAN framework trains two competing DNNs -the generator and the discriminator. The generator learns a non-linear transformation of a predefined noise distribution to fit the distribution of a target dataset, while the discriminator provides feedback by simultaneously approximating a distance or divergence between the generated and the target distribution. The choice of this distance leads to the well-known different GAN algorithms, e.g., Wasserstein GANs [4,10], Least Squares GANs [24], or Non-saturating GANs [9]. However, Lucic et al. [22] showed that this choice has only a minor impact on the performance.Learning conditional distributions between images can be accomplished by additionally feeding a condition (additional scans, dose level, etc.) into both the generator and discriminator. In particular, for image-to-image translation tasks, these conditional GANs have been successfully applied using paired [14,25,27] and unpaired training data [35]. Within these methods, an additional content (cycle) loss typically penalizes pixel-wise deviations (e.g., 1 ) from a corresponding reference to enforce structural similarity, whereas a local adversarial loss (discriminator with local receptive field) controls textural similarity. In addition, embeddings have been used to inject metadata [7,18]. To study the GBCA accumulation behavior, we collected 453 CE scans with non-standard GBCA doses in the set of {10%, 20%, 33%} along with the corresponding standard-dose (0.1 mmol/kg) scan after applying the remaining contrast agent. Using this dataset, we aim at the semantic interpolation of the GBCA signal at various fractional dose levels. To this end, we use GANs to learn the contrast enhancement behavior from the dataset collective and thereby enable the synthesis of contrast signals at various dose levels for individual cases. Further, to minimize the smoothing effect [19] of typical content losses (e.g., 1 or perceptual [16]), we develop a noise-preserving content loss function based on the Wasserstein distance between paired image patches calculated using a Sinkhornstyle algorithm. This novel loss enables a faithful generation of noise, which is important for the identification of enhancing pathologies and their usability as additional training data.With this in mind, the contributions of this work are as follows:-synthesis of GBCA behavior at various doses using conditional GANs, -loss enabling interpolation of dose levels present in training data, -noise-preserving content loss function to generate realistic synthetic images."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.0,Methodology,"Given a native image x NA (i.e. without any contrast agent injection) and a CE standard-dose image x SD , our conditional GAN approach synthesizes CE lowdose images xLD for selected dose levels d ∈ D ⊂ [0, 1] from a uniform noise image z ∼ N (0, Id), see Fig. 1. To focus the generation on the contrast agent signal, our model predicts residual images ŷLD ; the corresponding low-dose can be obtained by xLD = x NA + ŷLD .For training and evaluation, we consider samples (x NA , x SD , y LD , d, B) of a dataset DS, where y LD = x LD -x NA is the residual image of a real CE low-dose scan x LD with dose level d ∈ D and B ∈ {1.5, 3} is the field-strength in Tesla of the used scanner. To simplify learning of the contrast accumulation behavior, we adapt the preprocessing pipeline of BraTS [6]. Further details of the dataset and the preprocessing are in the supplementary material."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.1,Conditional GANs for Contrast Signal Synthesis,"Our approach is built on the insight that contrast enhancement is an inherently local phenomenon and the necessary information for the synthesis task can be extracted from a local neighborhood within an image. Therefore, we use as generator g θ a convolutional neural network (CNN) that is based on the U-Net [29] along with a local attention mechanism. The architecture design and the implementation details can be found in the supplementary material. As illustrated in Fig. 1, the generator uses a 3D noise sample z ∼ N(0, Id) along with the native and SD images (x NA , x SD ) as input. The synthesis is guided by the metadata ( d B) , containing the artificial dose level d ∈ D as well as the field strength of the corresponding scanner B ∈ {1.5, 3}. In particular, the metadata is injected into every residual block of the generator using an embedding, motivated by the recent success of diffusion-based models [13].To learn this generator, a convolutional discriminator f φ is used, which is in turn trained to distinguish the generated residual images ŷLD with random dose level d from the real residual images y LD with the associated real dose level d. To make this a non-trivial task, label smoothing on the metadata is used, i.e., the real dose is augmented by zero-mean additive Gaussian noise with standard deviation 0.05. The discriminator architecture essentially implements the encoding side of the generator, however, no local attention layers are used as suggested by [20]. Like the generator, the discriminator is conditioned on the metadata using an embedding, which is not shared between both networks.For training of the generator θ and discriminator φ, we consider the losswhich consists of a Wasserstein GAN loss L GAN , a gradient penalty loss L GP , and a content loss L C that are relatively weighted by scalar non-negative factors λ GP and λ C . In detail, the Wasserstein GAN loss reads as  Lipschitz continuous with factor 1 in its arguments as required by Wasserstein GANs [10]. Here, ŷLD = g θ (z, ĉ) and the Lipschitz penalty is evaluated at convex combinations of real and synthetic images and condition tuples (essentially dose levels). Finally, using a distance C , the content loss L C (θ) := E (xNA,xSD,yLD,d,B)∼U (DS),z∼N (0,Id) C g θ (z, c) , y LD guides the generator g θ towards residual images in the dataset. Thus, it teaches the generator the principles of contrast enhancement. Typically, the 1 -norm is used as a distance function, which leads to smooth results since it also penalizes deviations from the noise in y LD ."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.2,Noise-Preserving Content Loss,"To generate realistic CE images, it is also important to retain the original noise characteristics. Therefore, we introduce a novel loss that accounts for deviations in local statistics using optimal transport between empirical distributions of paired patches, as illustrated in Fig. 2. Let x, x ∈ R n 3 be patches of size n × n × n extracted from the same location of a real and synthetic image, respectively. The Wasserstein distance of the associated empirical distributions using a transport plan T ∈ R n 3 ×n 3where 1 is the vector of ones of size n 3 . In contrast to the element-wise difference penalization of the 1 -distance, the Wasserstein distance accounts for mismatches between distributions. To illustrate this, let us, for instance, assume that both patches are Gaussian distributed (x ∼ N (μ, σ), x ∼ N (μ, σ)), which is a coarse simplification of real MRI noise [2]. In this case, the Wasserstein distance reduces to second-order momentum matching, i.e., W 2 (x, x) = (μ -μ) 2 +(σ -σ) 2 . Thus, the Wasserstein distance generalizes this distributional loss to any distribution within paired patches.To efficiently solve problem (2), we use the inexact proximal point algorithm [34]. This algorithm is parallelized and applied to all non-overlapping patch pairs, to obtain our noise-preserving content losswhere P p extracts a local n × n × n patch at location p ∈ P = {0, n, 2n, . . .} 3 using periodic boundary conditions. Note that we compute the expectation over offsets o ∈ O = {0, 1, . . . , n 2 } 3 to avoid patching artifacts. In the numerical implementation, only a single offset is sampled for time and memory constraints."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,3.0,Numerical Results,"In this section, we evaluate the proposed conditional GAN approach with a particular focus on different content loss distance functions. All synthesis models were trained on 250 samples acquired on 1.5T and 3T Philips Achieva scanners and evaluated on 193 test cases, all collected at site 1 . Further details of the dataset, model and training can be found in the supplementary. In our experiments, we observed that the choice of the content loss distance function C (ŷ, y) strongly influences the performance. Thus, we consider the different cases:Following Johnsen et al. [16], h(x) is the VGG-16 model [32] up to relu3_3.A qualitative comparison of the different distance functions C is visualized in Fig. 3. The first column depicts synthesized images using the 1 -norm as the distance function. These images depict a plausible contrast signal, however, suffer from unrealistic smooth homogeneous regions. An improvement thereof is shown by the perceptual content loss (VGG). The NP-loss leads to a further improvement not only in the contrast signal behavior but also in the realism of the noise texture, cf. zoom regions in the lower corners.To highlight the generalization capabilities, we depict in the bottom row of Fig. 3 a sample from site 2 , which was acquired using a Philips Ingenia scanner. Moreover, the GBCA gadoterate was used, while our training data only consists of scans using the GBCA gadobutrol. Nevertheless, all models present realistically synthesized LD images. Comparing the zooms of the LD images, we observe that our NP-loss leads to a better synthesis of noise and thereby to Fig. 3. Qualitative comparison of synthesized images using different loss functions to the corresponding reference xLD. While the 1 loss yields smooth low-dose images, the noise pattern is preserved to some extent using the VGG loss; our loss helps to further retain the noise characteristics. more realistic LD images. In the 1 and VGG columns, the noise is not faithfully synthesized, thus it is visually easy to spot the enhancing pathological regions.For completeness, a quantitative ablation of the considered distance functions on the test images of site 1 is shown in Table 1. Although neither maximizing PSNR nor SSIM [33] is our objective, we observe on-par performances of the perceptual (VGG) and our proposed content loss (NP) with the standard 1 distance function. Using the SD image, we define CE pixels as those pixels at which the intensity increases by at least 10% compared to the native scan. An example of these CE regions is illustrated in the supplementary. Thus, the mean absolute error for CE pixels (MAE CE ) quantifies the enhancement behavior. Further, we estimate the standard deviation of the non-CE pixels and report the MAE to the ground truth standard deviation (MAE σ ). As shown in Table 1, our loss outperforms the other content losses to a large extent on both metrics, proving its effectiveness for faithful contrast enhancement and noise generation. Further statistical analyses are presented in the supplementary.Table 1. Quantitative comparison of the low-dose synthesis methods. The central columns present metrics evaluated on the synthesized low-dose images, whereas the right columns evaluate the effect of purely synthesized data for training the standarddose prediction model [26]. Note, that the PSNR/SSIM of the standard dose prediction model was always evaluated on real LD images. The definitions of the mean absolute error on the contrast enhancement (MAECE) and on the noise standard deviation (MAEσ) are in Sect. Next, we evaluate the effect of synthesized LD images on the performance of a virtual contrast model (VCM). In particular, we consider the state-of-the-art 2.5D U-Net model [11,23,26], which predicts an SD image given a corresponding native and LD image, see supplementary for further details. The columns on the right of Table 1 list the average PSNR and SSIM score on the real 33% LD subset of our test data from site 1 . The bottom row depicts the performance if just real 33% LD images are used for training the VCM as an upper bound. In contrast, the other entries on the right list the performance if only synthesized LD images are used for training. Both metrics show that the samples synthesized using our NP-loss model are superior to both 1 and VGG.To determine the effectiveness of the LD synthesis models at different settings, we acquired 160 data samples from 1.5T and 3T Philips Ingenia scanners at site 2 . This site used the GBCA gadoterate, which has a lower relaxivity compared to gadobutrol used at site 1 [15]. For 80 samples real LD images were acquired, which are used for testing. Using the VCM solely trained on the real 33% LD data of site 1 yields an average PSNR and MAE CE on the test samples of site 2 of 40.04 and 0.092, respectively. Extending the training data for the VCM by synthesized LD images from our model with NP-loss, we get a significantly improvemed (p < 0.001) PSNR score of 40.37 and MAE CE of 0.075.Finally, Fig. 4 visualizes synthesized LD images on the BraTS dataset [6] along with the associated VCM outputs. Comparing the predicted SD images xSD using 10% and 33% synthesized LD images xLD , we observe that the weakly enhancing tumor at the bottom zoom is not preserved in the case of 10%, enabling evaluation of dose reduction methods on known pathological regions.  [6] along with the native (left) and real SD image (right). We also included non-fractional dosage levels (17% and 47%) to showcase the wide applicability of our algorithm. Top: the tumor is well contrasted in all xSD even for 10%. Bottom: the subtle enhancement of the tumor cannot be recovered from the 10% LD image."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,4.0,Conclusions,"In this work, we used conditional GANs to synthesize contrast-enhanced images using non-standard GBCA doses. To this end, we introduced a novel noisepreserving content loss motivated by optimal transport theory. Numerous numerical experiments showed that our content loss improves the faithful synthesis of low-dose images. Further, the performance of virtual contrast models increases if training data is extended by synthesized images from our GAN model trained by the noise-preserving content loss."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_57.
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,1.0,Introduction,"Deep learning has achieved remarkable success in medical image segmentation when the training and test data are independent and identically distributed (i.i.d) [4,13,21]. However, in many practical situations, training and test data are collected by different medical imaging devices, leading to the presence of distribution shifts. Therefore, the models trained on source-domain data perform poorly on target-domain data. An effective solution for this issue is to fine-tune the models with labeled target-domain data to adapt to the target-domain distributions [9], but it is impractical to label the target-domain data considering the high annotation cost. Existing unsupervised domain adaptation (UDA) methods [7,16,22] make full use of the labeled source-domain data and the unlabeled target-domain data in the model training, but even the target-domain data may not be available in the model training due to various practical problems.Domain generalization (DG) methods exploit the diversity of source domains to improve the model generalization [8,14,24] when target-domain data is not available in the model training. However, it is also difficult and cost-consuming to collect multiple source-domain datasets with different domain distributions for DG. Another promising solution is test-time adaptation (TTA), which aims to gradually update the model parameters to adapt to target-domain distributions by learning from test data at test time. TTA shows greater flexibility than DG as the models could be pre-trained on single source-domain data. In TTA, a mainstream strategy is to adjust the affine parameters in BN layers for domain adaptation at test time by unsupervised loss, such as PTBN [12] and TENT [19]. Besides, auxiliary self-supervised tasks [10,17] and contrastive learning [3,20] are also considerable for TTA.TTA methods have also been recently applied in medical image applications. Ma et al. [11] innovated distribution calibration by dynamically aggregating multiple representative classifiers via TTA to deal with arbitrary label shifts. Hu et al. [6] designed regional nuclear-norm loss and contour regularization loss for TTA on medical image segmentation tasks. Bateson et al. [1] performed inference by minimizing the entropy of predictions and a class-ratio prior, and integrated shape priors through penalty constraints for guide adaptation. Varsavsky et al. [18] introduced domain adversarial learning and consistency training in TTA for sclerosis lesion segmentation. These TTA methods have a common limitation of using a fixed learning rate for all test samples. Since test samples arrive sequentially and the scale of domain shift would change frequently, a fixed learning rate would be sub-optimal for TTA. DLTTA [23] proposed a memory bank-based discrepancy measurement for dynamic learning rate adjustment of TTA to effectively adapt the model to the varying domain shift. However, we find that the influence of domain shift on different semantic categories may also be different, DLTTA performed global domain adaptation for all semantic categories.In this paper, we present Semantic-Aware Test-Time Adaptation (SATTA) for cross-domain medical image segmentation, aiming to perform individual domain adaptation for each semantic category at test time. SATTA first utilizes an uncertainty estimation module to effectively measure the discrepancies of different semantic categories in domain shift. Based on the estimated discrepancies, a semantic adaptive learning rate is then developed to achieve a personalized degree of adaptation for each semantic category. Lastly, a semantic proxy contrastive loss is proposed to individually adjust the model parameters with the semantic adaptive learning rate. Our SATTA is evaluated on retinal fluid segmentation based on spectral-domain optical coherence tomography (SD-OCT) images, and the experimental results show superior performance than other state-of-the-art TTA methods."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.1,Test-Time Adaptation Review,"Given a labeled source-domain dataset S = {(x s n , y s n )} N s n=1 , model parameters θ are pre-trained on S by supervised risk minimization:where L sup is the supervised loss for model optimization, such as the crossentropy loss. However, for an unlabeled target-domain datasetthat has different domain distributions with S, the model F θ s may have an obvious performance degeneration. To make the model F θ s adapt to the targetdomain distributions, an unsupervised TTA loss L tta (such as rotation prediction loss [17], entropy minimization loss [19], contrastive loss [3], etc.) is designed to fine-tune model based on target-domain samples at test time:where η is learning rate, θ t 0 is initialized with θ s . The final prediction on x t n can be given by y"
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.2,Semantic Adaptive Learning Rate,"Pseudo-labeling for Semantic Aggregation. where each category proxy w c can be regarded as the high-dimensional representative of the category. Category predictor measures the similarities between pixel embeddings and all category proxies. We represent the classification process of pixel p i as:where f (•) is the feature extractor, C is the category number, h and w are the height and width of images. Since pixel-wise labels are not available for targetdomain samples at test time, we are hard to obtain the semantic information of all pixel embeddings directly. To address this problem, we assign pseudo labels to all pixel embeddings by passing them through the category predictor and then aggregate all pixel embeddings into C semantic clusters asaccording to their pseudo labels.Semantic Uncertainty Estimation. After performing semantic aggregation by pseudo-labeling, we need to estimate the varying discrepancies of domain shift on categories. Here, we employ Monte Carlo Dropout [5] for semantic uncertainty estimation. We enable dropout at test time and perform L stochastic forward passes through the model to obtain a set of predictive outputs for pixel embedding e i :where M(•) is a mapping network that maps the pixel embedding e i into an additional probability output. Then we estimate the standard deviation of the L outputs as the uncertainty score of e i :Here we take a single pixel as an example to show the uncertainty score computation, it should be noted that all pixels are performed parallel computation in the semantic segmentation model. Therefore, the computation cost does not increase with the number of pixels. For category c, its semantic uncertainty score can be calculated by:where N Ωc is the number of pixel embeddings in Ω c . U c captures the unique semantic domain discrepancy over category c. Later, semantic adaptive learning rate η c of category c for TTA is obtained directly based on the semantic domain discrepancy U c :where α is a scale factor. In this work, α could be set as the learning rate used for the model pre-training with the source-domain dataset. Each semantic category has its own individual learning rate in each iteration."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.3,Semantic Proxy Contrastive Learning,"General contrastive losses focus on exploring rich sample-to-sample relations, but they are hard to learn specific semantic information from samples. Proxy contrastive loss can model semantic relations by category proxies, as category proxies are more robust intuitively to noise samples [24]. Therefore, a proxy contrastive loss is more suitable for unsupervised TTA optimization with our proposed semantic adaptive learning rate.Projection Heads. We regard each category proxy as the anchor and consider all proxy-to-sample relations. Since proxy-based methods converge very easily, we consider applying projection heads to map both pixel embeddings and category proxies to a new feature space where proxy contrastive loss is applied.We use a three-layer MLP H 1 (•) for projecting pixel embeddings and one-layer MLP H 2 (•) for projecting category proxy weights. The new pixel embedding and category proxy weight can be given by z i = H 1 (e i ) and v c = H 2 (w c ).Top-K Selection. Pixel embeddings with high uncertainty scores contribute little to semantic proxy contrastive learning. Besides, the computation cost is huge for all pixel embeddings. To address this problem, we select K pixel embeddings with the highest confidence from each semantic cluster Ω c . Specifically, for each semantic cluster Ω c , we first order all pixel embeddings in it from smallest to largest according to their uncertainty scores. Then we select the first K pixel embeddings as the new semantic cluster Ω c for the next proxy contrastive loss by Ω c = T opK(Order(Ω c )).Semantic Proxy Contrastive Loss. For an anchor category cluster Ω c , we associate all pixel embeddings in it with category proxy weight v c to form the positive pairs. We ignore the sample-to-sample positive pairs and only consider the sample-to-sample negative pairs. The semantic proxy contrastive loss for category c can be given by:where {z i } K i=1 are obtained by x and C is the number of categories appearing in samples."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.4,Training and Adaptation Procedure,"The overview of our SATTA is shown in Fig. 1. Given the source-domain dataset, the model parameters θ are pre-trained by the combination of supervised cross-entropy loss and semantic proxy contrastive loss:At test time, for a target-domain sample at time step n, we perform a forward pass to obtain semantic clusters and uncertainty scores and calculate the semantic adaptive learning rate of each category to serve for semantic proxy contrastive loss. For category c, the model parameters are updated to achieve desired adaptation by:The updated model parameters are stored in a memory bank and will be loaded for the next domain adaptation of category c. If category c does not appear in the test sample by pseudo-labeling, we ignore the update of model parameters θ c n-1 . We only update the parameters of the feature extractor and freeze the parameters of the category predictor."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.1,Materials,"Our SATTA was evaluated on retinal fluid segmentation based on RETOUCH challenge [2], which is a representative benchmark for segmenting all of the three fluid types in SD-OCT images, including intraretinal fluid (IRF), subretinal fluid  [15] w/o CD 0.716 0.794 0.802 0.738 0.904 0.786 0.711 0.664 0.768 U-Net [15] w/ CD 0.637 0.751 0.667 0.587 0.733 0.624 0.536 0.512 0.565 TENT [19] 0.648 0.772 0.733 0.605 0.828 0.703 0.589 0.603 0.637 FTTA [6] 0.663 0.769 0.746 0.632 0.831 0.729 0.611 0.618 0.669 TTAS [1] 0.672 0.774 0.762 0.674 0.865 0.762 0.663 0.641 0.724 CoTTA [20] 0 (SRF) and pigment epithelial detachment (PED). SD-OCT images were acquired by three different vendors: Cirrus, Spectralis, and Topcon. The training set consists of 3072 (Cirrus), 1176 (Spectralis), and 3072 (Topcon) SD-OCT images, and the test set consists of 1792 (Cirrus), 686 (Spectralis) and 1792 (Topcon) SD-OCT images. We regard the SD-OCT images from three different vendors as three different domains, namely Domain-1 (Cirrus), Domain-2 (Spectralis), and Domain-3 (Topcon). We employ the dice similarity coefficient (DSC) as the quantitative segmentation metric and a higher DSC indicates a better segmentation performance."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.2,Comparison with State-of-the-Arts,"We compare our SATTA with four state-of-the-art TTA methods, including TENT [19], FTTA [6], TTAS [1] and CoTTA [20]. The four comparative methods have been reviewed in Sect. 1. To verify the TTA performance on cross-domain retinal fluid segmentation based on the RETOUCH challenge with three different domains, we train the segmentation models on two source domains and run TTA methods on the remaining target domain at test time. We carry out three times until all of three domains are tested as unseen target domains. For fair comparisons, all of TTA methods use U-Net [15] as a feature extractor and share the same experimental setting, such as initial learning rate, batch size, etc. The quantitative comparison results are presented in Table 1. We also include ""U-Net w/ CD"" as the lower bound and ""U-Net w/o CD"" as the upper bound, where ""CD"" denotes cross domain. ""U-Net w/o CD"" denotes that the segmentation model is trained and tested on the samples from the same domain. ""U-Net w/ CD"" denotes that the segmentation model is trained and tested on the samples from different domains, without any domain adaptation. We observe that different TTA methods consistently improve the segmentation performance over ""U-Net w/ CD"". Our SATTA achieves the highest DSC than other methods. The qualitative comparison results are shown in Fig. 2. These visual results confirm  that a segmentation model trained only on source-domain distributions performs poorly on target-domain distributions without domain adaptation."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.3,Ablation Study,"We conduct ablation studies to analyze the key factors regarding our SATTA. We first explore the effect of K value in the Top-K selection strategy. The Top-K selection strategy aims to select pixel embeddings with high confidence scores to improve the semantic proxy contrastive learning and reduce the computation cost significantly. Figure 3(a) shows the effect of different K values on three domains for IRF, SRF, and PED. The DSC values consistently increase when rising the K value from 10 to 20, generally, peak when the K value is between 20 and 25, and consistently decrease when further rising K value. This affirms that the pixel embeddings with high confidence scores are conducive to semantic proxy contrastive learning while the pixel embeddings with low confidence scores weaken semantic proxy contrastive learning.We also investigate the effect of the initial learning rate. We select different initial learning rates from the set {5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5} for TTA, and Fig. 3(b) shows the total average DSC values of all TTA methods. Our SATTA consistently performs better than other state-of-the-art TTA methods. We also find that different initial learning rates actually affect the domain adaptation ability. Therefore, a proper initial learning rate is essential for TTA methods."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,4.0,Conclusion,"In this paper, we present the SATTA method for cross-domain medical image segmentation. Aiming at the problem that the domain shift has different effects on the semantic categories, our SATTA provides a semantic adaptive parameter optimization scheme at test time. Although our SATTA shows superior crossdomain segmentation performance than other state-of-the-art methods, it still has a limitation. Since SATTA adjusts the model for each semantic category, it is not quite suitable for the samples with too many semantic categories due to high computation costs."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_14.
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,1.0,Introduction,"Ultrasound imaging is a very effective technique for breast lesion diagnosis, which has high sensitivity. Automatically detecting breast lesions is a challenging problem with a potential to aid in improving the efficiency of radiologists in ultrasound-based breast cancer diagnosis [18,21]. Some of the challenges associated with automatic breast lesion detection include blurry boundaries and changeable sizes of breast lesions.Most existing breast lesion detection methods can be categorized into imagebased [10,11,16,17,19] and video-based [1,9] breast lesion detection approaches. Image-based breast lesion detection approaches perform detection in each frame independently. Compared to image-based breast lesion detection approaches, methods based on videos are capable of utilizing temporal information for improved detection performance. For instance, Chen et al. [1] exploited temporal coherence for semi-supervised video-based breast lesion detection. Recently, Lin et al. [9] proposed a feature aggregation network, termed as CVA-Net, that executes intra-video and inter-video fusions at both video and clip levels based on attention blocks. Although the recent CVA-Net aggregates clip and video level features, we distinguish two key issues that hamper its performance. First, the self-attention based cross-frame feature fusion is a global-level operation and it operates once before the encoder-decoder, thereby ignoring the useful local information and in turn missing an effective deep feature fusion. Second, CVA-Net only performs one-frame prediction based on multiple frame inputs, which is very time-consuming.To address the aforementioned issues, we propose a spatial-temporal deformable attention based network, named STNet, for detecting the breast lesions in ultrasound videos. Within our STNet, we introduce a spatial-temporal deformable attention module to fuse multi-scale spatial-temporal information among different frames, and further integrate it into each layer of the encoder and decoder. In this way, different from the recent CVA-Net, our proposed STNet performs both deep and local feature fusion. In addition, we introduce multiframe prediction with encoder feature shuffle operation that shares the backbone and encoder features, and only perform multi-frame prediction in the decoder. This enables us to significantly accelerate the detection speed of the proposed approach. We conduct extensive experiments on a public breast lesion ultrasound video dataset, named BLUVD-186 [9]. The experimental results validate the efficacy of our proposed STNet that has a superior detection performance. For example, our proposed STNet achieves a mAP of 40.0% with an absolute gain of 3.9% in terms of detection accuracy, while operating at two times faster, compared to the recent CVA-Net [9]."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.0,Method,"Here, we describe our proposed spatial-temporal deformable attention based framework, named STNet, for detecting breast lesions in the ultrasound videos. Figure 1(a) presents the overall architecture of our proposed STNet, which is built on the end-to-end detector deformable DETR [22]. Within our STNet, we introduce spatial-temporal deformable attention into the encoder and the decoder. As in CVA-Net [9], we take six frames I k-1 , I k , I k+1 , I r1 , I r2 , I r3 from one ultrasound video as inputs, where there are three neighboring frames I k-1 , I k , I k+1 and three randomly-selected frames I r1 , I r2 , I r3 . Given these input frames, we use the backbone, such as ResNet-50 [6], to extract deep multi-scale features F k-1 , F k , F k+1 , F r1 , F r2 , F r3 . Afterwards, we introduce a spatial-temporal deformable attention based encoder (ST-Encoder) to perform intra-frame and inter-frame multi-scale feature fusion. Then, we introduce a spatial-temporal deformable attention based decoder (ST-Decoder) to generate output feature embeddings P k , which are fed to a classifier and a box predictor for classification and bounding-box regression. During inference, we take three neighboring frames and three randomly-selected frames as the inputs, and simultaneously predict the results of three neighboring frames using our encoder feature shuffle strategy. As a result, our approach operates at a faster inference speed."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.1,Spatial-Temporal Deformable Attention,"Given a reference point, deformable attention [22] aggregates the features of a group of key sampling points near it. Compared to original transformer selfattention [13], deformable attention has low-complexity along with a faster convergence speed. Motivated by this, we adopt deformable attention for breast lesion detection and extend it to spatial-temporal deformable attention (STDA). Our STDA not only aggregates the features of current frame, but also aggregates the features of the rest of the frames. Figure 2 presents the structure of our Given a query feature and reference point, our STDA not only fuses multi-scale features within a frame, but also aggregates multi-scale features between different frames.proposed STDA. Let F t = F l t L l=1 represent the set of multi-scale feature maps at frame t, where F l t ∈ R C×H l ×W l is the feature map at level l. Given the query features p q and corresponding reference points z q , the spatio-temporal multiscale attention is given as:where m represents multi-head index and k is sampling point index. W m is a linear layer, A tlqk indicates attention weight of sampling point, and Δp tlqk indicates sample offset of sampling point. φ l normalizes the coordinates p q by the scale of feature map F l t . The sampling offset Δp tlqk is predicted by the query feature z q with a linear layer. The attention weight A tlqk is predicted by feeding query feature z q to a linear layer and a softmax layer. As a result, the sum of attention weights is equal to one as(Compared to the standard deformable attention, the proposed spatial-temporal deformable attention fully exploits spatial information within frame and temporal information across frames."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.2,Spatial-Temporal Deformable Attention Based Encoder and Decoder,"Here, we integrate the proposed spatial-temporal deformable attention (STDA) into encoder and decoder (called ST-Encoder and ST-Decoder). As shown in Fig. 1(b), ST-Encoder takes deep multi-scale feature maps F k-1 , F k , F k+1 , F r1 , F r2 , F r3 as inputs. Afterwards, we employ STDA to perform spatial and temporal fusion and generate the fused multi-scale feature maps, where the query corresponds to each pixel in multi-scale feature maps. Then, the fused feature map goes through a feed-forward network (FFN) to generate the output feature mapsSimilar to the original deformable DETR, we adopt cascade structure to stack six STDA and FFN layers in ST-Encoder.The ST-Decoder takes the output feature maps E k-1 , E k , E k+1 , E r1 , E r2 , E r3 and a set of learnable queries Q ∈ R N ×C as inputs. The learnable queries first go through a self-attention layer. Afterwards, STDA performs cross-attention operation between these feature maps and the queries, where the key elements are these output feature maps of ST-Encoder. Then, we employ a FFN layer to generate the prediction features P k ∈ R N ×C . We also stack six self-attention, STDA, and FFN layers in ST-Decoder for deep feature extraction."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.3,Multi-frame Prediction with Encoder Feature Shuffle,"As discussed above, the proposed STNet adopts six frames to predict the results of one frame. Although STNet fully exploits temporal information for improved breast lesion detection, it becomes time-consuming for multi-frame prediction. To accelerate the detection speed, we introduce multi-frame prediction with encoder feature shuffle during inference. Instead of going through the entire network several times, we first share deep multi-scale feature maps before encoder and second perform the decoder several times for multi-frame prediction. To perform multi-frame prediction only in the decoder, we propose the encoder feature shuffle operation shown in Fig. 1(d). By exchanging the order of neighboring frame I k-1 , I k , I k+1 , the decoder can predict the results of three neighboring frames, respectively. Compared to the original STNet, the proposed encoder feature shuffle strategy only employs decoder forward three frames and accelerates the inference speed."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.1,Dataset and Implementation Details,"Dataset. We conduct the experiments on the public BLUVD-186 dataset [9], comprising 186 videos including 112 malignant and 74 benign cases. The dataset has totally 25,458 ultrasound frames, where the number of frames in a video ranges from 28 to 413. The videos encompass a comprehensive tumor scan, from its initial appearance to its largest section and eventual disappearance. All videos were captured using PHILIPS TIS L9-3 and LOGIQ-E9. The grounding-truths in a frame, including breast lesion bounding-boxes and corresponding categories, are labeled by two pathologists, which have eight years of professional background in the field of breast pathology. We adopt the same dataset splits as in Table 1. State-of-the-art quantitative comparison of our approach with existing methods in literature on the BLUVD-186 dataset. Our approach achieves a superior performance on three different metrics. Compared to the recent CVA-Net [9], our approach obtains a gain of 3.9% in terms of overall AP. We show the best results in bold."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,,Method,"Type Backbone AP AP50 AP75 GFL [7] image ResNet-50 23.4 46.3 22.2 Cascade RPN [14] image ResNet-50 24.8 42.4 27.3 Faster R-CNN [12] image ResNet-50 25.2 49.2 22.3 VFNet [20] image ResNet-50 28.0 47.1 31.0 RetinaNet [8] image ResNet-50 29.5 50.4 32.4DFF [24] video ResNet-50 25.8 48.5 25.1 FGFA [23] video ResNet-50 26.1 49.7 27.0 SELSA [15] video ResNet-50 26.4 45.6 29.6 Temporal ROI Align [5] video ResNet-50 29.0 49.9 33.1 MEGA [2] video ResNet-50 32.3 57.2 35.7 CVA-Net [9] video ResNet-50 36. the previous work CVA-Net [9], to guarantee a fair comparison. Specifically, the testing set comprises 38 videos randomly selected from all 186 videos, while the rest of the videos are used as the training set.Evaluation Metrics. Three commonly-used metrics are employed for performance evaluation of breast lesion detection methods on the ultrasound videos, namely average precision (AP), AP 50 , and AP 75 .Implementation Details. We employ the ResNet-50 [6] pre-trained on Ima-geNet [3], and use Xavier [4] to initialize the remaining network parameters. To enhance the diversity of training data, all videos are randomly subjected to horizontal flipping, cropping, and resizing. Similar to that of CVA-Net, we employ a two-phase training strategy to achieve better convergence. In the first phase, we employ Adam optimizer to train the model for 8 epochs. We then fine-tune the model for another 20 epochs with the SGD optimizer. Throughout both phases of training, we adopt the consistent hyper-parameters, where the learning rate is 5 × 10 -5 and the weight decay is 1 × 10 -4 . We train the model on a single NVIDIA A100 GPU and set the batch size as 1."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.2,State-of-the-Art Comparison,"Our proposed approach is compared with eleven state-of-the-art methods, comprising image-based and video-based methods. We report the detection performance of these state-of-the-art methods generated by CVA-Net [9]. Specifically, CVA-Net acquires the detection performance of these methods by utilizing their publicly available codes or re-implementing them if no publicly available codes. Quantitative Comparisons. Table 1 presents the state-of-the-art quantitative comparison of our approach with the eleven existing breast lesion video detection methods in literature. As a general trend, video-based methods tend to yield higher average precision (AP), AP50, and AP75 scores compared to image-based breast lesion detection methods. Among the eleven existing methods, the recent CVA-Net [9] achieves the best overall AP score of 36.1, AP50 score of 65.1, and AP75 score of 38.5. Our proposed STNet method consistently outperforms CVA-Net [9] on all three metrics (AP, AP50, and AP75). Specifically, our STNet achieves a significant improvement in the overall AP score from 36.1 to 40.0, the AP50 score from 65.1 to 70.3, and the AP75 score from 38.5 to 43.3. The significant improvement demonstrates the efficacy of our approach for detecting breast lesions in ultrasound videos.Qualitative Comparisons. Figure 3 presents the qualitative breast lesion detection comparison between CVA-Net and our proposed approach on an ultrasound video containing the benign breast lesions. Moreover, we show the ground truth of each frame on the third row for reference. The first row of the figure shows that CVA-Net struggles to identify the breast lesions in the second and third frames. Further, although CVA-Net manages to identify the breast lesions in the first and fifth frames, the classification results are inaccurate (as highlighted by the blue rectangle in Fig. 3). In contrast, our STNet method in the second row of Fig. 3 accurately detects the breast lesions in all video frames and achieves accurate classification performance for each frame. Inference Speed Comparison. We present the inference speed comparison between our proposed STNet and CVA-Net on an NVIDIA RTX 3090 GPU using the same environment. We use FPS (frames per second) as the performance metric. Specifically, our proposed STNet achieves an averaged inference speed of 21.84 FPS, while CVA-Net achieves an averaged speed of 12.17 FPS. Our model operates around two times faster than CVA-Net, which we attribute to the ability of our model to predict three frames simultaneously."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.3,Ablation Study,"Effectiveness of STDA: To show the efficacy of our proposed STDA, we perform different ablation studies. The first baseline network, referred as ""Baseline + Single-frame"", uses the original deformable DETR and takes a single frame as input. The second baseline network, referred as ""Baseline + Multi-frame"", uses modified deformable DETR with multi-head attention module to fuse six input frames. For the third study, labeled ""ST-Encoder + DA-Decoder"", we retain the encoder with STDA in our model but replace the STDA in the decoder with the conventional deformable attention. Similarly, in the fourth study, labeled ""DA-Encoder + ST-Decoder"", we retain the decoder with STDA in our model but replace the STDA in the encoder with the conventional deformable attention. As shown in Table 2, the results show that ""ST-Encoder + DA-Decoder"" and ""DA-Encoder + ST-Decoder"" improve the AP by 4.7 and 5.6, respectively, compared to ""Baseline + Single-frame"". This demonstrates that STDA can effectively perform intra-frame and inter-frame multi-scale feature fusion, even when only partially adopted in the encoder or decoder. Furthermore, our proposed STNet improves the AP by 5.1 and 4.2 compared to ""ST-Encoder + DA-Decoder"" and ""DA-Encoder + ST-Decoder"", respectively, indicating that the integration of STDA in both the encoder and decoder is crucial for achieving superior detection performance."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,4.0,Conclusion,"We propose a novel breast lesion detection approach for ultrasound videos, termed as STNet, which performs local spatial-temporal feature fusion and deep feature aggregation in each stage of both encoder and decoder using our spatial-temporal deformable attention module. Additionally, we introduce the encoder feature shuffle strategy that enables multi-frame prediction during inference, thereby enabling us to accelerate the inference speed while maintaining better detection performance. The experiments conducted on a public breast lesion ultrasound video dataset show the efficacy of our STNet, resulting in a superior detection performance while operating at a fast inference speed. We believe STNet presents a promising solution and will help further promote future research in the direction of efficient and accurate breast lesion detection in videos."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,1.0,Introduction,"Deep Neural Networks (DNNs) have successfully been applied in research and industry for a multitude of complex tasks. This includes various medical F. Pahde and M. Dreyer-Contributed equally. Fig. 1. Our R2R life cycle for revealing and revising spurious behavior of any pretrained DNN. Firstly, we identify model weaknesses by finding either outliers in explanations using SpRAy (1a) or suspicious concepts using zoomed-in CRP concept visualizations (1b). Secondly (2), SpRAy clusters or collecting the top reference samples allows us to label artifactual samples and to compute an artifact CAV, which we use to model and localize the artifact in latent and input space, respectively. At this point, the artifact localization can be leveraged for (3) model correction, and (4) to evaluate the model's performance on a poisoned test set and measure its remaining attention on the artifact.applications for which DNNs have even shown to be superior to medical experts, such as with Melanoma detection [5]. However, the reasoning of these highly complex and non-linear models is generally not transparent [23,24], and as such, their decisions may be biased towards unintended or undesired features, potentially caused by shortcut learning [2,9,14,27]. Particularly in high-stake decision processes, such as medical applications, unreliable or poorly understood model behavior may pose severe security risks.The field of XAI brings light into the black boxes of DNNs and provides a better understanding of their decision processes. As such, local XAI methods reveal (input) features that are most relevant to a model, which, for image data, can be presented as heatmaps. In contrast, global XAI methods (e.g., [12,14]) reveal general prediction strategies employed or features encoded by a model, which is necessary for the identification and understanding of systematic (mis-)behavior. Acting on the insights from explanations, various methods have been introduced to correct for undesired model behavior [31]. While multiple approaches exist for either revealing or revising model biases, only few combine both steps, to be applicable as a framework. Such frameworks, however, either rely heavily on human feedback [25,29], are limited to specific bias types [2], or require laborintensive annotations for both model evaluation and correction [13,25].To that end, we propose Reveal to Revise (R2R), an iterative XAI life cycle requiring low amounts of human interaction that consists of four phases, illustrated in Fig. 1. Specifically, R2R allows to first (1) identify spurious model behavior and secondly, to (2) label and localize artifacts in an automated fashion. The generated annotations are then leveraged to (3) correct and (4) (re-)evaluate the model, followed by a repetition of the entire life cycle if required. For revealing model bias, we propose two orthogonal XAI approaches: While Spectral Relevance Analysis (SpRAy) [14] automatically finds outliers in model explanations (potentially caused by the use of spurious features), Concept Relevance Propagation (CRP) [1] precisely communicates the globally learned concepts of a DNN. For model revision, we apply and compare the methods of Class Artifact Compensation (ClArC) [2], Contextual Decomposition Explanation Penalization (CDEP) [20] and Right for the Right Reason (RRR) [22], penalizing attention on artifacts via ground truth masks automatically generated in step (2). The artifact masks are further used for evaluation on a poisoned test set and to measure the remaining attention on the bias. We demonstrate the applicability and high automation of R2R on two medical tasks, including Melanoma detection and bone age estimation, using the VGG-16, ResNet-18 and EfficientNet-B0 DNN architectures. In our experiments, we correct model behavior w.r.t. dataset-intrinsic, as well as synthetic artifacts in a controlled setting. Lastly, we showcase the R2R life cycle through multiple iterations, unveiling and unlearning different biases."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,2.0,Related Work,"Among other methods, e.g., leveraging auxiliary information [15,18,19,21], or training on de-biased representations [4,16], shortcut unlearning is often approached with XAI. The majority of related works introduce methods to either identify spurious behavior [1,14], or to align the model behavior with pre-defined priors [20,22], with only a few combining both, such as the eXplanatory Interactive Learning (XIL) framework [29] or the approach introduced by Anders et al. [2]. The former is based on presenting individual local explanations to a human, who, if necessary, provides feedback used for model correction [25,29]. However, studying individual predictions is slow and labor-extensive, limiting its practicability. In contrast, the authors of [2] use SpRAy [14] for the detection of spurious model behavior and labeling of artifactual samples. In addition to SpRAy, we suggest to study latent features of the model via CRP concept visualizations [1] as a tool for more fine-grained model inspection, catching systematic misbehavior which would not be visible through SpRAy clusters.Most model correction methods require dense annotations, such as labels for artifactual samples or artifact localization masks, which are either crafted heuristically or by hand [13,20]. In our R2R framework, we automate the annotation by following [2] for data labeling through SpRAy outlier clusters, or by collecting the most representative samples of bias concepts according to CRP. The spatial artifact localization is further automated by computing artifact heatmaps as outlined in Sect. 3.1, thereby considerably easing the step from bias identification to correction.Existing works for model correction measure the performance on the original or clean test set, with corrected models often showing an improved generalization [13,20]. A more targeted approach for measuring the artifact's influence is the evaluation on poisoned data [25], for which R2R is well suited by using its localization scheme to first extract artifacts and to then poison clean test samples. By precisely localizing artifacts, R2R further allows to measure the model's attention on an artifact through attribution heatmaps."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.0,Reveal to Revise Framework,"Our Reveal to Revise (R2R) framework comprises the entire XAI life cycle, including methods for (1) the identification of model bias, (2) artifact labeling and localization, (3) the correction of detected misbehavior, and (4) the evaluation of the improved model. To that end, we now describe the methods used for R2R."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.1,Data Artifact Identification and Localization,"The identification of spurious data artifacts using CRP concept visualizations or SpRAy clusters is firstly described, followed by our artifact localization approach. CRP Concept Visualizations. CRP [1] combines global concept visualization techniques with local feature attribution methods. This provides an understanding of the relevance of latent concepts for a prediction and their localization in the input. In this work, we use Layer-wise Relevance Propagation (LRP) [3] for feature attribution under CRP and for heatmaps in general, however, other local XAI methods can be used as well. Jointly with Relevance Maximization [1], CRP is well suited for the identification of spurious concepts by precisely narrowing down the input parts that have been most relevant for model inference, as shown in Fig. 1 (bottom left) for band-aid concepts, where irrelevant background is overlaid with black semi-transparent color. The collection of top-ranked reference samples for spurious concepts allows us to label artifactual data.Explanation Outliers Through SpRAy. Alternatively, SpRAy [14] is a strategy to find outliers in local explanations, which are likely to stem from spurious model behavior, such as the use of a Clever Hans features, i.e., features correlating with a certain class that are unrelated to the actual task. Following [2,14], we apply SpRAy by clustering latent attributions computed through LRP. The SpRAy clusters then naturally allow us to label data containing the bias.Artifact Localization. We automate artifact localization by training a Concept Activation Vector (CAV) h l to model the artifact in latent space of a layer l, representing the direction from artifactual to non-artifactual samples obtained from a linear classifier. The artifact localization is given by a modified backward pass on the biased model with LRP for an artifact sample x, where we initialize the relevances R l (x) at layer l aswith activations a l and element-wise multiplication operator •. This is equivalent to explaining the output from the linear classifier given as a l (x) • h l . Using a threshold, the resulting CAV heatmap can be further processed into a binary mask to crop out the artifact from any corrupted sample, as illustrated in Fig. 1 (bottom center )."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.2,Methods for Model Correction,"In the following, we present the methods used for mitigating model biases.ClArC for Latent Space Correction. Methods from the ClArC framework correct model (mis-)behavior w.r.t. an artifact by modeling its direction h in latent space using CAVs [12]. The framework consists of two methods, namely Augmentive ClArC (a-ClArC) and Projective ClArC (p-ClArC). While a-ClArC adds h l to the activations a l of layer l for all samples in a fine-tuning phase, hence teaching the model to be invariant towards that direction, p-ClArC suppresses the artifact direction during the test phase and does not require any fine-tuning. More precisely, the perturbed activations a l are given bywith perturbation strength γ(x) dependent on input x. Parameter γ(x) is chosen such that the activation in direction of the CAV is as high as the average value over non-artifactual or artifactual samples for p-ClArC or a-ClArC, respectively.RRR and CDEP for Correction Through Prior Knowledge. Model correction using RRR [22] or CDEP [20] is based on an additional λ-weighted loss term (besides the cross-entropy loss L CE ) for neural network training that aligns the use of features by the model f θ , described by an explanation exp θ , to a defined prior explanation exp prior . The authors of RRR propose to penalize the model's attention on unfavorable artifacts using the input gradient w.r.t. the cross-entropy loss, leading towith a binary mask M prior (x) localizing an artifact and class label y true . Alternatively, CDEP [20] proposes to use CD [17] importance scores β(x s ) for a feature subset x s based on the forward pass instead of gradient to align the model's attention. Penalizing artifact features via masked input x M results in(4)"
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.0,Experiments,"The experimental section is divided into the two parts of (1) identification, mitigation and evaluation of spurious model behavior with various correction methods and (2) showcasing the whole R2R framework in an iterative fashion. "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.1,Experimental Setup,"We train VGG-16 [26], ResNet-18 [11] and EfficientNet-B0 [28] models on the ISIC 2019 dataset [7,8,30] for skin lesion classification and Pediatric Bone Age dataset [10] for bone age estimation based on hand radiographs. Besides evaluating our methodology on data-intrinsic artifacts occurring in these datasets, we artificially insert an artifact into data samples in a controlled setting. Specifically, we insert a ""Clever Hans"" text (shown in Fig. 2) into a subset of training samples of one specific class. See Appendix A.1 for additional experiment details."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.2,Revealing and Revising Spurious Model Behavior,"Revealing Bias: In the first step of the R2R life cycle, we can reveal the use of several artifacts by the examined models, including the well-known band-aid, ruler and skin marker [6] and our synthetic Clever Hans for the ISIC dataset, as shown in Fig. 2 for VGG-16. Here, we show concept visualizations and cropped out artifacts based on our automatic artifact localization scheme described in Sect. 3.1. The ""band-aid"" use can be further identified via SpRAy, as illustrated in Fig. 3 (right). Besides the synthetic Clever Hans for bone age classification, we encountered the use of ""L"" markings, resulting from physical lead markers placed by radiologist to specify the anatomical side. Interestingly, the ""L"" markings are larger for hands of younger children, as all hands are scaled to similar size [10], offering the model to learn a shortcut by estimating the bone age based on the relative size of the ""L"" markings, instead of valid features. While we revealed the ""L"" marking bias using CRP, we did not find corresponding SpRAy clusters, underlining the importance of both approaches for model investigation.Revising Model Behavior: Having revealed spurious behavior, we now revise the models, beginning with model correction. Specifically, we correct for the band-aid, ""L"" markings as well as synthetic artifacts. The skin marker and ruler  "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.3,Iterative Model Correction with R2R,"Showcasing the full R2R life cycle (as shown in Fig. 1), we now perform multiple R2R iterations, revealing and revising undesired model behavior step by step. Specifically, we successively correct the VGG-16 model w.r.t. the skin marker, band-aid, and ruler artifacts discovered in Sect. 4.2 using RRR. In order to prevent the model from re-learning previously unlearned artifacts, we keep the previous artifact-specific RRR losses intact. Thus, we are able to correct for all artifacts, with evaluation results given in Appendix A.2, applying the same metrics as in Sect. 4.2. In Fig. 3, we show exemplary attribution heatmaps for all artifacts after each iteration. While there are large amounts of relevance on all artifacts initially, it can successfully be reduced in the according iterations to correct the model behavior w.r.t. skin marker (SM), band-aids (BA), and rulers (R). It is to note, that correcting for the skin marker also (slightly) improved the model w.r.t. other artifacts, which might result from corresponding latent features that are not independent, as shown by CRP visualizations in Fig. 2 for skin marker. Moreover, we show the SpRAy embedding of training samples after the first iteration in Fig. 3 (right), revealing an isolated cluster with samples containing the band-aid artifact, which dissipates after the correction step."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,5.0,Conclusion,"We present R2R, an XAI life cycle to reveal and revise spurious model behavior requiring minimal human interaction via high automation. To reveal model bias, R2R relies on CRP and SpRAy. Whereas SpRAy automatically points out Clever Hans behavior by analyzing large sets of attribution data, CRP allows for a finegrained investigation of spurious concepts learned by a model. Moreover, CRP is ideal for large datasets, as the concept space dimension remains constant. By automatically localizing artifacts, we successfully perform model revision, thereby reducing attention on the artifact and leading to improved performance on corrupted data. When applying R2R iteratively, we did not find the emergence of new biases, which, however, might happen if larger parts of the model are finetuned or retrained to correct strong biases. Future research directions include the application to non-localizable artifacts, and addressing fairness issues in DNNs."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,,Acknowledgements,". This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B), BIFOLD (01IS18025A, 01IS18037I)]; the European Union's Horizon 2020 research and innovation programme (EU Horizon 2020) as grant [iToBoS (965221)]; the European Union's Horizon 2022 research and innovation programme (EU Horizon Europe) as grant [TEMA (101093003)]; the state of Berlin within the innovation support program ProFIT (IBB) as grant [BerDiBa (10174498)]; and the German Research Foundation [DFG KI-FOR 5363]."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 56. We evaluate the effectiveness of model corrections based on two metrics: the attributed fraction of relevance to artifacts and prediction performance on both the original and a poisoned test set (in terms of F1-score and accuracy). Whereas in the synthetic case, we simply insert the artifact into all samples to poison the test set, data-intrinsic artifacts are cropped from random artifactual samples using our artifact localization strategy. Note that artifacts might overlap clinically informative features in poisoned samples, limiting the comparability of poisoned and original test performance. As shown in Tab. 1 (ISIC 2019) and Appendix A.2 (Bone Age), we are generally able to improve model behavior with all methods. The only exception is the synthetic artifact for VGG-16, where only RRR mitigates the bias to a certain extent, indicating that the artifact signal is too strong for the model. Here, fine-tuning only the last layer is not sufficient to learn alternative prediction strategies. Interestingly, despite successfully decreasing the models' output sensitivity towards artifacts,"
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,1.0,Introduction,"Deep learning for medical image analysis is a key tool to facilitate precision medicine and support clinical decision making. However, it has become increasingly evident that biases in the training data can lead to obstacles for clinical implementation. In this work, we define bias as a property of the data (e.g., class/attribute imbalance, spurious correlations) used for training a model that can lead to shortcut learning and/or failure to adequately represent data subgroups, which may lead to reduced generalizability and/or fairness when applied in real-world scenarios. For instance, such biases have been shown to lead to poor generalization capabilities of models evaluated on cohorts with sociodemographic population statistics different to those that it was trained on [9], which can lead to systematic misdiagnosis of subpopulations [16,18]. Moreover, image acquisition biases can act as spurious correlations to the target (shortcut learning) [8,23].Due to these problems, a plethora of research has recently gone towards bias mitigation [13,14,25,26] and data harmonization [2,5,23]. However, the utility of real-world medical images to assess and address bias-related challenges is often limited and may not be a comprehensive or sustainable solution. This is because all real-world datasets inherently suffer from biases that can be related to cohort selection, varying scanners and protocols, biases in ""ground truth"" labels, or any other (un-)known confounding factors associated with the data or the labels. Additionally, many medical imaging datasets do not contain suitable sociodemographic information or representation to adequately investigate the full range of bias scenarios that could be encountered in practice, especially when considering intersectional analyses [22]. Moreover, limitations in the seminal work on underdiagnosis disparities in deep learning models for chest X-ray analysis [18] have been identified since the various sources of bias present in the dataset could not be effectively distinguished from algorithmic bias [3] and known confounding factors were not rigorously accounted for [15]. However, even when known confounders such as disease prevalence between groups are considered, it may not be possible to adequately correct for them and unknown confounders and associated spurious correlations may still exist that go unaccounted for, such as annotation bias in labels used for training. Thus, it is very challenging to understand how biases in medical image data affect deep learning pipelines, especially if it is unknown what biases are present in the dataset, their magnitude and frequency, and how to correct them. As noted by various researchers, ""understanding the root cause of bias [. . . ] is a key step towards eliminating that bias"" [19]. Therefore, there is a need for a resource that enables researchers to objectively study how biases in medical images affect deep learning models, without the limitations associated with real-world datasets. As a first step towards addressing this need, we propose a flexible framework for generating synthetic neuroimaging data with controlled simulation of realistic biases.Current methods that have been proposed for fully controlled simulation of features in deep learning datasets, where generating factors can be fully disentangled and are well known in advance, are largely limited to toy problems or MNIST-like scenarios [4]. On the other hand, a considerable amount of recent research has gone into the supervised and unsupervised disentanglement of generating factors of medical images with generative models that can subsequently be used to synthesize data with specific factor variations [7,11]. However, in such setups, unknown biases could still exist, the true generating factors of real-world data are usually unknown, and it is often impossible to spatially localize an effect. Therefore, we believe that such standard generative models do not offer the flexibility and control of the data generation mechanism that is required to fully analyze the effect of data biases on deep learning models. With our proposed framework, we aim to provide a method for synthesizing realistic image data with a fidelity similar to standard generative models, while still providing a high level of flexibility and control over the type, scale, and proportions of simulated bias features that is usually only available in MNIST-like setups.In this work, we simulate brain magnetic resonance (MR) images with regionspecific morphology variations representing disease and bias effects. We also introduce global morphological variation representing distinct synthetic subjects. This option facilitates bridging the relationship between understanding the impacts of biases alone, and how biases combine with real-world variation when training deep learning models. We utilize neuroimaging data as an initial use case and focus on morphological biases in this work. However, the proposed modular framework is not limited to neuroimaging problems and could be modified to introduce other bias effects, such as gray value effects caused by acquisition parameters or pathologies. Ultimately, this framework can serve as a tool for generating datasets to facilitate analysis of how deep learning models handle various sources of bias. With complete control over the number of samples, types of bias, number of subgroups with different biases, intersections of biased subgroups, and strength and proportion of bias in target classes, datasets generated with this framework can be used as a tool for evaluating how proposed or state-of-the-art models are affected by biases in terms of performance, explainable AI, uncertainty, etc., as well as for benchmarking bias mitigation and data harmonization strategies on a wide range of realistic, controlled scenarios.The contributions of this paper are summarized as follows: (1) We propose a flexible framework for simulating brain MR datasets, which contain variable morphological disease and bias effects, as a first step towards the controlled and systematic study of how biases in medical imaging data affect deep learning pipelines. (2) We show how this modular framework can be customized to facilitate the investigation of a vast range of data cases that can lead to biased deep learning models. (3) We provide empirical evidence that data generated using this framework can be used to mimic realistic morphological biases in neuroimaging that lead to undesirable performance in a convolutional neural network, and show how these biases can be investigated with explainable AI methods."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,2.0,Methods,"The purpose of the proposed framework is to generate a dataset for a multi-class classification problem consisting of synthetic T1-weighted MR images, with N images I i and associated labels corresponding to m disease classes. For simplicity, in this description of the methods, we focus on the binary classification task (m=2) with disease labels corresponding to disease (D) and no disease (ND). All images are derived by applying non-linear diffeomorphic transformations to a template image I T , which represents an average brain morphology. More specifically, we consider three types of transformations: (1) ϕ S , a subject morphology, (2) ϕ D , a disease (target) effect, and (3) ϕ B , a bias effect. ϕ S is a global nonlinear transformation that deforms I T into a (simulated) subject morphology. In contrast, ϕ D and ϕ B are spatially localized deformations that only modify I T locally to introduce an effect (ϕ D ) that can be used to differentiate disease classes, and a bias effect (ϕ B ). In our setup, each synthetic image is generated by sampling the transformations ϕ S , ϕ D , and optionally ϕ B from dedicated generative models (Fig. 1A and Suppl. Mat. Fig. 1). Moreover, we assume that all diffeomorphic transformations are parameterized via stationary velocity fieldse.g., ϕ S = exp(v S ), where v S denotes the velocity field and exp(•) is the group exponential map from the Log-Euclidean framework, which can be efficiently computed via the scaling-and-squaring algorithm; see [1]. The resulting dataset is defined by the user-specified sample size, number of target disease classes, number of subgroups within the dataset containing bias effects, whether intersubject variability effects are introduced to the datasets, types and degree of each respective effect, and proportions of each respective class and bias group.Principal Component Analysis-Based Generative Models for Simulating Effects/Variability. To apply anatomically realistic morphological deformations to our template neuroimaging dataset in this work, we fit a principal component analysis (PCA) to the velocity fields of real T1-weighted MR images of different healthy subjects, which were non-linearly registered to the template image I T . We treat the resulting low-dimensional affine subspace model as a generative model and sample velocity fields representing a range of real anatomical variation from it. For region-specific effects (ϕ D and ϕ B ), the real T1-weighted MR image velocity fields within the regions defined by a label atlas are masked prior to PCA fitting, whereas the full brain is used in the PCA model for simulating subject morphology (ϕ S ). Thus, by sampling velocity fields v D , v B , and v S from the latent space of the respective subspace models, we can model disease, bias, and subject morphology as variations within an expected extent of human neuroanatomy.Disease and Bias Effects. We model disease (ϕ D ) and bias (ϕ B ) effects as morphological deformations localized to specific brain regions. We also assume that datasets belonging to each disease class have these localized effects sampled from respective distributions in a bimodal Gaussian mixture model within the PCA subspace of the disease effect model. We assume that bias effects are introduced as an additional morphological deformation in a separate brain region, and that these effects are sampled from a Gaussian distribution within the PCA subspace of the bias effect model. In general, an arbitrary number of target classes and bias groups can be introduced to the datasets in a similar sampling procedure.Subject Morphology. To better emulate anatomical variation in clinical data and warrant the use of deep learning models, global morphological variation representing distinct subjects (ϕ S ) are applied to the entire anatomy within I T . These are also sampled from a Gaussian distribution within the PCA subspace of the dedicated subject morphology model.Introducing Effects to the Template Image. The sparsely defined velocity fields for the disease and bias effects, v D and v B , are densified using the scattered grid B-spline method [10] to produce a dense velocity field that includes both effects (if present). If inter-subject variability is desired, the conjugate action mechanism [12] is used to transport the deformation field to the ""subject"" space, where the ""subject"" is generated using the sampled v S /ϕ S from the subject morphology PCA model. Framework Customization. For this initial work, we utilize velocity fields from real-world datasets to simulate anatomically realistic effects representing disease features, bias features, and subject morphology via different PCA-based generative models. Although real-world datasets do contain biases, the way in which we propose introducing these effects into the synthetic dataset is highly controlled in such a way where it is known exactly which and how many regions represent either disease or bias effects. Thus, this approach enables a controlled study of bias while benefiting from the utilization of 3D medical images that are representative of real-world clinical data. Moreover, due to the modularity of the proposed framework, such effects can also be introduced through a variety of other methods for generating deformation fields, ranging from highly precise but simple (e.g., single displacement vectors) to more realistic but increasingly complex approaches (e.g., generative models). Furthermore, in this work, we simulate morphological changes in brain images via diffeomorphic transformations as a use case, but the framework can be adapted to other disease or bias effects that would alter the topology (e.g., gray value changes or lesions). Moreover, other imaging modalities or body regions (e.g., cardiac MRI) as well as other generative models (e.g., generative adversarial networks) could be integrated."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,3.0,Experiments and Results,"To evaluate our synthetic datasets in a deep learning pipeline, we trained a CNN to predict whether images from biased datasets belong to the disease (D) or no disease (ND) class. More precisely, we evaluated (1) how the proportion of datasets containing bias features within the D class, and (2) how the spatial proximity between the disease region and bias region affect the performance and explainability (XAI) results of a CNN trained to classify D from ND cases (Fig. 1B). All experiments were performed with Keras/Tensorflow v. 2.10. Simulated Datasets. The SRI24 Normal Adult Brain Anatomy atlas [17] was used as the template image and each PCA model for sampling morphological effects was trained on T1-weighted MRI data from 50 subjects who were part of the IXI database of healthy individuals 1 . Velocity fields for this dataset were estimated by utilizing ITK's VariationalRegistration framework [6,24]. The left insular cortex was selected as the brain region for the disease effect, and the brain regions used to model bias effects were either the left putamen, right putamen, or right postcentral gyrus as defined by the LPB40 atlas [20], depending on the desired spatial proximity. The datasets belonging to the D and ND classes had effects sampled from N (0, 1) and N (2, 1), respectively, along the first principal component of the generative model for the disease region, and the datasets with bias features had effects sampled from N (2, 1) along the first principal components of the models for the respective bias regions. Inter-subject variability effects were sampled from a Gaussian distribution of N (0, 1) along the first 10 principal components of the subject morphology generative model. Experiments. To evaluate the effect on model performance and XAI in relation to the proportion of datasets containing bias features within the disease class, the generated datasets had either 60% or 80% of the simulated images from the D class containing the bias feature, with 30% of the simulated images from the ND class containing the bias feature for all experiments. To evaluate the effect of proximity between disease and bias regions, the distances between regions were defined as either near, middle, or far, for the left putamen, right putamen, and right postcentral gyrus, respectively (see Fig. 2B). Each simulated dataset contained a balanced representation of D and ND labels. The proximity experiments were performed under both 60% and 80% conditions defined by the proportion experiments. Model performance with the biased datasets was compared against a baseline experiment in which the datasets do not contain any simulated bias features but only the disease effects."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,,Model and Training.,"A CNN was used as a model for predicting whether datasets belonged to the D or ND class. The model consisted of 5 blocks each containing a convolutional layer with (3×3×3) kernel, batch normalization, sigmoid activation, and (2×2×2) max pooling. The convolutional filter sizes were 32, 64, 128, 256, and 512 for each respective block. The sixth block contained average pooling, dropout (rate=0.2), and a dense classification layer with softmax activation. Binary cross entropy loss, Adam optimizer (learning rate = 1e -4 ), and batch size 4 with early stopping based on validation loss (patience=30) were used for training. Each experiment simulated and used 500 datasets of voxel dimensions (173×211×155) with a 55%/15%/30% train/validation/test split, stratified by disease and bias labels.Evaluation. Model performance was evaluated using accuracy, sensitivity, and specificity computed for the aggregate test set, as well as separately for the bias (B) and no bias (NB) groups. Results are reported as the mean ± standard deviation of the models with 5 different weight initialization seeds on the same train/validation/test splits, following [18]. The SmoothGrad (SG) [21] method was used for XAI evaluation. Average SG maps were computed with 25 individual SG maps (5 from each seed) for the datasets in the test set with the bias feature, which were correctly identified as being in the disease class.Results and Discussion. The results of our evaluation are summarized in Fig. 2, with full quantitative results shown in Tables 1 and2 in the Supplementary Material. As seen in Fig. 2A, for all conditions with simulated dataset bias, the sensitivity is higher and specificity is lower within the B group, while the opposite was found for the NB group. Due to the higher representation of biased datasets in the D class, it seems reasonable to assume that the model uses the presence of bias features as a shortcut for predicting the disease state, and thus predicts the D class more often for the B group, resulting in fewer true negatives. Within the NB group, the absence of bias features seems to be also used as a shortcut for predicting the ND class, resulting in a higher number of ND class predictions and consequently fewer true positives. While these shortcuts are apparent in all experiments utilizing biased datasets, there is a stronger relationship between disease-bias region proximity and the degree of the shortcuts (measured by lower specificity in the bias group and lower sensitivity in the NB group) when the dataset has 80% of the D class containing the bias effect compared to 60%. In these 80% conditions, it was observed that the sensitivity in the NB group decreases as a function of region proximity, indicating that the model uses the absence of the bias effect as a shortcut for predicting the ND class more often when regions are further away. Likewise, specificity in the B group increases as a function of region proximity, indicating that the model uses the presence of the bias as a shortcut for predicting the D class label less often when regions are further away. A potential explanation for this may be that the CNN used has a spatially localized receptive field. Thus, when the bias and disease regions are near to each other, the network learns to associate them more closely and predicts the D class label more often for images with the bias effect. When the regions are farther apart from each other, the CNN becomes more tuned to recognize bias effects separately from disease effects. Thus, when the bias effect is not present, the model assumes the image belongs to the ND class. Furthermore, as seen in Fig. 2B, with 60% of the D class containing the bias feature, the SG maps show minor activation in the region with the bias effect, particularly in the far proximity condition. However, when 80% of the D class contains the bias feature, the SG maps highlight the bias regions considerably more intensely for all proximities analyzed. Even though the model still uses prediction shortcuts, which affect performance of the B and NB groups when 60% of the images from the D class exhibit the bias feature, the regions associated with the bias are less clearly identifiable in the group-averaged SG saliency maps, suggesting that XAI may not always be a reliable tool to uncover sources of bias in medical image data."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,4.0,Conclusion,"In this work, we presented a flexible and modular framework for simulating bias in medical imaging datasets using realistic morphological effects in neuroimaging as a use case. By sampling brain region-specific morphological variation representing the disease state and bias features from generative models in a controlled manner, we can generate synthetic datasets of arbitrary size and composition, which enables the investigation of a vast range of dataset bias scenarios and corresponding impacts on deep learning pipelines. Directions for future work with this framework are extensive and could include the analysis of more variations of bias proportions and proximities on alternate model architectures (e.g., vision transformers), evaluation of state-of-the-art bias mitigation strategies on various dataset compositions, as well as assessing other potential limitations of explainability methods as a tool for investigating bias. We believe that our work provides a strong foundation for advancing understanding of bias in deep learning for medical image analysis and consequently developing responsible models and methods for clinical use."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 46.
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,1.0,Introduction,"Quantitative Magnetic Resonance Imaging (QMRI) is used to identify tissue's intrinsic physical properties, including the spin-lattice magnetic relaxation time (T1), and the spin-spin magnetic relaxation time (T2) [23]. Compared to conventional weighted (qualitative) MRI that focuses on tissue's contrast of brightness and darkness, QMRI reveals tissue's intrinsic properties with quantitative values and associated physical interpretations. Since different tissues are characterized by their distinct properties values, QMRI shows great potential to reduce subjectivity, with advantages in many areas including diagnosis, tissue characterization, investigation of disease pathologies, personalized medical treatment, and therapeutic assessment [1,10,28].Despite various benefits, most QMRI approaches suffer from slow imaging speeds, and usually provide only a single intrinsic property at a time (e.g., quantification of T1 alone, followed by T2 alone), resulting in low throughput. Magnetic Resonance Fingerprinting (MRF) provides an alternative QMRI framework to achieve multi-property quantification simultaneously [16]. Given a pseudorandom radio frequency (RF) pulse sequence, a distinct magnetic response, i.e., fingerprint/signature, from each specific tissue is observed and then used to predict the target intrinsic tissue properties, e.g., T1 and T2 values. Therefore, multi-property quantification boils down to an inverse problem that aims to infer underlying tissue properties from the observed magnetic responses.In this work, we propose an MRI physics-regularized deep learning model for fast and robust MRF, called BlochNet as shown in Fig. 1. BlochNet adopts a supervised encoder-decoder framework where the encoder network solves the primary inverse problem that predicts tissue properties from input magnetic responses, while the decoder acts as a regularizer that guides the training of the encoder. In particular, the decoder leverages a Bloch equations-based MRI physics model to reconstruct the input responses from the estimated tissue properties, and compares the reconstructed inputs with the original input to provide an additional loss for the regularization purpose.The rationale underlying the design is that domain knowledge such as wellfounded physics principles bring additional useful constraints that can effectively reduce the solution space of an inverse problem. This contributes to an optimized solution, in particular, for an (ill-posed) inverse problem [11,12,15,18]. Our results verify that the proposed approach exhibits improved robustness and generalization performance in both synthetic and real MRF data in two different out-of-distribution (OOD) settings. The major contributions include:-The proposed BlochNet incorporates an MRI physics model to the decoding mechanism, which plays a role to regularize the training of the encoder. We expect that such a physics-based design can provide useful training signals for the encoder to better solve the MRF problems. -We improve the efficiency of the implementation of the Bloch equations, which reduce the computation overhead such that the MRI physics-based decoding model can be used directly as a differentiable module and trained end-to-end in neural networks (e.g., the decoder in BlochNet). -Compared to existing methods, BlochNet shows consistently better generalization performance across synthetic, phantom and real MRF data, and across different types of RF pulse sequences."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,2.0,Background and Related Works,"Since tissue properties lead to magnetic responses according to the MRI dynamics, quantifying tissue's properties via QMRI/MRF is a typical inverse problem, also an anti-causal task. The core idea of MRF is based on the fact that for each specific tissue, a pseudo-random pulse sequence leads to a unique magnetic response (i.e., magnetization along the temporal dimension) which can serve as an identifiable signal signature, analogous to a ""fingerprint"" for the corresponding tissue. Once the unique identifiable magnetic responses are obtained, the estimation of tissue properties reduces to a pattern recognition problem.Various approaches have been developed to solve the MRF problem, using either model-based techniques, e.g., dictionary matching (DM), compressive sensing, or learning-based / data-driven techniques.Model-Based Approaches. In the original MRF work [16], this task is approached via dictionary matching (DM) which finds the best matching entry in a pre-computed dictionary for each inquiry magnetic response. Accordingly, the best matching dictionary entry leads to multiple tissue properties directly via a look-up-table (LUT) operation. To alleviate the extensive computation overhead and storage burden, a number of model-based MRF approaches [7,20,25] were proposed to incorporate additional useful priors, e.g. sparsity, low rank, in order to improve reconstruction performance as well as reduce computational complexity.Learning-Based Approaches. To address some shortcomings of model-based methods, learning-based approaches have been proposed for fast MRF by replacing the dictionary with a compact neural network. In particular, motivated by the success of deep learning in a number of tasks, there is an emerging trend [5,6,9,24] that suggests to use a trained neural network as a substitute for the MRF dictionary and LUT, so that the time-consuming dictionary matching operation can be avoided and replaced by an efficient inference through a trained network. However, despite the good performance delivered by neural networks, most of the learning-based methods were designed without taking into account the MRI physics underlying the imaging process, and may inevitably suffer from some limitations, such as degraded robustness and generalizability.Physics-Informed Learning. Another highly related line of research is modelbased learning [21,22] which provides a promising path to integrate domain knowledge with learned priors, thereby fusing the benefits of model-based methods with learning-based methods. Typical examples include algorithm unrolling [21], physics-informed neural networks (PINNs) [22], and other variants. Incorporating physics priors into the neural network design and training has demonstrated benefits in a broad range of applications [3,17], in particular for medical imaging [5,28,29]. In a similar spirit, we aim to incorporate the physics model that describes the MRI dynamics into learning-based MRF approaches so that the learned model can demonstrate improved data efficiency, robustness and generalization. Note that, in contrast to standard PINNs which are used to solve a PDE, our task is to solve an inverse PDE. Furthermore, we propose to leverage the PDE as a regularization. (More related work and comparisons are provided in the Appendix.)"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.1,BlochNet: Regularized Networks by Physics-Based Decoding,"The proposed BlochNet adopts a supervised encoder-decoder framework where the encoder solves the primary inverse problem that predicts tissue properties from input magnetic responses, while the decoder solves an auxiliary task that reconstructs the inputs from the estimated tissue properties using the Bloch equations-based MRI physics model. We highlight that a sophisticated MRI physics model is tailored and exploited as the decoder. The rationale behind such a design lies in the fact that the data generation mechanism represented by MRI physics is a useful constraint that can effectively reduce the solution space of the inverse problem. Therefore, the physics-based decoder will act as a strong regularizer that can provide informative feedback and contribute to the training of a better encoder. We expect the physics prior can introduce a better and generalizable inductive bias to the encoder. Similar ideas have been explored in some other domains [11,12,18].Specifically, in the proposed approach, the encoder uses a three-layer fully connected neural network to address the inverse problem that predicts T1, T2 tissue properties from input magnetic responses. Given an enquiry magnetic response X n ∈ C L for the n-th voxel where L denotes the length of each magnetic response, e.g. L = 1000 in our experiments, the encoder E outputs predicted tissue properties Θn = { T 1 n , T 2 n } ∈ R p where p = 2 denotes the number of tissue properties to be predicted.Note that, the estimation of tissue properties Θ from magnetic responses X requires long enough sequences L > p to create identifiable signal evolutions that distinguish different tissues. Hence, this operation nonlinearly maps the magnetic responses from a high-dimensional manifold to a low-dimensional manifold.In contrast, the decoder reconstructs the input magnetic responses from the estimated tissue properties Θn by solving the Bloch equations [2]  where M = [M x , M y , M z ] denotes the magnetization vector. M 0 denotes the equilibrium magnetization; B denotes the magnetic field; and γ denotes the gyromagnetic ratio. (More details of Bloch equations are provided in the Appendix.)"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.2,Fast EPG for Solving Bloch Equations,"Since there is no general analytic solution to the Bloch equations, numerical solutions such as EPG formalism are often adopted. However, a limitation of the released EPG implementation [26] is its slow computation speed in solving the Bloch equations. To circumvent this, recurrent neural networks [14] and generative adversarial networks [27] have been applied as surrogates for the Bloch equation. However, these surrogate models require a lot of training data and may generate inaccurate magnetic responses on unseen tissue properties and RF pulse settings due to complex physics dynamics and potential overfitting risks.Instead, we adapt the EPG implementation [26] to achieve a much more efficient implementation, making it practical to use the exact MRI physics model as a decoder in the training procedure. Specifically, the adaptations involve incorporating the PyTorch jit package for efficient parallelization, using batch-wise computation for the 3 Bloch stages (including nutation+forced precession, rotation, and relaxation in Fourier domain), and handling complex values in Pytorch efficiently. The improvement leads to 500 times faster generation of magnetic responses for 1,000 sequences on CPU, making repeated EPG computations feasible during training. 1 (More details can be found in the Appendix.)"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.3,Loss Function,"The loss function consists of two parts: the mean squared error (MSE) between the ground truth and the predicted tissue properties, referred to as embedding loss, and the MSE between the input and the reconstructed signatures, referred to as reconstruction loss,"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.0,Experiment Results,"In this section, we perform an evaluation of the proposed method and conduct a comparison with other state-of-the-art MRF methods. We evaluate the generalization performance of all models across different data distributions and different RF pulse sequences. The evaluation metric is the MSE in log-scale, and therefore, the unit is the squared millisecond in log-scale. For our model, we use 3-layer encoder-decoder with varying hidden units, Adam optimizer (lr=1e-3), and maximum epochs of 100 with early stopping based on the validation set on GTX 1080 Ti. We performed ten independent trials, the results of which are presented in Table 1 and2. The associated standard deviations are provided in the Appendix. "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.1,Data Settings and Baseline Methods,"We exploit three types of data including synthetic data, phantom MRI data, and anatomical MRI data. In particular, synthetic data (around 80,000 samples) is used for training, while phantom data (85,645 samples) and anatomical data (7,499 samples) is for evaluation. More details about the three datasets are provided in the Appendix. We compare our approach with 6 representative state-of-the-art MRF methods, including dictionary matching (DM) [16], Fullyconnected deep neural network (FC) [6], Hybrid deep learning (HYDRA) [24] as well as two auto-encoder methods with RNN encoder and RNN decoder(RNN-RNN) and FC encoder and FC decoder(FC-FC), respectively."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.2,Experiments of Evaluating Generalization Performance,"We evaluate the generalization performance of various models on two types of experiment settings: 1) across different data distributions, including synthetic, phantom and anatomical MRF data; 2) across different RF pulse sequences with different flip angles. In addition, a series of ablation studies were conducted via comparison with other methods that use different types of decoders, as shown in Table 1 and Table 2, for example, comparing BlochNet (using a physics-based decoder) with FC (using no decoder) and FC-FC (using a learned decoder) to show the effect of the encoder and the effect of MRI physics.Generalization Across Different Data Distributions. Due to limited anatomical data with ground truth T1, T2 values, it is common practice to use a large amount of synthetic data to train models to avoid potential overfitting, and then perform validation on anatomical data [4,7,8,13,19,20,24,25]. Following the same routine, we perform model training on synthetic MRF data, followed by model testing on phantom and anatomical data, in order to evaluate the generalization performance of trained models across different data distributions. Table 1 includes the mean squared error (MSE) between the ground truth and predicted tissue properties for seven approaches on phantom and anatomical data (in log-scale). As shown in the table, the dictionary-matching approach gave the worst performance, because the pre-computed dictionary and LUT did not cover the OOD data samples that could be quite different from the already contained dictionary entries. Interestingly, the results show that the reconstruction loss provides benefits to between-data generalization for autoencoder models(FC-FC or RNN-RNN), in comparison with non-autoencoder models(FC or RNN), respectively, on both phantom and anatomical MRF data. Furthermore, our BlochNet outperforms all other models, indicating that reconstruction loss from the physics-based decoder has the best regularization effect that contributes to improved encoder training.Figure 3 shows the predicted tissue properties using various models on anatomical MRF data. Each individual model shows different prediction characteristics. Specifically, HYDRA suffers from a higher loss at the rim region of the  Generalization Across Different RF Pulse Sequences. In this experiment, we perform model training on one RF pulse sequence and evaluate the trained models on a different RF pulse sequence. Specifically, we adopted 3 different RF pulse sequences, including FISP [16], Spline5 [14], Spline11Noisy [14] with their flip angles shown in Figure 1 in the Appendix. More details are provided in Appendix. FISP is used exclusively in the testing stage, while Spline5 and Spline11Noisy are used exclusively in the training stage. Under such settings, the performance of our BlochNet and other six models is compared in Table 2.In spite of degraded performance for all models under different train and test RF pulse sequences, the results clearly show the advantage of autoencoder (FC-FC or RNN-RNN) models over non-autoencoder models(FC or RNN), which confirms the benefits of incorporating a decoder to derive the reconstruction loss as additional regularization. Furthermore, the proposed BlochNet demonstrates significant gains over competing methods in such challenging cases on both phantom and anatomical MRF data.In Fig. 4, FC model (left) makes poor predictions on both T1 and T2 values with high variance, because it cannot infer tissue properties from input signatures generated from different combinations of T1 and T2 values. Autoencoder(FC-FC) model (third column) shows more aligned and better inferences with lower variance, but still has high deviation between predicted and gold-standard values. In comparison, our BlochNet outputs predictions that are closest to the goldstandard with the lowest error. This confirms the benefits of our physics-based decoder that guides the encoder to learn the underlying anti-causal mechanism effectively."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,5.0,Discussion,"We present statistical significance tests on 10 trials for pairwise group comparisons using Tukey HSD test after the Normality Test and repeated ANOVA. For results in Table 1, corresponding to setting 1 (a relatively easy problem as the RF pulses used in training and testing are the same), our method is not always statistically better to every compared method, since all baseline methods can perform reasonably well. However, in setting 2, a much more challenging OOD setting where the RF pulses used in testing are different from those used in training and therefore can lead to different magnetic responses, our method is always statistically better (p-value < 0.001) than compared methods according to the results in Table 2. This demonstrates the robustness and generalizability of our method. As far as we are concerned, no existing approaches achieved satisfactory results in such an OOD case. While there is still ample room for improvement across all the methods, our approach took a critical step forward by incorporating physics knowledge. (More details can be found in the Appendix.) "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,6.0,Conclusion,"We propose BlochNet, a novel MRI physics-informed learning model, which consistently outperforms competing methods with better robustness and generalizability in MRF problems. In future work, we will consider k-space subsampling and incorporating spatial information for faster and more efficient QMRI/MRF."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 42.
DeDA: Deep Directed Accumulator,1.0,Introduction,"Over the past decade, we have observed the substantial success of Convolutional Neural Networks (CNNs) in a multitude of grid-based medical imaging applications, such as magnetic resonance imaging (MRI) reconstruction [20,27] and lesion segmentation [13,25]. Despite the effectiveness of general inductive biases like translation equivariance [15] and locality [16], the diverse nature of The gradient field map of the QSM images presents normalized gradient vectors (the darker the blue, the larger the gradient vector's magnitude). The two rightmost columns display gradient magnitude maps Vs and QSM value maps Vu processed by DA-TR (see Sect. diseases represented in medical images necessitates highly domain-specific knowledge. Consequently, the question of how to incorporate domain-specific inductive biases, or priors, beyond general ones into neural networks for medical image processing remains an open challenge.In this study, we strive to answer this question by addressing the identification problem associated with a specific type of multiple sclerosis (MS) lesion, referred to as a chronic active lesion, or rim+ lesion. Histopathology studies characterize rim+ lesions by an iron-rich rim of activated macrophages and microglia [2,6,9,14]. These lesions are visible with in-vivo quantitative susceptibility mapping (QSM) [7,22] and phase imaging techniques [1,2]. Notably, they display a paramagnetic hyperintense rim at the edge (see Fig. 1). Despite several efforts to tackle the issue [4,18,24], a clinically reliable solution remains elusive.Given the limited amount of data and high class imbalance, it's more advantageous to explicitly incorporate domain knowledge into the network as priors. As illustrated in Fig. 1, rim+ lesions distinguish themselves from rim-lesions in three primary aspects. Firstly, rim+ lesions exhibit a hyperintense ring-like structure at the lesion's edge on QSM. Secondly, a higher magnitude of gradients is observed near the edge of rim+ lesions, a feature not present in rim-lesions. Lastly, rim+ lesions are characterized by radially oriented gradients at the edge, whereas rim-lesions lack such structured orientations.In this work, we introduce the Deep Directed Accumulator (DeDA), a novel image processing operation. DeDA, symmetric to grid sampling within a neural network's forward-backward framework, explicitly encodes the aforementioned prior information. Given a feature map and sampling grids, DeDA creates an accumulator space, quantizes it into finite intervals, and accumulates feature values. DeDA can also be viewed as a generalized discrete Radon transform, as it accumulates values between two discrete feature spaces. Our contributions are twofold: Firstly, we present DeDA, a simple yet powerful method that augments neural networks' representation capacity by explicitly incorporating domain-specific priors. Secondly, our experimental results on rim+ lesion identification demonstrate a notable improvement of 10.1% in partial area under the receiver operating characteristic curve (pROC AUC) and a 10.2% improvement in area under the precision recall curve (PR AUC), outperforming existing state-of-the-art methods."
DeDA: Deep Directed Accumulator,2.0,Methodology,"Numerous signal processing techniques, including the Fourier transform, Radon transform, and Hough transform, map discrete signals from image space to another functional space. We call this new space accumulator space, where each cell's value in the new space constitutes a weighted sum of values from all cells in the original image space. For our purposes, an appealing feature of the accumulator space is that local convolutions within it, like those in Hough and sinogram spaces, result in global aggregation of structural features, such as lines, in the feature map space. This proves beneficial for incorporating geometric priors into neural networks. Differing from attention-based methods, this convolution in accumulator space explicitly captures long-range information through direct geometric prior parameterization."
DeDA: Deep Directed Accumulator,2.1,Differentiable Directed Accumulation,"The process of transforming an image to an accumulator space involves a critical step, directed accumulation (DA), in which a cell from the accumulator space is pointed by multiple cells from the image space. Figure 2, Eq. (1) and Eq. ( 3) have shown that this DA operation is a symmetric operation to the grid sampling [12] within the forward-backward learning framework, where the backward pass of DA possesses the same structure as the forward pass of grid sampling if only one sampling grid is given, and vice versa for the forward pass. In addition, DA is further generalized to allow multiple sampling grids to accumulate values from the source feature map. Here we briefly review the grid sampling method and then derive the proposed DeDA.Grid Sampling: Given a source feature map U ∈ R C×H×W , a sampling grid G ∈ R 2×H ×W = (G x , G y ) specifying pixel locations to read from U, and a kernel function K() defining the image interpolation, then the output value of a particular position (i, j) at the target feature map V ∈ R C×H ×W can be written as follows: where the kernel function K() can be replaced with any other specified kernels, e.g. integer sampling kernel δ(Here x + 0.5 rounds x to the nearest integer and δ() is the Kronecker delta function. The gradients with respect to U and G for back propagation can be defined accordingly [12].the number of grids), and a kernel function K(), the output value of a particular position (i, j) at the target feature map V can be written as follows:It is worth noting that the spatial dimension of the grid G[k] should be the same as that of U, but the first dimension of G[k] can be an arbitrary number as long as it aligns with the number of spatial dimensions of V, e.g. if givenBasically, the DeDA operation in Eq. ( 2) performs a tensor mapping by D : (U, G; K) → V, where K is the sampling kernel. For simplicity, function D() will be used to denote the DeDA forward for the rest of the paper.To allow back propagation for training networks with DeDA, the gradients with respect to U are derived using the chain rule as follows:The gradient tensor with respect to V is A. The structure of Eq. ( 3) reduces to Eq. ( 1) when N = 1, indicating DeDA's symmetry with grid sampling. Given identical transformations for each channel c in DeDA's forward and backward passes, we denote the feature map with spatial dimensions alone henceforth. , where is a small real value to avoid zero denominator. The mesh grids of U are denoted as M x (value range: (0, H -1)) and M y (value range: (0, W -1)). We can then generate a set of sampling grids as follows:whereand N = max(H, W ). Now the DeDA-based transformation of Rim (DA-TR) can be formulated as V s = D(S, G; K) and V u = D(U, G; K), where the integer sampling kernel is used. It is worth noting that feature and gradient magnitude values are accumulated separately due to differences of image intensity and gradients between rim+ and rim-lesions (see Fig. 1)."
DeDA: Deep Directed Accumulator,,Network Layer for DA-TR:,"To gain more representation ability and capture long-range contextual information, DA-TR is applied to both intermediate feature maps and original images. As can be seen from Fig. 3, image patches of lesions are processed through a set of convolutional layers with each consisting of a 3 × 3 × 3 or 1 × 1 × 1 convolution, a batch normalization [11] and a ReLU activation function, followed by a DA-TR layer and a 1 × 1 × 1 convolutional layer. The first 1×1×1 conv layer is used to fuse feature maps and original image patches for better feature embedding, and the second one is used to fuse DeDA transformed gradient magnitude maps V s and feature maps V u . It is worth noting that only in-plane rims are observed, and thus the DA-TR is performed on the 2D feature map slices along the axial direction."
DeDA: Deep Directed Accumulator,3.0,Experiments and Results,"For fair and consistent comparison, the dataset applied in the previous work [24] was asked for and used to demonstrate the performance of the proposed DeDAbased rim parameterization DA-TR. A total of 172 subjects were included in the dataset, and 177 lesions were identified as rim+ lesions and 3986 lesions were identified as rim-lesions, please refer to [24] for more details about the image acquisition and pre-processing."
DeDA: Deep Directed Accumulator,3.1,Comparator Methods and Implementation Details,"Comparator Methods: Three methods have been developed so far for rim+ lesion identification, of which APRL [18] and RimNet [4] are on phase imaging and QSMRim-Net [24] is on QSM. In comparison with these methods, we use QSM along with T2-FLAIR images as the network inputs for RimNet and QSMRimNet, and use the QSM image to extract first-order radiomic features for APRL. Furthermore, we applied residual networks (ResNet) [10], vision transformer (ViT) [8], Swin transformer [17], and Nested transformer [28] as backbone architecture for our application, and determined that ResNet with 18 convolution layers works the best. Transformer-based networks with fewer inductive biases rely heavily on the use of a large training dataset or depends strongly on the feature reuse [19], as a result, these networks as well as CNNs with deeper structures are prone to overfit small datasets.Implementation Details: A stratified five-fold cross-validation procedure was applied to train and validate the performance, and all experiments including ablation study were carried out within this setting. Each lesion was cropped into patches with a fixed size of 32 × 32 × 8 voxels. Random flipping, random affine transformation and random Gaussian blurring were used to augment our data. More details of the training procedure can be found out in the supplementary materials."
DeDA: Deep Directed Accumulator,3.2,Results and Ablation Study,"Lesion-wise Results: To evaluate the performance of each method and produce clinically relevant results, pROC curves with false positive rates (FPRs) in the range of (0, 0.1) and PR curves of the different validation folds were interpolated using piece-wise constant interpolation and averaged to show the overall performance at the lesion level. For each curve, AUC was computed directly from the interpolated and averaged curves. The binary indicators of rim+/rimlesions were generated by thresholding the model probabilities to maximize the F 1 score, where F 1 = 2• precision • sensitivity precision + sensitivity . In addition, accuracy, F1 score, sensitivity, specificity, and precision were used to characterize the performance of each method. Table 1 and Fig. 4 show the lesion-wise performance metrics of the proposed methods in comparison with the other methods. DA-TR-Net outperformed the other competitors in all evaluation metrics. With a slightly higher overall accuracy and specificity with other methods, DA-TR-Net resulted in a 5.5%, 15.4% and 39.4% improvement in F 1 score, 10.1%, 13.6% and 30.0% improvement in pROC (FPR < 0.1) AUC, and 10.2%, 18.5% and 54.0% improvement in PR AUC compared to QSMRimNet, RimNet and APRL, respectively."
DeDA: Deep Directed Accumulator,,Subject-wise Results:,"We also evaluated the performance at the subjectlevel. Pearson's correlation coefficient was used to measure the correlation model predicted count and human expert count. Mean Squared Error (MSE) was also used to measure the averaged accuracy for the model predicted count. Figure 4a shows the scatter-plot for the predicted count v.s. the human expert count, along with the identity line, and the Pearson's correlation coefficient (ρ) for DA-TR-Net was ρ = 0.93(95%CI : 0.90, 0.95) As can be seen from Table 1, the Pearson's correlations and MSE for the proposed DA-TR-Net was found higher than other competitors. This demonstrates that the performance of DA-TR-Net at the subject-level is statistically significantly higher than that of APRL, Rim-Net, and QSMRim-Net (Table 2)."
DeDA: Deep Directed Accumulator,,Ablation Study:,"We conducted an ablation study to investigate the effects of each component accompanied with DA-TR. First, we examined the effects of applying the proposed DA-TR to the latent feature maps and raw images. Second, we examined the effects of using V u and V s , because rim+ lesions differ from rim-lesions in both gradient magnitudes and values at the edge of the lesion. We then investigated how multi-radius rim parameterization can affect the results, as the size of rim+ lesions vary greatly with a radius from 5 to 15 among different subjects. Results from models #1, #2 and #4 show that the rim parametrization DA-TR is useful for rim+ identification, and DA-TR used in the latent feature map space performs even better. Comparing model #3 and #4, one can see that accumulating both gradient magnitudes and feature values is beneficial. The consistent performance improvement from model #4 to #5 and from model #5 to #6 has demonstrated the effectiveness of applying multi-radius rim parameterization. More results on backbone networks can be found in the appendix."
DeDA: Deep Directed Accumulator,3.3,Discussions,"Medical images often require processing of a primary target or region of interest (ROI), such as rims, left ventricles, or tumors. These ROIs frequently exhibit distinct geometric structures [26] or possess specific spatial relationships [25] with their surroundings. Capturing these characteristics poses a challenge for modern neural networks, especially given limited and imbalanced training data. While differentiable grid sampling [12] can tackle some of these issues within a certain scope, another major class involving transformations (e.g. Hough transform [3]) that necessitate directed accumulation is overlooked. Our proposed DeDA bridges this gap, enabling the use of image transformations with directed accumulation within a neural network. This allows for the parametrization of geometric shapes and the modeling of spatial correlations in a differentiable manner.While the study focuses on rim+ lesion identification, the proposed DeDA can be extended to other applications. These include the utilization of polar transformation for skin lesion recognition/segmentation, symmetric circular transformation for cardiac image registration [23], parabola transformation for curvilinear structure segmentation [21], and high-dimensional bilateral filtering [5]."
DeDA: Deep Directed Accumulator,4.0,Conclusions,"We present DeDA, an image processing operation that helps parameterize rim and effectively incorporates prior information into networks through a value accumulation process. The experimental results demonstrate that DeDA surpasses existing state-of-the-art methods in all evaluation metrics by a significant margin. Furthermore, DeDA's versatility extends beyond lesion identification and can be applied in other image processing applications such as Hough transform, bilateral grid, and Polar transform. We are excited about the potential of DeDA to advance numerous medical applications and other image processing tasks."
DeDA: Deep Directed Accumulator,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 72.
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,1.0,Introduction,"Deep learning techniques have achieved unprecedented success in the field of medical image classification, but this is largely due to large amount of annotated data [5,18,20]. However, obtaining large amounts of high-quality annotated data is usually expensive and time-consuming, especially in the field of pathology image processing [5,[12][13][14]18]. Therefore, a very important issue is how to obtain the highest model performance with a limited annotation budget. The unlabeled sample pool contains K target categories (red-boxed images) and L non-target categories (blue-boxed images). Existing AL methods cannot accurately distinguish whether the samples are from the target classes or not, thus querying a large number of non-target samples and wasting the annotation budget, while our method can accurately query samples from the target categories. (Color figure online)Active learning (AL) is an effective approach to address this issue from a data selection perspective, which selects the most informative samples from an unlabeled sample pool for experts to label and improves the performance of the trained model with reduced labeling cost [1,2,9,10,16,17,19]. However, existing AL methods usually work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model, which does not meet the needs of some real-world scenarios [11]. Figure 1 shows an AL scenario for pathology image classification in an open world, which is very common in clinical practice. In this scenario, the Whole Slide Images (WSIs) are cut into many small patches that compose the unlabeled sample pool, where each patch may belong to tumor, lymph, normal tissue, fat, stroma, debris, background, and many other categories. However, it is not necessary to perform finegrained annotation and classification for all categories in clinical applications. For example, in the cell classification task, only patches of tumor, lymphatic and normal cells need to be labeled and classified by the target model. Since the nontarget patches are not necessary for training the classifier, labeling them would waste a large amount of budget. We call this scenario in which the unlabeled pool consists of both target class and non-target class samples open-set AL problem. Most existing AL algorithms can only work in the closed-set setting. Even worse, in the open-set setting, they even query more non-target samples because these samples tend to have greater uncertainty compared to the target class samples [11]. Therefore, for real-world open-set pathology image classification scenarios, an AL method that can accurately query the most informative samples from the target classes is urgently needed.Recently, Ning et al. [11] proposed the first AL algorithm for open-set annotation in the field of natural images. They first trained a network to detect target class samples using a small number of initially labeled samples, and then modeled the maximum activation value (MAV) distribution of each sample using a Gaussian mixture model [15] (GMM) to actively select the most deterministic target class samples for labeling. Although promising performance is achieved, their detection of target class samples is based on the activation layer values of the detection network which has limited accuracy and high uncertainty with small initial training samples.In this paper, we propose a novel AL framework under an open-set scenario, and denote it as OpenAL, which cannot only query as many target class samples as possible but also query the most informative samples from the target classes. OpenAL adopts an iterative query paradigm and uses a two-stage sample selection strategy in each query. In the first stage, we do not rely on a detection network to select target class samples and instead, we propose a feature-based target sample selection strategy. Specifically, we first train a feature extractor using all samples in a self-supervised learning manner, and map all samples to the feature space. There are three types of samples in the feature space, the unlabeled samples, the target class samples labeled in previous iterations, and the non-target class samples queried in previous iterations but not being labeled. Then we select the unlabeled samples that are close to the target class samples and far from the non-target class samples to form a candidate set. In the second stage, we select the most informative samples from the candidate set by utilizing a model-based informative sample selection strategy. In this stage, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained with the target class samples labeled in previous iterations, and select the samples with the highest model uncertainty as the final selected samples in this round of query. After the second stage, the queried samples are sent for annotation, which includes distinguishing target and non-target class samples and giving a fine-grained label to every target class sample. After that, we train the classifier again using all the fine-grained labeled target class samples.We conducted two experiments with different matching ratios (ratio of the number of target class samples to the total number of samples) on a public 9-class colorectal cancer pathology image dataset. The experimental results demonstrate that OpenAL can significantly improve the query quality of target class samples and obtain higher performance with equivalent labeling cost compared with the current state-of-the-art AL methods. To the best of our knowledge, this is the first open-set AL work in the field of pathology image analysis."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.0,Method,"We consider the AL task for pathology image classification in an open-set scenario. The unlabeled sample pool P U consists of K classes of target samples and L classes of non-target samples (usually, K < L). N iterative queries are performed to query a fixed number of samples in each iteration, and the objective is to select as many target class samples as possible from P U in each query, while selecting as many informative samples as possible in the target class samples. Each queried sample is given to experts for labeling, and the experts will give fine-grained category labels for target class samples, while only giving a ""non-target class samples"" label for non-target class samples. "
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.1,Framework Overview,"Figure 2 illustrates the workflow of the proposed method, OpenAL. OpenAL performs a total of N iterative queries, and each query is divided into two stages. In Stage 1, OpenAL uses a feature-based target sample selection (FTSS) strategy to query the target class samples from the unlabeled sample pool to form a candidate set. Specifically, we first train a feature extractor with all samples by self-supervised learning, and map all samples to the feature space. Then we model the distribution of all unlabeled samples, all labeled target class samples from previous iterations, and all non-target class samples queried in previous iterations in the feature space, and select the unlabeled samples that are close to the target class samples and far from the non-target class samples. In Stage 2, OpenAL adopts a model-based informative sample selection (MISS) strategy. Specifically, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained in the last iteration, and select the samples with the highest model uncertainty as the final selected samples, which are sent to experts for annotation. After obtaining new labeled samples, we train the classifier using all fine-grained labeled target class samples with cross-entropy as the loss function. The FTSS strategy is described in Sect. 2.2, and the MISS strategy is described in Sect. 2.3."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.2,Feature-Based Target Sample Selection,"Self-supervised Feature Representation. First, we use all samples to train a feature extractor by self-supervised learning and map all samples to the latent feature space. Here, we adopt DINO [3,4] as the self-supervised network because of its outstanding performance.Sample Scoring and Selection in the Feature Space. Then we define a scoring function on the base of the distribution of unlabeled samples, labeled target class samples and non-target class samples queried in previous iterations. Every unlabeled sample in the current iteration is given a score, and a smaller score indicates that the sample is more likely to come from the target classes. The scoring function is defined in Eq. 1.where s i denotes the score of the unlabeled sample x U i . s ti measures the distance between x U i and the distribution of features derived from all the labeled target class samples. The smaller s ti is, the closer x U i is to the known sample distribution of the target classes, and the more likely x U i is from a target class. Similarly, s wi measures the distance between x U i and the distribution of features derived from all the queried non-target class samples. The smaller s wi is, the closer x U i is from the known distribution of non-target class samples, and the less likely x U i is from the target class. After scoring all the unlabeled samples, we select the top ε% samples with the smallest scores to form the candidate set. In this paper, we empirically take twice the current iterative labeling budget (number of samples submitted to experts for labeling) as the sample number of the candidate set. Below, we give the definitions of s ti and s wi .Distance-Based Feature Distribution Modeling. We propose a category and Mahalanobis distance-based feature distribution modeling approach for calculating s ti and s wi . The definitions of these two values are slightly different, and we first present the calculation of s ti , followed by that of s wi .For all labeled target class samples from previous iterations, their fine-grained labels are known, so we represent these samples as different clusters in the feature space according to their true class labels, where a cluster is denoted as C L t (t = 1, . . . , K). Next, we calculate the score s ti for z U i using the Mahalanobis distance (MD) according to Eq. 2. MD is widely used to measure the distance between a point and a distribution because it takes into account the mean and variance of the distribution, which is very suitable for our scenario.where D(•) denotes the MD function, μ t and Σ t are the mean and covariance of the samples in the target class t, and Nom(•) is the normalization function. It can be seen that s ti is essentially the minimum distance of the unlabeled sample x U i to each target class cluster. For all the queried non-target class samples from previous iterations, since they do not have fine-grained labels, we first use the K-means algorithm to cluster their features into w classes, where a cluster is denoted as C L w (w = 1, . . . , W ).W is set to 9 in this paper. Next, we calculate the score s wi for z U i using the MD according to Eq. 4.) where μ w and Σ w are the mean and covariance of the non-target class sample features in the wth cluster. It can be seen that s wi is essentially the minimum distance of z U i to each cluster of known non-target class samples. The within-cluster selection and dynamic cluster changes between rounds significantly enhance the diversity of the selected samples and reduce redundancy."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.3,Model-Based Informative Sample Selection,"To select the most informative samples from the candidate set, we utilize the model-based informative sample selection strategy in Stage 2. We measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained in the last iteration and select the samples with the highest model uncertainty as the final selected samples. The entropy of the model output is a simple and effective way to measure sample uncertainty [7,8]. Therefore, we calculate the entropy of the model for the samples in the candidate set and select 50% of them with the highest entropy as the final samples in the current iteration."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.1,"Dataset, Settings, Metrics and Competitors","To validate the effectiveness of OpenAL, we conducted two experiments with different matching ratios (the ratio of the number of samples in the target class to the total number of samples) on a 9-class public colorectal cancer pathology image classification dataset (NCT-CRC-HE-100K) [6]. The dataset contains a total of 100,000 patches of pathology images with fine-grained labeling, with nine categories including Adipose (ADI 10%), background (BACK 11%), debris (DEB 11%), lymphocytes (LYM 12%), mucus (MUC 9%), smooth muscle (MUS 14%), normal colon mucosa (NORM 9%), cancer-associated stroma (STR 10%), and colorectal adenocarcinoma epithelium (TUM, 14%). To construct the openset datasets, we selected three classes, TUM, LYM and NORM, as the target classes and the remaining classes as the non-target classes. We selected these target classes to simulate a possible scenario for pathological cell classification in clinical practice. Technically, target classes can be randomly chosen. In the two experiments, we set the matching ratio to 33% (3 target classes, 6 non-target classes), and 42% (3 target classes, 4 non-target classes), respectively.Metrics. Following [11], we use three metrics, precision, recall and accuracy to compare the performance of each AL method. We use precision and recall to measure the performance of different methods in target class sample selection.As defined in Eq. 5, precision is the proportion of the target class samples among the total samples queried in each query and recall is the ratio of the number of the queried target class samples to the number of all the target class samples in the unlabeled sample pool.where k m denotes the number of target class samples queried in the mth query, l m denotes the number of non-target class samples queried in the mth query, and n target denotes the number of target class samples in the original unlabeled sample pool. Obviously, the higher the precision and recall are, the more target class samples are queried, and the more effective the trained target class classifier will be. We measure the final performance of each AL method using the accuracy of the final classifier on the test set of target class samples.Competitors. We compare the proposed OpenAL to random sampling and five AL methods, LfOSA [11], Uncertainty [7,8], Certainty [7,8], Coreset [17] and RA [20], of which only LfOSA [11] is designed for open-set AL. For all AL methods, we randomly selected 1% of the samples to label and used them as the initial labeled set for model initialization. It is worth noting that the initial labeled samples contain target class samples as well as non-target class samples, but the non-target class samples are not fine-grained labeled. After each query round, we train a ResNet18 model of 100 epochs, using SGD as the optimizer with momentum of 0.9, weight decay of 5e-4, initial learning rate of 0.01, and batchsize of 128. The annotation budget for each query is 5% of all samples, and the length of the candidate set is twice the budget for each query. For each method, we ran four experiments and recorded the average results for four randomly selected seeds."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.2,Performance Comparison,"Figure 3 A and B show the precision, recall and model accuracy of all comparing methods at 33% and 42% matching ratios, respectively. It can be seen that OpenAL outperforms the other methods in almost all metrics and all query numbers regardless of the matching ratio. Particularly, OpenAL significantly outperforms LfOSA [11], which is specifically designed for open-set AL. The inferior performance of the AL methods based on the closed-set assumption is due to the fact that they are unable to accurately identify more target class samples, thus wasting a large amount of annotation budget. Although LfOSA [11] utilizes a dedicated network for target class sample detection, the performance of the detection network is not stable when the number of training samples is small, thus limiting its performance. In contrast, our method uses a novel feature-based target sample selection strategy and achieves the best performance. Upon analysis, our OpenAL is capable of effectively maintaining the balance of sample numbers across different classes during active learning. We visualize the cumulative sampling ratios of OpenAL for the target classes in each round  on the original dataset with a 33% matching ratio, as shown in Fig. 4A. Additionally, we visualize the cumulative sampling ratios of the LfOSA method on the same setting in Fig. 4B. It can be observed that in the first 4 rounds, LYM samples are either not selected or selected very few times. This severe sample imbalance weakens the performance of LfOSA compared to random selection initially. Conversely, our method selects target class samples with a more bal-anced distribution. Furthermore, we constructed a more imbalanced setting for the target classes LYM (6000 samples), NORM (3000 samples), and TUM (9000 samples), yet the cumulative sampling ratios of our method for these three target classes remain fairly balanced, as shown in Fig. 4C."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.3,Ablation Study,"To further validate the effectiveness of each component of OpenAL, we conducted an ablation test at a matching ratio of 33%. Figure 3C shows the results, where w/o s w indicates that the distance score of non-target class samples is not used in the scoring of Feature-based Target Sample Selection (FTSS), w/o s t indicates that the distance score of target class samples is not used, w/o MISS means no Model-based Informative Sample Selection is used, i.e., the length of the candidate set is directly set to the annotation budget in each query, and only MISS means no FTSS strategy is used, but only uncertainty is used to select samples.It can be seen that the distance modeling of both the target class samples and the non-target class samples is essential in the FTSS strategy, and missing either one results in a decrease in performance. Although the MISS strategy does not significantly facilitate the selection of target class samples, it can effectively help select the most informative samples among the samples in the candidate set, thus further improving the model performance with a limited labeling budget. In contrast, when the samples are selected based on uncertainty alone, the performance decreases significantly due to the inability to accurately select the target class samples. The above experiments demonstrate the effectiveness of each component of OpenAL."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,4.0,Conclusion,"In this paper, we present a new open-set scenario of active learning for pathology image classification, which is more practical in real-world applications. We propose a novel AL framework for this open-set scenario, OpenAL, which addresses the challenge of accurately querying the most informative target class samples in an unlabeled sample pool containing a large number of non-target samples. OpenAL significantly outperforms state-of-the-art AL methods on real pathology image classification tasks. More importantly, in clinical applications, on one hand, OpenAL can be used to query informative target class samples for experts to label, thus enabling better training of target class classifiers under limited budgets. On the other hand, when applying the classifier for future testing, it is also possible to use the feature-based target sample selection strategy in the OpenAL framework to achieve an open-set classifier. Therefore, this framework can be applied to both datasets containing only target class samples and datasets also containing a large number of non-target class samples during testing."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,1.0,Introduction,"Deep neural networks have greatly advanced medical image analysis in recent years [12]. However, a large amount of annotated data is required for training, which is time-consuming and error-prone, especially in medical image segmentation task that needs pixel-wise annotations. Moreover, a segmentation model trained on one clinical centre (source domain) often fails to generalize well when deployed in a new centre (target domain) due to the discrepancy in the data distribution [2,9,16]. Unsupervised domain adaptation (UDA) [5,14,17] seeks to tackle this dilemma by transferring the knowledge from label-rich source domain to label-rare target domain. However, the source data may become inaccessible due to storage and privacy concerns in medical settings, which hinders the wide applications of domain adaptation. Towards this obstacle, great interests have been invoked to explore source-free domain adaptation (SFDA) [2,6,8,9,16], where a model pre-trained on the labeled source data are adapted to the unlabeled target domain without accessing source data. Though great successes, how to achieve adaptation to the unlabeled target domain with the knowledge from multiple source domains under privacy protection is still an open question to be solved.To this end, we study a practical and challenging domain adaptation problem which explores transferable knowledge from multiple source domains to target domain with only pre-trained source models rather than the source data, namely multi-source model adaptation (MSMA). Although MSMA methods [1,3,7] have made great progress for natural object recognition, there is still a blank in the multi-source-free domain adaptive medical image segmentation. Directly applying existing MSMA methods on medical image segmentation by optimizing all source segmentation models are time-consuming and inefficient due to larger model capacity of segmentation model than classification model. Another trivial solutions to tackle MSMA via SFDA methods [2,6,16,18] are to adapt each source model individually and simply take an average prediction of adapted models. However, this strategy does not take into account the varying contributions of different source models to the target domain, which can result in negative transfer from less related source domains. To rank pre-trained models, transferability metrics [10,13,20] have been widely applied to measure the domain relevance or task relevance for transfer learning, but all of them need target annotations, which is not accessible for multi-source model adaptation. Automatically select an optimal subset of the source models without requiring source data and target annotations in an unsupervised fashion is of far-reaching significance for MSMA.To address this problem, we develop a novel Transferability-Guided Model Adaptation (TGMA) model, which represents the first attempt to solve MSMA in medical image segmentation. Specifically, a label-free transferability metric (LFTM) is designed to evaluate the relevance between source and target domain without access to the source data. Based on the designed LFTM, we can compute instance-level transferability matrix (ITM) to achieve pseudo-label correction for precise supervision, and domain-level transferability matrix (DTM) to accomplish model selection for better target initialization. To this end, we can achieve adaptation to unlabeled target domain with clean pseudo label and proper model initialization. The main contributions are summarized as: "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.0,Method,"In MSMA scenario, we address the problem of jointly adapting multiple segmentation models, trained on a variety domains, to a new unlabeled target domain. Formally, let us consider we have a set of source models {F sj } M j=1 , where the j th model {F sj } is a segmentation model learned using the source dataset, with N j data points, where x i sj and y i sj denote the i-th source image and the corresponding segmentation label respectively. Now, given a target unlabeled dataset D t = {x t i } Nt i=1 , the problem is to learn a segmentation model F t , using only the learned source models, without any access to the source dataset. Figure 1 gives an overview of our proposed TGMA framework.To eliminate negative transfer by domain-dissimilar source models, we design a label-free transferability metric to evaluate the transferability of source models in an unsupervised manner for the first time. Before target training, an instancelevel transferability matrix (ITM) is computed to rectify target pseudo labels, and a domain-level transferability matrix (DTM) is calculated to achieve model selection for better model initialization. Based on the rectified pseudo labels and selected models, target segmentation model is trained with dice loss to achieve model adaptation."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.1,Label-Free Transferability Metric,"Most of multi-source model adaptation approaches [1,7] treat all source models equally, leading to negative transfer from irrelevant source domains. To avoid this type of negative transfer, it is important to critically evaluate the relevance of prior knowledge from each source domain to the target domain, and to focus on the most relevant source domains for learning in the target domain. However, it's challenging to evaluate the domain relevance in the absence of source data and target ground truths. To identify the transferability of source models, we develop a label-free transferability metric (LFTM) on the basis of attentive masking consistency to prevent negative transfer for the first time. Our metric is designed based on two assumptions: 1) Sample relevance: similar samples should hold identical predictions; 2) Model stability: if a source model makes accurate decision on this sample, little permutation on irrelevant regions will not influence the prediction. We follow these two assumptions to construct augmented sample by attentive masking, and compute the consistency as the transferability.Given unlabeled target data D t = {x i t } N i=1 and a pre-trained source segmentation model F s k , we import them to the LFTM estimator and compute the transferability metric LF T M (x t , F s k ) with only twice forwards as shown in Fig. 1. In the first forward process, the original target sample x t is passed into the source model F s k to generate segmentation map P s k = F s k (x t ). Based on the assumption that masking the normal regions from the diseased image will not affect the lesion regions, we preserve the segmentation region of the original image and randomly mask the other regions to generate masked image x m k t . Since the segmentation results may be affected by receptive field, we enlarge the segmentation map P s k to D(P s k ) by dilation. Then masked image x mt t is generated by combination of enlarged lesion regions and masked normal regions:where M (x t ) is the masking operation to randomly remove pixels. In the second forward process, the masked target sample x m k t is passed into the source modelThen we calculate the dice score between these two predictions as transferability metric:The larger the LFTM is, the more stable the source model is on the target sample. With M source models and N t target samples, we can compute the instance-level transferability matrix (ITM) T instance ∈ R M ×Nt , which can be utilized to correct target pseudo labels. Averaging T instance on the domain-space can generate domain-level transferability matrix (DTM) T domain ∈ R M ×1 , which represents the contribution of each source model to the target domain. The detailed process is illustrated in Transferability Matrix Estimation of Fig. 1."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.2,Transferability-Guided Model Adaptation,"The basic pipeline for target training needs accurate pseudo labels and suitable model initialization. While there are multiple pseudo labels and source models, simply averaging them as target supervision and model initialization is trivial solution, which ignores the contribution differences of these source domains.To tackle this problem, we propose a transferability-guided model adaptation (TGMA) framework on the basis of LFTM, which consists of two modules: Label Correction and Model Selection. Based on the instance-level transferability matrix T instance , we re-weight the pseudo labels generated by multiple source models to achieve pseudo label correction. With the domain-level transferability matrix T domain , we select the most portable source model as the main model initialization and make full use of other source models by weighted optimization strategy.Transferability-Guided Label Correction. In MSMA, we generate pseudo labels as supervision because no target ground truth is available. However, with multiple pseudo labels predicted by source models for a target sample, prior works [1,7] typically average these labels equally to obtain the final pseudo label. However, negative source models that are poorly suited to the target domain may generate inaccurate pseudo labels, resulting in noisy or unreliable training data. To eliminate negative transfer and improve pseudo-label correction, we can re-weight model predictions from all source models using the calculated instance-level transferability matrix T instance .Taking a target sample x t for example, we pass this sample to source models {F s1 , F s2 , ..., F sM } to obtain corresponding predictions {P s1 , P s2 , ..., P sM }. We take argmax operation on these predictions to generate one-hot pseudo labels {y s1 , y s2 , ..., y sM }, where y = argmax(P ). The instance-level transferability matrix T instance is applied on these pseudo labels to achieve noise correction by contribution re-weighting:where each pseudo label is weighted by the corresponding LFTM score for better combination. This strategy largely prevents the negative transfer problem caused by noisy labels of those domain-irrelevant source models.Transferability-Guided Model Selection. Previous MSMA methods [1,7] usually treat all models equally and optimize all source models parameters to achieve adaptation to the target domain. On the one hand, they ignore the negative transfer problem led by some less related domains. On the other hand, optimizing all source parameters is time-consuming and inefficient. To better make full use of the source models, we utilize the calculated domain-level transferability matrix T domain to rank all source models. With T domain representing the transferability of source models, we choose the best source model as main network F main and the second best model as auxiliary network F aux . Only initialing the target model from F main may ignore complementary knowledge of other source models, while optimizing all source models are inefficient. To obtain a compromise solution, we take the second model as auxiliary parameter knowledge. Then a weighted optimization strategy is utilized on the best model and the auxiliary model with weight W main and W aul respectively:where L dice is calculated on the combined target prediction and corresponding pseudo label. This loss optimizes model parameter W main * F main + W aux * F aux . The model selection strategy choose optimal source model while makes full use of those sub-optimal source models for better model initialization, thus avoiding the negative transfer by those domain-irrelevant domains."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.1,Dataset,"Extensive experiments are conducted to verify the effectiveness of our proposed framework on Prostate MR (PMR) dataset which is collected and labeled from six different public data sources for prostate segmentation [15]. All of the MRI images have been re-sampled to the same spacing and centercropped with the size of 384 × 384. We divide them into six sites, each of which contains {261, 384, 158, 468, 421, 175} slices. We denote these six sites as {A, B, C, D, E, F } for convenience. At each adaptation process, five sites are selected as source domains and the rest one is set as the target domain. We conduct leave-one-domain-out experiments by selecting one domain to hold out as the target. For example, → A denotes adapting source models from {B, C, D, E, F } to unlabeled images of A."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.2,Implementation Details and Evaluation Metrics,"The framework is implemented with Pytorch 1.7.0 using an NVIDIA RTX 2080Ti GPU. Following [15], we adopt UNet as our segmentation backbone. We train the target model for 200 epochs with the batch size of 6. Adam optimizer is adopted with the momentum of 0.9 and 0.999, and the learning rate is set to 0.001. We adopt the well-known metrics Dice score for segmentation evaluation.  "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.3,Comparison with State-of-the-Arts,"We compare our methods to several domain adaptation frameworks, including the single source-free domain adaptation (SFDA) [6,16,18], multi-source domain adaptation (MSDA) [4,11,21] and multi-source model adaptation (MSMA) [1,7] methods. For implementation, as most of these methods are originally designed for the image classification task, we try out best to keep their design principle and adapt them to our image segmentation task. Specifically, SFDA methods are performed on each source model and averaging the adapted model predictions as the final results. The results on prostate segmentation is listed in Table 1. As observed, MSDA methods shows superior performance than MSMA approaches due to access to the source data. Notably, compared with SFDA and MSMA approaches, our TGMA achieves higher performance on nearly all metrics with 67.32% on Average Dice. These clear improvements benefit from our LFTM metric which considers the different contributions of each source model, and largely eliminate negative transfer from the perspective of pseudo label generation and model initialization. Without the rectification by instance-level transferability matrix (Ours w/o ITM), the pseudo labels are simply generated by average combination of predictions from source models. The significant decrease in performance by 4.38% on Average Dice highlights the criticality of weighting pseudo labels with scores that reflect the relevance of the source domains. Without the model selection by domain-level transferability matrix (Ours w/o DTM), the target models are initialized from each source pre-trained network and trained separately, leading to 2.61% performance drop on Average Dice. It demonstrates that model initialization is also essential to the transfer learning. Moreover, Fig. 2 shows the segmentation results of different methods on two typical cases.We observe that our model with transferability guidance can well eliminate the negative transfer interference by some domain-irrelevant domains. "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.4,Ablation Analysis,"The performance improvement mainly comes from our designed LFTM to detect negative transfer. There are some other unsupervised metrics that can evaluate model stability, such as entropy, rotation-consistency and crop-consistency. To better evaluate the effectiveness of LFTM, we apply these unsupervised metrics to estimate ITM and DTM for label correction and model selection. The comparison results are shown in Table 2. It's obvious that our proposed LFTM outperforms other unsupervised metrics with a large margin. Entropy may make overconfident decisions on model predictions, thus leading to high transferability on those domain-irrelevant source models. Rotation and Cropping are simple data augmentation methods, which can only evaluate the model stability. Our proposed LFTM makes full use of the segmentation mask to construct featurenearest sample, thus applying sample relevance to evaluate model transferability.Removing dilation operation leads to 1.71% performance degradation on Average Dice, revealing the effect of receptive field."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,4.0,Conclusion,"In this paper, we study a practical domain adaptation problem, named multisource model adaptation where only multiple pre-trained source segmentation models rather than the source data are provided for adaptation to unlabeled target domain. To eliminate the negative transfer by domain-dissimilar source models, we design a label-free transferability metric based on the attentive masking consistency to evaluate the transferability of each source segmentation model with only target images. Using this metric, we calculate two types of transferability matrices: an instance-level matrix to adjust the target pseudo label, and a domain-level matrix to choose an optimal subset for improved model initialization."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,,Table 2 .,MethodSource Data→ A → B → C → D → E → F Average
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,1.0,Introduction,"One-shot federated learning (FL) allows a global model to be trained through a single communication round without sharing data between clients [6,8,15,33,35]. This approach significantly reduces the risk of attack and communication costs compared to FL [21] and allows for decentralized training under extreme conditions. For instance, one-shot FL has emerged as a viable solution for reducing significant transmission costs in scenarios where patient data is only accessible within an isolated network requiring in-person transfer of client models. Since one-shot FL can only access clients' models once during training, recent oneshot FL suggests generating images and using them to transfer knowledge from multiple client models for global model training using knowledge distillation (KD) [33]. However, the lack of diversity in the generated images often leads to overfitting, posing a significant challenge for one-shot FL. To address this issue, [22,33] propose to enhance the transferability of client models by generating diverse natural images near the decision boundary. Compared to natural images, the decision boundaries in medical data are often more complex (e.g., less discriminative as shown in Fig. 1), which limits the applicability of existing one-shot FL approaches to this application. Note, while the challenges in medical data and client heterogeneity can be mitigated through multiple communication rounds [12,18,23,36], the one-shot scenario presents a unique difficulty. Through this study, we reveal the inherent drawbacks of existing one-shot FL methods for medical data (see Table 1), and suggest a more suitable approach to address existing challenges e.g., overfitting.To prevent global model overfitting, we attempt to leverage random noise as a training source for KD (see Fig. 2). Baradad et al. [1] employs diverse types of structured noise for training in order to account for the difference between real images and random noise. However, due to the diversity of medical data [3,13,14], seeking a common noise space is more challenging than in natural images. Hence, we exploit DeepInversion [30], which synthesizes structured proxy noise specific to a task and thus ensures that generated noise matches the properties of medical data. Specifically, we first gather client models on the central server, where each client model is trained on its own dataset. Next, we synthesize images from random noise and store all intermediate samples in memory. Also, as images in the early stages of synthesis (i.e., close to random noise) are different from real images, we design noise-adapted client models that employ adaptive batch normalization (AdaBN) [16]. AdaBN is based on the assumption that domainrelated knowledge is represented by the statistics of the batch normalization (BN) [11] and label-related knowledge is stored in the weight matrix of each layer, ultimately enhancing the KD signal for random noise. Lastly, we train a global model through KD with both the original-and noise-adapted client models using memory-stored images, repeating until global model convergences.The contributions are as follows: (i) We propose one-shot FL leveraging image synthesis with client model adaptation. This allows to transfer knowledge from client models to the global model with synthesized images ranging from random noise to realistic images and contributes to preventing overfitting. (ii) We employ noise-adapted client models using AdaBN to produce a better KD signal for random noise. (iii) Comprehensive experiments on five small-and three large-scale medical image classification datasets consisting of microscopy, dermatoscopy, oct, histology, x-ray, and retinal images reveal that our method outperforms state-of-the-art one-shot FL methods.Related Work. Due to the challenges of one-shot FL, prior methods were trained on public data [8,15], applying dataset distillation [35], or sharing additional information [6]. However, these assumptions may not hold for several real world scenarios, posing a challenge for their practical application. Recently, Zhang et al. [33] proposed the one-shot FL DENSE, which transfers knowledge from an ensemble of client models using KD and generated images. To enhance the transferability of client models, DENSE generates diverse images near the decision boundary to improve its accuracy. However, DENSE does not perform well in one-shot FL for medical data due to the complexity of decision boundaries. While DENSE diversifies generation using a generator, we propose to avoid overfitting by using synthesized images ranging from random noise to realistic images. For data-free KD [19], DeepInversion [30] synthesizes images by optimizing RGB pixels with cross-entropy and regularization losses and improves synthesis quality by minimizing feature statistics in BN layers. DAFL [2] uses a generator for image synthesis with a teacher model as a discriminator. To prevent student model overfitting, ZSKT [22] synthesizes images that exhibit mismatch between the student and teacher models. Unlike the methods that choose the best image as a training source for KD, our approach utilizes all intermediate synthesized images to prevent overfitting. Also, while Raikward et al. [24] proposed a method for KD that uses random noise as a training source, it requires real images during training and needs to adjust BN layer statistics multiple times iteratively. In contrast, our method performs one-shot FL without requiring real images during training."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,2.0,Method,"The overall training processes are shown in Fig. 2 andAlgorithm k with respect to data D k , the objective of FL is to train a global model W g , which represents all data D = {D 1 , . . . , D k }. Motivated by [17,33,34], KD enables the transfer of knowledge from client models W c to the global model W g . Due to restricted access of D, prior works [2,30,33] use synthetic images x as a training source for KD. However, since x may be monotonous for robust training, overfitting is a significant challenge in one-shot FL. To address this, we employ random Gaussian noise N (0, 1) as a training source for KD [1]. However, in contrast with [1], N (0, 1) does not capture common medical properties. Hence, we employ DeepInversion [30] to ensure random noise retains characteristics of D. Details regarding image synthesis with DeepInversion are described in the following section. Image Synthesis. Given random noise x ∈ R H×W ×C initialized from N (0, 1), where H, W , and C denote height, width, and channels; the objective of image synthesis is to ensure x possesses a certain property of D. To achieve this, we optimize RGB pixels of x to synthesize a class-conditioned image with respect to a specific label y for I iterations. Formally,where L CE , L BN , and L T V are cross-entropy, BN, and total variation losses [20]. Hyper-parameters λ BN and λ T V are used to balance the losses. Cross-entropy loss enables the synthesis of an image with respect to the label y, and total variation loss encourages image synthesis consistency. Additionally,  training source, our method employs all intermediate synthesized samples for KD. Thus we store all intermediate samples and the corresponding noise level λ (e.g., 1i/I for i steps) in memory during I iterations. Due to the visual difference between N (0, 1) and D, we design noise-adapted client models using AdaBN [16] to provide better KD signals for x. The following section will describe more details regarding noise-adapted client models.Noise Adaptation. BN [11] was proposed to mitigate internal covariate shifts, allowing to provide consistent input distributions to subsequent layers. Due to the existing discrepancy between N (0, 1) and D, there is no guarantee BN will provide consistent input to subsequent parameters and may lead to poor model predictions. Thus we adapt N (0, 1) by iteratively adjusting the running statistics of BN using AdaBN [16], producing better logit signals for KD. Formally,where α represents momentum and x is a sample stored in memory. Initially, μ and σ2 are set to μ and σ 2 . The samples in memory ranging from characteristic images for D to N (0, 1) by gradually adjusting μ and σ2 towards N (0, 1) through Eq. 2 for I steps. With this in mind, we now describe how to train the global model. [17,33,34]. We denote W c with original μ and σ 2 as W c , and denote W c with μ and σ2 as Ŵ c . Since x, W c , and Ŵ c are used for KD, this enables the model to avoid overfitting without being negatively impacted during global model training. Formally,"
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,Global Model Training. KD allows to train a global model with multiple client models,"where λ denotes a noise level stored in memory. L KD (x; W c , W g ) denotes the Kullback-Leibler divergence between p(x; W c ) and p(x; W g ) where p(•) is an ensemble (averaging) prediction of given models with a temperature on softmax inputs [10]. Overall, W g is trained for I steps. To clarify, random noise contributes to avoiding overfitting, while noise-adapted client models help to produce a better KD signal for random noise, improving robust global model training. These processes i.e., Image Synthesis, Noise Adaptation, and Global Model Training are repeated until the global model W g converges."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,3.0,Experiments,"Datasets. For evaluation, we use five small-scale (28×28) medical image classification datasets i.e., Blood, Derma, Oct, Path, and Tissue from MedMNIST [29]. Additionally, we use three large-scale (224×224) datasets i.e., RSNA, Diabetic, and ISIC from RSNA Pneumonia Detection [25], Diabetic Retinopathy Detection [7], and ISIC2019-HAM-BCN20000 [4,5,28].Experimental Settings. We explore three scenarios i.e., (i) data heterogeneity levels, (ii) impact on large-scale datasets, and (iii) model heterogeneity i.e., each client has different architectures. In (i), Blood, Derma, Oct, Path, and Tissue datasets are used with Independent and Identically Distributed (IID) clients and Dirichlet distributed [31] clients with α = 0.6 and α = 0.3. For (ii), RSNA, Diabetic, and ISIC datasets are used with IID clients, including ISIC where each client has a different image acquisition system [27]. For (iii), client models used either ResNet18 [9], ResNet34 [9], WRN-16-2 [32], VGG16(with BN) [26], and VGG8(with BN) [26], respectively.Comparison Methods. We employ three one-shot FL methods: FedAvg [21] with single communication, DAFL [2], and DENSE [33], each evaluated using global model accuracy obtained on test data. For the upper bound, we report the FedAvg with 100 communications. For ablations, we evaluate (a) without image synthesis (w/o IS), (b) without image synthesis and noise adaptation (w/o IS&Ada) with only N (0, 1) used for training, (c) without noise adaptation (w/o Ada), and (d) without intermediate random noise (w/o N ), this is equivalent to DeepInversion [30] in a one-shot FL scenario. For w/o N , we synthesize all images and perform KD. For a fair comparison, we follow each method's original implementation and matched all training/parameter settings. For DAFL, an ensemble of client models was used as the teacher model following [17,33,34] with KD used for global model training. On large-scale datasets, an ImageNet pre-trained model was used with balanced classification accuracy reported for evaluation as in [27].Implementation Details. We used ResNet18 [9] for our experiments with five clients by default. Client models were trained for 100 epochs with SGD optimizer using learning rate (LR) 1e-3 and batch size 128. For image synthesis, we used Adam optimizer with LR 5e-2 for 100 epochs with 500 and 1,000 synthesis iterations (i.e., I) for small-and large-scale datasets, with batch sizes 256 and 50, respectively. Following [30], λ T V = 0.000025 and λ BN = 10, with KD temperature T = 20 and momentum α = 0.9. "
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,3.1,Main Results,"Table 1 shows the accuracy on five datasets with different heterogeneity levels.FedISCA outperforms all one-shot FL methods across all datasets regardless of the level of heterogeneity. In Table 2, FedISCA also reports improved performance against the compared methods, validating the viability of our approach on real-world large-scale data. On the contrary, DAFL and DENSE performed poorly on medical data since significant accuracy gaps exist between the upper bound and each competitor (except Derma). Additionally, though FedAvg reports higher accuracy for multiple communication rounds, it shows significantly lower accuracy for single communication (FedAvg(1)). To better explain this phenomenon, we analyzed the accuracy of FedAvg(1) by comparing the variance between client model parameters i.e., client models with high variance e.g., Path IID(=36.10), yield lower accuracy compared to those with low variance e.g., Derma IID(=0.01). This suggests that the variance of client models is correlated with the accuracy of FedAvg(1). In Fig. 3, we show the synthesized images of FedISCA, DAFL [2], and DENSE [33] on eight datasets. FedISCA generates more realistic images compared to the competitors. Note that DENSE aims to generate a diverse image (e.g., generating highly transferable samples) distributed near the decision boundary, which may not be realistic. Although these methods have achieved higher accuracy on natural data, our experiments reveal that this assumption does not hold in the medical domain. In addition, DENSE outperforms FedAvg(1) on small-scale  Ablations. We report ablation results in Table 1 and 2. In the medical field, generating realistic images is crucial for one-shot FL, as the accuracy of w/o IS, and w/o IS&Ada is significantly lower compared to FedISCA; this validates the need for image synthesis. However, relying on image synthesis alone is not enough to achieve high accuracy, as neither w/o Ada nor w/o N achieve the best accuracy across all datasets. w/o N performs worse than w/o Ada in most datasets (except Blood and Derma), showing that solely relying on the best image is not sufficient for robust training. On the contrary, the accuracy of FedISCA suggests that noise-adapted client models alleviate the negative effects of random noise, resulting in high accuracy. Overall, the experimental results support the idea that both components play an essential role in medical oneshot FL. Additionally, we also evaluate the variance in BN statistics between the original and noise-adapted client models. Here, we found that high variance (e.g., RSNA(=0.0018)), yields improved accuracy compared to those with lower variance (e.g., Diabetic(=0.0008)). Finally, Table 3 shows the accuracy of a global model trained on client models with model heterogeneity. The proposed method reports the best accuracy among all competitors, equally demonstrating the effectiveness of our method in one-shot FL with diverse types of model architectures."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,4.0,Conclusion,"We present a novel one-shot FL framework that uses image synthesis and client model adaptation with KD. We demonstrate that (i) random noise significantly reduces the risk of overfitting, resulting in robust global model training; (ii) noiseadapted client models enhance the KD signal leading to high accuracy; and (iii) through experiments on eight datasets, our method outperforms the state-ofthe-art one-shot FL methods on medical data. Further investigation into severe heterogeneity in clients will be a topic of future research."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,,Recall that our method employs random noise x that has D's characteristics for training. In contrast to DeepInversion which selects the best image as a Input: memory ← [ ]
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,Table 3 .,"datasets (except Tissue), but its accuracy is lower than FedAvg(1) on large-scale datasets. This suggests a difficulty in large-scale image generation i.e., the generator in DENSE deteriorates global model training and leads to lower accuracy, while FedAvg(1) achieves high accuracy due to the low client model variance e.g., RSNA(=0.61), Diabetic(=0.04), and ISIC(=0.09)."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_49.
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,1.0,Introduction,"Non-nutritive sucking (NNS) is an infant oral sucking pattern characterized by the absence of nutrient delivery [11]. NNS reflects neural and motor development in early life [16] and may reduce the risk of SIDS [18,24], the leading cause of death for US infants aged 1-12 months [2]. However, studying the relationship between NNS patterns and breathing, feeding, and arousal during sleep has been challenging due to the difficulty of measuring the NNS signal.NNS occurs in bursts of 6-12 sucks at 2 Hz per suck, with bursts happening a few times per minute during high activity periods [25]. However, active periods are sporadic, representing only a few minutes per hour, creating a burden for researchers studying characteristics of NNS. Current transducer-based approaches (see Fig. 1) are effective, but expensive, limited to research use, and may affect the sucking behavior [26]. This motivates our development of an end-to-end computer vision system to recognize and segment NNS actions from lengthy videos, enabling applications in automatic screening and telehealth, with a focus on high precision to enable periods of sucking activity to be reliably extracted for analysis by human experts.Our contributions address the fine-grained NNS action recognition problem of classifying 2.5 s video clips, and the NNS action segmentation problem of detecting NNS activity in minute-long video clips. The action recognition method uses convolutional long short-term memory networks for spatiotemporal learning. We address data scarcity and reliability issues in real-world baby monitor footage using tailored infant pose state estimation, focusing on the face and pacifier region, and enhancing it with dense optical flow. The action segmentation method aggregates local NNS recognition signals from sliding windows.We present two new datasets in our work: the NNS clinical in-crib dataset, consisting of 183 h of nighttime in-crib baby monitor footage collected from 19 infants and annotated for NNS activity and pacifier use by our interdisciplinary team of behavioral psychology and machine learning researchers, and the NNS in-the-wild dataset, consisting of 10 naturalistic infant video clips annotated for NNS activity. Figure 1 displays sample frames from both datasets.Our main contributions are (1) creation of the first infant video datasets manually annotated with NNS activity; (2) development of an NNS classification system using a convolutional long short-term memory network, aided by infant domain-specific face localization, video stabilization, and customized signal enhancement, and (3) successful NNS segmentation on longer clips by aggregating local NNS recognition results across sliding windows."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,2.0,Related Work,"Current methods for measuring the NNS signal are limited to the pressured transducer approach [25], and a video-based approach that uses facial landmark detection to extract the jaw movement signal [9]. The latter relies on a 3D morphable face model [10] learned from adult face data, limiting its accuracy, given the domain gap between infant and adult faces [22]; its output also does not directly address NNS classification or segmentation. Our approach offers an efficient, end-to-end solution for both tasks and is freely available.Action recognition is the task of identifying the action label of a short video clip from a set of predetermined classes. In our case, we wish to classify short infant clips based on the presence or absence of NNS. As with many action recognition algorithms, our core model is based on extending 2D convolutional neural networks to the temporal dimension for spatiotemporal data processing.In particular, we make use of sequential networks (such as long short-term memory (LSTM) networks) after frame-wise convolution to enhance mediumrange temporal dependencies [23].Action segmentation is the task of identifying the periods of time during which specified events occur, often from longer untrimmed videos containing mixed activities. We follow an approach to segmentation common in limiteddata contexts, patching together signals from a local low-level layer-our NNS action recognition-to obtain a global segmentation result [5].  involves dividing the video into short sliding windows, applying our NNS action recognition module to classify NNS vs. non-NNS (or obtain confidence scores), and aggregating the output classes (or scores) to generate a segmentation result with predicted start and end timestamps."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,3.1,NNS Action Recognition,"The core of our model is the NNS action recognition system shown in Fig. 2b. It consists of a frame-based preprocessing module and a spatiotemporal classifier. The preprocessing module utilizes a pre-trained model, while only the spatiotemporal classifier is trained with our data.Preprocessing Module. Our frame-based preprocessing module applies the following transformations in sequence. All three steps are used to produce training data for the subsequent spatiotemporal classifier, but during inference, the data augmentation step is not applicable and is omitted.Smooth Facial Crop. The RetinaFace face detector [4] is applied to frames in each clip until a face bounding box is found and propagated to earlier and later frames using the minimum output sum of squared error (MOSSE) tracker [1]. To smooth the facial bounding box sequence and address temporal discontinuity, saliency corners [19] are detected from the initial frame and tracked to the next frame using the Lucas-Kanade optical flow algorithm [14]. The trajectory is smoothed using a moving average filter and applied to each bounding box to stabilize the facial area. The raw input video is then cropped to this smoothed bounding box, resulting in a video featuring the face alone. Data Augmentation. When preprocessing videos to create training data for the spatiotemporal classifier, we apply random rotations, scaling, and flipping to the face-cropped video, to improve generalizability in our data-limited setting.Optical Flow. After trimming and augmentation, we calculate the short-time dense optical flow [13] between adjacent frames, and map the results into the hue, saturation, and value (HSV) color space by cascading the optical flow direction vector and magnitude of each pixel. This highlights the apparent motion between frames, magnifying subtle NNS movements (as illustrated in Supp. Fig. S3.) 1 .Spatiotemporal-Based Action Classifier. Finally, the optical flow video is processed by a spatiotemporal model that outputs an action class label (NNS or non-NNS). Two-dimensional convolutional neural networks extract spatial representations from static images, which are then fed in sequence to a temporal convolution network for spatiotemporal processing. The final classification outcome is the output of the last temporal convolution network unit."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,3.2,NNS Action Segmentation,"To segment NNS actions in mixed videos with transitions between NNS and non-NNS activity, we applied NNS recognition in 2.5 s sliding windows and aggregated results to predict start and end timestamps. This window length provides fine-grained resolution for segmentation while being long enough (26 frames at a 10 Hz frame rate) for consistent human and machine detection of NNS behavior. To address concerns about the coarseness of this resolution, we tested the following window-aggregation configurations, the latter two of which have finer 0.5 s effective resolutions: Tiled: 2.5 s windows precisely tile the length of the video with no overlaps, and the classification outcome for each window is taken directly to be the segmentation outcome for that window. Sliding: 2.5 s windows are slid across with 0.5 s overlaps, and the classification outcome for each window is assigned to its (unique) middle-fifth 0.5 s segment as the segmentation outcome. Smoothed: 2.5 s windows are slid across with 0.5 s overlaps, the classification confidence score for each window is assigned to its middle-fifth 0.5 s segment, a 2.5 s moving average of these confidence scores are taken, then the averaged confidence scores are thresholded for the final segmentation outcome. Tens of thousands of timestamps for NNS and pacifier activity were placed, by two trained behavioral coders per video. For NNS, the definition of an event segment was taken to be an NNS burst: a sequence of sucks with <1 s gaps between. We restrict our subsequent study to NNS during pacifier use, which was annotated more consistently. Cohen κ annotator agreement of NNS events during pacifier use (among 10 pacifier-using infants) averaged 0.83 in 10 s incidence windows, indicating strong agreement by behavioral coding standards, but we performed further manual selection to increase precision for machine learning use, as detailed below2 . We also created a smaller but publicly available NNS in-the-wild dataset of 14 YouTube videos featuring infants in natural conditions, with lengths ranging from 1 to 30 min, and similar annotations.From each of these two datasets, we extracted 2.5 s clips for the classification task and 60 s clips for the segmentation task. In the NNS clinical in-crib dataset, we restricted our attention to six infant videos containing enough NNS activity during pacifier use for meaningful clip extraction. From each of these, we randomly drew up to 80 2.5 s clips consisting entirely of NNS activity and 80 2.5 s clips containing non-NNS activity for classification, for a total of 960; and five 60 s clips featuring transitions between NNS and non-NNS activity for segmentation, for a total of 30; redrawing if available when annotations were not sufficiently accurate. In the NNS in-the-wild dataset, we restricted to five infants exhibiting sufficient NNS activity during pacifier use, from which we drew 38 2.5 s clips each of NNS and no NNS activity for classification, for a total of 76; and from two to 26 60 s clips of mixed activity from each infant for segmentation, for a total of 39; again redrawing in cases of poor annotations. The 2.5 s clips for classification are equally balanced for NNS and non-NNS activity to support machine learning training; the 60 s mixed clips intended for segmentation intentionally over-represent NNS compared to its natural incidence rate (see Supp. Table S1), to enable meaningful statistical conclusions."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,4.2,NNS Recognition Implementation and Results,"For the spatiotemporal core of our NNS action recognition, we experimented with four configurations of 2D convolutional networks, a 1-layer CNN, ResNet18, ResNet50, and ResNet101 [8] (all ResNet are pre-trained using ImageNet dataset and we finetune their last fully connected layer on our data); and three configurations of sequential networks, an LSTM, a bi-directional LSTM, and a transformer model [21] 3 . The models were trained for 20 epochs under a learning rate of 0.0001, and the best model was chosen based on a held-out validation set.We trained and tested this method with NNS clinical in-crib data from six infant subjects under a subject-wise leave-one-out cross-validation paradigm. Action recognition accuracies are reported on the top left of Table 1. The ResNet18-LSTM configuration performed best, achieving 94.9% average accuracy over six infants using optical flow input. The strong performance (≥85.2%) across all configurations indicates the viability of the overall method. We also evaluated a model trained on all six infants from the clinical in-crib dataset on the independent in-the-wild dataset. Results on the bottom left of Table 1 again show strong cross-configuration performance (≥79.5%), with ResNet101-Transformer reaching 92.3%, demonstrating strong generalizability of the method. As expected, models trained on the clinical in-crib data test worse on the independent in-the-wild data. But interestingly, models with the smaller ResNet18 network suffered steep drop-offs in performance when tested on the in-the-wild data, while models based on the complex ResNet101 fared better under the domain shift. Beyond this, it is hard to identify clear trends between configurations or capacities and performance.Optical Flow Ablation. Performance of all models with raw RGB input replacing optical flow frames can be found on the right side of Table 1. The results are weak and close to random guessing, demonstrating the critical role played by optical flow in detecting the subtle NNS signal. This can also be seen clearly in the sample optical flow frames visualized in Supp. Fig. S3."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,4.3,NNS Segmentation Results,"Adopting the best ResNet18-LSTM recognition model, we tested the three configurations of the derived segmentation method on the 60 s mixed activity clips, under the same leave-one-out cross-validation paradigm on the six infants. In addition to the default classifier threshold of 0.5 used by our recognition model, we tested a 0.9 threshold to coax higher precision, as motivated in Sect. 1. We use the standard evaluation metrics of average precision AP t and average recall AR t based on hits and misses defined by an intersection-over-union (IoU) with threshold t, across common thresholds t ∈ {0.1, 0.3, 0.5}4 . Averages are taken with subjects given equal weight, and results tabulated in Table 2.The metrics reveal strong performance from all methods and both confidence thresholds on both test sets. Generally, as expected, setting a higher confidence threshold or employing the more tempered tiled or smoothed aggregation methods favours precision, while lowering the confidence threshold or employing the more responsive sliding aggregation method favours recall. The results are excellent at the IoU threshold of 0.1 but degrade as the threshold is raised, suggesting that while these methods can readily perceive NNS behavior, they are still limited by the underlying ground truth annotator accuracy. The consistency of the performance of the model across both cross-validation testing in the clinical in-crib dataset and the independent testing on the NNS in-the-wild dataset suggests strong generalizability. Figure 3 visualizes predictions (and underlying confidence scores) of the sliding model configuration with a confidence threshold of 0.9, highlighting the excellent precision characteristics and illustrating the overall challenges of the detection problem.  "
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,5.0,Conclusion,"We present our novel computer vision method for the detection of non-nutritive sucking from videos, with a spatiotemporal action recognition model for classifying short video clips and a segmentation model for determining event timestamps in longer videos. Our work is grounded in our methodological collection and annotation of infant video data from varied settings. We use domain-specific techniques such as dense optical flow and infant state tracking to detect subtle sucking movements and ameliorate a relative scarcity of data. Future work could improve the robustness these methods in challenging examples of NNS activity, such as more ambiguous sucking or sucking while moving. This would require more precisely and reliably annotated data to train and evaluate, which in our experience could be difficult to obtain. An alternative approach would be to aim for more robust but less exacting, split-second results. Beyond improvements to the core NNS detection algorithms, algorithmic extraction of NNS signal characteristics, such as individual suck frequency, strength, duration, and temporal pattern, could further NNS research and one day aid in clinical care."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_55.
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,1.0,Introduction,"People perceive the world with signals from different modalities, which often carry complementary information about varying aspects of an object or event of interest. Therefore, collecting and utilizing multimodal information is crucial for Artificial Intelligence to understand the world around us. Data collected from various sensors (e.g., microphones, cameras, motion controllers) are used to identify human activity [4]. Moreover, multimodal medical images obtained from different scanning protocols (e.g., Computed Tomography, Magnetic Resonance Imaging) are employed for disease diagnosis [12]. Satisfactory performances have been achieved with these multimodal data.In practical application, however, modality missing is a common scenario. Wirelessly connected sensors may occasionally disconnect and temporarily be unable to send any data [3]. Medical images may be missing due to artifacts and diverse patient conditions [11]. In these unexpected situations, any combinatorial subset of available modalities can be given as input. To handle this, one intuitive solution is to train a dedicated model on all possible subsets of available modalities [6,14,23]. However, these methods are ineffective and timeconsuming. Another way is to predict missing modalities and perform with the completed modalities [20]. But, these approaches also require additional prediction networks for each missing situation, and the quality of the recovered data directly affects the performance, especially when there are only a few available modalities. Recently, fusing the available modalities into a shared representation received wide attention. However, it is particularly challenging due to the varying number of input modalities, which results in the N-to-One fusion problem.Currently, existing fusion strategies to tackle this challenge can be broadly grouped into three categories: the arithmetic strategy, the selection strategy and the convolution strategy. As shown in Fig. 1(a), in the arithmetic strategy, feature representations of available modalities are merged by an arithmetic function, such as averaging, computing the first and second moments or other designed formulas [10,13,17]. For the selection strategy, as shown in Fig. 1(b), each value of fused representation is selected from the values at the corresponding position of the inputs. The selection rule can be defined as max, min or probabilitybased [2,8,19]. Although the above two fusion strategies are easily scalable to various data missing situations, their fusion operation is hard-coded. All available modalities contribute equally and their latent correlations are neglected. Unlike hard-coding the fusion operation, in the convolution strategy, the convolutional fusion network automatically learns how to fuse these feature representations, which is beneficial to exploiting the correlation between multiple modalities. However, as shown in Fig. 1(c), this fusion strategy needs a constant number of data to meet the requirements of the input channels in the convolutional network. Therefore, it has to simulate missing data by crudely zero-padding or replacing it with similar modalities, which inevitably introduces a bias in computation and causes performance degradation [5,18,25].Transformer has achieved success in the field of computer vision, demonstrating that self-attention mechanism has the ability to capture the latent correlation of image tokens. However, no work has explored the effectiveness of self-attention mechanism on the N-to-One fusion, where N is variable during training, rather than fixed. Furthermore, the calculation of self-attention does not require a fixed number of tokens as input, which represents a potential for handling missing data. Therefore, we propose a self-attention based fusion block (SFusion) to tackle the problems of the above fusion strategies. As shown in Fig. 1(d), SFusion can handle any number of input data instead of fixing its number. In addition, SFusion is a learning-based fusion strategy that consists of two components: the correlation extraction (CE) module and the modal attention (MA) module. In the CE module, feature representations extracted from available modalities are projected as tokens and fed into the self-attention layers to learn multimodal correlations. Based on these correlations, a modal softmax function is proposed to generate weight maps in the MA module. Finally, it builds a shared feature representation by fusing the varying inputs with the weight maps.The contributions of this work are:-We propose SFusion, which is a data-dependent fusion strategy without impersonating missing modalities. It can learn the latent correlations between different modalities and builds a shared representation adaptively. -The SFusion is not limited to specific deep learning architectures. It takes inputs from any kind of upstream processing model and serves as the input of the downstream decision model, which enables applying the SFusion to various backbone networks for different tasks. -We provide qualitative and quantitative performance evaluations on activity recognition with the SHL [22] dataset and brain tumor segmentation with the BraTS2020 [1] dataset. The results show the superiority of SFusion over competing fusion strategies."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.1,Method Overview,"For multiple modalities, let k ∈ K ⊆ {1, 2, . . . , S} index a specific modality, within the available modality set of K, where S is the number of all possible modalities. Given an input f k ∈ R B×C×R f , B and C denote the batch size and the number of channels, respectively. R f represents the shape of feature representation extracted from the k-th modality of a sample data, which can be 1D (L), 2D (H×W), 3D (D×H×W) or higher-dimensional. In addition, I = {f k |k ∈ K} denotes the input set of feature representations from all the available modalities. Our goal is to learn a fusion function F that can project I into a shared feature representation f s , denoted as F (I) → f s . To achieve the goal, we design an N-to-One fusion block, SFusion. The architecture is shown in Fig. 2, which consists of two modules: correlation extraction (CE) module and modal attention (MA) module. "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.2,Correlation Extraction,"Given the feature representationThen, we obtain the concatenation of all the tokens z 0 ∈ R B×T ×C , where T = R × |K|, and |K| denotes the number of available modalities. Given z 0 , the stack of eight self-attention layers (SAL) are introduced to learn the latent multimodal correlations. Each layer includes a multi-head attention (MHA) block and a fully connected feed-forward network (FFN) [21]. Layer normalization (LN) is applied before every block. The outputs of the x-th (x ∈ [1, 2, . . . , 8]) layer can be describe as:(1)Therefore, we get z l ∈ R B×T ×C , which is the last SAL output. By reverting z l to the size of |K| × B × C × R f , we obtain the output I = {f k |k ∈ K} of CE as:where r(•) and split(•) are the reshape and split operations, and I is the set of calculated feature representations f k ∈ R B×C×R f which contains multimodal correlations and has the same size as the original input f k ."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.3,Modal Attention,"Given the calculated feature representations set I , the weight map m k is generated with the modal attention mechanism. Feature representations extracted from different modalities are expected to have different weights for fusion at the voxel level. Therefore, we introduce a modal-wise and voxel-level softmax function to generate the weight maps from I , as shown in Fig. 3. We denote the i-th voxel of f k and m k as v i k and m i k , respectively. e is the natural logarithm. The value of weight map m k can be defined as:By element-wise multiplying input feature map f k with the corresponding weight map m k and summing all the modalities, we can obtain a fused feature map f s as:Since the sum of m i 1 , . . . m i |K| is 1, the value range of fused feature representation f s remains stable to improve the robustness for variable input modalities. Moreover, the relative sizes of v i 1 , . . . v i |K| (contain the latent multi-modal correlations learned from the CE module) are retained in the corresponding weights. In particular, when only one modality is available, all the values of the weight map are 1, which means f s = f k (k ∈ K, |K| = 1). In this case, the input feature representation remains unchanged. It enables the backbone network (the upstream processing model and the downstream decision model) to enhance its capability to encode and decode information from different modalities rather than relying on a particular one. It is crucial for variable multimodal data analysis."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.1,Datasets,"SHL2019. The SHL (Sussex-Huawei Locomotion) Challenge 2019 [22] dataset provides data from seven sensors of a smartphone to recognize eight modes of locomotion and transportation (activities), including still, walking, run, bike, car, bus, train, and subway. The sensor data are collected from smartphones of a   person with four locations, including the bag, trousers front pocket, breast pocket and hand. Each location is called ""Bag"", ""Hips"", ""Torso"", and ""Hand"", respectively. Data acquired from the locations except the ""Hand"" are given in the train subset, while the validation subset provides the data of all four locations. In the test subset, only unlabeled ""Hand"" location data are available.BraTS2020. The BraTS2020 [1] dataset provide four modality scans: T1ce, T1, T2, FLAIR for brain tumor segmentation. It contains 369 subjects. To better represent the clinical application tasks, there are three mutually inclusive tumor regions: the enhancing tumor (ET), the tumor core (TC), and the whole tumor (WT) [1]. We select 70% data as training data, while 10% and 20% as validation and test data respectively. To prevent overfitting, two data augmentation techniques (randomly flip the axes and rotate with a random angle in [-10 • , 10 • ]) are applied during training. We apply z-score normalization [15] to the volumes individually and randomly crop 128×128×128 patches as inputs to the networks."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.2,Baseline Methods,"EmbraceNet. In the experiments on activity recognition, we compare SFusion with EmbraceNet [9], which employs a selection strategy (shown in Fig. 1 (b)) by generating feature masks (r 1 , r 2 , . . . , r 7 ) with the rule of giving equal chances to all available modalities during each value selection. For a fair comparison, as shown in Fig. 4 (a), we adopt the same processing (P) and decision (D) model as used in [9]. We obtain the performance of our fusion strategy by replacing EmbraceNet with SFusion. Following [9] setting, the batch size is set to 8. A crossentropy loss and the Adam optimization method [16] with β 1 = 0.9, β 2 = 0.999 are employed. The learning rate is initially set to 1 × 10 -4 and reduced by a factor of 2 at every 1 × 10 5 steps. A total of 5 × 10 5 training steps are executed."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,,GFF.,"In the experiments on brain tumor segmentation, we compare SFusion with a gated feature fusion block (GFF) [5], which belongs to the convolution strategy (shown in Fig. 1(c)). As shown in Fig. 4 (b), a feature disentanglement architecture is employed. Multimodal medical images are decomposed into the modality-invariant content and the modality-specific appearance code by encoders E c and E a , respectively. The content codes (e.g., c 2 and c 3 , shown in Fig. 4 (b)) of missing modalities are simulated with zero values. Then, all content codes are fused into a shared representation c s by GFF. Given c s , the tumor segmentation results are generated by the decoder D s . For a fair comparison, we adopt the same encoders (E c i and E a i ) and decoders (D s and D r i ) as used in [5]. We obtain the performance of our fusion strategy by replacing GFF with SFusion and removing the zero-padding operation. The training max_epoch is set to 200. Following [5] setting, the batch size is set to 1. Adam [16] is utilized with a learning rate of 1 × 10 -4 and progressively multiplies it by (1 -epoch / max_epoch) 0.9 . Losses of L KL , L rec and L seg are employed as [5]. During training, to simulate real missing modalities scenarios, each training patient's data is fixed to one of 15 possible missing cases. For a comprehensive evaluation, we test the performance of all 15 cases for each test patient.Our implementations are on an NVIDIA RTX 3090(24G) with PyTorch 1.8.1."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.3,Results,"Activity recognition. We compare SFusion with the EmbraceNet [9] on SHL2019. As shown in Table 1, we also compare the results of other fusion methods, which use the same processing (P) model and decision (D) model as [9]. (1) In the early fusion method, the data of seven sensors are concatenated along their  C dimension. The prediction results are obtained by inputting the concatenation into a network of P and D in series. (2) For the intermediate fusion approach, the EmbraceNet is replaced with the concatenation of feature representations along their R f dimension. (3) In the late fusion method, an independent network of P and D in series is trained for each sensor, and then the decision is made from the averaged softmax outputs. (4) In the confidence fusion model, the EmbraceNet is replaced with the confidence calculation and fusion layers in [7]. The results of different fusion methods on the validation data are presented in Table 1. Our proposed SFusion outperforms the EmbraceNet in all four smartphone locations and improves the overall accuracy from 65.22% to 67.47%.Brain Tumor Segmentation. The quantitative segmentation results are shown in Table 2. Compared with GFF, the network integrated with SFusion achieves better average performance over the 15 possible combinations in all three tasks. In particular, SFusion outperforms GFF for all the possible combinations in TC segmentation. Overall, SFusion achieves better Dice scores in most situations (13,15,13 situations for WT, TC and ET segmentation, respectively). In addition, we conduct the statistical significance analysis. The number of situations with significant improvement are 6, 10 and 8 for WT, TC and ET, respectively. It is provided by a Wilcoxon test (p-values < 0.05). Besides, we find no significant drop in performance caused by SFusion. In addition, we compare the SF_FDGF (where GFF is replaced by SFusion) with current state-of-the-art methods. Table 4 presents the average dice of 15 situations. For a fair comparison, we conduct experiments on BraTS2018, adopt the same data partition as [24], and cite the results in [24]. SF_FDGF achieves the best performance and verifies the effectiveness of the SFusion.Ablation Experiments. The correlation extraction (CE) module and the modal attention (MA) module are two key components in SFusion. We evaluate the SFusion without CE and MA, respectively. SFusion without CE denotes that feature representations are directly fed into the MA module (Fig. 2). SFusion without MA means that we directly add the calculated feature representations (I ) up to get the fusion result. As shown in Table 1, we can find that SFusion without CE performs worse than other methods. Compared with EmbraceNet, the improvement of SFusion without MA is inconspicuous. As shown in Table . 3, we present the averaged performance over the 15 possible combinations on BraTS2020. It shows that both the CE and MA module lead to performance improvement across all the tumor regions. Therefore, ablation experiments on two different tasks show that both CE and MA play an important role in SFusion."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,4.0,Conclusion,"In this paper, we propose a self-attention based N-to-One fusion block SFusion to tackle the problem of multimodal missing modalities fusion. As a data-dependent fusion strategy, SFusion can automatically learn the latent correlations between different modalities and builds a shared feature representation. The entire fusion process is based on available data without simulating missing modalities. In addition, SFusion has compatibility with any kind of upstream processing model and downstream decision model, making it universally applicable to different tasks. We show that it can be integrated into existing backbone networks by replacing their fusion operation or block to improve activity recognition and achieve brain tumor segmentation performance. In particular, by integrating with SFusion, SF_FDGF achieves the state-of-the-art performance. In the future, we will explore other tasks related to variable multimodal fusion with SFusion."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,,Table 1 .,EmbraceNet †[9] 63.68 67.98 81.58 47.63 65.22 SFusion 67.41 68.91 85.22 48.35 67.47 SFusion w/o CE 56.82 63.14 74.69 46.70 60.33 SFusion w/o MA 65.01 67.95 83.49 47.52 65.99
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,1.0,Introduction,"Automated image segmentation is a fundamental task in many medical imaging applications, such as diagnosis [24], treatment planning [6], radiation therapy planning, and tumor resection surgeries [7,12]. In the current literature, numerous fully-supervised deep learning (DL) methods have become dominant in the medical image segmentation task [5,16,18]. They can achieve their full potential when trained on large amounts of fully annotated data, which is often unavailable in the medical domain. Medical data annotation requires expert knowledge, and exhaustive labor, especially for volumetric images [17]. Moreover, supervised DL-based methods are not sufficiently generalizable to previously unseen classes.To address these limitations, few-shot segmentation (FSS) methods have been proposed [21][22][23]25], that segment an unseen class based on just a few annotated samples. The main FSS approaches use the idea of meta-learning [9,11,13] and apply supervised learning to train a few-shot model. However, to avoid overfitting and improve the generalization capability of FSS models, they rely on a large number of related tasks or classes. This can be challenging as it may require a large amount of annotated data, which may not always be available. Although some works on FSS techniques focus on training with fewer data [4,20,26], they require re-training before applying to unseen classes. To eliminate the need for annotated data during training and re-training on unseen classes, some recent works have proposed self-supervised FSS methods for 3D medical images which use superpixel-based pseudo-labels as supervision during training [8,19]. These methods design their self-supervised tasks (support-query pairs) by applying a predefined transformation (e.g., geometric and intensity transformation) on a support image (i.e., a random slice of a volume) to synthetically form a query one. Thus, these methods do not take into account intra-volume information and context that may be important for the accurate segmentation of volumetric images during inference.We propose a novel volume-informed self-supervised approach for Few-Shot 3D Segmentation (VISA-FSS). Generally, VISA-FSS aims to exploit information beyond 2D image slices by learning inter-slice information and continuous shape changes that intrinsically exists among consecutive slices within a 3D image. To this end, we introduce a novel type of self-supervised tasks (see Sect. 2.2) that builds more varied and realistic self-supervised FSS tasks during training. Besides of generating synthetic queries (like [19] by applying geometric or intensity transformation on the support images), we also utilize consecutive slices within a 3D volume as support and query images. This novel type of task generation (in addition to diversifying the tasks) allows us to present a 2.5D loss function that enforces mask continuity between the prediction of adjacent queries. In addition, to provide pseudo-labels for consecutive slices, we propose the superpixel propagation strategy (SPPS). It propagates the superpixel of a support slice into query ones by using flow field vectors that exist between adjacent slices within a 3D image. We then introduce a novel strategy for volumetric segmentation during inference that also exploits inter-slice information within query volumes. It propagates a segmentation mask among consecutive slices using the few-shot segmenter trained by VISA-FSS. Comprehensive experiments demonstrate the superiority of our method against state-of-the-art FSS approaches."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.0,Methodology,"In this section, we introduce our proposed VISA-FSS for 3D medical image segmentation. Our method goes beyond 2D image slices and exploits intra-volume information during training. To this end, VISA-FSS designs more varied and realistic self-supervised FSS tasks (support-query pairs) based on two types of transformations: 1) applying a predefined transformation (e.g., geometric and intensity transformation as used in [8,19]) on a random slice as support image to synthetically make a query one, 2) taking consecutive slices in a 3D volume as support and query images to learn continuous shape transformation that exists intrinsically between consecutive slices within a volumetric image (see Sect. 2.2). Moreover, the volumetric view of task generation in the second type of tasks allows us to go beyond 2D loss functions. Thus, in Sect. 2.2, we present a 2.5D loss function that enforces mask continuity between the prediction of adjacent queries during training the few-shot segmenter. In this way, the trained few-shot segmenter is able to effectively segment a new class in a query slice given a support slice, regardless of whether it is in a different volume (due to learning the first type of tasks) or in the same query volume (due to learning the second type of tasks). Finally, we propose a volumetric segmentation strategy for inference time which is elaborated upon in Sect. 2.3."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.1,Problem Setup,"In FSS, a training dataset D tr = {(x i , y i (l))} Ntr i=1 , l ∈ L tr , and a testing dataset D te = {(x i , y i (l))} Nte i=1 , l ∈ L te are available, where (x i , y i (l)) denotes an imagemask pair of the binary class l. L tr and L te are the training and testing classes, respectively, and L tr ∩ L te = ∅. The objective is to train a segmentation model on D tr that is directly applicable to segment an unseen class l ∈ L te in a query image, x q ∈ D te , given a few support set {(x j s , y j s (l))} p j=1 ⊂ D te . Here, q and s indicate that an image or mask is from a query or support set. To simplify notation afterwards, we assume p = 1, which indicates the number of support images. During training, a few-shot segmenter takes a support-query pair (S, Q) as the input data, where Q = {(x i q , y i q (l))} ⊂ D tr , and S = {(x j s , y j s (l))} ⊂ D tr . Then, the model is trained according to the cross-entropy loss on each support-query pair as follows: L(θ) = -log p θ (y q |x q , S). In this work, we model p θ (y q |x q , S) using the prototypical network introduced in [19], called ALPNet. However, the network architecture is not the main focus of this paper, since our VISA-FSS framework can be applied to any FSS network. The main idea of VISA-FSS is to learn a few-shot segmenter according to novel tasks designed in Sect. 2.2 to be effectively applicable for volumetric segmentation."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.2,Self-supervised Task Generation,"There is a large level of information in a 3D medical image over its 2D image slices, while prior FSS methods [8,19] ignore intra-volume information for creating their self-supervised tasks during training, although they are finally applied to segment volumetric images during inference. Previous approaches employ a predefined transformation (e.g., geometric and intensity) to form support-query pairs. We call these predefined transformations as synthetic transformations. On the other hand, there is continuous shape transformation that intrinsically exists between consecutive slices within a volume (we name them realistic transformation). VISA-FSS aims to, besides synthetic transformations, exploit realistic ones to learn more varied and realistic tasks. Figure 1 outlines a graphical overview of the proposed VISA-FSS framework, which involves the use of two types of selfsupervised FSS tasks to train the few-shot segmenter. The two types of tasks are synthetic tasks and realistic tasks: Synthetic Tasks. In the first type, tasks are formed the same as in [8,19]. For each slice x i s , its superpixels are extracted by the unsupervised algorithm [10], and its pseudo-mask is generated by randomly selecting one of its superpixels as a pseudo-organ. Thus, the support is formed as S = (x i s , y i s (l)) ⊂ D tr , where l denotes the chosen superpixel. Then, after applying a random synthetic transformation T on S, the synthetic query will be prepared, i.e., Q s = (x i q , y i q (l)) = (T (x i s ), T (y i s (l))). In this way, the (S, Q s ) pair is taken as the input data of the few-shot segmenter, presenting a 1-way 1-shot segmentation problem. A schematic view of a representative (S, Q s ) pair is given in the blue block of Fig. 1.Realistic Tasks. To make the second type of task, we take 2m adjacent slices of the support image x i s , as our query images {x j q } j∈N (i) , where N (i) = {i-m, ..., i-1, i+ 1, i+ m}. These query images can be considered as real deformations of the support image. This encourages the few-shot segmenter to learn intra-volume information contrary to the first type of task. Importantly, pseudo-label generation for consecutive slices is the main challenge. To solve this problem, we introduce a novel strategy called SPPS that propagates the pseudo-label of the support image into query ones. Specifically, we consecutively apply flow field vectors that exist between adjacent image slices on y i s (l) to generate pseudo-label y j q (l) as follows:) for j > m, and) for j < m, where φ(x i , x j ) is the flow field vector between x i and x j , which can be computed by deformably registering the two images using VoxelMorph [2] or Vol2Flow [3]. A schematic illustration of pseudo-label generation using SPPS is depicted in the Supplementary Materials. The yellow block in Fig. 1 demonstrates a representative (S, Q r ) pair formed using realistic tasks, where Q r = {(x j q , y j q (l))} j∈N (i) .Loss Function. The network is trained end-to-end in two stages. In the first stage, we train the few-shot segmenter on both types of synthetic and realistic tasks using the segmentation loss employed in [19] and regularization loss defined in [25], which are based on the standard cross-entropy loss. Specifically, in each iteration, the segmentation loss L seg can be followed as:, which is applied on a random query x i q (formed by synthetic or realistic transformations) to predict the segmentation mask ŷi q (l), where l ∈ L tr . The regularization loss L reg is defined to segment the class l in its corresponding support image x i s , as follows:Overall, in each iteration, the loss function during the first-stage training isIn the second stage of training, we aim to exploit information beyond 2D image slices in a volumetric image by employing realistic tasks.To this end, we define the 2.5D loss function, L 2.5D , which enforces mask continuity among the prediction of adjacent queries. The proposed L 2.5D profits the Dice loss [18] to measure the similarity between the predicted mask of 2m adjacent slices of the support image x i s as follows:Specifically, the loss function compares the predicted mask of a query slice with the predicted mask of its adjacent slice and penalizes any discontinuities between them. This helps ensure that the model produces consistent and coherent segmentation masks across multiple slices, improving the overall quality and accuracy of the segmentation. Hence, in the second-stage training, we train the network only on realistic tasks using the loss function:, where λ 1 is linearly increased from 0 to 0.5 every 1000th iteration during training. Finally, after self-supervised learning, the few-shot segmenter can be directly utilized for inference on unseen classes."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.3,Volumetric Segmentation Strategy,"During inference, the goal is to segment query volumes based on a support volume with only a sparse set of human-annotated slices, while the few-shot segmenter is trained with 2D images. To evaluate 2D segmentation on 3D volumetric images, we take inspiration from [21] and propose the volumetric segmentation propagation strategy (VSPS). Assume, X s = {x 1 s , x 2 s , ..., x ns s } and X q = {x 1 q , x 2 q , ..., x nq q } denote support and query volumes, comprising of n s and n q consecutive slices, respectively. We follow the same setting as [8,19,21] in which slices containing semantic class l are divided into K equally-spaced groups, including [X 1 s , X 2 s , ..., X K s ] in the support, and [X 1 q , X 2 q , ..., X K q ] in the query volume, where X k indicates the set of slices in the k th group. Suppose, in each of the k groups in the support volume, the manual annotation of the middle slice [(x c s ) 1 , (x c s ) 2 , ..., (x c s ) K ] are available as in [8,19,21]. For volumetric segmentation, previous methods [8,19,21], for each group k ∈ {1, ..., K}, pair the annotated center slice in the support volume with all the unannotated slices of the corresponding group in the query volume. More precisely, ((x c s ) k , (y c s ) k ) is considered as the support for all slices in X k q , where (y c s ) k is annotation of the center slice (x c s ) k . Finally, they use the 2D few-shot segmenter to find the mask of each of the query slices individually and therefore segment the whole query volume accordingly. In this work, we exploit the VSPS algorithm, which is based on two steps. In the first step, an inter-volume task is constructed to segment the center slice of each group in the query volume. More precisely, the center slice of each query group, (x c q ) k , is segmented using ((x c s ) k , (y c s ) k ) as the support. Then, by employing the volumetric view even in the inference time, we construct intra-volume tasks to segment other slices of each group. Formally, VSPS consecutively segments each (x j q ) k ∈ X k q , starting (x c q ) k , with respect to the image-mask pair of its previous slice, i.e., ((x j-1 q ) k , (ŷ j-1 q ) k ).In fact, we first find the pseudo-mask of (x c q ) k using the 2D few-shot segmenter and consequently consider this pseudo-annotated slice as the support for all other slices in X k q . It is worth mentioning that our task generation strategy discussed in Sect. 2.2 is capable of handling such intra-volume tasks. Further details of the VSPS algorithm are brought in the Supplementary Materials."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,3.1,Experimental Setup,"To unify experiment results, we follow the evaluation protocol established by [19], such as Hyper-parameters, data preprocessing techniques, evaluation metric (i.e., Dice score), and compared methods. The architecture and implementation of the network are exactly the same as developed in [19]. Moreover, during inference, a support volume with 3 annotated slices (i.e., K = 3) is used as a reference to segment each query volume, the same as in [19]. Also, we set m = 3, taking 3 adjacent slices of the support image as consecutive query images. However, the effect of this hyper-parameter is investigated in the Supplementary Materials.Dataset. Following [8,19], we perform experiments on two common medical benchmarks, including abdominal CT image scans from MICCAI 2015 Multi-Atlas Abdomen Labeling challenge [15] and abdominal MRI image scans from ISBI 2019 Combined Healthy Abdominal Organ Segmentation Challenge [14]. In addition, in all experiments, average results are reported according to 5-fold cross-validation on four anatomical structures the same as in [8,19], including left kidney (LK), right kidney (RK), spleen, and liver."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,3.2,Results and Discussion,"Comparison with Existing Approaches. Table 1 compares VISA-FSS with state-of-the-art FSS methods in terms of Dice, including: Vanilla PANet [25], SE-Net [21], SSL-RPNet [23], SSL-ALPNet [19], and CRAPNet [8]. Vanilla PANet and SE-Net are baselines on natural and medical images, respectively, which utilize manual annotations for training. SSL-RPNet, SSL-ALPNet, and CRAP-Net are self-supervised methods that construct their FSS tasks using synthetic transformations (e.g., geometric and intensity) in the same way, and are only different in the network architecture. As demonstrated, VISA-FSS outperforms vanilla PANet and SE-Net without using any manual annotation in its training phase. Moreover, the performance gains of VISA-FSS compared with SSL-RPNet, SSL-ALPNet, and CRAPNet highlight the benefit of learning continuous shape transformation among consecutive slices within a 3D image for volumetric segmentation. Also, the performance of VISA-FSS was evaluated using Hausedorff Distance and Surface Dice metrics on CT and MRI datasets. On the CT dataset, VISA-FSS reduced SSLALPNet's Hausedorff Distance from 30.07 to 23.62  Effect of Task Generation. To investigate the effect of realistic tasks in selfsupervised FSS models, we perform an ablation study on the absence of this type of task. The experiment results are given in rows (a) and (b) of Table 2. As expected, performance gains can be observed when both synthetic and realistic tasks are employed during training. This can highlight that the use of more and diverse tasks improves the performance of FSS models. Of note, to generate pseudo-label for consecutive slices, instead of SPPS, we can also employ supervoxel generation strategies like the popular SLIC algorithm [1]. However, we observed that by doing so the performance is 66.83 in the term of mean Dice score, under-performing SPPS (row (b) in Table 2) by about 8%. It can be inferred that contrary to SLIC, SPPS implicitly takes pseudolabel shape continuity into account due to its propagation process, which can help construct effective realistic tasks. To intuitively illustrate this issue, visual comparison of some pseudo-labels generated by SLIC and SPPS is depicted in the Supplementary Materials. In addition, to demonstrate the importance of the 2.5D loss function defined in Eq. 1 during training, we report the performance with and without L 2.5D in Table 2 (see row (d) and (e)). We observe over 1% increase in the average Dice due to applying the 2.5D loss function. "
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,4.0,Conclusion,"This work introduces a novel framework called VISA-FSS, which aims to perform few-shot 3D segmentation without requiring any manual annotations during training. VISA-FSS leverages inter-slice information and continuous shape changes that exist across consecutive slices within a 3D image. During training, it uses consecutive slices within a 3D volume as support and query images, as well as support-query pairs generated by applying geometric and intensity transformations. This allows us to exploit intra-volume information and introduce a 2.5D loss function that penalizes the model for making predictions that are discontinuous among adjacent slices. Finally, during inference, a novel strategy for volumetric segmentation is introduced to employ the volumetric view even during the testing time."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_11.
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,1.0,Introduction,"Cardiovascular diseases continue to be the primary cause of death worldwide. Imaging of myocardial fibrosis/scar provides both diagnostic and prognostic information. Cardiac magnetic resonance (CMR) late gadolinium enhancement (LGE) is the gold standard for myocardial scar evaluation in ischemic and non-ischemic heart disease [7,8]. In LGE, imaging is performed 10-15 minutes after infusion of 0.1-0.2 mmol/kg of gadolinium-based contrast agent. However, many patients who undergo clinical CMR do not have any scars on LGE. While traditionally, gadolinium was considered safe; recent data show deposition of gadolinium in many organs, which is directly associated with the total dose of gadolinium [4,11,12]. Considering patients receiving multiple MRI scans throughout their life, it is important to minimize gadolinium use in imaging protocols with high field. Beyond patient safety, there is significant costs associated with gadolinium. Furthermore, concerns have arisen regarding environmental contamination due to excessive gadolinium use [5,15].Recently, deep learning (DL) based methods have been proposed to limit gadolinium use by creating virtual LGE-like images using short-axis (sax) cine [22] or combined with native T 1 images [24]. The relationship between the motion field and myocardial infarction has also been explored to detect scar areas by learning temporal (dynamic) representations from cine images [20,21,23]. Another approach used radiomics alone [1,10,13] or combined with DL for myocardial scar screening [3]. Although promising, previous methods have several limitations. Changes in the mechanical properties of the myocardium caused by infarction can lead to regional wall motion abnormalities (WMA) [9]. However, WMA can occur in patients without scars [2]. Furthermore, the presence of scar may not always be accompanied by WMA, especially in non-ischemic heart disease [2], posing a significant challenge in detecting scars in such cases. The changes in contrast alone in cine images do not provide sufficient information to detect a scar [3]. Moreover, existing methods often overlook the 4D nature of the data (3D+time) and treat it as a 3D (2D+time) instead. Additionally, they require the detection of the left ventricle and were trained on 2D cine images that match LGE images, while in clinical practice, such information is not available beforehand. Finally, one of the major limitations is the lack of studies on large heterogeneous datasets [9]."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,,Contribution:,"In this study, we develop a novel end-to-end deep spatiotemporal residual attention neural network (ST-RAN) for scar detection using whole heart imaging in ischemic and non-ischemic heart diseases. The proposed model leverages spatial information to capture changes in contrast and temporal information to capture WMA to detect scars, in a large heterogeneous dataset. To achieve this, we propose a novel efficient Conv3Plus1D layer that deploys a factorized 4D (3D+time) receptive field, to simultaneously extract hierarchical spatial features and deep temporal features (comprehensive spatiotemporal features), distinguishing between patients with and without a scar. We introduce a multi-scale residual attention block that learns global and local motions to detect significant and subtle changes, the latter more present in patients with small scar sizes and nearly preserved wall motion. We validate our proposed model on a large cohort of patients with and without scars, showing the robustness of the model, outperforming state-of-the-art methods. Architecture overview. Our model takes an input a set of short-axis cine images of the whole heart, consisting of 20 phases, which are fed to a novel Conv3Plus1D layer, to extract spatial and temporal features. After batch normalization and nonlinear transformation, the feature maps are fed to a series of residual attention blocks (RAB) at different scales to extract global and local features, subtle to changes due to myocardial scar. After the RAB, a global average pooling followed by a fully connected are used to predict presence of a scar."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.0,Methods,"As illustrated in Fig. 1, given a time series of a 3D volume of the heart, our goal is to predict the presence of a scar (Pscar). Inspired by recent work in image processing, we rely on residual attention network [18] to learn heart motion at different scales. In contrast to Zhang et al. [23] where they used a recurrent neural network to learn local motion features and an optical flow module to learn global motion features, our model enhances attention at different scales. This approach allows our model to capture local and global motion features within a single end-to-end network, reducing the complexity of using a twostream model. The multi-scale temporal kernel allows to detect global motion, more likely to be related to a large scar size, and local motion to better estimate WMA at each segment. To address the heterogeneity of scar distribution, we have incorporated a spatiotemporal module to control the contribution of spatial features at different scales."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.1,Spatiotemporal Decomposition Using 4D(3D+time) Layers,"In sax cine, the data is in 4-dimension consisting of a 3D volume (stack of sax slices) with a temporal dimension. Therefore, effective representation of spatiotemporal features is crucial for accurate analysis. Inspired by Squeeze & Excitation network [6,14] and spatiotemporal network [17], we develop a novel efficient Conv3Plus1D layer that deploys a factorized 4D (3D+time) receptive field by applying a 3D spatial convolution of 3×3×3 to extract hierarchical spatial features, followed by a 1D temporal convolution of 11×1×1 to extract deep temporal features, as shown in Fig. 2. The large temporal filter allows maintain of the longrange dependency between the 20 frames. The input to the Conv3Plus1D layer is a 4D tensor D ∈ R X×Y ×Z×T where X is width, Y is high, Z is depth and T is time encoded in the channel-wise direction. The spatial convolution W s is applied to input volume D si across X × Y × Z and its feature map output is F si = W s × D si , where i = 1....n and n is the number of feature maps in the T direction. The spatial attention module is trained to assign an attention score a fk for each feature map F si , and patch K in X × Y × Z direction given as:where W c1 and W c2 are the weights for the fully connected layers 1 and 2 in the spatial attention module. The temporal convolution W t is applied across a volume input D ti across X × Y × T where the feature map output iswhere i = 1....m and m is the number of feature maps in the Z direction. The spatiotemporal attention module is trained to assign an attention score a F k for each feature map F ti and patch K in X × Y × T direction given as:where W c3 and W c4 are the weights for the fully connected layers 1 and 2 in the spatiotemporal attention module.To simplify the Eqs. ( 1) and ( 2), we have excluded the bias parameters. Following the application of softmax activation, the attention scores indicate the significance of each region in space and time, in determining the existence of a scar. Regions that are highly relevant to scar detection have scores near 1, while scar-free regions have scores near 0. Our proposed layer enables the learning of a more complete representation in spatial and temporal directions, surpassing a simple feature combination approach. Factorizing spatial and temporal kernels allows for reducing the model's parameters from 46M to 1.4M while learning rich spatiotemporal data representation."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.2,Residual Attention Blocks,"The motion patterns of the heart can evolve over time and scale. The residual attention network builds a stack of attention modules that generate attentionaware features. As the layers deepen, attention-aware features scale adaptively, enabling the detection of spatial and temporal subtle changes to be enhanced. This enhancement is crucial in accurately detecting small scar sizes. By aggregating information from tissues and motion across multiple scales, the attention module is able to learn and assign relative importance to each region with regard to the presence of a scar. The feature maps at a scale i where i = 1....4, are input to two fully-connected layers to encode spatial-wise and temporal-wise dependencies defined as G = W Ri1 * ReLU (V i * W Ri2 ), with W Ri1 being the weights for the first fully connected layer at scale i, and W Ri2 being the weights for the second fully connected layer at scale i. The output G is then passed through the softmax activation to obtain the spatiotemporal residual weights, which will be applied to the input map V i to extract the spatiotemporal features at scale i."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.3,Network Details,"The proposed network applies first a Conv3Plus1D layer followed by a batch normalization to mitigate internal covariate shift in the data, applies a small regularization effect, and a non-linear activation function ReLU. The output feature map is then resized through a customized resize frames layer that downsamples the size of the input by a factor of 2. This helps in increasing the batch size (×2 folds) and accelerates training (×12 folds) and testing (×4 folds) while maintaining the same high performance. This is followed by four attention residual blocks. Each block consists of residual attention applied to the input feature map and two sets of Conv3Plus1D layers, each followed by a layer normalization and ReLU. When layer normalization is located inside the residual blocks, it allows to speed-up convergence without a need for a learning rate warm-up stage [19]. Then a projection layer is applied to match the input data's last dimension and the residual block's output. When different-sized filters are used, and the input data is downsampled, the output may have a different number of filters than the input. The projection layer is used to project the last dimension of the input data to match the new filter size so that the input and output can be added together to form the residual connection. The concatenation helps tackle the vanishing gradient problem due to the 4D nature of the input data with a large 3D volume and long-range temporal dependency. By using all these layers, the network can learn from the input data more effectively, handle different-sized inputs and outputs, and learn to recognize patterns in the temporal domain. Finally, to allow the model to learn scar-specific feature representations, a global 3D average pooling is applied to enforce correspondences between feature maps and the probability of a scar, the latter estimated through one fully connected neuron with a sigmoid."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,3.0,Materials and Implementation Details,"Data Aacquisition. Cine images were collected breath-hold electrocardiogramgated balanced steady-state free precession sequence of 10 sax slices. The data were acquired from institution anonymous from 2016 to 2020 using multivendor (GE Healthcare, Siemens Healthineers) and different field strengths (1.5 T, 3 T). The institutional review board approved the use of CMR data for research with a consent waiver. Patient information was handled in compliance with the Health Insurance Portability and Accountability Act. Patients were referred for a clinical CMR for different cardiac indications, resulting in a heterogeneous patient cohort, necessary for better evaluation of the model performance. In total, 3000 patients (1697 males, 54 ± 18 years) were used for training and evaluation. The data were split into training (n=2000, 762 scar+), validation (n=500, 169 scar+), and testing (n=500, 199 scar+). All images were cropped at the center to a size of 128 × 128 and normalized to a fixed intensity range (from 0 to 1).Implementation Details. The model's optimization was performed using a mini-batch stochastic gradient descent of 64 with an initial learning rate of 0.001 and a weight decay of 0.0001 when the validation loss plateaus. The model was trained for a maximum of 500 epochs with an early stopping of 70. The binary cross-entropy loss function and binary accuracy metric for both training and validation were monitored to avoid overfitting and underfitting. All models were implemented using TensorFlow version 2.4.1 and trained on an NVIDIA DGX-1 system equipped with 8 T V100 graphics processing units (each with 32 GB memory and 5120 cores). All selected hyperparameters were optimized experimentally. DeLong's test was used to compare the AUC of the different models. All tests were two-sided with a significance level = 0.05."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,4.0,Experiments and Results,"Ablation Study on the Impact of Different Components Design. We first perform an ablation study to evaluate each component's contribution to our proposed model. We test the effect of having spatial kernels only (S-CNN), temporal kernels only (T-CNN), and a combination of both (ST-CNN). We test the impact of spatial and temporal attention (AST-CNN) and residual attention (ST-RAN) on the performance of our network. To this end, we train five variants of our model. We can observe from Table 1 that temporal information significantly outperforms spatial information alone (AUC= 0.81 vs. 0.74, P = 0.004). By combining both spatial and temporal information, we further increase the model's sensitivity (0.83 vs. 0.74, P < 0.001). Combining both spatial and temporal kernels with residual attention block yields the best performance on all dataset with an AUC of 0.84 and a sensitivity of 0.90. For both ischemic and non-ischemic heart diseases, ST-RAN showed higher sensitivity while having the lowest false-negative compared to others (Table 2)."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,,Comparison with State-of-the-Art Methods.,"We then compare our model with state-of-the-art methods trained and tested on the same dataset for myocardial scar detection, including 3D (2D + time) spatiotemporal CNN (3D-STCNN) [17], 3D-CNN [16], and CNN with a long short-term memory network (CNN-LSTM) [23]. Our proposed model yields the best performance with a sensitivity of 0.90 and an F 1 -score of 0.72, significantly outperforming all other methods (all P < 0.05), as shown in Table 1. We also compare different models on ischemic and non-ischemic patients, as shown in Table 2. We can notice the superiority of our model on both ischemic and non-ischemic patients outperforming other methods based on sensitivity, true positive, and false negative."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,5.0,Discussion,"Recent works using deep learning have shown the promise of contrast-free shortaxis cine images to detect scars based on WMA in ischemic patients. However, these methods have limitations in detecting scar in non-ischemic heart diseases. Moreover, the large heterogeneous number of patients without scar in the dataset and with WMA has degraded these models' performance in detecting scar in ischemic patients. In contrast, our approach utilizes both spatial and temporal information to detect scar. Our model demonstrates superior performance over To overcome the complexity of 4D convolution, we propose an effective training and inference strategy based on spatiotemporal factorization 4D (3D+time). This approach allows for a reduction in model parameters by a factor of 32 while maintaining high performance. The proposed layer extracts both spatial and temporal features, while enhancing attention on features in both directions, to detect subtle differences in left ventricle myocardial texture, as well as in cardiac motion.In future work, we will investigate multimodality learning, incorporating other sequences such as T 1 maps, to enhance the model's precision to an even greater degree."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,6.0,Conclusion,"We propose a spatiotemporal residual attention neural network for myocardial scar detection, and we tackled the challenging non-ischemic patients. We showed that our model works on ischemic heart disease as well. Our results demonstrate the potential of our model in unmasking hidden information in native sax cine images, and allows for better detection of patients with a high likelihood of having a myocardial scar. These results indicate the potential of our proposed model in screening patients with and without a scar, thus, saving patients from unnecessary gadolinium based contrast agent administration, reducing costs, and environmental pollution. Finally, our proposed network has potential applications in various clinical contexts that require 4D processing."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,1.0,Introduction,"Skin cancer is one of the most common cancers all over the world. Serious skin diseases such as melanoma can be life-threatening, making early detection and treatment essential [3]. As computer-aided diagnosis matures, recent advances with deep learning techniques such as CNNs have significantly improved the performance of skin lesion classification [7,8]. However, as data-hungry approaches, deep learning models require large balanced and high-quality datasets to meet the In SCL, head classes are overtreated leading to optimization concentrating on head classes. By contrast, ECL utilizes the proxies to enhance the learning of tail classes and treats all classes equally according to balanced contrastive theory [24]. Moreover, the enriched relations in samples and proxies are helped for better representations.accuracy and robustness requirements in applications, which is hard to suffice due to the long-tailed occurrence of diseases in the real-world. Long-tailed problem is usually caused by differences in incidence rate and difficulties in data collection. Some diseases are common while others are rare, making it difficult to collect balanced data [13]. This will cause the head classes to account for the majority of the samples and the tail classes only have small portions. Thus, existing public skin datasets usually suffer from imbalanced problems which then results in class bias of classifier, for example, poor model performance especially on tail lesion types.To tackle the challenge of learning unbiased classifiers with imbalanced data, many previous works focus on three main ideas, including re-sampling data [1,18], re-weighting loss [2,15,22] and re-balancing training strategies [10,23]. Resampling methods over-sample tail classes or under-sample head classes, reweighting methods adjust the weights of losses on class-level or instance-level, and re-balancing methods decouple the representation learning and classifier learning into two stages or assign the weights between features from different sampling branches [21]. Despite the great results achieved, these methods either manually interfere with the original data distribution or improve the accuracy of minority classes at the cost of reducing that of majority classes [12,13].Recently, contrastive learning (CL) methods pose great potential for representation learning when trained on imbalanced data [4,14]. Among them, supervised contrastive learning (SCL) [11] aggregates semantically similar samples and separates different classes by training in pairs, leading to impressive success in long-tailed classification of both natural and medical images [16]. However, there still remain some defects: (1) Current SCL-based methods utilize the information of minority classes insufficiently. Since tail classes are sampled with low probability, each training mini-batch inherits the long-tail distribution, making parameter updates less dependent on tail classes. (2) SCL loss focuses more on optimizing the head classes with much larger gradients than tail classes, which means tail classes are all pushed farther away from heads [24]. (3) Most methods only consider the impact of sample size (""imbalanced data"") on the classification accuracy of skin diseases, while ignoring the diagnostic difficulty of the diseases themselves (""imbalanced diagnosis difficulty"").To address the above issues, we propose a class-Enhancement Contrastive Learning (ECL) method for skin lesion classification, differences between SCL and ECL are illustrated in Fig. 1. For sufficiently utilizing the tail data information, we attempt to address the solution from a proxy-based perspective. A proxy can be regarded as the representative of a specific class set as learnable parameters. We propose a novel hybrid-proxy model to generate proxies for enhancing different classes with a reversed imbalanced strategy, i.e., the fewer samples in a class, the more proxies the class has. These learnable proxies are optimized with a cycle update strategy that captures original data distribution to mitigate the quality degradation caused by the lack of minority samples in a mini-batch. Furthermore, we propose a balanced-hybrid-proxy loss, besides introducing balanced contrastive learning (BCL) [24]. The new loss treats all classes equally and utilizes sample-to-sample, proxy-to-sample and proxy-to-proxy relations to improve representation learning. Moreover, we design a balanced-weighted crossentropy loss which follows a curriculum learning schedule by considering both imbalanced data and diagnosis difficulty.Our contributions can be summarized as follows: (1) We propose an ECL framework for long-tailed skin lesion classification. Information of classes are enhanced by the designed hybrid-proxy model with a cycle update strategy. (2) We present a balanced-hybrid-proxy loss to balance the optimization of each class and leverage relations among samples and proxies. (3) A new balancedweighted cross-entropy loss is designed for an unbiased classifier, which considers both ""imbalanced data"" and ""imbalanced diagnosis difficulty"". (4) Experimental results demonstrate that the proposed framework outperforms other state-of-theart methods on two imbalanced dermoscopic image datasets and the ablation study shows the effectiveness of each element."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.0,Methods,"The overall end-to-end framework of ECL is presented in Fig. 2. The network consists of two parallel branches: a contrastive learning (CL) branch for representative learning and a classifier learning branch. The two branches take in different augmentations T i , i ∈ {1, 2} from input images X and the backbone is shared between branches to learn the features Xi , i ∈ {1, 2}. We use a fully connected layer as a logistic projection for classification g(•) : X → Ỹ and a one-hidden layer MLP h(•) : X → Z ∈ R d as a sample embedding head where d denotes the dimension. L 2 -normalization is applied to Z by using inner product as distance measurement in CL. Both the class-dependent proxies generated by hybrid-proxy model and the embeddings of samples are used to calculate balanced-weighted cross-entropy loss, thus capturing the rich relations of samples and proxies. For better representation, we design a cycle update strategy to optimize the proxies' parameters in hybrid-proxy model, together with a curriculum learning schedule for achieving unbiased classifiers. The details are introduced as follows.  "
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.1,Hybrid-Proxy Model,"The proposed hybrid-proxy model consists of a set of class-dependent proxies andN p c is the proxy number in this class. Since samples in a mini-batch follow imbalanced data distribution, these proxies are designed to be generated in a reversed imbalanced way by giving more representative proxies of tail classes for enhancing the information of minority samples. Let us denote the sample number of class c as N c and the maximum in all classes as N max . The proxy number N p c can be obtained by calculating the imbalanced factor Nmax Nc of each class:In this way, the tail classes have more proxies while head classes have less, thus alleviating the imbalanced problem in a mini-batch.As we know, a gradient descent algorithm will generally be executed to update the parameters after training a mini-batch of samples. However, when dealing with an imbalanced dataset, tail samples in a batch contribute little to the update of their corresponding proxies due to the low probability of being sampled. So how to get better representative proxies? Here we propose a cycle update strategy for the optimization of the parameters. Specifically, we introduce the gradient accumulation method into the training process to update proxies asynchronously. The proxies are updated only after a finished epoch that all data has been processed by the framework with the gradients accumulated. With such a strategy, tail proxies can be optimized in a view of whole data distribution, thus playing better roles in class information enhancement. Algorithm 1 presents the details of the training process."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.2,Balanced-Hybrid-Proxy Loss,"To tackle the problem that SCL loss pays more attention on head classes, we introduce BCL and propose balanced-hybrid-proxy loss to treat classes equally. Given a batch of samples B = (xbe the feature embeddings in a batch and B denotes the batch size. For an anchor sample z i ∈ Z in class c, we unify the positive image set as z + = {z j |y j = y i = c, j = i}. Also for an anchor proxy p c i , we unify all positive proxies as p + . The proposed balanced-hybrid-proxy loss pulls points (both samples and proxies) in the same class together, while pushes apart samples from different classes in embedding space by using dot product as a similarity measure, which can be formulated as follows:where B c means the sample number of class c in a batch, τ is the temperature parameter. In addition, we further define Z c and P c as a subset with the label c of Z and P respectively. The average operation in the denominator of balancedhybrid-proxy loss can effectively reduce the gradients of the head classes, making an equal contribution to optimizing each class. Note that our loss differs from BCL as we enrich the learning of relations between samples and proxies. Sampleto-sample, proxy-to-sample and proxy-to-proxy relations in the proposed loss have the potential to promote network's representation learning. Moreover, as the skin datasets are often small, richer relations can effectively help form a high-quality distribution in the embedding space and improve the separation of features."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.3,Balanced-Weighted Cross-Entropy Loss,"Taking both ""imbalanced data"" and ""imbalanced diagnosis difficulty"" into consideration, we design a curriculum schedule and propose balanced-weighted cross-entropy loss to train an unbiased classifier. The training phase are divided into three stages. We first train a general classifier, then in the second stage we assign larger weight to tail classes for ""imbalanced data"". In the last stage, we utilize the results on the validation set as the diagnosis difficulty indicator of skin disease types to update the weights for ""imbalanced diagnosis difficulty"". The loss is given by:where w denotes the weight and ỹ denotes the network prediction. We assume f e c is the evaluation result of class c on validation set after epoch e and we use f1-score in our experiments. The network is trained for E epochs, E 1 and E 2 are hyperparameters for stages. The final loss is given by Loss = λL BHP +μL BW CE where λ and μ are the hyperparameters which control the impact of losses."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,3.1,Dataset and Implementation Details,"Dataset and Evaluation Metrics. We evaluate the ECL on two publicly available dermoscopic datasets ISIC2018 [5,19] and ISIC2019 [5,6,19]. The 2018 We adopt five metrics for evaluation: accuracy (Acc), average precision (Pre), average sensitivity (Sen), macro f1-score (F1) and macro area under curve (AUC). Acc and F1 are considered as the most important metrics in this task.Implementation Details. The proposed algorithm is implemented in Python with Pytorch library and runs on a PC equipped with an NVIDIA A100 GPU. We use ResNet50 [9] as backbone and the embedding dimension d is set to 128. We use SGD as the optimizer with the weight decay 1e-4. The initial learning rate is set to 0.002 and decayed by cosine schedule. We train the network for 100 epochs with a batch size of 64. The hyperparameters E 1 , E 2 , τ , λ, and μ are set to 20, 50, 0.01, 1, and 2 respectively. We use the default data augmentation strategy on ImageNet in [9] as T 1 for classification branch. And for CL branch, we add random grayscale, rotation, and vertical flip in T 1 as T 2 to enrich the data representations. Meanwhile, we only conduct the resize operation to ensure input size 224 × 224 × 3 during testing process. The models with the highest Acc on validation set are chosen for testing. We conduct experiments in 3 independent runs and report the standard deviations in the supplementary material."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,3.2,Experimental Results,"Quantitative Results. To evaluate the performance of our ECL, we compare our method with 10 advanced methods. Among them, focal loss [15], LDAM-DRW [2], logit adjust [17], and MWNL [22] are the re-weighting loss methods. BBN [23] is the methods based on re-balancing training strategy while Hybrid-SC [20], SCL [11,16], BCL [24], TSC [14] and ours are the CL-based methods. Moreover, MWNL and SCL have been verified to perform well in the skin disease classification task. To ensure fairness, we re-train all methods by rerun their released codes on our divided datasets with the same experimental settings. We also confirmed that all models have converged and choose the best eval checkpoints. The results are shown in Table 1. It can be seen that ECL has a significant advantage with the highest level in most metrics on two datasets. Noticeably, our ECL outperforms other imbalanced methods by great gains, e.g., 2.56% in Pre on ISIC2018 compared with SCL and 4.33% in F1 on ISIC2019 dataset compared with TSC. Furthermore, we draw the confusion matrixes after normalization in Fig. 3, which illustrate that ECL has significantly improved most of the categories, from minority to majority.  S2). First, we directly move the contrastive learning (CL) branch and replaced the balanced-weighted cross-entropy (BWCE) loss with cross-entropy (CE) loss. We can see from the results that adding CL branch can significantly improve the network's data representation ability with better performance than only adopting a classifier branch. And our BWCE loss can help in learning a more unbiased classifier with an improvement of 2.7% in F1 compared to CE in dual branch setting. Then we train the ECL w/o cycle update strategy. The overall performance of the network has declined compared with training w/ the strategy, indicating that this strategy can better enhance proxies learning through the whole data distribution. In the end, we also set the proxies' number of different classes equal to explore whether the classification ability of the network is improved due to the increase in the number of proxies. With more proxies, metrics fluctuate and do not increase significantly. However, the result of using proxies generated by reversed balanced way in hybrid-proxy model (HPM) outperforms equal proxies in nearly all metrics, which proves that giving more proxies to tail classes can effectively enhance and enrich the information."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,4.0,Conclusion,"In this work, we present a class-enhancement contrastive learning framework, named ECL, for long-tailed skin lesion classification. Hybrid-proxy model and balanced-hybrid-proxy loss are proposed to tackle the problem that SCL-based methods pay less attention to the learning of tail classes. Class-dependent proxies are generated in hybrid-proxy model to enhance information of tail classes, where rich relations between samples and proxies are utilized to improve representation learning of the network. Furthermore, balanced-weighted cross-entropy loss is designed to help train an unbiased classifier by considering both ""imbalanced data"" and ""imbalanced diagnosis difficulty"". Extensive experiments on ISIC2018 and ISIC2019 datasets have demonstrated the effectiveness and superiority of ECL over other compared methods."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,,Algorithm 1 :,"Input: 7 else 8 Loss(θ, φ) = λLBHP ( z , P) + μLBW CE ({yi, ỹi} B ) 9 grad t θ = ∇ θ Loss(θ), grad t φ = ∇ φ Loss(φ) // calculate gradients 10 θ ← θlr * grad t θ // update parameters θ of model 11 φ ← φ -T t lr * grad t φ // update parameters φ of P 12 if e > E2 then 13 f e = V alidate(model, X val )"
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,,Table 2 .,"Ablation Study. To further verify the effectiveness of the designs in ECL, we conduct a detailed ablation study shown in Table2(the results on ISIC2018 are shown in supplementary material Table"
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 23.
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,1.0,Introduction and Related Work,"Ultrasound (US) is one of the most common imaging techniques in medical practice, with applications to fetal imaging, cardiac imaging, sports medicine, and more. With the rise of US for routine clinical care, there is a growing interest in applying computer vision techniques to automate or enhance the analysis of US imagery [13]. Many US examinations involve the collection of video clips showing different anatomical regions. The medical imaging community is in the early stages of applying techniques from the video recognition community to US recognition tasks. These applications face several challenges arising from the nature of US as an imaging modality, differences between US imagery and natural imagery, and the lack of large representative datasets. To make matters worse, the collection of large medical datasets is often unethical or prohibitively costly. There is, therefore, a significant need for efficient methods that can produce high levels of performance using the minimum number of samples. In this work, we propose an efficient US video recognition architecture that takes advantage the nature of common US recognition tasks.To design an efficient US recognition architecture, it is necessary to consider the space of US recognition tasks and evaluate the algorithmic structures needed to efficiently capture the semantics in those settings. We posit that many of these tasks amount to the identification of specific visual characteristics at key moments in the clip. The identification of the standard plane in fetal head US depends on recognizing key structures in fetal brain tissue [3,19]; the quality assessment of FAST clips [24] relies on the ability to recognize that key organs and other structures have been visualized in the clip; view identification relies on recognizing orientation of the anatomical structures in relation to one another [8,11]; and the quantification of heart function requires measurement of ventricular volumes at two key moments in the cardiac cycle [22]. Based on these observations, we propose a novel US Video Network (USVN) that treats frames as independent and unordered. USVN constructs expressive video representations by combining information from multiple frames using a novel multi-head attention mechanism. We demonstrate a setting in which USVN yields better performance and far better sample efficiency than a competing model that includes temporal features. We also demonstrate that, in a setting where temporal dependence is important, USVN lags behind the competing model. These contrasting outcomes demonstrate the importance of tailoring the model architecture to the structure of the US recognition task in data-constrained settings.A large body of work has addressed video recognition tasks, including object tracking [14], temporal action localization [28], captioning [1], action recognition [30], and many others. Driven by the availability of large human action datasets, the field of action recognition has focused on the need to capture expressive spatiotemporal features. This has led to the development of two-stream networks using optical flow [21], the use of 3D convolutional networks [10,25], and, of course, the use of transformer-based architectures [15,18]. Our main point of departure with these methods is the importance placed upon temporal features. We posit that temporal features are not relevant in some common US tasks and that excluding these features leads to better sample efficiency. To explore this idea, we assume temporal independence a priori, placing our problem formulation in the format of a Multi-instance Learning (MIL) task.Multi-instance learning (MIL) describes the situation where labels apply to bags of instances rather than to individual instances. Instances within a bag are assumed to be unordered and, conditional on the bag label, independent from one another [2]. Under our assumption that all video frames can be treated independently, video recognition can be viewed as MIL where the bag is the video, and the instances are the frames. MIL has a long history of applications to video recognition that predates deep learning [5,6,23,29]. In the classical formulation of MIL it is assumed that instances have unobserved labels, and the task is to extract these as latent variables and aggregate them to predict the baglevel label. In their paper Attention-based deep multiple instance learning Ilse, Tomczak, and Welling [9] depart from this classical perspective by aggregating embeddings rather than instance labels. We take a similar approach. Unlike their work, however, we use multiple attention heads focused on different subspaces of the image-level embeddings, with their work as a special case of ours. To our knowledge, we are the first to introduce a MIL formalism using multiple attention heads in this way.There is growing interest in applying action recognition techniques to medical US video with applications to fetal [3,19,20], abdominal [11,24], and cardiac [4,8,17,22] US. Most existing applications make MIL assumptions but only apply a fixed pooling function to frame-level labels. Howard et al. [8] apply a range of techniques, including average pooling, two-stream networks, and 3D convolutions to identifying cardiac views. They conclude that two-stream networks yield the best performance. The authors do not test any methods that adaptively pool frame information in a time-independent manner. Lei et al. [12] specifically consider the detection of Patent Ductus Arteriosus (PDA). They make MIL assumptions by applying the video-level label to the individual frames and training a 2D CNN to estimate these noisy labels. Video-level labels are generated by applying a decision threshold to the frame-level predictions and then voting with equal weight across frames. Ouyang et al. [16] use 3D convolutions, specifically the R(2+1)D architecture [25], to predict ejection fraction from cardiac US obtaining human-level performance. They do not assess the performance of any time-independent methods. Among these examples, we see a divide between methods that have no ability to adaptively weight different frames and those that can express arbitrary spatiotemporal features. We fill this gap by proposing a time-independent method that adaptively pools information from different moments in time."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,2.1,USVN,"Architecture. Our video recognition architecture, shown in Fig. 1, pools information across frames using a multi-head attention mechanism. Like the attention mechanism in the transformer architecture [26], we compute attentions over subspaces of the frame-level representations. We hypothesize that US video recognition requires the detection of distinct visual features that may appear at different points of time in the video. The individual attention heads can function as detectors of these features. Unlike ordinary multi-head attention, the subspaces are not compared with other frames in the sequence but with a set of global query vectors inferred during training. The use of global query vectors arises from our inductive prior that the recognition task amounts to locating key pieces of information at any point in the sequence, and the inferred query vectors are representations of that key information.Frames are first embedded into 2048-dimensional vectors using a CNN encoder. This encoder is initialized via ImageNet pretraining and fine-tuned during training. Rather than learn N a projections from scratch for the attention weighting, we simply partition the frame representations into N a vectors h t i each of size d a = 2048/N a and rely on the final convolutional layers of the CNN to adapt. We then compute the un-normalized attention scores via dot product with the global query vectors:The resulting scores are normalized resulting in N a attention vectors, a i = softmax(λ i ), where the arrow notation represents vectorization in time. The video-level representation from the i th head is then simply H i = a i • h i , and the full video representation is the concatenation H = concat([H 1 , H 2 , . . . , H Na ]). The video-level prediction can then be computed using a shallow fully-connected network, y = f (H).Augmentation by Frame Sampling. Because USVN treats all frames independently, it is not necessary to use contiguous spans of frames during training. Instead, we randomly sample fixed-size sets of frames from each video. This can have a regularizing effect by using novel frames for each training epoch. During evaluation we use all video frames. We accommodate the varying numbers of frames in each video by zero padding and masked attention.Model Interpretability. We identify prototype frames for each attention head. These prototypes produce embedding subspace vectors h t i that are closely aligned with the corresponding query vector q i . These prototype images can then be qualitatively evaluated by the clinical specialist (see Supplemental Material)."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,2.2,Benchmark Implementations,"A simple and common approach for video recognition is to use fixed pooling functions to aggregate the frame-level representations across time, treating each element of the representation as a channel. We evaluate this approach using max and average pooling functions. Our attention-based method can implement average pooling by assigning equal weight to all frames for each attention head. Neglecting potential optimization challenges, this suggests that attention-based pooling should be at least as good as average pooling. On the other hand, our model can only approximate max pooling in the N a = 2048 case by assigning very large, positive values to the single-element query vectors causing the attentions to become sharply concentrated at one time step. However, this solution pushes the softmax over time into regions with very small gradients. We conclude that max pooling can learn video representations that cannot be expressed by USVN (and vice versa). R(2+1)D is a 3D CNN video recognition architecture that decomposes the spatial and temporal convolution into two successive steps [25]. First, a 2D convolution is applied over space then a 1D convolution is applied over time. Compared to its 3D ResNet counterparts on Sports-1M and Kinetics datasets, R(2+1)D is a very capable model that can learn complex features while having the same number of parameters in a more data-efficient way. We choose to benchmark against this architecture due to its efficiency and because this is the architecture used by Ouyang et al. to achieve human-level performance on the EchoNet-Dynamic US dataset [16]."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.1,Datasets,"Patent Ductus Arteriosus (PDA). PDA is an opening between the aorta and pulmonary artery that, in severe cases, can cause heart failure shortly after birth. Ultrasound imaging is the primary diagnostic tool for detecting and characterizing PDA. Specifically, doppler US imaging can visualize the motion of the blood through the PDA opening. This motion appears as a characteristic blob of color in the region of the PDA. Physicians are trained to recognize the color and shape of the blob as well as where it appears in relation to other visible anatomy. Superficially, this recognition task makes no reference to the dynamics of the video. We therefore expect that temporal features are not required for accurate PDA recognition. For this dataset we train USVN to predict whether or not an image indicates the presence of PDA. The model output, y, is therefore a single number interpreted as the log-odds of PDA.We retrospectively collected a set of 1,145 doppler US clips from 165 distinct examinations involving 66 distinct patients. Each clip was labeled to indicate the presence (661 clips) or absence (484 clips) of PDA. Patients were divided into training (44), validation (11), and test (11) sets with stratification on the presence of PDA. These sets contained 755, 118, and 272 videos, respectively.The large variation in the number of videos in the validation and test sets results from the fact that patients have a variable number of examinations ranging from 1 to 10. The echocardiograms were obtained by registered sonographers and level 3 echocardiographers. For each of these videos, a masking and cropping transformation was performed to remove text and instrument information from the scanning area.For this dataset, we train USVN to predict ejection fraction. Rather than predict EF directly, we output a tuple of real numbers (y 1 , y 2 ) and insert them in place of ESV and EDV in Eq. ( 1). This choice is motivated by the knowledge that ESV and EDV are determined from different phases of the cardiac cycle. We speculate that decomposing EF into ESV and EDV effectively linearizes the estimation of EF as a function of the video representation H with different attention heads responsible for estimating ESV and EDV."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.2,Results,"Model Performance. Table 1 summarizes the performance of USVN and our benchmark implementations on the PDA and EchoNet tasks. For PDA classification, we evaluate using the area under the ROC curve (ROC AUC). For EchoNet, we use the percent of variance explained (r 2 ). USVN results are based on N a = 16 and N a = 128 for PDA and EchoNet, respectively, based on a hyperparameter search (see Supplemental Material). For the PDA dataset, we expected that temporal features are not beneficial and, indeed, we see that R(2+1)D performs worse than all other methods, likely due to the unneeded capacity in the temporal convolutions and the relatively small size of the PDA dataset. USVN leads to a small benefit over average and max pooling for this task. The EchoNet task does benefit from modeling temporal features as indicated by R(2+1)D obtaining the highest score. However, USVN significantly outperforms the fixed pooling methods and is surprisingly close to R(2+1)D. This suggests that temporal features play a relatively small part in explaining the variability in the EchoNet dataset. Sample Efficiency. In Fig. 2 we evaluate the sample efficiency of USVN by artificially limiting the amount of training data. In the case of PDA, we downsample the number of patients because videos from a single patient are correlated with one another. For EchoNet, we downsample the number of videos. In both cases, we use the full validation and test sets to better isolate variation due to limited training data from variation due to model selection and evaluation.For PDA, R(2+1)D underperforms the time-independent methods, and the gap is larger for smaller numbers of training patients (see Fig. 2, top panel). Surprisingly, USVN and average pooling have very similar performance across samples and saturate for a small subset of the available patients. R(2+1)D needs all available patients to approach a similar level of performance. This result aligns with our expectation that the inductive prior of time independence can yield sample efficiency benefits when applied to the appropriate task. R(2+1)D outperforms the time-independent models across all samples for the EchoNet task (see Fig. 2, bottom panel). Despite being a much simpler architecture than R(2+1)D and approaching similar levels of performance, USVN does not exhibit any sample efficiency benefits in the low-data regime for the EchoNet task. Solving the EchoNet task with spatial features alone may require more adaptation of the pretrained encoder than is required when solving with temporal features. For instance, it may be possible through extensive adaptation of the encoder network to recognize the visual characteristics associated with the end of diastole. However, the end of diastole may also manifest as, for example, an extremum in time of some visual characteristic. A model with access to temporal features such as R(2+1)D may be able to capture such an extremum with relatively little adaptation of the pretrained network."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.3,Implementation Details,"For the fixed pooling methods and USVN, we use an ImageNet-pretrained ResNet50 image encoder provided through the timm library [27]. We train using the timm implementation of the AdamP optimizer [7] with β 1, 2 = 0.9, 0.999, weight decay of 0.001, batch size of 20 clips, and initial learning rates of 3 • 10 -5 and 0.001 for PDA and EchoNet, respectively. We sample 32 frames per clip during training. We reduce the learning rate by a factor of 10 after 3 epochs with no improvement of the validation loss, and we terminate training after ten consecutive epochs of no improvement. We use 50% dropout on the inputs to the linear layer for each dataset.To reproduce the results of R(2+1)D on Echonet Dynamic Dataset by Ouyang et al. [16], we cloned their github repo and re-ran their experiments with their best found hyperparameters. Our training runs show similar, if not better, results than stated in the original work. To adapt the model for PDA classification, we modified their data loader, training script, and the R(2+1)D model to allow PDA images. We also removed the manual bias term initialization, left over from predicting ejection fraction on the fully connected linear layer, and initialize it randomly instead. Finally, we replaced MSE loss with binary cross entropy with logits in the training loop. Every run was done for 45 epochs with a batch size of 20 for Echonet Dynamic dataset and 10 for PDA dataset. Model saving occurred for every epoch that showed improvement to the validation loss."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,4.0,Conclusions and Discussion,"The field of video recognition has been driven by large human action recognition datasets. Unlike videos of human actions, the accurate recognition of medical ultrasound images often only requires identifying key pieces of information at any point in the video and does not make reference to the sequence of events. The contrast between results for the PDA task (where USVN excels) and the EchoNet task (where USVN suffers) demonstrates the importance of tailoring the model architecture to the task at hand in data-constrained settings. Our results suggest that models developed for human action recognition are not optimal in some practical scenarios involving medical ultrasound and that models that assume temporal independence have better sample efficiency. We introduce an architecture, USVN, that is tailored to the medical ultrasound context and demonstrate a situation where the inductive prior of time independence leads to significant sample efficiency benefits. We also present a situation where temporal features are relevant and show that, even for very small datasets, USVN produces no efficiency benefits. Practitioners of deep learning who work with medical ultrasound in the low-data regime should take care to match the architecture choice to the nature of the recognition task."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,,Table 1 .,"EchoNet-Dynamic. The Echonet Dynamic dataset consists of 10,030 apical-4 chamber echocardiograms downsampled to 112 × 112. Each study has clinical measurements: ejection fraction (EF), end systolic volume (ESV), and end diastolic volume (EDV). EF is commonly used to assess cardiac function and is computed from ESV and EDV as"
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,1.0,Introduction,"Myocardial infarction (MI) is the deadliest cardiovascular disease in the developed world [22]. Consequently, an ability to predict future MI events is of immense importance on both an individual and population health level, as it would allow for improved risk stratification, preventative care, and treatment planning. In current clinical practice, MI prediction is typically based on volumetric biomarkers, such as ejection fraction. These can be derived from cardiac cine magnetic resonance imaging (MRI), which is considered the gold standard modality for cardiac function assessment [28]. While such metrics are relatively easy to calculate and interpret, they only approximate the complex 3D morphology and physiology of the heart with a single value, which hinders further improvements in predictive accuracy. Consequently, considerable research efforts have been dedicated to developing new methods capable of extracting novel biomarkers from images or segmentation masks using machine learning and deep learning techniques [1,16,17,21,23,29,30,34]. However, their focus on 2D data still limits the discovery of more intricate biomarkers whose important role for MI prediction and cardiac function assessment has previously been shown [10,12,20,29]. In order to efficiently process true 3D anatomical shape information, geometric deep learning methods for point clouds have recently been increasingly used for various cardiac image-based tasks [5,7,8,18,19,32,35].In this work, we propose the multi-objective point cloud autoencoder as a novel geometric deep learning approach for interpretable MI prediction, based on 3D cardiac shape information. Its specialized multi-branch architecture allows for the direct and efficient processing of high resolution 3D point cloud representations of the multi-class cardiac anatomy at multiple time points of the cardiac cycle, while simultaneously predicting future MI events. Crucially, a lowdimensional latent space vector captures task-specific 3D shape information as an orderly multivariate probability distribution, offering pathology-specific separability and allowing for a straightforward visual analysis of associations between 3D structure and latent encodings. The resulting high explainability considerably boosts the method's clinical applicability and sets it apart from previous black-box deep learning approaches for MI classification [11,14,18]. To the best of our knowledge, this is the first point cloud deep learning approach to combine full 3D shape processing and multi-objective learning with an explicit focus on method interpretability for MI prediction."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.1,Dataset and Preprocessing,"We select the cine MRI acquisitions of 470 subjects of the UK Biobank study as our dataset in this work [24]. All images were acquired with a voxel resolution of 1.8 × 1.8 × 8.0 mm 3 for short-axis and 1.8 × 1.8 × 6.0 mm 3 for long-axis slices using a balanced steady-state free precession (bSSFP) protocol [25]. Half of the subjects in our dataset experienced an MI event after the image acquisition date First, a point cloud deep learning-based encoder branch captures multi-scale shape information from multi-class and multi-temporal input anatomies in a low-dimensional latent space vector. Then, the resulting encodings are used in a reconstruction branch to recreate the original input shapes and in a prediction branch to output a clinical outcome probability (in this case for incident MI events).(incident MI) as indicated by UK Biobank field IDs 42001 and 42000. The other 50% of subjects are considered as normal control cases. They were chosen to be free of any cardiovascular disease and other pathologies frequently observed in the UK Biobank study, following a similar selection as previous works [2,6,9] (see Table 1 of the Supplementary Material). For each subject, we reconstruct 3D multi-class point cloud representations of their biventricular anatomy from the corresponding raw cine MR images at both the end-diastolic (ED) and endsystolic (ES) phases of the cardiac cycle with the fully automatic multi-step process proposed in [3,4,13], and use them as inputs for our networks."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.2,Network Architecture,"The architecture of the multi-objective point cloud autoencoder consists of three task-specific branches, namely an encoder, a reconstruction, and a prediction branch, which are connected by a low-dimensional latent space vector (Fig. 1).Concatenated multi-class point clouds at the ED and ES phases of the cardiac cycle with shape (2 * p)×4 are first fed into the encoder branch as network inputs where (2 * p) represents the number of points p in the ED and ES point clouds and 4 are the x, y, z coordinate values in 3D space and a class label to encode the three cardiac substructures, namely left ventricular (LV) endocardium, LV epicardium, and right ventricular (RV) endocardium. The inputs are then passed through the point cloud-specific encoder, which is composed of two connected PointNet-style [26,27] blocks and a multi-layer perceptron (MLP), before outputting both a mean and standard deviation (SD) vector of size 1 × z. Next, the reparameterization trick is applied to these two vectors, and the resulting latent space vector is used as an input to both the reconstruction and predic-tion branches. This ensures that the latent space is influenced by both tasks during training and thus encourages an interpretable distribution that is both discriminative enough for the prediction task and also descriptive enough to allow accurate reconstruction. The reconstruction branch [33] starts with a MLP to produce an intermediate coarse point cloud output, which assures that the final fine point cloud preserves the global shape. It is then followed by a FoldingNetstyle [31] layer to obtain the final dense output point cloud with both a local and global shape focus. The preliminary coarse and the dense output point cloud are represented as m × 3 × (2 * 3) and n × 3 × (2 * 3) tensors respectively, where m and n refer to the number of points with n >> m, the 3 to the spatial 3D coordinates, and the (2 * 3) to the three cardiac substructures at ED and ES. In this work, we use the same total number of points to represent both the input and dense output point clouds. The prediction branch combines a Dropout layer, a MLP, and a Sigmoid activation function."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.3,Loss and Training,"The loss function of the multi-objective point cloud autoencoder consists of the sum of three subloss terms, each representing a different training objective in the multi-task setting, and weighted by two parameters β and γ.(The first loss term, L reconstruction , encourages the network to accurately reconstruct input anatomies and thereby capture important shape information. It contains two subloss terms and a weighting parameter α.(Here, C and T refer to the number of cardiac substructures and phases respectively. We use C = 3 and T = 2 in this work. The L coarse and L dense loss terms compare the respective coarse and dense output predictions of the network with the same input point cloud using the symmetric Chamfer distance (CD). The weighting parameter α is increased stepwise from smaller (0.01) to larger (2.0) values during training in a monotonic annealing schedule to encourage the network to first focus on a good global reconstruction and gradually put more emphasis on a high local accuracy as training progresses. The second loss term in Eq. ( 1), L KL , calculates the Kullback-Leibler divergence between the network's latent space and a multivariate standard normal distribution, which encourages high latent space quality and improves regularization. The third loss term, L CE , refers to the binary cross entropy loss between the network's outcome prediction and the gold standard encoding. We again use a monotonic annealing schedule for the weighting parameter β to balance latent space quality and output accuracy and for γ to gradually put more focus on improving prediction performance. Hereby, we choose stepwise increases from 0.001 to 0.01 for β and from 1.0 to 5.0 for γ, based on empirical findings. We randomly split the dataset into 70% training, 5% validation, and 25% test data. We train the network with the Adam optimizer and a mini-batch size of 8 for ∼80,000 steps, since no improvement on the validation data was achieved during the 10,000 prior steps. The method is implemented using the TensorFlow library and has a post-training run time of ∼15 ms. All experiments are performed on a GeForce RTX 2070 Graphics Card with 8 GB memory."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.1,Input Shape Reconstruction,"In our first experiment, we evaluate whether the multi-objective point cloud autoencoder is able to accurately reconstruct the ED and ES input anatomies. To this end, we pass all anatomies of the test dataset through the trained network and visualize both the input and corresponding predicted point clouds of three sample cases in Fig. 2. We observe good local and global shape alignment between the input and predicted anatomies in all cases. Relationships between cardiac substructures and between ED and ES phases are accurately retained.Next, we quantify the reconstruction performance by calculating the symmetric Chamfer distances between the respective input and reconstructed point clouds of all subjects in the test dataset separately for each cardiac substructure and phase (Table 1). We find mean Chamfer distance values below the underlying acquisition's pixel resolution for both phases and all cardiac substructures. "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.2,Myocardial Infarction Prediction,"We next evaluate the performance of the network for incident MI prediction as its second task. To this end, we first obtain both the gold standard MI outcomes and the MI predictions of our pre-trained network for all cases in the test dataset and quantify its performance using five common binary classification metrics (Table 2). To compare with clinical benchmarks, we select LV ejection fraction (EF) and the combination of LV and RV EF as widely used metrics and use each of them as input features for two separate logistic regression models. In addition, we choose a hierarchical convolutional neural network (CNN) and a standard PointNet [26] with 2D segmentation masks and 3D anatomy point clouds at ED and ES as respective inputs, as additional benchmarks (Table 2). We find that the proposed multi-objective point cloud autoencoder outperforms all other approaches with improvements of 19% in terms of Area Under the Receiver Operating Characteristic (AUROC) curve."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.3,Task-Specific Latent Space Analysis,"In addition to validating the reconstruction and prediction performance of our network, we also investigate the ability of its latent space to store high-resolution 3D shape data in an interpretable and pathology-specific manner. To this end, we first pass the anatomy point clouds of both normal and MI cases through the encoder branch of the pre-trained network to obtain their respective latent space encodings. We then apply the Laplacian eigenmap [15] algorithm to the encodings as a non-linear dimensionality reduction technique and visualize the resulting 2D eigenmap of the latent space distribution in Fig. 3. In addition, in order to study associations between the latent subject encodings and their 3D anatomical shapes, we select 6 cases encoded at salient locations in the eigenmap and plot their pertinent 3D anatomies (Fig. 3).We observe a clear differentiation between the encoded normal and MI cases in the eigenmap. Furthermore, the sample anatomies positioned in the area of normal subjects typically exhibit noticeably different shape patterns to the ones located in the MI cluster. For example, the left middle MI subject shows much smaller volume and myocardial thickness changes between ED and ES anatomies than the three normal cases. "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.4,Ablation Study,"In order to assess the contributions of important parts of our network architecture, we next ablate multiple key components and study their effects on prediction performance. More specifically, we individually remove the dropout layer, the KL loss term, and the reconstruction branch, retrain each of the three ablated networks, and report their MI prediction results on the test dataset in Table 3. In addition, we investigate the importance of the multi-objective setting by first training the point cloud autoencoder without a prediction branch with a single reconstruction objective and then applying a logistic regression model for MI prediction to the learned general-purpose latent space representation (Table 3). "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,4.0,Discussion and Conclusion,"In this paper, we have presented the multi-objective point cloud autoencoder as a novel geometric deep learning approach for interpretable MI prediction. The network is able to reconstruct input point clouds with high accuracy and only small localized smoothness artifacts despite the difficult multi-task setting. This shows the suitability of its architecture for efficient multi-scale feature extraction and its ability to effectively capture important 3D shape information in its latent space. Furthermore, the network can simultaneously process all three cardiac substructures at both ED and ES, indicating high flexibility and a potential for further extensions to the full cardiac cycle or other cardiac substructures. In addition, it also allows for more complex 3D shape-based biomarkers to be learned based on inter-temporal and inter-anatomical relationships. All these results are achieved directly on point cloud data, which offers a considerably more efficient storage of anatomical surface information than widely used voxelgrid-based deep learning approaches. Furthermore, the method is fast, fully automatic, and can be readily incorporated into a 3D shape analysis pipeline with cine MRI inputs.The network also outperforms both machine learning techniques based on widely used clinical biomarkers as well as other deep learning approaches for MI prediction. On the one hand, this corroborates previous findings on the increased utility of full 3D shape information compared to single-valued or 2D biomarkers for MI assessment [14,20,29]. On the other hand, it shows the higher capacity of the proposed architecture and training process to extract important novel 3D biomarkers relevant for MI prediction. While we only study MI classification as a sample use case in this work, we believe that the proposed approach can be easily applied to other 3D shape-related pathologies or risk factors.The network achieves these results based on a highly interpretable latent space with a clear differentiation between normal and MI subject encodings. Furthermore, the observed associations between encodings and 3D shapes demonstrate that the latent space is not only discriminative but also that the differentiation is based on clinically plausible 3D shape differences, such as reduced myocardial thinning between ED and ES in MI subjects which is indicative of impaired contraction ability of the heart. This greatly improves the explainability and applicability of the approach, as new subject phenotypes can be quickly and easily compared to other ones with similar encodings. Furthermore, the latent map not only shows well known associations of EF and MI but also a clear differentiation between some normal and MI cases with similar EF values. This indicates that the network is able to capture more intricate biomarkers that go beyond ejection fraction and to successfully utilize them in its MI prediction task while retaining high interpretability.Finally, we show in our ablation studies that all major components of the architecture improve predictive accuracy. We hypothesize that the dropout layer, KL divergence term, and reconstruction branch introduce useful constraints, which have a positive regularizing effect and aid generalization. The multiobjective training procedure accounts for the largest performance gain. This is likely due to the exploited synergies of multiple tasks, which we also believe to be the primary reason for the high separability in the latent space."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 50.
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,1.0,Introduction,"The past years have witnessed a rapid growth of applying deep learning methods in medical imaging [31]. As the performance improves continuously, researchers also find that deep learning models attempt to distinguish illness by using features that are related to a sample's demographic attributes, especially sensitive ones, such as skin tone or gender. The biased performance due to sensitive attributes within different subgroups is defined as unfairness [16]. For example, Seyyed-Kalantari et. al. [21] find that their models trained on chest X-Ray dataset show a significant disparity of True Positive Ratio (TPR) between male and female subgroups. Similar evaluations are done on brain MRI [17], dermatology [12], and mammography [15], which shows that unfairness issues exist extensively in medical applications. If the unfairness of deep learning models is not handled properly, healthcare disparity increases, and human fundamental rights are not guaranteed. Thus, there is a pressing need on investigating unfairness mitigation to eliminate critical biased inference in deep learning models.There are two groups of methods to tackle unfairness. The first group proceeds implicitly with fairness through unawareness [7] by leaving out sensitive attributes when training a single model or deriving invariant representation and ignoring them subjectively when making a decision. However, plenty of evaluations prove that this may lead to unfairness, due to the entangled correlation between sensitive attributes and other variables in the data, and statistical difference between features of different subgroups. The second group explicitly takes sensitive attributes into consideration when training models, for example, train independent models for unfairness mitigation [18,25] with no parameters shared between subgroups. However, this may result in degraded performance because the amount of data for model building is reduced (see Table 1).It is natural to consider whether it is possible to inherit the advantages from both worlds, that is, learning a single model on the whole dataset yet still with explicit modeling of sensitive attributes. Therefore, we propose a framework with a powerful adapter termed Fair Adaptive Batch Normalization (FairAd-aBN). Specifically, FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups, by adding only a few parameters compared with backbones. Thanks to FairAdaBN, the proposed architecture can minimize statistical differences between subgroups and learn subgroup-specific features for unfairness mitigation, which improves model fairness and reserves model precision at the same time. In addition, to intensify the models' ability for balancing performance and fairness, a new loss function named Statistical Disparity Loss (L SD ), is introduced to optimize the statistical disparity in mini-batches and specify fairness constraints on network optimization. L SD also enhances information transmission between subgroups, which is rare for independent models. Finally, a perfect model should have both higher precision and fairness compared to current well-fitted models. However, most of the existing unfairness mitigation methods sacrifice overall performance for building a fairer model [20,22]. Therefore, following the idea of discovering the fairness-accuracy Pareto frontier [32], we propose a novel metric for evaluating the Fairness-Accuracy Trade-off Efficiency (FATE), urging researchers to pay attention to the performance and fairness simultaneously when building prediction models. We evaluate the proposed method based on its application to mitigating unfairness in dermatology diagnosis.To sum up, our contributions are as follows: "
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,2.0,Related Work,"According to [4], unfairness mitigation can be categorized into pre-processing, in-processing, and post-processing based on the instruction stage.Pre-processing. Pre-processing methods focus on the quality of the training set, by organizing fair datasets via datasets combination [21], using generative adversarial networks [11] or sketching model [27] to generate extra images, or directly resampling the train set [18,28]. However, most methods in this category need huge effort due to the preciousness of medical data.Post-processing. Although calibration has been widely used in unfairness mitigation in machine learning tasks, medical applications prefer to use pruning strategies. For example, Wu et. al [26] mitigate unfairness by pruning a pretrained diagnosis model considering the difference of feature importance between subgroups. However, their method needs extra time except for training a precise classification model, while our FairAdaBN is a one-step method.In-Processing. In-processing methods mainly consist of two folds. Some studies mitigate unfairness by directly adding fairness constraints to the cost functions [28], which often leads to overfitting. Another category of research mitigates unfairness by designing complex network architectures like adversarial network [14,30] or representation learning [5]. This family of methods relies heavily on the accuracy of sensitive attribute classifiers in the adversarial branch, leads to bigger models and cannot make full use of pre-trained weights. While our method does not increase the number of parameters significantly and can be applied to several common backbones for dermatology diagnosis."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,3.0,FairAdaBN,"Problem Definition. We assume a medical imaging dataset D = {d 1 , d 2 , ..., d N } with N samples, the i-th sample d i consists of input image X i , sensitive attributes A i and classification ground truth label Y i . i.e.A is a binary variable (e.g., skin tone, gender), which splits the dataset into the unprivileged group, D A=0 , which has a lower average performance than the overall performance, and the privileged group, D A=1 , which has a higher average performance than the overall performance. Using accuracy as the performance metric for example, for a neural network f θ (•), our goal is to minimize the accuracy gap between D A=0 and D A=1 by finding a proper θ.  In this paper, we propose FairAdaBN, which replaces normalization layers in vanilla models with adaptive batch normalization layers, while sharing other layers between subgroups. The overview of our method is shown in Fig. 1.Batch normalization (BN) is a ubiquitous network layer that normalizes minibatch features using statistics [10]. Let x ∈ R C×W ×H denote a given layer's output feature map, where C, W, H is the number of channels, width, and height of the feature map. The BN function is defined as:where μ(x), σ(x) is the mean and standard deviation of the feature map computed in the mini-batch, β and γ denotes the learnable affine parameters. We implant the attribute awareness into BN, named FairAdaBN, by parallelizing multiple normalization blocks that are carefully designed for each subgroup. Specifically, for subgroup D A=a , its adaptive affine parameter γ a and β a are learnt by samples in D A=a . Thus, the adaptive BN function for subgroup D A=a is given by Eq. 3.where a is the index of the sensitive attribute corresponding to the current input image, μ α , σ α are computed across subgroups independently.The FairAdaBN acquires subgroup-specific knowledge by learning the affine parameter γ and β. Therefore, the feature maps of subgroups can be aligned and the unfair representation between privileged and unprivileged groups can be mitigated. By applying FairAdaBN on vanilla backbones, the network can learn subgroup-agnostic feature representations by the sharing parameters of convolution layers, and subgroup-specific feature representations using respective BN parameters, resulting in lower fairness criteria. The detailed structure of FairAdaBN is shown in Fig. 1, we display the minimum unit of ResNet for simplification. Note that the normalization layer in the residual branch is not changed for faster convergence.In this paper, we aim to retain skin lesion classification accuracy and improve model fairness simultaneously. The loss function consists of two parts: (i) the cross-entropy loss, L CE , constraining the prediction precision, and (2) the statistical disparity loss L SD as in Eq. 4, aiming to minimize the difference of prediction probability between subgroups and give extra limits on fairness.where N cg means the number of classification categories.The overall loss function is given by the sum of the two parts, with a hyperparameter α to adjust the degree of constraint on fairness."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.1,Evaluation Metrics,"Lots of fairness criteria are proposed including statistical parity [7], equalized odds [9], equal opportunity [9], counterfactual fairness [13], etc. In this paper, we use equal opportunity and equalized odds as fairness criteria. For equal opportunity, we split it into EOpp0 and EOpp1 considering the ground truth label.However, these metrics only evaluate the level of fairness while do not consider the trade-off between fairness and accuracy. Therefore, inspired by [6], we propose FATE, a metric that evaluates the balance between normalized improvement of fairness and normalized drop of accuracy. The formulas of FATE on different fairness criteria are shown below:where F C can be one of EOpp0, EOpp1, EOdd. ACC denotes accuracy. The subscript m and b denote the mitigation model and baseline model, respectively. λ is a weighting factor that adjusts the requirements for fairness pre-defined by the user considering the real application, here we define λ = 1.0 for simplification. A model obtains a higher FATE if it mitigates unfairness and maintains accuracy.Note that FATE should be combined with utility metrics and fairness metrics, rather than independently."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.2,Dataset and Network Configuration,"We use two well-known dermatology datasets to evaluate the proposed method. The Fitzpatrick-17k dataset [8] contains 16,577 dermatology images in 9 diagnostic categories. The skin tone is labeled with Fitzpatrick's skin phenotype. In this paper, we regard Skin Type I to III as light, and Skin Type IV to VI as dark for simplicity, resulting in a ratio of dark : light ≈ 3 : 7. The ISIC 2019 dataset [1,2,24] contains 25,331 images among 9 different diagnostic categories. We use gender as the sensitive attribute, where female : male ≈ 4.5 : 5.5. Based on subgroup analysis, dark and female are treated as the privileged group, and light and male are treated as the unprivileged group. We randomly split the dataset into train, validation, and test with a ratio of 6:2:2. The models are trained for 600 epochs and the model with the highest validation accuracy is selected for testing. The images are resized or cropped to 128 × 128 for both datasets. Random flipping and random rotation are used for data augmentation. The experiments are carried out on 8 × NVIDIA 3090 GPUs, implemented on PyTorch, and are repeated 3 times. Pre-trained weights from ImageNet are used for all models. The networks are trained using AdamW optimizer with weight decay. The batch size and learning rate are set as 128 and 1e-4, respectively. The hyper-parameter α = 1.0."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.3,Results,"We compare FairAdaBN with Vanilla (ResNet-152), Resampling [18], Ind (independently trained models for each subgroup) [18], GroupDRO [19], EnD [23], and CFair [29], which are commonly used for unfairness mitigation.Results on Fitzpatrick-17k Dataset. Table 1 shows the result of these seven methods on Fitzpatrick-17k dataset. Compared to the Vanilla model, Resampling has a comparable utility, but cannot improve fairness. FairAdaBN achieves the lowest unfairness with only a small drop in accuracy. Besides, FairAdaBN has the highest FATE on all fairness criteria. This is because Ind does not share common information between subgroups, and only part of the dataset is used for training. GroupDRO and EnD rely on the discrimination of features from different subgroups, which is indistinguishable for this task. CFair is more efficient on balanced datasets, while the ratio between light and dark is skewed."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,,Results on ISIC 2019 Dataset.,"Table 1 shows the results on ISIC 2019 dataset. FairAdaBN is the fairest method among the seven methods. Resampling improves fairness sightly but does not outperform ours. GroupDRO mitigates EOpp0 while increasing unfairness on Eopp1 and Eodd. Ind and CFair cannot mitigate unfairness in ISIC 2019 dataset and EnD increases unfairness on EOpp0. The FATE Metric. Figure 2 shows the values of FATE. According to [3], the closer the curve is to the top left corner, the smaller the fairness-accuracy   2, we find that by adding L SD , Eopp 0 decreases significantly, from 1.07 × 10 -2 to 0.48 × 10 -2 . Besides, although adding L SD on ResNet alone increases fairness criteria unexpectedly, fairness criteria decrease when using FairAdaBN and L SD simultaneously. The reason could be the potential connection between FairAd-aBN and L SD , due to the similar form dealing with subgroups.Hyper-parameter α. Our experiments show that α = 1.0 has the best fairness scores and FATE compared to α = 0.1 and α = 2.0. Therefore we select α = 1.0 as our final setting."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,5.0,Conclusion,"We propose FairAdaBN, a simple but effective framework for unfairness mitigation in dermatological disease classification. Extensive experiments illustrate that the proposed framework can mitigate unfairness compared to models without fair constraints, and has a higher fairness-accuracy trade-off efficiency compared with other unfairness mitigation methods. By plugging FairAdaBN into several backbones, its generalization ability is proved. However, the current study only evaluates the effectiveness of FairAdaBN on dermatology datasets, and its generalization ability on other datasets (chest X-Ray, brain MRI) or tasks (segmentation, detection), where unfairness issues also exist, needs to be evaluated in the future. We also plan to explore the unfairness mitigation effectiveness for other universal models [31]."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,,.48 0.09 7.67 3.86 7.73 3.95 48.79 23.04 23.45,"Different Backbones. Firstly, we test FairAdaBN's compatibility on different backbones, by applying FairAdaBN on VGG-19-BN and DenseNet-121. Note that the first and last BN in DenseNet are not changed. The result is shown in"
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,1.0,Introduction,"Deep learning has achieved promising performance in computer-aided diagnosis [1,12,14,24], but it relies on large-scale labeled data to train, which is challenging in medical imaging due to label scarcity and high annotation cost [3,25]. Specifically, expert annotations are required for medical data, which can be costly and time-consuming, especially in tasks such as 3D image segmentation.Transferring pre-trained models to downstream tasks is an effective solution for addressing the label-limited problem [8], but fine-tuning the full network with small downstream data is prone to overfitting [16]. Recently, prompt tuning [5,18] is emerging from natural language processing (NLP), which introduces additional tunable prompt parameters to the pre-trained model and updates only prompt parameters using supervision signals obtained from a few downstream training samples while keeping the entire pre-trained unchanged. By tuning only a few parameters, prompt tuning makes better use of pre-trained knowledge. It avoids driving the entire model with few downstream data, which enables it to outperform traditional fine-tuning in limited labeled data. Building on the recent success of prompt tuning in NLP [5], instead of designing text prompts and Transformer models, we explore visual prompts on Convolutional Neural Networks (CNNs) and the potential to address data limitations in medical imaging.However, previous prompt tuning research [18,28], whether on language or visual models, has focused solely on the model-centric approach. For instance, CoOp [29] models a prompt's context using a set of learnable vectors and optimizes it on a few downstream data, without discussing what kind of samples are more suitable for learning prompts. VPT [13] explores prompt tuning with a vision Transformer, and SPM [17] attempts to handle downstream segmentation tasks through prompt tuning on CNNs, which are also model-centric. However, in downstream tasks with limited labeled data, selective labeling as a data-centric method is crucial for determining which samples are valuable for learning, similar to Active Learning (AL) [23]. In AL, given the initial labeled data, the model actively selects a subset of valuable samples for labeling and improves performance with minimum annotation effort. Nevertheless, directly combining prompt tuning with AL presents several problems. First, unlike the task-specific models trained with initial data in AL, the task-agnostic pre-trained model (e.g., trained by related but not identical supervised or self-supervised task) is employed for data selection with prompt tuning. Second, in prompt tuning, the pre-trained model is frozen, which may render some AL methods inapplicable, such as those previously based on backbone gradient [9] and feature [19]. Third, merging prompt tuning with AL takes work. Their interplay must be considered. However, previous AL methods [27] did not consider the existence of prompts or use prompts to estimate sample value.Therefore, this paper proposes the first framework for selective labeling and prompt tuning (SLPT), combining model-centric and data-centric methods to improve performance in medical label-limited scenarios. We make three main contributions: (1) We design a novel feature-aware prompt updater embedded in the pre-trained model to guide prompt tuning in deep layers. (2) We propose a diversified visual prompt tuning mechanism that provides multi-prompt-based discrepant predictions for selective labeling. (3) We introduce the TESLA strategy which includes both unsupervised diversity selection via task-agnostic features and supervised selection considering prompt-based uncertainty. The results show that SLPT outperforms fine-tuning with just 6% of tunable parameters and achieves 94% of full-data performance by selecting only 5% of labeled data."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.0,Methodology,"Given a task-agnostic pre-trained model and unlabeled data for an initial medical task, we propose SLPT to improve model performance. SLPT consists of three components, as illustrated in Fig. 1: (a) a prompt-based visual model, (b) diversified visual prompt tuning, and (c) tandem selective labeling. Specifically, with SLPT, we can select valuable data to label and tune the model via prompts, which helps the model overcome label-limited medical scenarios."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.1,Prompt-Based Visual Model,"The pre-trained model, learned by supervised or unsupervised training, is a powerful tool for improving performance on label-limited downstream tasks. Finetuning a large pre-trained model with limited data may be suboptimal and prone to overfitting [16]. To overcome this issue, we draw inspiration from NLP [18] and explore prompt tuning on visual models. In order to facilitate prompt tuning on the model's deep layers, we introduce the Feature-aware Prompt Updater (FPU). FPUs are inserted into the network to update deep prompts and features. In Fig. 1(a), an FPU receives two inputs, feature map F out i-1 and prompt P i-1 , of the same shape, and updates to F i and P i through two parallel branches. In the feature branch, F out i-1 and P i-1 are concatenated and fed into a 1x1 convolution and fusion module. The fusion module utilizes ASPP [7] to extract multi-scale contexts. Then a SE [11] module for channel attention enhances context by channel. Finally, the attention output and F out i-1 are element-wise multiplied and added to obtain the updated feature F i . In the prompt branch, the updated feature F i is concatenated with the previous prompt P i-1 , and a parameter-efficient depth-separable convolution is employed to generate the updated prompt P i .To incorporate FPU into a pre-trained model, we consider the model comprising N modular M i (i = 1, ..., N ) and a head output layer. After each M i , we insert an F P U i . Given the input F in i-1 and prompt P i-1 , we have the output feature F i , updated prompt P i and prediction Y as follows:where input X = F 0 , FPU and Head are tuned while M i is not tunable."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.2,Diversified Visual Prompt Tuning,"Inspired by multi-prompt learning [18] in NLP, we investigate using multiple visual prompts to evaluate prompt-based uncertainty. However, initializing and optimizing K prompts directly can significantly increase parameters and may not ensure prompt diversity. To address these challenges, we propose a diversified visual prompt tuning approach. As shown in Fig. 1(b), our method generates K prompts P k ∈ R 1×D×H×W from a meta promptthrough K different upsampling and convolution operations UpConv k . P M is initialized from the statistical probability map of the foreground category, similar to [17]. Specifically, we set the foreground to 1 and the background to 0 in the groundtruth mask, and then average all masks and downsample to 1To enhance prompt diversity, we introduce a prompt diversity loss L div that regularizes the cosine similarity between the generated prompts and maximizes their diversity. This loss is formulated as follows:where P k1 and P k2 represent the k 1 -th and k 2 -th generated prompts, respectively, and || • || 2 denotes the L2 norm. By incorporating the prompt diversity loss, we aim to generate a set of diverse prompts for our visual model. In NLP, using multiple prompts can produce discrepant predictions [2] that help estimate prompt-based uncertainty. Drawing inspiration, we propose a visual prompt tuning approach that associates diverse prompts with discrepant predictions. To achieve this, we design K different data augmentation, heads, and losses based on corresponding K prompts. By varying hyperparameters, we can achieve different data augmentation strengths, increasing the model's diversity and generalization. Different predictions Y k are generated by K heads, each where k = 1, ..., K, M F P U is the pre-trained model with FPU, CE is the crossentropy loss, and λ 1 = λ 2 = λ 3 = 1 weight each loss component. Y represents the ground truth and L is the total loss."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.3,Tandem Selective Labeling,"Previous studies overlook the critical issue of data selection for downstream tasks, especially when available labels are limited. To address this challenge, we propose a novel strategy called TESLA. TESLA consists of two tandem steps: unsupervised diversity selection and supervised uncertainty selection. The first step aims to maximize the diversity of the selected data, while the second step aims to select the most uncertain samples based on diverse prompts.Step 0: Unsupervised Diversity Selection. Since we do not have any labels in the initial and our pre-trained model is task-agnostic, we select diverse samples to cover the entire dataset. To achieve this, we leverage the pre-trained model to obtain feature representations for all unlabeled data. Although these features are task-independent, they capture the underlying relationships, with similar samples having closer feature distances. We apply the k-center method from Coreset [22], which identifies the B samples that best represent the diversity of the data based on these features. These selected samples are then annotated and serve as the initial dataset for downstream tasks.Step 1: Supervised Uncertainty Selection. After prompt tuning with the initial dataset, we obtain a task-specific model that can be used to evaluate data value under supervised training. Since only prompt-related parameters can be tuned while others are frozen, we assess prompt-based uncertainty via diverse prompts, considering inter-prompts uncertainty and intra-prompts uncertainty.In the former, we compute the multi-prompt-based divergence map D, given K probability predictions Y k through K diverse prompts P k , as follows:where KL refers to the KL divergence [15]. Then, we have the divergence score S d = Mean(D), which reflects inter-prompts uncertainty.In the latter, we evaluate intra-prompts uncertainty by computing the mean prediction of the prompts and propose to estimate prompt-based gradients as the model's performance depends on the update of prompt parameters θ p . However, for these unlabeled samples, computing their supervised loss and gradient directly is not feasible. Therefore, we use the entropy of the model's predictions as a proxy for loss. Specifically, we calculate the entropy-based prompt gradient score S g for each unlabeled sample as follows:To avoid manual weight adjustment, we employ multiplication instead of addition. We calculate our uncertainty score S as follows:where max(•) finds the maximum value. We sort the unlabeled data by their corresponding S values in ascending order and select the top B data to annotate."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.1,Experimental Settings,"Datasets and Pre-trained Model. We conducted experiments on automating liver tumor segmentation in contrast-enhanced CT scans, a crucial task in liver cancer diagnosis and surgical planning [1]. Although there are publicly available liver tumor datasets [1,24], they only contain major tumor types and differ in image characteristics and label distribution from our hospital's data. Deploying a model trained from public data to our hospital directly will be problematic.Collecting large-scale data from our hospital and training a new model will be expensive. Therefore, we can use the model trained from them as a starting point and use SLPT to adapt it to our hospital with minimum cost. We collected a dataset from our in-house hospital comprising 941 CT scans with eight categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. It covers both major and rare tumor types. Our objective is to segment all types of lesions accurately. We utilized a pre-trained model for liver segmentation using supervised learning on two public datasets [24] with no data overlap with our downstream task. The nnUNet [12] was used to preprocess and sample the data into 24 × 256 × 256 patches for training. To evaluate the performance, we employed a 5-fold crossvalidation (752 for selection, 189 for test).Metrics. We evaluated lesion segmentation performance using pixel-wise and lesion-wise metrics. For pixel-wise evaluation, we used the Dice per case, a commonly used metric [1]. For lesion-wise evaluation, we first do connected component analysis to predicted and ground truth masks to extract lesion instances, and then compute precision and recall per case [20]. A predicted lesion is regarded as a TP if its overlap with ground truth is higher than 0.2 in Dice. Competing Approaches. In the prompt tuning experiment, we compared our method with three types of tuning: full parameter update (Fine-tuning, Learn-from-Scratch), partial parameter update (Head-tuning, Encoder-tuning, Decoder-tuning), and prompt update (SPM [17]). In the unsupervised diversity selection experiment, we compared our method with random sampling. In the supervised uncertainty selection experiment, we compared our method with random sampling, diversity sampling (Coreset [22], CoreCGN [6]), and uncertainty sampling (Entropy, MC Dropout [10], Ensemble [4], UncertainGCN [6], Ent-gn [26]). Unlike Ensemble, our method was on multi-prompt-based heads. Furthermore, unlike Ent-gn, which computed the entropy-based gradient from a single prediction, we calculated a stable entropy from the muti-prompt-based mean predictions and solely considered the prompt gradient.Training Setup. We conducted the experiments using the Pytorch framework on a single NVIDIA Tesla V100 GPU. The nnUNet [12] framework was used for 3D lesion segmentation with training 500 epochs at an initial learning rate of 0.01. We integrated 13 FPUs behind each upsampling or downsampling of nnUNet, adding only 2.7M parameters. During training, we set k = 3 and employed diverse data augmentation techniques such as scale, elastic, rotation, and mirror. Three sets of TL parameters is (α 1,2,3 = 0.5,0.7,0.3, β 1,2,3 = 0.5,0.3,0.7). To ensure fairness and eliminate model ensemble effects, we only used the model's prediction with k = 1 during testing. We used fixed random seeds and 5-fold cross-validation for all segmentation experiments."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.2,Results,"Evaluation of Prompt Tuning. Since we aim to evaluate the efficacy of prompt tuning on limited labeled data in Table 1, we create a sub-dataset of approximately 5% (40/752) from the original dataset. Specifically, we calculate the class probability distribution vector for each sample based on the pixel class in the mask and use CoreSet with these vectors to select 40 class-balanced samples. Using this sub-dataset, we evaluated various tuning methods for limited  1. Fine-tuning all parameters served as the strongest baseline, but our method, which utilizes only 6% tunable parameters, outperformed it by 5.4%. Although SPM also outperforms fine-tuning, our methods outperform SPM by 1.18% and save 0.44M tunable parameters with more efficient FPU. In cases of limited data, fine-tuning tends to overfit on a larger number of parameters, while prompt tuning does not. The pre-trained model is crucial for downstream tasks with limited data, as it improves performance by 9.52% compared to Learn-from-Scratch. Among the three partial tuning methods, the number of tuning parameters positively correlates with the model's performance, but they are challenging to surpass fine-tuning.Evaluation of Selective Labeling. We conducted steps 0 (unsupervised selection) and 1 (supervised selection) from the unlabeled 752 data and compared our approach with other competing methods, as shown in Table 2. In step 0, without any labeled data, our diversity selection outperformed the random baseline by 1.86%. Building upon the 20 data points selected by our method in step 0, we proceeded to step 1, where we compared our method with eight other data selection strategies in supervised mode. As a result, our approach outperformed other methods because of prompt-based uncertainty, such as Ent-gn and Ensemble, by 2.05% and 1.46%, respectively. Our approach outperformed Coreset by 6.05% and CoreGCN by 5.43%. We also outperformed UncertainGCN by 1.93%. MC Dropout and Entropy underperformed in our prompt tuning, likely due to the difficulty of learning such uncertain data with only a few prompt parameters. Notably, our method outperformed random sampling by 10.28%. These results demonstrate the effectiveness of our data selection approach in practical tasks.Ablation Studies. We conducted ablation studies on S d and S g in TESLA. As shown in Table 2, the complete TESLA achieved the best performance, outperforming the version without S d by 1.84% and the version without S g by 1.98%. It shows that each component plays a critical role in improving performance."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,4.0,Conclusions,"We proposed a pipeline called SLPT that enhances model performance in labellimited scenarios. With only 6% of tunable prompt parameters, SLPT outperforms fine-tuning due to the feature-aware prompt updater. Moreover, we presented a diversified visual prompt tuning and a TESLA strategy that combines unsupervised and supervised selection to build annotated datasets for downstream tasks. SLPT pipeline is a promising solution for practical medical tasks with limited data, providing good performance, few tunable parameters, and low labeling costs. Future work can explore the potential of SLPT in other domains."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,1.0,Introduction,"With the increasing demand for accurate and efficient medical image analysis, Semi-supervised segmentation methods offer a viable solution to tackle the problems associated with scarce labeled data and mitigate the reliance on manual expert annotation. It is often not feasible to annotate all images in a dataset. By exploring the information contained in the unlabeled data, semi-supervised learning [1,2] can help to improve segmentation performance compared to using only a small set of annotated examples.Consistency constraint is a widely-used solution in semi-supervised segmentation to improve performance by making the prediction and/or intermediate features remain consistent under different perturbations. However, it's challenging to obtain universal and appropriate perturbations (e.g., augmentation [3], contexts [4], and decoders [5]) across different tasks. In addition, the efficacy of the consistency loss utilized in semi-supervised segmentation models could be weakened by minor perturbations that have no discernible effect on the predicted results. Conversely, unsuitable perturbations or unclear boundaries between structures could introduce inaccurate supervisory signals, causing a build-up of errors and leading to sub-optimal performance of the model. Recently, some unsupervised prototypical learning methods [6][7][8][9][10] apply the feature matching operation based on the category prototypes to generate the pseudo labels in the semi-supervised segmentation task. Then, the consistency constraint is enforced between the model's prediction and the corresponding prototypical prediction to enhance the model's performance. For example, Xu, et al. [6] propose a cyclic prototype consistency learning framework which involves a two-way flow of information between labeled and unlabeled data. Wu, et al. [7] suggest to facilitate the convergence of class-specific features towards their corresponding high-quality prototypes by promoting their alignment. Zhang, et al. [8,10] exploit the feature distances from prototypes to facilitate online correction of the pseudo label in the training course. Limited by the quantity of prototypes and insufficient feature relation learning, the only one global category prototype used in [6][7][8] for feature matching might omit diversity and impair the representation capability.To put it briefly, prior research has not fully addressed the robustness and variability of prediction results in response to perturbations. To address this, unlike the global prototypes in [6,7], we propose a novel prototype generation method, namely self-aware and cross-sample class prototypes, which generates two distinct prototype predictions to enhance semantic information interaction and ensure disagreement in consistency training. We also propose to use prediction uncertainty between self-aware prototype prediction and multiple predictions to re-weight the consistency constraint loss of cross-sample prototypes. By doing so, we can reduce the adverse effects of label noise in challenging areas such as low-contrast regions or adhesive edges, resulting in a more stable consistency constraint training process. This, in turn, would lead to significantly improved model performance and accuracy. Lastly, we present SCP-Net, a parameter-free semi-supervised segmentation framework (Fig. 1) that incorporates both types of prototypical consistency constraints.The main contributions of this paper can be summarized as: (1) We conduct an in-depth study on prototype-based semi-supervised segmentation methods and propose self-aware prototype prediction and cross-sample prototype predic-tion to ensure appropriate prediction diversity in consistency learning. (2) To enhance the intra-class compactness of pseudo labels, we propose a self-aware prototypical consistency learning method. (3) To boost the stability and reliability of cross-sample prototypical consistency learning, we design a dual loss re-weighting method which helps to reduce the negative effect of noisy pseudo labels. (4) Extensive experiments on ACDC and PROMISE12 datasets have demonstrated that SCP-Net effectively utilizes the unlabeled data and improves semi-supervised segmentation performance with a low annotation ratio."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.0,Method,"In the semi-supervised segmentation task, the training set is divided into the labeled setC×H×W , where H, W , and C are the height, width, and class number, respectively. Our objective is to enhance the segmentation performance of the model by extracting additional knowledge from the unlabeled dataset D u ."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.1,Self-cross Prototypical Prediction,"The prototype in segmentation refers to the aggregated representation that captures the common characteristics of some pixel-wise features from a particular object or class. Let p c k (i) denote the probability of pixel i belonging to class c, f k ∈ R D×H×W represent the feature map of sample k. The class-wise prototypes q c k is defined as follows:Let B denote the batch size. In the iterative training process, one mini-batch contains B × C prototypes for sample k = 1 and other samples with index j = 2, 3, • • • , B. Then, feature similarity is calculated according to the selfaware prototype q c k or cross-sample prototypes q c j to form multiple segmentation probability matrices. Specifically, ŝc kk is the self-aware prototypical similarity map via calculating the cosine similarity between the feature map f k and the prototype vector q c k as Eq. 2:Then, sof tmax function is applied to generate the self-aware probability prediction pkk ∈ R C×H×W based on ŝkk ∈ R C×H×W . Since q c k is aggregated in sample k itself, which can align f k with more homologous features, ensuring the intra-class consistency of prediction. Similarly, we can obtain B -1 cross-sample prototypical similarity maps ŝc kj following Eq. 3:This step ensures that features are associated and that information is exchanged in a cross-image manner. To enhance the reliability of prediction, we take the multiple similarity estimations ŝkj ∈ R C×H×W into consideration and integrate them to get the cross-sample probability prediction pko ∈ R C×H×W : "
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.2,Prototypical Prediction Uncertainty,"To effectively evaluate the predication consistency and training stability in semisupervised settings, we propose a prototypical prediction uncertainty estimation method based on the similarity matrices ŝkk and ŝkj . First, we generate B binary represented mask mkn ∈ R C×H×W via argmax operation and one-hot encoding operation, where n = 1, 2, • • • , B. Then, we sum all masks mkn and dividing it by B to get a normalized probability pnorm as:And a normalized entropy is estimated from pnorm , denoted as e k ∈ R H×W :where e k serves as the overall confidence of multiple prototypical predictions, and a higher entropy equals more prediction uncertainty. Then, we use e k to adjust the pixel-wise weight of labeled and unlabeled samples, which will be elaborated in next subsection."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.3,Unsupervised Prototypical Consistency Constraint,"To enhance the prediction diversity and training effectiveness in consistency learning and mitigate the negative effect of noisy predictions in pkk and pkj , we propose two unsupervised prototypical consistency constraints (PCC) in SPC-Net benefiting from the self-aware prototypical prediction pkk , cross-sample prototypical prediction pkj , and the corresponding uncertainty estimation e k .Self-aware Prototypical Consistency Constraint (SPCC). To boost the intra-class compactness of segmentation prediction, we propose a SPCC method which applies pkk as pseudo-label supervision. Therefore, the loss function of SPCC is formulated as:Cross-sample Prototypical Consistency Constraint (CPCC). To derive dependable knowledge from other training samples, we propose a dual-weighting method for CPCC. First, we take the uncertainty estimation e k into account, which reflects the prediction stability. A higher value of e k indicates that pseudo labels with greater uncertainty may be more susceptible to errors. However, these regions provide valuable information for segmentation performance. To reduce the influence of the suspicious pseudo labels and adjust the contribution of these crucial supervisory signals during training, we incorporate e k in CPCC by setting a weight w 1ki = 1-e ki . Second, we introduce the self-aware probability prediction pkk into the CPCC module. Specifically, we calculate the maximum value of pkk along class c, termed as the self-aware confidence weight w 2ki :w 2k can further enhance the reliability of CPCC. Therefore, the optimized function of CPCC is calculated between cross-sample prototypical prediction pko and pk :Loss Function of SCP-Net We use the combination of cross-entropy loss L ce and Dice loss L Dice to supervise the training process of labeled set [11], which is defined as:For both labeled data and unlabeled data, we leverage L spcc and L cpcc to provide unsupervised consistency constraints for network training and explore the valuable unlabeled knowledge. To sum it up, the overall loss function of SCPNet is the combination of the supervised loss and the unsupervised consistency loss, which is formulated as:λ (t) = 0.1 • e -5(1-t/tmax)2 is a weight using a time-dependent Gaussian warming up function [12] to balance the supervised loss and unsupervised loss. t represents the current training iteration, and t max is the total iterations."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,3.0,Experiments and Results,"Dataset and Evaluation Metric. We validate the effectiveness of our method on two public benchmarks, namely the Automated Cardiac Diagnosis Challenge 1 (ACDC) dataset [13] and the Prostate MR Image Segmentation challenge 2 (PROMISE12) dataset [14]. Implementation Details. Our method adopts U-Net [16] as the baseline. We use the stochastic gradient descent (SGD) optimizer with an initial learning rate of 0.1, and apply the ""poly"" learning rate policy to update the learning rate during training. The batch size is set to 24. Each batch includes 12 labeled slices and 12 unlabeled slices. To alleviate overfitting, we employ random flipping and random rotation to augment data. All comparison experiments and ablation experiments follow the same setup for a fair comparison, we use the same experimental setup for all comparison and ablation experiments. All frameworks are implemented with PyTorch and conducted on a computer with a 3.0 GHz CPU, 128 GB RAM, and four NVIDIA GeForce RTX 3090 GPUs.  Comparison with Other Methods. To demonstrate the effectiveness of SCP-Net, we compare it with 7 state-of-the-art methods for semi-supervised segmentation and fully-supervised (100% labeled ratio) limited supervised (20% labeled ratio) baseline. The quantitative analysis results of ACDC dataset are shown in Table 1. SCP-Net significantly outperforms the limited supervised baseline by 7.02%, 6.13%, and 6.32% on DSC for RV, Myo, and LV, respectively. SCP-Net achieves comparable DSC and ASSD to the fully supervised baseline. (89.69% vs 91.78 and 0.73 vs 0.52). Compared with other methods, SCP-Net achieves the best DSC and ASSD, which is 1.58% and 0.24 higher than the second-best metric, respectively. Moreover, we visualize several segmentation examples of ACDC dataset in Fig. 2. SCP-Net yields consistent and accurate segmentation results for the RV, Myo, and LV classes according to ground truth (GT), proving that the unsupervised prototypical consistency constraints effectively extract valuable unlabeled information for segmentation performance improvement. Table 3 in supplementary material reports the quantitative result for prostate segmentation. We also perform the limited supervised and fully supervised training with 10% labeled ratio and 100% labeled ratio, respectively. SCP-Net surpasses the limited supervised baseline by 16.18% on DSC, and 10.35 on ASSD. In addition, SCP-Net gains the highest DSC of 77.06%, which is 5.63% higher than the second-best CCT. All improvements suggest that SPCC and CPCC are beneficial for exploiting unlabeled information. We also visualize some prostate segmentation examples in the last two rows of Fig. 2. We can observe that SCP-Net generates anatomically-plausible results for prostate segmentation.  Ablation Study. To demonstrate the effectiveness of the key design of SCP-Net, we perform ablation study on PROMISE12 dataset by gradually adding loss components. Table 2 reports the results of ablation results. It can be observed that both the design of SPCC and CPCC promote the semi-supervised segmentation performance according to the first three rows, which demonstrates that PCC extracts valuable information from the image itself and other images, making them well-suited for semi-supervised segmentation. We also visualize the prototypical prediction pkk and pko for different structures in Fig. 3. These predictions are consistent with the ground truths and show intra-class compactness and inter-class discrepancy, which validates that PCC provides effective supervision for semi-supervised segmentation. In the last three rows, the gradually improving performance verifies that the integration of prediction uncertainty w 1 and self-aware confidence w 2 in CPCC improves the reliability and stability of consistency training."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"To summarize, our proposed SCP-Net, which leverages self-aware and crosssample prototypical consistency learning, has successfully tackled the challenges of prediction diversity and training effectiveness in semi-supervised consistency learning. The intra-class compactness of pseudo label is boosted by SPCC.The dual loss re-weighting method of CPCC enhances the model's reliability. The superior segmentation performance demonstrates that SCP-Net effectively exploits the useful unlabeled information to improve segmentation performance given limited annotated data. Moving forward, our focus will be on investigating the feasibility of learning an adaptable number of prototypes that can effectively handle varying levels of category complexity. By doing so, we expect to enhance the quality of prototypical predictions and improve the overall performance."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,,Table 2 .,Lseg + Lcpcc + Lspcc 7 28 w/o w/o 74.99 4.05 Lseg + Lcpcc + Lspcc 7 28 w w/o 76.12 3.78 Lseg + Lcpcc + Lspcc 7 28 w w 77.06 3.52
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_18.
